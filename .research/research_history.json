{
  "research_topic": "Improving efficiency of hyperparameter optimization",
  "queries": [
    "efficient hyperparameter optimization",
    "adaptive hyperparameter tuning",
    "parallel Bayesian optimization",
    "multi-fidelity hyperparameter optimization",
    "hyperband early stopping"
  ],
  "research_study_list": [
    {
      "title": "Bayesian Optimization for Iterative Learning"
    },
    {
      "title": "Hyperparameter Optimization through Neural Network Partitioning"
    },
    {
      "title": "Implicit differentiation of Lasso-type models for hyperparameter optimization"
    },
    {
      "title": "A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization"
    },
    {
      "title": "Efficient Hyperparameter Optimization with Adaptive Fidelity Identification"
    },
    {
      "title": "A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization"
    },
    {
      "title": "Bayesian Optimization for Iterative Learning"
    },
    {
      "title": "Learning to Mutate with Hypergradient Guided Population"
    },
    {
      "title": "Hyperparameter Optimization through Neural Network Partitioning"
    },
    {
      "title": "DP-HyPO: An Adaptive Private Framework for Hyperparameter Optimization"
    },
    {
      "title": "Differentiable Expected Hypervolume Improvement for Parallel Multi-Objective Bayesian Optimization"
    },
    {
      "title": "Multi-fidelity Bayesian Optimization with Max-value Entropy Search and its Parallelization"
    },
    {
      "title": "Parallel Bayesian Optimization of Multiple Noisy Objectives with Expected Hypervolume Improvement"
    },
    {
      "title": "Diversity-Guided Multi-Objective Bayesian Optimization With Batch Evaluations"
    },
    {
      "title": "Collaborative Bayesian Optimization with Fair Regret"
    },
    {
      "title": "Multi-Fidelity Bayesian Optimization via Deep Neural Networks"
    },
    {
      "title": "Efficient Hyperparameter Optimization with Adaptive Fidelity Identification"
    },
    {
      "title": "Batch Multi-Fidelity Bayesian Optimization with  Deep Auto-Regressive Networks"
    },
    {
      "title": "Supervising the Multi-Fidelity Race of Hyperparameter Configurations"
    },
    {
      "title": "Supervising the Multi-Fidelity Race of Hyperparameter Configurations"
    },
    {
      "title": "Conformal Inference is (almost) Free for Neural Networks Trained with Early Stopping"
    },
    {
      "title": "Scaling Laws for Hyperparameter Optimization"
    },
    {
      "title": "Efficient Hyperparameter Optimization with Adaptive Fidelity Identification"
    },
    {
      "title": "Bayesian Optimization for Iterative Learning"
    },
    {
      "title": "Provably Efficient Online Hyperparameter Optimization with Population-Based Bandits"
    }
  ]
}