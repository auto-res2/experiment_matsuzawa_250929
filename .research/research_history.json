{
  "research_topic": "Improving efficiency of hyperparameter optimization",
  "queries": [
    "efficient hyperparameter optimization",
    "adaptive hyperparameter tuning",
    "parallel Bayesian optimization",
    "multi-fidelity hyperparameter optimization",
    "hyperband early stopping"
  ],
  "research_study_list": [
    {
      "title": "Bayesian Optimization for Iterative Learning",
      "abstract": "The performance of deep (reinforcement) learning systems crucially depends on\nthe choice of hyperparameters. Their tuning is notoriously expensive, typically\nrequiring an iterative training process to run for numerous steps to\nconvergence. Traditional tuning algorithms only consider the final performance\nof hyperparameters acquired after many expensive iterations and ignore\nintermediate information from earlier training steps. In this paper, we present\na Bayesian optimization (BO) approach which exploits the iterative structure of\nlearning algorithms for efficient hyperparameter tuning. We propose to learn an\nevaluation function compressing learning progress at any stage of the training\nprocess into a single numeric score according to both training success and\nstability. Our BO framework is then balancing the benefit of assessing a\nhyperparameter setting over additional training steps against their computation\ncost. We further increase model efficiency by selectively including scores from\ndifferent training steps for any evaluated hyperparameter set. We demonstrate\nthe efficiency of our algorithm by tuning hyperparameters for the training of\ndeep reinforcement learning agents and convolutional neural networks. Our\nalgorithm outperforms all existing baselines in identifying optimal\nhyperparameters in minimal time.",
      "full_text": "Bayesian Optimization for Iterative Learning Vu Nguyen ∗ University of Oxford vu@robots.ox.ac.uk Sebastian Schulze ∗ University of Oxford sebastian.schulze@eng.ox.ac.uk Michael A. Osborne University of Oxford mosb@robots.ox.ac.uk Abstract The performance of deep (reinforcement) learning systems crucially depends on the choice of hyperparameters. Their tuning is notoriously expensive, typically requiring an iterative training process to run for numerous steps to convergence. Traditional tuning algorithms only consider the ﬁnal performance of hyperparam- eters acquired after many expensive iterations and ignore intermediate information from earlier training steps. In this paper, we present a Bayesian optimization (BO) approach which exploits the iterative structure of learning algorithms for efﬁcient hyperparameter tuning. We propose to learn an evaluation function compress- ing learning progress at any stage of the training process into a single numeric score according to both training success and stability. Our BO framework is then balancing the beneﬁt of assessing a hyperparameter setting over additional train- ing steps against their computation cost. We further increase model efﬁciency by selectively including scores from different training steps for any evaluated hyper- parameter set. We demonstrate the efﬁciency of our algorithm by tuning hyperpa- rameters for the training of deep reinforcement learning agents and convolutional neural networks. Our algorithm outperforms all existing baselines in identifying optimal hyperparameters in minimal time. 1 Introduction Deep learning (DL) and deep reinforcement learning (DRL) have led to impressive breakthroughs in a broad range of applications such as game play [26, 36], motor control [43], and image recognition [20]. To maintain general applicability, these algorithms expose sets of hyperparameters to adapt their behavior to any particular task at hand. This ﬂexibility comes at the price of having to tune an additional set of parameters – poor settings lead to drastic performance losses [11, 30, 37]. On top of being notoriously sensitive to these choices, deep (reinforcement) learning systems often have high training costs, in computational resources and time. For example, a single training run on the Atari Breakout game took approximately 75 hours on a GPU cluster [26]. Tuning DRL parameters is further complicated as only noisy evaluations of an agent’s ﬁnal performance are obtainable. Bayesian optimization (BO) [12, 28, 35] has recently achieved considerable success in optimizing these hyperparameters. This approach casts the tuning process as a global optimization problem based on noisy evaluations of a black-box function f . BO constructs a surrogate model typically using a Gaussian process (GP) [31], over this unknown function. This GP surrogate is used to build an acquisition function [13, 44] which suggests the next hyperparameter to evaluate. In modern machine learning (ML) algorithms [15], the training process is often conducted in an iterative manner. A natural example is given by deep learning where training is usually based on stochastic gradient descent and other iterative procedures. Similarly, the training of reinforcement learning agents is mostly carried out using multiple episodes. The knowledge accumulated during these training iterations can be useful to inform BO. However, most existing BO approaches [35] ∗These authors contributed equally. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada. arXiv:1909.09593v5  [cs.LG]  16 Jan 2021deﬁne the objective function as the average performance over the ﬁnal training iterations. In doing so, they ignore the information contained in the preceding training steps. In this paper, we present a Bayesian optimization approach for tuning algorithms where iterative learning is available – the cases of deep learning and deep reinforcement learning. First, we consider the joint space of input hyperparameters and number of training iterations to capture the learning progress at different time steps in the training process. We then propose to transform the whole training curve into a numeric score according to user preference. To learn across the joint space efﬁciently, we introduce a data augmentation technique leveraging intermediate information from the iterative process. By exploiting the iterative structure of training procedures, we encourage our algorithm to consider running a larger number of cheap (but high-utility) experiments, when cost- ignorant algorithms would only be able to run a few expensive ones. We demonstrate the efﬁciency of our algorithm on training DRL agents on several well-known benchmarks as well as the training of convolutional neural networks. In particular, our algorithm outperforms existing baselines in ﬁnding the best hyperparameter in terms of wall-clock time. Our main contributions are: • an algorithm to optimize the learning curve of a ML algorithm by using training curve compression, instead of averaged ﬁnal performance; • an approach to learn the compression curve from the data and a data augmentation tech- nique for increased sample-efﬁciency; • demonstration on tuning DRL and convolutional neural networks. 2 Related Work in Iteration-Efﬁcient Bayesian Optimization The ﬁrst algorithm category employs stopping criteria to terminate some training runs early and allo- cate resources towards more promising settings. These criteria typically involve projecting towards a ﬁnal score from early training stages. Freeze-thaw BO [42] models the training loss over time us- ing a GP regressor under the assumption that the training loss roughly follows an exponential decay. Based on this projection, training resources are allocated to the most promising settings. Hyperband [8, 23] dynamically allocates computational resources (e.g. training epochs or dataset size) through random sampling and eliminates under-performing hyperparameter settings by successive halving. Attempts have also been made to improve the epoch efﬁciency of other hyperparameter optimization algorithms in [5, 7, 18] which predict the ﬁnal learning outcome based on partially trained learning curves to identify hyperparameter settings that are expected to under-perform and early-stop them. In the context of DRL, however, these stopping criteria, including the exponential decay assumed in Freeze-thaw BO [42], may not be applicable, due to the unpredictable ﬂuctuations of DRL reward curves. In the supplement, we illustrate the noisiness of DRL training. The second category [16, 17, 23, 41, 48] aims to reduce the resource consumption of BO by utilizing low-ﬁdelity functions which can be obtained by using a subset of the training data or by training the ML model for a small number of iterations. Multi-task BO [41] requires the user to deﬁne a division of the dataset into pre-deﬁned and discrete subtasks. Multi-ﬁdelity BO with continuous approximation (BOCA) [16] and hierarchical partition [34] extend this idea to continuous settings. Speciﬁcally, BOCA ﬁrst selects the hyperparameter input and then the corresponding ﬁdelity to be evaluated at. The ﬁdelity in this context refers to the use of different number of learning iterations. Analogous to BOCA’s consideration of continuous ﬁdelities, Fabolas [17] proposes to model the combined space of input hyperparameter and dataset size and then select the optimal input and dataset size jointly. The above approaches typically identify performance of hyperparameters via the average (either training or validation) loss of the last learning iterations. Thereby, they do not account for potential noise in the learning process (e.g., they might select unstable settings that jumped to high perfor- mance in the last couple of iterations). 3 Bayesian Optimization for Iterative Learning (BOIL) Problem setting. We consider training a machine learning algorithm given a d-dimensional hy- perparameter x ∈X ⊂Rd for t iterations. This process has a training time costc(x,t) and produces 20 100 200 300 400 500 #Episode t 0.80 0.85 0.90 0.95 1.00x Tmin Tmax Augmented Obs Observation 0 100 200 300 400 500 0 50 100 150 200Score 4 18 34 8 45 5 14 26Reward Curve Sigmoid Func 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l( . |m * 0 ,g* 0 ) for Cifar10 m* 0 =-4.0 g* 0 =1.476 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l( . |m * 0 ,g* 0 ) for Reacher m* 0 =2.779 g* 0 =1.973 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l( . |m * 0 ,g* 0 ) for CartPole m* 0 =-3.266 g* 0 =3.0 Figure 1: Left: the score in pink box is a convolution of the reward curve r(·| x = 0.9,t = 500) and a Sigmoid function l(u |g0,m0) = 1 1+exp(−g0[u−m0]) up to time step t. Bottom: observations are selected to augment the dataset (red dots). The heatmap indicates the GP predictive mean µ for f across the number of episodest used to train an agent. Tmin and Tmax are two user-deﬁned thresholds for the number of training episodes. x is a hyperparameter to be tuned. Right: we learn the optimal parameter g∗ 0 and m∗ 0 for each experiment separately. training evaluations r(·| x,t) for t iterations, t ∈[Tmin,Tmax]. These could be episode rewards in DRL or training accuracies in DL. An important property of iterative training is that we know the whole curve at preceding steps r(t′|x,t), ∀t′≤t. Given the raw training curve r(·| x,t), we assume an underlying smoothed black-box function f , deﬁned in Sec. 3.2. Formally, we aim to ﬁnd x∗= argmaxx∈X f (x,Tmax); at the same time, we want to keep the overall training time, ∑N i=1 c(xi,ti), of evaluated settings [xi,ti] as low as possible. We summarize our variables in Table 1 in the supplement for ease of reading. 3.1 Selecting a next point using iteration-efﬁcient modeling We follow popular designs in [17, 19, 39, 41] and model the cost-sensitive black-box function as f (x,t) ∼GP(0,k([x,t],[x′,t′])), where k is an appropriate covariance functions and [x,t] ∈Rd+1. For simplicity and robustness, the cost function c(x,t) is approximated by a linear regressor. De- pending on the setting, it may be more appropriate to employ a second GP or different parametric model if the cost has a more complex dependence on hyperparameters x and iterations t. We regu- larly (re-)optimize both kernel and cost function parameters in between point acquisitions. More speciﬁcally, we choose the covariance function as a productk ([x,t],[x′,t′]) =k(x,x′)×k(t,t′) to induce joint similarities over parameter and iteration space. We estimate the predictive mean and uncertainty for a GP [31] at any input z∗= [x∗,t∗] as µ (z∗) =k∗ [ K +σ2 y I ]−1 y (1) σ2 (z∗) =k∗∗−k∗ [ K +σ2 y I ]−1 kT ∗ (2) where y = [yi]∀i, k∗= [k (z∗,zi)]∀i, K = [k (zi,zj)]∀i, j, k∗∗= k (z∗,z∗), and σ2 y is the noise variance of f . Cost predictions at any particular parameter x and time t are given by µc([x∗,t∗]) =βT [x,t], where β is directly computed from data {Z = [xi,ti],c = [ci]}∀i as β = (ZT Z)−1Zc [1]. Our goal is to select a point with high function value (exploitation), high uncertainty (exploration) and low cost (cheap). At each iteration n, we query the input parameter xn and the number of iteration tn [38, 48]: zn = [xn,tn] = argmax x∈X ,t∈[Tmin,Tmax] α(x,t)/µc(x,t). (3) 3Although our framework is available for any acquisition choices [13, 22, 47], to cope with output noise, we follow [45] and slight modify the expected improvement criterion using the maximum mean GP prediction µmax n . Let λ = µn(z)−µmaxn σn(z) , we then have a closed-form for the new expected improvement (EI) as αEI n (z) =σn (z)φ (λ) + [µn (z)−µmax n ]Φ(λ) where φ is the standard normal p.d.f., Φ is the c.d.f, µn and σn are the GP predictive mean and variance deﬁned in Eq. (1) and Eq. (2), respectively. 3.2 Training curve compression and estimating the transformation function Existing BO approaches [4, 23] typically deﬁne the objective function as an average loss over the ﬁnal learning episodes. However, this does not take into consideration how stable performance is or the training stage at which it has been achieved. We argue that averaging learning losses is likely misleading due to the noise and ﬂuctuations of our observations (learning curves) – particularly during the early stages of training. We propose to compress the whole learning curve into a numeric score via a preference function representing the user’s desired training curve. In the following, we use the Sigmoid function (speciﬁcally the Logistic function) to compute the utility score as y = ˆy(r,m0,g0) =r(·|x,t)•l(·|m0,g0) = t ∑ u=1 r(u |x,t) 1 +exp(−g0 [u −m0]) (4) where •is a dot product, a Logistic function l(·| m0,g0) is parameterized by a growth parameter g0 deﬁning a slope and the middle point of the curve m0. The optimal parameters g0 and m0 are estimated directly from the data. We illustrate different shapes of l parameterized by g0 and m0 in the appendix. The Sigmoid preference has a number of desirable properties. As early weights are small, less credit is given to ﬂuctuations at the initial stages, making it less likely for our surrogate to be biased towards randomly well performing settings. However, as weights monotonically increase, hyperparameters with improving performance are preferred. As weights saturate over time, stable, high performing conﬁgurations are preferred over short “performance spikes” characteristic of un- stable training. Lastly, this utility score assigns higher values to the same performance if it is being maintained over more episodes. Learning the transformation function from data. Different compression curves l(), parameter- ized by different choices of g0 and m0 in Eq. (4), may lead to different utilities y and thus affect the performance. The optimal values of g∗ 0 and m∗ 0 are unknown in advance. Therefore, we propose to learn these values g∗ 0 and m∗ 0 directly from the data. Our intuition is that the ‘optimal’ compression curve l(m∗ 0,g∗ 0) will lead to a better ﬁt of the GP. This better GP surrogate model, thus, will result in better prediction as well as optimization performance. We parameterize the GP log marginal likelihood L [31] as the function of m0 and g0: L(m0,g0) =1 2 ˆyT ( K +σ2 y I )−1 ˆy −1 2 ln ⏐⏐K +σ2 y I ⏐⏐ +const (5) where σ2 y is the output noise variance, ˆy is the function of m0 and g0 deﬁned in Eq. (4). We optimize m0 and g0 (jointly with other GP hyperparameters) using multi-start gradient descent. We derive the derivative ∂L ∂m0 = ∂L ∂ ˆy ∂ ˆy ∂m0 and ∂L ∂g0 = ∂L ∂ ˆy ∂ ˆy ∂g0 which can be computed analytically as: ∂L ∂ ˆy = ( K +σ2 y IN )−1 ˆy; ∂ ˆy ∂m0 = −g0 ×exp(−g0 [u −m0]) [1 +exp(−g0 [u −m0])]2 ; ∂ ˆy ∂g0 = −m0 ×exp(−g0 [u −m0]) [1 +exp(−g0 [u −m0])]2 . The estimated compression curves are illustrated in Right Fig. 1 and in Sec. 4.1. 3.3 Augmenting the training data When evaluating a parameter x over t iterations, we obtain not only a ﬁnal score but also all reward sequences r(t′|x,t),∀t′= 1,..., t. The auxiliary information from the curve can be useful for BO. Therefore, we propose to augment the information from the curve into the sample set of our GP model. A naïve approach for augmentation is to add a full curve of points {[x, j],yj}t j=1 where yj is computed using Eq. (4). However, this approach can be redundant and may im- pose serious issues in the conditioning of the GP covariance matrix. As we cluster 40.80 0.85 0.90 0.95 1.00 200 300 400 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 x GP variance 2 0 2 4 6 8 10 12 0.000 0.025 0.050 0.075 0.100 0.125 0.150 0.175 0.80 0.85 0.90 0.95 1.00 200 300 400 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 GP variance 400 320 240 160 80 0 80 160 240 0.000 0.005 0.010 0.015 0.020 0.025 0.030 0.035 0.040 x Figure 2: GP with different settings. Left: our augmentation. Right: using a full curve. If we add too many observations, the GP covariance matrix becomes ill-conditioned. On the right, the GP ﬁt is poor with a large mean estimate range of [−400,240] even though the output is standardized N (0,1). All x-axis are over x, a hyperparameter to be tuned. more evaluations closely, the conditioning of the GP covariance degrades further, as dis- cussed in [24]. This conditioning issue is especially serious in our noisy DRL settings. 0 10 20 30 40 50 60 Iterations 0 5 10 15 20 25Log of Condition Number Condition Number of GP Covariance Augmentation No Augmentation Full Curve Reasonable Threshold Figure 3: The condition number of GP covari- ance matrix deteriorates if we add the whole curve of points into a GP. The large condition number indicates the nearness to singularity. We highlight this effect on GP estimation in Fig. 2 wherein the GP mean varies erratically when the natural log of the condition number of the GP co- variance matrix goes above 25 (see Fig. 3) as we include the whole curve. Selecting subset of points from the curve. Dif- ferent solutions, such as the addition of artiﬁcial noise or altering the kernel’s length-scales, have been proposed. We decide to use an active learn- ing approach [10, 29] as sampled data points are expected to contain a lot of redundant informa- tion. As a consequence, the loss of information from sub-sampling the data should be minimal and information-eroding modiﬁcation of the ker- nel matrix itself can be avoided. As a side beneﬁt, the reduced number of sampled points speeds up inference in our GP models. In particular, we select samples at the maximum of the GP predictive uncertainty. Formally, we sequentially select a set Z = [z1,...zM], zm = [x,tm], by varying tm while keeping x ﬁxed as zm =argmax ∀t′≤t σ([x,t′] |D′),∀m ≤M s.t. lnof cond(K) ≤δ (6) where D′= D∪{zj = [x,tj]}m−1 j=1 . This sub-optimisation problem is done in a one-dimensional space of t′∈{Tmin,..., t}, thus it is cheap to optimize using (multi-start) gradient descent (the derivative of GP predictive variance is available [31]). Alternatively, a ﬁxed-size grid could be considered, but this could cause conditioning issues when a point in the grid [ x,tgrid ] is placed near another existing point [ x′,tgrid ] , i.e., ||x −x′||2 ≤ε for some small ε. These generated points Z are used to calculate the output r(zm) and augmented into the observation set (X,Y ) for ﬁtting the GP. The number of samplesM is adaptively chosen such that the natural log of the condition number of the covariance matrix is less than a threshold. This is to ensure that the GP covariance matrix condition number behaves well by reducing the number of unnecessary points added to the GP at later stages. We compute the utility score ym given zm for each augmented point using Eq. (4). In addition, we can estimate the running time cm using the predictive mean µc(zm). We illustrate the augmented observations and estimated scores in Fig. 1. We summarize the overall algorithm in Alg. 1. To enforce non-negativity and numerical stability, we make use of the transformations α ←log[1 +exp(α)] and µc ←log[1 +exp(µc)]. 4 Experiments We assess our model by tuning hyperparameters for two DRL agents on three environments and a CNN on two datasets. We provide additional illustrations and experiments in the appendix. 5Algorithm 1 Bayesian Optimization with Iterative Learning (BOIL) Input: #iter N, initial data D0, z = [x,t]. Output: optimal x∗and y∗= max∀y∈DN y 1: for n = 1....N do 2: Fit a GP to estimate µf (),σf () from Eqs. (1,2) and a LR for cost µc() 3: Select zn = argmaxx,t α(x,t)/µc(x,t) and observe a curve r and a cost c from f (zn) 4: Compressing the learning curve r(zn) into numeric score using Eq. (4). 5: Sample augmented points zn,m,yn,m,cn,m,∀m ≤M given the curve and Dn in Eq. (6) 6: Augment the data into Dn and estimate Logistic curve hyperparameters m0 and g0. 7: end for Experimental setup. All experimental results are averaged over 20 independent runs with differ- ent random seeds. Final performance is estimated by evaluating the chosen hyperparameter over the maximum number of iterations. All experiments are executed on a NVIDIA 1080 GTX GPU using the tensorﬂow-gpu Python package. The DRL environments are available through the OpenAI gym [3] and Mujoco [43]. Our DRL implementations are based on the open source from Open AI Baselines [6]. We release our implementation at https://github.com/ntienvu/BOIL. We use square-exponential kernels for the GP in our model and estimate their parameters by maxi- mizing the marginal likelihood [31]. We set the maximum number of augmented points to beM = 15 and a threshold for a natural log of GP condition numberδ = 20. We note that the optimization over- head is much less than the black-box function evaluation time. Baselines. We compare with Hyperband [23] which demonstrated empirical successes in tuning deep learning applications in an iteration-efﬁcient manner. We extend the discrete multi-task BO [41] to the continuous case – which can also be seen as continuous multi-ﬁdelity BO [16, 39] as in our setting, they both consider cost-sensitivity and iteration-efﬁciency. We, therefore, label the two baselines as continuous multi-task/ﬁdelity BO (CM-T/F-BO). We have ignored the minor difference in these settings, such as multi-task approaches jointly optimizes the ﬁdelity and input while BOCA [16] ﬁrst selects the input and then the ﬁdelity. Our focus is to demonstrate the effectiveness of optimizing the learning curve using compression and augmentation techniques. We therefore omit the comparison of various acquisition functions and kernel choices which can easily be used in our model. We also do not compare with Fabolas [17] which is designed to vary dataset sizes, not iteration numbers. We would expect the performance of Fabolas to be close to CM-T/F-BO. We are unable to compare with FreezeThaw as the code is not available. However, the curves in our setting are not exponential decays and thus ill-suited to their model (see last ﬁgure in the appendix). We have considered an ablation study in the appendix using a time kernel following the exponential decay proposed in Freeze-thaw method [42]. Task descriptions. We consider three DRL settings including a Dueling DQN (DDQN) [46] agent in the CartPole-v0 environment and Advantage Actor Critic (A2C) [25] agents in the InvertedPendulum-v2 and Reacher-v2 environments. In addition to the DRL applications, we tune 6 hyperparameters for training a convolutional neural network [21] on the SVHN dataset and CI- FAR10. Due to space considerations, we refer to the appendix for further details. 4.1 Model illustration /uni00000013/uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni00000013 /uni00000018 /uni00000014/uni00000013 /uni00000014/uni00000018/uni00000006/uni00000024/uni00000058/uni0000004a/uni00000050/uni00000048/uni00000051/uni00000057/uni00000048/uni00000047/uni00000003/uni00000032/uni00000045/uni00000056 /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni00000057/uni00000048/uni00000047/uni00000003/uni00000024/uni00000058/uni0000004a/uni00000050/uni00000048/uni00000051/uni00000057/uni00000048/uni00000047/uni00000003/uni00000032/uni00000045/uni00000056 Figure 4: DDQN on CartPole. The number of augmented observations reduces over time. We ﬁrst illustrate the estimated compression func- tion l(m∗ 0,g∗ 0) in Right Fig. 1 from different experi- ments. These Logistic parameters g∗ 0 and m∗ 0 are es- timated by maximizing the GP marginal likelihood and used for compressing the curve. We show that the estimated curve from CartPole tends to reach the highest performance much earlier than Reacher because CartPole is somewhat easier to train than Reacher. We next examine the count of augmented observa- tions generated per iteration in Fig. 4. Although this number is ﬂuctuating, it tends to reduce over 6/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000018/uni00000013/uni00000013 /uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048 /uni0000001a/uni00000013 /uni00000019/uni00000013 /uni00000018/uni00000013 /uni00000017/uni00000013 /uni00000016/uni00000013 /uni00000015/uni00000013 /uni00000014/uni00000013 /uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni00000035/uni00000048/uni00000044/uni00000046/uni0000004b/uni00000048/uni00000055/uni00000040/uni00000003/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047/uni00000003/uni00000058/uni00000056/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000032/uni00000053/uni00000057/uni0000004c/uni00000050/uni0000004c/uni0000005d/uni00000048/uni00000047/uni00000003/uni00000033/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013/uni00000015/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013/uni0000001b/uni00000013/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000014/uni00000015/uni00000013/uni00000013/uni00000014/uni00000017/uni00000013/uni00000013 /uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048 /uni00000015/uni00000018 /uni00000018/uni00000013 /uni0000001a/uni00000018 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000015/uni00000018 /uni00000014/uni00000018/uni00000013 /uni00000014/uni0000001a/uni00000018 /uni00000015/uni00000013/uni00000013/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni0000002c/uni00000059/uni00000033/uni00000048/uni00000051/uni00000040/uni00000003/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047/uni00000003/uni00000058/uni00000056/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000032/uni00000053/uni00000057/uni0000004c/uni00000050/uni0000004c/uni0000005d/uni00000048/uni00000047/uni00000003/uni00000033/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000025/uni00000032/uni0000002c/uni0000002f Figure 5: The learning curves of the best found parameters by different approaches. The curves show that BO-L and BOIL reliably identify parameters leading to stable training. BOIL takes only half total time to ﬁnd this optimal curve. time. BOIL does not add more augmented observations at the later stage when we have gained sufﬁcient information and GP covariance conditioning falls below our threshold δ = 20. 4.2 Ablation study of curve compression To demonstrate the impact of our training curve compression, we compare BOIL to vanilla Bayesian optimization (BO) and with compression (BO-L) given the same number of iterations at Tmax. We show that using the curve compression leads to stable performance, as opposed to the existing tech- nique of averaging the last iterations. We plot the learning curves of the best hyperparameters identiﬁed by BO, BO-L and BOIL. Fig. 5 shows the learning progress over Tmax episodes for each of these. The curves are smoothed by averaging over 100 consecutive episodes for increased clarity. We ﬁrst note that all three algorithms eventually obtain similar performance at the end of learning. However, since BO-L and BOIL take into account the preceding learning steps, they achieve higher performance more quickly. Furthermore, they achieve this more reliably as evidenced by the smaller error bars (shaded regions). 4.3 Tuning deep reinforcement learning and CNN We now optimize hyperparameters for deep reinforcement learning algorithms; in fact, this applica- tion motivated the development of BOIL. The combinations of hyperparameters to be tuned, target DRL algorithm and environment can be found in the appendix. Comparisons by iterations and real-time. Fig. 6 illustrates the performance of different algo- rithms against the number of iterations as well as real-time (the plots for CIFAR10 are in the ap- pendix). The performance is the utility score of the best hyperparameters identiﬁed by the baselines. Across all three tasks, BOIL identiﬁes optimal hyperparameters using signiﬁcantly less computation time than other approaches. The plots show that other approaches such as BO and BO-L can identify well-performing hyperpa- rameters in fewer iterations than BOIL. However, they do so only considering costly, high-ﬁdelity evaluations resulting in signiﬁcantly higher evaluation times. In contrast to this behavior, BOIL ac- counts for the evaluation costs and chooses to initially evaluate low-ﬁdelity settings consuming less time. This allows fast assessments of a multitude of hyperparameters. The information gathered here is then used to inform later point acquisitions. Hereby, the inclusion of augmented observations is crucial in offering useful information readily available from the data. In addition, this augmenta- tion is essential to prevent from the GP kernel issue instead of adding the full curve of points into our GP model. Hyperband [23] exhibits similar behavior in that it uses low ﬁdelity (small t) evaluations to reduce a pool of randomly sampled conﬁgurations before evaluating at high ﬁdelity (large t). To deal with noisy evaluations and other effects, this process is repeated several times. This puts Hyperband at a disadvantage particularly in the noisy DRL tasks. Since early performance ﬂuctuates hugely, Hyperband can be misled in where to allocate evaluation effort. It is then incapable of revising 7/uni00000013 /uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni00000014/uni00000014/uni0000001a/uni00000011/uni00000018 /uni00000014/uni00000015/uni00000013/uni00000011/uni00000013 /uni00000014/uni00000015/uni00000015/uni00000011/uni00000018 /uni00000014/uni00000015/uni00000018/uni00000011/uni00000013 /uni00000014/uni00000015/uni0000001a/uni00000011/uni00000018 /uni00000014/uni00000016/uni00000013/uni00000011/uni00000013 /uni00000014/uni00000016/uni00000015/uni00000011/uni00000018 /uni00000014/uni00000016/uni00000018/uni00000011/uni00000013 /uni00000014/uni00000016/uni0000001a/uni00000011/uni00000018/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000027/uni00000027/uni00000034/uni00000031/uni00000010/uni00000026/uni00000044/uni00000055/uni00000057/uni00000033/uni00000052/uni0000004f/uni00000048/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000003/uni00000044/uni00000051/uni00000047/uni00000003 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013 /uni00000018/uni00000013/uni00000014/uni00000013/uni00000013/uni00000014/uni00000018/uni00000013/uni00000015/uni00000013/uni00000013/uni00000015/uni00000018/uni00000013/uni00000016/uni00000013/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni00000014/uni00000014/uni0000001a/uni00000011/uni00000018 /uni00000014/uni00000015/uni00000013/uni00000011/uni00000013 /uni00000014/uni00000015/uni00000015/uni00000011/uni00000018 /uni00000014/uni00000015/uni00000018/uni00000011/uni00000013 /uni00000014/uni00000015/uni0000001a/uni00000011/uni00000018 /uni00000014/uni00000016/uni00000013/uni00000011/uni00000013 /uni00000014/uni00000016/uni00000015/uni00000011/uni00000018 /uni00000014/uni00000016/uni00000018/uni00000011/uni00000013 /uni00000014/uni00000016/uni0000001a/uni00000011/uni00000018/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000027/uni00000027/uni00000034/uni00000031/uni00000010/uni00000026/uni00000044/uni00000055/uni00000057/uni00000033/uni00000052/uni0000004f/uni00000048/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000003/uni00000044/uni00000051/uni00000047/uni00000003 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013 /uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni00000014/uni00000015/uni0000001b /uni00000014/uni00000015/uni0000001c /uni00000014/uni00000016/uni00000013 /uni00000014/uni00000016/uni00000014 /uni00000014/uni00000016/uni00000015 /uni00000014/uni00000016/uni00000016 /uni00000014/uni00000016/uni00000017/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni00000035/uni00000048/uni00000044/uni00000046/uni0000004b/uni00000048/uni00000055/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013 /uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni00000014/uni00000015/uni0000001b /uni00000014/uni00000015/uni0000001c /uni00000014/uni00000016/uni00000013 /uni00000014/uni00000016/uni00000014 /uni00000014/uni00000016/uni00000015 /uni00000014/uni00000016/uni00000016 /uni00000014/uni00000016/uni00000017/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni00000035/uni00000048/uni00000044/uni00000046/uni0000004b/uni00000048/uni00000055/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013 /uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni00000015/uni00000018 /uni00000016/uni00000013 /uni00000016/uni00000018 /uni00000017/uni00000013 /uni00000017/uni00000018 /uni00000018/uni00000013 /uni00000018/uni00000018 /uni00000019/uni00000013 /uni00000019/uni00000018/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c /uni0000003e/uni00000026/uni00000031/uni00000031/uni00000010/uni00000036/uni00000039/uni0000002b/uni00000031/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000019/uni00000003/uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000053/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013/uni0000001a/uni00000013/uni0000001b/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni00000015/uni00000018 /uni00000016/uni00000013 /uni00000016/uni00000018 /uni00000017/uni00000013 /uni00000017/uni00000018 /uni00000018/uni00000013 /uni00000018/uni00000018 /uni00000019/uni00000013 /uni00000019/uni00000018/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c /uni0000003e/uni00000026/uni00000031/uni00000031/uni00000010/uni00000036/uni00000039/uni0000002b/uni00000031/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000019/uni00000003/uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000053/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f Figure 6: Comparison over BO evaluations (Left) and real-time (Right). Given the same time bud- get, CM-T/F-BO, Hyperband and BOIL can take more evaluations than vanilla BO, BO-L and Rand. BOIL outperforms other competitors in ﬁnding the optimal parameters in an iteration-efﬁcient man- ner. CM-T/F-BO does not augment the observations from the curve and requires more evaluations. The results of InvertedPendulum and CNN-CIFAR10 are in the appendix. these choices until an entirely new pool of hyperparameters is sampled and evaluated from scratch. In contrast to this, BOIL is more ﬂexible than Hyperband in that it can freely explore-exploit the whole joint space. The GP surrogate hereby allows BOIL to generalize across hyperparameters and propagate information through the joint space. 5 Conclusion and Future work Our framework complements the existing BO toolbox for hyperparameter tuning with iterative learn- ing. We present a way of leveraging our understanding that later stages of the training process are informed by progress made in earlier ones. This results in a more iteration-efﬁcient hyperparame- ter tuning algorithm that is applicable to a broad range of machine learning systems. We evaluate its performance on a set of diverse benchmarks. The results demonstrate that our model surpasses the performance of well-established alternatives while consuming signiﬁcantly fewer resources. Fi- nally, we note that our approach is not necessarily speciﬁc to machine learning algorithms, but more generally applies to any process exhibiting an iterative structure to be exploited. 86 Broader Impact Our work aims at making the optimization of processes operating in a step-wise fashion more efﬁ- cient. As demonstrated this makes BOIL particularly well-suited to supporting supervised learning models and RL systems. By increasing training efﬁcience of these models, we hope to contribute to their widespread deployment whilst reducing the computational and therefore environmental cost their implementation has. Deep (reinforcement) learning systems ﬁnd application in a wide range of settings that directly contribute to real world decisions, e.g., natural language processing, visual task, autonomous driving and many more. As machine learning models building on our contributions are being deployed in the real world, we encourage practicioners to put in place necessary supervision and override mechanisms as precautions against potential failure. In a more general context, our algorithm may be seen as a step towards the construction of an automated pipeline for the training and deployment of machine learning models. A potential danger is that humans become further and further removed from the modelling process, making it harder to spot (potentially critical) failures. We do not see this as an argument against the construction of such a pipeline in principle, but instead encourage practicioners to reﬂect on potential biases indirectly encoded in the choice of data sets and models, they are feeding into said automated processes. The growing opacity of machine learning models is a concern of its own and which automated training procedures will only contribute to. Opposing this is a rapidly growing corpus of work addressing the interpretability of trained machine learning models and their decision making. These can and should be used to rigorously analyse ﬁnal training outcomes. Only then can we ensure that machine learning algorithm do indeed become a beneﬁcial source of information guiding real world policy making as opposed to opaque, unquestioned entities. While our main interest lies in the hyperparameter optimization of machine learning models, it should be noted that any iterative process depending on a set of parameters can make use of our con- tributions. Possible settings could, for instance, include the optimization of manufacturing pipelines in which factory setting are adjusted to increase productivity. 7 Acknowledgements S. Schulze is supported by an I-CASE studentship funded by the EPSRC and Dyson. References [1] Christopher M Bishop. Pattern recognition and machine learning. springer New York, 2006. [2] Eric Brochu, Vlad M Cora, and Nando De Freitas. A tutorial on Bayesian optimization of ex- pensive cost functions, with application to active user modeling and hierarchical reinforcement learning. arXiv preprint arXiv:1012.2599, 2010. [3] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016. [4] Yutian Chen, Aja Huang, Ziyu Wang, Ioannis Antonoglou, Julian Schrittwieser, David Silver, and Nando de Freitas. Bayesian optimization in AlphaGo. arXiv preprint arXiv:1812.06855, 2018. [5] Zhongxiang Dai, Haibin Yu, Bryan Kian Hsiang Low, and Patrick Jaillet. Bayesian optimiza- tion meets Bayesian optimal stopping. In International Conference on Machine Learning , pages 1496–1506, 2019. [6] Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford, John Schulman, Szymon Sidor, Yuhuai Wu, and Peter Zhokhov. Openai baselines. GitHub, GitHub repository, 2017. [7] Tobias Domhan, Jost Tobias Springenberg, and Frank Hutter. Speeding up automatic hy- perparameter optimization of deep neural networks by extrapolation of learning curves. In Twenty-Fourth International Joint Conference on Artiﬁcial Intelligence, 2015. 9[8] Stefan Falkner, Aaron Klein, and Frank Hutter. Bohb: Robust and efﬁcient hyperparameter optimization at scale. In International Conference on Machine Learning , pages 1436–1445, 2018. [9] Peter I Frazier. A tutorial on Bayesian optimization. arXiv preprint arXiv:1807.02811, 2018. [10] Yarin Gal, Riashat Islam, and Zoubin Ghahramani. Deep Bayesian active learning with image data. In Proceedings of the 34th International Conference on Machine Learning, pages 1183– 1192, 2017. [11] Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. Deep reinforcement learning that matters. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence, 2018. [12] Philipp Hennig and Christian J Schuler. Entropy search for information-efﬁcient global opti- mization. Journal of Machine Learning Research, 13:1809–1837, 2012. [13] José Miguel Hernández-Lobato, Matthew W Hoffman, and Zoubin Ghahramani. Predictive entropy search for efﬁcient global optimization of black-box functions. In Advances in Neural Information Processing Systems, pages 918–926, 2014. [14] Donald R Jones, Matthias Schonlau, and William J Welch. Efﬁcient global optimization of expensive black-box functions. Journal of Global optimization, 13(4):455–492, 1998. [15] M. I. Jordan and T. M. Mitchell. Machine learning: Trends, perspectives, and prospects. Science, 349(6245):255–260, 2015. [16] Kirthevasan Kandasamy, Gautam Dasarathy, Jeff Schneider, and Barnabás Póczos. Multi- ﬁdelity Bayesian optimisation with continuous approximations. In Proceedings of the 34th International Conference on Machine Learning, pages 1799–1808, 2017. [17] Aaron Klein, Stefan Falkner, Simon Bartels, Philipp Hennig, and Frank Hutter. Fast Bayesian optimization of machine learning hyperparameters on large datasets. In Artiﬁcial Intelligence and Statistics, pages 528–536, 2017. [18] Aaron Klein, Stefan Falkner, Jost Tobias Springenberg, and Frank Hutter. Learning curve pre- diction with Bayesian neural networks. International Conference on Learning Representations (ICLR), 2017. [19] Andreas Krause and Cheng S Ong. Contextual Gaussian process bandit optimization. In Advances in Neural Information Processing Systems, pages 2447–2455, 2011. [20] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolutional neural networks. In Advances in Neural Information Processing Systems, pages 1097–1105, 2012. [21] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. [22] Benjamin Letham, Brian Karrer, Guilherme Ottoni, Eytan Bakshy, et al. Constrained Bayesian optimization with noisy experiments. Bayesian Analysis, 14(2):495–519, 2019. [23] Lisha Li and Kevin Jamieson. Hyperband: A novel bandit-based approach to hyperparameter optimization. Journal of Machine Learning Research, 18:1–52, 2018. [24] Mark McLeod, Stephen Roberts, and Michael A Osborne. Optimization, fast and slow: Op- timally switching between local and Bayesian optimization. In International Conference on Machine Learning, pages 3440–3449, 2018. [25] V olodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforce- ment learning. In International conference on machine learning, pages 1928–1937, 2016. [26] V olodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. NIPS Deep Learning Workshop, 2013. [27] Vu Nguyen, Sunil Gupta, Santu Rana, Cheng Li, and Svetha Venkatesh. Regret for expected improvement over the best-observed value and stopping condition. In Proceedings of The 9th Asian Conference on Machine Learning (ACML), pages 279–294, 2017. [28] Vu Nguyen and Michael A Osborne. Knowing the what but not the where in Bayesian opti- mization. In International Conference on Machine Learning, pages 7317–7326, 2020. 10[29] Michael Osborne, Roman Garnett, Zoubin Ghahramani, David K Duvenaud, Stephen J Roberts, and Carl E Rasmussen. Active learning of model evidence using Bayesian quadrature. In Advances in Neural Information Processing Systems, pages 46–54, 2012. [30] Jack Parker-Holder, Vu Nguyen, and Stephen Roberts. Provably efﬁcient online hyperparame- ter optimization with population-based bandits. In Advances in Neural Information Processing Systems, 2020. [31] Carl Edward Rasmussen. Gaussian processes for machine learning. 2006. [32] Binxin Ru, Mark McLeod, Diego Granziol, and Michael A Osborne. Fast information-theoretic Bayesian optimisation. In International Conference on Machine Learning, pages 4381–4389, 2018. [33] Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. International Conference on Learning Representations, 2016. [34] Rajat Sen, Kirthevasan Kandasamy, and Sanjay Shakkottai. Multi-ﬁdelity black-box opti- mization with hierarchical partitions. In International conference on machine learning, pages 4538–4547, 2018. [35] Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P Adams, and Nando de Freitas. Taking the human out of the loop: A review of Bayesian optimization. Proceedings of the IEEE , 104(1):148–175, 2016. [36] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanc- tot, et al. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484, 2016. [37] Leslie N Smith. A disciplined approach to neural network hyper-parameters: Part 1–learning rate, batch size, momentum, and weight decay. arXiv preprint arXiv:1803.09820, 2018. [38] Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical Bayesian optimization of ma- chine learning algorithms. In Advances in Neural Information Processing Systems , pages 2951–2959, 2012. [39] Jialin Song, Yuxin Chen, and Yisong Yue. A general framework for multi-ﬁdelity Bayesian optimization with Gaussian processes. In The 22nd International Conference on Artiﬁcial Intelligence and Statistics, pages 3158–3167, 2019. [40] Niranjan Srinivas, Andreas Krause, Sham Kakade, and Matthias Seeger. Gaussian process optimization in the bandit setting: No regret and experimental design. In Proceedings of the 27th International Conference on Machine Learning, pages 1015–1022, 2010. [41] Kevin Swersky, Jasper Snoek, and Ryan P Adams. Multi-task Bayesian optimization. In Advances in Neural Information Processing Systems, pages 2004–2012, 2013. [42] Kevin Swersky, Jasper Snoek, and Ryan Prescott Adams. Freeze-thaw Bayesian optimization. arXiv preprint arXiv:1406.3896, 2014. [43] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026–5033. IEEE, 2012. [44] Zi Wang and Stefanie Jegelka. Max-value entropy search for efﬁcient Bayesian optimization. In International Conference on Machine Learning, pages 3627–3635, 2017. [45] Ziyu Wang and Nando de Freitas. Theoretical analysis of Bayesian optimisation with unknown Gaussian process hyper-parameters. arXiv preprint arXiv:1406.7758, 2014. [46] Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and Nando Freitas. Dueling network architectures for deep reinforcement learning. In International Conference on Machine Learning, pages 1995–2003, 2016. [47] Jian Wu and Peter Frazier. The parallel knowledge gradient method for batch Bayesian opti- mization. In Advances In Neural Information Processing Systems, pages 3126–3134, 2016. [48] Jian Wu, Saul Toscano-Palmerin, Peter I Frazier, and Andrew Gordon Wilson. Practical multi- ﬁdelity Bayesian optimization for hyperparameter tuning. In 35th Conference on Uncertainty in Artiﬁcial Intelligence, 2019. 11The following sections are intended to give the reader further insights into our design choices and a deeper understanding of the algorithms properties. First, we give a brief overview of Bayesian optimization with Gaussian processes. We then illustrate our models behavior on a two dimensional problem. Last, we give further details of our experiments for reproducibility purposes. A Bayesian Optimization Preliminaries Bayesian optimization is a sequential approach to global optimization of black-box functions with- out making use of derivatives. It uses two components: a learned surrogate model of the objective function and an acquisition function derived from the surrogate for selecting new points to inform the surrogate with. In-depth discussions beyond our brief overview can be found in recent surveys [2, 9, 35]. Notation. We summarize all of the notations used in our model in Table 1 for ease of reading. A.1 Gaussian processes We present the GP surrogate model for the black-box function f [31]. A GP deﬁnes a probability distribution over functions f under the assumption that any subset of points {(xi, f (xi)}is normally distributed. Formally, this is denoted as: f (x) ∼GP ( m(x),k ( x,x′)) , where m(x) and k (x,x′) are the mean and covariance functions, given by m(x) =E[ f (x)] and k(x,x′) =E [ ( f (x)−m(x))( f (x′)−m(x′))T ] . Typically, the mean of the GP is assumed to be zero everywhere. The kernel k(x,x′) can be thought of as a similarity measure relating f (x) and f (x′). Numerous kernels encoding different prior be- liefs about f (x) have been proposed. A popular choice is given by the square exponential kernel k(x,x′) =σ2 f exp [ −(x −x′)2/2σ2 l ] . The length- and output-scales σ2 l and σ2 f regulate the maximal covariance between two points and can be estimated using maximum marginal likelihood. The SE kernel encodes the belief that nearby points are highly correlated as it is maximized at k(x,x) =σ2 f and decays the further x and x′are separated. For predicting f∗= f (x∗) at a new data point x∗, assuming a zero mean m(x) =0, we have: [ f f∗ ] ∼N ( 0, [ K kT ∗ k∗ k∗∗ ]) (7) where k∗∗= k (x∗,x∗), k∗= [k (x∗,xi)]∀i≤N and K = [k (xi,xj)]∀i, j≤N . The conditional probability of p( f∗|f ) follows a univariate Gaussian distribution as p( f∗|f ) ∼N ( µ (x∗),σ2 (x∗) ) . Its mean and variance are given by: µ (x∗) =k∗K−1y σ2 (x∗) =k∗∗−k∗K−1kT ∗. As GPs give full uncertainty information with any prediction, they provide a ﬂexible nonparametric prior for Bayesian optimization. We refer the interested readers to [31] for further details on GPs. A.2 Acquisition function Bayesian optimization is typically applied in settings in which the objective function is expensive to evaluate. To minimize interactions with that objective, an acquisition function is deﬁned to reason about the selection of the next evaluation point xt+1 = argmaxx∈X αt (x). The acquisition func- tion is constructed from the predictive mean and variance of the surrogate to be easy to evaluate and represents the trade-off between exploration (of points with high predictive uncertainty) and exploitation (of points with high predictive mean). Thus, by design the acquisition function can be maximized with standard global optimization toolboxes. Among the many acquisition functions [12, 13, 14, 32, 40, 44] available in the literature, the expected improvement [14, 27, 45] is one of the most popular. 12Table 1: Notation List Parameter Domain Meaning d integer, N dimension, no. of hyperparameters to be optimized x vector,Rd input hyperparameter N integer, N maximum number of BO iterations Tmin, Tmax integer, N the min/max no of iterations for training a ML algorithm t ∈[Tmin,...Tmax] index of training steps M integer, N the maximum number of augmentation. We set M = 15. δ scalar, R threshold for rejecting augmentation when ln of cond(K) > δ m ∈{1,...M} index of augmenting variables n ∈{1,..., N} index of BO iterations z = [x,t] vector, Rd+1 concatenation of the parameter x and iteration t cn,m scalar, R training cost (sec) yn scalar, R transformed score at the BO iteration n yn,m scalar, R transformed score at the BO iteration n, training step m α(x,t) function acquisition function for performance µc(x,t) function estimation of the cost by LR given x and t r(. |x,t) function a raw learning curve, r(x,t) = [r(1 |x,t),...r(t′|x,t),r(t |x,t)] f (x,t) function a black-box function which is compressed from the above f () l (. |m0,g0) function Logistic curve l(u |m0,g0) = 1 1+exp(−g0[u−m0]) g0, g∗ 0 scalar, R a growth parameter deﬁning a slope, g∗ 0 = argmaxg0 L m0, m∗ 0 scalar, R a middle point parameter, m∗ 0 = argmaxm0 L L scalar, R Gaussian process log marginal likelihood A.3 GP kernels and treatment of GP hyperparameters We present the GP kernels and treatment of GP hyperparameters for the black-box function f . Although the raw learning curve in DRL is noisy, the transformed version using our proposed curve compression is smooth. Therefore, we use two squared exponential kernels for input hyperparameter and training iteration, respectively. That iskx(x,x′) =exp ( −||x−x′||2 2σ2x ) and kt (t,t′) =exp ( −||t−t′||2 2σ2t ) where the observation x and t are normalized to [0,1]d and the outcome y is standardized y ∼ N (0,1) for robustness. As a result, our product kernel becomes k ( [x,t],[x′,t′] ) = k(x,x′)×k(t,t′) =exp ( −||x −x′||2 2σ2x −||t −t′||2 2σ2t ) . The length-scales σx and σt are learnable parameters indicating the variability of the function with regards to the hyperparameter input x and number of training iterations t. Estimating appropriate values for them is critical as this represents the GPs prior regarding the sensitivity of performance w.r.t. changes in the number of training iterations and hyperparameters. For extremely large σt we expect the objective function to change very little for different numbers of training iterations. For small σt by contrast we expect drastic changes even for small differences. We estimate these GP hyperparameters (including the length-scalesσx, σt and the output noise varianceσy) by maximizing their log marginal likelihood [31]. We optimize Eq. (5) with a gradient-based optimizer, providing the analytical gradient to the algo- rithm. We start the optimization from the previous hyperparameter values θprev. If the optimization fails due to numerical issues, we keep the previous value of the hyperparameters. We reﬁt the hy- perparameters every 3×d function evaluations where d is the dimension. B Algorithm Illustration and Further Experiments Fig. 7 and Fig. 8 illustrate the behavior of our proposed algorithm BOIL on the example of opti- mizing the discount factor γ of Dueling DQN [46] on the CartPole problem. The two settings differ in the inclusion augmented observations into BOIL in Fig. 7 and CM-T/F-BO (or BOIL without augmented observations) in Fig. 8. 130.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 1.2 0.8 0.4 0.0 0.4 0.8 1.2 1.6 2.0 2.4 0.00 0.15 0.30 0.45 0.60 0.75 0.90 1.05 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 0.0 0.4 0.8 1.2 1.6 2.0 2.4 2.8 3.2 3.6 0.3 0.6 0.9 1.2 1.5 1.8 2.1 2.4 2.7 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0 2.5 0.00 0.06 0.12 0.18 0.24 0.30 0.36 0.42 0.48 0.64 0.72 0.80 0.88 0.96 1.04 1.12 1.20 1.28 1.36 0.0 0.4 0.8 1.2 1.6 2.0 2.4 2.8 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 1.6 1.2 0.8 0.4 0.0 0.4 0.8 1.2 1.6 2.0 0.00 0.15 0.30 0.45 0.60 0.75 0.90 1.05 0.64 0.72 0.80 0.88 0.96 1.04 1.12 1.20 1.28 1.36 0.0 0.4 0.8 1.2 1.6 2.0 2.4 2.8 3.2 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 2.4 1.6 0.8 0.0 0.8 1.6 2.4 3.2 0.000 0.025 0.050 0.075 0.100 0.125 0.150 0.175 0.200 0.64 0.72 0.80 0.88 0.96 1.04 1.12 1.20 1.28 1.36 0.0 0.6 1.2 1.8 2.4 3.0 3.6 4.2 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 2.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0 0.00 0.15 0.30 0.45 0.60 0.75 0.90 1.05 0.64 0.72 0.80 0.88 0.96 1.04 1.12 1.20 1.28 0.0 0.4 0.8 1.2 1.6 2.0 2.4 2.8 3.2 3.6 0.0 0.4 0.8 1.2 1.6 2.0 2.4 2.8 3.2 3.6 Figure 7: Illustration of BOIL on a 2-dimensional optimization task of DDQN on CartPole. The augmented observations ﬁll the joint hyperparameter-iteration space quickly to inform our surrogate. Our decision balances utility α against cost τ for iteration-efﬁciency. Especially in situations of multiple locations sharing the same utility value, our algorithm prefers to select the cheapest option. Table 2: Dueling DQN algorithm on CartPole problem. Variables Min Max Best Found x∗ γ discount factor 0 .8 1 0 .95586 learning rate model 1 e−6 0.01 0 .00589 #Episodes 300 800 - 140.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 2.0 1.6 1.2 0.8 0.4 0.0 0.4 0.8 0.00 0.15 0.30 0.45 0.60 0.75 0.90 1.05 0.60 0.75 0.90 1.05 1.20 1.35 1.50 1.65 1.80 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 0.0 0.3 0.6 0.9 1.2 1.5 1.8 2.1 2.4 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 1.6 1.2 0.8 0.4 0.0 0.4 0.8 1.2 0.00 0.15 0.30 0.45 0.60 0.75 0.90 1.05 0.60 0.75 0.90 1.05 1.20 1.35 1.50 1.65 1.80 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 1.6 1.2 0.8 0.4 0.0 0.4 0.8 1.2 1.6 0.00 0.08 0.16 0.24 0.32 0.40 0.48 0.56 0.64 0.60 0.75 0.90 1.05 1.20 1.35 1.50 1.65 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 0.30 0.45 0.60 0.75 0.90 1.05 1.20 1.35 1.50 1.65 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 1.6 1.2 0.8 0.4 0.0 0.4 0.8 1.2 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 2.0 1.6 1.2 0.8 0.4 0.0 0.4 0.8 1.2 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 Figure 8: Illustration of the Continuous Multi task/ﬁdelity BO (CM-T/F-BO) -- this is the case of BOIL without using augmented observations (same setting as Fig. 7). This version leads to less efﬁcient optimization as the additional iteration dimension requires more evaluation than optimizing the hyperparameters on their own. 15Table 3: A2C algorithm on Reacher (left) and InvertedPendulum (right). Variables Min Max Best Found x∗ γ discount factor 0 .8 1 0 .8 learning rate actor 1 e−6 0.01 0 .00071 learning rate critic 1 e−6 0.01 0 .00042 #Episodes 200 500 - Min Max Best Found x∗ 0.8 1 0 .95586 1e−6 0.01 0 .00589 1e−6 0.01 0 .00037 700 1500 - Table 4: Convolutional Neural Network. Variables Min Max Best Found x∗ ﬁlter size 1 8 5 pool size 1 5 5 batch size 16 1000 8 learning rate 1 e−6 0.01 0 .000484 momentum 0 .8 0 .999 0 .82852 decay 0 .9 0 .999 0 .9746 number of epoch 30 150 - In both cases, we plot the GP predictive mean in Eq. (1), GP predictive variance in Eq. (2), the acquisition function in Eq. (3), the predicted function and the ﬁnal decision function in Eq. (8). These equations are deﬁned in the main manuscript. As shown in the respective ﬁgures the ﬁnal decision function balances between utility and cost of any pair (γ,t) to achieve iteration efﬁciency. Especially in situations where multiple locations share the same utility value, our decision will prefer to select the cheapest option. Using the augmented observations in Fig. 7, our joint space is ﬁlled quicker with points and the uncertainty (GP variance) across it reduces faster than in Fig. 8 – the case of vanilla CM-T/F-BO without augmenting obser- vations. A second advantage of having augmented observations is that the algorithm is discouraged to select the same hyperparameter setting at lower ﬁdelity than a previous evaluation. We do not add the full curve as it can be redundant while causing the conditioning problem of the GP covariance matrix. B.1 Experiment settings We summarize the hyperparameter search ranges for A2C on Reacher and InvertedPendulum in Table 3, CNN on SHVN in Table 4 and DDQN on CartPole in Table 2. Additionally, we present the best found parameter x∗for these problems. Further details of the DRL agents are listed in Table 5. B.2 Learning Logistic Function We ﬁrst present the Logistic curve l(u |x,t) = 1 1+exp(−g0[u−m0]) using different choices of g0 and m0 in Fig. 10. We then learn from the data to get the optimal choices g∗ 0 and m∗ 0 presented in Fig. 11. Table 5: Further speciﬁcation for DRL agents Hyperparameter Value A2C Critic-network architecture [32,32] Actor-network architecture [32,32] Entropy coefﬁcient 0 .01 Dueling DQN Q-network architecture [50,50] ε-greedy (start, ﬁnal, number of steps) (1.0,0.05,10000) Buffer size 10000 Batch size 64 PER-α [33] 1 .0 PER-β (start, ﬁnal, number of steps) (1.0,0.6,1000) 160 100 200 300 400 500 Episodes 70 60 50 40 30 20 Average Reward Preference Curve as Sigmoid Best Found Reward Curve Sigmoid Curve 0 100 200 300 400 500 Episodes 90 80 70 60 50 40 30 20 10 Reward Preference Curve as Sigmoid Best Found Reward Curve Sigmoid Curve 0 100 200 300 400 500 Episodes 60 50 40 30 20 10 Average Reward Preference Curve as Log Best Found Reward Curve Log Curve 0 100 200 300 400 500 Episodes 100 80 60 40 20 0 Reward Preference Curve as Log Best Found Reward Curve Log Curve 0 100 200 300 400 500 Episodes 60 50 40 30 20 Average Reward Preference Curve as Average Best Found Reward Curve Average Curve 0 100 200 300 400 500 Episodes 80 60 40 20 0 Reward Preference Curve as Average Best Found Reward Curve Average Curve Figure 9: To highlight the robustness, we examine the results using different preference functions such as Sigmoid curve, Log curve, and Average curve on Reacher experiments. The results include the best found reward curve with different preference choices that show the robustness of our model. Left column: the best found curve using averaged reward over 100 consecutive episodes. Right column: the best found curve using the original reward. 17/uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000019 /uni00000013/uni00000011/uni0000001a m0/uni00000020/uni00000010/uni00000016/uni00000003g0/uni00000020/uni00000013/uni00000011/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000016/uni00000003g0/uni00000020/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000016/uni00000003g0/uni00000020/uni00000015 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000016/uni00000003g0/uni00000020/uni00000016 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000019 /uni00000013/uni00000011/uni0000001a m0/uni00000020/uni00000010/uni00000015/uni00000003g0/uni00000020/uni00000013/uni00000011/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000015/uni00000003g0/uni00000020/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000015/uni00000003g0/uni00000020/uni00000015 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000015/uni00000003g0/uni00000020/uni00000016 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000019 m0/uni00000020/uni00000010/uni00000014/uni00000003g0/uni00000020/uni00000013/uni00000011/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000014/uni00000003g0/uni00000020/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000014/uni00000003g0/uni00000020/uni00000015 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000014/uni00000003g0/uni00000020/uni00000016 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000019 m0/uni00000020/uni00000013/uni00000003g0/uni00000020/uni00000013/uni00000011/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000013/uni00000003g0/uni00000020/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000013/uni00000003g0/uni00000020/uni00000015 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000013/uni00000003g0/uni00000020/uni00000016 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000019 m0/uni00000020/uni00000014/uni00000003g0/uni00000020/uni00000013/uni00000011/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000014/uni00000003g0/uni00000020/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000014/uni00000003g0/uni00000020/uni00000015 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000014/uni00000003g0/uni00000020/uni00000016 Figure 10: Examples of Logistic function l(u) = 1 1+exp(−g0[u−m0]) with different values of middle parameter m0 and growth parameter g0. B.3 Robustness over Different Preference Functions We next study the learning effects with respect to different choices of the preference functions. We pick three preference functions including the Sigmoid, Log and Average to compute the utility score for each learning curve. Then, we report the best found reward curve under such choices. The experiments are tested using A2C on Reacher-v2. The results presented in Fig. 9 demonstrate the robustness of our model with the preference functions. B.4 Applying Freeze-Thaw BO in the settings considered While both the exponential decay in Freeze-Thaw BO [42] and our compression function encode preferences regarding training development, there is an important distinction between the two ap- proaches. Freeze-thaw BO utilises the exponential decay property to terminate the training curve, while BOIL only uses the sigmoid curve to guide the search. We refer to Fig. 13 for further illustra- tion of why Freeze-thaw BO struggles in DRL settings. B.5 Ablation Study using Freeze-Thraw Kernel for Time In the joint modeling framework of hyperparameter and time (iteration), we can replace the kernel either k(x,x) or k(t,t) with different choices. We, therefore, set up a new baseline of using the time- kernel k(t,t′) in Freeze-Thaw approach [42] which encodes the monotonously exponential decay from the curve. Particularly, we use the kernel deﬁned as k(t,t′) = βα (t +t′+β)α for parameters α,β > 0 which are optimized in the GP models. 186  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l( . | m * 0 ,g * 0 ) for CartPole m * 0 =-3.266   g * 0 =3.0 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l(m * 0 ,g * 0 ) for CNN_SHVN m * 0 =2.245   g * 0 =2.092 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l(m * 0 ,g * 0 ) for InvPendulum m * 0 =1.649   g * 0 =1.833 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l( . | m * 0 ,g * 0 ) for Cifar10 m * 0 =-4.0   g * 0 =1.476 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l( . | m * 0 ,g * 0 ) for Reacher m * 0 =2.779   g * 0 =1.973 Figure 11: We learn the suitable transformation curve directly from the data. We parameterized the Logistic curve as l (m0,g0) = 1 1+exp(−g0[1−m0]) then estimate g0 and m0. The estimated function l(m∗ 0,g∗ 0) is then used to compress our curve. The above plots are the estimated l() at different environments and datasets. /uni00000013/uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013/uni0000001a/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni0000001b/uni00000013 /uni0000001c/uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000014/uni00000013 /uni00000014/uni00000015/uni00000013/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni0000002c/uni00000051/uni00000059/uni00000048/uni00000055/uni00000057/uni00000048/uni00000047/uni00000033/uni00000048/uni00000051/uni00000047/uni00000058/uni0000004f/uni00000058/uni00000050/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013 /uni00000018/uni00000013/uni00000014/uni00000013/uni00000013/uni00000014/uni00000018/uni00000013/uni00000015/uni00000013/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni0000001b/uni00000013 /uni0000001c/uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000014/uni00000013 /uni00000014/uni00000015/uni00000013/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni0000002c/uni00000051/uni00000059/uni00000048/uni00000055/uni00000057/uni00000048/uni00000047/uni00000033/uni00000048/uni00000051/uni00000047/uni00000058/uni0000004f/uni00000058/uni00000050/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013 /uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni00000014/uni0000001a/uni00000011/uni00000018 /uni00000015/uni00000013/uni00000011/uni00000013 /uni00000015/uni00000015/uni00000011/uni00000018 /uni00000015/uni00000018/uni00000011/uni00000013 /uni00000015/uni0000001a/uni00000011/uni00000018 /uni00000016/uni00000013/uni00000011/uni00000013 /uni00000016/uni00000015/uni00000011/uni00000018/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c /uni0000003e/uni00000026/uni00000031/uni00000031/uni00000010/uni00000026/uni0000004c/uni00000049/uni00000044/uni00000055/uni00000014/uni00000013/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000019/uni00000003/uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000053/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000014/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000018/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni00000014/uni0000001a/uni00000011/uni00000018 /uni00000015/uni00000013/uni00000011/uni00000013 /uni00000015/uni00000015/uni00000011/uni00000018 /uni00000015/uni00000018/uni00000011/uni00000013 /uni00000015/uni0000001a/uni00000011/uni00000018 /uni00000016/uni00000013/uni00000011/uni00000013 /uni00000016/uni00000015/uni00000011/uni00000018/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c /uni0000003e/uni00000026/uni00000031/uni00000031/uni00000010/uni00000026/uni0000004c/uni00000049/uni00000044/uni00000055/uni00000014/uni00000013/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000019/uni00000003/uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000053/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f Figure 12: Tuning hyperparameters of a DRL on InvertedPendulum and a CNN model on CIFAR10. 190 250 500 750 1000 1250 1500 Epoch 0 20 40 60 80 100 120Reward Reward Curve Freeze-thaw 0 250 500 750 1000 1250 1500 Epoch 0 50 100 150 0 250 500 750 1000 1250 1500 Epoch 0 50 100 150 0 250 500 750 1000 1250 1500 Epoch 0 50 100 150 Reward Curves Examples using A2C on Inverted Pendulum Figure 13: Illustration of Freeze-thaw BO in DRL. Freeze-thaw BO will terminate training processes when training performance (in blue) signiﬁcantly drops (i.e. at the red locations) as the exponential decay model will predict low ﬁnal performance. In most RL enviroments noisy training curves are unavoidable. Thus, Freeze-thaw BO will dismiss all curves including good setting, never completing a single training run before the ﬁnal epoch. /uni00000013 /uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni00000014/uni00000015/uni0000001b /uni00000014/uni00000015/uni0000001c /uni00000014/uni00000016/uni00000013 /uni00000014/uni00000016/uni00000014 /uni00000014/uni00000016/uni00000015 /uni00000014/uni00000016/uni00000016 /uni00000014/uni00000016/uni00000017/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni00000035/uni00000048/uni00000044/uni00000046/uni0000004b/uni00000048/uni00000055/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000025/uni00000032/uni0000002c/uni0000002f/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni00000014/uni00000015/uni0000001b /uni00000014/uni00000015/uni0000001c /uni00000014/uni00000016/uni00000013 /uni00000014/uni00000016/uni00000014 /uni00000014/uni00000016/uni00000015 /uni00000014/uni00000016/uni00000016 /uni00000014/uni00000016/uni00000017/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni00000035/uni00000048/uni00000044/uni00000046/uni0000004b/uni00000048/uni00000055/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000025/uni00000032/uni0000002c/uni0000002f/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000013 /uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni0000001c/uni00000013 /uni0000001c/uni00000018 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000018 /uni00000014/uni00000014/uni00000013 /uni00000014/uni00000014/uni00000018 /uni00000014/uni00000015/uni00000013 /uni00000014/uni00000015/uni00000018/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni0000002c/uni00000051/uni00000059/uni00000048/uni00000055/uni00000057/uni00000048/uni00000047/uni00000033/uni00000048/uni00000051/uni00000047/uni00000058/uni0000004f/uni00000058/uni00000050/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000026/uni00000010/uni00000030/uni00000012/uni00000037/uni00000010/uni00000025/uni00000032/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000025/uni00000032/uni0000002c/uni0000002f/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000013 /uni00000018/uni00000013/uni00000014/uni00000013/uni00000013/uni00000014/uni00000018/uni00000013/uni00000015/uni00000013/uni00000013/uni00000015/uni00000018/uni00000013/uni00000016/uni00000013/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni0000001c/uni00000013 /uni0000001c/uni00000018 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000018 /uni00000014/uni00000014/uni00000013 /uni00000014/uni00000014/uni00000018 /uni00000014/uni00000015/uni00000013 /uni00000014/uni00000015/uni00000018/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni0000002c/uni00000051/uni00000059/uni00000048/uni00000055/uni00000057/uni00000048/uni00000047/uni00000033/uni00000048/uni00000051/uni00000047/uni00000058/uni0000004f/uni00000058/uni00000050/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000025/uni00000032/uni0000002c/uni0000002f/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a Figure 14: Comparison using freezethaw kernel for time component. We present the result in Fig. 14 that CM-T/F-BO is still less competitive to BOIL using this speciﬁc time kernel. The results again validate the robustness our approach cross different choices of kernel. B.6 Additional Experiments for Tuning DRL and CNN We present the additional experiments for tuning a DRL model using InvertedPendulum environ- ment and a CNN model using a subset of CIFAR10 in Fig. 12. Again, we show that the proposed model clearly gain advantages against the baselines in tuning hyperparameters for model with itera- tive learning information available. B.7 Examples of Deep Reinforcement Learning Training Curves Finally, we present examples of training curves produced by the deep reinforcement learning al- gorithm A2C in Fig. 15. These ﬂuctuate widely and it may not be trivial to deﬁne good stopping criteria as done for other applications in previous work [42]. 200 200 400 80 70 60 50 40 0 200 400 110 100 90 80 70 60 50 0 200 400 110 100 90 80 70 60 50 0 200 400 70 60 50 40 30 0 200 400 70 60 50 40 30 20 10 0 200 400 50 40 30 20 0 200 400 90 80 70 60 50 0 200 400 70 60 50 40 30 20 10 0 200 400 85 80 75 70 0 200 400 60 50 40 30 20 0 200 400 100 90 80 70 60 0 200 400 100 90 80 70 0 200 400 80 60 40 20 0 200 400 100 90 80 70 60 0 200 400 100 90 80 70 0 200 400 80 70 60 50 40 30 20 0 200 400 70 60 50 40 30 20 0 200 400 70 60 50 40 30 20 0 200 400 100 90 80 70 60 50 0 200 400 70 60 50 40 30 20 0 200 400 100 90 80 70 60 50 0 200 400 100 90 80 70 60 0 200 400 60 50 40 30 20 0 200 400 80 60 40 20 0 200 400 85 80 75 70 65 60 0 200 400 100 95 90 85 80 0 200 400 80 60 40 20 0 200 400 100 90 80 70 60 50 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 Figure 15: Examples of reward curves using A2C on Reacher-v2 (rows 1 −3) and on InvertedPendulum-v2 (rows 4 −6). Y-axis is the reward averaged over 100 consecutive episodes. X-axis is the episode. The noisy performance illustrated is typical of DRL settings and complicates the design of early stopping criteria. Due to the property of DRL, it is not trivial to decide when to stop the training curve. In addition, it will be misleading if we only take average over the last 100 iterations. 21",
      "meta_data": {
        "arxiv_id": "1909.09593v5",
        "authors": [
          "Vu Nguyen",
          "Sebastian Schulze",
          "Michael A Osborne"
        ],
        "published_date": "2019-09-20T16:14:34Z",
        "pdf_url": "https://arxiv.org/pdf/1909.09593v5.pdf"
      }
    },
    {
      "title": "Hyperparameter Optimization through Neural Network Partitioning",
      "abstract": "Well-tuned hyperparameters are crucial for obtaining good generalization\nbehavior in neural networks. They can enforce appropriate inductive biases,\nregularize the model and improve performance -- especially in the presence of\nlimited data. In this work, we propose a simple and efficient way for\noptimizing hyperparameters inspired by the marginal likelihood, an optimization\nobjective that requires no validation data. Our method partitions the training\ndata and a neural network model into $K$ data shards and parameter partitions,\nrespectively. Each partition is associated with and optimized only on specific\ndata shards. Combining these partitions into subnetworks allows us to define\nthe ``out-of-training-sample\" loss of a subnetwork, i.e., the loss on data\nshards unseen by the subnetwork, as the objective for hyperparameter\noptimization. We demonstrate that we can apply this objective to optimize a\nvariety of different hyperparameters in a single training run while being\nsignificantly computationally cheaper than alternative methods aiming to\noptimize the marginal likelihood for neural networks. Lastly, we also focus on\noptimizing hyperparameters in federated learning, where retraining and\ncross-validation are particularly challenging.",
      "full_text": "Published as a conference paper at ICLR 2023 HYPERPARAMETER OPTIMIZATION THROUGH NEURAL NETWORK PARTITIONING Bruno Mlodozeniec†∗, Matthias Reisser‡, Christos Louizos‡ †University of Cambridge, ‡Qualcomm AI Research bkm28@cam.ac.uk, {mreisser,clouizos}@qti.qualcomm.com ABSTRACT Well-tuned hyperparameters are crucial for obtaining good generalization behavior in neural networks. They can enforce appropriate inductive biases, regularize the model and improve performance — especially in the presence of limited data. In this work, we propose a simple and efﬁcient way for optimizing hyperparameters inspired by the marginal likelihood, an optimization objective that requires no validation data. Our method partitions the training data and a neural network model into K data shards and parameter partitions, respectively. Each partition is associated with and optimized only on speciﬁc data shards. Combining these partitions into subnetworks allows us to deﬁne the “out-of-training-sample” loss of a subnetwork, i.e., the loss on data shards unseen by the subnetwork, as the objective for hyperparameter optimization. We demonstrate that we can apply this objective to optimize a variety of different hyperparameters in a single training run while being signiﬁcantly computationally cheaper than alternative methods aiming to optimize the marginal likelihood for neural networks. Lastly, we also focus on optimizing hyperparameters in federated learning, where retraining and cross-validation are particularly challenging. 1 I NTRODUCTION Due to their remarkable generalization capabilities, deep neural networks have become the de-facto models for a wide range of complex tasks. Combining large models, large-enough datasets, and sufﬁcient computing capabilities enable researchers to train powerful models through gradient descent. Regardless of the data regime, however, the choice of hyperparameters — such as neural architecture, data augmentation strategies, regularization, or which optimizer to choose — plays a crucial role in the ﬁnal model’s generalization capabilities. Hyperparameters allow encoding good inductive biases that effectively constrain the models’ hypothesis space (e.g., convolutions for vision tasks), speed up learning, or prevent overﬁtting in the case of limited data. Whereas gradient descent enables the tuning of model parameters, accessing hyperparameter gradients is more complicated. The traditional and general way to optimize hyperparameters operates as follows; 1) partition the dataset into training and validation data1, 2) pick a set of hyperparameters and optimize the model on the training data, 3) measure the performance of the model on the validation data and ﬁnally 4) use the validation metric as a way to score models or perform search over the space of hyperparameters. This approach inherently requires training multiple models and consequently requires spending resources on models that will be discarded. Furthermore, traditional tuning requires a validation set since optimizing the hyperparameters on the training set alone cannot identify the right inductive biases. A canonical example is data augmentations — they are not expected to improve training set performance, but they greatly help with generalization. In the low data regime, deﬁning a validation set that cannot be used for tuning model parameters is undesirable. Picking the right amount of validation data is a hyperparameter in itself. The conventional rule of thumb to use ∼10% of all data can result in signiﬁcant overﬁtting, as pointed out by Lorraine et al. (2019) , when one has a sufﬁciently large number of hyperparameters to tune. Furthermore, a validation set can be challenging ∗Work done while at Qualcomm AI Research. Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc. and/or its subsidiaries. 1a third partition, the test or holdout set is used to estimate the ﬁnal model performance 1 arXiv:2304.14766v1  [cs.LG]  28 Apr 2023Published as a conference paper at ICLR 2023 to obtain in many use cases. An example is Federated Learning (FL) (McMahan et al., 2017), which we speciﬁcally consider in our experimental section. In FL, each extra training run (for,e.g., a speciﬁc hyperparameter setting) comes with additional, non-trivial costs. Different approaches have been proposed in order to address these challenges. Some schemes optimize hyperparameters during a single training run by making the hyperparameters part of the model (e.g., learning dropout rates with concrete dropout (Gal et al., 2017), learning architectures with DARTs (Liu et al., 2018) and learning data-augmentations with schemes as in Benton et al. (2020); van der Wilk et al. (2018)). In cases where the model does not depend on the hyperparameters directly but only indirectly through their effect on the value of the ﬁnal parameters (through optimization), schemes for differentiating through the training procedures have been proposed, such as Lorraine et al. (2019). Another way of optimizing hyperparameters without a validation set is through the canonical view on model selection (and hence hyperparameter optimization) through the Bayesian lens; the concept of optimizing the marginal likelihood. For deep neural networks, however, the marginal likelihood is difﬁcult to compute. Prior works have therefore developed various approximations for its use in deep learning models and used those to optimize hyperparameters in deep learning, such as those of data augmentation (Schw¨obel et al., 2021; Immer et al., 2022). Still, however, these come at a signiﬁcant added computational expense and do not scale to larger deep learning problems. This paper presents a novel approach to hyperparameter optimization, inspired by the marginal likelihood, that only requires a single training run and no validation set. Our method is more scalable than previous works that rely on marginal likelihood and Laplace approximations (which require computing or inverting a Hessian (Immer et al., 2021)) and is broadly applicable to any hierarchical modelling setup. 2 M ARGINAL LIKELIHOOD AND PRIOR WORK In Bayesian inference, the rules of probability dictate how any unknown, such as parameters w or hyperparameters ψ, should be determined given observed data D. Let p(w) be a prior over w and p(D|w,ψ) be a likelihood for Dwith ψbeing the hyperparameters. We are then interested in the posterior given the data p(w|D,ψ) =p(D|w,ψ)p(w)/p(D|ψ). The denominator term p(D|ψ) is known as the marginal likelihood, as it measures the probability of observing the data given ψ, irrespective of the value of w: p(D|ψ) = ∫ p(w)p(D|w,ψ)dw. Marginal likelihood has many desirable properties that make it a good criterion for model selection and hyperparameter optimization. It intuitively implements the essence of Occam’s Razor principle (MacKay, 2003, § 28). In the PAC-Bayesian literature, it has been shown that higher marginal likelihood gives tighter frequentist upper bounds on the generalization performance of a given model class (McAllester, 1998; Germain et al., 2016). It also has close links to cross-validation (see section 2.1) and can be computed from the training data alone. However, computation of the marginal likelihood in deep learning models is usually prohibitively expensive and many recent works have proposed schemes to approximate the marginal likelihood for differentiable model selection (Lyle et al., 2020; Immer et al., 2021; 2022; Schw¨obel et al., 2021). 2.1 “L EARNING SPEED ” PERSPECTIVE Lyle et al. (2020); Fong and Holmes (2020) pointed out the correspondence between “learning speed” and marginal likelihood. Namely, the marginal likelihood of the data Dconditioned on some hyperparameters ψcan be written as: log p(D|ψ) = ∑ k log Ep(w|D1:k−1,ψ) [p(Dk|w,ψ)] ≥ ∑ k Ep(w|D1:k−1,ψ) [log p(Dk|w,ψ)] (1) where (D1,..., DC) is an arbitrary partitioning of the training dataset Dinto Cshards or chunks2, and p(w|D1:k,ψ) is the posterior over parameters of a function fw : X → Y, from the input domain Xto the target domain Yafter seeing data in shards 1 through k. The right-hand side can be interpreted as a type of cross-validation in which we ﬁx an ordering over the shards and measure the “validation” performance on each shardDk using a model trained on the preceding shards D1:k−1. 2We use the terms “chunk” and “shard” interchangeably. 2Published as a conference paper at ICLR 2023 Alternatively, it can be viewed as the learning speed of a (probabilistic) model: i.e., a measure of how quickly it learns to perform well on new shards of data after only having been ﬁt to the previous shards (through exact Bayesian updating). This perspective neatly illustrates why models with higher marginal likelihood can exhibit good inductive biases, e.g., encoded through ψ, w and fw. Namely, such models can be expected to learn faster and generalize better after seeing fewer samples. For example, if the hypothesis space is constrained3to functions satisfying symmetries present in the data, we need fewer data to identify the correct function (Sokolic et al., 2017; Sannai et al., 2021). We argue that the “learning speed” aspect of marginal likelihood — i.e., measuring how well the model generalizes to new data in the training set, having been trained only on the previous data points — is the key property making marginal likelihood a useful tool for selecting hyperparameters. 2.2 T RAINING SPEED FOR HYPERPARAMETER OPTIMIZATION Computing the “learning speed”, requires samples from the posteriorp(w|D1:k,ψ). Unfortunately, in deep learning settings, such samples are impractical to obtain; thus, prior works have focused on more scalable alternatives. Lyle et al. (2020) propose to approximate the objective in Eq. 1 by looking at the training speed during standard training of a neural network by SGD. Speciﬁcally, they deﬁne the training speed as the reduction in the training loss after a single SGD parameter update, summed over all updates in the ﬁrst epoch. They argue that, during the ﬁrst epoch of training, after the neural network parameters, w, have been updated with SGD steps using data from shards D1:k, they can be approximately used in place of the sample from the posterior p(w|D1:k,ψ) in Eq. 1. They extend the analogy to training past one epoch and use the training speed estimate for model selection (Ru et al., 2021). As pointed out by the authors, however, the analogy between learning speed and training speed somewhat breaks down after 1 epoch of training. The network parameters have “seen” every datapoint in the training set after1 epoch, and hence the connection to measuring the model’s generalization capability is weakened. For the sake of scalability and alignment with deep learning practice, we also focus on simple pointwise approximations qk(w) = δ(w = ˆwk) to the posteriors p(w|D1:k,ψ). However, in contrast to prior work, we explicitly parametrize the learning procedure such that, at any given training iteration, we have access to a model that is trained only on a subset of the dataD1:k. In doing so, we can approximate the objective in Eq. 1, and thus use it to optimize the hyperparameters during the entire training run. 3 P ARTITIONED NEURAL NETWORKS Our goal is to optimize the objective LML (D,ψ) = C∑ k=1 Eqk−1(w) [log p(Dk|w,ψ)] (2) wrt. ψ, which is an approximation to the lower-bound presented in Eq. 1 above. In Appendix A, we show that the left-hand side is also a lower-bound on the marginal likelihood under some unobtrusive conditions. As mentioned in Section 2.2, our goal is to propose an architecture and a training scheme so that we can easily obtain models trained on only subsets of the data D1:k for all k throughout training. We propose that each {qk(w)}C k=1 optimizes a subset of the parameters of the neural network, in a manner that allows us to extract “subnetworks” from the main network that have been trained on speciﬁc chunks of data. We describe the partitioning scheme below. Partitioning the parameters Denote the concatenations of the weights of a neural networkw ∈RN. We can deﬁne a partitioning ((w1,..., wC),P) of the parameters into C partitions, such that w = Pconcat(w1,..., wC) for a permutation matrix P ∈{0,1}N×N. For ease of exposition, we drop the dependence on P, assuming that w is already arranged such that P is identity, P = IN×N. Given the partitioning (w1,..., wC) of the parameters, we then specify Csubnetworks with weights w(1) s ,..., w(C) s such that w(k) s = concat(w1,..., wk, ˆwk+1,..., ˆwC), where ˆwi are some default 3or if the learning algorithm is heavily biased towards returning hypotheses that satisfy a given invariance, e.g., through the use of a prior. 3Published as a conference paper at ICLR 2023 values not optimized during training4. More speciﬁcally, the k-th subnetwork, wk s, retains the ﬁrst kpartitions from the weight partitioning and sets the remaining parameters to ˆwk+1:C. Note that, if each wk is only updated on chunks D1:k, the subnetwork w(k) s is only comprised of weights that have been updated on D1:k. Thus, we can view the parameters of w(k) s as an approximation to qk(w). Although, given that a subset of the parameters in each w(k) s is ﬁxed, this would likely be a poor approximation to the true posterior over the weights given D1:k, it could be, intuitively, a reasonable approximation in function space5. Partitioned training Having partitioned the dataset Dinto Cchunks (D1,..., Dk), we update each partition wk by optimising the negative log-likelihood6on chunks D1:k using subnetwork w(k) s by computing the following gradients: ∇wkL ( D1:k,w(k) s ) = ∑ (x,y)∈D1:k ∇wk log p ( y ⏐⏐⏐x; w(k) s ,ψ ) . (3) We interleave stochastic gradient updates of each partition of the weights with updating the hyperpa- rameters ψusing LML in Eq. 2: ∇ψLML (D,ψ) ≈ C∑ k=2 ∑ (x,y)∈Dk ∇ψlog p ( y ⏐⏐⏐x,w(k−1) s ,ψ ) . (4) This can be seen as the sum of the out-of-sample losses for each subnetwork w(k) s . The scheme is illustrated in Figure 1. For details of how the updates are scheduled in our experiments, see Appendix I. Note that, while we could incorporate the gradient of the ﬁrst term from Eq. 1 corresponding to Eq0(w)[log p(D1|w,ψ)] in Eq. 4, we chose to leave it out. Hence, the gradient of Eq. 4 is of an estimate that can be viewed as an approximation to the conditional marginal likelihood log p(D2:C|D1,ψ). Conditional marginal likelihood has been shown to have many desirable properties for model selection and, in many cases, can be a better proxy for generalization (Lotﬁ et al., 2022). Weights: w = (w1,w2,w3) Alternate: Optimize parameters: log p ( D1 |(w1, ˆw2, ˆw3)   Subnet. 1 ,ψ ) w.r.t. w1 log p ( D1:2|(w1,w2, ˆw3)   Subnet. 2 ,ψ ) w.r.t. w2 log p ( D1:3|(w1,w2,w3)   Subnet. 3 ,ψ ) w.r.t. w3 Optimize hyper parameters ψon: log p ( D2|(w1, ˆw2, ˆw3)   Subnet. 1 ,ψ ) + logp ( D3|(w1,w2, ˆw3)   Subnet. 2 ,ψ ) Figure 1: Best viewed in colour. Illustration of the partitioning scheme for a single hidden layer perceptron with C = 3chunks. This procedure, inspired by the marginal likelihood, has several desirable properties compared to prior work. 1) Our objective is computationally efﬁcient, with a computational cost roughly corresponding to evaluating subnetworks on the training set. There is no need to compute nor invert a Hessian with 4e.g., ˆwi could be the value of the weights at initialization, or ˆwi = 0 corresponding to pruning those parameters and obtaining a proper subnetwork. 5Since a) the mapping from parameters to functions is not bijective and b) neural networks are highly overparameterised and can be heavily pruned while retaining performance (Frankle and Carbin, 2018), obtaining a good ﬁt to a subset of the training data with a subset of the model parameters should be possible. Furthermore, “scaling laws” indicate that the beneﬁt of having more parameters becomes apparent mostly for larger dataset sizes (Kaplan et al., 2020), thus it is reasonable for subnetworks ﬁt to more data to have more learnable parameters. 6Optionally with an added negative log-prior regularization term log p(w(k) s ). 4Published as a conference paper at ICLR 2023 respect to the weights, as in the Laplace approximation (Immer et al., 2021; 2022). 2) Our objective is readily amenable to optimization by stochastic gradient descent; we do not have to iterate over the entire training set to compute a single gradient update for the hyperparameters. 3) Compared to the training speed objective (Lyle et al., 2020), in our method, the training of the weights in each subnetwork progresses independently of the data in future chunks. Hence, it can be seen as more truthfully measuring the generalization capability of a model using a given set of hyperparameters. Partitioning Schemes There are several ways in which the neural network weights can be partitioned. In our experiments in Section 5, we partition the weights before beginning training by assigning a ﬁxed proportion of weights in each layer to a given partition at random. For each subnetwork, for the weight partitions corresponding to future chunks, we use the values of the weights at initialisation. For a discussion of partitioning schemes, see Appendix C. 4 R ELATED WORKS Hyperparameter optimization in deep learning Many works have tackled the challenge of op- timizing hyperparameters in deep learning. Works on implicit differentiation, such as the one by Lorraine et al. (2019), allow for optimizing training hyperparameters such as the learning rate, weight- decay, or other hyperparameters that affect the ﬁnal neural network weights only through the training routine. Other works have proposed ways to parameterize and optimize data-augmentations (Cubuk et al., 2018; Li et al., 2020), search-spaces for neural network architectures, as well as methods to optimize architectures using gradient-based optimization (Liu et al., 2018; Elsken et al., 2019). All of the above works have primarily relied on optimizing hyperparameters on a separate validation set and are compatible with the objective deﬁned in this work. Several works have also aimed to cast learning data augmentations as an invariance learning problem. They do so by parameterizing the model itself with data augmentations, and frame invariance learning as a model selection problem (van der Wilk et al., 2018; Benton et al., 2020; Schw¨obel et al., 2021; Nabarro et al., 2022; Immer et al., 2022). We compare against Benton et al. (2020) (“Augerino”) and Immer et al. (2022) (“Differentiable Laplace”) on this task in the experimental section. Hyperparameter optimization without a validation set A limited number of works consider learning hyperparameters without a validation set in a deep learning context. Benton et al. (2020) propose a simple method for learning invariances without a validation set by regularising invariance hyperparameters to those resulting in higher invariance. They show that the invariances found tend to be insensitive to the regularisation strength, determined by another hyperparameter. However, the method relies on being able to a priori deﬁne which hyperparameters lead to higher invariance through a suitable regularisation function. In more complex invariance learning settings, deﬁning the regulariser can be challenging. For example, if data-augmentation transformations were to be parameterized by a neural network (as proposed in Lorraine et al. (2019)), it is non-trivial to devise an adequate regulariser. We show that our method can be applied to such settings. Other works focus on deriving tractable approximations to the marginal likelihood for deep neural networks. Schw ¨obel et al. (2021) propose only marginalising-out the parameters in the last layer of the neural network by switching it out for a Gaussian Process. They treat the preceding layer effectively as a hyperparameter, and optimize invariance parameters using the marginal likelihood. Although they show promising results on MNIST, they found they “were unable to learn invariances for CIFAR-10” (Schw¨obel et al., 2021, §7) and highlighted the need to marginalise lower layers as well. In contrast, our objective can be seen as being inspired by marginal likelihood where arbitrary network layers can be “marginalised”, and works on datasets like CIFAR-10. Immer et al. (2022) have adapted the Laplace approximation (Immer et al., 2021) to make it tractable for learning data augmentations. In contrast to Schw¨obel et al. (2021), they approximately marginalize out all the network parameters, and performs favourably. Their approximation, however, requires approximations to a Hessian w.r.t. all network parameters; for that reason, their work reports results for architectures only up to a ResNet-14, whereas our method can easily scale to larger architectures. Hyperparameter optimization in FL Improving hyperparameter optimization is especially rele- vant to FL. Given the potential system level constraints (Wang et al., 2021), methods that optimize the hyperparameters and parameters in a single training run are preferred. On this note, Khodak et al. (2021) introduced FedEx and showed that it can successfully optimize the client optimizer 5Published as a conference paper at ICLR 2023 hyperparameters. FedEx relies on a training/validation split on the client level and uses a REIN- FORCE type of gradient (Williams, 1992) estimator, which usually exhibits high variance and needs baselines to reduce it (Mohamed et al., 2020). This is in contrast to partitioned networks, which use standard, low-variance backpropagation for the hyperparameters and no separate validation set per client. To optimize the other hyperparameters, Khodak et al. (2021) wrapped FedEx with a traditional hyperparameter optimization strategy, the successive halving algorithm. This is orthogonal to our method and could be applied to partitioned networks as well. In Zhou et al. (2021), the authors perform a hyperparameter search independently on each client with some off-the-shelf methods and then aggregate the results of the search at the server once in order to identify the best hyperparameter setting. The main drawback of this method compared to partitioned networks is that when the local client datasets are small, a client-speciﬁc validation set is not informative, and the aggregation happens only once. Finally, there is also the recent work from Seng et al. (2022) which performs hyperparameter optimization and neural architecture search in the federated setting. Similarly to prior works, it requires client-speciﬁc validation data in order to optimize the hyperparameters. 5 E XPERIMENTS 1 5 10 15 20 25 30 Num. inputs 0.7 0.8 0.9 1.0 Accuracy 0.5 0.4 0.3 0.2 0.1 0.0 Average Log-likelihoodPosthoc Diagonal Laplace Train T est 1 5 10 15 20 25 30 Num. inputs 103 102 Log Marginal Likelihood Estimate Partitioned (a) 0 400080001200016000200002400028000 Iteration 0 5 10 15 20 25Input Mask Element 0.00 0.25 0.50 0.75 1.00 Mask Probability  (b) Figure 2: (a) Demonstrating the ability of the marginal-likelihood inspired objective LML to identify the correct model on a toy input selection task. We plot the hyperparameter objective, train and test set accuracy, and train and test set log-likelihood with the partitioned networks method (left), and the post-hoc diagonal Laplace method (Immer et al., 2021) (right). (b) Mask over input features learned by partitioned networks over time. The ﬁrst 15 features are correctly identiﬁed. Input Selection To demonstrate that LML is a good objective for model selection that captures the desirable properties of the marginal likelihood, we ﬁrst deploy our method on the toy model selection task of Lyle et al. (2020): there the ﬁrst 15 features are informative, and the remaining15 are spurious y∼Bern (1 2 ) x = [ y+ ϵ1,...,y + ϵ15   Informative ,ϵ16,...,ϵ 30   Spurious ]⊺ ϵ1,...,ϵ 30 iid ∼N(0,1). We specify a ﬁxed mask over the inputs prior to training, where the ﬁrst Kinputs remain unmasked, and the remainder is masked. We expect that, given multiple models with different (ﬁxed) masks over the inputs, the proposed objective will be able to identify the correct one — i.e., the one that keeps only the informative features. We train multiple fully connected neural networks (MLPs) on a training set of 1000 examples using our method and compare the ﬁnal values of the LML objective. The results are shown in Figure 2a. LML correctly identiﬁes 15 input features as the optimum, and correlates well with test accuracy and log-likelihood. Training loss and training accuracy, on the other hand, cannot alone disambiguate whether to use 15 or more input features. Differentiable input selection We further show that we can learn the correct mask over the inputs in a differentiable manner using our method during a single training run. We parameterize a learnable mask over the inputs with a concrete Bernoulli distribution (Maddison et al., 2016) and treat the parameters of the mask distribution as a hyperparameter. We optimize them with respect to the proposed objective using our method. The evolution of the learned mask during training is shown in Figure 2b, where we see that we can correctly identify the ﬁrst 15 informative features. 6Published as a conference paper at ICLR 2023 Learning invariances through data-augmentations Following previous literature on learning soft invariances through learning data augmentations (Nabarro et al., 2022; van der Wilk et al., 2018; Benton et al., 2020; Schw ¨obel et al., 2021; Immer et al., 2022), we show that we can learn useful afﬁne image augmentations, resulting in gains in test accuracy. We specify afﬁne data augmentations as part of a probabilistic model as done by van der Wilk et al. (2018), averaging over multiple data augmentation samples during training and inference. This allows us to treat the data-augmentation distribution as a model hyperparameter rather than a training hyperparameter. For datasets, we consider MNIST, CIFAR10, TinyImagenet along with rotCIFAR10 and rotTinyImagenet, variants where the datapoints are randomly rotated at the beginning of training by angles sampled uniformly from [−π,π] (Immer et al., 2022). Experimental setup details are provided in Appendix I. For the CIFAR10 and rotCIFAR10 datasets, we consider as baselines standard training with no augmentations, Augerino (Benton et al., 2020) and Differentiable Laplace (Immer et al., 2022). Following Immer et al. (2022), we use ﬁxupResNets (Zhang et al., 2019) for the architectures. The results can be seen in Table 1. There, we observe that partitioned networks outperform all baselines in the case of CIFAR10 for both ResNet variants we consider. On RotCIFAR10, we observe that partitioned networks outperform the baseline and Augerino, but it is slightly outperformed by Differentiable Laplace, which optimizes additional prior hyperparameters. To demonstrate the scalability of partitioned networks, for the (rot)TinyImagenet experiments we consider a ResNet-50 architecture with GroupNorm(2). In Table 1 we observe that in both cases, partitioned networks learn invariances successfully and improve upon the baseline. Relative to Augerino, we observe that partitioned networks either improve (TinyImagenet) or are similar (rotTinyImagenet). Table 1: Test accuracy with learning afﬁne augmentations on (rot)CIFAR10 and (rot)TinyImagenet. Method Dataset Architecture Baseline Augerino Diff. Laplace Partitioned RotCIFAR10 ﬁxupResNet-8 54.2±0.4 75.4±0.2 79.5±0.6 79.1±0.0 CIFAR10 ﬁxupResNet-8 74.1±0.5 79.0±1.0 84.2±0.8 86.1±0.4 ﬁxupResNet-14 79.5±0.3 83.0±0.1 88.1±0.2 89.1±0.8 RotTinyImagenet ResNet-50 31.5±0.6 44.5±0.2 OOM7 43.9±0.3 TinyImagenet ResNet-50 44.2±0.5 41.1±0.2 OOM 48.6±0.0 Imbuing a model with useful invariances is particularly useful in the low-data regime, due to better data efﬁciency. To show that, we perform experiments where we artiﬁcially reduce the size of the training dataset. The results can be seen in Figure 3. We see that by learning augmentations with partitioned networks, we can drastically improve performance in the low-data regime upon a baseline that does not learn augmentations, while performing favorably against prior works in most cases. On MNIST, our method outperforms the last-layer marginal-likelihood method (last-layer ML) by Schw¨obel et al. (2021) in the large data regime but underperforms in the low-data regime. That is likely to be expected, as their work ﬁts a Gaussian Process (GP) at the last layer (Wilson et al., 2016), which is better tailored for the low-data regime and results into a more ﬂexible model (due to the GP corresponding to an additional, inﬁnite width, layer). Since the MNIST-CNN is sufﬁciently small to ﬁt multiple networks into memory, we also compare to a variant of our method where, instead of partitioning a single network, we train Cdifferent networks where network kis ﬁt on data D1:k. This serves as an upper bound on the performance of the partitioned networks. We see that by partitioning a single network, we can achieve almost equivalent accuracy. On CIFAR10, partitioned networks outperform all other works on all data sizes we considered. On RotCIFAR10, partitioned networks perform again favourably, but they are marginally outperformed by differentiable Laplace in the low-data regime. Compared to partitioned networks where we only optimize augmentations, differentiable Laplace also optimizes the precision of a Gaussian prior over the weights, which better combats overﬁtting in the low-data regime. On both the TinyImagenet and rotTinyImagenet experiments we observe that partitioned networks either outperform or are similar to the baselines on all data sizes considered. 7Out of memory error on a 32GB Nvidia V100. 7Published as a conference paper at ICLR 2023 5000 20000 60000 Dataset Size 0.98 0.99T est Accuracy Baseline Last-layer ML Augerino Diff. Laplace Partitioned (Ens.) Partitioned (a) MNIST 0.25 0.50 0.75 1 5 10 20 50 Dataset Size (x1000) 0.25 0.50 0.75  (b) (rot)CIFAR10 0.25 0.50 10 50 100 Dataset Size (x1000) 0.2 0.4  (c) (rot)TinyImagenet Figure 3: Learning afﬁne data augmentations on subsets of data. (b) uses a ﬁxupResNet-8 architecture whereas (c) a ResNet-50 architecture. (b,c) Top: normal dataset, bottom: rotated dataset. Comparisons to traditional training / validation split We further perform comparisons between partitioned networks and the more traditional training/validation split (denoted as validation set optimization) with additional ﬁnetuning to the task of learning data augmentations. This is realized as follows; we partition 20kCIFAR10 examples into training and validation data of speciﬁc proportions. We then either train a partitioned network (along with the hyperparameters on LML) on these two chunks of data or train a standard network on the training set while using the validation set loss to obtain gradients for the data augmentation hyperparameters. For the validation set optimization baseline, once the hyperparameters are optimized, the resulting network is ﬁnetuned on the whole dataset for 20 epochs. The results for varying chunk proportions are provided in Table 2. Table 2: Learning afﬁne augmentations with ﬁxupResNet-14 on subset of CIFAR-10 (20kexamples). NaN denotes that a run crashed. Chunk Proportions Method [0.3,0.7] [0 .5,0.5] [0 .7,0.3] [0 .8,0.2] [0 .9,0.1] Partitioned 82.9%±0.3 83.0%±0.01 83.7%±0.2 84.0%±0.6 84.6%±0.05 Validation set optim. NaN 78.9%±0.04 81.5%±0.2 82.6%±0.1 83.4%±0.1 +Finetune NaN 81.3%±0.09 82.5%±0.2 83.5%±0.1 83.8%±0.3 Table 3: Learning a feature extractor (ﬁrst 2 out of 3 stages of a Wide ResNet-20) as a hyperparameter on CIFAR10. Method Chunk Proportions Test accuracy Validation set optim. [0.9,0.1] 59 .6%±0.6 Partitioned [0.1,0.8,0.1] 87.3%±0.8 We can see that partitioned net- works (that do not employ ad- ditional ﬁnetuning) outperform validation set optimization with ﬁnetuning in all settings we tried. The gap does get smaller when we move to the more tra- ditional 90/10 splits for train- ing/validation: a 10% proportion for validation data is enough to optimize a handful of hyper- parameters (just 6 scalars). To corroborate this claim, we set up an additional experiment; we use a Wide ResNet-20 on the full CIFAR10 dataset, where the ﬁrst two out of the three stages (13 convolution layers) are considered as hyperparameters. The results for this setting can be seen in Table 3. We see that 10% validation data are not enough, and the validation set optimization baseline performs poorly. This is in contrast to partitioned networks, where with three chunks, we can learn all of these hyperparameters successfully. Note that, compared to Augerino, applying partitioned networks to this setting is straightforward. To apply Augerino, one would have to come up with a metric that can be used to regularize the feature extractor towards “higher invariance”. Partitioned networks for federated learning We consider federated learning (FL) (McMahan et al., 2017), a setting where data is distributed across many clients. In this setting, there are system properties that make hyperparameter optimization especially challenging (Wang et al., 2021). More speciﬁcally, obtaining a validation set and performing multiple training runs with different 8Published as a conference paper at ICLR 2023 hyperparameter settings might not be possible due to the additional communication and computation costs, and transient client availability (clients join and leave the training process at any time). Optimizing hyperparameters together with the model parameters in a single run is therefore especially beneﬁcial (Wang et al., 2021), and partitioned networks are a good ﬁt for FL. We extend our centralized experimental setup to FL by splitting all N clients into Cnon-overlapping chunks, such that each chunk is understood as the union of all clients’ data shards that belong to that chunk. During federated training, a client belonging to chunk ksequentially optimizes partitions wk:C through sub-networks w(k:C) s and computes a gradient wrt. the hyperparameters ψ. Note that partitions w1:k remain unchanged and do not need to be communicated back to the server. This reduction in upload costs is a welcome property for FL, where upload costs can bottleneck system design. The server receives the (hyper-) parameter updates, averages them, and applies the result as a “gradient” to the server-side model in the traditional federated manner (Reddi et al., 2020). For partitioned networks, the hyperparameters that we optimize are the data augmentation parameters and, since we also include dropout in these architectures, the dropout rates (with the concrete relaxation from Maddison et al. (2016)). As a baseline, we consider the standard federated training without learning hyperparameters (denoted as FedAvg) as well as learning the augmentation parameters with Augerino Benton et al. (2020). Please see Appendix J for a detailed explanation of our FL setup. Table 4 summarizes our results using different sub-sets and variations of MNIST and CIFAR10, where we also included rotMNIST Larochelle et al. (2007) as another dataset. We can see that partitioned networks allow training models that generalize better than both FedAvg and FedAvg with Augerino, at reduced communication costs. Especially when the true data-generating process and underlying source of non-i.i.d.-ness are explicitly accounted for — here in the form of rotation — the beneﬁts of learning the augmentations with partitioned networks become apparent. For example, we observe that on the rotated datasets, partitioned networks learn to correctly increase the rotation angle. Table 4: Validation accuracy averaged over the last10 evaluations, each 10 rounds apart; standard- error is computed across 4 random seeds. All datasets are adapted to the federated setting and are synthetically split to be non-i.i.d. sampled as described in Appendix J.2. Dataset & size ↑MNIST ↑RotMNIST ↓Upload Method 1.25k 5k 50k 1.25k 5k 50k [%] FedAvg 95.4%±0.1 97.4%±0.1 99.0%±0.1 80.5%±0.0 90.4%±0.5 96.8%±0.1 100 FedAvg + Augerino 94.2%±0.5 96.4%±0.1 99.1%±0.0 79.5%±0.3 89.0%±2.0 95.3%±0.2 100 FedAvg + Partitioned97.0%±0.1 98.3%±0.0 99.2%±0.1 85.7%±0.9 93.5%±0.6 97.8%±0.1 77 ↑CIFAR10 ↑RotCIFAR10 ↓Upload 1.25k 5k 45k 1.25k 5k 45k [%] FedAvg 50.2%±0.4 64.5%±0.3 79.2%±0.7 35.6%±0.3 45.2%±0.1 53.9%±1.1 100 FedAvg + Augerino 49.9%±0.8 65.0%±0.2 79.9%±0.4 36.1%±0.2 45.0%±0.2 56.4%±0.7 100 FedAvg + Partitioned50.8%±1.0 64.8%±0.4 81.5%±0.5 37.1%±0.2 45.3%±0.3 60.6%±0.2 91 6 D ISCUSSION We propose partitioned networks as a new method for hyperparameter optimization inspired by the marginal likelihood objective. It provides a general and scalable solution to ﬁnding hyperparameters in a single training run without requiring access to a validation set while introducing less additional overhead to the training task than existing approaches. We showed that partitioned networks are applicable on a wide range of tasks; they can identify the correct model on illustrative toy examples, they can learn data augmentations in a way that improves data efﬁciency, they can optimize general feature extractors as hyperparameters and they can also optimize dropout rates. In the federated setting, partitioned networks allow us to overcome practical challenges, reduce the communication overhead and obtain better models. The notion of partitioned networks we propose in this work is novel to the literature and an orthogonal approach to many existing hyperparameter tuning algorithms. Like any other method, partitioned networks come with their own limitations, e.g., needing a partitioning strategy. We expand upon them in appendix H. We hope to see our method successfully reducing the need to perform hyperparameter search through repeated training and thereby contribute to the community’s effort to reduce its carbon footprint. 9Published as a conference paper at ICLR 2023 REFERENCES Gregory Benton, Marc Finzi, and Andrew G Wilson. Augerino, github, com- mit=fd542eb90ac6b1c0959156c1f6ad2ba8719d8572. https://github.com/g-benton/ learning-invariances/. (on page 18) Gregory Benton, Marc Finzi, Pavel Izmailov, and Andrew G Wilson. Learning invariances in neural networks from training data. Advances in neural information processing systems, 33:17605–17616, 2020. (on page 2, 5, 7, 9, 16, 18, 20, 24, 25) Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment: Learning augmentation policies from data. arXiv preprint arXiv:1805.09501, 2018. (on page 5) Kamal Dys. Cifar10 resnet: 90+% accuracy;less than 5 min. https://www.kaggle.com/code/ kmldas/cifar10-resnet-90-accuracy-less-than-5-min . Accessed: 2022-09- 17. (on page 26) Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural architecture search: A survey. The Journal of Machine Learning Research, 20(1):1997–2017, 2019. (on page 5) Edwin Fong and Chris C Holmes. On the marginal likelihood and cross-validation. Biometrika, 107 (2):489–496, 2020. (on page 2) Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. arXiv preprint arXiv:1803.03635, 2018. (on page 4) Yarin Gal, Jiri Hron, and Alex Kendall. Concrete dropout.Advances in neural information processing systems, 30, 2017. (on page 2) Pascal Germain, Francis Bach, Alexandre Lacoste, and Simon Lacoste-Julien. Pac-bayesian theory meets bayesian inference. Advances in Neural Information Processing Systems, 29, 2016. (on page 2) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectiﬁers: Surpassing human-level performance on imagenet classiﬁcation. In Proceedings of the IEEE international conference on computer vision, pages 1026–1034, 2015. (on page 23) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In European conference on computer vision , pages 630–645. Springer, 2016. (on page 23) Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016. (on page 23) Alexander Immer and Tycho F. A. van der Ouderaa. Learning invariances with laplace ap- proximations (lila), github, commit=c0c4a09a109ed2f55e887def7d854b8a3a2330ef. https: //github.com/tychovdo/lila. (on page 17) Alexander Immer, Matthias Bauer, Vincent Fortuin, Gunnar R¨atsch, and Khan Mohammad Emtiyaz. Scalable marginal likelihood estimation for model selection in deep learning. In International Conference on Machine Learning, pages 4563–4573. PMLR, 2021. (on page 2, 5, 6, 24) Alexander Immer, Tycho F. A. van der Ouderaa, Gunnar R¨atsch, Vincent Fortuin, and Mark van der Wilk. Invariance learning in deep neural networks with differentiable laplace approximations, 2022. URL https://arxiv.org/abs/2202.10638. (on page 2, 5, 7, 15, 16, 17, 18, 22, 23, 24, 25) Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning, pages 448–456. PMLR, 2015. (on page 23) Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. (on page 4) 10Published as a conference paper at ICLR 2023 Mikhail Khodak, Renbo Tu, Tian Li, Liam Li, Maria-Florina F Balcan, Virginia Smith, and Ameet Talwalkar. Federated hyperparameter tuning: Challenges, baselines, and connections to weight- sharing. Advances in Neural Information Processing Systems, 34:19184–19197, 2021. (on page 5, 6) Hugo Larochelle, Dumitru Erhan, Aaron Courville, James Bergstra, and Yoshua Bengio. An empirical evaluation of deep architectures on problems with many factors of variation. In Proceedings of the 24th international conference on Machine learning, pages 473–480, 2007. (on page 9) Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge. CS 231N, 7(7):3, 2015. (on page 24) Yonggang Li, Guosheng Hu, Yongtao Wang, Timothy Hospedales, Neil M Robertson, and Yongxin Yang. Dada: Differentiable automatic data augmentation. arXiv preprint arXiv:2003.03780, 2020. (on page 5) Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. arXiv preprint arXiv:1806.09055, 2018. (on page 2, 5) Jonathan Lorraine, Paul Vicol, and David Duvenaud. Optimizing millions of hyperparameters by implicit differentiation. CoRR, abs/1911.02590, 2019. URL http://arxiv.org/abs/ 1911.02590. (on page 1, 2, 5) Sanae Lotﬁ, Pavel Izmailov, Gregory Benton, Micah Goldblum, and Andrew Gordon Wilson. Bayesian model selection, the marginal likelihood, and generalization. In Kamalika Chaud- huri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine Learning , volume 162 of Pro- ceedings of Machine Learning Research , pages 14223–14247. PMLR, 17–23 Jul 2022. URL https://proceedings.mlr.press/v162/lotfi22a.html. (on page 4) Clare Lyle, Lisa Schut, Robin Ru, Yarin Gal, and Mark van der Wilk. A bayesian perspective on training speed and model selection. Advances in Neural Information Processing Systems , 33: 10396–10408, 2020. (on page 2, 3, 5, 6) David JC MacKay. Information theory, inference and learning algorithms. Cambridge university press, 2003. (on page 2) Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous relaxation of discrete random variables. arXiv preprint arXiv:1611.00712, 2016. (on page 6, 9, 26, 27) David A McAllester. Some pac-bayesian theorems. In Proceedings of the eleventh annual conference on Computational learning theory, pages 230–234, 1998. (on page 2) Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efﬁcient learning of deep networks from decentralized data. In Artiﬁcial intelli- gence and statistics, pages 1273–1282. PMLR, 2017. (on page 2, 8) Shakir Mohamed, Mihaela Rosca, Michael Figurnov, and Andriy Mnih. Monte carlo gradient estimation in machine learning. J. Mach. Learn. Res., 21(132):1–62, 2020. (on page 6) Seth Nabarro, Stoil Ganev, Adri`a Garriga-Alonso, Vincent Fortuin, Mark van der Wilk, and Laurence Aitchison. Data augmentation in bayesian neural networks and the cold posterior effect. In Uncertainty in Artiﬁcial Intelligence, pages 1434–1444. PMLR, 2022. (on page 5, 7) Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and R. Gar- nett, editors, Advances in Neural Information Processing Systems 32, pages 8024–8035. Curran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/9015-pytorch-an- imperative-style-high-performance-deep-learning-library .pdf. (on page 22) 11Published as a conference paper at ICLR 2023 Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Kone ˇcn`y, Sanjiv Kumar, and H Brendan McMahan. Adaptive federated optimization. arXiv preprint arXiv:2003.00295, 2020. (on page 9, 26, 27) Robin Ru, Clare Lyle, Lisa Schut, Miroslav Fil, Mark van der Wilk, and Yarin Gal. Speedy performance estimation for neural architecture search. Advances in Neural Information Processing Systems, 34:4079–4092, 2021. (on page 3) Akiyoshi Sannai, Masaaki Imaizumi, and Makoto Kawano. Improved generalization bounds of group invariant/equivariant deep networks via quotient feature spaces. In Uncertainty in Artiﬁcial Intelligence, pages 771–780. PMLR, 2021. (on page 3) Pola Schw¨obel, Martin Jørgensen, Sebastian W. Ober, and Mark van der Wilk. Last layer marginal likelihood for invariance learning, 2021. URL https://arxiv.org/abs/2106.07512. (on page 2, 5, 7, 15, 16, 23, 24, 26, 27) Jonas Seng, Pooja Prasad, Devendra Singh Dhami, and Kristian Kersting. Hanf: Hyperparameter and neural architecture search in federated learning. arXiv preprint arXiv:2206.12342, 2022. (on page 6) Jure Sokolic, Raja Giryes, Guillermo Sapiro, and Miguel Rodrigues. Generalization error of invariant classiﬁers. In Artiﬁcial Intelligence and Statistics, pages 1094–1103. PMLR, 2017. (on page 3) Mark van der Wilk, Matthias Bauer, ST John, and James Hensman. Learning invariances using the marginal likelihood. Advances in Neural Information Processing Systems, 31, 2018. (on page 2, 5, 7, 16) Bastiaan S Veeling, Jasper Linmans, Jim Winkens, Taco Cohen, and Max Welling. Rotation equiv- ariant cnns for digital pathology. In International Conference on Medical image computing and computer-assisted intervention, pages 210–218. Springer, 2018. (on page 15) Jianyu Wang, Zachary Charles, Zheng Xu, Gauri Joshi, H Brendan McMahan, Maruan Al-Shedivat, Galen Andrew, Salman Avestimehr, Katharine Daly, Deepesh Data, et al. A ﬁeld guide to federated optimization. arXiv preprint arXiv:2107.06917, 2021. (on page 5, 8, 9) Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3):229–256, 1992. (on page 6) Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, and Eric P Xing. Deep kernel learning. In Artiﬁcial intelligence and statistics, pages 370–378. PMLR, 2016. (on page 7) Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European conference on computer vision (ECCV), pages 3–19, 2018. (on page 23, 26) Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016. (on page 23) Hongyi Zhang, Yann N Dauphin, and Tengyu Ma. Fixup initialization: Residual learning without normalization. arXiv preprint arXiv:1901.09321, 2019. (on page 7, 18, 23) Yi Zhou, Parikshit Ram, Theodoros Salonidis, Nathalie Baracaldo, Horst Samulowitz, and Heiko Ludwig. Flora: Single-shot hyper-parameter optimization for federated learning. arXiv preprint arXiv:2112.08524, 2021. (on page 6) 12Published as a conference paper at ICLR 2023 A LML IS A LOWER -BOUND TO THE MARGINAL LIKELIHOOD In this section, we show that the objective in equation 2 is a lower-bound on the marginal likelihood, under a mild assumption on each approximate posterior qk(w). The aim is to approximate: log p(D|ψ) = C∑ k=1 log p(Dk|D1:k−1,ψ) (5) Our partitioned approximation is given by: C∑ k=1 Eqk−1(w) [log p(Dk|w,ψ)] (6) We can get the equation for the gap between quantities in 5 and 6: gap = C∑ k=1 log p(Dk|D1:k−1,ψ) − C∑ k=1 Eqk−1(w) [log p(Dk|w,ψ)] (7) = C∑ k=1 Eqk−1(w) [log p(Dk|D1:k−1,ψ) −log p(Dk|w,ψ)] (8) = C∑ k=1 Eqk−1(w) [ log p(Dk|D1:k−1,ψ) p(Dk|w,ψ) ] (9) = C∑ k=1 Eqk−1(w)  log p(w,Dk|D1:k−1)    p(w|D1:k,ψ)p(Dk|D1:k−1,ψ)p(w|D1:k−1,ψ) p(w|D1:k,ψ)p(Dk|w,ψ)p(w|D1:k−1,ψ)   p(w,Dk|D1:k−1)   (10) = C∑ k=1 Eqk−1(w) [ log p(w|D1:k−1,ψ) p(w|D1:k,ψ) ] (11) = C∑ k=1 DKL [qk−1(w)∥p(w|D1:k,ψ)] −DKL [qk−1(w)∥p(w|D1:k−1,ψ)] (12) We now make two assumptions • DKL [qk−1(w)∥p(w|D1:k,ψ)] ≥DKL [qk(w)∥p(w|D1:k,ψ)]. This is motivated from the fact that qk(w) is trained on all data chunks D1:k so it is expected to be a better approxima- tion to the posterior p(w|D1:k), compared to qk−1(w) which is only trained on D1:k−1. • DKL [qC−1(w)∥p(w|D1:C,ψ)] ≥DKL [q0(w)∥p(w)]. Since we are free to choose the approximate posterior before seeing any data — q0(w)—, we can set it to be equal to the prior p(w) which, together with the positivity of the KL divergence, trivially satisﬁes this assumption. Therefore, by rearranging Eq. 12 and using our two assumptions we have that the gap is positive gap =−DKL [q0(w)∥p(w)] +DKL [qC−1(w)∥p(w|D1:C,ψ)] + C∑ k=1 DKL [qk−1(w)∥p(w|D1:k,ψ)] −DKL [qk(w)∥p(w|D1:k,ψ)] ≥0, (13) and our approximation is a lower bound to the marginal likelihood, i.e., log p(D|ψ) ≥ C∑ k=1 Eqk−1(w) [log p(Dk|w,ψ)] . (14) 13Published as a conference paper at ICLR 2023 B P ARTITIONED NETWORKS AS A SPECIFIC APPROXIMATION TO THE MARGINAL LIKELIHOOD In this section of the appendix, we show that the partitioned neural networks we presented in the paper are a particular instance of the approximation to the marginal likelihood shown in equation 2. Consider a dataset Dcomprised of C shards, i.e. D= (D1,..., DC), along with a model, e.g., a neural network, with parameters w ∈RDw, a prior p(w) = ∏Dw j=1 N(wj|0,λ) and a likelihood p(D|w,ψ) with hyperparameters ψ. Assuming a sequence over the dataset chunks, we can write out the true marginal likelihood as log p(D|ψ) = ∑ k log p(Dk|D1:k−1,ψ) = ∑ k log Ep(w|D1:k−1,ψ) [p(Dk|w,ψ)] (15) ≥ ∑ k Ep(w|D1:k−1,ψ) [log p(Dk|w,ψ)] . (16) Since the true posteriors p(w|D1:j,ψ) for j ∈{1,...,C }are intractable, we can use variational inference to approximate them with qφj(w) for j ∈{1,...,C }, with φj being the to-be-optimized parameters of the j’th variational approximation. Based on the result from Appendix A, whenqφj(w) are optimized to match the respective posteriors p(w|D1:j,ψ), we can use them to approximate the marginal likelihood as log p(D|ψ) ≥ ∑ k Eqφk−1 (w) [log p(Dk|w,ψ)] . (17) Partitioned networks correspond to a speciﬁc choice for the sequence of approximating distribution families qφk(w). Speciﬁcally, we partition the parameter space w into Cchunks, i.e., wk ∈RDwk, such that ∑ kDwk = Dw, and we associate each parameter chunk wk with a data shard Dk. Let rφk(wk) be base variational approximations over wk with parameters φk. Each approximate distribution qφk(w) is then deﬁned in terms of these base approximations, i.e., qφk(w) =   k−1∏ j=1 rφj(wj)  rφk(wk) ( K∏ m=k+1 r0(wm) ) (18) where r0(·) is some base distribution with no free parameters. In accordance with the assumptions in appendix A, we can then ﬁt each qφk(w) by minimising the KL-divergence to p(w|D1:k,ψ) – the posterior after seeing kchunks: DKL [qφk(w)∥p(w|D1:k,ψ)] =−Eqφk(w)[log p(D1:k|w,ψ)] +DKL [qφk(w)∥p(w)] + logp(D1:k|ψ) (19) (20) Finding the optimum with respect to φk: arg min φk DKL [qφk(w)∥p(w|D1:k,ψ)] = (21) = arg min φk −Eqφk(w)[log p(D1:k|w,ψ)] +DKL [qφk(w)∥p(w)] (22) = arg min φk −Eqφk(w)[log p(D1:k|w,ψ)] + DKL     k−1∏ j=1 rφj(wj)  rφk(wk) ( K∏ m=k+1 r0(wm) ) ∥ K∏ i p(wi)   (23) = arg min φk −Eqφk(w)[log p(D1:k|w,ψ)] +DKL [rφk(wk)∥p(wk)] . (24) We can now obtain partitioned networks by assuming that rφk(wk) = N(wk|φk,νI) for k ∈ {1,...,C }, r0(w) = N(w|ˆw,νI), with ˆw being the parameters at initialization (i.e., before we 14Published as a conference paper at ICLR 2023 update them on data) and taking ν →0, i.e., in machine-precision, the weights are deterministic. As noted in Section I.1, we scale the weight-decay regularizer forφk (whenever used) differently for each partition k, such that it can be interpreted as regularization towards a prior. In the experiments where we do not regularize φk according to p(wk) when we optimize them, this implicitly corresponds to λ→∞ (i.e. the limiting behaviour when the variance of p(w) goes to inﬁnity), which makes the contribution of the regularizer negligible. C P ARTITIONING SCHEMES There are several ways in which we could aim to partition the weights of a neural network. Throughout the experimental section 5, we partition the weights by assigning a ﬁxed proportion of weights in each layer to a given partition at random. We call this approach random weight partitioning. We also experimented with other partitioning schemes. For example, we tried assigning a ﬁxed proportion of a layer’s outputs (e.g., channels in a convolution layer) to each partition. All weights in a given layer that a speciﬁc output depends on would then be assigned to that partition. We call this approach node partitioning. Both approaches are illustrated in Figure 4. One beneﬁt of the node partitioning scheme is that it makes it possible to update multiple partitions with a single batch; This is because we can make a forward pass at each linear or convolutional layer with the full network parameters w, and, instead, mask the appropriate inputs and outputs to the layer to retrieve an equivalent computation to that with w(k) s . The gradients also need to be masked on the backward pass adequately. No such simpliﬁcation is possible with the random weight partitioning scheme; if we were to compute a backward pass for a single batch of examples using different subnetworks for each example, the memory overhead would grow linearly with the number of subnetworks used. In initial experiments, we found both random weight partitioning and node partitioning performed similarly. In the experimental section 5, we focused on the former, as it’s easier to reason about with relation to e.g., dropout. Throughout this work, partitioning happens prior to initiating training, and remains ﬁxed throughout. It might also be possible to partition the network parameters dynamically during training, which we leave for future work.   w11 w12 w13 w14 w15 w21 w22 w23 w24 w25 w31 w32 w33 w34 w35 w41 w42 w43 w44 w45 w51 w52 w53 w54 w55 w61 w62 w63 w64 w65   (a) Random weight partitioned In node assignment    Out node assignment      w11 w12 w13 w14 w15 w21 w22 w23 w24 w25 w31 w32 w33 w34 w35 w41 w42 w43 w44 w45 w51 w52 w53 w54 w55 w61 w62 w63 w64 w65   (b) Node partitioned Figure 4: Figures showing how the weights within a single weight matrix W ∈R6×5 for a linear layer would be partitioned. D S CALABILITY In the paper, we claim that our method is scalable compared to Schw¨obel et al. (2021) and Immer et al. (2022). What constraints the scalability of the mentioned prior works, however, is different. For the Last Layer Marginal Likelihood, although the approach works on small datasets such as PCAM (Veeling et al., 2018) and MNIST, the authors report that they were unable to learn invariances 15Published as a conference paper at ICLR 2023 on larger datasets such as CIFAR10. In (Schw¨obel et al., 2021, section 7), they explore the issue of scalability in more detail, and showcase that last layer marginal likelihood is insufﬁcient. Differentiable Laplace performs well, even on more complex datasets, such as CIFAR10. Their scalability, however, is limited by the computational and memory complexity of their method, which we go into in more detail in the section below. D.1 C OMPLEXITY ANALYSIS First, we consider the scalability of our algorithm in terms of computational and memory complexity. In particular, we show that our method scales much more favourably compared to Differentiable Laplace (Immer et al., 2022). We present our analysis for a feed-forward model of depth L, with layer widths D8. In order to directly compare to Immer et al. (2022) and Benton et al. (2020), we consider the complexities in the invariance learning setup (Benton et al., 2020; van der Wilk et al., 2018) withSaugmentation samples. In other experiments, hyperparameter optimization setups, S can be taken to be 1. The notation is summarized in Table 5. N Number of datapoints in dataset D NB Batch size S Number of augmentation samples9 C Output size (number of classes) D Feedforward network layer widths L Feedforward network depth P Number of parameters (s.t. O(P) =O(LD2 + DC)) Table 5: Notation for complexity analysis. We consider the computational and memory costs of 1) obtaining a gradient with respect to the parameters 2) obtaining a gradient with respect to the hyperparameters, and 3) computing the value of the model/hyperparameter selection objective for each method. All analysis assumes computation on a Monte-Carlo estimate of the objective on a single batch of data. In Tables 6 and 7, we assume that C <D, and hence, for the clarity of comparison, sometimes fold a factor depending Cinto a factor depending on Dif it’s clearly smaller. This hiding of the factors was only done for Differentiable Laplace, which is the worst scaling method. D.1.1 C OMPUTATIONAL COMPLEXITY Param. Backward Hyperparam. Backward Hyperparam. Objective Partitioned O(NBPS) O(NBPS) O(NBPS) Augerino O(NBPS) O(NBPS) O(NBPS) Diff. Laplace O(NBPS) O(NBPS+NCP +NCDLS + LD3) O(NPS + NCP +NCDLS + LD3) Table 6: Computational Complexities. The two terms highlighted for Augerino can be computed in a single backward pass. For Differentiable Laplace, the terms in blue can be amortised over multiple hyperparameter backward passes. That is why, in their method, they propose updating the hyperparameters once every epoch on (possibly) multiple batches of data, rather than once on every batch as is done with Partitioned Networks and Augerino. 8This is for the ease of comparison. Same upper bound complexities will hold for a network of variable sizes Dℓ for ℓ∈[L], where D= maxℓ Dℓ 9Only relevant for invariance learning. 16Published as a conference paper at ICLR 2023 D.1.2 M EMORY COMPLEXITY The memory complexities for Partitioned Networks, Augerino, and Differentiable Laplace are shown in Table 7. Crucially, the memory required to update the hyperparameters for Differentiable Laplace scales as O(NBSLD2 + P), with a term depending on the square of the network widths. This can become prohibitively expensive for larger models, and is likely the reason why their paper only considers experiments on architectures with widths up to a maximum of 256. Param. Backward Hyperparam. Backward Hyperparam. Objective Partitioned O(NBSLD+ P) O(NBSLD+ P) O(NBSD+ P) Augerino O(NBSLD+ P) O(NBSLD+ P) O(NBSD+ P) Diff. Laplace O(NBSLD+ P) O(NBSLD2 + P) O(NBSLD2 + P) Table 7: Memory Complexities. Differences are highlighted in red. D.2 P RACTICAL SCALABILITY A complexity analysis in big- Onotation as provided by us in the previous sections allows to understand scalability in the limit, but constant terms that manifest in practice are still of interest. In this section we aim present real timing measurements for our method in comparison to Augerino and Differential Laplace, and elaborate on what overhead might be expected with respect to standard neural network training. The empirical timings measurements on an NVIDIA RTX 3080-10GB GPU are shown in Table 8. We used a batch-size of 250, 200 for the MNIST and CIFAR10 experiments respectively, and 20 augmentation samples, just like in our main experiments in Table 1 and Figure 3. As can be seen, the overhead from using a partitioned network is fairly negligible compared to a standard forward and backward pass. The one difference compared to Augerino is, however, the fact that a separate forward-backward pass needs to be made to update the hyperparameters and regular parameters. This necessity is something that can be side-stepped with alternative partitioning schemes, as preliminarily mentioned in appendix C, and is an interesting direction for future research. MNIST CIFAR10 Method CNN ﬁxupResNet-8 ﬁxupResNet-14 Augerino ×1 ×1 ×1 Diff. Laplace† Param. ×1 ×1 ×1 Hyperparam. ×2015.6 ×18.2 - Partitioned Param. ×1.08 ×1.17 ×1.21 Hyperparam. ×1.08 ×1.08 ×1.09 Table 8: Relative empirical time increase with respect to a regular parameter update during standard training. †The timing multipliers with respect to the baseline for ﬁxupResNet-8 are taken from the timings reported in (Immer et al., 2022, Appendix D.4). On the ResNet-14, we get an out-of- memory error during the hyperparam. update step with Differentiable Laplace on the NVIDIA RTX 3080-10GB GPU when running with the ofﬁcial codebase (Immer and van der Ouderaa). Memory Overhead Our proposed method’s memory consumption scales in the same way as Augerino or vanilla neural network training. There is a minor constant memory overhead due to having to store the assignment of weights to partitions. In general, only log Cbits per parameter are necessary to store the partition assignments, whereCis the number of chunks. In our implementation, we only consider C <28, and hence store the assignments in byte tensors. This means that the partitioned models require extra 25% memory for storing the parameters (when using 32bit ﬂoats to represent the parameters). 17Published as a conference paper at ICLR 2023 If the “default” weight values (i.e. those denoted ˆwi in Figure 1) are non-zero, there is an additional overhead to storing those as well, which doubles the memory required to store the parameters. We observed there was no difference in performance when setting default weight values to 0 in architectures in which normalisation layers are used (i.e. most modern architectures). As such, we would in general recommend to set the default weight values to 0. However, we found setting default values to the initialised values to be necessary for stability of training deep normalisation-free architectures such as the ﬁxup architectures (Zhang et al., 2019) we used to compare with Differentiable Laplace. As their method is not compatible with BatchNorm, we used these architectures in our experiments, and hence used non-zero default values. Lastly, if the default weight values are set to the (random) initialisation values, it is possible to write a cleverer implementation in which only the random seeds are stored in memory, and the default values are re-generated every time they are need in a forward and a backward pass. This would make the memory overhead from storing the default values negligible. E N OTE ON AUGERINO In replicating Augerino (Benton et al., 2020) within our code-base and experimenting with the implementation, we discovered a pathological behaviour that is partly mirrored by the authors of Immer et al. (2022). In particular, note that the loss function (Benton et al., 2020, Equation (5)) proposed by the authors is problematic in the sense that for any regularization strength λ> 0, the optimal loss value is negative inﬁnity since the regularization term (negative L2-norm) is unbounded. In our experiments we observe that for a sufﬁciently-large value of λand after a sufﬁcient number of iterations, this behaviour indeed appears and training diverges. In practice, using Augerino therefore necessitates either careful tuning of λ, clipping the regularisation term (a method that introduces yet another hyperparameter), or other techniques such as early stopping. In the open-source repository for the submission (Benton et al.), it can be seen that on many experiments the authors use a ”safe” variant of the objective, in which they clip the regulariser (without pass-through of the gradient) once the l∞-norm of any of the hyperparameters becomes larger than an arbitrary threshold. Without using this adjustment, we found that the Augerino experiments on MNIST crashed every time with hyperparameters diverging to inﬁnity. F S ENSITIVITY TO PARTITIONING F.1 S ENSITIVITY IN TERMS OF FINAL PERFORMANCE (a) 2 chunks [8,1] [4,1] [1,1] [1,4] [1,8] Param. Proportions [8,1] [4,1] [1,1] [1,4] [1,8]Chunk Proportions 0.990 0.992 0.994 0.996 T est Accuracy  (b) 3 chunks [8,4,1][3,2,1][1,1,1][1,2,3][1,4,8] Param. Proportions [8,4,1] [3,2,1] [1,1,1] [1,2,3] [1,4,8]Chunk Proportions 0.990 0.992 0.994 0.996 T est Accuracy  (c) 4 chunks [8,4,2,1][4,3,2,1][1,1,1,1][1,2,3,4][1,2,4,8] Param. Proportions [8,4,2,1] [4,3,2,1] [1,1,1,1] [1,2,3,4] [1,2,4,8]Chunk Proportions 0.990 0.992 0.994 0.996 T est Accuracy Figure 5: Learning afﬁne augmentations on MNIST with a CNN ﬁt on all data. x- and y−ticks denote the ratios of parameters/datapoints assigned to each partition/chunk respectively. Partitioned networks allow for learning hyperparameters in a single training run, however, they introduce an additional hyperparameter in doing so: the partitioning scheme. The practitioner needs to choose the number of chunks C, the relative proportions of data in each chunk, and the relative proportions of parameters assigned to each of the Cpartitions wk. We investigate the sensitivity to the partitioning scheme here. We show that our results are fairly robust to partitioning through a grid-search over parameter partitions and chunk proportions on the afﬁne augmentation learning task on MNIST with the CNN architecture we use throughout this work. 18Published as a conference paper at ICLR 2023 (a) 2 chunks [8,1] [4,1] [1,1] [1,4] [1,8] Param. Proportions [8,1] [4,1] [1,1] [1,4] [1,8]Chunk Proportions 0.987 0.988 0.989 0.990 T est Accuracy  (b) 3 chunks [8,4,1][3,2,1][1,1,1][1,2,3][1,4,8] Param. Proportions [8,4,1] [3,2,1] [1,1,1] [1,2,3] [1,4,8]Chunk Proportions 0.987 0.988 0.989 0.990 T est Accuracy  (c) 4 chunks [8,4,2,1][4,3,2,1][1,1,1,1][1,2,3,4][1,2,4,8] Param. Proportions [8,4,2,1] [4,3,2,1] [1,1,1,1] [1,2,3,4] [1,2,4,8]Chunk Proportions 0.987 0.988 0.989 0.990 T est Accuracy Figure 6: Learning afﬁne augmentations on RotMNIST with a CNN ﬁt on all data. x- and y−ticks denote the ratios of parameters/datapoints assigned to each partition/chunk respectively. Figure 5 and Figure 6 show the test accuracy for a choice of chunk and parameter proportions across two, three and four chunks. The proportions are to be read as un-normalized distributions; for example, chunk proportions set to [1,8] denotes that there are 8×as many datapoints assigned to the second compared to the ﬁrst. Each conﬁguration was run with 2 random seeds, and we report the mean across those runs in the ﬁgure. The same architecture used was the same as for the main MNIST experiments in section 5 (see Appendix I.4 for details). We observe that for various partition/dataset-chunking conﬁgurations, all models achieve fairly similar ﬁnal test accuracy. There is a trend for models with a lot of parameters assigned to later chunks, but with few datapoints assigned to later chunks, to perform worse. While these results show a high level of robustness against the choice of additional hyperparameters introduced by our method, these results do show an opportunity or necessity for choosing the right partitioning scheme in order to achieve optimal performance. F.2 S ENSITIVITY IN TERMS OF HYPERPARAMETERS FOUND To compare how the different partitioning schemes qualitatively impact the hyperparameters that the method identiﬁes, we also retrain vanilla models from scratch using the hyperparameter values found using partitioned networks. Namely, we take the ﬁnal value of the hyperparameters learned with partitioned networks with a given partitioning scheme, and plot the ﬁnal test set accuracy of a vanilla neural network model trained from scratch with those hyperparameters. The results are shown in Figures 7 and 8. (a) 2 chunks [8,1] [4,1] [1,1] [1,4] [1,8] Param. Proportions [8,1] [4,1] [1,1] [1,4] [1,8]Chunk Proportions 0.990 0.992 0.994 0.996 T est Accuracy  (b) 3 chunks [8,4,1][3,2,1][1,1,1][1,2,3][1,4,8] Param. Proportions [8,4,1] [3,2,1] [1,1,1] [1,2,3] [1,4,8]Chunk Proportions 0.990 0.992 0.994 0.996 T est Accuracy  (c) 4 chunks [8,4,2,1][4,3,2,1][1,1,1,1][1,2,3,4][1,2,4,8] Param. Proportions [8,4,2,1] [4,3,2,1] [1,1,1,1] [1,2,3,4] [1,2,4,8]Chunk Proportions 0.990 0.992 0.994 0.996 T est Accuracy Figure 7: Standard neural network trained onMNIST with a CNN ﬁt on all data, with hyperparameters found using partitioned networks with chunk and parameter proportions corresponding to those in Figure 5. x- and y−ticks denote the ratios of parameters/datapoints assigned to each partition/chunk respectively. G H OW GOOD ARE THE HYPERPARAMETERS FOUND ? Here we show that the hyperparameters found by partitioned networks are also a good set of hyperparameters for vanilla neural networks retrained from scratch. This section expands on the 19Published as a conference paper at ICLR 2023 (a) 2 chunks [8,1] [4,1] [1,1] [1,4] [1,8] Param. Proportions [8,1] [4,1] [1,1] [1,4] [1,8]Chunk Proportions 0.987 0.988 0.989 0.990 T est Accuracy  (b) 3 chunks [8,4,1][3,2,1][1,1,1][1,2,3][1,4,8] Param. Proportions [8,4,1] [3,2,1] [1,1,1] [1,2,3] [1,4,8]Chunk Proportions 0.987 0.988 0.989 0.990 T est Accuracy  (c) 4 chunks [8,4,2,1][4,3,2,1][1,1,1,1][1,2,3,4][1,2,4,8] Param. Proportions [8,4,2,1] [4,3,2,1] [1,1,1,1] [1,2,3,4] [1,2,4,8]Chunk Proportions 0.987 0.988 0.989 0.990 T est Accuracy Figure 8: Standard neural network trained on RotMNIST with a CNN ﬁt on all data, with hyper- parameters found using partitioned networks with chunk and parameter proportions corresponding to those in Figure 6. x- and y−ticks denote the ratios of parameters/datapoints assigned to each partition/chunk respectively. experiment in section F.2. To validate this claim, we conducted a fairly extensive hyperparameter search on the afﬁne augmentation learning task on RotMNIST; we trained 200 models by ﬁrst sampling a set of afﬁne augmentation parameters uniformly at random from a predeﬁned range 10, and then training a neural network model (that averages across augmentation samples at train and test time, as described in Benton et al. (2020)) with standard neural training with those hyperparameters ﬁxed throughout. In Figure 9, we plot the ﬁnal test-set performance of all the models trained with those hyperparameters sampled from a ﬁxed range. Alongside, we show the hyperparameters and test-set performance of the partitioned networks as they progress throughout training. The partitioned networks consistently achieve ﬁnal test-set performance as good as that of the best hyperparameter conﬁgurations iden- tiﬁed through extensive random sampling of the space. We also show the test-set performance of neural network models, trained through standard training, with hyperparameters ﬁxed to the ﬁnal hyperparameter values identiﬁed by the partitioned networks. The hyperparameters identiﬁed by partitioned networks appear to also be good for regular neural networks; the standard neural networks with hyperparameters identiﬁed through partitioned training also outperform the extensive random sampling of the hyperparameter space. Furthermore, Figure 9 shows that partitioned networks do learn full rotation invariance on the RotMNIST task, i.e. when full rotation invariance is present in the data generating distribution. 0.0 0.2 0.4 Translation X 0.96 0.97 0.98 0.99T est Accuracy 0.0 0.2 0.4 Translation Y 0 /2 Rotation Random Sampling Partitioned Runs Partitioned Runs Final Partitioned Runs Final - Retrained 0.0 0.2 0.4 Scale X 0.0 0.2 0.4 0.6 Scale Y 0.0 0.2 0.4 Shear Figure 9: The test-set performance plotted alongside (1D projections of) afﬁne augmentation hyper- parameters on the RotMNIST task with MNIST-CNN. Final test-set accuracies are shown for the hyperparameters sampled randomly for a neural network model trained through standard training with those hyperparameters ﬁxed (+). For multiple partitioned networks runs, the plot shows the progres- sion of the identiﬁed hyperparameters and the test-set performance through the training run ( ), as well as the ﬁnal hyperparameters and test-set performance ( ). Lastly, the plot also shows the ﬁnal test-set accuracies of models trained through standard training on the ﬁnal hyperparameters identiﬁed through partitioned training ( ). 10The ranges were: Uniform(0,π) for the maximum rotation, and Uniform(0,1 2 ) for all the remaining afﬁne augmentation parameters (maximum shear, maximum x−and y−translation, and maximum x−and y−scale). 20Published as a conference paper at ICLR 2023 H L IMITATIONS As mentioned in the main text, our method improves upon existing work, but also comes with its own limitations. Complexity Inherent to our method — as presented in e.g. Figure 1 — is the necessity for an additional forward-backward pass to update the hyperparameters. Consequently, hyperparameter optimization has additional costs which, however, are signiﬁcantly less than the computational costs of existing work, as we discuss in more detail in Appendix D.1 and the experimental section. Furthermore, empirically, partitioned networks usually require more training iterations to converge. Performance Assuming the optimal hyper-parameters are given, training the full, non-partitioned networks based on those optimal values can be expected to yield better performance compared to the ﬁnal model found by partitioned training. Partitioning the network inherently constrains the network capacity, causing some loss of performance. Opportunities for alleviating this performance loss while still enjoying single-run hyperparameter optimization through partitioned training will be left to future work. These include for example adjusting training rounds or increasing network capacity in the ﬁrst place. Partitioning While partitioned networks allows for automatic optimization of, intuitively, hard to tune hyperparameters, such as augmentation parameters, they come with the additional limitation of requiring to partition both the data and the model. This introduces an additional hyperparameter, namely, the partitioning strategy. While our default strategy of assigning more parameters and data to the ﬁrst chunk works reasonably well on all of the experiments we consider, if one targets obtaining the best possible performance on a given task, the partitioning strategy might need additional tuning. We provide some empirical results about the sensitivity to partitioning in appendix F.1 I E XPERIMENTAL DETAILS I.1 P ARTITIONED TRAINING Partitioned parameter update scheduling The gradient computation of Equation 3, as described in the main text, requires that the data-points for updating a given subnetwork w(k) s come from the appropriate dataset chunks (x,y) ∈D1:k for a chunk k. Depending on the partitioning scheme (Appendix C), evaluating different subnetworks for different chunks can or cannot be done in a single mini-batch. More speciﬁcally, the random weight-partitioning we chose for our experiments requires a separate mini-batch per subnetwork (in order to keep the memory cost the same as for standard neural network training). An immediate question arising from a chunked dataset and several partitions is to deﬁne the order and frequency of updates across subnetworks. In our experiments we deﬁne (non-uniform) splits of the training dataset Dacross the Cchunks, which requires a tailored approach to sampling the data. More speciﬁcally, for a given (normalized) ratio of chunk-sizes [u1,...,u C], each iteration of partitioned training proceeds as follows: 1. Sample a partition index k∼Cat(u1,...,u C) 2. Sample a mini-batch ˜Dof examples uniformly from D1:k. 3. Evaluate log p( ˜D|w(k) s ,ψ) using subnetwork w(k) s and 4. compute the (stochastic) gradient wrt. partition parameters wk (Eq. 3). 5. Update partition parameters wk using an optimizer, such as SGD or Adam. This sampling scheme results in a data-point (x,y) ∈Dk from earlier chunks to be sampled more often. Concretely, the probability that an example in chunk kwill be sampled is ∝∑ i≤kui. This is done so that each partition wk is updated with equal probability on each of the examples in D1:k As a result, we use with replacement sampling for the partitioned network training throughout the experimental section. 21Published as a conference paper at ICLR 2023 Gradient optimization of partitioned parameters A consequence of per-partition updates with the random weight partitioning scheme (appendix C) is that, for a chosen partition wk to update, all other partitions do not receive a gradient update. In other words, the gradient at each iteration is sparse. Consequently, many off-the-shelve momentum-based optimizers will not account correctly. Speciﬁcally, we implement modiﬁcations to the PyTorch Paszke et al. (2019) provided optimizers that allow us to track per-partition momenta, number of steps, etc. Note that this creates a disconnect between the number of iterations across all partitions and the number of iterations per-partition. Doing so, however aligns the computational cost of training the partitioned network parameters with the cost of training regular neural network parameters. Regardless, we do not alter the way learning-rate schedulers behave in our experiments and anneal learning-rates according to the total number of iterations. Similarly, we report the total number of iterations when comparing against baselines that update all network-parameters per iteration. While a simple gradient-accumulation scheme across mini-batches would result in a single gradient across all partitions, this approach inherently clashes with non-uniform partitioning [u1,...,u C]. Instead, we chose to sequentially apply gradients computed on a single partition, as described in the previous paragraphs. A further advantage of this approach is that learning progress made by updating partition wk immediately inﬂuences (and can improve) the prediction of subnetworks w(k) s ,w(k+1) s ,..., w(C) s . Gradient optimization of hyperparameters Our partitioned network scheme makes it easy to compute stochastic gradients of the hyperparameter objective LML in Eq. 4 using batch gradient descent optimization methods. After every update to a randomly sampled network partition (see previous paragraph), we update hyperparamters ψas follows: • sample a dataset chunk index k ∼Cat(u2 Z ,..., uC Z ). Ratios are re-normalized to exclude D1. • sample a mini-batch ˜Dof examples uniformly from Dk (Note the choice of Dk instead of D1:k). • Evaluate log p( ˜D|w(k−1) s ,ψ) using subnetwork w(k−1) s and • compute the (stochastic) gradient wrt. hyperparameters ψ(Eq. 4). • Update partition parameters ψusing an optimizer, such as SGD or Adam. The above sampling procedure yields an unbiased estimate of gradients in eq. 4. The fact that we optimize hyperparameters with gradients based on data from a single chunk at a time is again a consequence of the random weight-partitioning scheme for the partitioned networks. It is possible to compute gradients wrt. ψfor mini-batches with examples from multiple chunks at a time. With the random weight partitioning scheme, this would result in an increased memory overhead. Lastly, we could also accumulate gradients from different chunks, similarly to Immer et al. (2022), and this would likely result in a lower-variance estimate per update . It is also possible to reduce the computational overhead of evaluating two mini-batches per iteration (one for updates to wk, one for ψ) as we do in our experiments by interleaving hyperparameter updates at less frequent intervals. We leave an exploration of these design choices to future work. Throughout all experiments, except those in the federated settings (see section J), we use the same batch-size for the hyperparameter udpates as for the regular parameter updates. Weight-decay For partitioned networks, whenever using weight-decay, we scale the weight decay for earlier partitions with the reciprocal of the number of examples in chunks used to optimize them, following the diagonal Gaussian prior interpretation of weight-decay. This makes the training compatible with the variational interpretation in Appendix B. I.2 P ARTITIONED AFFINE TRANSFORMATIONS In Appendix C we described how we realize partitioned versions of fully-connected and convolutional layers. Design choices for other parameterized network layers used in our experiments are described below. 22Published as a conference paper at ICLR 2023 Normalization layers It is common-place in most architectures to follow a normalization layer (such as BatchNorm (Ioffe and Szegedy, 2015), GroupNorm (Wu and He, 2018)) with an element- wise or channel-wise, afﬁne transformation. Namely, such a transformation multiplies its input h by a scale vector s and adds a bias vector b: o = h ∗s + b. For random weight-partitioned networks, we parameterize such afﬁne transformations by deﬁning separate vectors {s1,..., sC} and {b1,..., bC}for each partition; the actual scale and bias used in a given subnetwork w(k) s are s(k) s = ∏ i∈{1,...,k}si and b(k) s = ∑ i∈{1,...,k}bi respectively. This ensures that the ﬁnal afﬁne transformation for each subnetwork w(k) s depends on the parameters in the previous partitions [1,...,k −1]. Doing so increases the parameter count for the partitioned networks in architectures that use those normalization layers by a negligible amount. Scale and bias in FixUp networks The FixUp paper (Zhang et al., 2019) introduces extra scales and biases into the ResNet architecture that transform the entire output of the layers they follow. We turn these into “partitioned” parameters using the same scheme as that for scales and biases of afﬁne transformations following normalization layers. For partitioned networks, through-out the paper, we match the proportion of parameters assigned to each partition kin each layer to the proportion of data examples in the corresponding chunk Dk. I.3 A RCHITECTURE CHOICES Input selection experiments We use a fully-connected feed-forward neural network with2 hidden layers of size [256,256], and with GeLU (Hendrycks and Gimpel, 2016) activation functions. We initialise the weights using the Kaiming uniform scheme (He et al., 2015). For partitioned networks, we use the random-weight partitioning scheme. Fixup Resnet For all experiments using FixUp ResNets we follow Immer et al. (2022); Zhang et al. (2019), and use a 3-stage ResNet with channel-sizes (16,32,64) per stage, with identity skip- connections for the residual blocks as described in He et al. (2016). The residual stages are followed by average pooling and a ﬁnal linear layer with biases. We use 2D average pooling in the residual branches of the downsampling blocks.We initialize all the parameters as described in Zhang et al. (2019). Wide ResNet For all experiments using a Wide-ResNet-N-D (Zagoruyko and Komodakis, 2016), with N being the depth and D the width multiplier, we use a 3 stage ResNet with channel-sizes (16D,32D,64D). We use identity skip-connections for the residual blocks, as described in He et al. (2016), also sometimes known as ResNetV2. ResNet-50 We use the ”V2” version of Wide ResNet as described in (Zagoruyko and Komodakis, 2016) and replace BatchNormalization with GroupNormalization using 2 groups. We use the ’standard’ with withD= 1and three stages of 8 layers for a 50-layer deep ResNet. We use ReLU activations for all ResNet experiments throughout. MNIST CNN For the MNIST experiments, we use the same architecture as Schw¨obel et al. (2021) illustrated in the replicated Table 9. Table 9: CNN architecture for MNIST experiments Layer Speciﬁcation 2D convolution channels=20, kernel size=(5,5), padding=2, activation=ReLU Max pooling pool size=(2,2), stride=2 2D convolution channels=50, kernel size=(5,5), padding=2, activation=ReLU Max pooling pool size=(2,2), stride=2 Fully connected units=500, activation=ReLU Fully connected units=50, activation=ReLU Fully connected units=10, activation=Softmax 23Published as a conference paper at ICLR 2023 I.4 T RAINING DETAILS Learning afﬁne augmentations For the parametrization of the learnable afﬁne augmentation strategies, we follow prior works for a fair comparison. More speciﬁcally, for our MNIST based setup we follow the parametrization proposed in Schw¨obel et al. (2021) whereas for our CIFAR10 based setup we use the generator parametrization from Immer et al. (2022). Input selection experiments For the model selection (non-differentiable) input selection exper- iments, we train all variants with Adam with a learning rate of 0.001 and a batch-size of 256 for 10000 iterations. For both Laplace and partitioned networks, we do early stopping based on the marginal likelihood objective (LML for partitioned networks). We use weight-decay 0.0003 in both cases. For the post-hoc Laplace method, we use the diagonal Hessian approximation, following the recommendation in (Immer et al., 2021). For partitioned networks, we divide the data and parameters into 8 chunks of uniform sizes. We plot results averaged across 3 runs. Mask learning for input selection experiment We use the same optimizer settings as for the input selection experiment. We train for 30000 iterations, and optimize hyperparameters with Adam with a learning rate of 0.001. We divide the data and parameters into 4 uniform chunks. MNIST experiments We follow Schw¨obel et al. (2021), and optimize all methods with Adam with a learning rate of 0.001, no weight decay, and a batch-size of 200. For the partitioned net- works and Augerino results, we use 20 augmentation samples. We use an Adam optimizer for the hyperparameters with a learning rate of 0.001 (and default beta parameters). For Augerino on MNIST, we use the “safe” variant, as otherwise the hyperparameters and the loss diverge on every training run. We elaborate on this phenomenon in Appendix E. Otherwise, we follow the recommended settings from (Benton et al., 2020) and Immer et al. (2022), namely, a regularization strength of 0.01, and a learning rate for the hyperparameters of 0.05. For both MNIST and CIFAR experiments, we found it beneﬁcial to allocate more data to either the earlier, or the later, chunks. Hence, we use 3 chunks with [80%,10%,10%] split of examples for all MNIST and CIFAR experiments. CIFAR variations experiments We again follow Immer et al. (2022), and optimize all ResNet models with SGD with a learning rate of 0.1 decayed by a factor of 100×using Cosine An- nealing, and momentum of 0.9 (as is standard for ResNet models). We use a batch-size of 250. We again use Adam for hyperparameter optimization with a learning rate of 0.001 (and default beta parameters). We train our method for [2400,8000,12000,20000,40000] iterations on subsets [1000,5000,10000,20000,50000] respectively for CIFAR-10, just as in (Immer et al., 2022). For all methods, we used a weight-decay of 1e−4. For partitioned networks, we increase the weight decay for earlier partitions with the square root of the number of examples in chunks used to optimize them, following the diagonal Gaussian prior interpretation of weight-decay. We use3 chunks with [80%,10%,10%] split of examples. For RotCIFAR-10 results, we noticed our method hasn’t fully converged (based on training loss) in this number of iterations, and so we doubled the number of training iterations for the RotMNIST results. This slower convergence can be explained by the fact that, with our method, we only update a fraction of the network parameters at every iteration. TinyImagenet experiments Our experiments with TinyImagenet (Le and Yang, 2015) closely follow the setting for the CIFAR-10 experiments described above. Images are of size64x64 pixels, to be classiﬁed into one of 200 classes. The training-set consists of 100000 images and we compare our method against baselines on subset of [10000,50000,100000] datapoints. For the standard version of TinyImagenet, we train for [80000,80000,40000] steps respectively and for the rotated version of TinyImagenet we train for 120000 steps for all subset sizes. We tuned no other hyper-parameters compared to the CIFAR-10 setup and report our method’s result for a partitioning with[80%,20%] across 2 chunks after ﬁnding it to perform slightly better than a [80%,10%,10%] split across 3 chunks in a preliminary comparison. 24Published as a conference paper at ICLR 2023 Fine-tuning experiments For the ﬁne-tuning experiments in table 2, we trained a FixUp ResNet-14 on a subset of 20000 CIFAR10 examples, while optimizing afﬁne augmentations (following afﬁne augmentations parameterization in (Benton et al., 2020)). We used the same optimizer settings as for all other CIFAR experiments, and trained for 80000 iterations, decaying the learning rate with Cosine Annealing for the ﬁrst 60000 iterations. For ﬁne-tuning of validation-set optimization models, we used SGD with same settings, overriding only the learning rate to 0.01. We tried a learning rate of 0.01 and 0.001, and selected the one that was most favourable for the baseline based on the test accuracy. We also tried training on the full CIFAR-10 dataset, but found that all methods ended up within a standard error of each other when more than 70% of the data was assigned to the ﬁrst chunk (or training set, in the case of validation set optimization). This indicates that CIFAR-10 is sufﬁciently larger that, when combined with afﬁne augmentation learning and the relatively small ResNet-14 architecture used, using the extra data in the 2nd partition (or the validation set) results in negligible gains. I.5 D ATASETS Input selection synthetic dataset For the input selection dataset, we sample 3000 datapoints for the training set as described in section 5, and we use a fresh sample of 1000 datapoints for the test set. RotMNIST Sometimes in the literature, RotMNIST referes to a speciﬁc subset of 12000 MNIST examples, whereas in other works, the full dataset with 60000 examples is used. In this work, following (Benton et al., 2020; Immer et al., 2022) we use the latter. J F EDERATED PARTITIONED TRAINING In this section, we explain how partitioned networks can be applied to the federated setting, as well as the experimental details. J.1 P ARTITIONED NETWORKS IN FL In order to apply partitioned networks to the federated setting, we randomly choose a partition for each client such that the marginal distribution of partitions follows a pre-determined ratio. A given chunk Dk therefore corresponds to the union of several clients’ datasets. Analogous to how “partitioned training” is discussed in the main text and Appendix I, we desire each partition wk to be updated on chunks D1:k. Equation 3 in the main text explains which data chunks are used to compute gradients wrt. parameter partition wk. An analogous perspective to this objective is visualized by the exemplary algorithm in Figure 1 and asks which partitions are inﬂuenced (i,e., updated) by data from chunk Dk: A data chunk Dk is used to compute gradients wrt. partitions wk:C through subnetworks w(k) s to w(C) s respectively. Consequently, a client whose dataset is assigned to chunkDk can compute gradients for all partitions wk:C. Updating network partitions Due to the weight-partitioned construction of the partitioned neural networks, it is not possible to compute gradients with respect to all partitions in a single batched forward-pass through the network. Additionally, a change to the partition parameters wk directly inﬂuences subnetworks w(k+1) s to w(C) s . In order to avoid the choice of ordering indices kto Cfor the client’s local update computation, we update each partition independently while keeping all other partitions initialised to the server-provided values that the client received in that round t: Denote Di,k as the dataset of client iwhere we keep index kto emphasize the client’s assignment to chunkk. Further denote wt+1 j,i as the partition wt j after having been updated by client ion dataset Di,k. wt+1 j,i = arg max wj log p ( Di,k|(wt 1,..., wt j, ˆwt j+1,..., ˆwt j+C),ψ ) ∀j ∈[k,C], (25) where the details of optimization are explained in the following section. We leave an explo- ration for different sequential updating schemes to future work. The ﬁnal update communi- cated by a client to the server consists of the concatenation of all updated parameter partitions 25Published as a conference paper at ICLR 2023 wt+1 .,i = concat(wt+1 k,i ,..., wt+1 C,i ). Note that partitions (wt 1,..., wt k−1) have not been modiﬁed and need not be communicated to the server. The resulting communication reductions make partitioned networks especially attractive to FL as data upload from client to server poses a signiﬁcant bottleneck. In practice, we expect the beneﬁts of these communication reductions to outweigh the additional computation burden of sequentially computing gradients wrt., to multiple partitions. The server receives wt+1 .,i from all clients that participates in round t, computes the delta’s with the global model and proceeds to average them to compute the server-side gradient in the typical federated learning fashion (Reddi et al., 2020). Updating hyperparameters The computation of gradients on a clientiwrt. ψis a straight-forward extension of equation 4 and the exemplary algorithm of Figure 1: ∇ψLML (Di,k,ψ) ≈∇ψlog p ( Di,k|w(t+1),(k−1) s,i ,ψ ) , (26) where Di,k corresponds to client i’s local dataset which is assigned to chunk k and w(t+1),(k−1) s corresponds to the (k−1)’th subnetwork after incorporating all updated partitionsw(t+1),(k−1) s,i = concat(wt 1,..., wt k−1,wt+1 k,i ,..., wt+1 C,i ). Note that we compute a full-batch update to ψin MNIST experiments and use a batch-size equal to the batch-size for the partitioned parameter updates for CIFAR10. Upon receiving these gradients from all clients in this round, the server averages them to form a server-side gradient. Conceptually, this approach to updating ψcorresponds to federated SGD. J.2 F EDERATED SETUP Non-i.i.d. partitioning For our federated experiments, we split the 50kMNIST and 45kCIFAR10 training data-points across 100 clients in a non-i.i.d. way to create the typical challenge to federated learning experiments. In order to simulate label-skew, we follow the recipe proposed in Reddi et al. (2020) with α= 1.0 for CIFAR10 and α= 0.1 for MNIST. Note that with α= 0.1, most clients have data corresponding to only a single digit. For our experiments on rotated versions of CIFAR10 and MNIST, we sample a degree of rotation per data-point and keep it ﬁxed during training. In order to create a non-i.i.d partitioning across the clients, we bin data-points according to their degree of rotation into 10 bins and sample using the same technique as for label-skew with α = 0.1 for both datasets. Learning curves are computed using the 10k MNIST and 5k CIFAR10 validation data-points respectively. For the rotated dataset experiments, we rotate the validation set in the same manner as the training set. Architectures and experimental setup We use the convolutional network provided at Schw¨obel et al. (2021) for MNIST and the ResNet-9 (Dys) model for CIFAR10 but with group normaliza- tion (Wu and He, 2018) instead of batch normalization. We include (learnable) dropout using the continuous relaxation proposed at Maddison et al. (2016) between layers for both architectures. We select 3 chunks for MNIST with a [0.7,0.2,0.1] ratio for both, client-assignments and parameter- partition sizes. For CIFAR10, we found a [0.9,0.1] split across 2 sub-networks to be beneﬁcial. In addition to dropout logits, ψencompasses parameters for afﬁne transformations, i.e., shear, trans- lation, scale and rotation. We report results after 2kand 5krounds, respectively, and the expected communication costs as a percentage of the non-partitioned baseline. Shared setting In order to elaborate on the details to reproduce our results, we ﬁrst focus on the settings that apply across all federated experiments. We randomly sample the corresponding subset of 1.25k, 5kdata-points from the full training set and keep that selection ﬁxed across experiments (i,e., baselines and partitioned networks) as well as seeds. The subsequent partitioning across clients as detailed in the previous paragraph is equally kept ﬁxed across experiments and seeds. Each client computes updates for one epoch of its local dataset, which, for the low data regimes of 1.25k data-points globally, results in single update per client using the entire local dataset. We averaged over 10 augmentation samples for the forward pass in both training and inference. MNIST & RotMNIST For 5k data-points and correspondingly 50 data-points on average per client, most clients perform a single update step. A small selection of clients with more than 64 data- 26Published as a conference paper at ICLR 2023 points performs two updates per round. For the experiments using the full dataset and a mini-batch size of 64, each client performs multiple updates per round. After initial exploration on the baseline FedAvg task, we select a local learning-rate of 5e−2 and apply standard SGD. The server performs Adam Reddi et al. (2020) with a learning rate of 1e−3 for the model parameters. We keep the other parameters of Adam at their standard PyTorch values. We ﬁnd this setting to generalize to the partitioned network experiments but found a higher learning rate of3e−3 for the hyper-parameters to be helpful. We chose the convolutional network from Schw¨obel et al. (2021) with (learned) dropout added between layers. The model’s dropout layers are initialized to drop10% of hidden activations. For the baseline model we keep the dropout-rate ﬁxed and found 10% to be more stable than 30%. CIFAR10 & RotCIFAR10 We ﬁx a mini-batch size of 32, leading to multiple updates per client per round in both, the full dataset regime as well as the5kdata-points setting. Similarly to the MNIST setting, we performed an initial exploration of hyperparameters on the baseline FedAvg task and use the same ones on partitioned networks. We used dropout on the middle layer of each block which was initialized to 0.1 for both the baseline and partitioned networks and whereas partitioned networks optimized it with LML and the concrete relaxation from Maddison et al. (2016), the baseline kept it ﬁxed. For the server side optimizer we used Adam with the default betas and a learning rate of 1e−2, whereas for the hyperparameters we used Adam with the default betas and a learning rate of 1e−3. In both cases we used an ϵ= 1e−7. For the local optimizer we used SGD with a learning rate of 10−0.5 and no momentum. J.3 MNIST LEARNING CURVES In Figure 10 we show learning curves for the three considered dataset sizes on the standard MNIST task. Each learning curve is created by computing a moving average across 10 evaluations, each of which is performed every 10 communication rounds, for each seed. We then compute the average and standard-error across sees and plot those values on the y-axis. On the x-axis we denote the total communication costs (up- and download) to showcase the partitioned networks reduction in communication overhead. We see that especially for the low dataset regime, training has not converged yet and we expect performance to improve for an increased number of iterations. Figure 10: Learning curves for MNIST experiments on 1.25k, 5kand 50kdata-points respectively. 27",
      "meta_data": {
        "arxiv_id": "2304.14766v1",
        "authors": [
          "Bruno Mlodozeniec",
          "Matthias Reisser",
          "Christos Louizos"
        ],
        "published_date": "2023-04-28T11:24:41Z",
        "pdf_url": "https://arxiv.org/pdf/2304.14766v1.pdf"
      }
    },
    {
      "title": "Implicit differentiation of Lasso-type models for hyperparameter optimization",
      "abstract": "Setting regularization parameters for Lasso-type estimators is notoriously\ndifficult, though crucial in practice. The most popular hyperparameter\noptimization approach is grid-search using held-out validation data.\nGrid-search however requires to choose a predefined grid for each parameter,\nwhich scales exponentially in the number of parameters. Another approach is to\ncast hyperparameter optimization as a bi-level optimization problem, one can\nsolve by gradient descent. The key challenge for these methods is the\nestimation of the gradient with respect to the hyperparameters. Computing this\ngradient via forward or backward automatic differentiation is possible yet\nusually suffers from high memory consumption. Alternatively implicit\ndifferentiation typically involves solving a linear system which can be\nprohibitive and numerically unstable in high dimension. In addition, implicit\ndifferentiation usually assumes smooth loss functions, which is not the case\nfor Lasso-type problems. This work introduces an efficient implicit\ndifferentiation algorithm, without matrix inversion, tailored for Lasso-type\nproblems. Our approach scales to high-dimensional data by leveraging the\nsparsity of the solutions. Experiments demonstrate that the proposed method\noutperforms a large number of standard methods to optimize the error on\nheld-out data, or the Stein Unbiased Risk Estimator (SURE).",
      "full_text": "Implicit differentiation of Lasso-type models for hyperparameter optimization Quentin Bertrand* 1 Quentin Klopfenstein* 2 Mathieu Blondel3 Samuel Vaiter4 Alexandre Gramfort1 Joseph Salmon5 Abstract Setting regularization parameters for Lasso-type estimators is notoriously difﬁcult, though cru- cial in practice. The most popular hyperparam- eter optimization approach is grid-search using held-out validation data. Grid-search however re- quires to choose a predeﬁned grid for each pa- rameter, which scales exponentially in the num- ber of parameters. Another approach is to cast hyperparameter optimization as a bi-level opti- mization problem, one can solve by gradient de- scent. The key challenge for these methods is the estimation of the gradient w.r.t.the hyperpa- rameters. Computing this gradient via forward or backward automatic differentiation is possible yet usually suffers from high memory consump- tion. Alternatively implicit differentiation typi- cally involves solving a linear system which can be prohibitive and numerically unstable in high dimension. In addition, implicit differentiation usually assumes smooth loss functions, which is not the case for Lasso-type problems. This work introduces an efﬁcient implicit differentia- tion algorithm, without matrix inversion, tailored for Lasso-type problems. Our approach scales to high-dimensional data by leveraging the sparsity of the solutions. Experiments demonstrate that the proposed method outperforms a large num- ber of standard methods to optimize the error on held-out data, or the Stein Unbiased Risk Esti- mator (SURE). *Equal contribution 1Université Paris-Saclay, Inria, CEA, Palaiseau, France 2Institut Mathématique de Bourgogne, Univer- sité de Bourgogne, Dijon, France 3Google Research, Brain team, Paris, France 4CNRS and Institut Mathématique de Bourgogne, Université de Bourgogne, Dijon, France 5IMAG, Université de Montpellier, CNRS, Montpellier, France. Correspondence to: Quentin Bertrand <quentin.bertrand@inria.fr>, Quentin Klopfen- stein <quentin.klopfenstein@u-bourgogne.fr>. Proceedings of the 37 th International Conference on Machine Learning, Online, PMLR 119, 2020. Copyright 2020 by the au- thor(s). 1. Introduction In many statistical applications, the number of parame- ters p is much larger than the number of observations n. In such scenarios, a popular approach to tackle linear re- gression problems is to consider convex ℓ1-type penalties, used in Lasso (Tibshirani, 1996), Group-Lasso (Yuan and Lin, 2006), Elastic-Net (Zou and Hastie, 2005) or adap- tive Lasso (Zou, 2006). These Lasso-type estimators rely on regularization hyperparameters, trading data ﬁdelity against sparsity. Unfortunately, setting these hyperparame- ters is hard in practice: estimators based on ℓ1-type penal- ties are indeed more sensitive to the choice of hyperparam- eters than ℓ2 regularized estimators. To control for overﬁtting, it is customary to use different datasets for model training ( i.e., computing the regression coefﬁcients) and hyperparameter selection ( i.e., choosing the best regularization parameters). A metric, e.g., hold- out loss , is optimized on a validation dataset (Stone and Ramer, 1965). Alternatively one can rely on a statistical criteria that penalizes complex models such as AIC/BIC (Liu et al., 2011) or SURE (Stein Unbiased Risk Estima- tor, Stein 1981). In all cases, hyperparameters are tuned to optimize a chosen metric. The canonical hyperparameter optimization method is grid-search. It consists in ﬁtting and selecting the best model over a predeﬁned grid of parameter values. The complexity of grid-search is exponential with the number of hyperparameters, making it only competitive when the number of hyperparameters is small. Other hyperparameter selection strategies include random search (Bergstra and Bengio, 2012) and Bayesian optimization (Brochu et al., 2010; Snoek et al., 2012) that aims to learn an approxima- tion of the metric over the parameter space and rely on an exploration policy to ﬁnd the optimum. Another line of work for hyperparameter optimization (HO) relies on gradient descent in the hyperparameter space. This strategy has been widely explored for smooth objective functions (Larsen et al., 1996; Bengio, 2000; Larsen et al., 2012). The main challenge for this class of methods is estimating the gradient w.r.t.the hyperparame- ters. Gradient estimation techniques are mostly divided in two categories. Implicit differentiation requires the exact arXiv:2002.08943v3  [stat.ML]  3 Sep 2020Implicit differentiation of Lasso-type models for hyperparameter optimization solution of the optimization problem and involves the res- olution of a linear system (Bengio, 2000). This can be ex- pensive to compute and lead to numerical instabilities, es- pecially when the system is ill-conditioned (Lorraine et al., 2019). Alternatively, iterative differentiation computes the gradient using the iterates of an optimization algorithm. Backward iterative differentiation (Domke, 2012) is com- putationally efﬁcient when the number of hyperparameters is large. However it is memory consuming since it requires storing all intermediate iterates. In contrast, forward itera- tive differentiation (Deledalle et al., 2014; Franceschi et al., 2017) does not require storing the iterates but can be com- putationally expensive with a large number of hyperparam- eters; see Baydin et al. (2018) for a survey. This article proposes to investigate the use of these meth- ods to set the regularization hyperparameters in an auto- matic fashion for Lasso-type problems. To cover the cases of both low and high number of hyperparameters, two esti- mators are investigated, namely the Lasso and the weighted Lasso which have respectively one or as many parameters as features. Our contributions are as follows: • We show that forward iterative differentiation of block coordinate descent (BCD), a state-of-the-art solver for Lasso-type problems, converges towards the true gra- dient. Crucially, we show that this scheme converges linearly once the support is identiﬁed and that its limit does not depend of the initial starting point. • These results lead to the proposed algorithm (Algo- rithm 2) where the computation of the Jacobian is de- coupled from the computation of the regression co- efﬁcients. The later can be done with state-of-the-art convex solvers, and interestingly, it does not require solving a linear system, potentially ill-conditioned. • We show through an extensive benchmark on simu- lated and real high dimensional data that the proposed method outperforms state-of-the-art HO methods. Our work is somewhat similar to Gregor and LeCun (2010); Xin et al. (2016); Borgerding et al. (2017); Liu et al. (2018); Wu et al. (2019), where the solution is differenti- ated w.r.t. optimization parameters instead of the regular- ization parameter. However the goal is very different as they want to accelerate the optimization algorithm whereas we provide an efﬁcient algorithm to compute the gradient. Notation The design matrix is X ∈Rn×p (corresponding to nsamples and pfeatures) and the observation vector is y ∈Rn. The regularization parameter, possibly multivari- ate, is denoted by λ = (λ1,...,λ r)⊤ ∈Rr. We denote ˆβ(λ) ∈Rp the regression coefﬁcients associated to λ. We denote ˆJ(λ) ≜ (∇λˆβ(λ) 1 ,..., ∇λˆβ(λ) p )⊤∈Rp×r the weak Jacobian (Evans and Gariepy, 1992) of ˆβ(λ) w.r.t.λ. For a function ψ : Rp ×Rr →R with weak derivatives of order two, we denote by∇βψ(β,λ) ∈Rp(resp. ∇λ(β,λ) ∈Rr) its weak gradient w.r.t.the ﬁrst parameter (resp. the second parameter). The weak Hessian ∇2ψ(β,λ) is a matrix in R(p+r)×(p+r) which has a block structure ∇2ψ(β,λ) = (∇2 βψ(β,λ) ∇2 β,λψ(β,λ) ∇2 λ,βψ(β,λ) ∇2 λψ(β,λ) ) . The support of ˆβ(λ) (the indices of non-zero coefﬁcients) is denoted by ˆS(λ), and ˆs(λ) represents its cardinality (i.e., the number of non-zero coefﬁcients). The sign vec- tor sign ˆβ(λ) ∈Rp is the vector of component-wise signs (with the convention thatsign(0) = 0) of ˆβ(λ). Note that to ease the reading, we drop λin the notation when it is clear from the context and use ˆβ, ˆJ, ˆS and ˆs. The Mahalanobis distance of a vector x ∈Rp and a matrix A ≻0 is noted ∥x∥A ≜ √ x⊤A−1x. 2. Background 2.1. Problem setting To favor sparse coefﬁcients, we consider Lasso-type es- timators based on non-smooth regularization functions. Such problems consist in ﬁnding: ˆβ(λ) ∈arg min β∈Rp ψ(β,λ) . (1) The Lasso (Tibshirani, 1996) is recovered, with the number of hyperparameters set to r= 1: ψ(β,λ) = 1 2n∥y−Xβ∥2 2 + eλ∥β∥1 , (2) while the weighted Lasso (wLasso, Zou 2006, introduced to reduce the bias of the Lasso) has r= phyperparameters and reads: ψ(β,λ) = 1 2n∥y−Xβ∥2 2 + p∑ j=1 eλj|βj|. (3) Note that we adopt the hyperparameter parametrization of Pedregosa (2016), i.e., we write the regularization parame- ter as eλ. This avoids working with a positivity constraint in the optimization process and ﬁxes scaling issues in the line search. It is also coherent with the usual choice of a geometric grid for grid-search (Friedman et al., 2010). Remark 1. Other formulations could be investigated like Elastic-Net or non-convex formulation, e.g., MCP (Zhang, 2010). Our theory does not cover non-convex cases, though we illustrate that it behaves properly numerically. Handling such non-convex cases is left as a question for future work. The HO problem can be expressed as a nested bi-level op- timization problem. For a given differentiable criterion C: Rp ↦→R (e.g., hold-out loss or SURE), it reads:Implicit differentiation of Lasso-type models for hyperparameter optimization arg min λ∈Rr { L(λ) ≜ C ( ˆβ(λ) )} s.t. ˆβ(λ) ∈arg min β∈Rp ψ(β,λ) . (4) Note that SURE itself is not necessarily weakly differen- tiable w.r.t. ˆβ(λ). However a weakly differentiable approx- imation can be constructed (Ramani et al., 2008; Deledalle et al., 2014). Under the hypothesis that Problem (1) has a unique solution for every λ∈Rr, the function λ↦→ˆβ(λ) is weakly differentiable (Vaiter et al., 2013). Using the chain rule, the gradient of Lw.r.t.λthen writes: ∇λL(λ) = ˆJ⊤ (λ)∇C ( ˆβ(λ) ) . (5) Computing the weak Jacobian ˆJ(λ) of the inner problem is the main challenge, as once the hypergradient ∇λL(λ) has been computed, one can use usual gradient descent, λ(t+1) = λ(t) −ρ∇λL(λ(t)), for a step size ρ > 0. Note however that Lis usually non-convex and conver- gence towards a global minimum is not guaranteed. In this work, we propose an efﬁcient algorithm to compute ˆJ(λ) for Lasso-type problems, relying on improved forward dif- ferentiation. 2.2. Implicit differentiation (smooth case) Implicit differentiation, which can be traced back to Larsen et al. (1996), is based on the knowledge of ˆβ and requires solving a p×plinear system (Bengio, 2000, Sec. 4). Since then, it has been extensively applied in various contexts. Chapelle et al. (2002); Seeger (2008) used implicit differ- entiation to select hyperparameters of kernel-based mod- els. Kunisch and Pock (2013) applied it to image restora- tion. Pedregosa (2016) showed that each inner optimiza- tion problem could be solved only approximately, leverag- ing noisy gradients. Related to our work, Foo et al. (2008) applied implicit differentiation on a “weighted” Ridge-type estimator (i.e., a Ridge penalty with one λj per feature). Yet, all the aforementioned methods have a common draw- back : they are limited to the smooth setting, since they rely on optimality conditions for smooth optimization. They proceed as follows: if β ↦→ψ(β,λ) is a smooth convex function (for any ﬁxed λ) in Problem (1), then for all λ, the solution ˆβ(λ) satisﬁes the following ﬁxed point equation: ∇βψ ( ˆβ(λ),λ ) = 0 . (6) Then, this equation can be differentiated w.r.t.λ: ∇2 β,λψ( ˆβ(λ),λ) + ˆJ⊤ (λ)∇2 βψ( ˆβ(λ),λ) = 0. (7) Assuming that ∇2 βψ( ˆβ(λ),λ) is invertible this leads to a closed form solution for the weak Jacobian ˆJ(λ): ˆJ⊤ (λ) = −∇2 β,λψ ( ˆβ(λ),λ )( ∇2 βψ(β(λ),λ) )    p×p −1 , (8) which in practice is computed by solving a linear system. Unfortunately this approach cannot be generalized for non- smooth problems since Equation (6) no longer holds. 2.3. Implicit differentiation (non-smooth case) Related to our work Mairal et al. (2012) used implicit dif- ferentiation with respect to the dictionary ( X ∈Rn×p) on Elastic-Net models to perform dictionary learning. Regard- ing Lasso problems, the literature is quite scarce, see (Dos- sal et al., 2013; Zou et al., 2007) and (Vaiter et al., 2013; Tibshirani and Taylor, 2011) for a more generic setting encompassing weighted Lasso. General methods for gra- dient estimation of non-smooth optimization schemes ex- ist (Vaiter et al., 2017) but are not practical since they de- pend on a possibly ill-posed linear system to invert. Amos and Kolter (2017) have applied implicit differentiation on estimators based on quadratic objective function with lin- ear constraints, whereas Niculae and Blondel (2017) have used implicit differentiation on a smooth objective func- tion with simplex constraints. However none of these ap- proaches leverages the sparsity of Lasso-type estimators. 3. Hypergradients for Lasso-type problems To tackle hyperparameter optimization of non-smooth Lasso-type problems, we propose in this section an efﬁcient algorithm for hypergradient estimation. Our algorithm re- lies on implicit differentiation, thus enjoying low-memory cost, yet does not require to naively solve a (potentially ill-conditioned) linear system of equations. In the sequel, we assume access to a (weighted) Lasso solver, such as ISTA (Daubechies et al., 2004) or Block Coordinate De- scent (BCD, Tseng and Yun 2009, see also Algorithm 5). 3.1. Implicit differentiation Our starting point is the key observation that Lasso-type solvers induce a ﬁxed point iteration that we can leverage to compute a Jacobian. Indeed, proximal BCD algorithms (Tseng and Yun, 2009), consist in a local gradient step com- posed with a soft-thresholding step (ST),e.g., for the Lasso, for j ∈1,...,p : βj ←ST ( βj −X⊤ :,j(Xβ −y) ∥X:,j∥2 , neλ ∥X:,j∥2 ) (9) where ST(t,τ) = sign(t)·(|t|−τ)+ for any t∈R and τ ≥ 0 (extended for vectors component-wise). The solution ofImplicit differentiation of Lasso-type models for hyperparameter optimization the optimization problem satisﬁes, for anyα> 0, the ﬁxed- point equation (Combettes and Wajs, 2005, Prop. 3.1), for j ∈1,...,p : ˆβ(λ) j = ST ( ˆβ(λ) j −1 αX⊤ j,:(Xˆβ(λ) −y),neλ α ) . (10) The former can be differentiated w.r.t. λ, see Lemma A.1 in Appendix, leading to a closed form solution for the Ja- cobian J(λ) of the Lasso and the weighted Lasso. Proposition 1(Adapting Vaiter et al. 2013, Thm. 1) . Let ˆSbe the support of the vector ˆβ(λ). Suppose that X⊤ ˆSXˆS ≻0 , then a weak Jacobian ˆJ = ˆJ(λ) of the Lasso writes: ˆJˆS = −neλ( X⊤ ˆSXˆS )−1 sign ˆβˆS, (11) ˆJˆSc = 0 , (12) and for the weighted Lasso: ˆJˆS,ˆS = − ( X⊤ ˆSXˆS )−1 diag ( neλˆS ⊙sign ˆβˆS ) (13) ˆJj1,j2 = 0 if j1 /∈ˆSor if j2 /∈ˆS . (14) The proof of Proposition 1 can be found in Appendix A.1. Note that the positivity condition in Proposition 1 is satis- ﬁed if the (weighted) Lasso has a unique solution. More- over, even for multiple solutions cases, there exists at least one satisfying the positivity condition (Vaiter et al., 2013). Proposition 1 shows that the Jacobian of the weighted Lasso ˆJ(λ) ∈ Rp×p is row and column sparse. This is key for algorithmic efﬁciency. Indeed, a priori, one has to store a possibly dense p×p matrix, which is prohibitive when pis large. Proposition 1 leads to a simple algorithm (see Algorithm 1) to compute the Jacobian in a cheap way, as it only requires storing and inverting an ˆs×ˆs matrix. Even if the linear system to solve is of size ˆs×ˆs, instead of p×pfor smooth objective function, the system to invert can be ill-conditioned, especially when a large support size ˆsis encountered. This leads to numerical instabilities and slows down the resolution (see an illustration in Figure 2). Forward (Algorithm 3 in Appendix) and backward (Algo- rithm 4 in Appendix) iterative differentiation, which do not require solving linear systems, can overcome these issues. 3.2. Link with iterative differentiation Iterative differentiation in the ﬁeld of hyperparameter set- ting can be traced back to Domke (2012) who derived a backward differentiation algorithm for gradient descent, heavy ball and L-BFGS algorithms applied to smooth loss functions. Agrawal et al. (2019) generalized it to a spe- ciﬁc subset of convex programs. Maclaurin et al. (2015) derived a backward differentiation for stochastic gradient Algorithm 1IMPLICIT DIFFERENTIATION input : X ∈Rn×p,y ∈Rn,λ ∈R,niter ∈N // jointly compute coef. and Jacobian if Lasso then Get ˆβ = Lasso(X,y,λ,n iter) and its support ˆS. ˆJ = 0p ˆJˆS = −neλ(X⊤ ˆSXˆS)−1 sign ˆβˆS if wLasso then Get ˆβ = wLasso(X,y,λ,n iter) and its support ˆS. ˆJ= 0p×p ˆJˆS,ˆS = −(X⊤ ˆSXˆS)−1 diag(neλˆS ⊙sign ˆβˆS) return ˆβ, ˆJ descent. On the other hand Deledalle et al. (2014) used forward differentiation of (accelerated) proximal gradient descent for hyperparameter optimization with non-smooth penalties. Franceschi et al. (2017) proposed a benchmark of forward mode versus backward mode, varying the num- ber of hyperparameters to learn. Frecon et al. (2018) cast the problem of inferring the groups in a group-Lasso model as a bi-level optimization problem and solved it using back- ward differentiation. Forward differentiation consists in differentiating each step of the algorithm (w.r.t.λin our case). For the Lasso solved with BCD it amounts differentiating Equation (9), and leads to the following recursive equation for the Jacobian, for j ∈1,...p , with zj = βj −X⊤ :,j(Xβ −y)/∥X:,j∥2: Jj ←∂1 ST ( zj, neλ ∥X:,j∥2 )( Jj − 1 ∥X:,j∥2 X⊤ :,jXJ ) + ∂2 ST ( zj, neλ ∥X:,j∥2 ) neλ ∥X:,j∥2 , (15) see Algorithm 3 (in Appendix) for full details. Our proposed algorithm uses the fact that after a ﬁ- nite number of epochs ∂1 ST(zj,neλ/∥X:,j∥2) and ∂2 ST(zj,neλ/∥X:,j∥2) are constant (they no no longer depends on the current β). Indeed, the sign of ˆβ is iden- tiﬁed after a ﬁnite number of iterations thus the partial derivatives are constant. It is then possible to decouple the computation of the Jacobian by only solving Problem (1) in a ﬁrst step and then apply the forward differentiation recur- sion steps, see Algorithm 2. This can be seen as the forward counterpart in a non-smooth case of the recent paper Lor- raine et al. (2019). An additional beneﬁt of such updates is that they can be restricted to the (current) support, which leads to faster Jacobian computation. We now show that the Jacobian computed using forward differentiation and our method, Algorithm 2, converges to- ward the true Jacobian.Implicit differentiation of Lasso-type models for hyperparameter optimization Proposition 2. Assuming the Lasso solution (Prob- lem (2)) (or weighted Lasso Problem (3)) is unique, then Algorithms 2 and 3 converge toward the Jaco- bian ˆJ deﬁned in Proposition 1. Algorithm 3 com- putes the Jacobian along with the regression coefﬁ- cients, once the support has been identiﬁed, the Jaco- bian converges linearly. Algorithm 2 computes ﬁrst the coefﬁcients ˆβ and then the Jacobian ˆJ, provided that the support has been identiﬁed in the ﬁrst step, the convergence is linear in the second, with the same rate as Algorithm 3: ∥J(k+1) ˆS −ˆJ∥(X⊤ :,ˆSX:,ˆS)−1 ≤Ck∥J(k) ˆS −ˆJ∥(X⊤ :,ˆSX:,ˆS)−1 where C = ∥A(jˆs) ...A (j1)∥2 <1, j1,...,j ˆs are the indices of the support of ˆβin increasing order and A(js) = Idˆs− ( X⊤ :,ˆSX:,ˆS )1/2 :,js ∥X:,js∥ ( X⊤ :,ˆSX:,ˆS )1/2 js,: ∥X:,js∥ ∈Rˆs×ˆs. Proof of Proposition 2 can be found in Appendix A.2 and A.3. Remark 3. Uniqueness. As proved in Tibshirani (2013, Lem. 3 and 4) the set of (pathological) lambdas where the Lasso solution is not unique is typically empty. More- over if the Lasso solution is not unique, there could be a non-continuous solution path λ↦→ˆβ(λ), leaving only non- gradient based methods available. Even if Proposition 2 does not provide theoretical guarantees in such a patholog- ical setting, one can still apply Algorithms 2 and 3, see Appendix E.1 for experiments in this settings. Remark 4. Rate for the backward differentiation. The backward and forward differentiation compute the same quantity: ∇λL(λ), but the backward differentiation di- rectly computes the product given in Equation (5) leading to the gradient ofL(λ). Proposition 2 provides rates for the convergence of the Jacobian Jwhich leads to rates for the gradient i.e., for the backward algorithm as well. As an illustration, Figure 1 shows the times of computa- tion of a single gradient ∇λL(λ) and the distance to “op- timum” of this gradient as a function of the number of it- erations in the inner optimization problem for the forward iterative differentiation (Algorithm 3), the backward iter- ative differentiation (Algorithm 4), and the proposed algo- rithm (Algorithm 2). The backward iterative differentiation is several order of magnitude slower than the forward and our implicit forward method. Moreover, once the support has been identiﬁed (after 20 iterations) the proposed im- plicit forward method converges faster than other methods. Note also that in Propositions 1 and 2 the Jacobian for the Imp. F. Iterdiﬀ. (ours)F. Iterdiﬀ.B. Iterdiﬀ. 20 40 60 Number of iterations 10−1 100 101 Times (s) 20 40 60 Number of iterations 10−7 10−5 Objective minus optimum Figure 1.Time to compute a single gradient(Synthetic data, Lasso, n,p = 1000,2000). Inﬂuence on the number of iterations of BCD (in the inner optimization problem of Problem (4)) on the computation time (left) and the distance to “optimum” of the gra- dient ∇λL(λ)(right) for the Lasso estimator. The “optimum” is here the gradient given by implicit differentiation (Algorithm 1). Lasso only depends on the support (i.e., the indices of the non-zero coefﬁcients) of the regression coefﬁcients ˆβ(λ). In other words, once the support of ˆβ(λ) is correctly identi- ﬁed, even if the value of the non-zeros coefﬁcients are not correctly estimated, the Jacobian is exact, see Sun et al. (2019) for support identiﬁcation guarantees. 4. Experiments Our Python code is released as an open source package: https://github.com/QB3/sparse-ho. All the experiments are written in Python using Numba (Lam et al., 2015) for the critical parts such as the BCD loop. We com- pare our gradient computation technique against other com- petitors (see the competitors section) on the HO problem (Problem (4)). Solving the inner optimization problem.Note that our proposed method, implicit forward differentiation, has the appealing property that it can be used with any solver. For instance for the Lasso one can combine the proposed al- gorithm with state of the art solver such as Massias et al. (2018) which would be tedious to combine with iterative differentiation methods. However for the comparison to be fair, for all methods we have used the same vanilla BCD algorithm (recalled in Algorithm 5). We stop the Lasso- types solver when f(β(k+1))−f(β(k)) f(0) <ϵtol ,where f is the cost function of the Lasso or wLasso and ϵtol a given toler- ance. The tolerance is ﬁxed at ϵtol = 10−5 for all methods throughout the different benchmarks. Line search. For each hypergradient-based method, the gradient step is combined with a line-search strategy fol- lowing the work of Pedregosa (2016)1. Initialization. Since the function to optimize Lis not con- 1see https://github.com/fabianp/hoag for detailsImplicit differentiation of Lasso-type models for hyperparameter optimization Table 1.Summary of cost in time and space for each method Mode Computed Space Time Space Time quantity (Lasso) (Lasso) (wLasso) (wLasso) F. Iterdiff. J O(p) O(2npniter) O(p2) O(np2niter) B. Iterdiff. J⊤v O(2pniter) O(npniter + np2niter) O(p2niter) O(npniter + np2niter) Implicit J⊤v O(p) O(npniter + ˆs3) O(p+ ˆs2) O(npniter + ˆs3) Imp. F. Iterdiff. J O(p) O(npniter + nˆsniter_jac) O(p+ ˆs2) O(npniter + nˆs2nit_jac) Algorithm 2IMP. F. I TERDIFF . (proposed) input : X ∈Rn×p,y ∈Rn,λ ∈R,niter,niter_jac ∈N init : J= 0 // sequentially compute coef. & Jacobian if Lasso then Get ˆβ = Lasso(X,y,λ,n iter) and its support ˆS. dr= −X:,ˆSJˆS // trick for cheap updates if wLasso then Get ˆβ = wLasso(X,y,λ,n iter) and its support ˆS. dr= −X:,ˆSJˆS,ˆS for k= 0,...,n iter_jac −1 do for j ∈ˆSdo if Lasso then Jold = Jj // trick for cheap update // diff. Equation (9) w.r.t. λ Jj += X⊤ :,jdr ∥X:,j∥2 − neλ ∥X:,j∥2 sign ˆβj // O(n) dr−= X:,j(Jj,: −Jold) // O(n) if wLasso then Jold = Jj,: // trick for cheap update // diff. Equation (9) w.r.t. λ Jj,ˆS += 1 ∥X:,j∥2 X⊤ :,jdr // O(n×ˆs) Jj,j −= neλj ∥X:,j∥2 sign ˆβj // O(1) dr−= X:,j ⊗(Jj,: −Jold) // O(n×ˆs) return ˆβ,J vex, initialization plays a crucial role in the ﬁnal solution as well as the convergence of the algorithm. For instance, initializing λ = λinit in a ﬂat zone of L(λ) could lead to slow convergence. In the numerical experiments, the Lasso is initialized with λinit = λmax −log(10), where λmax is the smallest λsuch that 0 is a solution of Problem (2). Competitors. In this section we compare the empirical performance of implicit forward differentiation algorithm to different competitors. Competitors are divided in two categories. Firstly, the ones relying on hyperparameter gra- dient: • Imp. F. Iterdiff. : implicit forward differentiation (proposed) described in Algorithm 2. • Implicit: implicit differentiation, which requires solv- ing a ˆs×ˆslinear system as described in Algorithm 1. • F. Iterdiff.: forward differentiation (Deledalle et al., 2014; Franceschi et al., 2017) which jointly computes the regression coefﬁcients ˆβas well as the Jacobian ˆJ as shown in Algorithm 3. Secondly, the ones not based on hyperparameter gradient: • Grid-search: as recommended by Friedman et al. (2010), we use 100 values on a uniformly-spaced grid from λmax to λmax −4 log(10). • Random-search: we sample uniformly at random 100 values taken on the same interval as for the Grid-search [λmax −4 log(10);λmax], as suggested by Bergstra et al. (2013). • Lattice Hyp.: lattice hypercube sampling (Bousquet et al., 2017), combines the idea of grid-search and random-search. We used the sampling scheme of Bouhlel et al. (2019) and their code 2 to sample the points to evaluate the function on. • Bayesian: sequential model based optimization (SMBO) using a Gaussian process to model the objec- tive function. We used the implementation of Bergstra et al. (2013).3 The constraints space for the hyperpa- rameter search was set in[λmax −4 log(10);λmax], and the expected improvement (EI) was used as aquisition function. The cost and the quantity computed by each algorithm can be found in Table 1. The backward differentiation (Domke, 2012) is not included in the benchmark in Figure 2 since it was several orders of magnitude slower than the other techniques (see Figure 1). This is due to the high cost of the BCD algorithm in backward mode, see Table 1. 4.1. Application to held-out loss When using the held-out loss, each dataset(X,y) is split in 3 equal parts: the training set (Xtrain,ytrain), the validation set (Xval,yval) and the test set (Xtest,ytest). 2https://github.com/SMTorg/smt 3https://github.com/hyperopt/hyperoptImplicit differentiation of Lasso-type models for hyperparameter optimization (Lasso, held-out criterion). For the Lasso and the held-out loss, the bilevel optimization Problem (4) reads: arg min λ∈R ∥yval −Xval ˆβ(λ)∥2 (16) s.t. ˆβ(λ) ∈arg min β∈Rp 1 2n∥ytrain −Xtrainβ∥2 2 + eλ∥β∥1 . Figure 2 (top) shows on 3 datasets (see Appendix D for dataset details) the distance to the “optimum” of ∥yval − Xval ˆβ(λ)∥2 as a function of time. Here the goal is to ﬁnd λ solution of Problem (16). The “optimum” is chosen as the minimum of ∥yval −Xval ˆβ(λ)∥2 among all the meth- ods. Figure 2 (bottom) shows the loss ∥ytest −Xtest ˆβ(λ)∥2 on the test set (independent from the training set and the validation set). This illustrates how well the estimator gen- eralizes. Firstly, it can be seen that on all datasets the pro- posed implicit forward differentiation outperforms forward differentiation which illustrates Proposition 2 and corrobo- rates the cost of each algorithm in Table 1. Secondly, it can be seen that on the 20news dataset (Figure 2, top) the im- plicit differentiation (Algorithm 1) convergence is slower than implicit forward differentiation, forward differentia- tion, and even slower than the grid-search. In this case, this is due to the very slow convergence of the conjugate gra- dient algorithm (Nocedal and Wright, 2006) when solving the ill-conditioned linear system in Algorithm 1. (MCP , held-out criterion). We also applied our algorithm on an estimator based on a non-convex penalty: the MCP (Zhang, 2010) with 2 hyperparameters. Since the penalty is non-convex the estimator may not be continuous w.r.t.hy- perparameters and the theory developed above does not hold. However experimentally implicit forward differen- tiation outperforms forward differentiation for the HO, see Appendix C for full details. 4.2. Application to another criterion: SURE Evaluating models on held-out data makes sense if the de- sign is formed from random samples as it is often consid- ered in supervised learning. However, this assumption does not hold for certain kinds of applications in signal or image processing. For these applications, the held-out loss cannot be used as the criterion for optimizing the hyperparame- ters of a given model. In this case, one may use a proxy of the prediction risk, like the Stein Unbiased Risk Estimation (SURE, Stein (1981)). The SURE is an unbiased estimator of the prediction risk under weak differentiable conditions. The drawback of this criterion is that it requires the knowl- edge of the variance of the noise. The SURE is deﬁned as follows: SURE(λ) =∥y−Xˆβ(λ)∥2−nσ2+2σ2dof( ˆβ(λ)) , where the degrees of freedom (dof Efron 1986) is deﬁned as dof( ˆβ(λ)) =∑n i=1 cov(yi,(Xˆβ(λ))i)/σ2 .The dof can be seen a measure of the complexity of the model, for in- stance for the Lasso dof ( ˆβ(λ)) = ˆs, see Zou et al. (2007). The SURE can thus be seen as a criterion trading data- ﬁdelity against model complexity. However, the dof is not differentiable (not even continuous in the Lasso case), yet it is possible to construct a weakly differentiable ap- proximation of it based on Finite Differences Monte-Carlo (see Deledalle et al. 2014 for full details), with ϵ >0 and δ∼N(0,Idn): dofFDMC(y,λ,δ,ϵ ) =1 ϵ⟨Xˆβ(λ)(y+ ϵδ) −Xˆβ(λ)(y),δ⟩. We use this smooth approximation in the bi-level optimiza- tion problem to ﬁnd the best hyperparameter. The bi-level optimization problem then reads: arg min λ∈R ∥y−Xˆβ(λ)∥2 + 2σ2dofFDMC(y,λ,δ,ϵ ) (17) s.t. ˆβ(λ)(y) ∈arg min β∈Rp 1 2n∥y−Xβ∥2 2 + eλ∥β∥1 ˆβ(λ)(y+ ϵδ) ∈arg min β∈Rp 1 2n∥y+ ϵδ−Xβ∥2 2 + eλ∥β∥1 Note that solving this problem requires the computation of two (instead of one for the held-out loss) Jacobians w.r.t.λ of the solution ˆβ(λ) at the points yand y+ ϵδ. (Lasso, SURE criterion). To investigate the estimation per- formance of the implicit forward differentiation in com- parison to the competitors described above, we used as metric the (normalized) Mean Squared Error (MSE) de- ﬁned as MSE ≜ ∥ˆβ−β∗∥2/∥β∗∥2. The entries of the design matrix X ∈Rn×p are i.i.d. random Gaussian vari- ables N(0,1). The number of rows is ﬁxed to n = 100. Then, we generated β∗with 5 non-zero coefﬁcients equals to 1. The vector y was computed by adding to Xβ∗addi- tive Gaussian noise controlled by the Signal-to-Noise Ra- tio: SNR ≜ ∥Xβ∗∥/∥y−Xβ∗∥(here SNR = 3). Fol- lowing Deledalle et al. (2014), we set ϵ = 2σ/n0.3. We varied the number of featurespbetween 200 and 10,000 on a linear grid of size 10. For a ﬁxed number of features, we performed 50 repetitions and each point of the curves rep- resents the mean of these repetitions. Comparing efﬁciency in time between methods is difﬁcult since they are not di- rectly comparable. Indeed, grid-search and random-search discretize the HO space whereas others methods work in the continuous space which is already an advantage. How- ever, to be able to compare the hypergradient methods and possibly compare them to the others, we computed the to- tal amount of time for a method to return its optimal value of λ. In order to have a fair comparison, we compared 50 evaluations of the line-search for each hypergradient meth- ods, 50 evaluations of the Bayesian methods and ﬁnally 50 evaluations on ﬁxed or random grid. We are aware that the cost of each of these evaluations is not the same but it al- lows to see that our method stays competitive in time with optimizing one parameter. Moreover we will also see that our method scales better with a large number of hyperpa- rameters to optimize.Implicit differentiation of Lasso-type models for hyperparameter optimization Imp. F. Iterdiﬀ. (ours) Implicit F. Iterdiﬀ. Grid-search Bayesian Random-search Lattice Hyp. 0.0 0.5 1.0 10−5 10−4 10−3 10−2 10−1 100 Objective minus optimum rcv1 (p = 19, 959) 0 5 10 15 10−3 10−2 10−1 100 101 102 20news (p = 130, 107) 0 100 200 300 10−4 10−3 10−2 10−1 100 101 ﬁnance (p = 1, 668, 737) 0.0 0.5 1.0 Time (s) 10−1 100 Loss on test set 0 5 10 15 Time (s) 101 102 0 100 200 300 Time (s) 10−1 100 101 Figure 2.Computation time for the HO of the Lasso on real data.Distance to “optimum” (top) and performance (bottom) on the test set for the Lasso for 3 different datasets: rcv1, 20news and ﬁnance. Imp. F. Iterdiﬀ. (ours) Implicit F. Iterdiﬀ. Grid-search Bayesian Random-search 200 2500 5000 7500 10000 Number of features (p) 0.000 0.001 0.002 0.003 0.004 relative MSE 200 2500 5000 7500 10000 Number of features (p) 10−1 100 101 102 Time (s) Figure 3.Lasso: estimation performance. Estimation relative Mean Squared Error (left) and running time (right) as a function of the number of features for the Lasso model. Figure 3 shows the inﬂuence of the number of features on the relative MSE (ie. MSE of a method minus the MSE of our implicit forward method) and the computation time. First, MSE of all gradient based methods is lower than the other methods which means that ˆβ(λ) leads to a better es- timation when λ is chosen via the gradient based meth- ods. This illustrates that continuous optimization for hy- perparameter selection leads to better estimation perfor- mance than discrete or Bayesian optimization. Yet, the running time of our proposed method is the lowest of all hypergradient-based strategies and competes with the grid- search and the random-search. (Weighted Lasso vs Lasso, SURE criterion). As our method leverages the sparsity of the solution, it can be used for HO with a large number of hyperparameters, contrary to classi- cal forward differentiation. The weighted Lasso (wLasso, Zou 2006) has p hyperparameters and was introduced to reduce the bias of the Lasso. However setting the phyper- parameters is impossible with grid-search. Figure 4 shows the estimation MSE and the running time of the different methods to obtain the hyperparameter val- ues as a function of the number of features used to simu- late the data. The simulation setting is here the same as for the Lasso problems investigated in Figure 3 ( n = 100, SNR = 3). We compared the classical Lasso estimator and the weighted Lasso estimator where the regularization hy- perparameter was chosen using implicit forward differenti- ation and the forward iterative differentiation as described in Algorithm 3. Problem (4) is not convex for the weighted Lasso and a descent algorithm like ours can be trapped in local minima, crucially depending on the starting point λinit. To alleviate this problem, we introduced a regular- ized version of Problem (4): arg min λ∈R C ( ˆβ(λ) ) + γ p∑ j λ2 j s.t. ˆβ(λ) ∈arg min β∈Rp ≜ ψ(β,λ) . (18) The solution obtained by solving Equation (18) is then used as the initialization λ(0) for our algorithm. In this experiment the regularization term is constant γ =Implicit differentiation of Lasso-type models for hyperparameter optimization Lasso F. Iterdiﬀ. Lasso Implicit Lasso Backward Lasso Imp. F. Iterdiﬀ. (ours) wLasso F. Iterdiﬀ. wLasso Implicit wLasso Backward wLasso Imp. F. Iterdiﬀ. (ours) 200 2500 5000 7500 10000 Number of features (p) 0.00 0.05 0.10 0.15 MSE 200 2500 5000 7500 10000 Number of features (p) 10−1 100 101 102 103 Time (s) Figure 4.Lasso vs wLasso.Estimation Mean Squared Error (left) and running (right) of competitors as a function of the number of features for the weighted Lasso and Lasso models. C(β(λmax))/10. We see in Figure 4 that the weighted Lasso gives a lower MSE than the Lasso and allows for a better recovery of β∗. This experiment shows that the amount of time needed to obtain the vector of hyperparameters of the weighted Lasso via our algorithm is in the same range as for obtaining the unique hyperparameter of the Lasso prob- lem. It also shows that our proposed method is much faster than the naive way of computing the Jacobian using for- ward or backward iterative differentiation. The implicit dif- ferentiation method stays competitive for the wLasso due to the small support of the solution and hence a small ma- trix to inverse. A maximum running time threshold was used for this experiment checking the running time at each line-search iteration, explaining why the forward differen- tiation and backward differentiation of the wLasso does not explode in time on Figure 4. Conclusion In this work we studied the performance of several methods to select hyperparameters of Lasso-type estimators show- ing results for the Lasso and the weighted Lasso, which have respectively one or phyperparameters. We exploited the sparsity of the solutions and the speciﬁc structure of the iterates of forward differentiation, leading to our im- plicit forward differentiation algorithm that computes efﬁ- ciently the full Jacobian of these estimatorsw.r.t.the hyper- parameters. This allowed us to select them through a stan- dard gradient descent and have an approach that scales to a high number of hyperparameters. Importantly, contrary to a classical implicit differentiation approach, the proposed algorithm does not require solving a linear system. Fi- nally, thanks to its two steps nature, it is possible to lever- age in the ﬁrst step the availability of state-of-the-art Lasso solvers that make use of techniques such as active sets or screening rules. Such algorithms, that involve calls to in- ner solvers run on subsets of features, are discontinuous w.r.t.hyperparameters which would signiﬁcantly challenge a single step approach based on automatic differentiation. Acknowledgments This work was funded by ERC Start- ing Grant SLAB ERC-StG-676943 and ANR GraVa ANR- 18-CE40-0005.Implicit differentiation of Lasso-type models for hyperparameter optimization References A. Agrawal, B. Amos, S. Barratt, S. Boyd, S. Diamond, and J. Z. Kolter. Differentiable convex optimization layers. In Advances in neural information processing systems , pages 9558–9570, 2019. B. Amos and J. Z. Kolter. Optnet: Differentiable optimiza- tion as a layer in neural networks. In ICML, volume 70, pages 136–145, 2017. A. G. Baydin, B. A. Pearlmutter, A. A. Radul, and J. M. Siskind. Automatic differentiation in machine learning: a survey. J. Mach. Learn. Res., 18(153):1–43, 2018. Y . Bengio. Gradient-based optimization of hyperparame- ters. Neural computation, 12(8):1889–1900, 2000. J. Bergstra and Y . Bengio. Random search for hyper- parameter optimization. J. Mach. Learn. Res., 2012. J. Bergstra, D. Yamins, and D. D. Cox. Hyperopt: A python library for optimizing the hyperparameters of machine learning algorithms. In Proceedings of the 12th Python in science conference, pages 13–20, 2013. M. Borgerding, P. Schniter, and S. Rangan. Amp-inspired deep networks for sparse linear inverse problems. IEEE Transactions on Signal Processing , 65(16):4293–4308, 2017. M. A. Bouhlel, J. T. Hwang, N. Bartoli, R. Lafage, J. Mor- lier, and J. R. R. A. Martins. A python surrogate model- ing framework with derivatives. Advances in Engineer- ing Software, page 102662, 2019. ISSN 0965-9978. doi: https://doi.org/10.1016/j.advengsoft.2019.03.005. O. Bousquet, S. Gelly, K. Kurach, O. Teytaud, and D. Vin- cent. Critical hyper-parameters: No random, no cry. arXiv preprint arXiv:1706.03200, 2017. P. Breheny and J. Huang. Coordinate descent algorithms for nonconvex penalized regression, with applications to biological feature selection. Ann. Appl. Stat. , 5(1):232, 2011. E. Brochu, V . M. Cora, and N. De Freitas. A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical re- inforcement learning. 2010. O. Chapelle, V . Vapnik, O. Bousquet, and S. Mukherjee. Choosing multiple parameters for support vector ma- chines. Machine learning, 46(1-3):131–159, 2002. P. L. Combettes and V . R. Wajs. Signal recovery by proxi- mal forward-backward splitting. Multiscale Modeling & Simulation, 4(4):1168–1200, 2005. I. Daubechies, M. Defrise, and C. De Mol. An iterative thresholding algorithm for linear inverse problems with a sparsity constraint. Comm. Pure Appl. Math., 57(11): 1413–1457, 2004. C.-A. Deledalle, S. Vaiter, J. Fadili, and G. Peyré. Stein Unbiased GrAdient estimator of the Risk (SUGAR) for multiple parameter selection. SIAM J. Imaging Sci. , 7 (4):2448–2487, 2014. J. Domke. Generic methods for optimization-based model- ing. In AISTATS, volume 22, pages 318–326, 2012. C. Dossal, M. Kachour, M.J. Fadili, G. Peyré, and C. Ches- neau. The degrees of freedom of the lasso for general design matrix. Statistica Sinica, 23(2):809–828, 2013. B. Efron. How biased is the apparent error rate of a pre- diction rule? J. Amer. Statist. Assoc., 81(394):461–470, 1986. L. C. Evans and R. F. Gariepy. Measure theory and ﬁne properties of functions. CRC Press, 1992. C. S. Foo, C. B. Do, and A. Y . Ng. Efﬁcient multiple hyper- parameter learning for log-linear models. InAdvances in neural information processing systems, pages 377–384, 2008. L. Franceschi, M. Donini, P. Frasconi, and M. Pontil. For- ward and reverse gradient-based hyperparameter opti- mization. In ICML, pages 1165–1173, 2017. J. Frecon, S. Salzo, and M. Pontil. Bilevel learning of the group lasso structure. InAdvances in Neural Information Processing Systems, pages 8301–8311, 2018. J. Friedman, T. J. Hastie, and R. Tibshirani. Regulariza- tion paths for generalized linear models via coordinate descent. J. Stat. Softw., 33(1):1–22, 2010. K. Gregor and Y . LeCun. Learning fast approximations of sparse coding. In ICML, pages 399–406, 2010. E. Hale, W. Yin, and Y . Zhang. Fixed-point continuation for ℓ1-minimization: Methodology and convergence. SIAM J. Optim., 19(3):1107–1130, 2008. K. Kunisch and T. Pock. A bilevel optimization approach for parameter learning in variational models. SIAM J. Imaging Sci., 6(2):938–983, 2013. S. K. Lam, A. Pitrou, and S. Seibert. Numba: A LLVM- based Python JIT Compiler. In Proceedings of the Sec- ond Workshop on the LLVM Compiler Infrastructure in HPC, pages 1–6. ACM, 2015.Implicit differentiation of Lasso-type models for hyperparameter optimization J. Larsen, L. K. Hansen, C. Svarer, and M. Ohlsson. Design and regularization of neural networks: the optimal use of a validation set. In Neural Networks for Signal Process- ing VI. Proceedings of the 1996 IEEE Signal Processing Society Workshop, 1996. J. Larsen, C. Svarer, L. N. Andersen, and L. K. Hansen. Adaptive regularization in neural network modeling. In Neural Networks: Tricks of the Trade - Second Edition , pages 111–130. Springer, 2012. J. Liu, X. Chen, Z. Wang, and W. Yin. Alista: Analytic weights are as good as learned weights in lista. In Inter- national Conference on Learning Representations, 2018. W. Liu, Y . Yang, et al. Parametric or nonparametric? a parametricness index for model selection. Ann. Statist., 39(4):2074–2102, 2011. J. Lorraine, P. Vicol, and D. Duvenaud. Optimizing millions of hyperparameters by implicit differentiation. arXiv preprint arXiv:1911.02590, 2019. D. Maclaurin, D. Duvenaud, and Ryan Adams. Gradient- based hyperparameter optimization through reversible learning. In ICML, volume 37, pages 2113–2122, 2015. J. Mairal, F. Bach, and J. Ponce. Task-driven dictionary learning. IEEE Trans. Pattern Anal. Mach. Intell., 34(4): 791–804, 2012. M. Massias, A. Gramfort, and J. Salmon. Celer: a Fast Solver for the Lasso with Dual Extrapolation. In ICML, volume 80, pages 3315–3324, 2018. M. Massias, S. Vaiter, A. Gramfort, and J. Salmon. Dual extrapolation for sparse generalized linear models.arXiv preprint arXiv:1907.05830, 2019. V . Niculae and M. Blondel. A regularized framework for sparse and structured neural attention. In Advances in neural information processing systems , pages 3338– 3348, 2017. J. Nocedal and S. J. Wright. Numerical optimization . Springer Series in Operations Research and Financial Engineering. Springer, New York, second edition, 2006. F. Pedregosa. Hyperparameter optimization with approxi- mate gradient. In ICML, 2016. S. Ramani, T. Blu, and M. Unser. Monte-Carlo SURE: a black-box optimization of regularization parameters for general denoising algorithms. IEEE Trans. Image Pro- cess., 17(9):1540–1554, 2008. M. W. Seeger. Cross-validation optimization for large scale structured classiﬁcation kernel methods. J. Mach. Learn. Res., 9:1147–1178, 2008. J. Snoek, H. Larochelle, and R. P. Adams. Practical bayesian optimization of machine learning algorithms. In Advances in neural information processing systems , 2012. E. Soubies, L. Blanc-Féraud, and G. Aubert. A uniﬁed view of exact continuous penalties for ℓ2-ℓ0 minimiza- tion. SIAM J. Optim., 27(3):2034–2060, 2017. C. M. Stein. Estimation of the mean of a multivariate nor- mal distribution. Ann. Statist., 9(6):1135–1151, 1981. L. R. A. Stone and J.C. Ramer. Estimating W AIS IQ from Shipley Scale scores: Another cross-validation. Journal of clinical psychology, 21(3):297–297, 1965. Y . Sun, H. Jeong, J. Nutini, and M. Schmidt. Are we there yet? manifold identiﬁcation of gradient-related proxi- mal methods. In AISTATS, volume 89, pages 1110–1119, 2019. R. Tibshirani. Regression shrinkage and selection via the lasso. J. R. Stat. Soc. Ser. B Stat. Methodol., 58(1):267– 288, 1996. R. J. Tibshirani. The lasso problem and uniqueness. Elec- tron. J. Stat., 7:1456–1490, 2013. R. J. Tibshirani and J. Taylor. The solution path of the generalized lasso. Ann. Statist., 39(3):1335–1371, 2011. P. Tseng and S. Yun. Block-coordinate gradient descent method for linearly constrained nonsmooth separable optimization. J. Optim. Theory Appl., 140(3):513, 2009. S. Vaiter, C.-A. Deledalle, G. Peyré, C. Dossal, and J. Fadili. Local behavior of sparse analysis regulariza- tion: Applications to risk estimation. Appl. Comput. Harmon. Anal., 35(3):433–451, 2013. S. Vaiter, C.-A. Deledalle, G. Peyré, J. M. Fadili, and C. Dossal. The degrees of freedom of partly smooth reg- ularizers. Ann. Inst. Stat. Math., 69(4):791–832, 2017. K. Wu, Y . Guo, Z. Li, and C. Zhang. Sparse coding with gated learned ista. In International Conference on Learning Representations, 2019. B. Xin, Y . Wang, W. Gao, D. Wipf, and B. Wang. Maximal sparsity with deep networks? In Advances in Neural In- formation Processing Systems, pages 4340–4348, 2016. M. Yuan and Y . Lin. Model selection and estimation in regression with grouped variables. J. R. Stat. Soc. Ser. B Stat. Methodol., 68(1):49–67, 2006. C.-H. Zhang. Nearly unbiased variable selection under minimax concave penalty. Ann. Statist., 38(2):894–942, 2010.Implicit differentiation of Lasso-type models for hyperparameter optimization H. Zou. The adaptive lasso and its oracle properties. J. Amer. Statist. Assoc., 101(476):1418–1429, 2006. H. Zou and T. J. Hastie. Regularization and variable se- lection via the elastic net. J. R. Stat. Soc. Ser. B Stat. Methodol., 67(2):301–320, 2005. H. Zou, T. J. Hastie, and R. Tibshirani. On the “degrees of freedom” of the lasso. Ann. Statist., 35(5):2173–2192, 2007.Implicit differentiation of Lasso-type models for hyperparameter optimization A. Proofs A.1. Proof of Proposition 1 We start by a lemma on the weak derivative of the soft-thresholding. Lemma A.1. The soft-thresholding ST :R×R+ ↦→R deﬁned by ST(t,τ) = sign(t) ·(|t|−τ)+ is weakly differentiable with weak derivatives ∂1 ST(t,τ) =1{|t|>τ} , (19) and ∂2 ST(t,τ) =−sign(t) ·1{|t|>τ} , (20) where 1{|t|>τ}= { 1, if |t|>τ, 0, otherwise. (21) Proof. See (Deledalle et al., 2014, Proposition 1) Proof. (Proposition 1, Lasso ISTA) The soft-thresholding is differentiable almost everywhere (a.e.), thus Equation (10) can be differentiated a.e. thanks to the previous lemma, and for any α> 0 ˆJ=   1{|ˆβ1|>0} ... 1{|ˆβp|>0}  ⊙ ( Idp−1 αX⊤X ) ˆJ− neλ α   sign( ˆβ1)1{|ˆβ1|>0} ... sign( ˆβp)1{|ˆβp|>0}   . Inspecting coordinates inside and outside the support of ˆβleads to: { ˆJˆSc = 0 ˆJˆS = ˆJˆS −1 αX⊤ :,ˆSX:,ˆS ˆJˆS −neλ α sign ˆβˆS . (22) Rearranging the term of Equation (22) it yields: X⊤ :,ˆSX:,ˆS ˆJˆS = −neλsign ˆβˆS (23) ˆJˆS = −neλ ( X⊤ :,ˆSX:,ˆS )−1 sign ˆβˆS . (24) (Proposition 1, Lasso BCD) The ﬁxed point equations for the BCD case is ˆβj = ST ( ˆβj − 1 ∥X:j∥2 2 X⊤ :j(Xˆβj −y), neλ ∥X:j∥2 2 ) . (25) As before we can differentiate this ﬁxed point equation Equation (25) ˆJj = 1{|ˆβj|>τ}· ( ˆJj − 1 ∥X:j∥2 2 X⊤ :jX ˆJ ) − neλ ∥X:j∥2 2 sign (ˆβj)1{|ˆβj|>τ} , (26) leading to the same result.Implicit differentiation of Lasso-type models for hyperparameter optimization A.2. Proof of Proposition 2 in the ISTA case Proof. (Lasso case, ISTA) In Algorithm 3, β(k) follows ISTA steps, thus (β(k))l∈N converges toward the solution of the Lasso ˆβ. Let ˆS be the support of the Lasso estimator ˆβ, and ν( ˆS) > 0 the smallest eigenvalue of X⊤ :,ˆSX:,ˆS. Under uniqueness assumption proximal gradient descent ( a.k.a. ISTA) achieves sign identiﬁcation (Hale et al., 2008), i.e., there exists k0 ∈N such that for all k≥k0 −1: sign β(k+1) = signˆβ . (27) Recalling the update of the Jacobian Jfor the Lasso solved with ISTA is the following: J(k+1) = ⏐⏐⏐sign β(k+1) ⏐⏐⏐⊙ ( Id − 1 ∥X∥2 2 X⊤X ) J(k) − neλ ∥X∥2 2 sign β(k+1) , it is clear that J(k) is sparse with the sparsity pattern β(k) for all k≥k0. Thus we have that for all k≥k0: J(k+1) ˆS = J(k) ˆS − 1 ∥X∥2 2 X⊤ :,ˆSXJ(k) − neλ ∥X∥2 2 sign ˆβˆS = J(k) ˆS − 1 ∥X∥2 2 X⊤ :,ˆSX:,ˆSJ(k) ˆS − neλ ∥X∥2 2 sign ˆβˆS = ( IdˆS− 1 ∥X∥2 2 X⊤ :,ˆSX:,ˆS ) J(k) ˆS − neλ ∥X∥2 2 sign ˆβˆS. (28) One can remark that ˆJdeﬁned in Equation (11), satisﬁes the following: ˆJˆS = ( IdˆS− 1 ∥X∥2 2 X⊤ :,ˆSX:,ˆS ) ˆJˆS − neλ ∥X∥2 2 sign ˆβˆS . (29) Combining Equations (28) and (29) and denoting ν( ˆS) >0 the smallest eigenvalue of X⊤ ˆSXˆS, we have for all k≥k0: J(k+1) ˆS − ˆJˆS = ( IdˆS− 1 ∥X∥2 2 X⊤ :,ˆSX:,ˆS )( J(k) ˆS − ˆJˆS ) ∥J(k+1) ˆS − ˆJˆS∥2 ≤ ( 1 − ν( ˆS) ∥X∥2 2 ) ∥J(k) ˆS − ˆJˆS∥2 ∥J(k) ˆS − ˆJˆS∥2 ≤ ( 1 − ν( ˆS) ∥X∥2 2 )k−k0 ∥J(k0) ˆS − ˆJˆS∥2 . Thus the sequence of Jacobian ( J(k)) k∈N converges linearly to ˆJonce the support is identiﬁed. Proof. (wLasso case, ISTA) Recalling the update of the Jacobian J ∈Rp×p for the wLasso solved with ISTA is the following: J(k+1) = ⏐⏐⏐sign β(k+1) ⏐⏐⏐⊙ ( Id − 1 ∥X∥2 2 X⊤X ) J(k) − neλ ∥X∥2 2 diag ( sign β(k+1) ) , (30) The proof follows exactly the same steps as the ISTA Lasso case to show convergence in spectral norm of the sequence (J(k))k∈N toward ˆJ.Implicit differentiation of Lasso-type models for hyperparameter optimization A.3. Proof of Proposition 2 in the BCD case The goal of the proof is to show that iterations of the Jacobian sequence (J(k))k∈N generated by the Block Coordinate Descent algorithm (Algorithm 3) converges toward the true Jacobian ˆJ. The main difﬁculty of the proof is to show that the Jacobian sequence follows a Vector AutoRegressive (V AR, see Massias et al. (2019, Thm. 10) for more detail),i.e., the main difﬁculty is to show that there exists k0 such that for all k≥k0: J(k+1) = AJ(k) + B , (31) with A∈Rp×p a contracting operator and B ∈Rp. We follow exactly the proof of Massias et al. (2019, Thm. 10). Proof. (Lasso, BCD, forward differentiation (Algorithm 3)) Let j1,...,j S be the indices of the support of ˆβ, in increasing order. As the sign is identiﬁed, coefﬁcients outside the support are 0 and remain 0. We decompose the k-th epoch of coordinate descent into individual coordinate updates: Let ˜β(0) ∈Rp denote the initialization (i.e., the beginning of the epoch, ), ˜β(1) = β(k) the iterate after coordinate j1 has been updated, etc., up to ˜β(S) after coordinate jS has been updated, i.e., at the end of the epoch ( ˜β(S) = β(k+1)). Let s ∈S, then ˜β(s) and ˜β(s−1) are equal everywhere, except at coordinate js: ˜J(s) js = ˜J(s−1) js − 1 ∥X:,js∥2 X⊤ :,jsX ˜J(s−1) − 1 ∥Xjs∥2 sign βjs after sign identiﬁcation we have: = ˜J(s−1) js − 1 ∥X:,js∥2 X⊤ :,jsX:,ˆS ˜J(s−1) ˆS − 1 ∥X:,js∥2 sign ˆβjs ˜J(s) ˆS = ( Idˆs− 1 ∥X:,js∥2 ejse⊤ jsX⊤ :,ˆSX:,ˆS )    As ˜J(s−1) ˆS − 1 ∥X:,js∥2 sign ˆβjs ( X⊤ :,ˆSX:,ˆS )1/2 ˜J(s) ˆS =  Idˆs− ( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥ ejse⊤ js ( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥      A(s) ( X⊤ :,ˆSX:,ˆS )1/2 ˜J(s−1) ˆS − ( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥2 sign ˆβjs    b(s) We thus have: ( X⊤ :,ˆSX:,ˆS )1/2 ˜J(ˆs) ˆS = A(ˆs) ...A (1)    A∈Rˆs×ˆs ( X⊤ :,ˆSX:,ˆS )1/2 J(1) ˆS + AS...A 2b1 + ··· + ASbS−1 + bS   b∈Rˆs . After sign identiﬁcation and a full update of coordinate descent we thus have: ( X⊤ :,ˆSX:,ˆS )1/2 J(t+1) ˆS = A ( X⊤ :,ˆSX:,ˆS )1/2 J(t) ˆS + b . (32) Lemma A.2. ∥As∥2 ≤1 , Moreover if A(s)x = ∥x∥then x∈vect   ( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥ ejs   ⊤ (33) Proof. ( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥ ejse⊤ js ( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥Implicit differentiation of Lasso-type models for hyperparameter optimization is a symmetric rank 1 matrix, its non-zero eigenvalue is e⊤ js ( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥ ( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥ ejs = e⊤ js X⊤ :,ˆSX:,ˆS ∥X:,js∥2 ejs = 1 . An eigenvector associated to this non-zeros eigenvalue is ( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥ ejs . Asis symmetric and real, is diagonalisable in an orthogonal basis, it has eigenvalue1 with multiplicity ˆs−1 and eigenvalue 0 with multiplicity 1. Moreover if ∥Ax∥= ∥x∥, then x∈vect (( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥ ejs )⊤ . Lemma A.3. ∥A∥2 <1 . Proof. A= A(ˆs) ...A (1) We have ∥A∥≤∥ A(ˆs)∥   ≤1 ... ∥A(1)∥   ≤1 ≤1 . Let x∈Rˆs such that ∥Ax∥= ∥x∥, we thus have for all s∈1,..., ˆs, A(s)x = ∥x∥. Using Lemma A.3 we have that for all s∈1,..., ˆsx ∈vect (( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥ ejs )⊤ , i.e., x∈vect (( X⊤ :,ˆSX:,ˆS )1/2)⊤ = {0}because X⊤ :,ˆSX:,ˆS ≻0 Using Equation (32) we have: ∥J(t+1) ˆS − ˆJ∥(X⊤ :,ˆSX:,ˆS)−1 ≤∥A∥2∥J(t) ˆS − ˆJ∥(X⊤ :,ˆSX:,ˆS)−1 , (34) with ∥A∥2 < 1, which leads to the desire result. Since the recursion of the Jacobian sequences of Algorithm 2 and Algorithm 2 are the same once the support is identiﬁed, the proof of convergence of Algorithm 2 is the same (provided that support identiﬁcation has been achieved). Proof. (wLasso case, BCD) As for the Lasso case: ˜J(s) js,: = ˜J(s−1) js,: − 1 ∥X:,js∥2 X⊤ :,jsX ˜J(s−1) − 1 ∥Xjs∥2 sign βjsejse⊤ js after sign identiﬁcation we have: ˜J(s) js,ˆS = ˜J(s−1) js,ˆS − 1 ∥X:,js∥2 X⊤ :,jsX:,ˆS ˜J(s−1) ˆS,ˆS − 1 ∥X:,js∥2 sign ˆβjsejse⊤ js (X⊤ :,ˆSX:,ˆS)1/2 ˜J(s) ˆS,ˆS = ( Idn− (X⊤ :,ˆSX:,ˆS)1/2ejse⊤ js(X⊤ :,ˆSX:,ˆS)1/2 ∥X:,js∥2 )    A(s) (X⊤ :,ˆSX:,ˆS)1/2 ˜J(s−1) ˆS,ˆS −sign ˆβjs ∥X:,js∥2 (X⊤ :,ˆSX:,ˆS)1/2    B(s) ejse⊤ js (X⊤ :,ˆSX:,ˆS)1/2 ˜J(ˆs) ˆS,ˆS = A(ˆs) ...A (1)    A∈Rˆs×ˆs (X⊤ :,ˆSX:,ˆS)1/2 ˜J(0) ˆS,ˆS + A(ˆs) ...A (2)B(1)ej1 e⊤ j1 + ··· + B(ˆs)ejˆse⊤ jˆs    D∈Rˆs×ˆs . (35) As in the Lasso case, Equation (35) leads to linear convergence once the support is identiﬁed for Algorithms 2 and 3.Implicit differentiation of Lasso-type models for hyperparameter optimization B. Block coordinate descent algorithms Algorithm 3 presents the forward iteration scheme which computes iteratively the solution of the Lasso or wLasso jointly with the Jacobian computation. This is the naive way of computing the Jacobian without taking advantage of its sparsity. Eventually, it requires to differentiate every lines of code w.r.t. to λ and take advantage of the BCD updates for cheap updates on the Jacobian as well. Algorithm 3FORWARD ITERDIFF (Deledalle et al., 2014; Franceschi et al., 2017) input : X ∈Rn×p,y ∈Rn,λ ∈R,niter ∈N // jointly compute coef. & Jacobian β = 0 // potentially warm started J= 0 // potentially warm started r= y−Xβ dr= −XJ for k= 0,...,n iter −1 do for j = 0,...,p −1 do // update the regression coefficients βold = βj zj = βj + 1 ∥X:,j∥2 X⊤ :,jr // gradient step βj = ST(zj,neλ/∥X:,j∥2) // proximal step r−= X:,j(βj −βold) // update the Jacobian if Lasso then Jold = Jj Jj = |sign βj| ( Jj + 1 ∥X:,j∥2 X⊤ :,jdr ) // diff. w.r.t. λ Jj −= neλ ∥X:,j∥2 sign βj // diff. w.r.t. λ drj −= X:,j(Jj −Jold) if wLasso then Jold = Jj,: Jj,: = |sign βj| ( Jj,: + 1 ∥X:,j∥2 X⊤ :,jdr ) // diff. w.r.t. λ1,...,λ p Jj,j −= neλj ∥X:,j∥2 sign βj // diff. w.r.t. λ1,...,λ p dr−= X:,j(Jj −Jold) return βniter ,Jniter (λ) Algorithm 4 describes the backward iterative differentiation algorithm used for benchmark. Backward differentiation requires the storage of every updates on β. As Figure 1 shows, this algorithm is not efﬁcient for our case because the function to differentiate f : R →Rp ( f : Rp →Rp, for the wLasso) has a higher dimension output space than the input space. The storage is also an issue mainly for the wLasso case which makes this algorithm difﬁcult to use in practice in our context. Algorithm 5 presents the classical BCD iterative scheme for solving the Lasso problem using the composition of a gradient step with the soft-thresholding operator.Implicit differentiation of Lasso-type models for hyperparameter optimization Algorithm 4BACKWARD ITERDIFF (Domke, 2012) input : X ∈Rn×p,y ∈Rn,λ ∈R,niter ∈N // backward computation of ˆβ and ˆJ⊤ (λ)α β = 0 // potentially warm started // compute the regression coefficients and store the iterates for k= 0,...,n iter −1 do for j = 0,...,p −1 do βold = βj zj = βj + 1 ∥X:,j∥2 X⊤ :,jr // gradient step βj = ST(zj,neλ/∥X:,j∥2) // proximal step r−= X:,j(βj −βold) // Init. backward differentiation g= 0 // g stores ˆJ⊤ λ α // compute the Jacobian for k= niter down to 1 do for j = 0,...,p −1 do if Lasso then g−= neλ ∥X:,j∥2 αjsign β(k) j αj ∗= |sign β(k) j | α−= 1 ∥X:,j∥2 αjX⊤ :,jX // O(np) if wLasso then gj −= neλj ∥X:,j∥2 αjsign β(k) j αj ∗= |sign β(k) j | α−= 1 ∥X:,j∥2 αjX⊤ :,jX return βniter ,g(1) Algorithm 5BCD FOR THE LASSO (Friedman et al., 2010) input : X ∈Rn×p,y ∈Rn,λ ∈R,β(0) ∈Rp,niter ∈N β = β(0) // warm start for k= 0,...,n iter −1 do for j = 0,...,p −1 do βold = βj zj = βj + 1 ∥X:,j∥2 X⊤ :,jr // gradient step βj = ST(zj,neλ/∥X:,j∥2) // proximal step r−= X:,j(βj −βold) return βniterImplicit differentiation of Lasso-type models for hyperparameter optimization C. Derivations for MCP Let us remind the deﬁnition of the Minimax Concave Penalty (MCP) estimator introduced by Zhang (2010), also analyzed under the name CELE0 by Soubies et al. (2017). First of all, for any t∈R: pMCP λ,γ (t) = { λ|t|− t2 2γ, if |t|≤ γλ 1 2 γλ2, if |t|>γλ . (36) The proximity operator of pλ,γ for parameters λ >0 and γ >1 is deﬁned as follow (see Breheny and Huang 2011, Sec. 2.1): proxMCP λ,γ (t) = {ST(t,λ) 1−1 γ if |t|≤ γλ t if |t|>γλ . (37) For ourselves we choose as for the Lasso an exponential parametrization of the coefﬁcients, for λ∈R and γ >0: ˆβ(λ,γ)(y) ≜ arg min β∈Rp 1 2n∥y−Xβ∥2 2 + p∑ j=1 pMCP eλ,eγ (|βj|) . (38) Update rule for Coordinate Descent Below, we provide equation to update the coefﬁcient in the coordinate descent algorithm of the MCP: βj ←arg min βj∈R 1 2n∥y−βjX:,j − ∑ j′̸=j βj′X:,j′∥2 2 + p∑ j′̸=j pMCP eλ,eγ(βj′) +pMCP eλ,eγ(βj) = arg min βj∈R 1 2n∥y−βjX:,j − ∑ j′̸=j βj′X:,j′∥2 2 + pMCP eλ,eγ(βj) = arg min βj∈R ∥X:,j∥2 2   1 2n  βj − 1 ∥X:,j∥2 2 ⟨ y− ∑ j′̸=j βj′X:,j′,X:,j ⟩  2 + 1 ∥X:,j∥2 2 pMCP eλ,eγ(βj)   = arg min βj∈R   1 2n  βj − 1 ∥X:,j∥2 2 ⟨ y− ∑ j′̸=j βj′X:,j′,X:,j ⟩  2 + 1 ∥X:,j∥2 2 pMCP eλ,eγ(βj)   = arg min βj∈R   1 2Lj  βj − 1 ∥X:,j∥2 2 ⟨ y− ∑ j′̸=j βj′X:,j′,X:,j ⟩  2 + pMCP eλ,eγ(βj)  ,with Lj ≜ n ∥X:,j∥2 2 = proxMCP eλ/Lj,eγLj ( βj − 1X2 :,j X⊤ :,j(Xβ −y),λ ) . (39) One can write the following ﬁxed point equation satisﬁed by the estimator ˆβ, with Lj = ∥X:,j∥2 /n: ˆβj = proxMCP eλ/Lj,eγLj   ⟨ y− ∑ k̸=j ˆβkX:,k, X:,j ∥X:,j∥2 ⟩  = proxMCP eλ/Lj,eγLj ( ˆβj − 1 ∥X:,j∥2 X⊤ :,j ( Xˆβ−y )) . (40) Since the MCP penalty is non-convex, the estimator may not be continuous w.r.t. hyperparameters and gradient based hyperparameter optimization may not be theoretically justiﬁed. However we can differentiate the ﬁxed point equationImplicit differentiation of Lasso-type models for hyperparameter optimization Imp. F. iterdiﬀ. (ours) F. iterdiﬀ. Grid-search 0 2 4 10−4 10−3 10−2 10−1 100 Objective minus optimum rcv1 (p=19,959) 0 10 20 30 10−2 10−1 100 101 102  20news (p=130,107) 0 2 4 Time (s) 10−1 100 Loss on test set 0 10 20 30 Time (s) 101 102 Figure 5.Computation time for the HO of the MCP on real dataDistance to “optimum” (top) and performance (bottom) on the test set for the MCP. Equation (40) almost everywhere: ˆJj = ( ˆJj − 1 ∥X:j∥2 2 X⊤ :jX ˆJ ) · ∂proxMCP eλ/Lj,eγLj ∂t ( ˆβj − 1X2 :,j X⊤ :,j(Xβ −y) ) + eλ Lj ∂proxMCP eλ/Lj,eγLj ∂λ ( ˆβj − 1X2 :,j X⊤ :,j(Xβ −y) ) + eγLj ∂proxMCP eλ/Lj,eγLj ∂γ ( ˆβj − 1X2 :,j X⊤ :,j(Xβ −y) ) . (41) where ∂proxMCP λ,γ ∂t (t) = { |sign t| 1−1 γ , if |t|≤ λγ 1, otherwise , (42) ∂proxMCP λ,γ ∂λ (t) =    0, if |t|≤ λ −sign t 1−1 γ , if λ≤|t|≤ λγ 0, if |t|>λγ , (43) ∂proxMCP λ,γ ∂γ (t) = { −ST(t,λ) (γ−1)2 if |t|≤ λγ 0 if |t|>λγ . (44) Contrary to other methods, HO based algorithms do not scale exponentially in the number of hyperparameters. Here we propose experiments on the held-out loss with the MCP estimator (Zhang, 2010), which has 2 hyperparameters λand γ. Our algorithm can generalize to such non-smooth proximity-based estimator. Comments on Figure 5 (MCP , held-out criterion). Figure 5 (top) shows the convergence of the optimum on 2 datasets (rcv1 and 20news) for the MCP estimator. As before implicit forward differentiation outperforms forward differentiation illustrating Proposition 2 and Table 1.Implicit differentiation of Lasso-type models for hyperparameter optimization D. Datasets and implementation details The code used to produce all the ﬁgures as well as the implementation details can be found in the supplementary material in the forward_implicit/expesfolder. In particular in all experiments, for our algorithm, implicit forward differentiation, the size of the loop computing the Jacobian is ﬁxed: n_iter_jac = 100. Reminding that the goal is to compute the gradient: ˆJ⊤ (λ)∇C ( ˆβ(λ) ) , (45) we break the loop if ∥(J(k+1) −J(k))∇C( ˆβ(λ))∥≤∥∇C ( ˆβ(λ))∥×ϵjac , (46) with ϵjac = 10−3. All methods beneﬁt from warm start. D.1. Details on Figure 1 Figure 1 is done using synthetic data. As described in Section 4.2, X ∈ Rn×p is a Toeplitz correlated ma- trix, with correlation coefﬁcient ρ = 0 .9, (n,p) = (1000 ,2000). β ∈ Rp is chosen with 5 non-zero coefﬁ- cients chosen at random. Then y ∈ Rn is chosen to be equal to Xβ contaminated by some i.i.d. random Gaus- sian noise, we chose SNR = 3. For Figure 1 all the implementation details can be found in the joint code in the forward_implicit/examples/plot_time_to_compute_single_gradient.py ﬁle. Figure 1 shows the time of compu- tation of one gradient and the distance to ”optimum”. For this ﬁgure we evaluated the gradient in λ= λmax −ln(10). The ”optimum” is the gradient obtained using the implicit differentiation method. D.2. Details on Figure 2 Let us ﬁrst begin by a description of all the datasets and where they can be downloaded. rcv1. The rcv1 dataset can be downloaded here: https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/ datasets/multilabel.html#rcv1v2%20(topics;%20subsets). The dataset contains n = 20,242 sam- ples and p= 19,959 features. 20news. The 20news dataset can be downloaded here https://www.csie.ntu.edu.tw/~cjlin/ libsvmtools/datasets/multiclass.html#news20. The dataset contains n = 11 ,314 samples and p= 130,107 features. ﬁnance. The ﬁnance (E2006-log1p on libsvm) dataset can be downloaded here: https://www.csie.ntu.edu. tw/~cjlin/libsvmtools/datasets/regression.html#E2006-log1p. The dataset contains n= 16,087 samples and p= 1,668,737 features. All the implementation details can be found in the code: forward_implicit/expes/main_lasso_pred.py. D.3. Details on Figure 3 Figure 3 was performed using simulated data. The matrix X ∈Rn×p was obtained by simulated n×pi.i.d. Gaussian variables N(0,1). The number of rows was ﬁxed atn= 100and we changed the number of columnspfrom 200 to 10,000 on a linear grid of size 10. Then , we generated β∗with 5 coefﬁcients equal to 1 and the rest equals to 0. The vector y is equal to Xβ∗contaminated by some i.i.d. random Gaussian noise controlled by a SNR value of 3. We performed 50 repetitions for each value of pand computed the average MSE on these repetitions. The initial value for the line-search algorithm was set at λmax + ln(0.7) and the number of iterations for the Jacobian at 500 for the whole experiment. All the implementation details can be found in the code : forward_implicit/expes/main_lasso_est.py. D.4. Details on Figure 4 Figure 4 was performed using the same simulating process as described above only this time we performed only 25 repeti- tions for each value of p. We had to deal with the fact that Problem (4) is not convex for the weighted Lasso which means that our line-search algorithm could get stuck in local minima. In order to alleviate this problem, we introduced Equa- tion (18) to obtain an initial point for the line-search algorithm. We chose the regularization term to be constant and equalsImplicit differentiation of Lasso-type models for hyperparameter optimization to C(β(λmax))/10. We used a time treshold of 500 seconds which was hit only by the forward differentiation algorithm for the wLasso. The details about this experiment can be found in the code : forward_implicit/expes/main_wLasso.py.Implicit differentiation of Lasso-type models for hyperparameter optimization E. Supplementary experiments E.1. Experiments with a non-unique solution to the inner problem We recall here that the bi-level optimization Problem (4) is solved using gradient descent. We recall also that gradient descent may not converge toward a global minima since the optimized function λ↦→L(λ) may not be convex. It may be even worse: if the inner optimization problem has not a unique solution, the function λ ↦→L(λ) may not be continuous. However our algorithm can still be applied to compute the hypergradient. Figure 6 shows the time to compute a single (hyper)gradient when the solution to the inner problem is not unique. As proved for instance in Tibshirani (2013, Lemma 3 and 4), the set of parameters where the Lasso solution is not unique is typically ∅or a set whose Lebesgue measure is zero. Moreover, there exist settings such that the solution path (as a multivalued mapping) could be non-continuous, which leaves only non-gradient based methods available. Thus, we decided to not investigate the theory in such pathological settings. The authors are not aware of a classical dataset where non-uniqueness arises. Nevertheless, in the case where there existsλsuch that the solution set is not reduced to a singleton, our proposed algorithm can still be applied to any solution without theoretical guarantees. Experimental setting for non-uniqueness.For completeness, we run our methods on the following toy example Tibshi- rani (2013): we consider a design Xsuch that n= 100, p= 10000and X1,X2,X3 are generated iid following a standard normal distribution, X4 = (X2 +X3)/2 and X5,...,X p are generated i.i.d. following a standard normal distribution, then orthogonalized X5,...,X p w.r.t.Span(X1,...,X 4). We let y= −X1 +X2 +X3. We let the reader observe that when λ = 1/n, the solution set is not reduced to a singleton. In this case, similar conclusions are observed in Figure 6 as for other datasets: Imp. F. Iterdiff (ours) still outperforms its competitors. Imp. F. Iterdiﬀ. (ours) F. Iterdiﬀ. B. Iterdiﬀ. 6 8 10 Number of iterations 100 102 Times (s) 6 8 10 Number of iterations 10−11 10−8 10−5 Objective minus optimum Figure 6.Time to compute a single gradient with non-unique solution(Synthetic data, Lasso, n,p = 1000,10000). Inﬂuence on the number of iterations of BCD (in the inner optimization problem of Problem (4)) on the computation time (left) and the distance to “optimum” of the gradient∇λL(λ)(right) for the Lasso estimator.",
      "meta_data": {
        "arxiv_id": "2002.08943v3",
        "authors": [
          "Quentin Bertrand",
          "Quentin Klopfenstein",
          "Mathieu Blondel",
          "Samuel Vaiter",
          "Alexandre Gramfort",
          "Joseph Salmon"
        ],
        "published_date": "2020-02-20T18:43:42Z",
        "pdf_url": "https://arxiv.org/pdf/2002.08943v3.pdf"
      }
    },
    {
      "title": "A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization",
      "abstract": "An open problem in differentially private deep learning is hyperparameter\noptimization (HPO). DP-SGD introduces new hyperparameters and complicates\nexisting ones, forcing researchers to painstakingly tune hyperparameters with\nhundreds of trials, which in turn makes it impossible to account for the\nprivacy cost of HPO without destroying the utility. We propose an adaptive HPO\nmethod that uses cheap trials (in terms of privacy cost and runtime) to\nestimate optimal hyperparameters and scales them up. We obtain state-of-the-art\nperformance on 22 benchmark tasks, across computer vision and natural language\nprocessing, across pretraining and finetuning, across architectures and a wide\nrange of $\\varepsilon \\in [0.01,8.0]$, all while accounting for the privacy\ncost of HPO.",
      "full_text": "A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization Ashwinee Panda * 1 Xinyu Tang* 1 Saeed Mahloujifar 1 Vikash Sehwag 1 Prateek Mittal 1 Figure 1.Visualization of our method. We use low-cost trials (small ε) to estimate hyperparameters (HPs) and scale these up to the privacy budget for the final run. We combine multiple HPs together, and have a prior that the scaling is linear. Abstract An open problem in differentially private deep learning is hyperparameter optimization (HPO). DP-SGD introduces new hyperparameters and complicates existing ones, forcing researchers to painstakingly tune hyperparameters with hun- dreds of trials, which in turn makes it impossible to account for the privacy cost of HPO without de- stroying the utility. We propose an adaptive HPO method that uses cheap trials (in terms of privacy cost and runtime) to estimate optimal hyperparam- eters and scales them up. We obtain state-of-the- art performance on 22 benchmark tasks, across computer vision and natural language processing, across pretraining and finetuning, across architec- tures and a wide range ofε ∈ [0.01, 8.0], all while accounting for the privacy cost of HPO. *Equal contribution 1Princeton University. Correspondence to: Ashwinee Panda <ashwinee@princeton.edu>. Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). 0.5 1 2 4 8 Privacy/uni00A0Budget/uni00A0() 85 86 87 88 89 90ImageNet/uni00A0Acc/uni00A0(%) Ours/uni00A0(Merged/uni00AD38M) Mehta/uni00A02023/uni00A0(JFT) Berrada/uni00A02023/uni00A0(JFT) Figure 2.Evaluation on ImageNet-1k finetuning. Our HPO only requires paying the privacy cost once, and can then be used to find good HPs for all values of ε > 0.5. We outperform prior work (Mehta et al., 2023b; Berrada et al., 2023) because our HPO finds better HPs, even though prior work has better non-private performance and does not report the privacy cost of their HPO. 1. Introduction A crucial component of interfacing machine learning models closely with user data is ensuring that the process remains private (Team, 2017), and Differential Privacy (DP) is the gold standard for quantifying privacy risks and providing provable guarantees against attacks (Dwork et al., 2006). DP implies that the output of an algorithm e.g., the final weights trained by stochastic gradient descent (SGD) do not change much if a single datapoint in the dataset changes. Definition 1.1 (Differential Privacy). A randomized mech- anism M with domain D and range R preserves (ε, δ)- differential privacy iff for any two neighboring datasets D, D′ ∈ Dand for any subsetS ⊆ Rwe have Pr[M(D) ∈ S] ≤ eε Pr[M(D′) ∈ S] + δ where D and D′ are neighboring datasets if they differ in a single entry, ε is the privacy budget and δ is the failure probability. Differentially Private Stochastic Gradient Descent (DP- SGD) (Song et al., 2013; Abadi et al., 2016) is the stan- dard privacy-preserving training algorithm for training neu- ral networks on private data. For a batch size B and 1 arXiv:2212.04486v3  [cs.LG]  5 May 2024A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization learning rate η, DP-SGD has an update rule given by w(t+1) = w(t)− ηt |Bt| \u0000P i∈Bt 1 C clipC(∇ℓ(xi, w(t))) + σξ \u0001 where the changes to SGD are the per-sample gradient clipping clipC(∇ℓ(xi, w(t))) = C×∇ℓ(xi,w(t)) max(C,||∇ℓ(xi,w(t))||2) , and addition of noise sampled from a d-dimensional Gaussian distribution ξ ∼ N(0, 1) with standard deviation σ. These steps alter the bias-variance tradeoff of SGD and degrade utility, creating a challenging privacy-utility tradeoff. Because private training introduces additional hyperparame- ters, biases optimization by clipping the gradient, and im- poses privacy-utility tradeoffs for existing hyperparameters, hyperparameter optimization (HPO) in DP is challenging. Many prior works report doing hundreds of hyperparameter trials and do not report the privacy cost of HPO in their final privacy guarantee (De et al., 2022; Bu et al., 2022a;b; Mehta et al., 2023a;b; Berrada et al., 2023). These works either assume that HPO does not leak privacy, that the best HPs are known beforehand, or that they can be transferred from a public dataset that is similar to the private dataset. More recently, researchers have proposed methods that do private HPO (Papernot & Steinke, 2021; Koskela & Kulka- rni, 2023; Wang et al., 2023) with R ´enyi DP. These pri- vate HPO methods have been evaluated on MNIST and CIFAR10, but have not been validated on more challenging tasks in CV , or on LLMs. We propose a new private adaptive HPO method ( Figure 1), which we call the new linear scaling rule. We first estimate the optimal HPs for small privacy budgets. We then scale the searched HPs linearly up to larger privacy budgets. Our full method is described in Algorithm 2. We summarize our contributions: • We demonstrate that our new linear scaling rule reduces the computation and privacy cost of HPO by an order of magnitude without sacrificing performance • We compare our private HPO method to random search, grid search, and 3 prior methods for private HPO • We evaluate our private HPO on 22 tasks spanning com- puter vision and natural language processing, fine-tuning and training from scratch, training models spanning from ResNets to multi-billion-parameter Transformers • We find that models trained with our method can pro- vide good performance even when there is a large shift between public and private data 2. Design We provide a set of design goals for our adaptive private HPO method and explain their importance. We use simple axioms for optimization and privacy as building blocks to motivate the high-level design of our method. We conduct preliminary experiments to quantitatively determine the re- lationships between key hyperparameters. Ultimately we compose the many hyperparameters of interest in DP into a single scalar variable r, and present a simple adaptive approach for privately optimizing this parameter. 2.1. Design Goals We draw our goals from the two simple baselines for HPO, random search and grid search. We define random search as drawing hyperparameters from the search space randomly and doing a single run with the entire privacy budget. We discuss variations on random search, such as doing multiple runs with smaller privacy budgets, in Section 5. Random search has low runtime, is parallelizable, and has low privacy cost, but typically does not provide good performance when the hyperparameter search space is large and the set of viable solutions is sparse. Grid search typically has high runtime and privacy cost, and is also parallelizable. Given sufficient trials, grid search should approach the performance of the oracle, the run with perfectly chosen hyperparameters. Our method should provide: • Better performance than random search and almost as good as the oracle; if we define the error rate of any HPO method as the difference in performance between that method and the oracle, our method should reduce the error rate relative to random search significantly. • Better privacy cost than grid search and almost the same privacy cost as random search; the difference in privacy-utility tradeoff between our method and the or- acle should not be the difference between a run with ε = 0.1 and ε = 1.0, it should be the relatively smaller gap between ex. ε = 0.9 and ε = 1.0. 2.2. Building Blocks of Linear Scaling The design of our adaptive private HPO method is based on simple building blocks derived from known theorems in optimization and privacy. First we inspect the definition of DP-SGD and the nature of adaptive composition. Suppose we are taking T steps with noise σ to produce some ε guar- antee. If we relax our privacy guarantee, so now we want to achieve some ε∗ > ε, we can either a) fix T and reduceσ, b) increase T and fix σ, or c) some combination of (a) and (b). Second we turn to a rule of thumb that is popularly known as the original linear scaling rule; the optimal learning rate is inversely proportional to the noise scale in GD (Malladi et al., 2022). In the case of DP-GD, that is in the full batch setting where there is no noise due to SGD, the learning rate should be inversely proportional to σ. If we combine these two axioms, we get the following heuristic: 2A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization Proposition 2.1. If we are taking T steps with noise σ and learning rate η to achieve a target ε∗, we can achieve a target ˆε > ε∗ by either: a) Fix T, reduce σ, increase η) b) Increase T, fix σ, fix η c) Increase T slightly, reduce σ slightly, increase η slightly. We now formalize this intuition. 3. Analysis of Private Gradient Descent We analyze the excess empirical risk of private GD as the sum of two terms. The first term is the risk of non-private GD with the same hyperparameters. The second term is the divergence of private GD from non-private GD due to the added noise term. We consider optimizing a function using Differentially Pri- vate Gradient Descent (DP-GD). The presence of noise in GD introduces a deviation between the iterates of GD with noise, denoted as wT , and without noise, denoted as wTb , at iteration T. We first upper bound this deviation in expec- tation, which we refer to as the radius r. We then use this to upper bound the excess empirical risk of noisy GD. We finally use this bound to motivate the design of our private adaptive HPO method. 3.1. Assumptions We present four assumptions that simplify the conver- gence analysis. We acknowledge that these assumptions do not hold true in all settings, but nevertheless provide an important foundation for illustrating the intuition of our method. We empirically validate the success of our algo- rithm in complex neural network settings, such as training a 13B-parameter OPT Transformer model on the benchmark SQuAD task, in Section 3. • A function is α-strongly convex if for any two points x, yand any subgradient g at x, it holds that f(y) ≥ f(x) + g⊤(y − x) + α 2 ∥y − x∥2. • A function is β-smooth if its gradient is β- Lipschitz continuous, meaning for any two points x, y, ∥∇f(x) − ∇f(y)∥ ≤β∥x − y∥. • A function is L-Lipschitz if there exists a positive L such that |f(x) − f(y)| ≤L∥x − y∥ • A function satisfies the bounded gradient assumption if there exists a constant C such that E[∥∇f(w)∥ ≤ C ∀w ∈ Rd The bounded gradient assumption is implied by convexity and Lipschitzness. This allows us to ignore the impact of clipping in DP-SGD, which reduces the analysis to that of noisy GD. The noise added at each iteration for privacy has an expected norm ρ = √ d · σ, where d is the dimension of the model, and σ is the scale of the noise. The learning rate η satisfies 0 < η <2 β , ensuring convergence. Let c = max(|1 − ηα|, |1 − ηβ|), which characterizes the contraction factor in the optimization process. Given that η is chosen appropriately, we have 0 < c <1. 3.2. Definitions The empirical loss L(wT ) for a model parameterized by wT (ex. iterate T of GD) over a dataset D = {(xi, yi)}N i=1 is defined as the average loss over all training samples: L(wT ) = 1 N NX i=1 ℓ(f(xi; wT ), yi) The goal of our private Hyperparameter Optimization (HPO) is to find the hyperparameter set Λ∗ that minimizes the loss: Λ∗ = argmin Λ Lval(Λ) where Lval(Λ) denotes the loss on a validation dataset for a given hyperparameter configuration Λ. Because Noisy GD typically does not overfit due to the heavy regularization effect of the noise, and to make the convergence analysis straightforward, we use the empirical loss as a proxy for the validation loss throughout. We will analyze the excess empirical risk to motivate the design of our private HPO method. Let wT be the Tth iterate of noisy GD that optimizes a function satisfying the assumptions, and wTb be the Tth iterate of non-noisy GD that optimizes that same function. We define the excess empirical risk of noisy GD as: Rnoisy = E[L(wT )] − L(w∗), ≤ E[L(wT ) − L(wTb )] + L(wTb ) − L(w∗) ≤ E[L · ∥wT − wTb ∥] + Rnon-noisy Where L(w∗) denotes the empirical loss at the optimal pa- rameter set (without noise). Rnon-noisy = L(wTb ) − L(w∗ is the excess empirical risk of non-noisy GD. In the last line, we upper bounded the excess risk induced by noise L(wTb ) − L(w∗) by applying Lipschitzness of the loss. We now bound ∥wT − wTb ∥. Theorem 3.1. Let wT be the Tth iterate of noisy GD that optimizes an α-strongly convex and β-smooth function, and let wTb be the Tth iterate of non-noisy GD that optimizes 3A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization that same function. The ”noisy radius” distance, the ℓ2- norm between wT and wTb at iteration T, can be bounded in expectation as follows: E[∥wT − wTb ∥] ≤ ρη ×  T−1X i=0 ci ! = r Proof sketch. The full proof is in Appendix B.5. At each iteration the distance between the noisy iterate and the non- noisy iterate contracts by a factor ofc = max(|1−ηα|, |1− ηβ|) and then increases additively by ρη. The overall dis- tance then can be represented by scaling the additive noise term ρη by a geometric series that converges. Future work might incorporate additional factors such as momentum ac- celeration, bias introduced by clipping, or extend our analy- sis to the setting of more general neural networks. However, our objective here is to provide some theoretical intuition for our algorithm. Substituting Theorem 3.1 into the excess empirical risk we get Rnoisy ≤ Lr + Rnon-noisy where L is the Lipschitz constant, we can see that our pri- vate HPO needs to find HPs that are good for non-noisy optimization but do not create a large divergence between the noisy and non-noisy iterates. 4. Our Private HPO We have established a relationship between the excess em- pirical risk and the noisy radius. We can now connect this back to our goal of doing private HPO, which is to find the HPs that minimize the excess empirical risk. We want to find r∗ = r(ε), the optimal value of r for a given value of ε. We will first reduce the dimensionality of the search problem and then introduce a principled approximation. 4.1. Reducing the Dimensionality of HPO We want to reduce the dimensionality of HPO so that we can reduce the cost of HPO. For fixed ε, if we increase or decrease T then we will cor- respondingly increase or decrease σ by the Composition Theorem of DP. The actual statements of DP composition are somewhat complicated, but we can simplify them as say- ing σ grows slower than αT for some constant α. Because E[ρ] = √ dσ, we have that ρ grows slower than T. The geometric series converges to 1 1 − c as T increases, giving us E[∥wT − wTb ∥] ≤ (T η) · ( √ d 1 1 − c). Because we are interested in writing the radius in terms of hyperpa- rameters that we can optimize, we drop the second term for simplicity. Now we can write our hyperparameter of interest as r = η × T, reducing the 2D HPO to 1D. If we wanted to search for additional terms such as the batch size or clipping threshold, we could incorporate them into our theory, but we empirically find that it’s best to fix all other HPs to the values we provide and just search for η, T. 4.2. Our Private HPO In order to find the optimal r∗ = r(ε) without exhaustively searching, we need to approximate r(ε). A natural choice is Taylor approximation. We can sample points from r(ε) at different values of ε via random search, use this to ap- proximate a Taylor polynomial, and then use that Taylor polynomial to estimate r for any desired target ε. After we have our estimated r, we can decompose it into η, T by randomly sampling η, Tuntil their product is close to r. This is the procedure we use in Figure 2, paying for the privacy cost of building the approximation and then using it to estimate the optimal HPs for many values of ε ∈ [0.5, 8]. We now elaborate on the implementation of the method. The first-order Taylor approximation of a function r(ε) around a point ε0 is given by r(ε) ≈ r(ε0) + dr dε \f\f ε=ε0 · (ε − ε0), which linearly approximates r near ε0. Because we cannot analytically determine dr dε , we will have to ap- proximate this. To approximate the first-order Taylor polynomial we fit a line. We first use random search to find two empirical points (ε1, r(ε1)) and (ε2, r(ε2)). We then fit a line to these points to obtain the parameters of the linem, b(slope and intercept). We finally estimate the optimal r(εf ) = mεf + b such that the composition of privacy guarantees for the entire private HPO satisfies a target privacy budget according to Theorem 2.3. In practice we choose smaller values of ε for these points such as ε1 = 0.1, ε2 = 0.2, that we find provide a good privacy-utility tradeoff. More generally, we can approximate the Taylor polyno- mial by fitting a degree N polynomial with N + 1 points (ε1, r(ε1)) ··· (εN+1, r(εN+1)). We provide results com- paring the linear approximation to quadratic approximation in the 2nd common response PDF, but use the linear ap- proximation throughout our work because we find that it provides a good privacy-utility tradeoff. The full method is detailed in Algorithm 2. The final pri- vacy guarantee including the cost of HPO is given by Theo- rem 4.1. Theorem 4.1. The privacy guarantee of Algorithm 2 in terms of µ in f-DP is µt = q nµ2 1 + nµ2 2 + µ2 f . The proof and conversion to (ε, δ)-DP follow directly from Dong et al. (2022), so we defer it to Appendix B.5. 4A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization Table 1.Our method fixes six design choices: the architecture and initialization (for CV tasks only), the batch size (full batch), the optimizer (SGD with momentum=0.9), the accounting method (PLV where all prior HPO methods use RDP), and the clipping norm (unit clipping). We report the improvement derived from following each of these techniques with respect to a competitive baseline from prior work on CIFAR100 at ε = 0.1. Method Baseline Baseline Accuracy Improvement Classifier (no bias) (Mehta et al., 2023b) 71.3 0 .36 Zero Initialization Random Initialization (De et al., 2022) 64.85 6 .81 Gradient Descent SGD(Batch=4096) (De et al., 2022) 70.2 1 .46 Momentum (ρ = 0.9) ρ = 0 (Bu et al., 2022a) 69.02 2 .09 PLV Accounting RDP (De et al., 2022) 68.43 3 .23 Unit Clipping (C = 1) C ≪ 1 (Mehta et al., 2023a) 71.2 0 .46 Implementing our method requires decomposing a target ε, δ-DP guarantee into a set of µs; we provide code for this. 4.3. Limitations Although this theory does not hold in general for training neural networks, we quantitatively evaluate the heuristic we develop in Section 3.4 and find that our method holds even for the complex setting of training Transformers on NLP benchmarks. Our HPO also requires more runtime than random search because it is adaptive. Algorithm 1 Model Training Subroutine Initialize model weights w at 0 Decompose r into η, Twithout exceeding Tmax or ηmax Use the PLD accountant to calibrate σ given µ, T for i = 1, 2, . . . , Tdo Compute gradient with unit clipping and add noise ∇(i) = 1 |D| \u0000P i∈D clip1(∇ℓ(xi, w(i))) + σξ \u0001 Take a step with momentum: v(i) ← ρ · v(i−1) + ∇(i), w(i) ← w(i−1) − ηv(i) end for return trained model w Algorithm 2 Adaptive HPO Routine Inputs: Privacy parameters for hyperparameter sweeps and final run µ1, µ2, µf , number of runs per sweep n, maximum learning rate ηmax, maximum number of itera- tions Tmax, dataset D, model M Perform n runs with µ1 using Hyperparameter Sweep Subroutine (Algorithm 3); obtain the best-performing r1 Perform n runs with µ2 using Hyperparameter Sweep Subroutine (Algorithm 3), obtain the best-performing r2 Perform linear interpolation to estimate the slope α and bias b of the line r = αε + b given (µ1, r1), (µ2, r2) Set r∗ = αµf + b given the estimated linear interpolation Launch the Model Training Subroutine (Algorithm 1) with r∗, µf , obtaining the final performance Af Output: Final performance Af , trained model M Algorithm 3 Hyperparameter Sweep Subroutine Inputs: Privacy parameter µ, number of runs per sweep n, search space for r for i = 1, 2, . . . , ndo Uniformly sample r from the search space Launch Model Training Subroutine (Algorithm 1) with configuration r, µ, returning performance Pi if Pi is the best performance so far on the training set then set best-performing ri = r end for return best-performing ri 5. Evaluation We provide results on a range of image classification, distri- bution shift, and natural language processing tasks, for both finetuning of models pretrained on public data and training from scratch without any additional data. Due to the large scope of our evaluation, we defer all experimental details and full results for all datasets and models to Appendix A. We provide ablations on all steps of our method (B.2). We provide hyperparameter grid search results (B.4). We also provide the code to reproduce our results at this link. Datasets. Image classification: ImageNet (Deng et al., 2009), CIFAR10 (training from scratch and finetuning), CI- FAR100 (Krizhevsky et al., 2009), FashionMNIST (Xiao et al., 2017), STL10 (Coates et al., 2011), EMNIST (Co- hen et al., 2017). Because these image classification datasets are generally considered in-distribution of the pretraining data, we also provide results on a number of distribution shift datasets (Koh et al., 2020) . CI- FAR10 → STL, CIFAR10p1, CIFAR10C, CIFAR100 → CIFAR100C (Hendrycks & Dietterich, 2019), Water- birds (Sagawa et al., 2019), FMoW (Christie et al., 2017), and Camelyon17 (B ´andi et al., 2019). For NLP tasks we consider SQuAD (Rajpurkar et al., 2016) for Ques- tions Answering, text classification tasks from the GLUE benchmark (Wang et al., 2019a): SST-2, QNLI, QQP, MNLI(m/mm) and for next word generation we use Per- 5A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization sonaChat (Zhang et al., 2018a) and WikiText-2 (Merity et al., 2017), and Enron Emails (Klimt & Yang, 2004). 5.1. Effectiveness of the Linear Scaling Rule ImageNet (with Public Data) In Figure 2 we compare the performance of our method on ImageNet against the com- petitive prior works of Mehta et al. (2023b); Berrada et al. (2023). Note that these works do not report the privacy cost of HPO and pretrain their models with JFT, Google’s proprietary internal dataset; as a result the non-private per- formance of their models exceeds ours (rightmost points). Despite this, given sufficient budget (ε >0.5) we match or exceed their performance while accounting for the pri- vacy cost of HPO. The downside of our method is that for sufficiently small ε on sufficiently difficult datasets such as ImageNet, there is no way to keep the privacy cost of HPO small enough to retain enough budget to do a final run, because HP trials with too small a budget do not provide any information. We provide a deep dive into these points of comparison in Appendix A. CIFAR-10 (without Public Data) In Table 2 we compare our method to random search and the grid search baseline, which does not consider the privacy cost of HPO. We sig- nificantly outperform random search, and approach the per- formance of grid search. To the best of our knowledge, we are the first to provide competitive performance when train- ing on CIFAR10 without public data under a strict privacy budget while accounting for the privacy cost of HPO. Table 2.Performance comparison of different methods on CI- FAR10. Our method outperforms prior work in linear probing settings when using a feature extractor pretrained on CIFAR100. In the setting where we do not have public data, we compare to random search and grid search and our method greatly outperforms random search. CIFAR10 Acc with Public Data (ε = 1) Koskela & Kulkarni (2023) 67% Papernot & Steinke (2021) 66% Ours 70.5% CIFAR10 Acc without Public Data (ε = 1) Random Search 44% Grid Search (cost of HPO not incl.) 68% Ours 62.63 % 5.2. Comparison to other Private HPO We provide a detailed comparison to 5 prior works in pri- vate HPO as well as the baselines of random search and grid search, and explain the design choices that enable our method to dominate all prior work. 5.2.1. C OMPARISON TO RENYI HPO We compare to three prior works that use Renyi DP to ana- lyze HPO (Papernot & Steinke, 2021; Koskela & Kulkarni, 2023; Wang et al., 2023). In Table 2 we report that our linear scaling is 3.5% better on CIFAR10 at ε = 1 in the experimental setting of Koskela & Kulkarni (2023): linear probing on a ResNet20 check- point pretrained on CIFAR100. Koskela & Kulkarni (2023) achieve 67% on CIFAR10 at ε = 1. In the same setting, the method of Papernot & Steinke (2021) obtains 66%. We apply the linear scaling rule in the same setting, so that only the hyperparameters our method selects are different, and obtain 70.5% at ε = 1 . All methods use the same hyperparameter search space. The reason our method out- performs Koskela & Kulkarni (2023); Papernot & Steinke (2021) is because our prior is better than their random search, which is required by their method, enabling us to simulta- neously allocate a smaller portion of the privacy budget to HPO while still finding better hyperparameters. We also use PLD accounting which is tighter than the RDP accounting their method requires. Neither of these can be fixed; that is, we cannot modify their method to integrate the linear scaling prior or to use PLD accounting. Even with PLD accounting, we would not be able to make up for the gap in accuracy that comes from our adaptive method. An interesting question for future work is whether we can do RDP analysis of our adaptive method. More details in Appendix A.1. 5.2.2. C OMPARISON TO PARAMETER -FREE METHODS . A related area is parameter-free HPO, that builds optimizers that do not require specifying the learning rate as a hyper- parameter. In general it can be challenging to apply these parameter-free methods to DP, because the update rule for the scale of the gradient may not maintain its guarantees in the presence of noise (Li et al., 2023). Comparison to DPAdamWosm. One parameter-free opti- mizer specifically designed for DP is DPAdamWOSM (Mo- hapatra et al., 2021). On ImageNet DPAdamWOSM achieves 79% at ε = 1), which is 8% lower than our method (87% at ε = 1). We do not find that the data-independent learning rate selection works well for ImageNet, and still requires tuning the number of iterations (see Appendix A.1). Comparison to Mehta et al. (2023b) Mehta et al. (2023b) propose an approach where they fix the batch size to full batch, the number of steps to 1, and take a single step of DP-Adam with a very small learning rate. Their approach obtains just 81% at ε ≥ 1 on ImageNet for a model whose non-private accuracy is 88.7%, because they take only a single step. Our method smoothly interpolates between the low-r setting for small ε and the large-r setting for large ε, and outperforms their method across all privacy budgets. 6A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization Table 3.Comparing various HPO methods on CIFAR10 (without public data), CIFAR100, ImageNet, and SQuAD (with public data). Reported numbers are mean over 5 trials. Dataset Random Search Oracle Ours RERR CIFAR10 44 68 62.63 77.63 CIFAR100 84.44 89.62 89.10 84.85 ImageNet 81.2 88.6 86.7 73.97 SQuAD 49.33 82.43 78.08 86.85 5.2.3. C OMPARISON TO BASELINES : RANDOM SEARCH AND GRID SEARCH . Linear scaling significantly outperforms random search. In Table 3 we report the performance for random search, our method, the oracle, and the relative error rate reduction (RERR). Across all datasets, our method significantly out- performs random search. We use the same logarithmic grid for both our method and random search that can be found in Appendix A. We vary this grid and find that the larger the search space, the more our method outperforms random search. 0.0 0.2 0.4 0.6 0.8 1.0 0 20 40 60 80T est Accuracy  Accuracy via Linear Scaling Accuracy via Grid Search (a) 0.0 0.2 0.4 0.6 0.8 1.0 10 20 30 40Total Step Size (  × T) T otal step size by Linear Scaling T otal step size by Grid Search (b) Figure 3.Training the beit architecture on CIFAR100, the linear scaling rule produces values for r = η × T close to that of grid search, and the performance drop is only apparent at ε > 0.2 because of the cost of HPO, and vanishingly small for larger ε. Linear Scaling approaches grid search. We validate the effectiveness of linear scaling against the grid search base- line. In Fig. 3 (right) we compare Alg. 2 to the best run across 100 trials from the search space. The privacy cost of grid search is many times higher than that of our method at each value of ε, because we do not account for the pri- vacy cost of grid search to illustrate that even when our method has to account for the privacy cost of HPO and the oracle (grid search) does not, our method is competitive. Our method finds near-optimal hyperparameters with just a fraction of the runtime and privacy cost of grid search. 5.3. Empirical Analysis of Linear Scaling We now consider different architectures and validate our HPO method in the presence of distribution shifts. Full results can be found in Appendix B.1. Architecture Search. In Table 4 we apply our method to Table 4.We compare the best private and best non-private perfor- mances of all models on all datasets. We use the linear scaling rule to scale hyperparameters from ε = 0.1 to ε = 1, so our privacy analysis includes the cost of hyperparameter tuning. Model Dataset ε = 1 ε = ∞ Gap beitv2 CIFAR10 98.90 99 .00 0 .10 CIFAR100 89.10 91 .57 2 .47 FMNIST 91.02 91 .53 0 .51 STL10 99.69 99 .81 0 .12 EMNIST 81.77 82 .00 0 .23 convnext CIFAR10 96.75 97 .22 0 .47 CIFAR100 83.47 86 .59 3 .12 FMNIST 90.23 91 .13 0 .9 STL10 99.61 99 .71 0 .10 EMNIST 78.38 79 .05 0 .67 beit CIFAR10 98.19 98 .51 0 .32 CIFAR100 87.1 90 .08 2 .98 FMNIST 90.55 91 .6 1 .05 STL10 99.62 99 .78 0 .16 EMNIST 81.48 83 .25 1 .77 vit-L CIFAR10 98.29 98 .44 0 .40 CIFAR100 86.18 89 .72 3 .54 FMNIST 90.58 91 .37 0 .79 STL10 99.62 99 .76 0 .14 different architectures that can serve as good backbones for high-accuracy DP classification across CIFAR10, CI- FAR100, FMNIST, STL10, and EMNIST. The private-non private utility gap diminishes with model accuracy. One architecture, beitv2, performs the best on all benchmarks and also has the highest non-private zero-shot ImageNet accuracy (Wightman, 2019). We conclude that architecture search can be done without any privacy cost by selecting the model with the best zero-shot performance on a repre- sentative benchmark such as ImageNet. Distribution Shift. A concern in DP fine-tuning is that the pretraining datasets are too similar to the downstream tasks, which can violate privacy (Tram`er et al., 2022). In Table 5 we evaluate the robustness to distribution shift of models trained with our private HPO to non-private models, in the absence of any explicit regularization methods or any infor- mation about the distribution shift. These datasets are con- sidered benchmark tasks for distribution shifts (Kumar et al., 2022b;c; Mehta et al., 2022) and include data that is not in- distribution of the training data, making for a more realistic evaluation of the capabilities of our method to solve chal- lenging tasks. We show that DP-SGD provides robustness to covariate, subpopulation and label distribution shifts for synthetic and natural datasets. Full details in Appendix B.1. On Waterbirds, DP degrades the ID performance but actu- ally improves the OOD performance. On fMoW and Came- lyon17 that are datasets with a significant distribution shift from ImageNet and very different subgroups, DP does not significantly degrade performance and does not exacerbate 7A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization Table 5.Evaluating our DP-HPO method on datasets with distribution shifts at ε = 1. Waterbirds fMoW Camelyon C10 → STL C10 → C10p1 C10 → C10C C100 → C100C ID 92.31 45.44 93.91 98.90 98.90 98.90 89.65 OOD 91.59 35.31 93.55 98.82 97.85 89.98 68.69 Table 6.Linear scaling holds for GLUE tasks when training the full RoBERTa-base model Task ε Acc r = η × T SST-2 0.1 90.60 0.975 0.2 90.83 1.95 0.7 91.06 5.07 QNLI 0.1 82.54 3.9 0.2 84.00 4.68 1.3 86.25 26.52 QQP 0.1 81.07 11.7 0.2 82.21 17.55 1.2 84.69 64.35 MNLI(m/mm) 0.1 77.52(78.24) 11.7 0.2 79.40(79.98) 17.55 1.2 81.86(82.76) 64.35 disparities among subgroups. We also show that we can train models on CIFAR10 with DP and do zero-shot transfer to STL and CIFAR10p1. Finally, we evaluate the robustness of CIFAR-DP-trained models to the common corruptions of CIFAR10C/CIFAR100C, and note that DP training achieves some measure of intrinsic robustness to image corruptions. 5.4. Linear Scaling for language modeling Prior work has generally focused on either CV or NLP because the methods used in DP fine-tuning differ greatly across data modalities (Li et al., 2022b; Mehta et al., 2023a); here we show that our method extends to NLP by validat- ing on text classification and language modeling tasks with LoRA (Hu et al., 2021) and full fine-tuning. We fine-tune GPT-2 (Radford et al., 2019) with our method for three lan- guage modeling tasks that have been benchmarked in prior works (Li et al., 2022b; Shi et al., 2022; Gupta et al., 2022) on private fine-tuning: Persona-Chat (Zhang et al., 2018b), WikiText-2 (Merity et al., 2017) and Enron Emails (Klimt & Yang, 2004). We also fine-tune RoBERTa-base on four tasks in the GLUE benchmark: SST-2, QNLI, QQP and MNLI(m/mm) in Table 6. While prior works mainly fo- cus on ε in {3, 8}, in this work we are also interested in smaller εs like 0.1. Appendix C.2 includes the details for the experimental set-up. Linear scaling succeeds when random search fails. We consider the challenging setting from Malladi et al. (2024) of fine-tuning an OPT-13B model on just 1000 samples from SQuADv2 with DP-SGD-LoRA. Random search runs sometimes do not improve much over zero shot performance, because the search space is so large and the viable set so small. In the initial phases of our method, the trials do not always succeed. Regardless, our method achieves78%±3% close to the oracle 82%, a RERR of 87.5%. Random search performs poorly for NLP tasks because HPO is generally more challenging (Li et al., 2022b). In the rest of the NLP datasets we consider, we compare our performance to prior work that doesn’t consider the privacy cost of HPO. Linear scaling holds for NLP tasks We analyze the per- formance gap between estimated total step size and optimal total step size by grid search to understand how well linear scaling performs on language modeling tasks. Fig. 4 plots the optimal perplexity and perplexity by estimated total step size at different values ofε on Enron emails. We can see that the linear scaling rule generalizes well for reported values of ε and the perplexity by the estimated total step size is close to the optimal perplexity. From Table 6 we can see that our method also works for the GLUE benchmark. 0.0 0.5 1.0 1.5 2.0 2.5 3.0 10 11 12 13 14Perplexity Perplexity by linear rule Perplexity by grid search (a) Pareto Frontier for ε vs Test Perplexity. 0.0 0.5 1.0 1.5 2.0 2.5 3.0 0 5 10Total step size ( × T)  T otal step size by linear scaling T otal step size by grid search (b) Pareto Frontier for ε vs Total Step Size. Figure 4.The linear scaling rule (accounting for the privacy cost of hyperparameter tuning) is competitive with grid search (non- private, doing N trials each with the given ε) on the Enron Emails dataset. Left: y-axis is Perplexity (lower is better). The linear scaling rule outperforms prior results on dif- ferentially private language modeling tasks.We first run a qualitative evaluation on the previous benchmark SOTA (Li et al., 2022b) on PersonaChat trained with DP-SGD by fol- lowing the linear scaling rule to increase the number of epochs. We can see in Table 7 that we can push the per- plexity under 18 for ε = 3 and ε = 8; this performance is competitive with the non-private baseline. Furthermore, even when pushing for a stricter privacy guarantee ε = 0.5, we can still get perplexity of 21.25, that is better than the result of ε = 8 in (Li et al., 2022b). We also report the results of ablating these hyper-parameters and varying the 8A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization Table 7.Linear scaling holds when fine-tuning all layers of GPT2 on PersonaChat and outperforms Li et al. (2022b) ε (δ = 1 2|Dtrain|) 0.7 3 ∞ Li et al. (2022b) - 24.59 18 .52 Our Work 21.25 - 17.69 Table 8.Finetuning GPT-2 on WikiText-2 (δ = 10−6) and Enron (δ = 1 2|Dtrain|) with DP-SGD. Ppl is perplexity and TSS is Total Step Size. (∗ means estimated). Previously reported best perplexity of GPT-2 on WikiText-2 atε = 3is 28.84 in (Shi et al., 2022). ε 0.1 0 .2 0 .5 1 .4 2 .2 3 .0 WikiText-2 Ppl - 28.81 28.37 28.15 27.98 27 .69 TSS - 0.008 0 .02 0 .04∗ 0.08∗ 0.12∗ ε 0.1 0 .2 0 .7 1 .1 1 .9 2 .7 Enron Ppl 14.35 12.50 11.56 10.91 10.45 10 .14 TSS 0.10 0 .58 2 .02∗ 4.41∗ 9.19∗ 13.98∗ number of layers trained in Appendix C.3. We quantitatively validate the linear scaling rule on WikiText-2 and report the result in Table 8. For WikiText-2, a key observation is that when we compare our results to the best prior reported re- sults in (Shi et al., 2022), for the same number of passes over the training data (20), we obtain lower perplexity for ε = 0.2 than they report for ε = 3. That is, by just increas- ing the effective step size from ∼ 8 × 10−6 to ∼ 8 × 10−3 we can strengthen the privacy guarantee without degrading performance. 5.5. Additional Ablations We can apply our method to methods other than DP- SGD. Tang et al. (2024) recently proposed DP-ZO, a method for DP zeroth-order optimization, that privatizes the zeroth- order update to the model weights in the form of a scalar rather than the first-order gradient update. In Table 9 we find that our method can also optimize HPs for DP-ZO. Although our method fits a 1-d polynomial with 2 points, we can in principle fit any degree-d polynomial with d + 1 points. to approximate the relationship between r and ε. However, because using more points to fit the polynomial imposes more privacy cost for HPO, we use the same num- ber of points and degree throughout all experiments. It is likely that for some datasets, it’s important to tune the Method Mean Accuracy (Std) Random Search 82.53 (1.01) Our Private HPO 83.02 (0.86) Grid Search 83.87 (0.50) Table 9.Our method works beyond DP-SGD. Method Mean Accuracy (Std) Linear (2) 86.69 (0.86) Linear (3) 87.81 (0.86) Quadratic (3) 86.64 (1.08) Table 10.Using more points or a higher order approximation can improve performance. Method GPU Hours Wall-clock time (hours) Random 1 1 Grid 100 1 Ours 7 3 Table 11.Our method trades off runtime for performance with random search and grid search. privacy budget allocated to the smaller trials, the number of points, etc. However, we are not interested in tuning the hyperparameters of our hyperparameter optimization method. In Table 10 we evaluate the linear fit with 2 and 3 points for evaluation, and the quadratic fit with 3 points for evaluation, on ImageNet. Throughout the paper we search for the two main HPs of interest η, Tand fix other HPs such as batch size B and clipping threshold C. However, we can search for these as well. We can incorporate new HPs by updating the decom- position of r so that we optimize for the joint product of the HPs being optimized. We change it from r = η × T to r = η × T × B × C. We evaluate on CIFAR100. The performance of our method when optimizing η, T, B, Cis 87.9 ± 1.9, which is worse than the 89.10 where we opti- mized η, Tand fixed B, C. One limitation of our method is the runtime, shown in Ta- ble 11.. We have worse runtime than random search and worse parallelization than grid search, which is embarrass- ingly parallel while our method requires serial runs. Here the base time for a single HP trial is just 1 hour; this can change based on the task, but these proportions should remain similar. Random search just does 1 run so it has both the lowest GPU hours and wall-clock time. The oracle does a number of runs equal to the granularity of the search space, which here we approximate as 100. In the setting where 100 GPUs are available for the oracle, which may be realistic for large companies but is not realistic for our academic compute setting, these can all be done in parallel, so it uses 100 GPU hours but just 1 hour in wall-clock time. Our method typically does 7 runs: 3 for ε1, 3 for ε2, and 1 for εf , so the total number of GPU hours is 7. The serial dependency of our method requires that εf runs after ε1 9A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization and ε2, but the 3 runs for ε1, ε2 can be parallelized so the wall-clock time is just 3 hours. 6. Related Work and Discussion Related Work. We have performed detailed quantitative and qualitative comparisons to prior private HPO meth- ods (Papernot & Steinke, 2021; Koskela & Kulkarni, 2023; Wang et al., 2023). These build on earlier work by Liu & Talwar (2018) that can do HPO by increasing the privacy cost roughly threefold. We improve over these works by significantly reducing the privacy budget required by HPO and adopting a robust prior on our hyperparameter search. Many non-private HPO methods have been used by prior DP papers that do not report the privacy cost of HPO, and a valuable future task would be to consider privatizing these. Multiple prior works (De et al., 2022; Cattan et al., 2022; Bu et al., 2022a;b; Mehta et al., 2023a;b; Berrada et al., 2023; Li et al., 2022b;a; Hu et al., 2021) consider the task of max- imizing the privacy utility tradeoff of finetuning pretrained models. Although the main focus of our paper is private HPO, we also critically evaluate the efficacy of a range of techniques that have been proposed by these works such as data augmentation, fine-tuning the embedding layer, and weight averaging. A detailed discussion of these techniques is deferred to Appendix B.2 and Appendix C. Golatkar et al. (2022); Nasr et al. (2023); Amid et al. (2022) treat< 10% of the private training dataset and public and use it to improve DP-SGD. Although we do not use any private data during pretraining, future work can tackle applying our method to this alternate threat model. Sander et al. (2022) suggest doing HPO with smaller batch sizes and then scaling up the HPs to the full batch update. This idea is similar in spirit to the adaptive scaling that we propose, because the HP trials are cheaper from a runtime perspective than the final run. However, our approach is not only compute-efficient but also accounts for the privacy cost of HPO. Kuo et al. (2023) find that noisy HPO in the federated setting suffers, and suggest doing HPO on public proxy data (whose existence we don’t assume) and transferring it to the private dataset. Wang et al. (2023) propose a method for private adaptive HPO that provides an RDP guarantee for Bayesian HPO. They compare their method to random search under a total privacy budget of ε = 15 , where at each iteration they sample a new set of HPs from their prior, and update their prior, and at each iteration random search samples a new set of HPs uniformly from the search space; each run has a base privacy cost, and it takes many runs for the distribution to converge. Their method can be seen as a version of ours that lacks the linear scaling prior and does not use cheap trials to find the parameters for the prior. As a result, they use much more budget in order to find a good distribution for the HPs. This more flexible approach can be superior to ours in settings where the HPs are not linear. Discussion. DP researchers commonly confront the compute-intensive, privacy-expensive task of doing grid search with hundreds of trials to optimize the privacy-utility tradeoff of DP methods. Our work provides an alternative HPO method that reduces the compute and privacy costs of grid search by an order of magnitude without compromising accuracy across 20 tasks. Researchers using our method can accelerate the pace of research by reducing the compute needed to produce good results, and address the open ques- tion of accounting for the privacy cost of hyperparameter tuning, whether they are doing transfer learning in the pres- ence of domain shifts, training from scratch, or applying PEFT methods to LLMs. 7. Impact Statement This paper presents work whose goal is to improve the qual- ity of models trained with differential privacy. Privacy is at the heart of many ongoing debates about AI. We submit that any work that makes it easier for organizations to train and release models with DP guarantees will positively benefit society. Additionally, this paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. References Abadi, M., Chu, A., Goodfellow, I., McMahan, H. B., Mironov, I., Talwar, K., and Zhang, L. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communi- cations Security. ACM, oct 2016. doi: 10.1145/2976749. 2978318. Amid, E., Ganesh, A., Mathews, R., Ramaswamy, S., Song, S., Steinke, T., Steinke, T., Suriyakumar, V . M., Thakkar, O., and Thakurta, A. Public data-assisted mirror descent for private model training. In Proceedings of the 39th International Conference on Machine Learning, pp. 517– 535. PMLR, 2022. Bao, H., Dong, L., Piao, S., and Wei, F. Beit: Bert pre- training of image transformers, 2021. URL https:// arxiv.org/abs/2106.08254. Basu, P., Roy, T. S., Naidu, R., Muftuoglu, Z., Singh, S., and Mireshghallah, F. Benchmarking differential privacy and federated learning for bert models, 2022. Berrada, L., De, S., Shen, J. H., Hayes, J., Stanforth, R., Stutz, D., Kohli, P., Smith, S. L., and Balle, B. Unlock- ing accuracy and fairness in differentially private image classification, 2023. 10A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization Bu, Z., Mao, J., and Xu, S. Scalable and efficient training of large convolutional neural networks with differential privacy. arXiv preprint arXiv:2205.10683, 2022a. Bu, Z., Wang, Y .-X., Zha, S., and Karypis, G. Differentially private bias-term only fine-tuning of foundation models, 2022b. B´andi, P., Geessink, O., Manson, Q., Van Dijk, M., Balken- hol, M., Hermsen, M., Ehteshami Bejnordi, B., Lee, B., Paeng, K., Zhong, A., Li, Q., Zanjani, F. G., Zinger, S., Fukuta, K., Komura, D., Ovtcharov, V ., Cheng, S., Zeng, S., Thagaard, J., Dahl, A. B., Lin, H., Chen, H., Jacob- sson, L., Hedlund, M., C ¸etin, M., Halıcı, E., Jackson, H., Chen, R., Both, F., Franke, J., K ¨usters-Vandevelde, H., Vreuls, W., Bult, P., van Ginneken, B., van der Laak, J., and Litjens, G. From detection of individual metas- tases to classification of lymph node status at the pa- tient level: The camelyon17 challenge. IEEE Transac- tions on Medical Imaging , 38(2):550–560, 2019. doi: 10.1109/TMI.2018.2867350. Cattan, Y ., Choquette-Choo, C. A., Papernot, N., and Thakurta, A. Fine-tuning with differential privacy neces- sitates an additional hyperparameter search, 2022. URL https://arxiv.org/abs/2210.02156. Christie, G., Fendley, N., Wilson, J., and Mukherjee, R. Functional map of the world, 2017. URL https:// arxiv.org/abs/1711.07846. Coates, A., Ng, A., and Lee, H. An analysis of single- layer networks in unsupervised feature learning. In Pro- ceedings of the fourteenth international conference on artificial intelligence and statistics, pp. 215–223. JMLR Workshop and Conference Proceedings, 2011. Cohen, G., Afshar, S., Tapson, J., and van Schaik, A. Emnist: an extension of mnist to handwritten letters, 2017. URL https://arxiv.org/abs/1702.05373. Croce, F., Andriushchenko, M., Sehwag, V ., Debenedetti, E., Flammarion, N., Chiang, M., Mittal, P., and Hein, M. Robustbench: a standardized adversarial robustness benchmark, 2021. Cutkosky, A. and Mehta, H. Momentum improves nor- malized SGD. In III, H. D. and Singh, A. (eds.), Pro- ceedings of the 37th International Conference on Ma- chine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 2260–2268. PMLR, 13–18 Jul 2020. URL https://proceedings.mlr.press/ v119/cutkosky20b.html. De, S., Berrada, L., Hayes, J., Smith, S. L., and Balle, B. Unlocking high-accuracy differentially private im- age classification through scale, 2022. URL https: //arxiv.org/abs/2204.13650. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vi- sion and Pattern Recognition, pp. 248–255, 2009. doi: 10.1109/CVPR.2009.5206848. Diffenderfer, J., Bartoldson, B. R., Chaganti, S., Zhang, J., and Kailkhura, B. A winning hand: Compress- ing deep networks can improve out-of-distribution ro- bustness, 2021. URL https://arxiv.org/abs/ 2106.09129. Dong, J., Roth, A., and Su, W. J. Gaussian differential privacy, 2019. URL https://arxiv.org/abs/ 1905.02383. Dong, J., Roth, A., Su, W. J., et al. Gaussian differential privacy. Journal of the Royal Statistical Society Series B, 84(1):3–37, 2022. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. An image is worth 16x16 words: Transformers for image recognition at scale, 2020. URL https: //arxiv.org/abs/2010.11929. Dwork, C., McSherry, F., Nissim, K., and Smith, A. Cali- brating noise to sensitivity in private data analysis. InPro- ceedings of the Third Conference on Theory of Cryptog- raphy, TCC’06, pp. 265–284, Berlin, Heidelberg, 2006. Springer-Verlag. ISBN 3540327312. Dwork, C., Kohli, N., and Mulligan, D. Differential privacy in practice: Expose your epsilons! Journal of Privacy and Confidentiality, 9, 10 2019. doi: 10.29012/jpc.689. Erlingsson, U., Feldman, V ., Mironov, I., Raghunathan, A., Talwar, K., and Thakurta, A. Amplification by shuffling: From local to central differential privacy via anonymity, 2018. URL https://arxiv.org/abs/ 1811.12469. Fang, H., Li, X., Fan, C., and Li, P. Improved convergence of differential private SGD with gradient clipping. In The Eleventh International Conference on Learning Represen- tations, 2023a. URL https://openreview.net/ forum?id=FRLswckPXQ5. Fang, Y ., Sun, Q., Wang, X., Huang, T., Wang, X., and Cao, Y . Eva-02: A visual representation for neon genesis, 2023b. Geirhos, R., Rubisch, P., Michaelis, C., Bethge, M., Wich- mann, F. A., and Brendel, W. Imagenet-trained cnns are biased towards texture; increasing shape bias im- proves accuracy and robustness, 2018. URL https: //arxiv.org/abs/1811.12231. 11A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization Ghalebikesabi, S., Berrada, L., Gowal, S., Ktena, I., Stan- forth, R., Hayes, J., De, S., Smith, S. L., Wiles, O., and Balle, B. Differentially private diffusion models generate useful synthetic images, 2023. Golatkar, A., Achille, A., Wang, Y .-X., Roth, A., Kearns, M., and Soatto, S. Mixed differential privacy in computer vision, 2022. Gopi, S., Lee, Y . T., and Wutschitz, L. Numerical com- position of differential privacy, 2021. URL https: //arxiv.org/abs/2106.02848. Goyal, P., Doll ´ar, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola, A., Tulloch, A., Jia, Y ., and He, K. Accurate, large minibatch sgd: Training imagenet in 1 hour, 2017. URL https://arxiv.org/abs/ 1706.02677. Gupta, S., Huang, Y ., Zhong, Z., Gao, T., Li, K., and Chen, D. Recovering private text in federated learning of lan- guage models. In Advances in Neural Information Pro- cessing Systems (NeurIPS), 2022. Hendrycks, D. and Dietterich, T. Benchmarking neural network robustness to common corruptions and pertur- bations, 2019. URL https://arxiv.org/abs/ 1903.12261. Hoffer, E., Ben-Nun, T., Hubara, I., Giladi, N., Hoefler, T., and Soudry, D. Augment your batch: better training with larger batches, 2019. URL https://arxiv.org/ abs/1901.09335. Hu, E. J., Shen, Y ., Wallis, P., Allen-Zhu, Z., Li, Y ., Wang, S., Wang, L., and Chen, W. Lora: Low-rank adaptation of large language models, 2021. Hulkund, N., Suriyakumar, V . M., Killian, T. W., and Ghas- semi, M. Limits of algorithmic stability for distributional generalization, 2023. URL https://openreview. net/forum?id=PoU_NgCStE5. Izmailov, P., Podoprikhin, D., Garipov, T., Vetrov, D., and Wilson, A. G. Averaging weights leads to wider op- tima and better generalization, 2018. URL https: //arxiv.org/abs/1803.05407. Kingma, D. P. and Ba, J. Adam: A method for stochastic op- timization, 2014. URL https://arxiv.org/abs/ 1412.6980. Klimt, B. and Yang, Y . The enron corpus: A new dataset for email classification research. In European conference on machine learning, pp. 217–226. Springer, 2004. Koh, P. W., Sagawa, S., Marklund, H., Xie, S. M., Zhang, M., Balsubramani, A., Hu, W., Yasunaga, M., Phillips, R. L., Gao, I., Lee, T., David, E., Stavness, I., Guo, W., Earnshaw, B. A., Haque, I. S., Beery, S., Leskovec, J., Kundaje, A., Pierson, E., Levine, S., Finn, C., and Liang, P. Wilds: A benchmark of in-the-wild distribu- tion shifts, 2020. URL https://arxiv.org/abs/ 2012.07421. Koskela, A. and Kulkarni, T. Practical differentially private hyperparameter tuning with subsampling, 2023. Krizhevsky, A. et al. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009. Kulynych, B., Yang, Y .-Y ., Yu, Y ., Błasiok, J., and Nakkiran, P. What you see is what you get: Principled deep learning via distributional generalization, 2022. Kumar, A., Raghunathan, A., Jones, R., Ma, T., and Liang, P. Fine-tuning can distort pretrained features and un- derperform out-of-distribution, 2022a. URL https: //arxiv.org/abs/2202.10054. Kumar, A., Raghunathan, A., Jones, R., Ma, T., and Liang, P. Fine-tuning can distort pretrained features and un- derperform out-of-distribution, 2022b. URL https: //arxiv.org/abs/2202.10054. Kumar, A., Shen, R., Bubeck, S., and Gunasekar, S. How to fine-tune vision models with sgd, 2022c. URL https: //arxiv.org/abs/2211.09359. Kuo, K., Thaker, P., Khodak, M., Nguyen, J., Jiang, D., Talwalkar, A., and Smith, V . On noisy evaluation in federated hyperparameter tuning, 2023. Li, H., Chaudhari, P., Yang, H., Lam, M., Ravichan- dran, A., Bhotika, R., and Soatto, S. Rethinking the hyperparameters for fine-tuning, 2020. URL https: //arxiv.org/abs/2002.11770. Li, T., Zaheer, M., Liu, K. Z., Reddi, S. J., McMahan, H. B., and Smith, V . Differentially private adaptive optimization with delayed preconditioners, 2023. Li, X., Liu, D., Hashimoto, T., Inan, H. A., Kulkarni, J., Lee, Y ., and Thakurta, A. G. When does differentially private learning not suffer in high dimensions? In Advances in Neural Information Processing Systems , pp. 28616– 28630, 2022a. Li, X., Tram`er, F., Liang, P., and Hashimoto, T. Large lan- guage models can be strong differentially private learners. In International Conference on Learning Representations, 2022b. Liu, J. and Talwar, K. Private selection from private candi- dates, 2018. 12A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization Liu, Z., Mao, H., Wu, C.-Y ., Feichtenhofer, C., Darrell, T., and Xie, S. A convnet for the 2020s, 2022. URL https://arxiv.org/abs/2201.03545. Malladi, S., Lyu, K., Panigrahi, A., and Arora, S. On the SDEs and scaling rules for adaptive gradient algo- rithms. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/ forum?id=F2mhzjHkQP. Malladi, S., Gao, T., Nichani, E., Damian, A., Lee, J. D., Chen, D., and Arora, S. Fine-tuning language models with just forward passes, 2024. McMahan, H. B., Ramage, D., Talwar, K., and Zhang, L. Learning differentially private recurrent language mod- els, 2017. URL https://arxiv.org/abs/1710. 06963. Mehta, H., Krichene, W., Thakurta, A. G., Kurakin, A., and Cutkosky, A. Differentially private image classifica- tion from features. Transactions on Machine Learning Research, 2023a. ISSN 2835-8856. URL https:// openreview.net/forum?id=Cj6pLclmwT. Ex- pert Certification. Mehta, H., Thakurta, A. G., Kurakin, A., and Cutkosky, A. Towards large scale transfer learning for differentially pri- vate image classification.Transactions on Machine Learn- ing Research, 2023b. ISSN 2835-8856. URL https:// openreview.net/forum?id=Uu8WwCFpQv. Ex- pert Certification. Mehta, R., Albiero, V ., Chen, L., Evtimov, I., Glaser, T., Li, Z., and Hassner, T. You only need a good embeddings extractor to fix spurious correlations, 2022. URLhttps: //arxiv.org/abs/2212.06254. Merity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer sentinel mixture models. In International Conference on Learning Representations, 2017. URL https:// openreview.net/forum?id=Byj72udxe. Mohapatra, S., Sasy, S., He, X., Kamath, G., and Thakkar, O. The role of adaptive optimizers for honest private hyperparameter selection, 2021. Nasr, M., Mahloujifar, S., Tang, X., Mittal, P., and Houmansadr, A. Effectively using public data in pri- vacy preserving machine learning. In Proceedings of the 40th International Conference on Machine Learning, pp. 25718–25732. PMLR, 2023. Panda, A., Mahloujifar, S., Bhagoji, A. N., Chakraborty, S., and Mittal, P. Sparsefed: Mitigating model poisoning attacks in federated learning with sparsification, 2021. URL https://arxiv.org/abs/2112.06274. Papernot, N. and Steinke, T. Hyperparameter tuning with renyi differential privacy, 2021. URL https: //arxiv.org/abs/2110.03620. Peng, Z., Dong, L., Bao, H., Ye, Q., and Wei, F. Beit v2: Masked image modeling with vector-quantized visual tokenizers, 2022. URL https://arxiv.org/abs/ 2208.06366. Polyak, B. and Juditsky, A. B. Acceleration of stochastic approximation by averaging. Siam Journal on Control and Optimization, 30:838–855, 1992. Qian, N. On the momentum term in gradient descent learn- ing algorithms. Neural networks, 12(1):145–151, 1999. Qiao, S., Wang, H., Liu, C., Shen, W., and Yuille, A. Micro-batch training with batch-channel normalization and weight standardization, 2019. URL https:// arxiv.org/abs/1903.10520. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. Language models are unsupervised multitask learners. 2019. Rahimian, H. and Mehrotra, S. Frameworks and results in distributionally robust optimization. Open Journal of Mathematical Optimization, 3:1–85, jul 2022. doi: 10. 5802/ojmo.15. URL https://doi.org/10.5802% 2Fojmo.15. Rajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing , pp. 2383– 2392, 2016. Ryu, E. K. and Boyd, S. P. A primer on monotone operator methods. 2015. Sagawa, S., Koh, P. W., Hashimoto, T. B., and Liang, P. Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case gen- eralization, 2019. URL https://arxiv.org/abs/ 1911.08731. Sander, T., Stock, P., and Sablayrolles, A. Tan without a burn: Scaling laws of dp-sgd, 2022. URL https: //arxiv.org/abs/2210.03403. Shejwalkar, V ., Ganesh, A., Mathews, R., Thakkar, O., and Thakurta, A. Recycling scraps: Improving private learn- ing by leveraging intermediate checkpoints, 2022. URL https://arxiv.org/abs/2210.01864. Shi, W., Chen, S., Zhang, C., Jia, R., and Yu, Z. Just fine- tune twice: Selective differential privacy for large lan- guage models. arXiv preprint arXiv:2204.07667, 2022. 13A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization Shorten, C. and Khoshgoftaar, T. M. A survey on image data augmentation for deep learning. Journal of Big Data, 6(1):60, Jul 2019. ISSN 2196-1115. doi: 10.1186/ s40537-019-0197-0. URL https://doi.org/10. 1186/s40537-019-0197-0 . Song, S., Chaudhuri, K., and Sarwate, A. D. Stochastic gradient descent with differentially private updates. In 2013 IEEE Global Conference on Signal and Information Processing, pp. 245–248, 2013. doi: 10.1109/GlobalSIP. 2013.6736861. Sun, Z., Suresh, A. T., and Menon, A. K. The importance of feature preprocessing for differentially private linear optimization. In International Conference on Learning Representations, 2024. Sutskever, I., Martens, J., Dahl, G., and Hinton, G. On the importance of initialization and momentum in deep learn- ing. In Dasgupta, S. and McAllester, D. (eds.), Proceed- ings of the 30th International Conference on Machine Learning, volume 28 of Proceedings of Machine Learn- ing Research, pp. 1139–1147, Atlanta, Georgia, USA, 17– 19 Jun 2013. PMLR. URL https://proceedings. mlr.press/v28/sutskever13.html. Tang, X., Panda, A., Sehwag, V ., and Mittal, P. Differen- tially private image classification by learning priors from random processes. In Advances in Neural Information Processing Systems, 2023. Tang, X., Panda, A., Nasr, M., Mahloujifar, S., and Mittal, P. Private fine-tuning of large language models with zeroth- order optimization, 2024. Team, A. Learning with privacy at scale, 2017. URL https://docs-assets.developer. apple.com/ml-research/papers/ learning-with-privacy-at-scale.pdf . Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and Jegou, H. Training data-efficient image transform- ers; distillation through attention. In Meila, M. and Zhang, T. (eds.), Proceedings of the 38th International Confer- ence on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 10347–10357. PMLR, 18–24 Jul 2021. URLhttps://proceedings.mlr. press/v139/touvron21a.html. Tram`er, F., Kamath, G., and Carlini, N. Considerations for differentially private learning with large-scale public pretraining, 2022. Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. R. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In International Conference on Learning Representations, 2019a. URL https://openreview.net/forum? id=rJ4km2R5t7. Wang, H., Gao, S., Zhang, H., Su, W. J., and Shen, M. DP- hyPO: An adaptive private framework for hyperparameter optimization. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https: //openreview.net/forum?id=3Py8A1j5N3. Wang, Y .-X., Balle, B., and Kasiviswanathan, S. P. Sub- sampled renyi differential privacy and analytical mo- ments accountant. In Chaudhuri, K. and Sugiyama, M. (eds.), Proceedings of the Twenty-Second Interna- tional Conference on Artificial Intelligence and Statis- tics, volume 89 of Proceedings of Machine Learning Research, pp. 1226–1235. PMLR, 16–18 Apr 2019b. URL https://proceedings.mlr.press/v89/ wang19b.html. Wightman, R. Pytorch image models. https://github. com/rwightman/pytorch-image-models, 2019. Wolf, T., Debut, L., Sanh, V ., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer, S., von Platen, P., Ma, C., Jernite, Y ., Plu, J., Xu, C., Scao, T. L., Gugger, S., Drame, M., Lhoest, Q., and Rush, A. M. Huggingface’s transformers: State-of-the-art natural language processing, 2019. URL https://arxiv.org/abs/1910.03771. Xiao, H., Rasul, K., and V ollgraf, R. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms, 2017. URL https://arxiv.org/abs/ 1708.07747. Yousefpour, A., Shilov, I., Sablayrolles, A., Testuggine, D., Prasad, K., Malek, M., Nguyen, J., Ghosh, S., Bharadwaj, A., Zhao, J., Cormode, G., and Mironov, I. Opacus: User- friendly differential privacy library in pytorch, 2021. URL https://arxiv.org/abs/2109.12298. Yu, D., Zhang, H., Chen, W., Yin, J., and Liu, T.-Y . Large scale private learning via low-rank reparametrization, 2021. Zhang, S., Dinan, E., Urbanek, J., Szlam, A., Kiela, D., and Weston, J. Personalizing dialogue agents: I have a dog, do you have pets too?, 2018a. Zhang, S., Dinan, E., Urbanek, J., Szlam, A., Kiela, D., and Weston, J. Personalizing dialogue agents: I have a dog, do you have pets too?, 2018b. URL https: //arxiv.org/abs/1801.07243. Zhu, Y . and Wang, Y .-X. Poission subsampled r´enyi dif- ferential privacy. In Proceedings of the 36th Interna- tional Conference on Machine Learning, pp. 7634–7642. PMLR, 2019. 14A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization A. Further Results for Computer Vision Tasks Our code is available at the following URL: https://anonymous.4open.science/r/dp-custom-32B9/README.md A.1. Experimental Set-up Hyperparameter Search Space. We use a logarithmic grid for the learning rate η ∈ [10−7, 10−4]. We use the same grid for the CIFAR training from scratch experiments, and the NLP experiments. We scale the learning rate by the batch size (the original linear scaling rule). The number of epochs depends on the maximum number of iterations that we can do in the provided time. ImageNet details. The architecture is a modernized ViT (Fang et al., 2023b) pretrained on IN-21k Deng et al. (2009) with CLIP. We use a resource-efficient finetuning approach where we create a linear layer aggregating the intermediate representations from each Transformer block, following Tang et al. (2023). We apply the method from Sun et al. (2024) to preprocess the features, allocating a budget of ε = 0.05 for the private mean estimation. For the HPO, we do 3 runs at ε = 0.1, followed by 3 runs at ε = 0.2 and a final run at ε = 0.88, which produces a cumulative privacy cost including HPO of ε = 1.0. Here we express the privacy values in terms of ε for brevity, but the actual expressions are in terms of µ, the parameter for f-DP. Because of the nature of composition, the hyperparameter search only costs us the difference in performance between ε = 0.88 and ε = 1.0, which is minimal. We search across values of T ranging from 1 (a single epoch) to 20 (the most we can do in the maximum amount of time a job will run on our cluster). (Berrada et al., 2023) report that fine-tuning the full architecture produces better results than linear probing. However, we lack the computational resources to do full fine-tuning of large transformers, but we can do linear probing of the extracted features in under an hour on a single A100. CIFAR training from scratch. We use the model from Tang et al. (2023), a WideResNet-16-4, and train only the last layer on the extracted features from previous layers. The model is pretrained on synthetic data that does not resemble real-world data. We choose this model because it is the SOTA model for CIFAR training from scratch, and we want to validate that our private HPO can produce competitive results in a setting where the zero-shot performance is poor (indeed, the zero-shot performance of this model is just random chance, because it has never seen any real images before) but the ceiling for performance is quite high. Comparison to DPAdamWOSM details. We implement DPAdamWOSM (Mohapatra et al., 2021) to the best of our ability in wosm impl.py since there is no code available, and tune the necessary hyperparameter T (# of epochs) between 1 and 200 and report the performance for the best value of T without accounting for the privacy cost of this tuning. The rest of the hyperparameter choices and model architecture mirror our own. At a high level, our linear scaling rule attempts to do a data-dependent learning rate selection, while DPAdamWOSM does a data-independent learning rate selection. It is natural that for hard tasks (ImageNet) the data-independent choice may not work well. We note that while DPAdamWOSM does not require tuning the learning rate, we still need to tune the number of epochs. Therefore, even if further tuning for DPAdamWOSM could match the utility of the linear scaling rule, it would not match the privacy guarantee. Ultimately we think these works are compatible, because we can use our HPO to tune the number of epochs in DPAdamWOSM. Comparison to Koskela & Kulkarni (2023) details. . We implement the ability to train on a subset of ImageNet in our codebase by passing the flags start idx, end idx. As an initial test, we tried doing HPO on half the dataset by passing start idx=0, end idx=625. Our code will produce a random permutation of the chunks of ImageNet (1251 total) and then load the first half. We compare η = 0.01, η= 1.0 on this half-dataset. On the full dataset, these produce very different performance; η = 0.01 achieves 81% at ε = 1.0, while η = 1.0 achieves 87%. However, on the half dataset, the first learning rate achieves 43.2% performance, while the second achieves 41.4%. Inspecting the loss curves, we find that both learning rates overfit the training dataset and do not generalize to the validation set, but the second learning rate overfits more. We then try training two models on disjoint sets of the dataset and combining them via parallel composition. This achieves 83%, which is worse than training on the entire dataset. This may be an interesting direction for future work. We tried a number of other strategies to try and scale the idea of Koskela & Kulkarni (2023) to ImageNet scale, such as weight decay, smaller learning rate, single-epoch training, etc. but were unable to produce a recipe where performance on the half-dataset was consistently positively correlated with performance on the full dataset. We suspect that there is a factor of the number of classes that is needed to properly calibrate the subsampling. 15A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization Table 12.Linear Scaling on ImageNet is competitive with (Mehta et al., 2023b) and (Mehta et al., 2023a) ε (Mehta et al., 2023b) (Mehta et al., 2023a) Ours r = η × T 0.25 75.6 - 79.0 250 0.50 79.4 86.1 81.6 750 1.00 81.1 86.8 83.2 1100 2.00 81.5 87.4 84.2 2000 10.0 81.7 - 85.4 2000 ∞ 86.9 88.9 85.7 2000 Models. We evaluate five models: two masked-image modeling transformers, beit (Bao et al., 2021) and beitv2 (Peng et al., 2022), their backbone architecture ViT (Dosovitskiy et al., 2020) at both the base and large scales, and the pure convolutional architecture convnext (Liu et al., 2022). All models are pretrained on ImageNet-21k (Deng et al., 2009). These models span a range of input resolutions: beitv2 (224x224), convnext, vit-base, vit-large (384x384), and beit (512x512) and we upsample images to the necessary input size. For text generation we use GPT-2 (Radford et al., 2019) at the smallest scale, and RoBERTa-base. Availability. Our results tune open source models from the PyTorch timm package (Wightman, 2019) using existing privacy accounting from (Gopi et al., 2021) and per-sample clipping code in (Yousefpour et al., 2021), and can be reproduced in minutes. B. Further ImageNet Results. We perform additional experiments on ImageNet with the same architecture as prior work to better understand the tradeoffs of our method. We use a ViT-g that was pretrained on laion-2b, to compare to the ViT-g models in prior work that were pretrained on JFT-4b. It is trivial that linear scaling outperforms a naive grid search, but we also compare the effectiveness of linear scaling against the hyperparameter selection strategies used in prior work (Mehta et al., 2023a). We find that use of our new rule can unlock significant improvements for a range of ε when we hold both approaches accountable for the privacy cost of hyperparameter tuning. We apply linear scaling to the ViT model used in (Mehta et al., 2023a) on CIFAR100. Although (Mehta et al., 2023a) do not directly state the hyperparameters for their best results, they specify that they use 200 hyperparameter trials with Bayesian optimization. While they obtain RDP guarantees, these guarantees do not include the privacy cost of non-privately tuning hyperparameters. We apply the linear scaling rule to extrapolate a value ofr from ε = 0.1 to ε = 1, obtaining r = 20 = η(0.2) × T(100). We recover performance of82.7% for ε = 1, a 2% improvement over the best result for DP-Adam in (Mehta et al., 2023a) while accounting for the privacy cost of hyperparameter tuning. They obtain their best result for DP-Adam at T = 10, but we cannot compute the corresponding value of r because they do not provide η. However, because they use a clipping norm of 0.005 we can reasonably infer that their value of r is ≈ 1000× smaller than ours. This is farther from the optimal non-private training, as evidenced by the performance gap. Linear Scaling scales to ImageNet In Table 12 we do a granular comparison between our method and (Mehta et al., 2023b;a). We observe that our method is competitive with (Mehta et al., 2023a) even when accounting for the privacy cost of hyperparameter search, and that the linear scaling rule holds up at the scale of ImageNet for very large values of r = η × T. The non-private accuracy of their closed-source model is 3.2% higher than our open-source model, and so the private accuracy at ε = 2 is also 3.2% higher. However, ultimately our method and the method of Mehta et al. (2023a) are complementary, because their method introduces new hyperparameters that we intuit our linear scaling rule can optimize. We attempted to validate this intuition empirically but were unable to reproduce the results of Mehta et al. (2023a) because they and Mehta et al. (2023b) pretrain on the closed- source JFT dataset with billions of images. We note that all numbers we report for models pretrained on ImageNet-21k using first-order methods surpass those in (Mehta et al., 2023a), but for sufficiently small values of ε on harder datasets the second-order methods they propose provide better performance. We note that the method in Mehta et al. (2023a) only works for vision tasks, whereas our approach works for both vision and language tasks. 16A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization ε1 ε2 εf Acc Std - - 1.0 99.00 0.01 0.01 0.05 0.99 98.88 0.01 0.05 0.1 0.96 98.85 0.03 0.05 0.2 0.9 98.81 0.01 0.1 0.2 0.88 98.81 0.01 0.2 0.3 0.7 98.79 0.03 Table 13.The marginal cost of our HPO method is low. The first row represents the oracle. The dataset is CIFAR10. The marginal cost of linear scaling is low. Table 13 shows that the marginal cost of our HPO method is low. In the case of CIFAR10, this is because the oracle at ε = 0.1 achieves > 98% accuracy. Linear Scaling produces robust results. In Fig. 3 we report that following Algorithm 2 produces new state-of-the-art results for all values of ε, shown in Table 7. In Appendix A.1 we provide detailed computations of the linear interpolation for multiple datasets and in Appendix B.4 we provide full results across the entire hyperparameter search space. 5 10 20 30 40 50 60 70 80 90 100 epochs 0.05 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 lr  = 0.05 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.10 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.20 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.30 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.40 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.50 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.60 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.70 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.80 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.90 5 10 20 30 40 50 60 70 80 90 100 epochs  = 1.00 Figure 5.Heatmaps for beit on CIFAR100. ε increases from 0.05 → 1.0 left to right on the grid-axis, iterationsT increases from 5 → 100 left to right on the individual plot axis, and the learning rate η increases from 0.05 ↓ 1 top to bottom on the individual plot axis. As ε increases, left to right, the optimal value of η × T increases in accordance with the new linear scaling rule. Prior work has generally operated in the top-left regime, that is often suboptimal. Decomposing r into η, T One of the advantages of our search method is that we combine the parameters that we need to search into one meta-parameter, the radius r, which allows us to perform linear interpolation and also allows us to improve the runtime of the intermediate HP trials. We uniformly sample r from the search space defined by rmin = ηmin ×Tmin, (rmax = ηmax ×Tmax. We evaluate 3 methods for decomposing r. 1) We decompose r by sampling η, Tfrom their search spaces until their product is close to the target r. 2) We sample T uniformly, then get η = r/T . 3) We sample η, Tuniformly from their search spaces. We don’t observe any significant difference between these methods. Note that the product of uniform distributions is not uniform. The robustness of the rule that “combinations of η, Tthat evaluate to the same product perform similarly” is crucial to the success of our method, because it enables us to fit a line rather than a more complex function that might require more evaluations. In Figure 6 and Figure 5 our results validate that this rule is robust: we can move from one set of hyperparameters to another similarly performing set of hyperparameters by increasing the number of iterations T by a constant factor and decreasing the learning rate η by the same factor (or vice versa). We find that any inaccuracy incurred by estimating the best value of r with the linear scaling rule will not reduce accuracy by much compared to doing grid search for the optimal value of r, but does reduce the privacy cost of hyperparameter tuning immensely. Method to Reduce ε ε = 0.7 Degradation from ε = 1.0 Subsampling 41.34% 45.66% Increasing Noise 84.07% 2.93% Table 14.Comparing methods to reduce privacy cost (ε) on ImageNet. Increasing noise is more effective than subsampling for reducing ε with minimal performance degradation. 17A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization 0 20 40 60 80 100 R = × T 0 1 2 3 4 5T est Accuracy Gap(Relative)    R vs TestAccuracy =[0.1,0.2,0.3,0.4,0.5] =0.05 =0.10 =0.20 =0.30 =0.40 =0.50 Figure 6.A scatter plot of r = η × T the total step size vs the relative gap in test accuracy on CIFAR100 Beitv2; this gap is measured as the difference between the test accuracy at the plotted value of r and the optimal value of r. Optimizing r for any value of ε and transferring this, e.g. via the linear scaling rule, will not reduce accuracy by much compared to the optimal hyperparameters. Figure 7.We compare the best private and best non-private performances of all models on all datasets. We use the linear scaling rule to scale hyperparameters from ε = 0.1 to ε = 1, so our privacy analysis includes the cost of hyperparameter tuning. Model Dataset ε = 1 ε = ∞ Gap beitv2 CIFAR10 98.90 99 .00 0 .10 CIFAR100 89.10 91 .57 2 .47 FMNIST 91.02 91 .53 0 .51 STL10 99.69 99 .81 0 .12 EMNIST 81.77 82 .00 0 .23 convnext CIFAR10 96.75 97 .22 0 .47 CIFAR100 83.47 86 .59 3 .12 FMNIST 90.23 91 .13 0 .9 STL10 99.61 99 .71 0 .10 EMNIST 78.38 79 .05 0 .67 beit CIFAR10 98.19 98 .51 0 .32 CIFAR100 87.1 90 .08 2 .98 FMNIST 90.55 91 .6 1 .05 STL10 99.62 99 .78 0 .16 EMNIST 81.48 83 .25 1 .77 vit-L CIFAR10 98.29 98 .44 0 .40 CIFAR100 86.18 89 .72 3 .54 FMNIST 90.58 91 .37 0 .79 STL10 99.62 99 .76 0 .14 B.1. Linear Scaling enables empirical analysis Many interesting questions in DP fine-tuning remain unanswered because of the immense computational overhead of evaluating hundreds of hyperparameter trials for each privacy budget, model architecture and dataset (Mehta et al., 2023a). We now employ the linear scaling rule to efficiently answer key questions in DP fine-tuning for vision tasks. Impact of model architectures on differential privacy Many pretrained model architectures are available (Wolf et al., 2019) but prior work has generally engaged with a single architecture, e.g. beit (Bu et al., 2022a) or ViT (Mehta et al., 2023b). We leverage our method to answer three questions: • What model architectures can provide good DP classifiers? • Is the best model task-specific, e.g., is an architecture search required? • Does the private-non private utility gap depend on the model architecture? We report our findings in Tab. 7. We evaluate multiple transformer architectures in ViT (Dosovitskiy et al., 2020), beitv1 (Bao et al., 2021) and beitv2 (Peng et al., 2022), as well as the purely convolutional architecture Convnext (Liu et al., 2022). We 18A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization Figure 8.In-distribution (ID) and out-of-distribution (OOD) performance on benchmark distribution shift datasets. We use the linear scaling rule to scale hyperparameters from ε = 0.1 to ε = 1, so our privacy analysis includes the cost of hyperparameter tuning. Dataset ε = 1.0 ID(OOD) Prior ( ε = ∞) Waterbirds 92.31 (91.59) 98.3(80.4) fMoW 45.44 (35.31) 49.1 (36.6) Camelyon 93.91 (93.55) 99.5 (96.5) C10 → STL 99.0 (98.82) 97.5 (90.7) C10 → C10p1 99.0 (97.85) 97.5 (93.5) C10 → C10C 99.0 (89.98) 96.56 (92.78) C100 → C100C 89.65 (68.69) 81.16 (72.06) find that all architectures can serve as good backbones for high-accuracy DP classification. This is somewhat surprising because the different inductive biases of transformers and purely convolutional architectures tend to produce differently structured features, but we reason that the noise added by DP will ‘smooth out’ these decision boundaries regardless of architecture. We note that one architecture, beitv2, performs the best on all benchmarks and also has the highest non-private ImageNet accuracy (Wightman, 2019). We therefore recommend that practitioners do not worry about architecture search when fine-tuning as this can incur further privacy costs, and instead pick the best model available. We are encouraged to report that the private-non private utility gap diminishes with model accuracy, enabling us to report for the first time lossless privacy of 99.0% on CIFAR10 at ε = 1 (without considering the cost of HPO) and the gap is only < 0.10% if we consider the cost of HPO. We expect that as pretrained models become even better, future works may even be able to attain lossless privacy on CIFAR100, that we note remains somewhat challenging for private fine-tuning. We harness these insights for our next analyses. DP models are robust to distribution shifts. If we assume the existence of some publicly available data for pretraining and then do DP fine-tuning on the private data, it’s crucial that there is no privacy leakage between the public data and private data. There is only 0 distribution shift when public = private, and this violates the key assumption (no privacy leakage because public and private data are sufficiently different) in DP fine-tuning. If the public data is so different from the private data that it can be used for pretraining without privacy leakage, there must be some distribution shift. Benchmarking performance on datasets with distribution shifts is increasingly important because real-world problems almost always contain distribution shift between model training and inference (Rahimian & Mehrotra, 2022). We show that DP-SGD provides robustness to covariate, subpopulation and label distribution shifts for synthetic and natural datasets. We compare to other methods that consider this question. Details on OOD Experiments We specify exact details for all OOD experiments. Our training details are drawn from prior work (Kumar et al., 2022c;b; Diffenderfer et al., 2021). Waterbirds: the ID→OOD contains a well-studied spurious correlation in the binary classification problem. (Mehta et al., 2022) evaluate vision transformers without using group knowledge and obtain ≈ 80 % ID accuracy, but much worse (≈ 60%) OOD accuracy, and (Kumar et al., 2022c) tailor their method to this task and get the reported results. Surprisingly, just fine-tuning a linear model on the extracted features outperforms both works for OOD accuracy for ε = 0.1. This trend (sacrificing ID accuracy for increased OOD robustness) is seen in other OOD results, and we hypothesize that this is due to the inherent regularization present in DP-SGD. Fmow: we train on region 3 (ID) and evaluate on regions 1,2 (OOD), following (Kumar et al., 2022b). Camelyon17: we again follow (Kumar et al., 2022b). CIFAR10 → STL10, CIFAR10p1: We train privately on CIFAR10 using our best hyperparameters returned from the linear scaling rule and then transfer this to STL10/CIFAR10p1, with the label reassignment following (Kumar et al., 2022c). Common Corruptions: We evaluate on the average severity of the ’gaussian blur’ corruption. We leverage our method to answer three questions: • Can DP help when there is a domain shift from private fine-tuning to test? 19A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization • Can DP help when there is a domain shift from public data to private fine-tuning? • Can DP fine-tuned models perform well in the zero-shot setting? In Table 8 we compare the performance of our method across 8 benchmarks and find that the answer to all three of these questions is yes. The Waterbirds dataset is a well-known benchmark for evaluating the robustness of models to spurious correlations. There is a domain shift between the private training data and the private test data created by class imbalance. We are surprised to find that in the absence of any other regularization methods, DP fine-tuning actually improves performance on the OOD split. We hypothesize that the lackluster OOD non-private performance is caused by the model overfitting to the spurious correlation in the training data, and that the inherent regularization of DP prevents the model from memorizing this spurious correlation. By comparing our results to Mehta et al. (2022) we determine that this robustness is unique to DP rather than an artifact of the pretrained model. Although DP does significantly degrade the ID performance, in situations where minimizing OOD error is more important, we believe that DP by itself can mitigate the domain shift from private fine-tuning to test. Because our central assumption in DP fine-tuning is that there is no privacy leakage from the pretraining data to the private training data, it is important to understand how DP fine-tuning performs when there is a distribution shift between public data and private data. fMoW (Christie et al., 2017) and Camelyon17 (B ´andi et al., 2019) are two datasets that represent a signficant distribution from the pretraining data (ImageNet). We observe a similar relationship between ID and OOD degradation as above, where the OOD degradation is somewhat mitigated by DP. If we compare our results on Camelyon to the best results in Ghalebikesabi et al. (2023) we find that we can improve their best performance from 91.1% at ε = 10 to 93.91% at ε = 1. Although performance on fMoW remains quite poor, we note that it is not significantly worse than in the non-private setting. We believe that DP fine-tuning from pretrained models remains a viable strategy even when the publicly available pretraining data has a very large distribution shift from the private target data. We finally consider the zero-shot setting, where we fine-tune a model on CIFAR and then transfer it without updating any parameters to private test datasets that once again represent a distribution shift from CIFAR. We report the performance in the OOD column. For the more minute distribution shifts of STL and CIFAR10p1 we find that the fine-tuned classifier can achieve remarkable performance without ever updating parameters on these datasets; that is, we just remap the labels as per (Kumar et al., 2022b). CIFAR10C and CIFAR100C represent larger distribution shifts and are used to benchmark the robustness of models to commonly reported image corruptions (Hendrycks & Dietterich, 2019). Our OOD performance on these larger distribution shifts is much worse, particularly for CIFAR100 where there is a> 20% degradation. Although this is lower than the top result on the RobustBench leaderboard (Croce et al., 2021) obtains 85% accuracy, we note that once again we used no additional methods beyond DP to ensure robustness but managed to achieve reasonable performance to distribution shifts in zero-shot classification. Comparison to other works on distribution shift under DP. Prior work in distributionally robust optimization (DRO) has addressed this problem by using knowledge of the relative imbalances between groups, but recent work with vision transformers has shown that linear probing can perform well on datasets with distribution shifts (Mehta et al., 2022; Kumar et al., 2022a;c). Kulynych et al. (2022) proposes DP-IS-SGD that improves the robustness of DP-SGD by removing per-sample gradient clipping (therefore removing the introduced bias but also losing the privacy guarantee; see 4.2) and uses knowledge of the groups to sample subpopulations at different rates to improve robustness. Because our method uses DP-GD to maximize the signal-to-noise ratio of updates and requires clipping (because our primary goal is the privacy guarantee, unlike Kulynych et al. (2022) which focuses on DRO) and we do not assume knowledge of groups, we cannot make use of DP-IS-SGD. Hulkund et al. (2023) concludes that ”[DP-SGD] is not a good candidate for improving robustness under covariate or subpopulation shift, as it comes at a major cost to accuracy.” This conclusion runs counter to our findings, and we believe the reason is because their numerical findings are not conclusive. Our interpretation of their results is that because their DP-SGD degrades accuracy, it should also increase robustness; however we find that even when DP-SGD does not degrade accuracy it still improves robustness. B.2. Detailed Ablations In this subsection we deal with detailed ablations of each step in the method that we use. We ablate each step and show their individual benefits in Table 15. At a high level, we want to maximize the signal-to-noise ratio of updates, accelerate training to minimize the impact of noise on the optimization trajectory, and apply the linear scaling rule to select the best 20A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization Table 15.Our method fixes six design choices: the architecture and initialization (for CV tasks only), the batch size (full batch), the optimizer (SGD with momentum=0.9), the accounting method (PLV where all prior HPO methods use RDP), and the clipping norm (unit clipping). We report the improvement derived from following each of these techniques with respect to a competitive baseline from prior work on CIFAR100 at ε = 0.1. Method Baseline Baseline Accuracy Improvement Classifier (no bias) (Mehta et al., 2023b) 71.3 0 .36 Zero Initialization Random Initialization (De et al., 2022) 64.85 6 .81 Gradient Descent SGD(Batch=4096) (De et al., 2022) 70.2 1 .46 Momentum (ρ = 0.9) ρ = 0 (Bu et al., 2022a) 69.02 2 .09 PLV Accounting RDP (De et al., 2022) 68.43 3 .23 Unit Clipping (C = 1) C ≪ 1 (Mehta et al., 2023a) 71.2 0 .46 hyperparameters while maintaining a given overall privacy budget. 1) Extract features from a private dataset using an open source feature extractor pretrained on a public dataset. A valid criticism of this approach in private fine-tuning is that the fine-tuning dataset can be in-distribution with the training dataset, and this may violate privacy. To address this we evaluate our method on eight datasets that have been used as distribution shift benchmarks in Sec. 5. 2) Zero-initialize a linear classifier that maps features to classes. Prior work has studied full network fine-tuning (Cattan et al., 2022; Bu et al., 2022a; De et al., 2022) but we find that by doing logistic regression on a linear classifier we minimize the number of parameters, and mitigate the curse of dimensionality. We further simplify the choice of initialization by initializing all parameters to zero. 3) Apply linear scaling to privately select the step size and number of steps. We propose a new linear scaling rule: increase either the step size η or number of steps T so that the total step size r = η × T is linear in ε. This reduces the hyperparameter search to a binary search in r. Furthermore we can do a hyperparameter search for r using a small privacy budget, and then linearly scale up this value to minimize the cost of hyperparameter search(Alg. 2). Using privacy loss accounting enables us to get competitive accuracy for privacy budgets as small as ε = 0.01, so these low-cost trials can inform better hyperparameters. our method already minimizes the private-nonprivate performance gap atε = 1.0 as we show in Table 7, so spending ε = 0.1 for hyperparameter tuning does not significantly degrade accuracy. Unless stated explicitly otherwise, all privacy-utility tradeoffs reported for our method in the main body include the privacy cost of hyperparameter tuning via the linear scaling rule. 4) Compute the full batch gradient. This optimizes the signal-to-noise ratio of the update and enables use of large step sizes (Goyal et al., 2017). We achieve 91.52% accuracy on CIFAR10 (|D| = 5e4) for ε = 0.01 when training for 100 epochs with noise multiplier σ = 2561. When the noise is divided by the batch size, the effective noise multiplier is σ |B|=5e4 ≈ 0.05 and the SNR is 1 0.05 = 20. When we use subsampling with sampling probability p = 0.2 and train for the same number of epochs under the same privacy budget, our effective noise multiplier is σ |B| = 1145 1e4 = 0.114, and the corresponding SNR of 1 0.114 = 8.7 is much worse than in the full batch setting. 5) Clip per-sample gradients to unit norm. As per Eq. 1 reducing the per-sample gradient below 1 is equivalent to reducing η (and thus reducing the step size) while simultaneously biasing optimization. By setting c = 1 we can simplify r = η × T × c to r = η × T. 6) Use privacy loss variable accounting. Gopi et al. (2021) provides a tool to calibrate Gaussian noise for the given privacy budget and add noise to the gradient: this enables budgeting for small values of ε without underestimating privacy expenditure. 7) Use momentum. Acceleration has a host of well-known benefits for optimization and is ubiquitous in non-private optimization (Qian, 1999; Kingma & Ba, 2014), but prior work has not always used momentum because it can lead DP-SGD astray when the SNR of updates is low (De et al., 2022). Because we optimize the SNR of individual updates in (4), we can make use of momentum. 21A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization 0 10 20 30 40 50 60 Epochs 10 20 30 40 50 60 70T est Acc Momentum Ablation.   Dataset=CIFAR100, Arch=beit, =0.1 = 0 = 0.9 (a) Momentum Acceleration 40 42 44 46 48 50 Epochs 65 66 67 68 69 70 71 72 73T est Acc Momentum Post-Processing.   Dataset=CIFAR100, Arch=beit, =0.1 T est Accuracy Free Step (b) Momentum Post-Processing Figure 9.Ablation of momentum parameter during training (left) and post processing of the parameter exponential moving average stored in the momentum buffer to take an extra step ’for free’ (right). Use of both methods increases performance slightly. Momentum Accelerates Convergence. Despite the exhaustive study of the acceleration of gradient descent with momen- tum done by prior work (Sutskever et al., 2013; Qian, 1999) work on DP-SGD generally eschews the use of a momentum term. A notable exception (Mehta et al., 2023a) use AdamW rather than SGD with momentum; in a later section we discuss the reason to prefer SGD with momentum. The reason to use momentum to accelerated the convergence of DP-SGD is straightforward: the exponentially moving average of noisy gradients will have higher SNR than individual gradients. Furthermore, momentum is shown to provably benefit normalized SGD (Cutkosky & Mehta, 2020). In Fig. 9 we observe that momentum complements our new linear scaling rule and accelerates convergence. Separately, we report the improvement of taking a step ’for free’ in the direction of the exponential moving average stored during training in the momentum buffer. Note that this exponential moving average is in no way tied to momentum, and it is equivalent to perform DP-SGD without acceleration, store an exponential moving average of gradients with decay parameter γ = 0.9, and then take an additional step in the direction of the stored gradient average after training has finished; we only use the momentum buffer for ease of implementation. As we discuss above when introducing the new linear scaling rule, we maximize performance by maximizing SNR and terminating training while the model is still improving. Intuitively we therefore expect that the momentum buffer will contain a good estimate of the direction of the next step that we would have taken had we continued training, and taking a step in this direction with our usual learning rate should only improve performance without any privacy loss. We use momentum with ρ = 0.9 in all other experiments and also take a ’free step’ at the end of private training. 0 10 20 30 40 50 60 Epochs 10 20 30 40 50 60 70T est Acc Batch Size Ablation.   Dataset=CIFAR100, Arch=beit, =0.1 10e3 Full Batch 1e3 5e3 (a) Batch Size 0 10 20 30 40 50 60 Epochs 10 20 30 40 50 60 70T est Accuracy Batch Size Ablation.   Dataset=CIFAR100, Arch=beit, =0.1 SGD GD (b) Gradient Descent vs SGD Figure 10.Ablation of batch size. Left: We vary the batch size using the learning rate and number of iterations tuned for full batch; all other batch sizes perform much worse. Right: We compare SGD and GD. For SGD we tune the batch size jointly with learning rate and number of iterations, arriving at a batch size of 4096 and plot the best performing run against full batch. 22A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization Full Batches Optimize Signal-to-Noise Ratio. Since its inception, the use of privacy amplification via Poisson subsam- pling and RDP has been a mainstay in the DP community (Zhu & Wang, 2019; Wang et al., 2019b; Erlingsson et al., 2018). Prior work almost universally uses privacy amplification via subsampling, but as early as (McMahan et al., 2017), and more recently in (De et al., 2022) it has become apparent that DP-SGD can actually benefit from large batch sizes because the signal-to-noise ratio (SNR) improves. Note that the noise term in 1 is divided by the batch size, so if we are willing to give up amplification via subsampling entirely, we can reduce the noise by a factor of5e4 for the benchmark computer vision tasks. In Fig. 10 we report the improvement of full-batch DP-GD over Poisson subsampled DP-SGD. We attribute the success of DP-GD to the improvement in SNR. For example, we achieve 91.52% accuracy on CIFAR10 for ε = 0.01 when training for 100 epochs with learning rate η = 0.01 and noise multiplier σ = 2561. When the noise is divided by the batch size, the effective noise multiplier is σ |B|=5e4 = 0.05 and the SNR is 1 0.051 = 20. When we use subsampling with sampling probability p = 0.2 and train for the same number of epochs under the same privacy budget, our effective noise multiplier is σ |B| = 1145 1e4 = 0.114, and the corresponding SNR of 1 0.114 = 8.7 is much worse than in the full batch setting. Although at first glance our analysis merely supports the typical conclusion that large batches are better in DP-SGD, (De et al., 2022) observe that DP-SGD is still preferrable to DP-GD because minibatching produces the optimal choice of noise multiplier. Our findings run counter to this: as discussed above, we contend that performance depends not only on the optimal noise multiplier but on our new linear scaling rule, and DP-GD unlocks the use of larger step sizes (Goyal et al., 2017). We use DP-GD instead of DP-SGD in all other experiments, removing the batch size from the hyperparameter tuning process and improving the overall privacy cost of deploying our baselines (Papernot & Steinke, 2021). B.3. A Critical Evaluation of Proposed Techniques for Fine-Tuning Prior work has proposed a number of ad-hoc techniques that improve performance in DP fine-tuning. Here we critically evaluate these techniques in the our method regime, and analyze why they reduce performance in our setting. Small Clipping Norms Bias Optimization. The standard deviation of the noise added in DP-SGD scales with the sensitivity of the update, defined by the clipping norm parameter. To decrease the amount of noise added, prior work has used very strict clipping (Mehta et al., 2023a; Bu et al., 2022a). Intuitively, if the clipping norm parameter is already chosen to be some value smaller than the norm of the unclipped gradient, the gradient estimator is no longer unbiased and this may have a negative impact on optimization. In Fig. 12 we observe that decreasing the clipping norm below 1 only degrades performance. As we can see in equation 1, further decreasing the clipping norm is equivalent to training with a smaller learning rate, and this is suboptimal because Fig. 17 indicates that we can prefer to use larger learning rates. We use a clipping norm of 1 in all other experiments. Initializing Weights to Zero Mitigates Variance in DP-GD. (Qiao et al., 2019) propose initializing the model parameters to very small values to improve the stability of micro-batch training, and (De et al., 2022) find that applying this technique to DP-SGD improves performance. In Fig. 11 we ablate the effectiveness of zero initialization with standard He initialization and find that the best performance comes from initializing the weights uniformly to zero. We initialize the classifier weights to zero in all other experiments. Weight Averaging Cannot Catch Up To Accelerated Fine-Tuning. (Shejwalkar et al., 2022) perform an in-depth empirical analysis and find that averaging the intermediate model checkpoints reduces the variance of DP-SGD and improves model performance. (De et al., 2022) first proposed the use of an Exponential Moving Average (EMA) to mitigate the noise introduced by DP-SGD. Previously, methods that use stochastic weight averaging (SWA) during SGD have been proposed and are even available by default in PyTorch (Izmailov et al., 2018). The idea of averaging weights to increase acceleration was first proposed by (Polyak & Juditsky, 1992), and is theoretically well-founded. In Fig. 13 we compare EMA and SW A with no averaging and find that no averaging performs the best. This is because weight averaging methods work well when optimization has converged and the model is plotting a trajectory that orbits around a local minima in the loss landscape (Izmailov et al., 2018). That is to say, the model’s distance from the initialization does not continually increase and at some point stabilizes so that the weight averaging method can ’catch up’. However, as discussed in Fig. 3 the optimal number of iterations for our method is to train for longer epochs without decaying the learning rate for convergence, because when the model converges the SNR decays. This is corroborated by Fig. 13, where we see that the distance from initialization is monotonically increasing. Our findings run counter to those of (Shejwalkar et al., 2022) for hyperparameters in line with our proposed linear scaling rule because we find that the best optimization regime for our method is precisely one where weight averaging can never catch up to the optimization trajectory. Therefore, the averaging methods only serve 23A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization 0 10 20 30 40 50 60 Epochs 0 10 20 30 40 50 60 70T est Accuracy Weight Initialization Ablation.   Dataset=CIFAR100, Arch=beit, =0.1 He Zero (a) Weight Initialization 30 35 40 45 50 55 60 Epochs 60 62 64 66 68 70 72T est Acc Weight Decay Ablation.   Dataset=CIFAR100, Arch=beit, =0.1 0.05 5e-05 0.005 0.0005 0 (b) Weight Decay Figure 11.Ablation of two previously proposed methods: zero initialization of parameters and weight decay. Zero initialization increases accuracy in all experiments, but weight decay only degrades performance. 50 52 54 56 58 60 Epochs 68.0 68.5 69.0 69.5 70.0 70.5 71.0 71.5 72.0 72.5T est Acc Clipping Ablation.   Dataset=CIFAR100, Arch=beit, =0.1 1.0 0.1 0.01 0.001 Figure 12.Because reducing the clipping norm is equivalent to reducing the learning rate, reducing the clipping norm below 1 only degrades performance on CIFAR100 for the beit architecture at ε = 0.1. 50 51 52 53 54 55 56 57 58 59 Epochs 65 66 67 68 69 70 71 72Accuracy Weight Averaging.  Dataset=CIFAR100, Arch=beit, =0.1 model_acc ema_acc swa_acc (a) Weight Averaging 0 2 4 6 8 Iteration 0 5 10 15 20Distance from Initialization Weight Trajectory   Dataset=CIFAR100, = 0.1 beit beitv2 (b) Weight Trajectory Figure 13.Left:Ablation of Weight Averaging. Right: Plot of distance from initialization. Weight Averaging does not improve performance because the model is monotonically moving away from the initialization and weight averaging cannot ’catch up’. 24A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization to lag one step behind no averaging. Data Augmentation Does Not Work When Freezing Embeddings. Data augmentation is used during training to bias the model towards selecting features that are invariant to the rotations we use in the augmentations. (Geirhos et al., 2018) find that feature extractors pretrained on ImageNet are naturally biased towards texture features. (De et al., 2022) eschew traditional data augmentation and instead propose the use of multiple dataset augmentations or ”batch augmentation”, first introduced by (Hoffer et al., 2019), to mitigate the variance of DP-SGD. In Fig. 14 we ablate the effectiveness of batch augmentation and find that it does not noticeably improve accuracy during transfer learning. This is because dataset augmentation changes the prior of the model when training the entire network (Shorten & Khoshgoftaar, 2019), but when we freeze all layers but the classifier, the model does not have the capacity to change to optimize for the prior introduced by data augmentation, because the embedding layer is frozen. 0 2 4 6 8 10 12 Epochs 10 20 30 40 50 60T est Acc Data Augmultation Ablation.   Dataset=CIFAR100, Arch=beitv2, =0.1 No augmult Augmult=16 (a) CIFAR100 (hard) 0 2 4 6 8 10 12 14 Epochs 40 50 60 70 80 90 100T est Acc Data Augmultation Ablation.   Dataset=CIFAR10, Arch=beitv2, =0.1 No augmult Augmult=16 (b) CIFAR10 (easy) Figure 14.Ablation of Data Augmultation on two datasets. On both datasets, Data Augmultation lags behind the baseline because there is much more training data, and even at the end, Data Augmultation does not have a noticeable improvement. Weight Decay Is Not Needed When Freezing Embeddings. Regularization methods such as weight decay are commonly used during pretraining to prevent overfitting, and the feature extractors we use are pretrained with AdamW (Dosovitskiy et al., 2020). One of the benefits of weight decay during fine-tuning is limiting the change of the embedding layer to not overfit and thus retain the features learned during pretraining (Kumar et al., 2022b). In the ongoing debate on whether to use weight decay during fine-tuning (Touvron et al., 2021), we submit that weight decay should not be used in private fine-tuning. In Fig. 11 we ablate a range of values of the weight decay parameter and observe that increasing the weight decay beyond a negligible amount (the gradient norm is ≈ 1e − 2) only decreases accuracy, and no value of the weight decay increases accuracy. There are two reasons for this. The first is that we initialize the weights of the model to zero, so we do not expect the gradients to be large. The second is that we only train the last layer, and therefore there is no need to regularize the training of the embedding layer. This supports the conclusion of (Kumar et al., 2022c) that SGD with momentum is outperforms AdamW as long as the embedding layer is not updated. B.4. Hyperparameter Ablations We provide full heatmaps and pareto frontiers for all datasets and the 3 best performing models (we do not perform a full evaluation on the ViT in order to minimize any knowledge leak for the evaluation of the linear scaling rule with the strategy in (Mehta et al., 2023a)). We note that while all of these datasets are arguably in-distribution, our focus is on comparing the regime of optimization preferred by our method to those of other works, and this is achieved by producing results on benchmark tasks. We further note that STL10 is explicitly in-distribution for the pretraining dataset (ImageNet); we only use this dataset as a temporary stand-in for evaluation on ImageNet-1k, a common benchmark in prior work (Mehta et al., 2023a) to minimize the computational burden. Hyperparameter Tuning and Selecting Epsilon. Prior work often uses unrealistic values of ε that provide no real privacy guarantee. While some prior work makes the case that hyperparameters need to be tuned even for non-private learning 25A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization Table 16.We compare the best private and best non-private test accuracy performances of our method to prior work using models pretrained on ImageNet-21k and fine-tuned on CIFAR10 and CIFAR100. Full results are in Section 5. Model Dataset ε = 0.1 ε = 1 ε = ∞ Gap (1 − ∞) our method CIFAR10 98.65 99.00 99.00 0.00 CIFAR100 81.9 89.81 91.57 1 .76 (Mehta et al., 2023a) CIFAR10 95.8 96 .3 96 .6 0 .3 CIFAR100 78.5 82 .7 85 .29 2 .59 (Bu et al., 2022a) CIFAR10 - 96.7 97 .4 0 .7 CIFAR100 - 83.0 88 .4 5 .4 (Cattan et al., 2022) CIFAR10 - 95.0 96 .4 1 .4 CIFAR100 - 73.7 82 .1 8 .4 (De et al., 2022) CIFAR10 - 94.8 96 .6 1 .8 CIFAR100 - 67.4 81 .8 14 .4 and can be chosen beforehand, we show that this is not the case. Not only are the optimal choices of key hyperparameters different between training from scratch and transfer learning (Li et al., 2020), they are also different for non-private and private transfer learning (Li et al., 2022b; De et al., 2022). We now provide guidelines for selectingε and broad intuition behind our choice to design a system that minimizes dependence on hyperparameters. For a decade the standard values of ε proposed for privacy preserving statistics queries have fallen in the range of 0.1 in line with eε ≈ 1 + ε for ε ≪ 1 (Dwork et al., 2006), and recently surveyed DP deployments generally abide by the rule of selecting ε ≈ 0.1 (Dwork et al., 2019). We know that while all small values of ε generally behave the same, every large value of ε is fundamentally different in a unique way (Dwork et al., 2019). In line with these guidelines, we only evaluate ε ∈ [0.01, 1.0] and perform most of our ablations on the most challenging task where we can see a range of performance: CIFAR100 for ε = 0.1. B.5. Theory Proposition B.1. The model training subroutine in 2 is ( √ T /σ)-GDP . Corollary B.2. Algorithm 2 is (ϵ, Φ(−ϵ·σ/ √ T + √ T /2σ))−eϵ ·Φ(−ϵ·σ/ √ T − √ T /2σ))-DP . Also, forn-fold repetition, the algorithm is (ϵ, Φ(−ϵ · σ/ √ n · T + √ n · T /2σ)) − eϵ · Φ(−ϵ · σ/ √ n · T − √ n · T /2σ))-DP Proof of Proposition 4.1: Proof. Since we are using the full batch, each iteration of the algorithm is an instantiation of the Gaussian mechanism with sensitivity of 1 and Gaussian noise with standard deviation of σ. Hence, each iteration of the mechanism is (1/σ)-GDP by Theorem 3.7 in (Dong et al., 2019). Then, since we have the adaptive composition of T of these mechanisms, the algorithm is ( √ T /σ)-GDP overall, using the composition theorem for GDP, as stated in Corollary 3.3 in (Dong et al., 2019). Proof of Corollary B.2: Proof. This directly follows from the GDP to DP conversion as stated in Corollary 2.13 in (Dong et al., 2019).Why does our HPO have low privacy cost? Our HPO has low privacy cost because of the nature of composition under GDP. Consider one sweep of our method with n = 3 that evaluates some (T1, η1, σ1), (T2, η2, σ2)) and we extrapolate (Tf , ηf , σf ), that works out to ε1 = 0.1, ε2 = 0.2, εf = 0.88. The composition for this according to (Dong et al., 2022) isµf = q nµ2 1 + nµ2 2 + µ2 f for µ1 = p T1/σ2 1. If we convert µf to εf , δ= 1e −5-DP, we arrive at a final guarantee of(1, 1e −5)-DP. The cost of HPO here in terms of the privacy utility tradeoff is actually just the marginal utility between εf = 0.88 and εt = 1.0. As we will show in Section 5, in many cases this marginal utility is negligible, and the value of cheap one-time measures that improve the performance of the rest of training such as HPO is very much worth it due to the nature of composition under GDP. Proof of Thm. 3.1 The main idea of the proof is similar to the main result in Fang et al. (2023a) but is simpler because we only prove the result for linear models. 26A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization Proof. We first apply (Ryu & Boyd, 2015) to see that gradient descent with step size 2 β > η > 2 α+β on a α-strongly convex, β-smooth function is a max(1 − ηβ, 1 − ηα)-contraction. Call this latter quantity c. Now consider a sequence of benign updates from gradient descent wt b and a sequence of noisy updates for the same dataset wt. Given the contractive property of GD , we have the following: \f\f\f(wt b − η∇f(wt b)) − (wt − η∇f(w(t))) \f\f\f ≤ c \f\f\fwt b − wt−1 b \f\f\f (1) We apply the update rule in 1 and use Eq.1 w(t+1) = w(t) − η(∇f(w(t)) + σξ) (2)\f\f\fwt+1 b − wt+1 \f\f\f = (3) = \f\f\fwt b − η∇f(wt b) − w(t) + η∇f(w(t)) − σξ \f\f\f (4) ≤ c \f\f\fwt b − w(t) \f\f\f + ηρ (5) Now we have the following \f\f\fwt − wt b \f\f\f ≤ c \f\f\fwt−1 − wt−1 b \f\f\f + ρη (6) We now proceed via induction. Assume for T − 1 the statement of Thm. 3.1 holds. By Eq.6 and the induction hypothesis we have \f\f\fwT−1 − wT−1 b \f\f\f ≤ ρη × ( T−2X i ci) (7) \f\f\fwT − wT b \f\f\f ≤ c(ρη × ( T−2X i ci)) + ρη (8) \f\f\fwT − wT b \f\f\f ≤ ρη × ( T−1X i ci). (9) ρη × ( T−1X i ci) = ρη(1 − cT ) 1 − c ρη 1 − cT 1 − c = ρη(1 − cT ) η · min(α, β) = ρ(1 − cT ) min(α, β) The intuition is clear: at iteration 0 there is no divergence. At iteration 1 there is ηρ divergence. At iteration 2 the previous divergence contracts by c and increases by ηρ, so the divergence is c1ηρ + ηρ. At iteration 3 the divergence is c2ηρ + c1ηρ + ηρ = ηρ(c2 + c + 1). It remains to show that the conditions for convexity and smoothness are satisfied for the problem at hand. For the case of, ex, training a single linear layer on top of extracted features with GD, this is easy to prove. We defer to the analysis from Panda et al. (2021), which we reproduce here for the reader’s convenience. Example B.3 (Computing the Lipschitz constant for single-layer SGD training ( Panda et al. (2021))). We compute the coordinatewise Lipschitz constant of the SGD protocol for a single layer neural network defined asσ(θx), where σ is the softmax function and θ ∈ Rd are the network parameters. For cross-entropy loss-based training using datasetD, we show that the constant c = 1 4 . Formally, sup D∈Z,θ1,θ2∈M |G(θ1, D)[i] − G(θ2, D)[i]|1 ≤ 1 4|θ1 − θ2|1 for any coordinate index i ∈ [d] 27A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization Without loss of generality, we assume that dataset D is comprised of samples of the form (x, y), where x ∈ [0, 1]m, and y ∈ {0, 1}C is the one-hot encoded representation of any of the C classes. For the single layer neural network, the model parameters are denoted by θ ∈ RC×m, and the softmax layer by the function σ(·). The neural network can thus be represented as Φ(x, θ) = σ(θx). We define g(θ, x) = ∂L(Φ(x,θ),y) ∂θ where L is the softmax cross entropy loss function. For the SGD protocol,A(u) = u, and G(θ, D) = g(θ, x). Our goal is to find a Lipschitz constant L such that, for all indices i ∈ [C] and j ∈ [m], sup x∈D,θ1,θ2 |g(θ1, x)ij − g(θ2, x)ij|1 |θ1 − θ2|1 ≤ L (10) We define intermediate variable z = θx and the neural network output distribution p = σ(z), such that both p, z∈ RC. Note, for a given target class t, the cross entropy loss function L(p, y) = −log pt where pt = eztP j ezj . Thus, g(θ, x)ij = ∂L ∂θij = CX c=1 ∂L ∂zc ∂zc ∂θij . (11) Computing the terms of (11), we have ∂L ∂zc = pt − 1 for c = t; and ∂L ∂zc = pc otherwise; and ∂zc ∂θij = xj. Thus, g(x, θ)ij = xj(pt − 1) for i = t = xjpi for i ̸= t (12) We compute the Hessian of g(x, θ)ij as: ∂g(x, θ)ij ∂θkl = xjpt(1 − pt)xl for k = t = xjpk(1 − pk)xl for k ̸= t (13) where k ∈ [C], l∈ [m]. The maximum value of the Hessian in (13), occurs at xj = xl = 1, and pt = pk = 1 2 . Thus, max i,j,k,l ∂g(x, θ)ij ∂θkl ≤ 1 4 for k = t ≤ 1 4 for k ̸= t (14) To obtain the Lipschitz constant, we first define the function h(t) = g((1 − t)θ1 + tθ2, x)ij where t ∈ [0, 1] Thus, h(0) = g(θ1, x)ij and h(1) = g(θ2, x)ij. Since, the function h(t) is differentiable everywhere in (0, 1), using Mean Value Theorem, we know that there exists a pointt∗ ∈ (0, 1) such that: h(1) − h(0) ≤ h′(t∗) where h′(t) = (θ2 − θ1)g′((1 − t)θ1 + tθ2, x)ijkl. (15) Rewriting (10), we get sup x∈D,θ1,θ2 |g(θ1, x) − g(θ2, x)|1 ≤ sup x∈D,θ1,θ2 |max i,j {g(θ1, x)ij − g(θ2, x)ij}|1 Let i∗, j∗ correspond to the indices where the maximum in the above equation occurs. Combining (14) and (15), we get: sup x∈D,θ1,θ2 |g(θ1, x)i∗j∗ − g(θ2, x)i∗j∗ |1 ≤ 1 4|θ1 − θ2|1 (16) Comparing (16) with (10) we get c = 1 4 . 28A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization C. Furthur Results for Language Modeling Tasks In general it is not feasible to do full-batch experiments for the NLP tasks because the memory requirements of LLMs are very large. We therefore do the composition with the PoissonSubsampledGaussianMechanism class in the PLD accountant (Gopi et al., 2021), ensuring that our method still accounts for the privacy cost of HPO. C.1. Related Works Li et al. (2022b) provide methods for fine-tuning large language models under DP-SGD by proposing new clipping methods to mitigate the memory burden of per-sample gradient clipping. However, they do not achieve performance comparable to non-private models when fine-tuning a pretrained model on the PersonaChat dataset. We adapt their techniques to the hyperparameter settings that we show are optimal for DP fine-tuning, and produce similar performance to non-private fine-tuning on the PersonaChat dataset. Yu et al. (2021) report compelling results by only updating a sparse subset of the LLMs with LoRA (Hu et al., 2021). We fine-tune GPT2 and RoBerta; Basu et al. (2022) also fine-tune BERT models. C.2. Experimental Set-up for Finetuning Language Models Persona-Chat: We write code based on winners of ConvAI2 competition1 and private-transformers library.2 We first do clipping norm [0.1, 0.2, 0.5, 1.0], learning rate in [2, 5, 10, 20, 50] × 10−5, batch size 64 and epochs [3, 10, 20] at ε = 3 and ε = 8 and find that the clipping norm in this range achieves almost same perplexity with other hyperparams fixed. We then do hyperparameter tuning as reported in Table 17 to finetune GPT-2. Table 17.Set of hyper-parameters used in the finetuning GPT-2. Parameter Values Clipping Norm 0.1 Learning Rate [2, 5, 10, 20, 50, 100] × 10−5 Batch Size [64, 128, 256, 512, 1024, 2048] Epochs [3, 10, 20] WikiText-2: We write code based on the HuggingFace transformers library GPT-2 example,3 source code by (Shi et al., 2022)4 and private-transformers library. The hyperparameter range for grid search is reported in Table 18. Table 18.Set of hyper-parameters for grid search to finetune GPT-2 on WikiText-2.δ = 10−6. Parameter Values Clipping Norm 1 Batch Size 2048 (Full Batch) Epochs 20 Learning Rate for ε = 0.2 [2 , 4, 6, 8, 10] × 10−4 Learning Rate for ε = 0.5 [0 .8, 1, 2] × 10−3 Enron Email: For Enron email dataset, we use the preprocessed dataset in (Gupta et al., 2022), where the non-private baseline of finetuned GPT-2 on this dataset is 7.09. The hyperparameter range for grid search is reported in Table 19. C.3. Additional Results on Persona-Chat We report the perplexity of GPT-2 on the Persona-Chat dataset at different epochs and batch size in Figure 15 (with tuned learning rate in Table 17) and we can see that larger batch size and longer epochs can achieve better perplexity, which is 1https://github.com/huggingface/transfer-learning-conv-ai. 2https://github.com/lxuechen/private-transformers. 3HuggingFace transformers GPT-2 example code. 4https://github.com/wyshi/sdp transformers 29A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization Table 19.Set of hyper-parameters for grid search to finetune GPT-2 on Enron Email dataset. δ = 1 2|Dtrain|. Parameter Values Clipping Norm 1 Batch Size 1024 Epochs 5 Learning Rate for ε = 0.1 [2 , 3, 4, 5, 6, 7, 8, 9, 10] × 10−4 Learning Rate for ε = 0.2 [0 .6, 0.8, 1, 2, 3, 4, 6, 7] × 10−3 Learning Rate for ε = 0.5 [0 .4, 0.6, 0.8, 0.9, 1, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.8, 2] × 10−2 Learning Rate for ε = 1.0 [1 , 2, 3, 4, 5, 6, 7, 8] × 10−2 Learning Rate for ε = 2.0 [2 , 3, 4, 5, 6, 7, 8, 9, 10] × 10−2 Learning Rate for ε = 3.0 [0 .6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.6, 1.8, 2.0] × 10−1 consistent with our linear scale rule. Besides, we also investigate fine-tuning multiple layers. With letting the embedding layer and last LayerNorm layer in transformer trainable, we consider fine-tuning only last block in transformer, first and last block in transformer and report the result in Table 20 and we can see that the best perplexity is achieved by fine-tuning the whole model. 64 128 256 512 1024 Batch Size 20 10 3 Epochs 19.56 18.98 18.39 17.94 17.91 20.34 19.69 19.07 18.68 18.33 22.53 21.68 21.34 20.28 20.25 Ppl of finetuned GPT2 on Persona-Chat ( =3) 18 19 20 21 22 (a) ε = 3 64 128 256 512 1024 Batch Size 20 10 3 Epochs 18.95 18.40 17.83 17.34 17.27 19.61 19.01 18.43 18.03 17.65 21.36 20.60 20.38 19.41 19.33 Ppl of finetuned GPT2 on Persona-Chat ( =8) 17.5 18.0 18.5 19.0 19.5 20.0 20.5 21.0 (b) ε = 8 Figure 15.Comparison of perplexity at different batch size and epochs of GPT-2 on Persona-Chat dataset. Table 20.Finetuning GPT-2 on Persona-Chat dataset including full model and different layers of model. We also include non-private baseline. ε 3 8 Full 17.91 17 .27 Last Block 19.80 19 .20 First-Last-Block 18.93 18 .26 C.4. Addtional Results on WikiText-2 We run the grid-search experiment for ε ∈ {0.2, 0.5, 1, 2, 3} to evaluate the performance gap between the optimal total step size and the estimated total step size.5) and present the result in Figure 16. The linear rule scales well from ε ∈ {0.2, 0.5} to ε = 1. Though for ε ∈ {2, 3} the perplexity of total step size by linear scale rule is slightly higher than the optimal perplexity of total step size by grid search, the result by linear scale is better than previous SOTA (Shi et al., 2022), which is 28.84 at (ε = 3, δ= 10−6) by training 20 iterations. 5Due to the limit of computation resources, all experiments are done by training for 20 iterations. Further increasing the number of iterations will help improve the utility as shown by previous study (Li et al., 2022b; Shi et al., 2022), we leave longer iterations for further study. 30A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization 0.5 1.0 1.5 2.0 2.5 3.0 27.5 28.0 28.5Perplexity Perplexity by linear scaling Perplexity by grid search (a) Pareto Frontier for ε vs Test Perplexity 0.5 1.0 1.5 2.0 2.5 3.0 0.00 0.05 0.10 0.15 0.20 0.25Total step size ( × T)  T otal step size by linear scaling T otal step size by grid search (b) Pareto Frontier for ε vs Total Step Size Figure 16.The linear scaling rule (accounting for the privacy cost of hyperparameter tuning) is competitive with grid search (non-private, doing N trials each with the given ε) in range [0.2, 1.0] on the WikiText-2 dataset. Left: y-axis is Perplexity (lower is better). 1 5 10 20 30 40 50 60 70 80 90 100 epochs 0.01 0.05 0.1 0.15 0.2 0.25 0.5 1.0 lr  = 0.01 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.05 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.10 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.20 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.30 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.40 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.50 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.60 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.70 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.80 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.90 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 1.00 (a) CIFAR10 Beitv2 1 5 10 20 30 40 50 60 70 80 90 100 epochs 0.01 0.05 0.1 0.15 0.2 0.25 0.5 1.0 lr  = 0.01 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.05 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.10 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.20 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.30 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.40 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.50 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.60 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.70 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.80 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.90 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 1.00 (b) CIFAR10 Beit 1 5 10 20 30 40 50 60 70 80 90 100 epochs 0.01 0.05 0.1 0.15 0.2 0.25 0.5 1.0 lr  = 0.01 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.05 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.10 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.20 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.30 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.40 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.50 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.60 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.70 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.80 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.90 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 1.00 (c) CIFAR10 Convnext 1 5 10 20 30 40 50 60 70 80 90 100 epochs 0.01 0.05 0.1 0.15 0.2 0.25 0.5 1.0 lr  = 0.01 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.05 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.10 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.20 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.30 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.40 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.50 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.60 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.70 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.80 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.90 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 1.00 (d) CIFAR100 Beitv2 1 5 10 20 30 40 50 60 70 80 90 100 epochs 0.01 0.05 0.1 0.15 0.2 0.25 0.5 1.0 lr  = 0.01 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.05 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.10 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.20 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.30 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.40 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.50 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.60 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.70 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.80 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.90 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 1.00 (e) CIFAR100 Beit 1 5 10 20 30 40 50 60 70 80 90 100 epochs 0.01 0.05 0.1 0.15 0.2 0.25 0.5 1.0 lr  = 0.01 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.05 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.10 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.20 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.30 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.40 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.50 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.60 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.70 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.80 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.90 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 1.00 (f) CIFAR100 Convnext Figure 17.Heatmaps for the reported datasets and architectures; lighter is better. Note that the scale of the axes differs from the heatmaps in the main body; this will be fixed in a future update. ε increases left to right with a different value for each heatmap according to: [0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0], epochs increase from left to right on the x-axis of each heatmap according to: [1, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100], and the learning increases from top to bottom on the y-axis of each heatmap according to: [0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.5, 1.0]. As ε increases, left to right, the optimal hyperparameters trend towards longer training with lower learning rates (bottom right). 31A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization 1 5 10 20 30 40 50 60 70 80 90 100 epochs 0.01 0.05 0.1 0.15 0.2 0.25 0.5 1.0 lr  = 0.01 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.05 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.10 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.20 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.30 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.40 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.50 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.60 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.70 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.80 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.90 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 1.00 (a) STL10 Beitv2 1 5 10 20 30 40 50 60 70 80 90 100 epochs 0.01 0.05 0.1 0.15 0.2 0.25 0.5 1.0 lr  = 0.01 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.05 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.10 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.20 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.30 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.40 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.50 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.60 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.70 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.80 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.90 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 1.00 (b) STL10 Beit 1 5 10 20 30 40 50 60 70 80 90 100 epochs 0.01 0.05 0.1 0.15 0.2 0.25 0.5 1.0 lr  = 0.01 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.05 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.10 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.20 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.30 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.40 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.50 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.60 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.70 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.80 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.90 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 1.00 (c) STL10 Convnext 1 5 10 20 30 40 50 60 70 80 90 100 epochs 0.01 0.05 0.1 0.15 0.2 0.25 0.5 1.0 lr  = 0.01 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.05 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.10 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.20 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.30 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.40 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.50 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.60 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.70 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.80 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.90 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 1.00 (d) FashionMNIST Beitv2 1 5 10 20 30 40 50 60 70 80 90 100 epochs 0.01 0.05 0.1 0.15 0.2 0.25 0.5 1.0 lr  = 0.01 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.05 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.10 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.20 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.30 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.40 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.50 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.60 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.70 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.80 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.90 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 1.00 (e) FashionMNIST Beit 1 5 10 20 30 40 50 60 70 80 90 100 epochs 0.01 0.05 0.1 0.15 0.2 0.25 0.5 1.0 lr  = 0.01 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.05 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.10 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.20 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.30 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.40 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.50 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.60 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.70 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.80 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.90 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 1.00 (f) FashionMNIST Convnext Figure 18.Heatmaps for the reported datasets and architectures; lighter is better. Note that the scale of the axes differs from the heatmaps in the main body; this will be fixed in a future update. ε increases left to right with a different value for each heatmap according to: [0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0], epochs increase from left to right on the x-axis of each heatmap according to: [1, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100], and the learning increases from top to bottom on the y-axis of each heatmap according to: [0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.5, 1.0]. As ε increases, left to right, the optimal hyperparameters trend towards longer training with lower learning rates (bottom right). 32A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization 0.0 0.2 0.4 0.6 0.8 1.0 0 20 40 60 80T est Accuracy  vs Test Accuracy on Dataset=CIFAR100 beit beitv2 (a) CIFAR100 Test Accuracy 0.0 0.2 0.4 0.6 0.8 1.0 0 10 20 30 40 50 60 70 80Total Step Size (  × T)  vs Total Step Size on Dataset=CIFAR100 beit beitv2 (b) CIFAR100 Total Step Size 0.0 0.2 0.4 0.6 0.8 1.0 86 88 90 92 94 96 98T est Accuracy  vs Test Accuracy on Dataset=CIFAR10 beit convnext beitv2 (c) CIFAR10 Test Accuracy 0.0 0.2 0.4 0.6 0.8 1.0 0 20 40 60 80Total Step Size (  × T)  vs Total Step Size on Dataset=CIFAR10 beit convnext beitv2 (d) CIFAR10 Total Step Size Figure 19.Pareto frontier for ε vs test accuracy and total step size for CIFAR10, and CIFAR100. Beitv2 excels for larger values ofε but beit and convnext are better for smaller values of ε. The inflection point varies across datasets. 33A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization 0.0 0.2 0.4 0.6 0.8 1.0 80 82 84 86 88 90T est Accuracy  vs Test Accuracy on Dataset=FashionMNIST beit convnext beitv2 (a) FashionMNIST Test Accuracy 0.0 0.2 0.4 0.6 0.8 1.0 0 20 40 60 80Total Step Size (  × T)  vs Total Step Size on Dataset=FashionMNIST beit convnext beitv2 (b) FashionMNIST Total Step Size 0.0 0.2 0.4 0.6 0.8 1.0 20 30 40 50 60 70 80 90 100T est Accuracy  vs Test Accuracy on Dataset=STL10 beit convnext beitv2 (c) STL10 Test Accuracy 0.0 0.2 0.4 0.6 0.8 1.0 0 2 4 6 8Total Step Size (  × T)  vs Total Step Size on Dataset=STL10 beit convnext beitv2 (d) STL10 Total Step Size Figure 20.Pareto frontier for ε vs test accuracy and total step size for STL10 and FashionMNIST. Beitv2 excels for larger values of ε but beit and convnext are better for smaller values of ε. The inflection point varies across datasets. 34",
      "meta_data": {
        "arxiv_id": "2212.04486v3",
        "authors": [
          "Ashwinee Panda",
          "Xinyu Tang",
          "Saeed Mahloujifar",
          "Vikash Sehwag",
          "Prateek Mittal"
        ],
        "published_date": "2022-12-08T18:56:37Z",
        "pdf_url": "https://arxiv.org/pdf/2212.04486v3.pdf"
      }
    },
    {
      "title": "Efficient Hyperparameter Optimization with Adaptive Fidelity Identification",
      "abstract": "Hyperparameter optimization (HPO) and neural architecture search (NAS) are\npowerful in attaining state-of-the-art machine learning models, with Bayesian\noptimization (BO) standing out as a mainstream method. Extending BO into the\nmulti-fidelity setting has been an emerging research topic, but faces the\nchallenge of determining an appropriate fidelity for each hyperparameter\nconfiguration to fit the surrogate model. To tackle the challenge, we propose a\nmulti-fidelity BO method named FastBO, which adaptively decides the fidelity\nfor each configuration and efficiently offers strong performance. The\nadvantages are achieved based on the novel concepts of efficient point and\nsaturation point for each configuration.We also show that our adaptive fidelity\nidentification strategy provides a way to extend any single-fidelity method to\nthe multi-fidelity setting, highlighting its generality and applicability.",
      "full_text": "FastBO: Fast HPO and NAS with Adaptive Fidelity Identification Jiantong Jiang and Ajmal Mian The University of Western Australia, Perth WA 6009, Australia {jiantong.jiang@research.,ajmal.mian@}uwa.edu.au Abstract. Hyperparameter optimization (HPO) and neural architec- ture search (NAS) are powerful in attaining state-of-the-art machine learning models, with Bayesian optimization (BO) standing out as a mainstream method. Extending BO into the multi-fidelity setting has been an emerging research topic, but faces the challenge of determining an appropriate fidelity for each hyperparameter configuration to fit the surrogate model. To tackle the challenge, we propose a multi-fidelity BO method named FastBO, which adaptively decides the fidelity for each configuration and efficiently offers strong performance. The advantages are achieved based on the novel concepts ofefficient pointand saturation point for each configuration. We also show that our adaptive fidelity iden- tification strategy provides a way to extend any single-fidelity method to the multi-fidelity setting, highlighting its generality and applicability. Keywords: HPO · NAS · Multi-fidelity 1 Introduction HPO [9] and NAS [7] aim to find the hyperparameter configuration or archi- tecture λ∗ that minimizesf(λ), the performance obtained by configurationλ. BO [2,10,30] is an effective model-based method for HPO and NAS. It maintains asurrogate modelof the performancebasedon past evaluations ofconfigurations, which guides the choice of promising configurations to evaluate. Recent studies onBOhaveexploredexpertpriors[11,20,26,29],derivativeinformation[1,27,35], and enhancing the interpretability [5,36–39] of HPO and NAS [3,24,25]. However, standard BO requires full evaluations of configurations, which in- curssignificantcosts,especiallyconsideringtheescalatingmodelevaluationover- head. Despite efforts to accelerate model evaluation [13,15–17], smart strategies are required to widely adopt HPO and NAS. Thus, multi-fidelity methods have been proposed [4,12,21,22], where the fidelities mean the performance levels obtained under various resource levels. They follow the idea of successive halv- ing (SHA) [12]: initially, they evaluate many random configurations using few resources; then, based on the low-fidelity performances, only the well-performing ones successively continue to be evaluated with increasing resources. Follow-up studies [8,19,23,28,33] propose model-based multi-fidelity meth- ods, replacing random sampling with more informed models to improve samplearXiv:2409.00584v1  [cs.LG]  1 Sep 20242 J. Jiang and A. Mian efficiency. However, they are based on SHA, which assumes that learning curves of different configurations rarely intersect - a condition that often fails in prac- tice [32], i.e., early performance observations cannot always indicate the final fidelity performance. This raises a key challenge:What is the appropriate fidelity for each configuration to fit the surrogate model?In other words, which fidelity can offer observations that reliably indicate the final fidelity performance? Cur- rent methods struggle with this. Hyper-Tune [23] and BOHB [8] fit separate models for different fidelities, missing inter-fidelity correlations. FTBO [31] and A-BOHB [19] fit joint models but require strong assumptions. Salinas et al. [28] use the last observed fidelity performance, which may get inaccurate surrogate models as it widens the gap between poorly- and well-performing configurations. This paper is an extended abstract of our conference paper [14], highlighting key ideas and the main experimental results, while omitting finer details. 2 Key Idea of FastBO We propose a multi-fidelity extension of BO, namely FastBO, which tackles the challenge of deciding the appropriate fidelity for each configuration to fit the surrogate model. Here, we first propose the key concepts ofefficient point and saturation point, which are crucial in the optimization process. Then, we briefly describe the process of FastBO and highlight its generality. 2.1 Efficient Point and Saturation Point We first formally define the efficient point as follows. Definition 1 (Efficient point). For a given learning curveCi(r) of hyper- parameter configuration or architectureλi, wherer represents the resource level (also referred to as fidelity), the efficient pointei of λi is defined as:ei = min{r | Ci(r) − Ci(2r) < δ1}, whereδ1 is a predefined small threshold. The semantic of Definition 1 is that starting from the efficient point onwards, whentheresourcesaredoubled,theperformanceimprovementfallsbelowasmall threshold. Consequently, this point signifies a fidelity of performance achieved with comparably efficient resource usage. Thus, we make the following remark. Remark 1. The efficient points of the configurations can serve as their ap- propriate fidelities used for fitting the surrogate model. This is due to their (i) optimal resource-to-performance balance, (ii) ability to capture valuable learning curve trends, and (iii) customization for different hyperparameter configurations. We elaborate on the reasons as follows. Firstly, efficient points balance the trade- off between computational cost and result quality. Beyond the efficient point, allocating additional resources becomes less efficient. Secondly, efficient points capture valuable behaviors within the learning curves, enabling more informed decision-making. Thirdly, the ability to customize the fidelity for each specific configuration is an advantage. This adaptive approach is more reasonable than previous studies that use a fixed fidelity for all the configurations.FastBO: Fast HPO and NAS with Adaptive Fidelity Identification 3 1 FastBO Methodology•Process overviewWarm-up Learning curve modelingEfficient & saturation pointsConfig !! •Evaluate !! to the warm-up point to get early observation set.•Stop some bad configs. •Estimate !!’s learning curve from its observation set. Post-processing……•Adaptively extract the points from the learning curve. •Stop evaluating at efficient point.•Fit surrogate model. •Resume some best-performing configs to saturation point.Optimal config Fig. 1:Main process of FastBO. FastBO involves estimating efficient and saturation points, modeling learning curves, and auxiliary stages of warm-up and post-processing. Besides efficient points, we identify saturation points for all configurations as well. We provide the definition of the saturation point as follows. Definition 2 (Saturation point).For a given learning curveCi(r) of config- uration λi, wherer represents the resource level (also referred to as fidelity), the saturation pointsi of λi is defined as:si = min{r | ∀r′ > r,|Ci(r′)−Ci(r)| < δ2}, where δ2 is a predefined small threshold. The semantic of Definition 2 is that beyond the saturation point, the observed performance no longer exhibits notable variations with more resources. Thus, this point characterizes the fidelity at which the performance of a configuration stabilizes. Building on the above definition, we make the following remark. Remark 2. The saturation points of the configurations can serve as their ap- proximate final fidelities, as they provide performance results that meet predefined quality thresholds while reducing resource wastage. 2.2 FastBO Process and Generalization With the two crucial points, we show the main process of FastBO in Fig. 1. Each configurationλi first enters a warm-up stage to get its early observation set. Some configurations are terminated here if they are detected consecutive performance deterioration. Then, FastBO estimates the learning curve ofλi from its observation set. Thus, the efficient point and saturation points are adaptively extracted. After that, λi continues to be evaluated to its efficient point; the result is used to update the surrogate model. Finally, the post-processing stage let a small set of promising configurations resume evaluating to their saturation points, and the optimal configurations can be obtained. Generalizing FastBO to single-fidelity methods.The inefficiency of single- fidelity methods like BO stems from their reliance on expensive final fidelity evaluations. Notably, low-fidelity evaluations provide informative insights but are computationally cheaper. Therefore, we can extend single-fidelity methods to the multi-fidelity setting by acquiring the low-fidelity performance for each configuration to fit the surrogate model. To do this, we need to determine the fidelity used to fit the surrogate model. FastBO adaptively determines this fi- delity for each configuration by identifying its efficient point. While this adaptive identification strategy is described in the context of model-based methods, it can4 J. Jiang and A. Mian Fashion-MNIST Airlines Albert CovertypeChristine Fig. 2:Anytime performance on the LCBench benchmark. Slice CIFAR-10 CIFAR-100 ProteinImageNet16-120 (a) NAS-Bench-201 benchmark (b) FCNet benchmark Fig. 3:Anytime performance on(a) NAS-Bench-201 and(b) FCNet. be generalized to various single-fidelity methods. For example, when evaluating configurations within the population for an evolutionary algorithm-based HPO method, we can similarly evaluate the efficient point performances instead of the final performances and integrate them in the subsequent processes, such as selection and variation. To conclude, the proposed strategy provides a simple but effective way to bridge the gap between single- and multi-fidelity methods. 3 Experimental Evaluation We compare the performance of FastBO with random search (RS), standard BO [30], ASHA [21], Hyperband [22], PASHA [4], A-BOHB [19], A-CQR [28], BOHB [8], DyHPO [34], and Hyper-Tune [23]. The results on the LCBench [40], NAS-Bench-201 [6], and FCNet [18] benchmarks are shown in Figs. 2 and 3. Overall, FastBO can handle various performance metrics and shows strong any- time performance. We can observe that FastBO gains an advantage earlier than other methods, rapidly converging to the global optimum after the initial phase. 4 Conclusion and Discussion We propose FastBO, a model-based multi-fidelity HPO method, which excels in adaptively identifying the fidelity for each configuration to fit the surrogate modelandefficientlyprovidinghigh-qualityperformance.Theproposed adaptive fidelity identification strategy also provides a simple way to extend any single- fidelity method to the multi-fidelity setting. While this paper provides a strong foundation on HPO and NAS, we see challenges that demand future improve- ments. Future work could refine and expand Fast-BO to larger search spaces and distributed computing systems to improve its applicability and scalablity.FastBO: Fast HPO and NAS with Adaptive Fidelity Identification 5 References 1. Ament, S.E., Gomes, C.P.: Scalable first-order Bayesian Optimization via struc- tured automatic differentiation. In: International Conference on Machine Learning. pp. 500–516. PMLR (2022) 1 2. Bergstra, J., Bardenet, R., Bengio, Y., Kégl, B.: Algorithms for hyper-parameter optimization. Advances in Neural Information Processing Systems24 (2011) 1 3. Bischl, B., Binder, M., Lang, M., Pielok, T., Richter, J., Coors, S., Thomas, J., Ullmann, T., Becker, M., Boulesteix, A.L., et al.: Hyperparameter optimization: Foundations, algorithms, best practices, and open challenges. Wiley Interdisci- plinary Reviews: Data Mining and Knowledge Discovery13(2), e1484 (2023) 1 4. Bohdal, O., Balles, L., Wistuba, M., Ermis, B., Archambeau, C., Zappella, G.: PASHA: efficient HPO and NAS with progressive resource allocation. In: Interna- tional Conference on Learning Representations. OpenReview.net (2023) 1, 4 5. Chen, C., Li, O., Tao, D., Barnett, A., Rudin, C., Su, J.K.: This looks like that: deep learning for interpretable image recognition. Advances in Neural Information Processing Systems32 (2019) 1 6. Dong, X., Yang, Y.: NAS-Bench-201: Extending the scope of reproducible neu- ral architecture search. In: International Conference on Learning Representations (2020) 4 7. Elsken, T., Metzen, J.H., Hutter, F.: Neural architecture search: A survey. The Journal of Machine Learning Research20(1), 1997–2017 (2019) 1 8. Falkner, S., Klein, A., Hutter, F.: BOHB: Robust and efficient hyperparameter optimization at scale. In: International Conference on Machine Learning. pp. 1437– 1446. PMLR (2018) 1, 2, 4 9. Feurer, M., Hutter, F.: Hyperparameter optimization. Automated Machine Learn- ing: Methods, Systems, Challenges pp. 3–33 (2019) 1 10. Hutter, F., Hoos, H.H., Leyton-Brown, K.: Sequential model-based optimization for general algorithm configuration. In: Learning and Intelligent Optimization. pp. 507–523. Springer (2011) 1 11. Hvarfner,C.,Stoll,D.,Souza,A.L.F.,Lindauer,M.,Hutter,F.,Nardi,L.:$\\pi$BO: Augmenting acquisition functions with user beliefs for bayesian optimization. In: International Conference on Learning Representations. OpenReview.net (2022) 1 12. Jamieson, K., Talwalkar, A.: Non-stochastic best arm identification and hyperpa- rameter optimization. In: Artificial Intelligence and Statistics. pp. 240–248. PMLR (2016) 1 13. Jiang, J., Wen, Z., Mansoor, A., Mian, A.: Fast parallel exact inference on Bayesian networks. In: ACM SIGPLAN Annual Symposium on Principles and Practice of Parallel Programming. pp. 425–426 (2023) 1 14. Jiang, J., Wen, Z., Mansoor, A., Mian, A.: Efficient hyperparameter optimization with adaptive fidelity identification. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 26181–26190 (2024) 2 15. Jiang, J., Wen, Z., Mansoor, A., Mian, A.: Fast inference for probabilistic graphical models. In: 2024 USENIX Annual Technical Conference (USENIX ATC 24) (2024) 1 16. Jiang, J., Wen, Z., Mian, A.: Fast parallel bayesian network structure learning. In: IEEE International Parallel and Distributed Processing Symposium. pp. 617–627. IEEE (2022) 1 17. Jiang, J., Wen, Z., Yang, P., Mansoor, A., Mian, A.: Fast-pgm: Fast probabilistic graphical model learning and inference. arXiv preprint arXiv:2405.15605 (2024) 16 J. Jiang and A. Mian 18. Klein, A., Hutter, F.: Tabular benchmarks for joint architecture and hyperparam- eter optimization. arXiv preprint arXiv:1905.04970 (2019) 4 19. Klein, A., Tiao, L.C., Lienart, T., Archambeau, C., Seeger, M.: Model-based asynchronous hyperparameter and neural architecture search. arXiv preprint arXiv:2003.10865 (2020) 1, 2, 4 20. Li, C., Rana, S., Gupta, S., Nguyen, V., Venkatesh, S., Sutti, A., de Celis Leal, D.R.,Slezak,T.,Height,M.,Mohammed,M.,Gibson,I.:Acceleratingexperimental design by incorporating experimenter hunches. In: International Conference on Data Mining. pp. 257–266. IEEE Computer Society (2018) 1 21. Li,L.,Jamieson,K.,Rostamizadeh,A.,Gonina,E.,Ben-Tzur,J.,Hardt,M.,Recht, B., Talwalkar, A.: A system for massively parallel hyperparameter tuning. Proceed- ings of Machine Learning and Systems2, 230–246 (2020) 1, 4 22. Li, L., Jamieson, K., DeSalvo, G., Rostamizadeh, A., Talwalkar, A.: Hyperband: A novel bandit-based approach to hyperparameter optimization. Journal of Machine Learning Research18(1), 6765–6816 (2017) 1, 4 23. Li, Y., Shen, Y., Jiang, H., Zhang, W., Li, J., Liu, J., Zhang, C., Cui, B.: Hyper- tune: Towards efficient hyper-parameter tuning at scale. Proceedings of the VLDB Endowment 15(6), 1256–1265 (2022) 1, 2, 4 24. Moosbauer, J., Casalicchio, G., Lindauer, M., Bischl, B.: Improving accuracy of interpretability measures in hyperparameter optimization via Bayesian algorithm execution. arXiv preprint arXiv:2206.05447 (2022) 1 25. Moosbauer, J., Herbinger, J., Casalicchio, G., Lindauer, M., Bischl, B.: Explaining hyperparameter optimization via partial dependence plots. Advances in Neural Information Processing Systems34, 2280–2291 (2021) 1 26. Oh, C., Gavves, E., Welling, M.: BOCK: Bayesian optimization with cylindrical kernels. In: International Conference on Machine Learning. pp. 3868–3877. PMLR (2018) 1 27. Padidar, M., Zhu, X., Huang, L., Gardner, J., Bindel, D.: Scaling gaussian pro- cesses with derivative information using variational inference. Advances in Neural Information Processing Systems34, 6442–6453 (2021) 1 28. Salinas, D., Golebiowski, J., Klein, A., Seeger, M.W., Archambeau, C.: Optimizing hyperparameters with conformal quantile regression. In: International Conference on Machine Learning. vol. 202, pp. 29876–29893. PMLR (2023) 1, 2, 4 29. Shahriari, B., Bouchard-Côté, A., Freitas, N.: Unbounded Bayesian Optimization via regularization. In: Artificial intelligence and statistics. pp. 1168–1176. PMLR (2016) 1 30. Snoek, J., Larochelle, H., Adams, R.P.: Practical Bayesian optimization of machine learning algorithms. Advances in Neural Information Processing Systems25 (2012) 1, 4 31. Swersky, K., Snoek, J., Adams, R.P.: Freeze-thaw Bayesian optimization. arXiv preprint arXiv:1406.3896 (2014) 2 32. Viering, T., Loog, M.: The shape of learning curves: a review. IEEE Transactions on Pattern Analysis and Machine Intelligence (2022) 2 33. Wang, J., Xu, J., Wang, X.: Combination of hyperband and bayesian optimization for hyperparameter optimization in deep learning. arXiv preprint arXiv:1801.01596 (2018) 1 34. Wistuba, M., Kadra, A., Grabocka, J.: Supervising the multi-fidelity race of hy- perparameter configurations. Advances in Neural Information Processing Systems 35, 13470–13484 (2022) 4 35. Wu, J., Poloczek, M., Wilson, A.G., Frazier, P.: Bayesian optimization with gradi- ents. Advances in neural information processing systems30 (2017) 1FastBO: Fast HPO and NAS with Adaptive Fidelity Identification 7 36. Yang,P.,Akhtar,N.,Jiang,J.,Mian,A.:Backdoor-basedexplainableaibenchmark forhighfidelityevaluationofattributionmethods.arXivpreprintarXiv:2405.02344 (2024) 1 37. Yang, P., Akhtar, N., Shah, M., Mian, A.: Regulating model reliance on non-robust features by smoothing input marginal density. arXiv preprint arXiv:2407.04370 (2024) 1 38. Yang, P., Akhtar, N., Wen, Z., Mian, A.: Local path integration for attribution. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 37, pp. 3173–3180 (2023) 1 39. Yang, P., Akhtar, N., Wen, Z., Shah, M., Mian, A.: Re-calibrating feature attri- butions for model interpretation. In: International Conference on Learning Repre- sentations (2022) 1 40. Zimmer, L., Lindauer, M.T., Hutter, F.: Auto-Pytorch: Multi-fidelity metalearn- ing for efficient and robust AutoDL. IEEE Transactions on Pattern Analysis and Machine Intelligence43, 3079–3090 (2021) 4",
      "meta_data": {
        "arxiv_id": "2409.00584v1",
        "authors": [
          "Jiantong Jiang",
          "Ajmal Mian"
        ],
        "published_date": "2024-09-01T02:40:04Z",
        "pdf_url": "https://arxiv.org/pdf/2409.00584v1.pdf"
      }
    },
    {
      "title": "A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization",
      "abstract": "An open problem in differentially private deep learning is hyperparameter\noptimization (HPO). DP-SGD introduces new hyperparameters and complicates\nexisting ones, forcing researchers to painstakingly tune hyperparameters with\nhundreds of trials, which in turn makes it impossible to account for the\nprivacy cost of HPO without destroying the utility. We propose an adaptive HPO\nmethod that uses cheap trials (in terms of privacy cost and runtime) to\nestimate optimal hyperparameters and scales them up. We obtain state-of-the-art\nperformance on 22 benchmark tasks, across computer vision and natural language\nprocessing, across pretraining and finetuning, across architectures and a wide\nrange of $\\varepsilon \\in [0.01,8.0]$, all while accounting for the privacy\ncost of HPO.",
      "full_text": "A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization Ashwinee Panda * 1 Xinyu Tang* 1 Saeed Mahloujifar 1 Vikash Sehwag 1 Prateek Mittal 1 Figure 1.Visualization of our method. We use low-cost trials (small ε) to estimate hyperparameters (HPs) and scale these up to the privacy budget for the final run. We combine multiple HPs together, and have a prior that the scaling is linear. Abstract An open problem in differentially private deep learning is hyperparameter optimization (HPO). DP-SGD introduces new hyperparameters and complicates existing ones, forcing researchers to painstakingly tune hyperparameters with hun- dreds of trials, which in turn makes it impossible to account for the privacy cost of HPO without de- stroying the utility. We propose an adaptive HPO method that uses cheap trials (in terms of privacy cost and runtime) to estimate optimal hyperparam- eters and scales them up. We obtain state-of-the- art performance on 22 benchmark tasks, across computer vision and natural language processing, across pretraining and finetuning, across architec- tures and a wide range ofε ∈ [0.01, 8.0], all while accounting for the privacy cost of HPO. *Equal contribution 1Princeton University. Correspondence to: Ashwinee Panda <ashwinee@princeton.edu>. Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). 0.5 1 2 4 8 Privacy/uni00A0Budget/uni00A0() 85 86 87 88 89 90ImageNet/uni00A0Acc/uni00A0(%) Ours/uni00A0(Merged/uni00AD38M) Mehta/uni00A02023/uni00A0(JFT) Berrada/uni00A02023/uni00A0(JFT) Figure 2.Evaluation on ImageNet-1k finetuning. Our HPO only requires paying the privacy cost once, and can then be used to find good HPs for all values of ε > 0.5. We outperform prior work (Mehta et al., 2023b; Berrada et al., 2023) because our HPO finds better HPs, even though prior work has better non-private performance and does not report the privacy cost of their HPO. 1. Introduction A crucial component of interfacing machine learning models closely with user data is ensuring that the process remains private (Team, 2017), and Differential Privacy (DP) is the gold standard for quantifying privacy risks and providing provable guarantees against attacks (Dwork et al., 2006). DP implies that the output of an algorithm e.g., the final weights trained by stochastic gradient descent (SGD) do not change much if a single datapoint in the dataset changes. Definition 1.1 (Differential Privacy). A randomized mech- anism M with domain D and range R preserves (ε, δ)- differential privacy iff for any two neighboring datasets D, D′ ∈ Dand for any subsetS ⊆ Rwe have Pr[M(D) ∈ S] ≤ eε Pr[M(D′) ∈ S] + δ where D and D′ are neighboring datasets if they differ in a single entry, ε is the privacy budget and δ is the failure probability. Differentially Private Stochastic Gradient Descent (DP- SGD) (Song et al., 2013; Abadi et al., 2016) is the stan- dard privacy-preserving training algorithm for training neu- ral networks on private data. For a batch size B and 1 arXiv:2212.04486v3  [cs.LG]  5 May 2024A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization learning rate η, DP-SGD has an update rule given by w(t+1) = w(t)− ηt |Bt| \u0000P i∈Bt 1 C clipC(∇ℓ(xi, w(t))) + σξ \u0001 where the changes to SGD are the per-sample gradient clipping clipC(∇ℓ(xi, w(t))) = C×∇ℓ(xi,w(t)) max(C,||∇ℓ(xi,w(t))||2) , and addition of noise sampled from a d-dimensional Gaussian distribution ξ ∼ N(0, 1) with standard deviation σ. These steps alter the bias-variance tradeoff of SGD and degrade utility, creating a challenging privacy-utility tradeoff. Because private training introduces additional hyperparame- ters, biases optimization by clipping the gradient, and im- poses privacy-utility tradeoffs for existing hyperparameters, hyperparameter optimization (HPO) in DP is challenging. Many prior works report doing hundreds of hyperparameter trials and do not report the privacy cost of HPO in their final privacy guarantee (De et al., 2022; Bu et al., 2022a;b; Mehta et al., 2023a;b; Berrada et al., 2023). These works either assume that HPO does not leak privacy, that the best HPs are known beforehand, or that they can be transferred from a public dataset that is similar to the private dataset. More recently, researchers have proposed methods that do private HPO (Papernot & Steinke, 2021; Koskela & Kulka- rni, 2023; Wang et al., 2023) with R ´enyi DP. These pri- vate HPO methods have been evaluated on MNIST and CIFAR10, but have not been validated on more challenging tasks in CV , or on LLMs. We propose a new private adaptive HPO method ( Figure 1), which we call the new linear scaling rule. We first estimate the optimal HPs for small privacy budgets. We then scale the searched HPs linearly up to larger privacy budgets. Our full method is described in Algorithm 2. We summarize our contributions: • We demonstrate that our new linear scaling rule reduces the computation and privacy cost of HPO by an order of magnitude without sacrificing performance • We compare our private HPO method to random search, grid search, and 3 prior methods for private HPO • We evaluate our private HPO on 22 tasks spanning com- puter vision and natural language processing, fine-tuning and training from scratch, training models spanning from ResNets to multi-billion-parameter Transformers • We find that models trained with our method can pro- vide good performance even when there is a large shift between public and private data 2. Design We provide a set of design goals for our adaptive private HPO method and explain their importance. We use simple axioms for optimization and privacy as building blocks to motivate the high-level design of our method. We conduct preliminary experiments to quantitatively determine the re- lationships between key hyperparameters. Ultimately we compose the many hyperparameters of interest in DP into a single scalar variable r, and present a simple adaptive approach for privately optimizing this parameter. 2.1. Design Goals We draw our goals from the two simple baselines for HPO, random search and grid search. We define random search as drawing hyperparameters from the search space randomly and doing a single run with the entire privacy budget. We discuss variations on random search, such as doing multiple runs with smaller privacy budgets, in Section 5. Random search has low runtime, is parallelizable, and has low privacy cost, but typically does not provide good performance when the hyperparameter search space is large and the set of viable solutions is sparse. Grid search typically has high runtime and privacy cost, and is also parallelizable. Given sufficient trials, grid search should approach the performance of the oracle, the run with perfectly chosen hyperparameters. Our method should provide: • Better performance than random search and almost as good as the oracle; if we define the error rate of any HPO method as the difference in performance between that method and the oracle, our method should reduce the error rate relative to random search significantly. • Better privacy cost than grid search and almost the same privacy cost as random search; the difference in privacy-utility tradeoff between our method and the or- acle should not be the difference between a run with ε = 0.1 and ε = 1.0, it should be the relatively smaller gap between ex. ε = 0.9 and ε = 1.0. 2.2. Building Blocks of Linear Scaling The design of our adaptive private HPO method is based on simple building blocks derived from known theorems in optimization and privacy. First we inspect the definition of DP-SGD and the nature of adaptive composition. Suppose we are taking T steps with noise σ to produce some ε guar- antee. If we relax our privacy guarantee, so now we want to achieve some ε∗ > ε, we can either a) fix T and reduceσ, b) increase T and fix σ, or c) some combination of (a) and (b). Second we turn to a rule of thumb that is popularly known as the original linear scaling rule; the optimal learning rate is inversely proportional to the noise scale in GD (Malladi et al., 2022). In the case of DP-GD, that is in the full batch setting where there is no noise due to SGD, the learning rate should be inversely proportional to σ. If we combine these two axioms, we get the following heuristic: 2A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization Proposition 2.1. If we are taking T steps with noise σ and learning rate η to achieve a target ε∗, we can achieve a target ˆε > ε∗ by either: a) Fix T, reduce σ, increase η) b) Increase T, fix σ, fix η c) Increase T slightly, reduce σ slightly, increase η slightly. We now formalize this intuition. 3. Analysis of Private Gradient Descent We analyze the excess empirical risk of private GD as the sum of two terms. The first term is the risk of non-private GD with the same hyperparameters. The second term is the divergence of private GD from non-private GD due to the added noise term. We consider optimizing a function using Differentially Pri- vate Gradient Descent (DP-GD). The presence of noise in GD introduces a deviation between the iterates of GD with noise, denoted as wT , and without noise, denoted as wTb , at iteration T. We first upper bound this deviation in expec- tation, which we refer to as the radius r. We then use this to upper bound the excess empirical risk of noisy GD. We finally use this bound to motivate the design of our private adaptive HPO method. 3.1. Assumptions We present four assumptions that simplify the conver- gence analysis. We acknowledge that these assumptions do not hold true in all settings, but nevertheless provide an important foundation for illustrating the intuition of our method. We empirically validate the success of our algo- rithm in complex neural network settings, such as training a 13B-parameter OPT Transformer model on the benchmark SQuAD task, in Section 3. • A function is α-strongly convex if for any two points x, yand any subgradient g at x, it holds that f(y) ≥ f(x) + g⊤(y − x) + α 2 ∥y − x∥2. • A function is β-smooth if its gradient is β- Lipschitz continuous, meaning for any two points x, y, ∥∇f(x) − ∇f(y)∥ ≤β∥x − y∥. • A function is L-Lipschitz if there exists a positive L such that |f(x) − f(y)| ≤L∥x − y∥ • A function satisfies the bounded gradient assumption if there exists a constant C such that E[∥∇f(w)∥ ≤ C ∀w ∈ Rd The bounded gradient assumption is implied by convexity and Lipschitzness. This allows us to ignore the impact of clipping in DP-SGD, which reduces the analysis to that of noisy GD. The noise added at each iteration for privacy has an expected norm ρ = √ d · σ, where d is the dimension of the model, and σ is the scale of the noise. The learning rate η satisfies 0 < η <2 β , ensuring convergence. Let c = max(|1 − ηα|, |1 − ηβ|), which characterizes the contraction factor in the optimization process. Given that η is chosen appropriately, we have 0 < c <1. 3.2. Definitions The empirical loss L(wT ) for a model parameterized by wT (ex. iterate T of GD) over a dataset D = {(xi, yi)}N i=1 is defined as the average loss over all training samples: L(wT ) = 1 N NX i=1 ℓ(f(xi; wT ), yi) The goal of our private Hyperparameter Optimization (HPO) is to find the hyperparameter set Λ∗ that minimizes the loss: Λ∗ = argmin Λ Lval(Λ) where Lval(Λ) denotes the loss on a validation dataset for a given hyperparameter configuration Λ. Because Noisy GD typically does not overfit due to the heavy regularization effect of the noise, and to make the convergence analysis straightforward, we use the empirical loss as a proxy for the validation loss throughout. We will analyze the excess empirical risk to motivate the design of our private HPO method. Let wT be the Tth iterate of noisy GD that optimizes a function satisfying the assumptions, and wTb be the Tth iterate of non-noisy GD that optimizes that same function. We define the excess empirical risk of noisy GD as: Rnoisy = E[L(wT )] − L(w∗), ≤ E[L(wT ) − L(wTb )] + L(wTb ) − L(w∗) ≤ E[L · ∥wT − wTb ∥] + Rnon-noisy Where L(w∗) denotes the empirical loss at the optimal pa- rameter set (without noise). Rnon-noisy = L(wTb ) − L(w∗ is the excess empirical risk of non-noisy GD. In the last line, we upper bounded the excess risk induced by noise L(wTb ) − L(w∗) by applying Lipschitzness of the loss. We now bound ∥wT − wTb ∥. Theorem 3.1. Let wT be the Tth iterate of noisy GD that optimizes an α-strongly convex and β-smooth function, and let wTb be the Tth iterate of non-noisy GD that optimizes 3A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization that same function. The ”noisy radius” distance, the ℓ2- norm between wT and wTb at iteration T, can be bounded in expectation as follows: E[∥wT − wTb ∥] ≤ ρη ×  T−1X i=0 ci ! = r Proof sketch. The full proof is in Appendix B.5. At each iteration the distance between the noisy iterate and the non- noisy iterate contracts by a factor ofc = max(|1−ηα|, |1− ηβ|) and then increases additively by ρη. The overall dis- tance then can be represented by scaling the additive noise term ρη by a geometric series that converges. Future work might incorporate additional factors such as momentum ac- celeration, bias introduced by clipping, or extend our analy- sis to the setting of more general neural networks. However, our objective here is to provide some theoretical intuition for our algorithm. Substituting Theorem 3.1 into the excess empirical risk we get Rnoisy ≤ Lr + Rnon-noisy where L is the Lipschitz constant, we can see that our pri- vate HPO needs to find HPs that are good for non-noisy optimization but do not create a large divergence between the noisy and non-noisy iterates. 4. Our Private HPO We have established a relationship between the excess em- pirical risk and the noisy radius. We can now connect this back to our goal of doing private HPO, which is to find the HPs that minimize the excess empirical risk. We want to find r∗ = r(ε), the optimal value of r for a given value of ε. We will first reduce the dimensionality of the search problem and then introduce a principled approximation. 4.1. Reducing the Dimensionality of HPO We want to reduce the dimensionality of HPO so that we can reduce the cost of HPO. For fixed ε, if we increase or decrease T then we will cor- respondingly increase or decrease σ by the Composition Theorem of DP. The actual statements of DP composition are somewhat complicated, but we can simplify them as say- ing σ grows slower than αT for some constant α. Because E[ρ] = √ dσ, we have that ρ grows slower than T. The geometric series converges to 1 1 − c as T increases, giving us E[∥wT − wTb ∥] ≤ (T η) · ( √ d 1 1 − c). Because we are interested in writing the radius in terms of hyperpa- rameters that we can optimize, we drop the second term for simplicity. Now we can write our hyperparameter of interest as r = η × T, reducing the 2D HPO to 1D. If we wanted to search for additional terms such as the batch size or clipping threshold, we could incorporate them into our theory, but we empirically find that it’s best to fix all other HPs to the values we provide and just search for η, T. 4.2. Our Private HPO In order to find the optimal r∗ = r(ε) without exhaustively searching, we need to approximate r(ε). A natural choice is Taylor approximation. We can sample points from r(ε) at different values of ε via random search, use this to ap- proximate a Taylor polynomial, and then use that Taylor polynomial to estimate r for any desired target ε. After we have our estimated r, we can decompose it into η, T by randomly sampling η, Tuntil their product is close to r. This is the procedure we use in Figure 2, paying for the privacy cost of building the approximation and then using it to estimate the optimal HPs for many values of ε ∈ [0.5, 8]. We now elaborate on the implementation of the method. The first-order Taylor approximation of a function r(ε) around a point ε0 is given by r(ε) ≈ r(ε0) + dr dε \f\f ε=ε0 · (ε − ε0), which linearly approximates r near ε0. Because we cannot analytically determine dr dε , we will have to ap- proximate this. To approximate the first-order Taylor polynomial we fit a line. We first use random search to find two empirical points (ε1, r(ε1)) and (ε2, r(ε2)). We then fit a line to these points to obtain the parameters of the linem, b(slope and intercept). We finally estimate the optimal r(εf ) = mεf + b such that the composition of privacy guarantees for the entire private HPO satisfies a target privacy budget according to Theorem 2.3. In practice we choose smaller values of ε for these points such as ε1 = 0.1, ε2 = 0.2, that we find provide a good privacy-utility tradeoff. More generally, we can approximate the Taylor polyno- mial by fitting a degree N polynomial with N + 1 points (ε1, r(ε1)) ··· (εN+1, r(εN+1)). We provide results com- paring the linear approximation to quadratic approximation in the 2nd common response PDF, but use the linear ap- proximation throughout our work because we find that it provides a good privacy-utility tradeoff. The full method is detailed in Algorithm 2. The final pri- vacy guarantee including the cost of HPO is given by Theo- rem 4.1. Theorem 4.1. The privacy guarantee of Algorithm 2 in terms of µ in f-DP is µt = q nµ2 1 + nµ2 2 + µ2 f . The proof and conversion to (ε, δ)-DP follow directly from Dong et al. (2022), so we defer it to Appendix B.5. 4A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization Table 1.Our method fixes six design choices: the architecture and initialization (for CV tasks only), the batch size (full batch), the optimizer (SGD with momentum=0.9), the accounting method (PLV where all prior HPO methods use RDP), and the clipping norm (unit clipping). We report the improvement derived from following each of these techniques with respect to a competitive baseline from prior work on CIFAR100 at ε = 0.1. Method Baseline Baseline Accuracy Improvement Classifier (no bias) (Mehta et al., 2023b) 71.3 0 .36 Zero Initialization Random Initialization (De et al., 2022) 64.85 6 .81 Gradient Descent SGD(Batch=4096) (De et al., 2022) 70.2 1 .46 Momentum (ρ = 0.9) ρ = 0 (Bu et al., 2022a) 69.02 2 .09 PLV Accounting RDP (De et al., 2022) 68.43 3 .23 Unit Clipping (C = 1) C ≪ 1 (Mehta et al., 2023a) 71.2 0 .46 Implementing our method requires decomposing a target ε, δ-DP guarantee into a set of µs; we provide code for this. 4.3. Limitations Although this theory does not hold in general for training neural networks, we quantitatively evaluate the heuristic we develop in Section 3.4 and find that our method holds even for the complex setting of training Transformers on NLP benchmarks. Our HPO also requires more runtime than random search because it is adaptive. Algorithm 1 Model Training Subroutine Initialize model weights w at 0 Decompose r into η, Twithout exceeding Tmax or ηmax Use the PLD accountant to calibrate σ given µ, T for i = 1, 2, . . . , Tdo Compute gradient with unit clipping and add noise ∇(i) = 1 |D| \u0000P i∈D clip1(∇ℓ(xi, w(i))) + σξ \u0001 Take a step with momentum: v(i) ← ρ · v(i−1) + ∇(i), w(i) ← w(i−1) − ηv(i) end for return trained model w Algorithm 2 Adaptive HPO Routine Inputs: Privacy parameters for hyperparameter sweeps and final run µ1, µ2, µf , number of runs per sweep n, maximum learning rate ηmax, maximum number of itera- tions Tmax, dataset D, model M Perform n runs with µ1 using Hyperparameter Sweep Subroutine (Algorithm 3); obtain the best-performing r1 Perform n runs with µ2 using Hyperparameter Sweep Subroutine (Algorithm 3), obtain the best-performing r2 Perform linear interpolation to estimate the slope α and bias b of the line r = αε + b given (µ1, r1), (µ2, r2) Set r∗ = αµf + b given the estimated linear interpolation Launch the Model Training Subroutine (Algorithm 1) with r∗, µf , obtaining the final performance Af Output: Final performance Af , trained model M Algorithm 3 Hyperparameter Sweep Subroutine Inputs: Privacy parameter µ, number of runs per sweep n, search space for r for i = 1, 2, . . . , ndo Uniformly sample r from the search space Launch Model Training Subroutine (Algorithm 1) with configuration r, µ, returning performance Pi if Pi is the best performance so far on the training set then set best-performing ri = r end for return best-performing ri 5. Evaluation We provide results on a range of image classification, distri- bution shift, and natural language processing tasks, for both finetuning of models pretrained on public data and training from scratch without any additional data. Due to the large scope of our evaluation, we defer all experimental details and full results for all datasets and models to Appendix A. We provide ablations on all steps of our method (B.2). We provide hyperparameter grid search results (B.4). We also provide the code to reproduce our results at this link. Datasets. Image classification: ImageNet (Deng et al., 2009), CIFAR10 (training from scratch and finetuning), CI- FAR100 (Krizhevsky et al., 2009), FashionMNIST (Xiao et al., 2017), STL10 (Coates et al., 2011), EMNIST (Co- hen et al., 2017). Because these image classification datasets are generally considered in-distribution of the pretraining data, we also provide results on a number of distribution shift datasets (Koh et al., 2020) . CI- FAR10 → STL, CIFAR10p1, CIFAR10C, CIFAR100 → CIFAR100C (Hendrycks & Dietterich, 2019), Water- birds (Sagawa et al., 2019), FMoW (Christie et al., 2017), and Camelyon17 (B ´andi et al., 2019). For NLP tasks we consider SQuAD (Rajpurkar et al., 2016) for Ques- tions Answering, text classification tasks from the GLUE benchmark (Wang et al., 2019a): SST-2, QNLI, QQP, MNLI(m/mm) and for next word generation we use Per- 5A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization sonaChat (Zhang et al., 2018a) and WikiText-2 (Merity et al., 2017), and Enron Emails (Klimt & Yang, 2004). 5.1. Effectiveness of the Linear Scaling Rule ImageNet (with Public Data) In Figure 2 we compare the performance of our method on ImageNet against the com- petitive prior works of Mehta et al. (2023b); Berrada et al. (2023). Note that these works do not report the privacy cost of HPO and pretrain their models with JFT, Google’s proprietary internal dataset; as a result the non-private per- formance of their models exceeds ours (rightmost points). Despite this, given sufficient budget (ε >0.5) we match or exceed their performance while accounting for the pri- vacy cost of HPO. The downside of our method is that for sufficiently small ε on sufficiently difficult datasets such as ImageNet, there is no way to keep the privacy cost of HPO small enough to retain enough budget to do a final run, because HP trials with too small a budget do not provide any information. We provide a deep dive into these points of comparison in Appendix A. CIFAR-10 (without Public Data) In Table 2 we compare our method to random search and the grid search baseline, which does not consider the privacy cost of HPO. We sig- nificantly outperform random search, and approach the per- formance of grid search. To the best of our knowledge, we are the first to provide competitive performance when train- ing on CIFAR10 without public data under a strict privacy budget while accounting for the privacy cost of HPO. Table 2.Performance comparison of different methods on CI- FAR10. Our method outperforms prior work in linear probing settings when using a feature extractor pretrained on CIFAR100. In the setting where we do not have public data, we compare to random search and grid search and our method greatly outperforms random search. CIFAR10 Acc with Public Data (ε = 1) Koskela & Kulkarni (2023) 67% Papernot & Steinke (2021) 66% Ours 70.5% CIFAR10 Acc without Public Data (ε = 1) Random Search 44% Grid Search (cost of HPO not incl.) 68% Ours 62.63 % 5.2. Comparison to other Private HPO We provide a detailed comparison to 5 prior works in pri- vate HPO as well as the baselines of random search and grid search, and explain the design choices that enable our method to dominate all prior work. 5.2.1. C OMPARISON TO RENYI HPO We compare to three prior works that use Renyi DP to ana- lyze HPO (Papernot & Steinke, 2021; Koskela & Kulkarni, 2023; Wang et al., 2023). In Table 2 we report that our linear scaling is 3.5% better on CIFAR10 at ε = 1 in the experimental setting of Koskela & Kulkarni (2023): linear probing on a ResNet20 check- point pretrained on CIFAR100. Koskela & Kulkarni (2023) achieve 67% on CIFAR10 at ε = 1. In the same setting, the method of Papernot & Steinke (2021) obtains 66%. We apply the linear scaling rule in the same setting, so that only the hyperparameters our method selects are different, and obtain 70.5% at ε = 1 . All methods use the same hyperparameter search space. The reason our method out- performs Koskela & Kulkarni (2023); Papernot & Steinke (2021) is because our prior is better than their random search, which is required by their method, enabling us to simulta- neously allocate a smaller portion of the privacy budget to HPO while still finding better hyperparameters. We also use PLD accounting which is tighter than the RDP accounting their method requires. Neither of these can be fixed; that is, we cannot modify their method to integrate the linear scaling prior or to use PLD accounting. Even with PLD accounting, we would not be able to make up for the gap in accuracy that comes from our adaptive method. An interesting question for future work is whether we can do RDP analysis of our adaptive method. More details in Appendix A.1. 5.2.2. C OMPARISON TO PARAMETER -FREE METHODS . A related area is parameter-free HPO, that builds optimizers that do not require specifying the learning rate as a hyper- parameter. In general it can be challenging to apply these parameter-free methods to DP, because the update rule for the scale of the gradient may not maintain its guarantees in the presence of noise (Li et al., 2023). Comparison to DPAdamWosm. One parameter-free opti- mizer specifically designed for DP is DPAdamWOSM (Mo- hapatra et al., 2021). On ImageNet DPAdamWOSM achieves 79% at ε = 1), which is 8% lower than our method (87% at ε = 1). We do not find that the data-independent learning rate selection works well for ImageNet, and still requires tuning the number of iterations (see Appendix A.1). Comparison to Mehta et al. (2023b) Mehta et al. (2023b) propose an approach where they fix the batch size to full batch, the number of steps to 1, and take a single step of DP-Adam with a very small learning rate. Their approach obtains just 81% at ε ≥ 1 on ImageNet for a model whose non-private accuracy is 88.7%, because they take only a single step. Our method smoothly interpolates between the low-r setting for small ε and the large-r setting for large ε, and outperforms their method across all privacy budgets. 6A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization Table 3.Comparing various HPO methods on CIFAR10 (without public data), CIFAR100, ImageNet, and SQuAD (with public data). Reported numbers are mean over 5 trials. Dataset Random Search Oracle Ours RERR CIFAR10 44 68 62.63 77.63 CIFAR100 84.44 89.62 89.10 84.85 ImageNet 81.2 88.6 86.7 73.97 SQuAD 49.33 82.43 78.08 86.85 5.2.3. C OMPARISON TO BASELINES : RANDOM SEARCH AND GRID SEARCH . Linear scaling significantly outperforms random search. In Table 3 we report the performance for random search, our method, the oracle, and the relative error rate reduction (RERR). Across all datasets, our method significantly out- performs random search. We use the same logarithmic grid for both our method and random search that can be found in Appendix A. We vary this grid and find that the larger the search space, the more our method outperforms random search. 0.0 0.2 0.4 0.6 0.8 1.0 0 20 40 60 80T est Accuracy  Accuracy via Linear Scaling Accuracy via Grid Search (a) 0.0 0.2 0.4 0.6 0.8 1.0 10 20 30 40Total Step Size (  × T) T otal step size by Linear Scaling T otal step size by Grid Search (b) Figure 3.Training the beit architecture on CIFAR100, the linear scaling rule produces values for r = η × T close to that of grid search, and the performance drop is only apparent at ε > 0.2 because of the cost of HPO, and vanishingly small for larger ε. Linear Scaling approaches grid search. We validate the effectiveness of linear scaling against the grid search base- line. In Fig. 3 (right) we compare Alg. 2 to the best run across 100 trials from the search space. The privacy cost of grid search is many times higher than that of our method at each value of ε, because we do not account for the pri- vacy cost of grid search to illustrate that even when our method has to account for the privacy cost of HPO and the oracle (grid search) does not, our method is competitive. Our method finds near-optimal hyperparameters with just a fraction of the runtime and privacy cost of grid search. 5.3. Empirical Analysis of Linear Scaling We now consider different architectures and validate our HPO method in the presence of distribution shifts. Full results can be found in Appendix B.1. Architecture Search. In Table 4 we apply our method to Table 4.We compare the best private and best non-private perfor- mances of all models on all datasets. We use the linear scaling rule to scale hyperparameters from ε = 0.1 to ε = 1, so our privacy analysis includes the cost of hyperparameter tuning. Model Dataset ε = 1 ε = ∞ Gap beitv2 CIFAR10 98.90 99 .00 0 .10 CIFAR100 89.10 91 .57 2 .47 FMNIST 91.02 91 .53 0 .51 STL10 99.69 99 .81 0 .12 EMNIST 81.77 82 .00 0 .23 convnext CIFAR10 96.75 97 .22 0 .47 CIFAR100 83.47 86 .59 3 .12 FMNIST 90.23 91 .13 0 .9 STL10 99.61 99 .71 0 .10 EMNIST 78.38 79 .05 0 .67 beit CIFAR10 98.19 98 .51 0 .32 CIFAR100 87.1 90 .08 2 .98 FMNIST 90.55 91 .6 1 .05 STL10 99.62 99 .78 0 .16 EMNIST 81.48 83 .25 1 .77 vit-L CIFAR10 98.29 98 .44 0 .40 CIFAR100 86.18 89 .72 3 .54 FMNIST 90.58 91 .37 0 .79 STL10 99.62 99 .76 0 .14 different architectures that can serve as good backbones for high-accuracy DP classification across CIFAR10, CI- FAR100, FMNIST, STL10, and EMNIST. The private-non private utility gap diminishes with model accuracy. One architecture, beitv2, performs the best on all benchmarks and also has the highest non-private zero-shot ImageNet accuracy (Wightman, 2019). We conclude that architecture search can be done without any privacy cost by selecting the model with the best zero-shot performance on a repre- sentative benchmark such as ImageNet. Distribution Shift. A concern in DP fine-tuning is that the pretraining datasets are too similar to the downstream tasks, which can violate privacy (Tram`er et al., 2022). In Table 5 we evaluate the robustness to distribution shift of models trained with our private HPO to non-private models, in the absence of any explicit regularization methods or any infor- mation about the distribution shift. These datasets are con- sidered benchmark tasks for distribution shifts (Kumar et al., 2022b;c; Mehta et al., 2022) and include data that is not in- distribution of the training data, making for a more realistic evaluation of the capabilities of our method to solve chal- lenging tasks. We show that DP-SGD provides robustness to covariate, subpopulation and label distribution shifts for synthetic and natural datasets. Full details in Appendix B.1. On Waterbirds, DP degrades the ID performance but actu- ally improves the OOD performance. On fMoW and Came- lyon17 that are datasets with a significant distribution shift from ImageNet and very different subgroups, DP does not significantly degrade performance and does not exacerbate 7A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization Table 5.Evaluating our DP-HPO method on datasets with distribution shifts at ε = 1. Waterbirds fMoW Camelyon C10 → STL C10 → C10p1 C10 → C10C C100 → C100C ID 92.31 45.44 93.91 98.90 98.90 98.90 89.65 OOD 91.59 35.31 93.55 98.82 97.85 89.98 68.69 Table 6.Linear scaling holds for GLUE tasks when training the full RoBERTa-base model Task ε Acc r = η × T SST-2 0.1 90.60 0.975 0.2 90.83 1.95 0.7 91.06 5.07 QNLI 0.1 82.54 3.9 0.2 84.00 4.68 1.3 86.25 26.52 QQP 0.1 81.07 11.7 0.2 82.21 17.55 1.2 84.69 64.35 MNLI(m/mm) 0.1 77.52(78.24) 11.7 0.2 79.40(79.98) 17.55 1.2 81.86(82.76) 64.35 disparities among subgroups. We also show that we can train models on CIFAR10 with DP and do zero-shot transfer to STL and CIFAR10p1. Finally, we evaluate the robustness of CIFAR-DP-trained models to the common corruptions of CIFAR10C/CIFAR100C, and note that DP training achieves some measure of intrinsic robustness to image corruptions. 5.4. Linear Scaling for language modeling Prior work has generally focused on either CV or NLP because the methods used in DP fine-tuning differ greatly across data modalities (Li et al., 2022b; Mehta et al., 2023a); here we show that our method extends to NLP by validat- ing on text classification and language modeling tasks with LoRA (Hu et al., 2021) and full fine-tuning. We fine-tune GPT-2 (Radford et al., 2019) with our method for three lan- guage modeling tasks that have been benchmarked in prior works (Li et al., 2022b; Shi et al., 2022; Gupta et al., 2022) on private fine-tuning: Persona-Chat (Zhang et al., 2018b), WikiText-2 (Merity et al., 2017) and Enron Emails (Klimt & Yang, 2004). We also fine-tune RoBERTa-base on four tasks in the GLUE benchmark: SST-2, QNLI, QQP and MNLI(m/mm) in Table 6. While prior works mainly fo- cus on ε in {3, 8}, in this work we are also interested in smaller εs like 0.1. Appendix C.2 includes the details for the experimental set-up. Linear scaling succeeds when random search fails. We consider the challenging setting from Malladi et al. (2024) of fine-tuning an OPT-13B model on just 1000 samples from SQuADv2 with DP-SGD-LoRA. Random search runs sometimes do not improve much over zero shot performance, because the search space is so large and the viable set so small. In the initial phases of our method, the trials do not always succeed. Regardless, our method achieves78%±3% close to the oracle 82%, a RERR of 87.5%. Random search performs poorly for NLP tasks because HPO is generally more challenging (Li et al., 2022b). In the rest of the NLP datasets we consider, we compare our performance to prior work that doesn’t consider the privacy cost of HPO. Linear scaling holds for NLP tasks We analyze the per- formance gap between estimated total step size and optimal total step size by grid search to understand how well linear scaling performs on language modeling tasks. Fig. 4 plots the optimal perplexity and perplexity by estimated total step size at different values ofε on Enron emails. We can see that the linear scaling rule generalizes well for reported values of ε and the perplexity by the estimated total step size is close to the optimal perplexity. From Table 6 we can see that our method also works for the GLUE benchmark. 0.0 0.5 1.0 1.5 2.0 2.5 3.0 10 11 12 13 14Perplexity Perplexity by linear rule Perplexity by grid search (a) Pareto Frontier for ε vs Test Perplexity. 0.0 0.5 1.0 1.5 2.0 2.5 3.0 0 5 10Total step size ( × T)  T otal step size by linear scaling T otal step size by grid search (b) Pareto Frontier for ε vs Total Step Size. Figure 4.The linear scaling rule (accounting for the privacy cost of hyperparameter tuning) is competitive with grid search (non- private, doing N trials each with the given ε) on the Enron Emails dataset. Left: y-axis is Perplexity (lower is better). The linear scaling rule outperforms prior results on dif- ferentially private language modeling tasks.We first run a qualitative evaluation on the previous benchmark SOTA (Li et al., 2022b) on PersonaChat trained with DP-SGD by fol- lowing the linear scaling rule to increase the number of epochs. We can see in Table 7 that we can push the per- plexity under 18 for ε = 3 and ε = 8; this performance is competitive with the non-private baseline. Furthermore, even when pushing for a stricter privacy guarantee ε = 0.5, we can still get perplexity of 21.25, that is better than the result of ε = 8 in (Li et al., 2022b). We also report the results of ablating these hyper-parameters and varying the 8A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization Table 7.Linear scaling holds when fine-tuning all layers of GPT2 on PersonaChat and outperforms Li et al. (2022b) ε (δ = 1 2|Dtrain|) 0.7 3 ∞ Li et al. (2022b) - 24.59 18 .52 Our Work 21.25 - 17.69 Table 8.Finetuning GPT-2 on WikiText-2 (δ = 10−6) and Enron (δ = 1 2|Dtrain|) with DP-SGD. Ppl is perplexity and TSS is Total Step Size. (∗ means estimated). Previously reported best perplexity of GPT-2 on WikiText-2 atε = 3is 28.84 in (Shi et al., 2022). ε 0.1 0 .2 0 .5 1 .4 2 .2 3 .0 WikiText-2 Ppl - 28.81 28.37 28.15 27.98 27 .69 TSS - 0.008 0 .02 0 .04∗ 0.08∗ 0.12∗ ε 0.1 0 .2 0 .7 1 .1 1 .9 2 .7 Enron Ppl 14.35 12.50 11.56 10.91 10.45 10 .14 TSS 0.10 0 .58 2 .02∗ 4.41∗ 9.19∗ 13.98∗ number of layers trained in Appendix C.3. We quantitatively validate the linear scaling rule on WikiText-2 and report the result in Table 8. For WikiText-2, a key observation is that when we compare our results to the best prior reported re- sults in (Shi et al., 2022), for the same number of passes over the training data (20), we obtain lower perplexity for ε = 0.2 than they report for ε = 3. That is, by just increas- ing the effective step size from ∼ 8 × 10−6 to ∼ 8 × 10−3 we can strengthen the privacy guarantee without degrading performance. 5.5. Additional Ablations We can apply our method to methods other than DP- SGD. Tang et al. (2024) recently proposed DP-ZO, a method for DP zeroth-order optimization, that privatizes the zeroth- order update to the model weights in the form of a scalar rather than the first-order gradient update. In Table 9 we find that our method can also optimize HPs for DP-ZO. Although our method fits a 1-d polynomial with 2 points, we can in principle fit any degree-d polynomial with d + 1 points. to approximate the relationship between r and ε. However, because using more points to fit the polynomial imposes more privacy cost for HPO, we use the same num- ber of points and degree throughout all experiments. It is likely that for some datasets, it’s important to tune the Method Mean Accuracy (Std) Random Search 82.53 (1.01) Our Private HPO 83.02 (0.86) Grid Search 83.87 (0.50) Table 9.Our method works beyond DP-SGD. Method Mean Accuracy (Std) Linear (2) 86.69 (0.86) Linear (3) 87.81 (0.86) Quadratic (3) 86.64 (1.08) Table 10.Using more points or a higher order approximation can improve performance. Method GPU Hours Wall-clock time (hours) Random 1 1 Grid 100 1 Ours 7 3 Table 11.Our method trades off runtime for performance with random search and grid search. privacy budget allocated to the smaller trials, the number of points, etc. However, we are not interested in tuning the hyperparameters of our hyperparameter optimization method. In Table 10 we evaluate the linear fit with 2 and 3 points for evaluation, and the quadratic fit with 3 points for evaluation, on ImageNet. Throughout the paper we search for the two main HPs of interest η, Tand fix other HPs such as batch size B and clipping threshold C. However, we can search for these as well. We can incorporate new HPs by updating the decom- position of r so that we optimize for the joint product of the HPs being optimized. We change it from r = η × T to r = η × T × B × C. We evaluate on CIFAR100. The performance of our method when optimizing η, T, B, Cis 87.9 ± 1.9, which is worse than the 89.10 where we opti- mized η, Tand fixed B, C. One limitation of our method is the runtime, shown in Ta- ble 11.. We have worse runtime than random search and worse parallelization than grid search, which is embarrass- ingly parallel while our method requires serial runs. Here the base time for a single HP trial is just 1 hour; this can change based on the task, but these proportions should remain similar. Random search just does 1 run so it has both the lowest GPU hours and wall-clock time. The oracle does a number of runs equal to the granularity of the search space, which here we approximate as 100. In the setting where 100 GPUs are available for the oracle, which may be realistic for large companies but is not realistic for our academic compute setting, these can all be done in parallel, so it uses 100 GPU hours but just 1 hour in wall-clock time. Our method typically does 7 runs: 3 for ε1, 3 for ε2, and 1 for εf , so the total number of GPU hours is 7. The serial dependency of our method requires that εf runs after ε1 9A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization and ε2, but the 3 runs for ε1, ε2 can be parallelized so the wall-clock time is just 3 hours. 6. Related Work and Discussion Related Work. We have performed detailed quantitative and qualitative comparisons to prior private HPO meth- ods (Papernot & Steinke, 2021; Koskela & Kulkarni, 2023; Wang et al., 2023). These build on earlier work by Liu & Talwar (2018) that can do HPO by increasing the privacy cost roughly threefold. We improve over these works by significantly reducing the privacy budget required by HPO and adopting a robust prior on our hyperparameter search. Many non-private HPO methods have been used by prior DP papers that do not report the privacy cost of HPO, and a valuable future task would be to consider privatizing these. Multiple prior works (De et al., 2022; Cattan et al., 2022; Bu et al., 2022a;b; Mehta et al., 2023a;b; Berrada et al., 2023; Li et al., 2022b;a; Hu et al., 2021) consider the task of max- imizing the privacy utility tradeoff of finetuning pretrained models. Although the main focus of our paper is private HPO, we also critically evaluate the efficacy of a range of techniques that have been proposed by these works such as data augmentation, fine-tuning the embedding layer, and weight averaging. A detailed discussion of these techniques is deferred to Appendix B.2 and Appendix C. Golatkar et al. (2022); Nasr et al. (2023); Amid et al. (2022) treat< 10% of the private training dataset and public and use it to improve DP-SGD. Although we do not use any private data during pretraining, future work can tackle applying our method to this alternate threat model. Sander et al. (2022) suggest doing HPO with smaller batch sizes and then scaling up the HPs to the full batch update. This idea is similar in spirit to the adaptive scaling that we propose, because the HP trials are cheaper from a runtime perspective than the final run. However, our approach is not only compute-efficient but also accounts for the privacy cost of HPO. Kuo et al. (2023) find that noisy HPO in the federated setting suffers, and suggest doing HPO on public proxy data (whose existence we don’t assume) and transferring it to the private dataset. Wang et al. (2023) propose a method for private adaptive HPO that provides an RDP guarantee for Bayesian HPO. They compare their method to random search under a total privacy budget of ε = 15 , where at each iteration they sample a new set of HPs from their prior, and update their prior, and at each iteration random search samples a new set of HPs uniformly from the search space; each run has a base privacy cost, and it takes many runs for the distribution to converge. Their method can be seen as a version of ours that lacks the linear scaling prior and does not use cheap trials to find the parameters for the prior. As a result, they use much more budget in order to find a good distribution for the HPs. This more flexible approach can be superior to ours in settings where the HPs are not linear. Discussion. DP researchers commonly confront the compute-intensive, privacy-expensive task of doing grid search with hundreds of trials to optimize the privacy-utility tradeoff of DP methods. Our work provides an alternative HPO method that reduces the compute and privacy costs of grid search by an order of magnitude without compromising accuracy across 20 tasks. Researchers using our method can accelerate the pace of research by reducing the compute needed to produce good results, and address the open ques- tion of accounting for the privacy cost of hyperparameter tuning, whether they are doing transfer learning in the pres- ence of domain shifts, training from scratch, or applying PEFT methods to LLMs. 7. Impact Statement This paper presents work whose goal is to improve the qual- ity of models trained with differential privacy. Privacy is at the heart of many ongoing debates about AI. We submit that any work that makes it easier for organizations to train and release models with DP guarantees will positively benefit society. Additionally, this paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. References Abadi, M., Chu, A., Goodfellow, I., McMahan, H. B., Mironov, I., Talwar, K., and Zhang, L. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communi- cations Security. ACM, oct 2016. doi: 10.1145/2976749. 2978318. Amid, E., Ganesh, A., Mathews, R., Ramaswamy, S., Song, S., Steinke, T., Steinke, T., Suriyakumar, V . M., Thakkar, O., and Thakurta, A. Public data-assisted mirror descent for private model training. In Proceedings of the 39th International Conference on Machine Learning, pp. 517– 535. PMLR, 2022. Bao, H., Dong, L., Piao, S., and Wei, F. Beit: Bert pre- training of image transformers, 2021. URL https:// arxiv.org/abs/2106.08254. Basu, P., Roy, T. S., Naidu, R., Muftuoglu, Z., Singh, S., and Mireshghallah, F. Benchmarking differential privacy and federated learning for bert models, 2022. Berrada, L., De, S., Shen, J. H., Hayes, J., Stanforth, R., Stutz, D., Kohli, P., Smith, S. L., and Balle, B. Unlock- ing accuracy and fairness in differentially private image classification, 2023. 10A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization Bu, Z., Mao, J., and Xu, S. Scalable and efficient training of large convolutional neural networks with differential privacy. arXiv preprint arXiv:2205.10683, 2022a. Bu, Z., Wang, Y .-X., Zha, S., and Karypis, G. Differentially private bias-term only fine-tuning of foundation models, 2022b. B´andi, P., Geessink, O., Manson, Q., Van Dijk, M., Balken- hol, M., Hermsen, M., Ehteshami Bejnordi, B., Lee, B., Paeng, K., Zhong, A., Li, Q., Zanjani, F. G., Zinger, S., Fukuta, K., Komura, D., Ovtcharov, V ., Cheng, S., Zeng, S., Thagaard, J., Dahl, A. B., Lin, H., Chen, H., Jacob- sson, L., Hedlund, M., C ¸etin, M., Halıcı, E., Jackson, H., Chen, R., Both, F., Franke, J., K ¨usters-Vandevelde, H., Vreuls, W., Bult, P., van Ginneken, B., van der Laak, J., and Litjens, G. From detection of individual metas- tases to classification of lymph node status at the pa- tient level: The camelyon17 challenge. IEEE Transac- tions on Medical Imaging , 38(2):550–560, 2019. doi: 10.1109/TMI.2018.2867350. Cattan, Y ., Choquette-Choo, C. A., Papernot, N., and Thakurta, A. Fine-tuning with differential privacy neces- sitates an additional hyperparameter search, 2022. URL https://arxiv.org/abs/2210.02156. Christie, G., Fendley, N., Wilson, J., and Mukherjee, R. Functional map of the world, 2017. URL https:// arxiv.org/abs/1711.07846. Coates, A., Ng, A., and Lee, H. An analysis of single- layer networks in unsupervised feature learning. In Pro- ceedings of the fourteenth international conference on artificial intelligence and statistics, pp. 215–223. JMLR Workshop and Conference Proceedings, 2011. Cohen, G., Afshar, S., Tapson, J., and van Schaik, A. Emnist: an extension of mnist to handwritten letters, 2017. URL https://arxiv.org/abs/1702.05373. Croce, F., Andriushchenko, M., Sehwag, V ., Debenedetti, E., Flammarion, N., Chiang, M., Mittal, P., and Hein, M. Robustbench: a standardized adversarial robustness benchmark, 2021. Cutkosky, A. and Mehta, H. Momentum improves nor- malized SGD. In III, H. D. and Singh, A. (eds.), Pro- ceedings of the 37th International Conference on Ma- chine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 2260–2268. PMLR, 13–18 Jul 2020. URL https://proceedings.mlr.press/ v119/cutkosky20b.html. De, S., Berrada, L., Hayes, J., Smith, S. L., and Balle, B. Unlocking high-accuracy differentially private im- age classification through scale, 2022. URL https: //arxiv.org/abs/2204.13650. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vi- sion and Pattern Recognition, pp. 248–255, 2009. doi: 10.1109/CVPR.2009.5206848. Diffenderfer, J., Bartoldson, B. R., Chaganti, S., Zhang, J., and Kailkhura, B. A winning hand: Compress- ing deep networks can improve out-of-distribution ro- bustness, 2021. URL https://arxiv.org/abs/ 2106.09129. Dong, J., Roth, A., and Su, W. J. Gaussian differential privacy, 2019. URL https://arxiv.org/abs/ 1905.02383. Dong, J., Roth, A., Su, W. J., et al. Gaussian differential privacy. Journal of the Royal Statistical Society Series B, 84(1):3–37, 2022. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. An image is worth 16x16 words: Transformers for image recognition at scale, 2020. URL https: //arxiv.org/abs/2010.11929. Dwork, C., McSherry, F., Nissim, K., and Smith, A. Cali- brating noise to sensitivity in private data analysis. InPro- ceedings of the Third Conference on Theory of Cryptog- raphy, TCC’06, pp. 265–284, Berlin, Heidelberg, 2006. Springer-Verlag. ISBN 3540327312. Dwork, C., Kohli, N., and Mulligan, D. Differential privacy in practice: Expose your epsilons! Journal of Privacy and Confidentiality, 9, 10 2019. doi: 10.29012/jpc.689. Erlingsson, U., Feldman, V ., Mironov, I., Raghunathan, A., Talwar, K., and Thakurta, A. Amplification by shuffling: From local to central differential privacy via anonymity, 2018. URL https://arxiv.org/abs/ 1811.12469. Fang, H., Li, X., Fan, C., and Li, P. Improved convergence of differential private SGD with gradient clipping. In The Eleventh International Conference on Learning Represen- tations, 2023a. URL https://openreview.net/ forum?id=FRLswckPXQ5. Fang, Y ., Sun, Q., Wang, X., Huang, T., Wang, X., and Cao, Y . Eva-02: A visual representation for neon genesis, 2023b. Geirhos, R., Rubisch, P., Michaelis, C., Bethge, M., Wich- mann, F. A., and Brendel, W. Imagenet-trained cnns are biased towards texture; increasing shape bias im- proves accuracy and robustness, 2018. URL https: //arxiv.org/abs/1811.12231. 11A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization Ghalebikesabi, S., Berrada, L., Gowal, S., Ktena, I., Stan- forth, R., Hayes, J., De, S., Smith, S. L., Wiles, O., and Balle, B. Differentially private diffusion models generate useful synthetic images, 2023. Golatkar, A., Achille, A., Wang, Y .-X., Roth, A., Kearns, M., and Soatto, S. Mixed differential privacy in computer vision, 2022. Gopi, S., Lee, Y . T., and Wutschitz, L. Numerical com- position of differential privacy, 2021. URL https: //arxiv.org/abs/2106.02848. Goyal, P., Doll ´ar, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola, A., Tulloch, A., Jia, Y ., and He, K. Accurate, large minibatch sgd: Training imagenet in 1 hour, 2017. URL https://arxiv.org/abs/ 1706.02677. Gupta, S., Huang, Y ., Zhong, Z., Gao, T., Li, K., and Chen, D. Recovering private text in federated learning of lan- guage models. In Advances in Neural Information Pro- cessing Systems (NeurIPS), 2022. Hendrycks, D. and Dietterich, T. Benchmarking neural network robustness to common corruptions and pertur- bations, 2019. URL https://arxiv.org/abs/ 1903.12261. Hoffer, E., Ben-Nun, T., Hubara, I., Giladi, N., Hoefler, T., and Soudry, D. Augment your batch: better training with larger batches, 2019. URL https://arxiv.org/ abs/1901.09335. Hu, E. J., Shen, Y ., Wallis, P., Allen-Zhu, Z., Li, Y ., Wang, S., Wang, L., and Chen, W. Lora: Low-rank adaptation of large language models, 2021. Hulkund, N., Suriyakumar, V . M., Killian, T. W., and Ghas- semi, M. Limits of algorithmic stability for distributional generalization, 2023. URL https://openreview. net/forum?id=PoU_NgCStE5. Izmailov, P., Podoprikhin, D., Garipov, T., Vetrov, D., and Wilson, A. G. Averaging weights leads to wider op- tima and better generalization, 2018. URL https: //arxiv.org/abs/1803.05407. Kingma, D. P. and Ba, J. Adam: A method for stochastic op- timization, 2014. URL https://arxiv.org/abs/ 1412.6980. Klimt, B. and Yang, Y . The enron corpus: A new dataset for email classification research. In European conference on machine learning, pp. 217–226. Springer, 2004. Koh, P. W., Sagawa, S., Marklund, H., Xie, S. M., Zhang, M., Balsubramani, A., Hu, W., Yasunaga, M., Phillips, R. L., Gao, I., Lee, T., David, E., Stavness, I., Guo, W., Earnshaw, B. A., Haque, I. S., Beery, S., Leskovec, J., Kundaje, A., Pierson, E., Levine, S., Finn, C., and Liang, P. Wilds: A benchmark of in-the-wild distribu- tion shifts, 2020. URL https://arxiv.org/abs/ 2012.07421. Koskela, A. and Kulkarni, T. Practical differentially private hyperparameter tuning with subsampling, 2023. Krizhevsky, A. et al. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009. Kulynych, B., Yang, Y .-Y ., Yu, Y ., Błasiok, J., and Nakkiran, P. What you see is what you get: Principled deep learning via distributional generalization, 2022. Kumar, A., Raghunathan, A., Jones, R., Ma, T., and Liang, P. Fine-tuning can distort pretrained features and un- derperform out-of-distribution, 2022a. URL https: //arxiv.org/abs/2202.10054. Kumar, A., Raghunathan, A., Jones, R., Ma, T., and Liang, P. Fine-tuning can distort pretrained features and un- derperform out-of-distribution, 2022b. URL https: //arxiv.org/abs/2202.10054. Kumar, A., Shen, R., Bubeck, S., and Gunasekar, S. How to fine-tune vision models with sgd, 2022c. URL https: //arxiv.org/abs/2211.09359. Kuo, K., Thaker, P., Khodak, M., Nguyen, J., Jiang, D., Talwalkar, A., and Smith, V . On noisy evaluation in federated hyperparameter tuning, 2023. Li, H., Chaudhari, P., Yang, H., Lam, M., Ravichan- dran, A., Bhotika, R., and Soatto, S. Rethinking the hyperparameters for fine-tuning, 2020. URL https: //arxiv.org/abs/2002.11770. Li, T., Zaheer, M., Liu, K. Z., Reddi, S. J., McMahan, H. B., and Smith, V . Differentially private adaptive optimization with delayed preconditioners, 2023. Li, X., Liu, D., Hashimoto, T., Inan, H. A., Kulkarni, J., Lee, Y ., and Thakurta, A. G. When does differentially private learning not suffer in high dimensions? In Advances in Neural Information Processing Systems , pp. 28616– 28630, 2022a. Li, X., Tram`er, F., Liang, P., and Hashimoto, T. Large lan- guage models can be strong differentially private learners. In International Conference on Learning Representations, 2022b. Liu, J. and Talwar, K. Private selection from private candi- dates, 2018. 12A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization Liu, Z., Mao, H., Wu, C.-Y ., Feichtenhofer, C., Darrell, T., and Xie, S. A convnet for the 2020s, 2022. URL https://arxiv.org/abs/2201.03545. Malladi, S., Lyu, K., Panigrahi, A., and Arora, S. On the SDEs and scaling rules for adaptive gradient algo- rithms. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/ forum?id=F2mhzjHkQP. Malladi, S., Gao, T., Nichani, E., Damian, A., Lee, J. D., Chen, D., and Arora, S. Fine-tuning language models with just forward passes, 2024. McMahan, H. B., Ramage, D., Talwar, K., and Zhang, L. Learning differentially private recurrent language mod- els, 2017. URL https://arxiv.org/abs/1710. 06963. Mehta, H., Krichene, W., Thakurta, A. G., Kurakin, A., and Cutkosky, A. Differentially private image classifica- tion from features. Transactions on Machine Learning Research, 2023a. ISSN 2835-8856. URL https:// openreview.net/forum?id=Cj6pLclmwT. Ex- pert Certification. Mehta, H., Thakurta, A. G., Kurakin, A., and Cutkosky, A. Towards large scale transfer learning for differentially pri- vate image classification.Transactions on Machine Learn- ing Research, 2023b. ISSN 2835-8856. URL https:// openreview.net/forum?id=Uu8WwCFpQv. Ex- pert Certification. Mehta, R., Albiero, V ., Chen, L., Evtimov, I., Glaser, T., Li, Z., and Hassner, T. You only need a good embeddings extractor to fix spurious correlations, 2022. URLhttps: //arxiv.org/abs/2212.06254. Merity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer sentinel mixture models. In International Conference on Learning Representations, 2017. URL https:// openreview.net/forum?id=Byj72udxe. Mohapatra, S., Sasy, S., He, X., Kamath, G., and Thakkar, O. The role of adaptive optimizers for honest private hyperparameter selection, 2021. Nasr, M., Mahloujifar, S., Tang, X., Mittal, P., and Houmansadr, A. Effectively using public data in pri- vacy preserving machine learning. In Proceedings of the 40th International Conference on Machine Learning, pp. 25718–25732. PMLR, 2023. Panda, A., Mahloujifar, S., Bhagoji, A. N., Chakraborty, S., and Mittal, P. Sparsefed: Mitigating model poisoning attacks in federated learning with sparsification, 2021. URL https://arxiv.org/abs/2112.06274. Papernot, N. and Steinke, T. Hyperparameter tuning with renyi differential privacy, 2021. URL https: //arxiv.org/abs/2110.03620. Peng, Z., Dong, L., Bao, H., Ye, Q., and Wei, F. Beit v2: Masked image modeling with vector-quantized visual tokenizers, 2022. URL https://arxiv.org/abs/ 2208.06366. Polyak, B. and Juditsky, A. B. Acceleration of stochastic approximation by averaging. Siam Journal on Control and Optimization, 30:838–855, 1992. Qian, N. On the momentum term in gradient descent learn- ing algorithms. Neural networks, 12(1):145–151, 1999. Qiao, S., Wang, H., Liu, C., Shen, W., and Yuille, A. Micro-batch training with batch-channel normalization and weight standardization, 2019. URL https:// arxiv.org/abs/1903.10520. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. Language models are unsupervised multitask learners. 2019. Rahimian, H. and Mehrotra, S. Frameworks and results in distributionally robust optimization. Open Journal of Mathematical Optimization, 3:1–85, jul 2022. doi: 10. 5802/ojmo.15. URL https://doi.org/10.5802% 2Fojmo.15. Rajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing , pp. 2383– 2392, 2016. Ryu, E. K. and Boyd, S. P. A primer on monotone operator methods. 2015. Sagawa, S., Koh, P. W., Hashimoto, T. B., and Liang, P. Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case gen- eralization, 2019. URL https://arxiv.org/abs/ 1911.08731. Sander, T., Stock, P., and Sablayrolles, A. Tan without a burn: Scaling laws of dp-sgd, 2022. URL https: //arxiv.org/abs/2210.03403. Shejwalkar, V ., Ganesh, A., Mathews, R., Thakkar, O., and Thakurta, A. Recycling scraps: Improving private learn- ing by leveraging intermediate checkpoints, 2022. URL https://arxiv.org/abs/2210.01864. Shi, W., Chen, S., Zhang, C., Jia, R., and Yu, Z. Just fine- tune twice: Selective differential privacy for large lan- guage models. arXiv preprint arXiv:2204.07667, 2022. 13A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization Shorten, C. and Khoshgoftaar, T. M. A survey on image data augmentation for deep learning. Journal of Big Data, 6(1):60, Jul 2019. ISSN 2196-1115. doi: 10.1186/ s40537-019-0197-0. URL https://doi.org/10. 1186/s40537-019-0197-0 . Song, S., Chaudhuri, K., and Sarwate, A. D. Stochastic gradient descent with differentially private updates. In 2013 IEEE Global Conference on Signal and Information Processing, pp. 245–248, 2013. doi: 10.1109/GlobalSIP. 2013.6736861. Sun, Z., Suresh, A. T., and Menon, A. K. The importance of feature preprocessing for differentially private linear optimization. In International Conference on Learning Representations, 2024. Sutskever, I., Martens, J., Dahl, G., and Hinton, G. On the importance of initialization and momentum in deep learn- ing. In Dasgupta, S. and McAllester, D. (eds.), Proceed- ings of the 30th International Conference on Machine Learning, volume 28 of Proceedings of Machine Learn- ing Research, pp. 1139–1147, Atlanta, Georgia, USA, 17– 19 Jun 2013. PMLR. URL https://proceedings. mlr.press/v28/sutskever13.html. Tang, X., Panda, A., Sehwag, V ., and Mittal, P. Differen- tially private image classification by learning priors from random processes. In Advances in Neural Information Processing Systems, 2023. Tang, X., Panda, A., Nasr, M., Mahloujifar, S., and Mittal, P. Private fine-tuning of large language models with zeroth- order optimization, 2024. Team, A. Learning with privacy at scale, 2017. URL https://docs-assets.developer. apple.com/ml-research/papers/ learning-with-privacy-at-scale.pdf . Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and Jegou, H. Training data-efficient image transform- ers; distillation through attention. In Meila, M. and Zhang, T. (eds.), Proceedings of the 38th International Confer- ence on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 10347–10357. PMLR, 18–24 Jul 2021. URLhttps://proceedings.mlr. press/v139/touvron21a.html. Tram`er, F., Kamath, G., and Carlini, N. Considerations for differentially private learning with large-scale public pretraining, 2022. Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. R. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In International Conference on Learning Representations, 2019a. URL https://openreview.net/forum? id=rJ4km2R5t7. Wang, H., Gao, S., Zhang, H., Su, W. J., and Shen, M. DP- hyPO: An adaptive private framework for hyperparameter optimization. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https: //openreview.net/forum?id=3Py8A1j5N3. Wang, Y .-X., Balle, B., and Kasiviswanathan, S. P. Sub- sampled renyi differential privacy and analytical mo- ments accountant. In Chaudhuri, K. and Sugiyama, M. (eds.), Proceedings of the Twenty-Second Interna- tional Conference on Artificial Intelligence and Statis- tics, volume 89 of Proceedings of Machine Learning Research, pp. 1226–1235. PMLR, 16–18 Apr 2019b. URL https://proceedings.mlr.press/v89/ wang19b.html. Wightman, R. Pytorch image models. https://github. com/rwightman/pytorch-image-models, 2019. Wolf, T., Debut, L., Sanh, V ., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer, S., von Platen, P., Ma, C., Jernite, Y ., Plu, J., Xu, C., Scao, T. L., Gugger, S., Drame, M., Lhoest, Q., and Rush, A. M. Huggingface’s transformers: State-of-the-art natural language processing, 2019. URL https://arxiv.org/abs/1910.03771. Xiao, H., Rasul, K., and V ollgraf, R. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms, 2017. URL https://arxiv.org/abs/ 1708.07747. Yousefpour, A., Shilov, I., Sablayrolles, A., Testuggine, D., Prasad, K., Malek, M., Nguyen, J., Ghosh, S., Bharadwaj, A., Zhao, J., Cormode, G., and Mironov, I. Opacus: User- friendly differential privacy library in pytorch, 2021. URL https://arxiv.org/abs/2109.12298. Yu, D., Zhang, H., Chen, W., Yin, J., and Liu, T.-Y . Large scale private learning via low-rank reparametrization, 2021. Zhang, S., Dinan, E., Urbanek, J., Szlam, A., Kiela, D., and Weston, J. Personalizing dialogue agents: I have a dog, do you have pets too?, 2018a. Zhang, S., Dinan, E., Urbanek, J., Szlam, A., Kiela, D., and Weston, J. Personalizing dialogue agents: I have a dog, do you have pets too?, 2018b. URL https: //arxiv.org/abs/1801.07243. Zhu, Y . and Wang, Y .-X. Poission subsampled r´enyi dif- ferential privacy. In Proceedings of the 36th Interna- tional Conference on Machine Learning, pp. 7634–7642. PMLR, 2019. 14A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization A. Further Results for Computer Vision Tasks Our code is available at the following URL: https://anonymous.4open.science/r/dp-custom-32B9/README.md A.1. Experimental Set-up Hyperparameter Search Space. We use a logarithmic grid for the learning rate η ∈ [10−7, 10−4]. We use the same grid for the CIFAR training from scratch experiments, and the NLP experiments. We scale the learning rate by the batch size (the original linear scaling rule). The number of epochs depends on the maximum number of iterations that we can do in the provided time. ImageNet details. The architecture is a modernized ViT (Fang et al., 2023b) pretrained on IN-21k Deng et al. (2009) with CLIP. We use a resource-efficient finetuning approach where we create a linear layer aggregating the intermediate representations from each Transformer block, following Tang et al. (2023). We apply the method from Sun et al. (2024) to preprocess the features, allocating a budget of ε = 0.05 for the private mean estimation. For the HPO, we do 3 runs at ε = 0.1, followed by 3 runs at ε = 0.2 and a final run at ε = 0.88, which produces a cumulative privacy cost including HPO of ε = 1.0. Here we express the privacy values in terms of ε for brevity, but the actual expressions are in terms of µ, the parameter for f-DP. Because of the nature of composition, the hyperparameter search only costs us the difference in performance between ε = 0.88 and ε = 1.0, which is minimal. We search across values of T ranging from 1 (a single epoch) to 20 (the most we can do in the maximum amount of time a job will run on our cluster). (Berrada et al., 2023) report that fine-tuning the full architecture produces better results than linear probing. However, we lack the computational resources to do full fine-tuning of large transformers, but we can do linear probing of the extracted features in under an hour on a single A100. CIFAR training from scratch. We use the model from Tang et al. (2023), a WideResNet-16-4, and train only the last layer on the extracted features from previous layers. The model is pretrained on synthetic data that does not resemble real-world data. We choose this model because it is the SOTA model for CIFAR training from scratch, and we want to validate that our private HPO can produce competitive results in a setting where the zero-shot performance is poor (indeed, the zero-shot performance of this model is just random chance, because it has never seen any real images before) but the ceiling for performance is quite high. Comparison to DPAdamWOSM details. We implement DPAdamWOSM (Mohapatra et al., 2021) to the best of our ability in wosm impl.py since there is no code available, and tune the necessary hyperparameter T (# of epochs) between 1 and 200 and report the performance for the best value of T without accounting for the privacy cost of this tuning. The rest of the hyperparameter choices and model architecture mirror our own. At a high level, our linear scaling rule attempts to do a data-dependent learning rate selection, while DPAdamWOSM does a data-independent learning rate selection. It is natural that for hard tasks (ImageNet) the data-independent choice may not work well. We note that while DPAdamWOSM does not require tuning the learning rate, we still need to tune the number of epochs. Therefore, even if further tuning for DPAdamWOSM could match the utility of the linear scaling rule, it would not match the privacy guarantee. Ultimately we think these works are compatible, because we can use our HPO to tune the number of epochs in DPAdamWOSM. Comparison to Koskela & Kulkarni (2023) details. . We implement the ability to train on a subset of ImageNet in our codebase by passing the flags start idx, end idx. As an initial test, we tried doing HPO on half the dataset by passing start idx=0, end idx=625. Our code will produce a random permutation of the chunks of ImageNet (1251 total) and then load the first half. We compare η = 0.01, η= 1.0 on this half-dataset. On the full dataset, these produce very different performance; η = 0.01 achieves 81% at ε = 1.0, while η = 1.0 achieves 87%. However, on the half dataset, the first learning rate achieves 43.2% performance, while the second achieves 41.4%. Inspecting the loss curves, we find that both learning rates overfit the training dataset and do not generalize to the validation set, but the second learning rate overfits more. We then try training two models on disjoint sets of the dataset and combining them via parallel composition. This achieves 83%, which is worse than training on the entire dataset. This may be an interesting direction for future work. We tried a number of other strategies to try and scale the idea of Koskela & Kulkarni (2023) to ImageNet scale, such as weight decay, smaller learning rate, single-epoch training, etc. but were unable to produce a recipe where performance on the half-dataset was consistently positively correlated with performance on the full dataset. We suspect that there is a factor of the number of classes that is needed to properly calibrate the subsampling. 15A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization Table 12.Linear Scaling on ImageNet is competitive with (Mehta et al., 2023b) and (Mehta et al., 2023a) ε (Mehta et al., 2023b) (Mehta et al., 2023a) Ours r = η × T 0.25 75.6 - 79.0 250 0.50 79.4 86.1 81.6 750 1.00 81.1 86.8 83.2 1100 2.00 81.5 87.4 84.2 2000 10.0 81.7 - 85.4 2000 ∞ 86.9 88.9 85.7 2000 Models. We evaluate five models: two masked-image modeling transformers, beit (Bao et al., 2021) and beitv2 (Peng et al., 2022), their backbone architecture ViT (Dosovitskiy et al., 2020) at both the base and large scales, and the pure convolutional architecture convnext (Liu et al., 2022). All models are pretrained on ImageNet-21k (Deng et al., 2009). These models span a range of input resolutions: beitv2 (224x224), convnext, vit-base, vit-large (384x384), and beit (512x512) and we upsample images to the necessary input size. For text generation we use GPT-2 (Radford et al., 2019) at the smallest scale, and RoBERTa-base. Availability. Our results tune open source models from the PyTorch timm package (Wightman, 2019) using existing privacy accounting from (Gopi et al., 2021) and per-sample clipping code in (Yousefpour et al., 2021), and can be reproduced in minutes. B. Further ImageNet Results. We perform additional experiments on ImageNet with the same architecture as prior work to better understand the tradeoffs of our method. We use a ViT-g that was pretrained on laion-2b, to compare to the ViT-g models in prior work that were pretrained on JFT-4b. It is trivial that linear scaling outperforms a naive grid search, but we also compare the effectiveness of linear scaling against the hyperparameter selection strategies used in prior work (Mehta et al., 2023a). We find that use of our new rule can unlock significant improvements for a range of ε when we hold both approaches accountable for the privacy cost of hyperparameter tuning. We apply linear scaling to the ViT model used in (Mehta et al., 2023a) on CIFAR100. Although (Mehta et al., 2023a) do not directly state the hyperparameters for their best results, they specify that they use 200 hyperparameter trials with Bayesian optimization. While they obtain RDP guarantees, these guarantees do not include the privacy cost of non-privately tuning hyperparameters. We apply the linear scaling rule to extrapolate a value ofr from ε = 0.1 to ε = 1, obtaining r = 20 = η(0.2) × T(100). We recover performance of82.7% for ε = 1, a 2% improvement over the best result for DP-Adam in (Mehta et al., 2023a) while accounting for the privacy cost of hyperparameter tuning. They obtain their best result for DP-Adam at T = 10, but we cannot compute the corresponding value of r because they do not provide η. However, because they use a clipping norm of 0.005 we can reasonably infer that their value of r is ≈ 1000× smaller than ours. This is farther from the optimal non-private training, as evidenced by the performance gap. Linear Scaling scales to ImageNet In Table 12 we do a granular comparison between our method and (Mehta et al., 2023b;a). We observe that our method is competitive with (Mehta et al., 2023a) even when accounting for the privacy cost of hyperparameter search, and that the linear scaling rule holds up at the scale of ImageNet for very large values of r = η × T. The non-private accuracy of their closed-source model is 3.2% higher than our open-source model, and so the private accuracy at ε = 2 is also 3.2% higher. However, ultimately our method and the method of Mehta et al. (2023a) are complementary, because their method introduces new hyperparameters that we intuit our linear scaling rule can optimize. We attempted to validate this intuition empirically but were unable to reproduce the results of Mehta et al. (2023a) because they and Mehta et al. (2023b) pretrain on the closed- source JFT dataset with billions of images. We note that all numbers we report for models pretrained on ImageNet-21k using first-order methods surpass those in (Mehta et al., 2023a), but for sufficiently small values of ε on harder datasets the second-order methods they propose provide better performance. We note that the method in Mehta et al. (2023a) only works for vision tasks, whereas our approach works for both vision and language tasks. 16A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization ε1 ε2 εf Acc Std - - 1.0 99.00 0.01 0.01 0.05 0.99 98.88 0.01 0.05 0.1 0.96 98.85 0.03 0.05 0.2 0.9 98.81 0.01 0.1 0.2 0.88 98.81 0.01 0.2 0.3 0.7 98.79 0.03 Table 13.The marginal cost of our HPO method is low. The first row represents the oracle. The dataset is CIFAR10. The marginal cost of linear scaling is low. Table 13 shows that the marginal cost of our HPO method is low. In the case of CIFAR10, this is because the oracle at ε = 0.1 achieves > 98% accuracy. Linear Scaling produces robust results. In Fig. 3 we report that following Algorithm 2 produces new state-of-the-art results for all values of ε, shown in Table 7. In Appendix A.1 we provide detailed computations of the linear interpolation for multiple datasets and in Appendix B.4 we provide full results across the entire hyperparameter search space. 5 10 20 30 40 50 60 70 80 90 100 epochs 0.05 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 lr  = 0.05 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.10 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.20 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.30 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.40 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.50 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.60 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.70 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.80 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.90 5 10 20 30 40 50 60 70 80 90 100 epochs  = 1.00 Figure 5.Heatmaps for beit on CIFAR100. ε increases from 0.05 → 1.0 left to right on the grid-axis, iterationsT increases from 5 → 100 left to right on the individual plot axis, and the learning rate η increases from 0.05 ↓ 1 top to bottom on the individual plot axis. As ε increases, left to right, the optimal value of η × T increases in accordance with the new linear scaling rule. Prior work has generally operated in the top-left regime, that is often suboptimal. Decomposing r into η, T One of the advantages of our search method is that we combine the parameters that we need to search into one meta-parameter, the radius r, which allows us to perform linear interpolation and also allows us to improve the runtime of the intermediate HP trials. We uniformly sample r from the search space defined by rmin = ηmin ×Tmin, (rmax = ηmax ×Tmax. We evaluate 3 methods for decomposing r. 1) We decompose r by sampling η, Tfrom their search spaces until their product is close to the target r. 2) We sample T uniformly, then get η = r/T . 3) We sample η, Tuniformly from their search spaces. We don’t observe any significant difference between these methods. Note that the product of uniform distributions is not uniform. The robustness of the rule that “combinations of η, Tthat evaluate to the same product perform similarly” is crucial to the success of our method, because it enables us to fit a line rather than a more complex function that might require more evaluations. In Figure 6 and Figure 5 our results validate that this rule is robust: we can move from one set of hyperparameters to another similarly performing set of hyperparameters by increasing the number of iterations T by a constant factor and decreasing the learning rate η by the same factor (or vice versa). We find that any inaccuracy incurred by estimating the best value of r with the linear scaling rule will not reduce accuracy by much compared to doing grid search for the optimal value of r, but does reduce the privacy cost of hyperparameter tuning immensely. Method to Reduce ε ε = 0.7 Degradation from ε = 1.0 Subsampling 41.34% 45.66% Increasing Noise 84.07% 2.93% Table 14.Comparing methods to reduce privacy cost (ε) on ImageNet. Increasing noise is more effective than subsampling for reducing ε with minimal performance degradation. 17A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization 0 20 40 60 80 100 R = × T 0 1 2 3 4 5T est Accuracy Gap(Relative)    R vs TestAccuracy =[0.1,0.2,0.3,0.4,0.5] =0.05 =0.10 =0.20 =0.30 =0.40 =0.50 Figure 6.A scatter plot of r = η × T the total step size vs the relative gap in test accuracy on CIFAR100 Beitv2; this gap is measured as the difference between the test accuracy at the plotted value of r and the optimal value of r. Optimizing r for any value of ε and transferring this, e.g. via the linear scaling rule, will not reduce accuracy by much compared to the optimal hyperparameters. Figure 7.We compare the best private and best non-private performances of all models on all datasets. We use the linear scaling rule to scale hyperparameters from ε = 0.1 to ε = 1, so our privacy analysis includes the cost of hyperparameter tuning. Model Dataset ε = 1 ε = ∞ Gap beitv2 CIFAR10 98.90 99 .00 0 .10 CIFAR100 89.10 91 .57 2 .47 FMNIST 91.02 91 .53 0 .51 STL10 99.69 99 .81 0 .12 EMNIST 81.77 82 .00 0 .23 convnext CIFAR10 96.75 97 .22 0 .47 CIFAR100 83.47 86 .59 3 .12 FMNIST 90.23 91 .13 0 .9 STL10 99.61 99 .71 0 .10 EMNIST 78.38 79 .05 0 .67 beit CIFAR10 98.19 98 .51 0 .32 CIFAR100 87.1 90 .08 2 .98 FMNIST 90.55 91 .6 1 .05 STL10 99.62 99 .78 0 .16 EMNIST 81.48 83 .25 1 .77 vit-L CIFAR10 98.29 98 .44 0 .40 CIFAR100 86.18 89 .72 3 .54 FMNIST 90.58 91 .37 0 .79 STL10 99.62 99 .76 0 .14 B.1. Linear Scaling enables empirical analysis Many interesting questions in DP fine-tuning remain unanswered because of the immense computational overhead of evaluating hundreds of hyperparameter trials for each privacy budget, model architecture and dataset (Mehta et al., 2023a). We now employ the linear scaling rule to efficiently answer key questions in DP fine-tuning for vision tasks. Impact of model architectures on differential privacy Many pretrained model architectures are available (Wolf et al., 2019) but prior work has generally engaged with a single architecture, e.g. beit (Bu et al., 2022a) or ViT (Mehta et al., 2023b). We leverage our method to answer three questions: • What model architectures can provide good DP classifiers? • Is the best model task-specific, e.g., is an architecture search required? • Does the private-non private utility gap depend on the model architecture? We report our findings in Tab. 7. We evaluate multiple transformer architectures in ViT (Dosovitskiy et al., 2020), beitv1 (Bao et al., 2021) and beitv2 (Peng et al., 2022), as well as the purely convolutional architecture Convnext (Liu et al., 2022). We 18A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization Figure 8.In-distribution (ID) and out-of-distribution (OOD) performance on benchmark distribution shift datasets. We use the linear scaling rule to scale hyperparameters from ε = 0.1 to ε = 1, so our privacy analysis includes the cost of hyperparameter tuning. Dataset ε = 1.0 ID(OOD) Prior ( ε = ∞) Waterbirds 92.31 (91.59) 98.3(80.4) fMoW 45.44 (35.31) 49.1 (36.6) Camelyon 93.91 (93.55) 99.5 (96.5) C10 → STL 99.0 (98.82) 97.5 (90.7) C10 → C10p1 99.0 (97.85) 97.5 (93.5) C10 → C10C 99.0 (89.98) 96.56 (92.78) C100 → C100C 89.65 (68.69) 81.16 (72.06) find that all architectures can serve as good backbones for high-accuracy DP classification. This is somewhat surprising because the different inductive biases of transformers and purely convolutional architectures tend to produce differently structured features, but we reason that the noise added by DP will ‘smooth out’ these decision boundaries regardless of architecture. We note that one architecture, beitv2, performs the best on all benchmarks and also has the highest non-private ImageNet accuracy (Wightman, 2019). We therefore recommend that practitioners do not worry about architecture search when fine-tuning as this can incur further privacy costs, and instead pick the best model available. We are encouraged to report that the private-non private utility gap diminishes with model accuracy, enabling us to report for the first time lossless privacy of 99.0% on CIFAR10 at ε = 1 (without considering the cost of HPO) and the gap is only < 0.10% if we consider the cost of HPO. We expect that as pretrained models become even better, future works may even be able to attain lossless privacy on CIFAR100, that we note remains somewhat challenging for private fine-tuning. We harness these insights for our next analyses. DP models are robust to distribution shifts. If we assume the existence of some publicly available data for pretraining and then do DP fine-tuning on the private data, it’s crucial that there is no privacy leakage between the public data and private data. There is only 0 distribution shift when public = private, and this violates the key assumption (no privacy leakage because public and private data are sufficiently different) in DP fine-tuning. If the public data is so different from the private data that it can be used for pretraining without privacy leakage, there must be some distribution shift. Benchmarking performance on datasets with distribution shifts is increasingly important because real-world problems almost always contain distribution shift between model training and inference (Rahimian & Mehrotra, 2022). We show that DP-SGD provides robustness to covariate, subpopulation and label distribution shifts for synthetic and natural datasets. We compare to other methods that consider this question. Details on OOD Experiments We specify exact details for all OOD experiments. Our training details are drawn from prior work (Kumar et al., 2022c;b; Diffenderfer et al., 2021). Waterbirds: the ID→OOD contains a well-studied spurious correlation in the binary classification problem. (Mehta et al., 2022) evaluate vision transformers without using group knowledge and obtain ≈ 80 % ID accuracy, but much worse (≈ 60%) OOD accuracy, and (Kumar et al., 2022c) tailor their method to this task and get the reported results. Surprisingly, just fine-tuning a linear model on the extracted features outperforms both works for OOD accuracy for ε = 0.1. This trend (sacrificing ID accuracy for increased OOD robustness) is seen in other OOD results, and we hypothesize that this is due to the inherent regularization present in DP-SGD. Fmow: we train on region 3 (ID) and evaluate on regions 1,2 (OOD), following (Kumar et al., 2022b). Camelyon17: we again follow (Kumar et al., 2022b). CIFAR10 → STL10, CIFAR10p1: We train privately on CIFAR10 using our best hyperparameters returned from the linear scaling rule and then transfer this to STL10/CIFAR10p1, with the label reassignment following (Kumar et al., 2022c). Common Corruptions: We evaluate on the average severity of the ’gaussian blur’ corruption. We leverage our method to answer three questions: • Can DP help when there is a domain shift from private fine-tuning to test? 19A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization • Can DP help when there is a domain shift from public data to private fine-tuning? • Can DP fine-tuned models perform well in the zero-shot setting? In Table 8 we compare the performance of our method across 8 benchmarks and find that the answer to all three of these questions is yes. The Waterbirds dataset is a well-known benchmark for evaluating the robustness of models to spurious correlations. There is a domain shift between the private training data and the private test data created by class imbalance. We are surprised to find that in the absence of any other regularization methods, DP fine-tuning actually improves performance on the OOD split. We hypothesize that the lackluster OOD non-private performance is caused by the model overfitting to the spurious correlation in the training data, and that the inherent regularization of DP prevents the model from memorizing this spurious correlation. By comparing our results to Mehta et al. (2022) we determine that this robustness is unique to DP rather than an artifact of the pretrained model. Although DP does significantly degrade the ID performance, in situations where minimizing OOD error is more important, we believe that DP by itself can mitigate the domain shift from private fine-tuning to test. Because our central assumption in DP fine-tuning is that there is no privacy leakage from the pretraining data to the private training data, it is important to understand how DP fine-tuning performs when there is a distribution shift between public data and private data. fMoW (Christie et al., 2017) and Camelyon17 (B ´andi et al., 2019) are two datasets that represent a signficant distribution from the pretraining data (ImageNet). We observe a similar relationship between ID and OOD degradation as above, where the OOD degradation is somewhat mitigated by DP. If we compare our results on Camelyon to the best results in Ghalebikesabi et al. (2023) we find that we can improve their best performance from 91.1% at ε = 10 to 93.91% at ε = 1. Although performance on fMoW remains quite poor, we note that it is not significantly worse than in the non-private setting. We believe that DP fine-tuning from pretrained models remains a viable strategy even when the publicly available pretraining data has a very large distribution shift from the private target data. We finally consider the zero-shot setting, where we fine-tune a model on CIFAR and then transfer it without updating any parameters to private test datasets that once again represent a distribution shift from CIFAR. We report the performance in the OOD column. For the more minute distribution shifts of STL and CIFAR10p1 we find that the fine-tuned classifier can achieve remarkable performance without ever updating parameters on these datasets; that is, we just remap the labels as per (Kumar et al., 2022b). CIFAR10C and CIFAR100C represent larger distribution shifts and are used to benchmark the robustness of models to commonly reported image corruptions (Hendrycks & Dietterich, 2019). Our OOD performance on these larger distribution shifts is much worse, particularly for CIFAR100 where there is a> 20% degradation. Although this is lower than the top result on the RobustBench leaderboard (Croce et al., 2021) obtains 85% accuracy, we note that once again we used no additional methods beyond DP to ensure robustness but managed to achieve reasonable performance to distribution shifts in zero-shot classification. Comparison to other works on distribution shift under DP. Prior work in distributionally robust optimization (DRO) has addressed this problem by using knowledge of the relative imbalances between groups, but recent work with vision transformers has shown that linear probing can perform well on datasets with distribution shifts (Mehta et al., 2022; Kumar et al., 2022a;c). Kulynych et al. (2022) proposes DP-IS-SGD that improves the robustness of DP-SGD by removing per-sample gradient clipping (therefore removing the introduced bias but also losing the privacy guarantee; see 4.2) and uses knowledge of the groups to sample subpopulations at different rates to improve robustness. Because our method uses DP-GD to maximize the signal-to-noise ratio of updates and requires clipping (because our primary goal is the privacy guarantee, unlike Kulynych et al. (2022) which focuses on DRO) and we do not assume knowledge of groups, we cannot make use of DP-IS-SGD. Hulkund et al. (2023) concludes that ”[DP-SGD] is not a good candidate for improving robustness under covariate or subpopulation shift, as it comes at a major cost to accuracy.” This conclusion runs counter to our findings, and we believe the reason is because their numerical findings are not conclusive. Our interpretation of their results is that because their DP-SGD degrades accuracy, it should also increase robustness; however we find that even when DP-SGD does not degrade accuracy it still improves robustness. B.2. Detailed Ablations In this subsection we deal with detailed ablations of each step in the method that we use. We ablate each step and show their individual benefits in Table 15. At a high level, we want to maximize the signal-to-noise ratio of updates, accelerate training to minimize the impact of noise on the optimization trajectory, and apply the linear scaling rule to select the best 20A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization Table 15.Our method fixes six design choices: the architecture and initialization (for CV tasks only), the batch size (full batch), the optimizer (SGD with momentum=0.9), the accounting method (PLV where all prior HPO methods use RDP), and the clipping norm (unit clipping). We report the improvement derived from following each of these techniques with respect to a competitive baseline from prior work on CIFAR100 at ε = 0.1. Method Baseline Baseline Accuracy Improvement Classifier (no bias) (Mehta et al., 2023b) 71.3 0 .36 Zero Initialization Random Initialization (De et al., 2022) 64.85 6 .81 Gradient Descent SGD(Batch=4096) (De et al., 2022) 70.2 1 .46 Momentum (ρ = 0.9) ρ = 0 (Bu et al., 2022a) 69.02 2 .09 PLV Accounting RDP (De et al., 2022) 68.43 3 .23 Unit Clipping (C = 1) C ≪ 1 (Mehta et al., 2023a) 71.2 0 .46 hyperparameters while maintaining a given overall privacy budget. 1) Extract features from a private dataset using an open source feature extractor pretrained on a public dataset. A valid criticism of this approach in private fine-tuning is that the fine-tuning dataset can be in-distribution with the training dataset, and this may violate privacy. To address this we evaluate our method on eight datasets that have been used as distribution shift benchmarks in Sec. 5. 2) Zero-initialize a linear classifier that maps features to classes. Prior work has studied full network fine-tuning (Cattan et al., 2022; Bu et al., 2022a; De et al., 2022) but we find that by doing logistic regression on a linear classifier we minimize the number of parameters, and mitigate the curse of dimensionality. We further simplify the choice of initialization by initializing all parameters to zero. 3) Apply linear scaling to privately select the step size and number of steps. We propose a new linear scaling rule: increase either the step size η or number of steps T so that the total step size r = η × T is linear in ε. This reduces the hyperparameter search to a binary search in r. Furthermore we can do a hyperparameter search for r using a small privacy budget, and then linearly scale up this value to minimize the cost of hyperparameter search(Alg. 2). Using privacy loss accounting enables us to get competitive accuracy for privacy budgets as small as ε = 0.01, so these low-cost trials can inform better hyperparameters. our method already minimizes the private-nonprivate performance gap atε = 1.0 as we show in Table 7, so spending ε = 0.1 for hyperparameter tuning does not significantly degrade accuracy. Unless stated explicitly otherwise, all privacy-utility tradeoffs reported for our method in the main body include the privacy cost of hyperparameter tuning via the linear scaling rule. 4) Compute the full batch gradient. This optimizes the signal-to-noise ratio of the update and enables use of large step sizes (Goyal et al., 2017). We achieve 91.52% accuracy on CIFAR10 (|D| = 5e4) for ε = 0.01 when training for 100 epochs with noise multiplier σ = 2561. When the noise is divided by the batch size, the effective noise multiplier is σ |B|=5e4 ≈ 0.05 and the SNR is 1 0.05 = 20. When we use subsampling with sampling probability p = 0.2 and train for the same number of epochs under the same privacy budget, our effective noise multiplier is σ |B| = 1145 1e4 = 0.114, and the corresponding SNR of 1 0.114 = 8.7 is much worse than in the full batch setting. 5) Clip per-sample gradients to unit norm. As per Eq. 1 reducing the per-sample gradient below 1 is equivalent to reducing η (and thus reducing the step size) while simultaneously biasing optimization. By setting c = 1 we can simplify r = η × T × c to r = η × T. 6) Use privacy loss variable accounting. Gopi et al. (2021) provides a tool to calibrate Gaussian noise for the given privacy budget and add noise to the gradient: this enables budgeting for small values of ε without underestimating privacy expenditure. 7) Use momentum. Acceleration has a host of well-known benefits for optimization and is ubiquitous in non-private optimization (Qian, 1999; Kingma & Ba, 2014), but prior work has not always used momentum because it can lead DP-SGD astray when the SNR of updates is low (De et al., 2022). Because we optimize the SNR of individual updates in (4), we can make use of momentum. 21A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization 0 10 20 30 40 50 60 Epochs 10 20 30 40 50 60 70T est Acc Momentum Ablation.   Dataset=CIFAR100, Arch=beit, =0.1 = 0 = 0.9 (a) Momentum Acceleration 40 42 44 46 48 50 Epochs 65 66 67 68 69 70 71 72 73T est Acc Momentum Post-Processing.   Dataset=CIFAR100, Arch=beit, =0.1 T est Accuracy Free Step (b) Momentum Post-Processing Figure 9.Ablation of momentum parameter during training (left) and post processing of the parameter exponential moving average stored in the momentum buffer to take an extra step ’for free’ (right). Use of both methods increases performance slightly. Momentum Accelerates Convergence. Despite the exhaustive study of the acceleration of gradient descent with momen- tum done by prior work (Sutskever et al., 2013; Qian, 1999) work on DP-SGD generally eschews the use of a momentum term. A notable exception (Mehta et al., 2023a) use AdamW rather than SGD with momentum; in a later section we discuss the reason to prefer SGD with momentum. The reason to use momentum to accelerated the convergence of DP-SGD is straightforward: the exponentially moving average of noisy gradients will have higher SNR than individual gradients. Furthermore, momentum is shown to provably benefit normalized SGD (Cutkosky & Mehta, 2020). In Fig. 9 we observe that momentum complements our new linear scaling rule and accelerates convergence. Separately, we report the improvement of taking a step ’for free’ in the direction of the exponential moving average stored during training in the momentum buffer. Note that this exponential moving average is in no way tied to momentum, and it is equivalent to perform DP-SGD without acceleration, store an exponential moving average of gradients with decay parameter γ = 0.9, and then take an additional step in the direction of the stored gradient average after training has finished; we only use the momentum buffer for ease of implementation. As we discuss above when introducing the new linear scaling rule, we maximize performance by maximizing SNR and terminating training while the model is still improving. Intuitively we therefore expect that the momentum buffer will contain a good estimate of the direction of the next step that we would have taken had we continued training, and taking a step in this direction with our usual learning rate should only improve performance without any privacy loss. We use momentum with ρ = 0.9 in all other experiments and also take a ’free step’ at the end of private training. 0 10 20 30 40 50 60 Epochs 10 20 30 40 50 60 70T est Acc Batch Size Ablation.   Dataset=CIFAR100, Arch=beit, =0.1 10e3 Full Batch 1e3 5e3 (a) Batch Size 0 10 20 30 40 50 60 Epochs 10 20 30 40 50 60 70T est Accuracy Batch Size Ablation.   Dataset=CIFAR100, Arch=beit, =0.1 SGD GD (b) Gradient Descent vs SGD Figure 10.Ablation of batch size. Left: We vary the batch size using the learning rate and number of iterations tuned for full batch; all other batch sizes perform much worse. Right: We compare SGD and GD. For SGD we tune the batch size jointly with learning rate and number of iterations, arriving at a batch size of 4096 and plot the best performing run against full batch. 22A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization Full Batches Optimize Signal-to-Noise Ratio. Since its inception, the use of privacy amplification via Poisson subsam- pling and RDP has been a mainstay in the DP community (Zhu & Wang, 2019; Wang et al., 2019b; Erlingsson et al., 2018). Prior work almost universally uses privacy amplification via subsampling, but as early as (McMahan et al., 2017), and more recently in (De et al., 2022) it has become apparent that DP-SGD can actually benefit from large batch sizes because the signal-to-noise ratio (SNR) improves. Note that the noise term in 1 is divided by the batch size, so if we are willing to give up amplification via subsampling entirely, we can reduce the noise by a factor of5e4 for the benchmark computer vision tasks. In Fig. 10 we report the improvement of full-batch DP-GD over Poisson subsampled DP-SGD. We attribute the success of DP-GD to the improvement in SNR. For example, we achieve 91.52% accuracy on CIFAR10 for ε = 0.01 when training for 100 epochs with learning rate η = 0.01 and noise multiplier σ = 2561. When the noise is divided by the batch size, the effective noise multiplier is σ |B|=5e4 = 0.05 and the SNR is 1 0.051 = 20. When we use subsampling with sampling probability p = 0.2 and train for the same number of epochs under the same privacy budget, our effective noise multiplier is σ |B| = 1145 1e4 = 0.114, and the corresponding SNR of 1 0.114 = 8.7 is much worse than in the full batch setting. Although at first glance our analysis merely supports the typical conclusion that large batches are better in DP-SGD, (De et al., 2022) observe that DP-SGD is still preferrable to DP-GD because minibatching produces the optimal choice of noise multiplier. Our findings run counter to this: as discussed above, we contend that performance depends not only on the optimal noise multiplier but on our new linear scaling rule, and DP-GD unlocks the use of larger step sizes (Goyal et al., 2017). We use DP-GD instead of DP-SGD in all other experiments, removing the batch size from the hyperparameter tuning process and improving the overall privacy cost of deploying our baselines (Papernot & Steinke, 2021). B.3. A Critical Evaluation of Proposed Techniques for Fine-Tuning Prior work has proposed a number of ad-hoc techniques that improve performance in DP fine-tuning. Here we critically evaluate these techniques in the our method regime, and analyze why they reduce performance in our setting. Small Clipping Norms Bias Optimization. The standard deviation of the noise added in DP-SGD scales with the sensitivity of the update, defined by the clipping norm parameter. To decrease the amount of noise added, prior work has used very strict clipping (Mehta et al., 2023a; Bu et al., 2022a). Intuitively, if the clipping norm parameter is already chosen to be some value smaller than the norm of the unclipped gradient, the gradient estimator is no longer unbiased and this may have a negative impact on optimization. In Fig. 12 we observe that decreasing the clipping norm below 1 only degrades performance. As we can see in equation 1, further decreasing the clipping norm is equivalent to training with a smaller learning rate, and this is suboptimal because Fig. 17 indicates that we can prefer to use larger learning rates. We use a clipping norm of 1 in all other experiments. Initializing Weights to Zero Mitigates Variance in DP-GD. (Qiao et al., 2019) propose initializing the model parameters to very small values to improve the stability of micro-batch training, and (De et al., 2022) find that applying this technique to DP-SGD improves performance. In Fig. 11 we ablate the effectiveness of zero initialization with standard He initialization and find that the best performance comes from initializing the weights uniformly to zero. We initialize the classifier weights to zero in all other experiments. Weight Averaging Cannot Catch Up To Accelerated Fine-Tuning. (Shejwalkar et al., 2022) perform an in-depth empirical analysis and find that averaging the intermediate model checkpoints reduces the variance of DP-SGD and improves model performance. (De et al., 2022) first proposed the use of an Exponential Moving Average (EMA) to mitigate the noise introduced by DP-SGD. Previously, methods that use stochastic weight averaging (SWA) during SGD have been proposed and are even available by default in PyTorch (Izmailov et al., 2018). The idea of averaging weights to increase acceleration was first proposed by (Polyak & Juditsky, 1992), and is theoretically well-founded. In Fig. 13 we compare EMA and SW A with no averaging and find that no averaging performs the best. This is because weight averaging methods work well when optimization has converged and the model is plotting a trajectory that orbits around a local minima in the loss landscape (Izmailov et al., 2018). That is to say, the model’s distance from the initialization does not continually increase and at some point stabilizes so that the weight averaging method can ’catch up’. However, as discussed in Fig. 3 the optimal number of iterations for our method is to train for longer epochs without decaying the learning rate for convergence, because when the model converges the SNR decays. This is corroborated by Fig. 13, where we see that the distance from initialization is monotonically increasing. Our findings run counter to those of (Shejwalkar et al., 2022) for hyperparameters in line with our proposed linear scaling rule because we find that the best optimization regime for our method is precisely one where weight averaging can never catch up to the optimization trajectory. Therefore, the averaging methods only serve 23A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization 0 10 20 30 40 50 60 Epochs 0 10 20 30 40 50 60 70T est Accuracy Weight Initialization Ablation.   Dataset=CIFAR100, Arch=beit, =0.1 He Zero (a) Weight Initialization 30 35 40 45 50 55 60 Epochs 60 62 64 66 68 70 72T est Acc Weight Decay Ablation.   Dataset=CIFAR100, Arch=beit, =0.1 0.05 5e-05 0.005 0.0005 0 (b) Weight Decay Figure 11.Ablation of two previously proposed methods: zero initialization of parameters and weight decay. Zero initialization increases accuracy in all experiments, but weight decay only degrades performance. 50 52 54 56 58 60 Epochs 68.0 68.5 69.0 69.5 70.0 70.5 71.0 71.5 72.0 72.5T est Acc Clipping Ablation.   Dataset=CIFAR100, Arch=beit, =0.1 1.0 0.1 0.01 0.001 Figure 12.Because reducing the clipping norm is equivalent to reducing the learning rate, reducing the clipping norm below 1 only degrades performance on CIFAR100 for the beit architecture at ε = 0.1. 50 51 52 53 54 55 56 57 58 59 Epochs 65 66 67 68 69 70 71 72Accuracy Weight Averaging.  Dataset=CIFAR100, Arch=beit, =0.1 model_acc ema_acc swa_acc (a) Weight Averaging 0 2 4 6 8 Iteration 0 5 10 15 20Distance from Initialization Weight Trajectory   Dataset=CIFAR100, = 0.1 beit beitv2 (b) Weight Trajectory Figure 13.Left:Ablation of Weight Averaging. Right: Plot of distance from initialization. Weight Averaging does not improve performance because the model is monotonically moving away from the initialization and weight averaging cannot ’catch up’. 24A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization to lag one step behind no averaging. Data Augmentation Does Not Work When Freezing Embeddings. Data augmentation is used during training to bias the model towards selecting features that are invariant to the rotations we use in the augmentations. (Geirhos et al., 2018) find that feature extractors pretrained on ImageNet are naturally biased towards texture features. (De et al., 2022) eschew traditional data augmentation and instead propose the use of multiple dataset augmentations or ”batch augmentation”, first introduced by (Hoffer et al., 2019), to mitigate the variance of DP-SGD. In Fig. 14 we ablate the effectiveness of batch augmentation and find that it does not noticeably improve accuracy during transfer learning. This is because dataset augmentation changes the prior of the model when training the entire network (Shorten & Khoshgoftaar, 2019), but when we freeze all layers but the classifier, the model does not have the capacity to change to optimize for the prior introduced by data augmentation, because the embedding layer is frozen. 0 2 4 6 8 10 12 Epochs 10 20 30 40 50 60T est Acc Data Augmultation Ablation.   Dataset=CIFAR100, Arch=beitv2, =0.1 No augmult Augmult=16 (a) CIFAR100 (hard) 0 2 4 6 8 10 12 14 Epochs 40 50 60 70 80 90 100T est Acc Data Augmultation Ablation.   Dataset=CIFAR10, Arch=beitv2, =0.1 No augmult Augmult=16 (b) CIFAR10 (easy) Figure 14.Ablation of Data Augmultation on two datasets. On both datasets, Data Augmultation lags behind the baseline because there is much more training data, and even at the end, Data Augmultation does not have a noticeable improvement. Weight Decay Is Not Needed When Freezing Embeddings. Regularization methods such as weight decay are commonly used during pretraining to prevent overfitting, and the feature extractors we use are pretrained with AdamW (Dosovitskiy et al., 2020). One of the benefits of weight decay during fine-tuning is limiting the change of the embedding layer to not overfit and thus retain the features learned during pretraining (Kumar et al., 2022b). In the ongoing debate on whether to use weight decay during fine-tuning (Touvron et al., 2021), we submit that weight decay should not be used in private fine-tuning. In Fig. 11 we ablate a range of values of the weight decay parameter and observe that increasing the weight decay beyond a negligible amount (the gradient norm is ≈ 1e − 2) only decreases accuracy, and no value of the weight decay increases accuracy. There are two reasons for this. The first is that we initialize the weights of the model to zero, so we do not expect the gradients to be large. The second is that we only train the last layer, and therefore there is no need to regularize the training of the embedding layer. This supports the conclusion of (Kumar et al., 2022c) that SGD with momentum is outperforms AdamW as long as the embedding layer is not updated. B.4. Hyperparameter Ablations We provide full heatmaps and pareto frontiers for all datasets and the 3 best performing models (we do not perform a full evaluation on the ViT in order to minimize any knowledge leak for the evaluation of the linear scaling rule with the strategy in (Mehta et al., 2023a)). We note that while all of these datasets are arguably in-distribution, our focus is on comparing the regime of optimization preferred by our method to those of other works, and this is achieved by producing results on benchmark tasks. We further note that STL10 is explicitly in-distribution for the pretraining dataset (ImageNet); we only use this dataset as a temporary stand-in for evaluation on ImageNet-1k, a common benchmark in prior work (Mehta et al., 2023a) to minimize the computational burden. Hyperparameter Tuning and Selecting Epsilon. Prior work often uses unrealistic values of ε that provide no real privacy guarantee. While some prior work makes the case that hyperparameters need to be tuned even for non-private learning 25A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization Table 16.We compare the best private and best non-private test accuracy performances of our method to prior work using models pretrained on ImageNet-21k and fine-tuned on CIFAR10 and CIFAR100. Full results are in Section 5. Model Dataset ε = 0.1 ε = 1 ε = ∞ Gap (1 − ∞) our method CIFAR10 98.65 99.00 99.00 0.00 CIFAR100 81.9 89.81 91.57 1 .76 (Mehta et al., 2023a) CIFAR10 95.8 96 .3 96 .6 0 .3 CIFAR100 78.5 82 .7 85 .29 2 .59 (Bu et al., 2022a) CIFAR10 - 96.7 97 .4 0 .7 CIFAR100 - 83.0 88 .4 5 .4 (Cattan et al., 2022) CIFAR10 - 95.0 96 .4 1 .4 CIFAR100 - 73.7 82 .1 8 .4 (De et al., 2022) CIFAR10 - 94.8 96 .6 1 .8 CIFAR100 - 67.4 81 .8 14 .4 and can be chosen beforehand, we show that this is not the case. Not only are the optimal choices of key hyperparameters different between training from scratch and transfer learning (Li et al., 2020), they are also different for non-private and private transfer learning (Li et al., 2022b; De et al., 2022). We now provide guidelines for selectingε and broad intuition behind our choice to design a system that minimizes dependence on hyperparameters. For a decade the standard values of ε proposed for privacy preserving statistics queries have fallen in the range of 0.1 in line with eε ≈ 1 + ε for ε ≪ 1 (Dwork et al., 2006), and recently surveyed DP deployments generally abide by the rule of selecting ε ≈ 0.1 (Dwork et al., 2019). We know that while all small values of ε generally behave the same, every large value of ε is fundamentally different in a unique way (Dwork et al., 2019). In line with these guidelines, we only evaluate ε ∈ [0.01, 1.0] and perform most of our ablations on the most challenging task where we can see a range of performance: CIFAR100 for ε = 0.1. B.5. Theory Proposition B.1. The model training subroutine in 2 is ( √ T /σ)-GDP . Corollary B.2. Algorithm 2 is (ϵ, Φ(−ϵ·σ/ √ T + √ T /2σ))−eϵ ·Φ(−ϵ·σ/ √ T − √ T /2σ))-DP . Also, forn-fold repetition, the algorithm is (ϵ, Φ(−ϵ · σ/ √ n · T + √ n · T /2σ)) − eϵ · Φ(−ϵ · σ/ √ n · T − √ n · T /2σ))-DP Proof of Proposition 4.1: Proof. Since we are using the full batch, each iteration of the algorithm is an instantiation of the Gaussian mechanism with sensitivity of 1 and Gaussian noise with standard deviation of σ. Hence, each iteration of the mechanism is (1/σ)-GDP by Theorem 3.7 in (Dong et al., 2019). Then, since we have the adaptive composition of T of these mechanisms, the algorithm is ( √ T /σ)-GDP overall, using the composition theorem for GDP, as stated in Corollary 3.3 in (Dong et al., 2019). Proof of Corollary B.2: Proof. This directly follows from the GDP to DP conversion as stated in Corollary 2.13 in (Dong et al., 2019).Why does our HPO have low privacy cost? Our HPO has low privacy cost because of the nature of composition under GDP. Consider one sweep of our method with n = 3 that evaluates some (T1, η1, σ1), (T2, η2, σ2)) and we extrapolate (Tf , ηf , σf ), that works out to ε1 = 0.1, ε2 = 0.2, εf = 0.88. The composition for this according to (Dong et al., 2022) isµf = q nµ2 1 + nµ2 2 + µ2 f for µ1 = p T1/σ2 1. If we convert µf to εf , δ= 1e −5-DP, we arrive at a final guarantee of(1, 1e −5)-DP. The cost of HPO here in terms of the privacy utility tradeoff is actually just the marginal utility between εf = 0.88 and εt = 1.0. As we will show in Section 5, in many cases this marginal utility is negligible, and the value of cheap one-time measures that improve the performance of the rest of training such as HPO is very much worth it due to the nature of composition under GDP. Proof of Thm. 3.1 The main idea of the proof is similar to the main result in Fang et al. (2023a) but is simpler because we only prove the result for linear models. 26A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization Proof. We first apply (Ryu & Boyd, 2015) to see that gradient descent with step size 2 β > η > 2 α+β on a α-strongly convex, β-smooth function is a max(1 − ηβ, 1 − ηα)-contraction. Call this latter quantity c. Now consider a sequence of benign updates from gradient descent wt b and a sequence of noisy updates for the same dataset wt. Given the contractive property of GD , we have the following: \f\f\f(wt b − η∇f(wt b)) − (wt − η∇f(w(t))) \f\f\f ≤ c \f\f\fwt b − wt−1 b \f\f\f (1) We apply the update rule in 1 and use Eq.1 w(t+1) = w(t) − η(∇f(w(t)) + σξ) (2)\f\f\fwt+1 b − wt+1 \f\f\f = (3) = \f\f\fwt b − η∇f(wt b) − w(t) + η∇f(w(t)) − σξ \f\f\f (4) ≤ c \f\f\fwt b − w(t) \f\f\f + ηρ (5) Now we have the following \f\f\fwt − wt b \f\f\f ≤ c \f\f\fwt−1 − wt−1 b \f\f\f + ρη (6) We now proceed via induction. Assume for T − 1 the statement of Thm. 3.1 holds. By Eq.6 and the induction hypothesis we have \f\f\fwT−1 − wT−1 b \f\f\f ≤ ρη × ( T−2X i ci) (7) \f\f\fwT − wT b \f\f\f ≤ c(ρη × ( T−2X i ci)) + ρη (8) \f\f\fwT − wT b \f\f\f ≤ ρη × ( T−1X i ci). (9) ρη × ( T−1X i ci) = ρη(1 − cT ) 1 − c ρη 1 − cT 1 − c = ρη(1 − cT ) η · min(α, β) = ρ(1 − cT ) min(α, β) The intuition is clear: at iteration 0 there is no divergence. At iteration 1 there is ηρ divergence. At iteration 2 the previous divergence contracts by c and increases by ηρ, so the divergence is c1ηρ + ηρ. At iteration 3 the divergence is c2ηρ + c1ηρ + ηρ = ηρ(c2 + c + 1). It remains to show that the conditions for convexity and smoothness are satisfied for the problem at hand. For the case of, ex, training a single linear layer on top of extracted features with GD, this is easy to prove. We defer to the analysis from Panda et al. (2021), which we reproduce here for the reader’s convenience. Example B.3 (Computing the Lipschitz constant for single-layer SGD training ( Panda et al. (2021))). We compute the coordinatewise Lipschitz constant of the SGD protocol for a single layer neural network defined asσ(θx), where σ is the softmax function and θ ∈ Rd are the network parameters. For cross-entropy loss-based training using datasetD, we show that the constant c = 1 4 . Formally, sup D∈Z,θ1,θ2∈M |G(θ1, D)[i] − G(θ2, D)[i]|1 ≤ 1 4|θ1 − θ2|1 for any coordinate index i ∈ [d] 27A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization Without loss of generality, we assume that dataset D is comprised of samples of the form (x, y), where x ∈ [0, 1]m, and y ∈ {0, 1}C is the one-hot encoded representation of any of the C classes. For the single layer neural network, the model parameters are denoted by θ ∈ RC×m, and the softmax layer by the function σ(·). The neural network can thus be represented as Φ(x, θ) = σ(θx). We define g(θ, x) = ∂L(Φ(x,θ),y) ∂θ where L is the softmax cross entropy loss function. For the SGD protocol,A(u) = u, and G(θ, D) = g(θ, x). Our goal is to find a Lipschitz constant L such that, for all indices i ∈ [C] and j ∈ [m], sup x∈D,θ1,θ2 |g(θ1, x)ij − g(θ2, x)ij|1 |θ1 − θ2|1 ≤ L (10) We define intermediate variable z = θx and the neural network output distribution p = σ(z), such that both p, z∈ RC. Note, for a given target class t, the cross entropy loss function L(p, y) = −log pt where pt = eztP j ezj . Thus, g(θ, x)ij = ∂L ∂θij = CX c=1 ∂L ∂zc ∂zc ∂θij . (11) Computing the terms of (11), we have ∂L ∂zc = pt − 1 for c = t; and ∂L ∂zc = pc otherwise; and ∂zc ∂θij = xj. Thus, g(x, θ)ij = xj(pt − 1) for i = t = xjpi for i ̸= t (12) We compute the Hessian of g(x, θ)ij as: ∂g(x, θ)ij ∂θkl = xjpt(1 − pt)xl for k = t = xjpk(1 − pk)xl for k ̸= t (13) where k ∈ [C], l∈ [m]. The maximum value of the Hessian in (13), occurs at xj = xl = 1, and pt = pk = 1 2 . Thus, max i,j,k,l ∂g(x, θ)ij ∂θkl ≤ 1 4 for k = t ≤ 1 4 for k ̸= t (14) To obtain the Lipschitz constant, we first define the function h(t) = g((1 − t)θ1 + tθ2, x)ij where t ∈ [0, 1] Thus, h(0) = g(θ1, x)ij and h(1) = g(θ2, x)ij. Since, the function h(t) is differentiable everywhere in (0, 1), using Mean Value Theorem, we know that there exists a pointt∗ ∈ (0, 1) such that: h(1) − h(0) ≤ h′(t∗) where h′(t) = (θ2 − θ1)g′((1 − t)θ1 + tθ2, x)ijkl. (15) Rewriting (10), we get sup x∈D,θ1,θ2 |g(θ1, x) − g(θ2, x)|1 ≤ sup x∈D,θ1,θ2 |max i,j {g(θ1, x)ij − g(θ2, x)ij}|1 Let i∗, j∗ correspond to the indices where the maximum in the above equation occurs. Combining (14) and (15), we get: sup x∈D,θ1,θ2 |g(θ1, x)i∗j∗ − g(θ2, x)i∗j∗ |1 ≤ 1 4|θ1 − θ2|1 (16) Comparing (16) with (10) we get c = 1 4 . 28A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization C. Furthur Results for Language Modeling Tasks In general it is not feasible to do full-batch experiments for the NLP tasks because the memory requirements of LLMs are very large. We therefore do the composition with the PoissonSubsampledGaussianMechanism class in the PLD accountant (Gopi et al., 2021), ensuring that our method still accounts for the privacy cost of HPO. C.1. Related Works Li et al. (2022b) provide methods for fine-tuning large language models under DP-SGD by proposing new clipping methods to mitigate the memory burden of per-sample gradient clipping. However, they do not achieve performance comparable to non-private models when fine-tuning a pretrained model on the PersonaChat dataset. We adapt their techniques to the hyperparameter settings that we show are optimal for DP fine-tuning, and produce similar performance to non-private fine-tuning on the PersonaChat dataset. Yu et al. (2021) report compelling results by only updating a sparse subset of the LLMs with LoRA (Hu et al., 2021). We fine-tune GPT2 and RoBerta; Basu et al. (2022) also fine-tune BERT models. C.2. Experimental Set-up for Finetuning Language Models Persona-Chat: We write code based on winners of ConvAI2 competition1 and private-transformers library.2 We first do clipping norm [0.1, 0.2, 0.5, 1.0], learning rate in [2, 5, 10, 20, 50] × 10−5, batch size 64 and epochs [3, 10, 20] at ε = 3 and ε = 8 and find that the clipping norm in this range achieves almost same perplexity with other hyperparams fixed. We then do hyperparameter tuning as reported in Table 17 to finetune GPT-2. Table 17.Set of hyper-parameters used in the finetuning GPT-2. Parameter Values Clipping Norm 0.1 Learning Rate [2, 5, 10, 20, 50, 100] × 10−5 Batch Size [64, 128, 256, 512, 1024, 2048] Epochs [3, 10, 20] WikiText-2: We write code based on the HuggingFace transformers library GPT-2 example,3 source code by (Shi et al., 2022)4 and private-transformers library. The hyperparameter range for grid search is reported in Table 18. Table 18.Set of hyper-parameters for grid search to finetune GPT-2 on WikiText-2.δ = 10−6. Parameter Values Clipping Norm 1 Batch Size 2048 (Full Batch) Epochs 20 Learning Rate for ε = 0.2 [2 , 4, 6, 8, 10] × 10−4 Learning Rate for ε = 0.5 [0 .8, 1, 2] × 10−3 Enron Email: For Enron email dataset, we use the preprocessed dataset in (Gupta et al., 2022), where the non-private baseline of finetuned GPT-2 on this dataset is 7.09. The hyperparameter range for grid search is reported in Table 19. C.3. Additional Results on Persona-Chat We report the perplexity of GPT-2 on the Persona-Chat dataset at different epochs and batch size in Figure 15 (with tuned learning rate in Table 17) and we can see that larger batch size and longer epochs can achieve better perplexity, which is 1https://github.com/huggingface/transfer-learning-conv-ai. 2https://github.com/lxuechen/private-transformers. 3HuggingFace transformers GPT-2 example code. 4https://github.com/wyshi/sdp transformers 29A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization Table 19.Set of hyper-parameters for grid search to finetune GPT-2 on Enron Email dataset. δ = 1 2|Dtrain|. Parameter Values Clipping Norm 1 Batch Size 1024 Epochs 5 Learning Rate for ε = 0.1 [2 , 3, 4, 5, 6, 7, 8, 9, 10] × 10−4 Learning Rate for ε = 0.2 [0 .6, 0.8, 1, 2, 3, 4, 6, 7] × 10−3 Learning Rate for ε = 0.5 [0 .4, 0.6, 0.8, 0.9, 1, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.8, 2] × 10−2 Learning Rate for ε = 1.0 [1 , 2, 3, 4, 5, 6, 7, 8] × 10−2 Learning Rate for ε = 2.0 [2 , 3, 4, 5, 6, 7, 8, 9, 10] × 10−2 Learning Rate for ε = 3.0 [0 .6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.6, 1.8, 2.0] × 10−1 consistent with our linear scale rule. Besides, we also investigate fine-tuning multiple layers. With letting the embedding layer and last LayerNorm layer in transformer trainable, we consider fine-tuning only last block in transformer, first and last block in transformer and report the result in Table 20 and we can see that the best perplexity is achieved by fine-tuning the whole model. 64 128 256 512 1024 Batch Size 20 10 3 Epochs 19.56 18.98 18.39 17.94 17.91 20.34 19.69 19.07 18.68 18.33 22.53 21.68 21.34 20.28 20.25 Ppl of finetuned GPT2 on Persona-Chat ( =3) 18 19 20 21 22 (a) ε = 3 64 128 256 512 1024 Batch Size 20 10 3 Epochs 18.95 18.40 17.83 17.34 17.27 19.61 19.01 18.43 18.03 17.65 21.36 20.60 20.38 19.41 19.33 Ppl of finetuned GPT2 on Persona-Chat ( =8) 17.5 18.0 18.5 19.0 19.5 20.0 20.5 21.0 (b) ε = 8 Figure 15.Comparison of perplexity at different batch size and epochs of GPT-2 on Persona-Chat dataset. Table 20.Finetuning GPT-2 on Persona-Chat dataset including full model and different layers of model. We also include non-private baseline. ε 3 8 Full 17.91 17 .27 Last Block 19.80 19 .20 First-Last-Block 18.93 18 .26 C.4. Addtional Results on WikiText-2 We run the grid-search experiment for ε ∈ {0.2, 0.5, 1, 2, 3} to evaluate the performance gap between the optimal total step size and the estimated total step size.5) and present the result in Figure 16. The linear rule scales well from ε ∈ {0.2, 0.5} to ε = 1. Though for ε ∈ {2, 3} the perplexity of total step size by linear scale rule is slightly higher than the optimal perplexity of total step size by grid search, the result by linear scale is better than previous SOTA (Shi et al., 2022), which is 28.84 at (ε = 3, δ= 10−6) by training 20 iterations. 5Due to the limit of computation resources, all experiments are done by training for 20 iterations. Further increasing the number of iterations will help improve the utility as shown by previous study (Li et al., 2022b; Shi et al., 2022), we leave longer iterations for further study. 30A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization 0.5 1.0 1.5 2.0 2.5 3.0 27.5 28.0 28.5Perplexity Perplexity by linear scaling Perplexity by grid search (a) Pareto Frontier for ε vs Test Perplexity 0.5 1.0 1.5 2.0 2.5 3.0 0.00 0.05 0.10 0.15 0.20 0.25Total step size ( × T)  T otal step size by linear scaling T otal step size by grid search (b) Pareto Frontier for ε vs Total Step Size Figure 16.The linear scaling rule (accounting for the privacy cost of hyperparameter tuning) is competitive with grid search (non-private, doing N trials each with the given ε) in range [0.2, 1.0] on the WikiText-2 dataset. Left: y-axis is Perplexity (lower is better). 1 5 10 20 30 40 50 60 70 80 90 100 epochs 0.01 0.05 0.1 0.15 0.2 0.25 0.5 1.0 lr  = 0.01 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.05 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.10 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.20 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.30 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.40 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.50 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.60 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.70 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.80 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.90 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 1.00 (a) CIFAR10 Beitv2 1 5 10 20 30 40 50 60 70 80 90 100 epochs 0.01 0.05 0.1 0.15 0.2 0.25 0.5 1.0 lr  = 0.01 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.05 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.10 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.20 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.30 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.40 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.50 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.60 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.70 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.80 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.90 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 1.00 (b) CIFAR10 Beit 1 5 10 20 30 40 50 60 70 80 90 100 epochs 0.01 0.05 0.1 0.15 0.2 0.25 0.5 1.0 lr  = 0.01 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.05 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.10 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.20 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.30 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.40 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.50 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.60 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.70 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.80 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.90 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 1.00 (c) CIFAR10 Convnext 1 5 10 20 30 40 50 60 70 80 90 100 epochs 0.01 0.05 0.1 0.15 0.2 0.25 0.5 1.0 lr  = 0.01 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.05 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.10 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.20 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.30 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.40 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.50 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.60 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.70 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.80 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.90 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 1.00 (d) CIFAR100 Beitv2 1 5 10 20 30 40 50 60 70 80 90 100 epochs 0.01 0.05 0.1 0.15 0.2 0.25 0.5 1.0 lr  = 0.01 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.05 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.10 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.20 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.30 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.40 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.50 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.60 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.70 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.80 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.90 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 1.00 (e) CIFAR100 Beit 1 5 10 20 30 40 50 60 70 80 90 100 epochs 0.01 0.05 0.1 0.15 0.2 0.25 0.5 1.0 lr  = 0.01 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.05 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.10 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.20 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.30 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.40 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.50 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.60 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.70 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.80 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.90 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 1.00 (f) CIFAR100 Convnext Figure 17.Heatmaps for the reported datasets and architectures; lighter is better. Note that the scale of the axes differs from the heatmaps in the main body; this will be fixed in a future update. ε increases left to right with a different value for each heatmap according to: [0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0], epochs increase from left to right on the x-axis of each heatmap according to: [1, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100], and the learning increases from top to bottom on the y-axis of each heatmap according to: [0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.5, 1.0]. As ε increases, left to right, the optimal hyperparameters trend towards longer training with lower learning rates (bottom right). 31A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization 1 5 10 20 30 40 50 60 70 80 90 100 epochs 0.01 0.05 0.1 0.15 0.2 0.25 0.5 1.0 lr  = 0.01 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.05 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.10 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.20 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.30 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.40 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.50 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.60 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.70 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.80 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.90 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 1.00 (a) STL10 Beitv2 1 5 10 20 30 40 50 60 70 80 90 100 epochs 0.01 0.05 0.1 0.15 0.2 0.25 0.5 1.0 lr  = 0.01 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.05 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.10 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.20 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.30 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.40 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.50 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.60 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.70 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.80 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.90 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 1.00 (b) STL10 Beit 1 5 10 20 30 40 50 60 70 80 90 100 epochs 0.01 0.05 0.1 0.15 0.2 0.25 0.5 1.0 lr  = 0.01 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.05 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.10 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.20 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.30 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.40 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.50 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.60 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.70 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.80 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.90 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 1.00 (c) STL10 Convnext 1 5 10 20 30 40 50 60 70 80 90 100 epochs 0.01 0.05 0.1 0.15 0.2 0.25 0.5 1.0 lr  = 0.01 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.05 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.10 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.20 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.30 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.40 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.50 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.60 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.70 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.80 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.90 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 1.00 (d) FashionMNIST Beitv2 1 5 10 20 30 40 50 60 70 80 90 100 epochs 0.01 0.05 0.1 0.15 0.2 0.25 0.5 1.0 lr  = 0.01 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.05 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.10 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.20 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.30 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.40 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.50 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.60 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.70 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.80 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.90 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 1.00 (e) FashionMNIST Beit 1 5 10 20 30 40 50 60 70 80 90 100 epochs 0.01 0.05 0.1 0.15 0.2 0.25 0.5 1.0 lr  = 0.01 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.05 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.10 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.20 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.30 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.40 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.50 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.60 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.70 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.80 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 0.90 1 5 10 20 30 40 50 60 70 80 90 100 epochs  = 1.00 (f) FashionMNIST Convnext Figure 18.Heatmaps for the reported datasets and architectures; lighter is better. Note that the scale of the axes differs from the heatmaps in the main body; this will be fixed in a future update. ε increases left to right with a different value for each heatmap according to: [0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0], epochs increase from left to right on the x-axis of each heatmap according to: [1, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100], and the learning increases from top to bottom on the y-axis of each heatmap according to: [0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.5, 1.0]. As ε increases, left to right, the optimal hyperparameters trend towards longer training with lower learning rates (bottom right). 32A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization 0.0 0.2 0.4 0.6 0.8 1.0 0 20 40 60 80T est Accuracy  vs Test Accuracy on Dataset=CIFAR100 beit beitv2 (a) CIFAR100 Test Accuracy 0.0 0.2 0.4 0.6 0.8 1.0 0 10 20 30 40 50 60 70 80Total Step Size (  × T)  vs Total Step Size on Dataset=CIFAR100 beit beitv2 (b) CIFAR100 Total Step Size 0.0 0.2 0.4 0.6 0.8 1.0 86 88 90 92 94 96 98T est Accuracy  vs Test Accuracy on Dataset=CIFAR10 beit convnext beitv2 (c) CIFAR10 Test Accuracy 0.0 0.2 0.4 0.6 0.8 1.0 0 20 40 60 80Total Step Size (  × T)  vs Total Step Size on Dataset=CIFAR10 beit convnext beitv2 (d) CIFAR10 Total Step Size Figure 19.Pareto frontier for ε vs test accuracy and total step size for CIFAR10, and CIFAR100. Beitv2 excels for larger values ofε but beit and convnext are better for smaller values of ε. The inflection point varies across datasets. 33A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization 0.0 0.2 0.4 0.6 0.8 1.0 80 82 84 86 88 90T est Accuracy  vs Test Accuracy on Dataset=FashionMNIST beit convnext beitv2 (a) FashionMNIST Test Accuracy 0.0 0.2 0.4 0.6 0.8 1.0 0 20 40 60 80Total Step Size (  × T)  vs Total Step Size on Dataset=FashionMNIST beit convnext beitv2 (b) FashionMNIST Total Step Size 0.0 0.2 0.4 0.6 0.8 1.0 20 30 40 50 60 70 80 90 100T est Accuracy  vs Test Accuracy on Dataset=STL10 beit convnext beitv2 (c) STL10 Test Accuracy 0.0 0.2 0.4 0.6 0.8 1.0 0 2 4 6 8Total Step Size (  × T)  vs Total Step Size on Dataset=STL10 beit convnext beitv2 (d) STL10 Total Step Size Figure 20.Pareto frontier for ε vs test accuracy and total step size for STL10 and FashionMNIST. Beitv2 excels for larger values of ε but beit and convnext are better for smaller values of ε. The inflection point varies across datasets. 34",
      "meta_data": {
        "arxiv_id": "2212.04486v3",
        "authors": [
          "Ashwinee Panda",
          "Xinyu Tang",
          "Saeed Mahloujifar",
          "Vikash Sehwag",
          "Prateek Mittal"
        ],
        "published_date": "2022-12-08T18:56:37Z",
        "pdf_url": "https://arxiv.org/pdf/2212.04486v3.pdf"
      }
    },
    {
      "title": "Bayesian Optimization for Iterative Learning",
      "abstract": "The performance of deep (reinforcement) learning systems crucially depends on\nthe choice of hyperparameters. Their tuning is notoriously expensive, typically\nrequiring an iterative training process to run for numerous steps to\nconvergence. Traditional tuning algorithms only consider the final performance\nof hyperparameters acquired after many expensive iterations and ignore\nintermediate information from earlier training steps. In this paper, we present\na Bayesian optimization (BO) approach which exploits the iterative structure of\nlearning algorithms for efficient hyperparameter tuning. We propose to learn an\nevaluation function compressing learning progress at any stage of the training\nprocess into a single numeric score according to both training success and\nstability. Our BO framework is then balancing the benefit of assessing a\nhyperparameter setting over additional training steps against their computation\ncost. We further increase model efficiency by selectively including scores from\ndifferent training steps for any evaluated hyperparameter set. We demonstrate\nthe efficiency of our algorithm by tuning hyperparameters for the training of\ndeep reinforcement learning agents and convolutional neural networks. Our\nalgorithm outperforms all existing baselines in identifying optimal\nhyperparameters in minimal time.",
      "full_text": "Bayesian Optimization for Iterative Learning Vu Nguyen ∗ University of Oxford vu@robots.ox.ac.uk Sebastian Schulze ∗ University of Oxford sebastian.schulze@eng.ox.ac.uk Michael A. Osborne University of Oxford mosb@robots.ox.ac.uk Abstract The performance of deep (reinforcement) learning systems crucially depends on the choice of hyperparameters. Their tuning is notoriously expensive, typically requiring an iterative training process to run for numerous steps to convergence. Traditional tuning algorithms only consider the ﬁnal performance of hyperparam- eters acquired after many expensive iterations and ignore intermediate information from earlier training steps. In this paper, we present a Bayesian optimization (BO) approach which exploits the iterative structure of learning algorithms for efﬁcient hyperparameter tuning. We propose to learn an evaluation function compress- ing learning progress at any stage of the training process into a single numeric score according to both training success and stability. Our BO framework is then balancing the beneﬁt of assessing a hyperparameter setting over additional train- ing steps against their computation cost. We further increase model efﬁciency by selectively including scores from different training steps for any evaluated hyper- parameter set. We demonstrate the efﬁciency of our algorithm by tuning hyperpa- rameters for the training of deep reinforcement learning agents and convolutional neural networks. Our algorithm outperforms all existing baselines in identifying optimal hyperparameters in minimal time. 1 Introduction Deep learning (DL) and deep reinforcement learning (DRL) have led to impressive breakthroughs in a broad range of applications such as game play [26, 36], motor control [43], and image recognition [20]. To maintain general applicability, these algorithms expose sets of hyperparameters to adapt their behavior to any particular task at hand. This ﬂexibility comes at the price of having to tune an additional set of parameters – poor settings lead to drastic performance losses [11, 30, 37]. On top of being notoriously sensitive to these choices, deep (reinforcement) learning systems often have high training costs, in computational resources and time. For example, a single training run on the Atari Breakout game took approximately 75 hours on a GPU cluster [26]. Tuning DRL parameters is further complicated as only noisy evaluations of an agent’s ﬁnal performance are obtainable. Bayesian optimization (BO) [12, 28, 35] has recently achieved considerable success in optimizing these hyperparameters. This approach casts the tuning process as a global optimization problem based on noisy evaluations of a black-box function f . BO constructs a surrogate model typically using a Gaussian process (GP) [31], over this unknown function. This GP surrogate is used to build an acquisition function [13, 44] which suggests the next hyperparameter to evaluate. In modern machine learning (ML) algorithms [15], the training process is often conducted in an iterative manner. A natural example is given by deep learning where training is usually based on stochastic gradient descent and other iterative procedures. Similarly, the training of reinforcement learning agents is mostly carried out using multiple episodes. The knowledge accumulated during these training iterations can be useful to inform BO. However, most existing BO approaches [35] ∗These authors contributed equally. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada. arXiv:1909.09593v5  [cs.LG]  16 Jan 2021deﬁne the objective function as the average performance over the ﬁnal training iterations. In doing so, they ignore the information contained in the preceding training steps. In this paper, we present a Bayesian optimization approach for tuning algorithms where iterative learning is available – the cases of deep learning and deep reinforcement learning. First, we consider the joint space of input hyperparameters and number of training iterations to capture the learning progress at different time steps in the training process. We then propose to transform the whole training curve into a numeric score according to user preference. To learn across the joint space efﬁciently, we introduce a data augmentation technique leveraging intermediate information from the iterative process. By exploiting the iterative structure of training procedures, we encourage our algorithm to consider running a larger number of cheap (but high-utility) experiments, when cost- ignorant algorithms would only be able to run a few expensive ones. We demonstrate the efﬁciency of our algorithm on training DRL agents on several well-known benchmarks as well as the training of convolutional neural networks. In particular, our algorithm outperforms existing baselines in ﬁnding the best hyperparameter in terms of wall-clock time. Our main contributions are: • an algorithm to optimize the learning curve of a ML algorithm by using training curve compression, instead of averaged ﬁnal performance; • an approach to learn the compression curve from the data and a data augmentation tech- nique for increased sample-efﬁciency; • demonstration on tuning DRL and convolutional neural networks. 2 Related Work in Iteration-Efﬁcient Bayesian Optimization The ﬁrst algorithm category employs stopping criteria to terminate some training runs early and allo- cate resources towards more promising settings. These criteria typically involve projecting towards a ﬁnal score from early training stages. Freeze-thaw BO [42] models the training loss over time us- ing a GP regressor under the assumption that the training loss roughly follows an exponential decay. Based on this projection, training resources are allocated to the most promising settings. Hyperband [8, 23] dynamically allocates computational resources (e.g. training epochs or dataset size) through random sampling and eliminates under-performing hyperparameter settings by successive halving. Attempts have also been made to improve the epoch efﬁciency of other hyperparameter optimization algorithms in [5, 7, 18] which predict the ﬁnal learning outcome based on partially trained learning curves to identify hyperparameter settings that are expected to under-perform and early-stop them. In the context of DRL, however, these stopping criteria, including the exponential decay assumed in Freeze-thaw BO [42], may not be applicable, due to the unpredictable ﬂuctuations of DRL reward curves. In the supplement, we illustrate the noisiness of DRL training. The second category [16, 17, 23, 41, 48] aims to reduce the resource consumption of BO by utilizing low-ﬁdelity functions which can be obtained by using a subset of the training data or by training the ML model for a small number of iterations. Multi-task BO [41] requires the user to deﬁne a division of the dataset into pre-deﬁned and discrete subtasks. Multi-ﬁdelity BO with continuous approximation (BOCA) [16] and hierarchical partition [34] extend this idea to continuous settings. Speciﬁcally, BOCA ﬁrst selects the hyperparameter input and then the corresponding ﬁdelity to be evaluated at. The ﬁdelity in this context refers to the use of different number of learning iterations. Analogous to BOCA’s consideration of continuous ﬁdelities, Fabolas [17] proposes to model the combined space of input hyperparameter and dataset size and then select the optimal input and dataset size jointly. The above approaches typically identify performance of hyperparameters via the average (either training or validation) loss of the last learning iterations. Thereby, they do not account for potential noise in the learning process (e.g., they might select unstable settings that jumped to high perfor- mance in the last couple of iterations). 3 Bayesian Optimization for Iterative Learning (BOIL) Problem setting. We consider training a machine learning algorithm given a d-dimensional hy- perparameter x ∈X ⊂Rd for t iterations. This process has a training time costc(x,t) and produces 20 100 200 300 400 500 #Episode t 0.80 0.85 0.90 0.95 1.00x Tmin Tmax Augmented Obs Observation 0 100 200 300 400 500 0 50 100 150 200Score 4 18 34 8 45 5 14 26Reward Curve Sigmoid Func 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l( . |m * 0 ,g* 0 ) for Cifar10 m* 0 =-4.0 g* 0 =1.476 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l( . |m * 0 ,g* 0 ) for Reacher m* 0 =2.779 g* 0 =1.973 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l( . |m * 0 ,g* 0 ) for CartPole m* 0 =-3.266 g* 0 =3.0 Figure 1: Left: the score in pink box is a convolution of the reward curve r(·| x = 0.9,t = 500) and a Sigmoid function l(u |g0,m0) = 1 1+exp(−g0[u−m0]) up to time step t. Bottom: observations are selected to augment the dataset (red dots). The heatmap indicates the GP predictive mean µ for f across the number of episodest used to train an agent. Tmin and Tmax are two user-deﬁned thresholds for the number of training episodes. x is a hyperparameter to be tuned. Right: we learn the optimal parameter g∗ 0 and m∗ 0 for each experiment separately. training evaluations r(·| x,t) for t iterations, t ∈[Tmin,Tmax]. These could be episode rewards in DRL or training accuracies in DL. An important property of iterative training is that we know the whole curve at preceding steps r(t′|x,t), ∀t′≤t. Given the raw training curve r(·| x,t), we assume an underlying smoothed black-box function f , deﬁned in Sec. 3.2. Formally, we aim to ﬁnd x∗= argmaxx∈X f (x,Tmax); at the same time, we want to keep the overall training time, ∑N i=1 c(xi,ti), of evaluated settings [xi,ti] as low as possible. We summarize our variables in Table 1 in the supplement for ease of reading. 3.1 Selecting a next point using iteration-efﬁcient modeling We follow popular designs in [17, 19, 39, 41] and model the cost-sensitive black-box function as f (x,t) ∼GP(0,k([x,t],[x′,t′])), where k is an appropriate covariance functions and [x,t] ∈Rd+1. For simplicity and robustness, the cost function c(x,t) is approximated by a linear regressor. De- pending on the setting, it may be more appropriate to employ a second GP or different parametric model if the cost has a more complex dependence on hyperparameters x and iterations t. We regu- larly (re-)optimize both kernel and cost function parameters in between point acquisitions. More speciﬁcally, we choose the covariance function as a productk ([x,t],[x′,t′]) =k(x,x′)×k(t,t′) to induce joint similarities over parameter and iteration space. We estimate the predictive mean and uncertainty for a GP [31] at any input z∗= [x∗,t∗] as µ (z∗) =k∗ [ K +σ2 y I ]−1 y (1) σ2 (z∗) =k∗∗−k∗ [ K +σ2 y I ]−1 kT ∗ (2) where y = [yi]∀i, k∗= [k (z∗,zi)]∀i, K = [k (zi,zj)]∀i, j, k∗∗= k (z∗,z∗), and σ2 y is the noise variance of f . Cost predictions at any particular parameter x and time t are given by µc([x∗,t∗]) =βT [x,t], where β is directly computed from data {Z = [xi,ti],c = [ci]}∀i as β = (ZT Z)−1Zc [1]. Our goal is to select a point with high function value (exploitation), high uncertainty (exploration) and low cost (cheap). At each iteration n, we query the input parameter xn and the number of iteration tn [38, 48]: zn = [xn,tn] = argmax x∈X ,t∈[Tmin,Tmax] α(x,t)/µc(x,t). (3) 3Although our framework is available for any acquisition choices [13, 22, 47], to cope with output noise, we follow [45] and slight modify the expected improvement criterion using the maximum mean GP prediction µmax n . Let λ = µn(z)−µmaxn σn(z) , we then have a closed-form for the new expected improvement (EI) as αEI n (z) =σn (z)φ (λ) + [µn (z)−µmax n ]Φ(λ) where φ is the standard normal p.d.f., Φ is the c.d.f, µn and σn are the GP predictive mean and variance deﬁned in Eq. (1) and Eq. (2), respectively. 3.2 Training curve compression and estimating the transformation function Existing BO approaches [4, 23] typically deﬁne the objective function as an average loss over the ﬁnal learning episodes. However, this does not take into consideration how stable performance is or the training stage at which it has been achieved. We argue that averaging learning losses is likely misleading due to the noise and ﬂuctuations of our observations (learning curves) – particularly during the early stages of training. We propose to compress the whole learning curve into a numeric score via a preference function representing the user’s desired training curve. In the following, we use the Sigmoid function (speciﬁcally the Logistic function) to compute the utility score as y = ˆy(r,m0,g0) =r(·|x,t)•l(·|m0,g0) = t ∑ u=1 r(u |x,t) 1 +exp(−g0 [u −m0]) (4) where •is a dot product, a Logistic function l(·| m0,g0) is parameterized by a growth parameter g0 deﬁning a slope and the middle point of the curve m0. The optimal parameters g0 and m0 are estimated directly from the data. We illustrate different shapes of l parameterized by g0 and m0 in the appendix. The Sigmoid preference has a number of desirable properties. As early weights are small, less credit is given to ﬂuctuations at the initial stages, making it less likely for our surrogate to be biased towards randomly well performing settings. However, as weights monotonically increase, hyperparameters with improving performance are preferred. As weights saturate over time, stable, high performing conﬁgurations are preferred over short “performance spikes” characteristic of un- stable training. Lastly, this utility score assigns higher values to the same performance if it is being maintained over more episodes. Learning the transformation function from data. Different compression curves l(), parameter- ized by different choices of g0 and m0 in Eq. (4), may lead to different utilities y and thus affect the performance. The optimal values of g∗ 0 and m∗ 0 are unknown in advance. Therefore, we propose to learn these values g∗ 0 and m∗ 0 directly from the data. Our intuition is that the ‘optimal’ compression curve l(m∗ 0,g∗ 0) will lead to a better ﬁt of the GP. This better GP surrogate model, thus, will result in better prediction as well as optimization performance. We parameterize the GP log marginal likelihood L [31] as the function of m0 and g0: L(m0,g0) =1 2 ˆyT ( K +σ2 y I )−1 ˆy −1 2 ln ⏐⏐K +σ2 y I ⏐⏐ +const (5) where σ2 y is the output noise variance, ˆy is the function of m0 and g0 deﬁned in Eq. (4). We optimize m0 and g0 (jointly with other GP hyperparameters) using multi-start gradient descent. We derive the derivative ∂L ∂m0 = ∂L ∂ ˆy ∂ ˆy ∂m0 and ∂L ∂g0 = ∂L ∂ ˆy ∂ ˆy ∂g0 which can be computed analytically as: ∂L ∂ ˆy = ( K +σ2 y IN )−1 ˆy; ∂ ˆy ∂m0 = −g0 ×exp(−g0 [u −m0]) [1 +exp(−g0 [u −m0])]2 ; ∂ ˆy ∂g0 = −m0 ×exp(−g0 [u −m0]) [1 +exp(−g0 [u −m0])]2 . The estimated compression curves are illustrated in Right Fig. 1 and in Sec. 4.1. 3.3 Augmenting the training data When evaluating a parameter x over t iterations, we obtain not only a ﬁnal score but also all reward sequences r(t′|x,t),∀t′= 1,..., t. The auxiliary information from the curve can be useful for BO. Therefore, we propose to augment the information from the curve into the sample set of our GP model. A naïve approach for augmentation is to add a full curve of points {[x, j],yj}t j=1 where yj is computed using Eq. (4). However, this approach can be redundant and may im- pose serious issues in the conditioning of the GP covariance matrix. As we cluster 40.80 0.85 0.90 0.95 1.00 200 300 400 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 x GP variance 2 0 2 4 6 8 10 12 0.000 0.025 0.050 0.075 0.100 0.125 0.150 0.175 0.80 0.85 0.90 0.95 1.00 200 300 400 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 GP variance 400 320 240 160 80 0 80 160 240 0.000 0.005 0.010 0.015 0.020 0.025 0.030 0.035 0.040 x Figure 2: GP with different settings. Left: our augmentation. Right: using a full curve. If we add too many observations, the GP covariance matrix becomes ill-conditioned. On the right, the GP ﬁt is poor with a large mean estimate range of [−400,240] even though the output is standardized N (0,1). All x-axis are over x, a hyperparameter to be tuned. more evaluations closely, the conditioning of the GP covariance degrades further, as dis- cussed in [24]. This conditioning issue is especially serious in our noisy DRL settings. 0 10 20 30 40 50 60 Iterations 0 5 10 15 20 25Log of Condition Number Condition Number of GP Covariance Augmentation No Augmentation Full Curve Reasonable Threshold Figure 3: The condition number of GP covari- ance matrix deteriorates if we add the whole curve of points into a GP. The large condition number indicates the nearness to singularity. We highlight this effect on GP estimation in Fig. 2 wherein the GP mean varies erratically when the natural log of the condition number of the GP co- variance matrix goes above 25 (see Fig. 3) as we include the whole curve. Selecting subset of points from the curve. Dif- ferent solutions, such as the addition of artiﬁcial noise or altering the kernel’s length-scales, have been proposed. We decide to use an active learn- ing approach [10, 29] as sampled data points are expected to contain a lot of redundant informa- tion. As a consequence, the loss of information from sub-sampling the data should be minimal and information-eroding modiﬁcation of the ker- nel matrix itself can be avoided. As a side beneﬁt, the reduced number of sampled points speeds up inference in our GP models. In particular, we select samples at the maximum of the GP predictive uncertainty. Formally, we sequentially select a set Z = [z1,...zM], zm = [x,tm], by varying tm while keeping x ﬁxed as zm =argmax ∀t′≤t σ([x,t′] |D′),∀m ≤M s.t. lnof cond(K) ≤δ (6) where D′= D∪{zj = [x,tj]}m−1 j=1 . This sub-optimisation problem is done in a one-dimensional space of t′∈{Tmin,..., t}, thus it is cheap to optimize using (multi-start) gradient descent (the derivative of GP predictive variance is available [31]). Alternatively, a ﬁxed-size grid could be considered, but this could cause conditioning issues when a point in the grid [ x,tgrid ] is placed near another existing point [ x′,tgrid ] , i.e., ||x −x′||2 ≤ε for some small ε. These generated points Z are used to calculate the output r(zm) and augmented into the observation set (X,Y ) for ﬁtting the GP. The number of samplesM is adaptively chosen such that the natural log of the condition number of the covariance matrix is less than a threshold. This is to ensure that the GP covariance matrix condition number behaves well by reducing the number of unnecessary points added to the GP at later stages. We compute the utility score ym given zm for each augmented point using Eq. (4). In addition, we can estimate the running time cm using the predictive mean µc(zm). We illustrate the augmented observations and estimated scores in Fig. 1. We summarize the overall algorithm in Alg. 1. To enforce non-negativity and numerical stability, we make use of the transformations α ←log[1 +exp(α)] and µc ←log[1 +exp(µc)]. 4 Experiments We assess our model by tuning hyperparameters for two DRL agents on three environments and a CNN on two datasets. We provide additional illustrations and experiments in the appendix. 5Algorithm 1 Bayesian Optimization with Iterative Learning (BOIL) Input: #iter N, initial data D0, z = [x,t]. Output: optimal x∗and y∗= max∀y∈DN y 1: for n = 1....N do 2: Fit a GP to estimate µf (),σf () from Eqs. (1,2) and a LR for cost µc() 3: Select zn = argmaxx,t α(x,t)/µc(x,t) and observe a curve r and a cost c from f (zn) 4: Compressing the learning curve r(zn) into numeric score using Eq. (4). 5: Sample augmented points zn,m,yn,m,cn,m,∀m ≤M given the curve and Dn in Eq. (6) 6: Augment the data into Dn and estimate Logistic curve hyperparameters m0 and g0. 7: end for Experimental setup. All experimental results are averaged over 20 independent runs with differ- ent random seeds. Final performance is estimated by evaluating the chosen hyperparameter over the maximum number of iterations. All experiments are executed on a NVIDIA 1080 GTX GPU using the tensorﬂow-gpu Python package. The DRL environments are available through the OpenAI gym [3] and Mujoco [43]. Our DRL implementations are based on the open source from Open AI Baselines [6]. We release our implementation at https://github.com/ntienvu/BOIL. We use square-exponential kernels for the GP in our model and estimate their parameters by maxi- mizing the marginal likelihood [31]. We set the maximum number of augmented points to beM = 15 and a threshold for a natural log of GP condition numberδ = 20. We note that the optimization over- head is much less than the black-box function evaluation time. Baselines. We compare with Hyperband [23] which demonstrated empirical successes in tuning deep learning applications in an iteration-efﬁcient manner. We extend the discrete multi-task BO [41] to the continuous case – which can also be seen as continuous multi-ﬁdelity BO [16, 39] as in our setting, they both consider cost-sensitivity and iteration-efﬁciency. We, therefore, label the two baselines as continuous multi-task/ﬁdelity BO (CM-T/F-BO). We have ignored the minor difference in these settings, such as multi-task approaches jointly optimizes the ﬁdelity and input while BOCA [16] ﬁrst selects the input and then the ﬁdelity. Our focus is to demonstrate the effectiveness of optimizing the learning curve using compression and augmentation techniques. We therefore omit the comparison of various acquisition functions and kernel choices which can easily be used in our model. We also do not compare with Fabolas [17] which is designed to vary dataset sizes, not iteration numbers. We would expect the performance of Fabolas to be close to CM-T/F-BO. We are unable to compare with FreezeThaw as the code is not available. However, the curves in our setting are not exponential decays and thus ill-suited to their model (see last ﬁgure in the appendix). We have considered an ablation study in the appendix using a time kernel following the exponential decay proposed in Freeze-thaw method [42]. Task descriptions. We consider three DRL settings including a Dueling DQN (DDQN) [46] agent in the CartPole-v0 environment and Advantage Actor Critic (A2C) [25] agents in the InvertedPendulum-v2 and Reacher-v2 environments. In addition to the DRL applications, we tune 6 hyperparameters for training a convolutional neural network [21] on the SVHN dataset and CI- FAR10. Due to space considerations, we refer to the appendix for further details. 4.1 Model illustration /uni00000013/uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni00000013 /uni00000018 /uni00000014/uni00000013 /uni00000014/uni00000018/uni00000006/uni00000024/uni00000058/uni0000004a/uni00000050/uni00000048/uni00000051/uni00000057/uni00000048/uni00000047/uni00000003/uni00000032/uni00000045/uni00000056 /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni00000057/uni00000048/uni00000047/uni00000003/uni00000024/uni00000058/uni0000004a/uni00000050/uni00000048/uni00000051/uni00000057/uni00000048/uni00000047/uni00000003/uni00000032/uni00000045/uni00000056 Figure 4: DDQN on CartPole. The number of augmented observations reduces over time. We ﬁrst illustrate the estimated compression func- tion l(m∗ 0,g∗ 0) in Right Fig. 1 from different experi- ments. These Logistic parameters g∗ 0 and m∗ 0 are es- timated by maximizing the GP marginal likelihood and used for compressing the curve. We show that the estimated curve from CartPole tends to reach the highest performance much earlier than Reacher because CartPole is somewhat easier to train than Reacher. We next examine the count of augmented observa- tions generated per iteration in Fig. 4. Although this number is ﬂuctuating, it tends to reduce over 6/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000018/uni00000013/uni00000013 /uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048 /uni0000001a/uni00000013 /uni00000019/uni00000013 /uni00000018/uni00000013 /uni00000017/uni00000013 /uni00000016/uni00000013 /uni00000015/uni00000013 /uni00000014/uni00000013 /uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni00000035/uni00000048/uni00000044/uni00000046/uni0000004b/uni00000048/uni00000055/uni00000040/uni00000003/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047/uni00000003/uni00000058/uni00000056/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000032/uni00000053/uni00000057/uni0000004c/uni00000050/uni0000004c/uni0000005d/uni00000048/uni00000047/uni00000003/uni00000033/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013/uni00000015/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013/uni0000001b/uni00000013/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000014/uni00000015/uni00000013/uni00000013/uni00000014/uni00000017/uni00000013/uni00000013 /uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048 /uni00000015/uni00000018 /uni00000018/uni00000013 /uni0000001a/uni00000018 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000015/uni00000018 /uni00000014/uni00000018/uni00000013 /uni00000014/uni0000001a/uni00000018 /uni00000015/uni00000013/uni00000013/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni0000002c/uni00000059/uni00000033/uni00000048/uni00000051/uni00000040/uni00000003/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047/uni00000003/uni00000058/uni00000056/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000032/uni00000053/uni00000057/uni0000004c/uni00000050/uni0000004c/uni0000005d/uni00000048/uni00000047/uni00000003/uni00000033/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000025/uni00000032/uni0000002c/uni0000002f Figure 5: The learning curves of the best found parameters by different approaches. The curves show that BO-L and BOIL reliably identify parameters leading to stable training. BOIL takes only half total time to ﬁnd this optimal curve. time. BOIL does not add more augmented observations at the later stage when we have gained sufﬁcient information and GP covariance conditioning falls below our threshold δ = 20. 4.2 Ablation study of curve compression To demonstrate the impact of our training curve compression, we compare BOIL to vanilla Bayesian optimization (BO) and with compression (BO-L) given the same number of iterations at Tmax. We show that using the curve compression leads to stable performance, as opposed to the existing tech- nique of averaging the last iterations. We plot the learning curves of the best hyperparameters identiﬁed by BO, BO-L and BOIL. Fig. 5 shows the learning progress over Tmax episodes for each of these. The curves are smoothed by averaging over 100 consecutive episodes for increased clarity. We ﬁrst note that all three algorithms eventually obtain similar performance at the end of learning. However, since BO-L and BOIL take into account the preceding learning steps, they achieve higher performance more quickly. Furthermore, they achieve this more reliably as evidenced by the smaller error bars (shaded regions). 4.3 Tuning deep reinforcement learning and CNN We now optimize hyperparameters for deep reinforcement learning algorithms; in fact, this applica- tion motivated the development of BOIL. The combinations of hyperparameters to be tuned, target DRL algorithm and environment can be found in the appendix. Comparisons by iterations and real-time. Fig. 6 illustrates the performance of different algo- rithms against the number of iterations as well as real-time (the plots for CIFAR10 are in the ap- pendix). The performance is the utility score of the best hyperparameters identiﬁed by the baselines. Across all three tasks, BOIL identiﬁes optimal hyperparameters using signiﬁcantly less computation time than other approaches. The plots show that other approaches such as BO and BO-L can identify well-performing hyperpa- rameters in fewer iterations than BOIL. However, they do so only considering costly, high-ﬁdelity evaluations resulting in signiﬁcantly higher evaluation times. In contrast to this behavior, BOIL ac- counts for the evaluation costs and chooses to initially evaluate low-ﬁdelity settings consuming less time. This allows fast assessments of a multitude of hyperparameters. The information gathered here is then used to inform later point acquisitions. Hereby, the inclusion of augmented observations is crucial in offering useful information readily available from the data. In addition, this augmenta- tion is essential to prevent from the GP kernel issue instead of adding the full curve of points into our GP model. Hyperband [23] exhibits similar behavior in that it uses low ﬁdelity (small t) evaluations to reduce a pool of randomly sampled conﬁgurations before evaluating at high ﬁdelity (large t). To deal with noisy evaluations and other effects, this process is repeated several times. This puts Hyperband at a disadvantage particularly in the noisy DRL tasks. Since early performance ﬂuctuates hugely, Hyperband can be misled in where to allocate evaluation effort. It is then incapable of revising 7/uni00000013 /uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni00000014/uni00000014/uni0000001a/uni00000011/uni00000018 /uni00000014/uni00000015/uni00000013/uni00000011/uni00000013 /uni00000014/uni00000015/uni00000015/uni00000011/uni00000018 /uni00000014/uni00000015/uni00000018/uni00000011/uni00000013 /uni00000014/uni00000015/uni0000001a/uni00000011/uni00000018 /uni00000014/uni00000016/uni00000013/uni00000011/uni00000013 /uni00000014/uni00000016/uni00000015/uni00000011/uni00000018 /uni00000014/uni00000016/uni00000018/uni00000011/uni00000013 /uni00000014/uni00000016/uni0000001a/uni00000011/uni00000018/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000027/uni00000027/uni00000034/uni00000031/uni00000010/uni00000026/uni00000044/uni00000055/uni00000057/uni00000033/uni00000052/uni0000004f/uni00000048/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000003/uni00000044/uni00000051/uni00000047/uni00000003 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013 /uni00000018/uni00000013/uni00000014/uni00000013/uni00000013/uni00000014/uni00000018/uni00000013/uni00000015/uni00000013/uni00000013/uni00000015/uni00000018/uni00000013/uni00000016/uni00000013/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni00000014/uni00000014/uni0000001a/uni00000011/uni00000018 /uni00000014/uni00000015/uni00000013/uni00000011/uni00000013 /uni00000014/uni00000015/uni00000015/uni00000011/uni00000018 /uni00000014/uni00000015/uni00000018/uni00000011/uni00000013 /uni00000014/uni00000015/uni0000001a/uni00000011/uni00000018 /uni00000014/uni00000016/uni00000013/uni00000011/uni00000013 /uni00000014/uni00000016/uni00000015/uni00000011/uni00000018 /uni00000014/uni00000016/uni00000018/uni00000011/uni00000013 /uni00000014/uni00000016/uni0000001a/uni00000011/uni00000018/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000027/uni00000027/uni00000034/uni00000031/uni00000010/uni00000026/uni00000044/uni00000055/uni00000057/uni00000033/uni00000052/uni0000004f/uni00000048/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000003/uni00000044/uni00000051/uni00000047/uni00000003 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013 /uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni00000014/uni00000015/uni0000001b /uni00000014/uni00000015/uni0000001c /uni00000014/uni00000016/uni00000013 /uni00000014/uni00000016/uni00000014 /uni00000014/uni00000016/uni00000015 /uni00000014/uni00000016/uni00000016 /uni00000014/uni00000016/uni00000017/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni00000035/uni00000048/uni00000044/uni00000046/uni0000004b/uni00000048/uni00000055/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013 /uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni00000014/uni00000015/uni0000001b /uni00000014/uni00000015/uni0000001c /uni00000014/uni00000016/uni00000013 /uni00000014/uni00000016/uni00000014 /uni00000014/uni00000016/uni00000015 /uni00000014/uni00000016/uni00000016 /uni00000014/uni00000016/uni00000017/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni00000035/uni00000048/uni00000044/uni00000046/uni0000004b/uni00000048/uni00000055/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013 /uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni00000015/uni00000018 /uni00000016/uni00000013 /uni00000016/uni00000018 /uni00000017/uni00000013 /uni00000017/uni00000018 /uni00000018/uni00000013 /uni00000018/uni00000018 /uni00000019/uni00000013 /uni00000019/uni00000018/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c /uni0000003e/uni00000026/uni00000031/uni00000031/uni00000010/uni00000036/uni00000039/uni0000002b/uni00000031/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000019/uni00000003/uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000053/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013/uni0000001a/uni00000013/uni0000001b/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni00000015/uni00000018 /uni00000016/uni00000013 /uni00000016/uni00000018 /uni00000017/uni00000013 /uni00000017/uni00000018 /uni00000018/uni00000013 /uni00000018/uni00000018 /uni00000019/uni00000013 /uni00000019/uni00000018/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c /uni0000003e/uni00000026/uni00000031/uni00000031/uni00000010/uni00000036/uni00000039/uni0000002b/uni00000031/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000019/uni00000003/uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000053/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f Figure 6: Comparison over BO evaluations (Left) and real-time (Right). Given the same time bud- get, CM-T/F-BO, Hyperband and BOIL can take more evaluations than vanilla BO, BO-L and Rand. BOIL outperforms other competitors in ﬁnding the optimal parameters in an iteration-efﬁcient man- ner. CM-T/F-BO does not augment the observations from the curve and requires more evaluations. The results of InvertedPendulum and CNN-CIFAR10 are in the appendix. these choices until an entirely new pool of hyperparameters is sampled and evaluated from scratch. In contrast to this, BOIL is more ﬂexible than Hyperband in that it can freely explore-exploit the whole joint space. The GP surrogate hereby allows BOIL to generalize across hyperparameters and propagate information through the joint space. 5 Conclusion and Future work Our framework complements the existing BO toolbox for hyperparameter tuning with iterative learn- ing. We present a way of leveraging our understanding that later stages of the training process are informed by progress made in earlier ones. This results in a more iteration-efﬁcient hyperparame- ter tuning algorithm that is applicable to a broad range of machine learning systems. We evaluate its performance on a set of diverse benchmarks. The results demonstrate that our model surpasses the performance of well-established alternatives while consuming signiﬁcantly fewer resources. Fi- nally, we note that our approach is not necessarily speciﬁc to machine learning algorithms, but more generally applies to any process exhibiting an iterative structure to be exploited. 86 Broader Impact Our work aims at making the optimization of processes operating in a step-wise fashion more efﬁ- cient. As demonstrated this makes BOIL particularly well-suited to supporting supervised learning models and RL systems. By increasing training efﬁcience of these models, we hope to contribute to their widespread deployment whilst reducing the computational and therefore environmental cost their implementation has. Deep (reinforcement) learning systems ﬁnd application in a wide range of settings that directly contribute to real world decisions, e.g., natural language processing, visual task, autonomous driving and many more. As machine learning models building on our contributions are being deployed in the real world, we encourage practicioners to put in place necessary supervision and override mechanisms as precautions against potential failure. In a more general context, our algorithm may be seen as a step towards the construction of an automated pipeline for the training and deployment of machine learning models. A potential danger is that humans become further and further removed from the modelling process, making it harder to spot (potentially critical) failures. We do not see this as an argument against the construction of such a pipeline in principle, but instead encourage practicioners to reﬂect on potential biases indirectly encoded in the choice of data sets and models, they are feeding into said automated processes. The growing opacity of machine learning models is a concern of its own and which automated training procedures will only contribute to. Opposing this is a rapidly growing corpus of work addressing the interpretability of trained machine learning models and their decision making. These can and should be used to rigorously analyse ﬁnal training outcomes. Only then can we ensure that machine learning algorithm do indeed become a beneﬁcial source of information guiding real world policy making as opposed to opaque, unquestioned entities. While our main interest lies in the hyperparameter optimization of machine learning models, it should be noted that any iterative process depending on a set of parameters can make use of our con- tributions. Possible settings could, for instance, include the optimization of manufacturing pipelines in which factory setting are adjusted to increase productivity. 7 Acknowledgements S. Schulze is supported by an I-CASE studentship funded by the EPSRC and Dyson. References [1] Christopher M Bishop. Pattern recognition and machine learning. springer New York, 2006. [2] Eric Brochu, Vlad M Cora, and Nando De Freitas. A tutorial on Bayesian optimization of ex- pensive cost functions, with application to active user modeling and hierarchical reinforcement learning. arXiv preprint arXiv:1012.2599, 2010. [3] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016. [4] Yutian Chen, Aja Huang, Ziyu Wang, Ioannis Antonoglou, Julian Schrittwieser, David Silver, and Nando de Freitas. Bayesian optimization in AlphaGo. arXiv preprint arXiv:1812.06855, 2018. [5] Zhongxiang Dai, Haibin Yu, Bryan Kian Hsiang Low, and Patrick Jaillet. Bayesian optimiza- tion meets Bayesian optimal stopping. In International Conference on Machine Learning , pages 1496–1506, 2019. [6] Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford, John Schulman, Szymon Sidor, Yuhuai Wu, and Peter Zhokhov. Openai baselines. GitHub, GitHub repository, 2017. [7] Tobias Domhan, Jost Tobias Springenberg, and Frank Hutter. Speeding up automatic hy- perparameter optimization of deep neural networks by extrapolation of learning curves. In Twenty-Fourth International Joint Conference on Artiﬁcial Intelligence, 2015. 9[8] Stefan Falkner, Aaron Klein, and Frank Hutter. Bohb: Robust and efﬁcient hyperparameter optimization at scale. In International Conference on Machine Learning , pages 1436–1445, 2018. [9] Peter I Frazier. A tutorial on Bayesian optimization. arXiv preprint arXiv:1807.02811, 2018. [10] Yarin Gal, Riashat Islam, and Zoubin Ghahramani. Deep Bayesian active learning with image data. In Proceedings of the 34th International Conference on Machine Learning, pages 1183– 1192, 2017. [11] Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. Deep reinforcement learning that matters. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence, 2018. [12] Philipp Hennig and Christian J Schuler. Entropy search for information-efﬁcient global opti- mization. Journal of Machine Learning Research, 13:1809–1837, 2012. [13] José Miguel Hernández-Lobato, Matthew W Hoffman, and Zoubin Ghahramani. Predictive entropy search for efﬁcient global optimization of black-box functions. In Advances in Neural Information Processing Systems, pages 918–926, 2014. [14] Donald R Jones, Matthias Schonlau, and William J Welch. Efﬁcient global optimization of expensive black-box functions. Journal of Global optimization, 13(4):455–492, 1998. [15] M. I. Jordan and T. M. Mitchell. Machine learning: Trends, perspectives, and prospects. Science, 349(6245):255–260, 2015. [16] Kirthevasan Kandasamy, Gautam Dasarathy, Jeff Schneider, and Barnabás Póczos. Multi- ﬁdelity Bayesian optimisation with continuous approximations. In Proceedings of the 34th International Conference on Machine Learning, pages 1799–1808, 2017. [17] Aaron Klein, Stefan Falkner, Simon Bartels, Philipp Hennig, and Frank Hutter. Fast Bayesian optimization of machine learning hyperparameters on large datasets. In Artiﬁcial Intelligence and Statistics, pages 528–536, 2017. [18] Aaron Klein, Stefan Falkner, Jost Tobias Springenberg, and Frank Hutter. Learning curve pre- diction with Bayesian neural networks. International Conference on Learning Representations (ICLR), 2017. [19] Andreas Krause and Cheng S Ong. Contextual Gaussian process bandit optimization. In Advances in Neural Information Processing Systems, pages 2447–2455, 2011. [20] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolutional neural networks. In Advances in Neural Information Processing Systems, pages 1097–1105, 2012. [21] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. [22] Benjamin Letham, Brian Karrer, Guilherme Ottoni, Eytan Bakshy, et al. Constrained Bayesian optimization with noisy experiments. Bayesian Analysis, 14(2):495–519, 2019. [23] Lisha Li and Kevin Jamieson. Hyperband: A novel bandit-based approach to hyperparameter optimization. Journal of Machine Learning Research, 18:1–52, 2018. [24] Mark McLeod, Stephen Roberts, and Michael A Osborne. Optimization, fast and slow: Op- timally switching between local and Bayesian optimization. In International Conference on Machine Learning, pages 3440–3449, 2018. [25] V olodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforce- ment learning. In International conference on machine learning, pages 1928–1937, 2016. [26] V olodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. NIPS Deep Learning Workshop, 2013. [27] Vu Nguyen, Sunil Gupta, Santu Rana, Cheng Li, and Svetha Venkatesh. Regret for expected improvement over the best-observed value and stopping condition. In Proceedings of The 9th Asian Conference on Machine Learning (ACML), pages 279–294, 2017. [28] Vu Nguyen and Michael A Osborne. Knowing the what but not the where in Bayesian opti- mization. In International Conference on Machine Learning, pages 7317–7326, 2020. 10[29] Michael Osborne, Roman Garnett, Zoubin Ghahramani, David K Duvenaud, Stephen J Roberts, and Carl E Rasmussen. Active learning of model evidence using Bayesian quadrature. In Advances in Neural Information Processing Systems, pages 46–54, 2012. [30] Jack Parker-Holder, Vu Nguyen, and Stephen Roberts. Provably efﬁcient online hyperparame- ter optimization with population-based bandits. In Advances in Neural Information Processing Systems, 2020. [31] Carl Edward Rasmussen. Gaussian processes for machine learning. 2006. [32] Binxin Ru, Mark McLeod, Diego Granziol, and Michael A Osborne. Fast information-theoretic Bayesian optimisation. In International Conference on Machine Learning, pages 4381–4389, 2018. [33] Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. International Conference on Learning Representations, 2016. [34] Rajat Sen, Kirthevasan Kandasamy, and Sanjay Shakkottai. Multi-ﬁdelity black-box opti- mization with hierarchical partitions. In International conference on machine learning, pages 4538–4547, 2018. [35] Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P Adams, and Nando de Freitas. Taking the human out of the loop: A review of Bayesian optimization. Proceedings of the IEEE , 104(1):148–175, 2016. [36] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanc- tot, et al. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484, 2016. [37] Leslie N Smith. A disciplined approach to neural network hyper-parameters: Part 1–learning rate, batch size, momentum, and weight decay. arXiv preprint arXiv:1803.09820, 2018. [38] Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical Bayesian optimization of ma- chine learning algorithms. In Advances in Neural Information Processing Systems , pages 2951–2959, 2012. [39] Jialin Song, Yuxin Chen, and Yisong Yue. A general framework for multi-ﬁdelity Bayesian optimization with Gaussian processes. In The 22nd International Conference on Artiﬁcial Intelligence and Statistics, pages 3158–3167, 2019. [40] Niranjan Srinivas, Andreas Krause, Sham Kakade, and Matthias Seeger. Gaussian process optimization in the bandit setting: No regret and experimental design. In Proceedings of the 27th International Conference on Machine Learning, pages 1015–1022, 2010. [41] Kevin Swersky, Jasper Snoek, and Ryan P Adams. Multi-task Bayesian optimization. In Advances in Neural Information Processing Systems, pages 2004–2012, 2013. [42] Kevin Swersky, Jasper Snoek, and Ryan Prescott Adams. Freeze-thaw Bayesian optimization. arXiv preprint arXiv:1406.3896, 2014. [43] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026–5033. IEEE, 2012. [44] Zi Wang and Stefanie Jegelka. Max-value entropy search for efﬁcient Bayesian optimization. In International Conference on Machine Learning, pages 3627–3635, 2017. [45] Ziyu Wang and Nando de Freitas. Theoretical analysis of Bayesian optimisation with unknown Gaussian process hyper-parameters. arXiv preprint arXiv:1406.7758, 2014. [46] Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and Nando Freitas. Dueling network architectures for deep reinforcement learning. In International Conference on Machine Learning, pages 1995–2003, 2016. [47] Jian Wu and Peter Frazier. The parallel knowledge gradient method for batch Bayesian opti- mization. In Advances In Neural Information Processing Systems, pages 3126–3134, 2016. [48] Jian Wu, Saul Toscano-Palmerin, Peter I Frazier, and Andrew Gordon Wilson. Practical multi- ﬁdelity Bayesian optimization for hyperparameter tuning. In 35th Conference on Uncertainty in Artiﬁcial Intelligence, 2019. 11The following sections are intended to give the reader further insights into our design choices and a deeper understanding of the algorithms properties. First, we give a brief overview of Bayesian optimization with Gaussian processes. We then illustrate our models behavior on a two dimensional problem. Last, we give further details of our experiments for reproducibility purposes. A Bayesian Optimization Preliminaries Bayesian optimization is a sequential approach to global optimization of black-box functions with- out making use of derivatives. It uses two components: a learned surrogate model of the objective function and an acquisition function derived from the surrogate for selecting new points to inform the surrogate with. In-depth discussions beyond our brief overview can be found in recent surveys [2, 9, 35]. Notation. We summarize all of the notations used in our model in Table 1 for ease of reading. A.1 Gaussian processes We present the GP surrogate model for the black-box function f [31]. A GP deﬁnes a probability distribution over functions f under the assumption that any subset of points {(xi, f (xi)}is normally distributed. Formally, this is denoted as: f (x) ∼GP ( m(x),k ( x,x′)) , where m(x) and k (x,x′) are the mean and covariance functions, given by m(x) =E[ f (x)] and k(x,x′) =E [ ( f (x)−m(x))( f (x′)−m(x′))T ] . Typically, the mean of the GP is assumed to be zero everywhere. The kernel k(x,x′) can be thought of as a similarity measure relating f (x) and f (x′). Numerous kernels encoding different prior be- liefs about f (x) have been proposed. A popular choice is given by the square exponential kernel k(x,x′) =σ2 f exp [ −(x −x′)2/2σ2 l ] . The length- and output-scales σ2 l and σ2 f regulate the maximal covariance between two points and can be estimated using maximum marginal likelihood. The SE kernel encodes the belief that nearby points are highly correlated as it is maximized at k(x,x) =σ2 f and decays the further x and x′are separated. For predicting f∗= f (x∗) at a new data point x∗, assuming a zero mean m(x) =0, we have: [ f f∗ ] ∼N ( 0, [ K kT ∗ k∗ k∗∗ ]) (7) where k∗∗= k (x∗,x∗), k∗= [k (x∗,xi)]∀i≤N and K = [k (xi,xj)]∀i, j≤N . The conditional probability of p( f∗|f ) follows a univariate Gaussian distribution as p( f∗|f ) ∼N ( µ (x∗),σ2 (x∗) ) . Its mean and variance are given by: µ (x∗) =k∗K−1y σ2 (x∗) =k∗∗−k∗K−1kT ∗. As GPs give full uncertainty information with any prediction, they provide a ﬂexible nonparametric prior for Bayesian optimization. We refer the interested readers to [31] for further details on GPs. A.2 Acquisition function Bayesian optimization is typically applied in settings in which the objective function is expensive to evaluate. To minimize interactions with that objective, an acquisition function is deﬁned to reason about the selection of the next evaluation point xt+1 = argmaxx∈X αt (x). The acquisition func- tion is constructed from the predictive mean and variance of the surrogate to be easy to evaluate and represents the trade-off between exploration (of points with high predictive uncertainty) and exploitation (of points with high predictive mean). Thus, by design the acquisition function can be maximized with standard global optimization toolboxes. Among the many acquisition functions [12, 13, 14, 32, 40, 44] available in the literature, the expected improvement [14, 27, 45] is one of the most popular. 12Table 1: Notation List Parameter Domain Meaning d integer, N dimension, no. of hyperparameters to be optimized x vector,Rd input hyperparameter N integer, N maximum number of BO iterations Tmin, Tmax integer, N the min/max no of iterations for training a ML algorithm t ∈[Tmin,...Tmax] index of training steps M integer, N the maximum number of augmentation. We set M = 15. δ scalar, R threshold for rejecting augmentation when ln of cond(K) > δ m ∈{1,...M} index of augmenting variables n ∈{1,..., N} index of BO iterations z = [x,t] vector, Rd+1 concatenation of the parameter x and iteration t cn,m scalar, R training cost (sec) yn scalar, R transformed score at the BO iteration n yn,m scalar, R transformed score at the BO iteration n, training step m α(x,t) function acquisition function for performance µc(x,t) function estimation of the cost by LR given x and t r(. |x,t) function a raw learning curve, r(x,t) = [r(1 |x,t),...r(t′|x,t),r(t |x,t)] f (x,t) function a black-box function which is compressed from the above f () l (. |m0,g0) function Logistic curve l(u |m0,g0) = 1 1+exp(−g0[u−m0]) g0, g∗ 0 scalar, R a growth parameter deﬁning a slope, g∗ 0 = argmaxg0 L m0, m∗ 0 scalar, R a middle point parameter, m∗ 0 = argmaxm0 L L scalar, R Gaussian process log marginal likelihood A.3 GP kernels and treatment of GP hyperparameters We present the GP kernels and treatment of GP hyperparameters for the black-box function f . Although the raw learning curve in DRL is noisy, the transformed version using our proposed curve compression is smooth. Therefore, we use two squared exponential kernels for input hyperparameter and training iteration, respectively. That iskx(x,x′) =exp ( −||x−x′||2 2σ2x ) and kt (t,t′) =exp ( −||t−t′||2 2σ2t ) where the observation x and t are normalized to [0,1]d and the outcome y is standardized y ∼ N (0,1) for robustness. As a result, our product kernel becomes k ( [x,t],[x′,t′] ) = k(x,x′)×k(t,t′) =exp ( −||x −x′||2 2σ2x −||t −t′||2 2σ2t ) . The length-scales σx and σt are learnable parameters indicating the variability of the function with regards to the hyperparameter input x and number of training iterations t. Estimating appropriate values for them is critical as this represents the GPs prior regarding the sensitivity of performance w.r.t. changes in the number of training iterations and hyperparameters. For extremely large σt we expect the objective function to change very little for different numbers of training iterations. For small σt by contrast we expect drastic changes even for small differences. We estimate these GP hyperparameters (including the length-scalesσx, σt and the output noise varianceσy) by maximizing their log marginal likelihood [31]. We optimize Eq. (5) with a gradient-based optimizer, providing the analytical gradient to the algo- rithm. We start the optimization from the previous hyperparameter values θprev. If the optimization fails due to numerical issues, we keep the previous value of the hyperparameters. We reﬁt the hy- perparameters every 3×d function evaluations where d is the dimension. B Algorithm Illustration and Further Experiments Fig. 7 and Fig. 8 illustrate the behavior of our proposed algorithm BOIL on the example of opti- mizing the discount factor γ of Dueling DQN [46] on the CartPole problem. The two settings differ in the inclusion augmented observations into BOIL in Fig. 7 and CM-T/F-BO (or BOIL without augmented observations) in Fig. 8. 130.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 1.2 0.8 0.4 0.0 0.4 0.8 1.2 1.6 2.0 2.4 0.00 0.15 0.30 0.45 0.60 0.75 0.90 1.05 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 0.0 0.4 0.8 1.2 1.6 2.0 2.4 2.8 3.2 3.6 0.3 0.6 0.9 1.2 1.5 1.8 2.1 2.4 2.7 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0 2.5 0.00 0.06 0.12 0.18 0.24 0.30 0.36 0.42 0.48 0.64 0.72 0.80 0.88 0.96 1.04 1.12 1.20 1.28 1.36 0.0 0.4 0.8 1.2 1.6 2.0 2.4 2.8 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 1.6 1.2 0.8 0.4 0.0 0.4 0.8 1.2 1.6 2.0 0.00 0.15 0.30 0.45 0.60 0.75 0.90 1.05 0.64 0.72 0.80 0.88 0.96 1.04 1.12 1.20 1.28 1.36 0.0 0.4 0.8 1.2 1.6 2.0 2.4 2.8 3.2 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 2.4 1.6 0.8 0.0 0.8 1.6 2.4 3.2 0.000 0.025 0.050 0.075 0.100 0.125 0.150 0.175 0.200 0.64 0.72 0.80 0.88 0.96 1.04 1.12 1.20 1.28 1.36 0.0 0.6 1.2 1.8 2.4 3.0 3.6 4.2 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 2.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0 0.00 0.15 0.30 0.45 0.60 0.75 0.90 1.05 0.64 0.72 0.80 0.88 0.96 1.04 1.12 1.20 1.28 0.0 0.4 0.8 1.2 1.6 2.0 2.4 2.8 3.2 3.6 0.0 0.4 0.8 1.2 1.6 2.0 2.4 2.8 3.2 3.6 Figure 7: Illustration of BOIL on a 2-dimensional optimization task of DDQN on CartPole. The augmented observations ﬁll the joint hyperparameter-iteration space quickly to inform our surrogate. Our decision balances utility α against cost τ for iteration-efﬁciency. Especially in situations of multiple locations sharing the same utility value, our algorithm prefers to select the cheapest option. Table 2: Dueling DQN algorithm on CartPole problem. Variables Min Max Best Found x∗ γ discount factor 0 .8 1 0 .95586 learning rate model 1 e−6 0.01 0 .00589 #Episodes 300 800 - 140.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 2.0 1.6 1.2 0.8 0.4 0.0 0.4 0.8 0.00 0.15 0.30 0.45 0.60 0.75 0.90 1.05 0.60 0.75 0.90 1.05 1.20 1.35 1.50 1.65 1.80 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 0.0 0.3 0.6 0.9 1.2 1.5 1.8 2.1 2.4 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 1.6 1.2 0.8 0.4 0.0 0.4 0.8 1.2 0.00 0.15 0.30 0.45 0.60 0.75 0.90 1.05 0.60 0.75 0.90 1.05 1.20 1.35 1.50 1.65 1.80 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 1.6 1.2 0.8 0.4 0.0 0.4 0.8 1.2 1.6 0.00 0.08 0.16 0.24 0.32 0.40 0.48 0.56 0.64 0.60 0.75 0.90 1.05 1.20 1.35 1.50 1.65 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 0.30 0.45 0.60 0.75 0.90 1.05 1.20 1.35 1.50 1.65 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 1.6 1.2 0.8 0.4 0.0 0.4 0.8 1.2 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 2.0 1.6 1.2 0.8 0.4 0.0 0.4 0.8 1.2 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 Figure 8: Illustration of the Continuous Multi task/ﬁdelity BO (CM-T/F-BO) -- this is the case of BOIL without using augmented observations (same setting as Fig. 7). This version leads to less efﬁcient optimization as the additional iteration dimension requires more evaluation than optimizing the hyperparameters on their own. 15Table 3: A2C algorithm on Reacher (left) and InvertedPendulum (right). Variables Min Max Best Found x∗ γ discount factor 0 .8 1 0 .8 learning rate actor 1 e−6 0.01 0 .00071 learning rate critic 1 e−6 0.01 0 .00042 #Episodes 200 500 - Min Max Best Found x∗ 0.8 1 0 .95586 1e−6 0.01 0 .00589 1e−6 0.01 0 .00037 700 1500 - Table 4: Convolutional Neural Network. Variables Min Max Best Found x∗ ﬁlter size 1 8 5 pool size 1 5 5 batch size 16 1000 8 learning rate 1 e−6 0.01 0 .000484 momentum 0 .8 0 .999 0 .82852 decay 0 .9 0 .999 0 .9746 number of epoch 30 150 - In both cases, we plot the GP predictive mean in Eq. (1), GP predictive variance in Eq. (2), the acquisition function in Eq. (3), the predicted function and the ﬁnal decision function in Eq. (8). These equations are deﬁned in the main manuscript. As shown in the respective ﬁgures the ﬁnal decision function balances between utility and cost of any pair (γ,t) to achieve iteration efﬁciency. Especially in situations where multiple locations share the same utility value, our decision will prefer to select the cheapest option. Using the augmented observations in Fig. 7, our joint space is ﬁlled quicker with points and the uncertainty (GP variance) across it reduces faster than in Fig. 8 – the case of vanilla CM-T/F-BO without augmenting obser- vations. A second advantage of having augmented observations is that the algorithm is discouraged to select the same hyperparameter setting at lower ﬁdelity than a previous evaluation. We do not add the full curve as it can be redundant while causing the conditioning problem of the GP covariance matrix. B.1 Experiment settings We summarize the hyperparameter search ranges for A2C on Reacher and InvertedPendulum in Table 3, CNN on SHVN in Table 4 and DDQN on CartPole in Table 2. Additionally, we present the best found parameter x∗for these problems. Further details of the DRL agents are listed in Table 5. B.2 Learning Logistic Function We ﬁrst present the Logistic curve l(u |x,t) = 1 1+exp(−g0[u−m0]) using different choices of g0 and m0 in Fig. 10. We then learn from the data to get the optimal choices g∗ 0 and m∗ 0 presented in Fig. 11. Table 5: Further speciﬁcation for DRL agents Hyperparameter Value A2C Critic-network architecture [32,32] Actor-network architecture [32,32] Entropy coefﬁcient 0 .01 Dueling DQN Q-network architecture [50,50] ε-greedy (start, ﬁnal, number of steps) (1.0,0.05,10000) Buffer size 10000 Batch size 64 PER-α [33] 1 .0 PER-β (start, ﬁnal, number of steps) (1.0,0.6,1000) 160 100 200 300 400 500 Episodes 70 60 50 40 30 20 Average Reward Preference Curve as Sigmoid Best Found Reward Curve Sigmoid Curve 0 100 200 300 400 500 Episodes 90 80 70 60 50 40 30 20 10 Reward Preference Curve as Sigmoid Best Found Reward Curve Sigmoid Curve 0 100 200 300 400 500 Episodes 60 50 40 30 20 10 Average Reward Preference Curve as Log Best Found Reward Curve Log Curve 0 100 200 300 400 500 Episodes 100 80 60 40 20 0 Reward Preference Curve as Log Best Found Reward Curve Log Curve 0 100 200 300 400 500 Episodes 60 50 40 30 20 Average Reward Preference Curve as Average Best Found Reward Curve Average Curve 0 100 200 300 400 500 Episodes 80 60 40 20 0 Reward Preference Curve as Average Best Found Reward Curve Average Curve Figure 9: To highlight the robustness, we examine the results using different preference functions such as Sigmoid curve, Log curve, and Average curve on Reacher experiments. The results include the best found reward curve with different preference choices that show the robustness of our model. Left column: the best found curve using averaged reward over 100 consecutive episodes. Right column: the best found curve using the original reward. 17/uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000019 /uni00000013/uni00000011/uni0000001a m0/uni00000020/uni00000010/uni00000016/uni00000003g0/uni00000020/uni00000013/uni00000011/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000016/uni00000003g0/uni00000020/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000016/uni00000003g0/uni00000020/uni00000015 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000016/uni00000003g0/uni00000020/uni00000016 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000019 /uni00000013/uni00000011/uni0000001a m0/uni00000020/uni00000010/uni00000015/uni00000003g0/uni00000020/uni00000013/uni00000011/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000015/uni00000003g0/uni00000020/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000015/uni00000003g0/uni00000020/uni00000015 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000015/uni00000003g0/uni00000020/uni00000016 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000019 m0/uni00000020/uni00000010/uni00000014/uni00000003g0/uni00000020/uni00000013/uni00000011/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000014/uni00000003g0/uni00000020/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000014/uni00000003g0/uni00000020/uni00000015 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000014/uni00000003g0/uni00000020/uni00000016 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000019 m0/uni00000020/uni00000013/uni00000003g0/uni00000020/uni00000013/uni00000011/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000013/uni00000003g0/uni00000020/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000013/uni00000003g0/uni00000020/uni00000015 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000013/uni00000003g0/uni00000020/uni00000016 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000019 m0/uni00000020/uni00000014/uni00000003g0/uni00000020/uni00000013/uni00000011/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000014/uni00000003g0/uni00000020/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000014/uni00000003g0/uni00000020/uni00000015 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000014/uni00000003g0/uni00000020/uni00000016 Figure 10: Examples of Logistic function l(u) = 1 1+exp(−g0[u−m0]) with different values of middle parameter m0 and growth parameter g0. B.3 Robustness over Different Preference Functions We next study the learning effects with respect to different choices of the preference functions. We pick three preference functions including the Sigmoid, Log and Average to compute the utility score for each learning curve. Then, we report the best found reward curve under such choices. The experiments are tested using A2C on Reacher-v2. The results presented in Fig. 9 demonstrate the robustness of our model with the preference functions. B.4 Applying Freeze-Thaw BO in the settings considered While both the exponential decay in Freeze-Thaw BO [42] and our compression function encode preferences regarding training development, there is an important distinction between the two ap- proaches. Freeze-thaw BO utilises the exponential decay property to terminate the training curve, while BOIL only uses the sigmoid curve to guide the search. We refer to Fig. 13 for further illustra- tion of why Freeze-thaw BO struggles in DRL settings. B.5 Ablation Study using Freeze-Thraw Kernel for Time In the joint modeling framework of hyperparameter and time (iteration), we can replace the kernel either k(x,x) or k(t,t) with different choices. We, therefore, set up a new baseline of using the time- kernel k(t,t′) in Freeze-Thaw approach [42] which encodes the monotonously exponential decay from the curve. Particularly, we use the kernel deﬁned as k(t,t′) = βα (t +t′+β)α for parameters α,β > 0 which are optimized in the GP models. 186  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l( . | m * 0 ,g * 0 ) for CartPole m * 0 =-3.266   g * 0 =3.0 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l(m * 0 ,g * 0 ) for CNN_SHVN m * 0 =2.245   g * 0 =2.092 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l(m * 0 ,g * 0 ) for InvPendulum m * 0 =1.649   g * 0 =1.833 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l( . | m * 0 ,g * 0 ) for Cifar10 m * 0 =-4.0   g * 0 =1.476 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l( . | m * 0 ,g * 0 ) for Reacher m * 0 =2.779   g * 0 =1.973 Figure 11: We learn the suitable transformation curve directly from the data. We parameterized the Logistic curve as l (m0,g0) = 1 1+exp(−g0[1−m0]) then estimate g0 and m0. The estimated function l(m∗ 0,g∗ 0) is then used to compress our curve. The above plots are the estimated l() at different environments and datasets. /uni00000013/uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013/uni0000001a/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni0000001b/uni00000013 /uni0000001c/uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000014/uni00000013 /uni00000014/uni00000015/uni00000013/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni0000002c/uni00000051/uni00000059/uni00000048/uni00000055/uni00000057/uni00000048/uni00000047/uni00000033/uni00000048/uni00000051/uni00000047/uni00000058/uni0000004f/uni00000058/uni00000050/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013 /uni00000018/uni00000013/uni00000014/uni00000013/uni00000013/uni00000014/uni00000018/uni00000013/uni00000015/uni00000013/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni0000001b/uni00000013 /uni0000001c/uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000014/uni00000013 /uni00000014/uni00000015/uni00000013/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni0000002c/uni00000051/uni00000059/uni00000048/uni00000055/uni00000057/uni00000048/uni00000047/uni00000033/uni00000048/uni00000051/uni00000047/uni00000058/uni0000004f/uni00000058/uni00000050/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013 /uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni00000014/uni0000001a/uni00000011/uni00000018 /uni00000015/uni00000013/uni00000011/uni00000013 /uni00000015/uni00000015/uni00000011/uni00000018 /uni00000015/uni00000018/uni00000011/uni00000013 /uni00000015/uni0000001a/uni00000011/uni00000018 /uni00000016/uni00000013/uni00000011/uni00000013 /uni00000016/uni00000015/uni00000011/uni00000018/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c /uni0000003e/uni00000026/uni00000031/uni00000031/uni00000010/uni00000026/uni0000004c/uni00000049/uni00000044/uni00000055/uni00000014/uni00000013/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000019/uni00000003/uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000053/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000014/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000018/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni00000014/uni0000001a/uni00000011/uni00000018 /uni00000015/uni00000013/uni00000011/uni00000013 /uni00000015/uni00000015/uni00000011/uni00000018 /uni00000015/uni00000018/uni00000011/uni00000013 /uni00000015/uni0000001a/uni00000011/uni00000018 /uni00000016/uni00000013/uni00000011/uni00000013 /uni00000016/uni00000015/uni00000011/uni00000018/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c /uni0000003e/uni00000026/uni00000031/uni00000031/uni00000010/uni00000026/uni0000004c/uni00000049/uni00000044/uni00000055/uni00000014/uni00000013/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000019/uni00000003/uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000053/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f Figure 12: Tuning hyperparameters of a DRL on InvertedPendulum and a CNN model on CIFAR10. 190 250 500 750 1000 1250 1500 Epoch 0 20 40 60 80 100 120Reward Reward Curve Freeze-thaw 0 250 500 750 1000 1250 1500 Epoch 0 50 100 150 0 250 500 750 1000 1250 1500 Epoch 0 50 100 150 0 250 500 750 1000 1250 1500 Epoch 0 50 100 150 Reward Curves Examples using A2C on Inverted Pendulum Figure 13: Illustration of Freeze-thaw BO in DRL. Freeze-thaw BO will terminate training processes when training performance (in blue) signiﬁcantly drops (i.e. at the red locations) as the exponential decay model will predict low ﬁnal performance. In most RL enviroments noisy training curves are unavoidable. Thus, Freeze-thaw BO will dismiss all curves including good setting, never completing a single training run before the ﬁnal epoch. /uni00000013 /uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni00000014/uni00000015/uni0000001b /uni00000014/uni00000015/uni0000001c /uni00000014/uni00000016/uni00000013 /uni00000014/uni00000016/uni00000014 /uni00000014/uni00000016/uni00000015 /uni00000014/uni00000016/uni00000016 /uni00000014/uni00000016/uni00000017/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni00000035/uni00000048/uni00000044/uni00000046/uni0000004b/uni00000048/uni00000055/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000025/uni00000032/uni0000002c/uni0000002f/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni00000014/uni00000015/uni0000001b /uni00000014/uni00000015/uni0000001c /uni00000014/uni00000016/uni00000013 /uni00000014/uni00000016/uni00000014 /uni00000014/uni00000016/uni00000015 /uni00000014/uni00000016/uni00000016 /uni00000014/uni00000016/uni00000017/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni00000035/uni00000048/uni00000044/uni00000046/uni0000004b/uni00000048/uni00000055/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000025/uni00000032/uni0000002c/uni0000002f/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000013 /uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni0000001c/uni00000013 /uni0000001c/uni00000018 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000018 /uni00000014/uni00000014/uni00000013 /uni00000014/uni00000014/uni00000018 /uni00000014/uni00000015/uni00000013 /uni00000014/uni00000015/uni00000018/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni0000002c/uni00000051/uni00000059/uni00000048/uni00000055/uni00000057/uni00000048/uni00000047/uni00000033/uni00000048/uni00000051/uni00000047/uni00000058/uni0000004f/uni00000058/uni00000050/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000026/uni00000010/uni00000030/uni00000012/uni00000037/uni00000010/uni00000025/uni00000032/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000025/uni00000032/uni0000002c/uni0000002f/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000013 /uni00000018/uni00000013/uni00000014/uni00000013/uni00000013/uni00000014/uni00000018/uni00000013/uni00000015/uni00000013/uni00000013/uni00000015/uni00000018/uni00000013/uni00000016/uni00000013/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni0000001c/uni00000013 /uni0000001c/uni00000018 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000018 /uni00000014/uni00000014/uni00000013 /uni00000014/uni00000014/uni00000018 /uni00000014/uni00000015/uni00000013 /uni00000014/uni00000015/uni00000018/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni0000002c/uni00000051/uni00000059/uni00000048/uni00000055/uni00000057/uni00000048/uni00000047/uni00000033/uni00000048/uni00000051/uni00000047/uni00000058/uni0000004f/uni00000058/uni00000050/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000025/uni00000032/uni0000002c/uni0000002f/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a Figure 14: Comparison using freezethaw kernel for time component. We present the result in Fig. 14 that CM-T/F-BO is still less competitive to BOIL using this speciﬁc time kernel. The results again validate the robustness our approach cross different choices of kernel. B.6 Additional Experiments for Tuning DRL and CNN We present the additional experiments for tuning a DRL model using InvertedPendulum environ- ment and a CNN model using a subset of CIFAR10 in Fig. 12. Again, we show that the proposed model clearly gain advantages against the baselines in tuning hyperparameters for model with itera- tive learning information available. B.7 Examples of Deep Reinforcement Learning Training Curves Finally, we present examples of training curves produced by the deep reinforcement learning al- gorithm A2C in Fig. 15. These ﬂuctuate widely and it may not be trivial to deﬁne good stopping criteria as done for other applications in previous work [42]. 200 200 400 80 70 60 50 40 0 200 400 110 100 90 80 70 60 50 0 200 400 110 100 90 80 70 60 50 0 200 400 70 60 50 40 30 0 200 400 70 60 50 40 30 20 10 0 200 400 50 40 30 20 0 200 400 90 80 70 60 50 0 200 400 70 60 50 40 30 20 10 0 200 400 85 80 75 70 0 200 400 60 50 40 30 20 0 200 400 100 90 80 70 60 0 200 400 100 90 80 70 0 200 400 80 60 40 20 0 200 400 100 90 80 70 60 0 200 400 100 90 80 70 0 200 400 80 70 60 50 40 30 20 0 200 400 70 60 50 40 30 20 0 200 400 70 60 50 40 30 20 0 200 400 100 90 80 70 60 50 0 200 400 70 60 50 40 30 20 0 200 400 100 90 80 70 60 50 0 200 400 100 90 80 70 60 0 200 400 60 50 40 30 20 0 200 400 80 60 40 20 0 200 400 85 80 75 70 65 60 0 200 400 100 95 90 85 80 0 200 400 80 60 40 20 0 200 400 100 90 80 70 60 50 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 Figure 15: Examples of reward curves using A2C on Reacher-v2 (rows 1 −3) and on InvertedPendulum-v2 (rows 4 −6). Y-axis is the reward averaged over 100 consecutive episodes. X-axis is the episode. The noisy performance illustrated is typical of DRL settings and complicates the design of early stopping criteria. Due to the property of DRL, it is not trivial to decide when to stop the training curve. In addition, it will be misleading if we only take average over the last 100 iterations. 21",
      "meta_data": {
        "arxiv_id": "1909.09593v5",
        "authors": [
          "Vu Nguyen",
          "Sebastian Schulze",
          "Michael A Osborne"
        ],
        "published_date": "2019-09-20T16:14:34Z",
        "pdf_url": "https://arxiv.org/pdf/1909.09593v5.pdf"
      }
    },
    {
      "title": "Learning to Mutate with Hypergradient Guided Population"
    },
    {
      "title": "Hyperparameter Optimization through Neural Network Partitioning",
      "abstract": "Well-tuned hyperparameters are crucial for obtaining good generalization\nbehavior in neural networks. They can enforce appropriate inductive biases,\nregularize the model and improve performance -- especially in the presence of\nlimited data. In this work, we propose a simple and efficient way for\noptimizing hyperparameters inspired by the marginal likelihood, an optimization\nobjective that requires no validation data. Our method partitions the training\ndata and a neural network model into $K$ data shards and parameter partitions,\nrespectively. Each partition is associated with and optimized only on specific\ndata shards. Combining these partitions into subnetworks allows us to define\nthe ``out-of-training-sample\" loss of a subnetwork, i.e., the loss on data\nshards unseen by the subnetwork, as the objective for hyperparameter\noptimization. We demonstrate that we can apply this objective to optimize a\nvariety of different hyperparameters in a single training run while being\nsignificantly computationally cheaper than alternative methods aiming to\noptimize the marginal likelihood for neural networks. Lastly, we also focus on\noptimizing hyperparameters in federated learning, where retraining and\ncross-validation are particularly challenging.",
      "full_text": "Published as a conference paper at ICLR 2023 HYPERPARAMETER OPTIMIZATION THROUGH NEURAL NETWORK PARTITIONING Bruno Mlodozeniec†∗, Matthias Reisser‡, Christos Louizos‡ †University of Cambridge, ‡Qualcomm AI Research bkm28@cam.ac.uk, {mreisser,clouizos}@qti.qualcomm.com ABSTRACT Well-tuned hyperparameters are crucial for obtaining good generalization behavior in neural networks. They can enforce appropriate inductive biases, regularize the model and improve performance — especially in the presence of limited data. In this work, we propose a simple and efﬁcient way for optimizing hyperparameters inspired by the marginal likelihood, an optimization objective that requires no validation data. Our method partitions the training data and a neural network model into K data shards and parameter partitions, respectively. Each partition is associated with and optimized only on speciﬁc data shards. Combining these partitions into subnetworks allows us to deﬁne the “out-of-training-sample” loss of a subnetwork, i.e., the loss on data shards unseen by the subnetwork, as the objective for hyperparameter optimization. We demonstrate that we can apply this objective to optimize a variety of different hyperparameters in a single training run while being signiﬁcantly computationally cheaper than alternative methods aiming to optimize the marginal likelihood for neural networks. Lastly, we also focus on optimizing hyperparameters in federated learning, where retraining and cross-validation are particularly challenging. 1 I NTRODUCTION Due to their remarkable generalization capabilities, deep neural networks have become the de-facto models for a wide range of complex tasks. Combining large models, large-enough datasets, and sufﬁcient computing capabilities enable researchers to train powerful models through gradient descent. Regardless of the data regime, however, the choice of hyperparameters — such as neural architecture, data augmentation strategies, regularization, or which optimizer to choose — plays a crucial role in the ﬁnal model’s generalization capabilities. Hyperparameters allow encoding good inductive biases that effectively constrain the models’ hypothesis space (e.g., convolutions for vision tasks), speed up learning, or prevent overﬁtting in the case of limited data. Whereas gradient descent enables the tuning of model parameters, accessing hyperparameter gradients is more complicated. The traditional and general way to optimize hyperparameters operates as follows; 1) partition the dataset into training and validation data1, 2) pick a set of hyperparameters and optimize the model on the training data, 3) measure the performance of the model on the validation data and ﬁnally 4) use the validation metric as a way to score models or perform search over the space of hyperparameters. This approach inherently requires training multiple models and consequently requires spending resources on models that will be discarded. Furthermore, traditional tuning requires a validation set since optimizing the hyperparameters on the training set alone cannot identify the right inductive biases. A canonical example is data augmentations — they are not expected to improve training set performance, but they greatly help with generalization. In the low data regime, deﬁning a validation set that cannot be used for tuning model parameters is undesirable. Picking the right amount of validation data is a hyperparameter in itself. The conventional rule of thumb to use ∼10% of all data can result in signiﬁcant overﬁtting, as pointed out by Lorraine et al. (2019) , when one has a sufﬁciently large number of hyperparameters to tune. Furthermore, a validation set can be challenging ∗Work done while at Qualcomm AI Research. Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc. and/or its subsidiaries. 1a third partition, the test or holdout set is used to estimate the ﬁnal model performance 1 arXiv:2304.14766v1  [cs.LG]  28 Apr 2023Published as a conference paper at ICLR 2023 to obtain in many use cases. An example is Federated Learning (FL) (McMahan et al., 2017), which we speciﬁcally consider in our experimental section. In FL, each extra training run (for,e.g., a speciﬁc hyperparameter setting) comes with additional, non-trivial costs. Different approaches have been proposed in order to address these challenges. Some schemes optimize hyperparameters during a single training run by making the hyperparameters part of the model (e.g., learning dropout rates with concrete dropout (Gal et al., 2017), learning architectures with DARTs (Liu et al., 2018) and learning data-augmentations with schemes as in Benton et al. (2020); van der Wilk et al. (2018)). In cases where the model does not depend on the hyperparameters directly but only indirectly through their effect on the value of the ﬁnal parameters (through optimization), schemes for differentiating through the training procedures have been proposed, such as Lorraine et al. (2019). Another way of optimizing hyperparameters without a validation set is through the canonical view on model selection (and hence hyperparameter optimization) through the Bayesian lens; the concept of optimizing the marginal likelihood. For deep neural networks, however, the marginal likelihood is difﬁcult to compute. Prior works have therefore developed various approximations for its use in deep learning models and used those to optimize hyperparameters in deep learning, such as those of data augmentation (Schw¨obel et al., 2021; Immer et al., 2022). Still, however, these come at a signiﬁcant added computational expense and do not scale to larger deep learning problems. This paper presents a novel approach to hyperparameter optimization, inspired by the marginal likelihood, that only requires a single training run and no validation set. Our method is more scalable than previous works that rely on marginal likelihood and Laplace approximations (which require computing or inverting a Hessian (Immer et al., 2021)) and is broadly applicable to any hierarchical modelling setup. 2 M ARGINAL LIKELIHOOD AND PRIOR WORK In Bayesian inference, the rules of probability dictate how any unknown, such as parameters w or hyperparameters ψ, should be determined given observed data D. Let p(w) be a prior over w and p(D|w,ψ) be a likelihood for Dwith ψbeing the hyperparameters. We are then interested in the posterior given the data p(w|D,ψ) =p(D|w,ψ)p(w)/p(D|ψ). The denominator term p(D|ψ) is known as the marginal likelihood, as it measures the probability of observing the data given ψ, irrespective of the value of w: p(D|ψ) = ∫ p(w)p(D|w,ψ)dw. Marginal likelihood has many desirable properties that make it a good criterion for model selection and hyperparameter optimization. It intuitively implements the essence of Occam’s Razor principle (MacKay, 2003, § 28). In the PAC-Bayesian literature, it has been shown that higher marginal likelihood gives tighter frequentist upper bounds on the generalization performance of a given model class (McAllester, 1998; Germain et al., 2016). It also has close links to cross-validation (see section 2.1) and can be computed from the training data alone. However, computation of the marginal likelihood in deep learning models is usually prohibitively expensive and many recent works have proposed schemes to approximate the marginal likelihood for differentiable model selection (Lyle et al., 2020; Immer et al., 2021; 2022; Schw¨obel et al., 2021). 2.1 “L EARNING SPEED ” PERSPECTIVE Lyle et al. (2020); Fong and Holmes (2020) pointed out the correspondence between “learning speed” and marginal likelihood. Namely, the marginal likelihood of the data Dconditioned on some hyperparameters ψcan be written as: log p(D|ψ) = ∑ k log Ep(w|D1:k−1,ψ) [p(Dk|w,ψ)] ≥ ∑ k Ep(w|D1:k−1,ψ) [log p(Dk|w,ψ)] (1) where (D1,..., DC) is an arbitrary partitioning of the training dataset Dinto Cshards or chunks2, and p(w|D1:k,ψ) is the posterior over parameters of a function fw : X → Y, from the input domain Xto the target domain Yafter seeing data in shards 1 through k. The right-hand side can be interpreted as a type of cross-validation in which we ﬁx an ordering over the shards and measure the “validation” performance on each shardDk using a model trained on the preceding shards D1:k−1. 2We use the terms “chunk” and “shard” interchangeably. 2Published as a conference paper at ICLR 2023 Alternatively, it can be viewed as the learning speed of a (probabilistic) model: i.e., a measure of how quickly it learns to perform well on new shards of data after only having been ﬁt to the previous shards (through exact Bayesian updating). This perspective neatly illustrates why models with higher marginal likelihood can exhibit good inductive biases, e.g., encoded through ψ, w and fw. Namely, such models can be expected to learn faster and generalize better after seeing fewer samples. For example, if the hypothesis space is constrained3to functions satisfying symmetries present in the data, we need fewer data to identify the correct function (Sokolic et al., 2017; Sannai et al., 2021). We argue that the “learning speed” aspect of marginal likelihood — i.e., measuring how well the model generalizes to new data in the training set, having been trained only on the previous data points — is the key property making marginal likelihood a useful tool for selecting hyperparameters. 2.2 T RAINING SPEED FOR HYPERPARAMETER OPTIMIZATION Computing the “learning speed”, requires samples from the posteriorp(w|D1:k,ψ). Unfortunately, in deep learning settings, such samples are impractical to obtain; thus, prior works have focused on more scalable alternatives. Lyle et al. (2020) propose to approximate the objective in Eq. 1 by looking at the training speed during standard training of a neural network by SGD. Speciﬁcally, they deﬁne the training speed as the reduction in the training loss after a single SGD parameter update, summed over all updates in the ﬁrst epoch. They argue that, during the ﬁrst epoch of training, after the neural network parameters, w, have been updated with SGD steps using data from shards D1:k, they can be approximately used in place of the sample from the posterior p(w|D1:k,ψ) in Eq. 1. They extend the analogy to training past one epoch and use the training speed estimate for model selection (Ru et al., 2021). As pointed out by the authors, however, the analogy between learning speed and training speed somewhat breaks down after 1 epoch of training. The network parameters have “seen” every datapoint in the training set after1 epoch, and hence the connection to measuring the model’s generalization capability is weakened. For the sake of scalability and alignment with deep learning practice, we also focus on simple pointwise approximations qk(w) = δ(w = ˆwk) to the posteriors p(w|D1:k,ψ). However, in contrast to prior work, we explicitly parametrize the learning procedure such that, at any given training iteration, we have access to a model that is trained only on a subset of the dataD1:k. In doing so, we can approximate the objective in Eq. 1, and thus use it to optimize the hyperparameters during the entire training run. 3 P ARTITIONED NEURAL NETWORKS Our goal is to optimize the objective LML (D,ψ) = C∑ k=1 Eqk−1(w) [log p(Dk|w,ψ)] (2) wrt. ψ, which is an approximation to the lower-bound presented in Eq. 1 above. In Appendix A, we show that the left-hand side is also a lower-bound on the marginal likelihood under some unobtrusive conditions. As mentioned in Section 2.2, our goal is to propose an architecture and a training scheme so that we can easily obtain models trained on only subsets of the data D1:k for all k throughout training. We propose that each {qk(w)}C k=1 optimizes a subset of the parameters of the neural network, in a manner that allows us to extract “subnetworks” from the main network that have been trained on speciﬁc chunks of data. We describe the partitioning scheme below. Partitioning the parameters Denote the concatenations of the weights of a neural networkw ∈RN. We can deﬁne a partitioning ((w1,..., wC),P) of the parameters into C partitions, such that w = Pconcat(w1,..., wC) for a permutation matrix P ∈{0,1}N×N. For ease of exposition, we drop the dependence on P, assuming that w is already arranged such that P is identity, P = IN×N. Given the partitioning (w1,..., wC) of the parameters, we then specify Csubnetworks with weights w(1) s ,..., w(C) s such that w(k) s = concat(w1,..., wk, ˆwk+1,..., ˆwC), where ˆwi are some default 3or if the learning algorithm is heavily biased towards returning hypotheses that satisfy a given invariance, e.g., through the use of a prior. 3Published as a conference paper at ICLR 2023 values not optimized during training4. More speciﬁcally, the k-th subnetwork, wk s, retains the ﬁrst kpartitions from the weight partitioning and sets the remaining parameters to ˆwk+1:C. Note that, if each wk is only updated on chunks D1:k, the subnetwork w(k) s is only comprised of weights that have been updated on D1:k. Thus, we can view the parameters of w(k) s as an approximation to qk(w). Although, given that a subset of the parameters in each w(k) s is ﬁxed, this would likely be a poor approximation to the true posterior over the weights given D1:k, it could be, intuitively, a reasonable approximation in function space5. Partitioned training Having partitioned the dataset Dinto Cchunks (D1,..., Dk), we update each partition wk by optimising the negative log-likelihood6on chunks D1:k using subnetwork w(k) s by computing the following gradients: ∇wkL ( D1:k,w(k) s ) = ∑ (x,y)∈D1:k ∇wk log p ( y ⏐⏐⏐x; w(k) s ,ψ ) . (3) We interleave stochastic gradient updates of each partition of the weights with updating the hyperpa- rameters ψusing LML in Eq. 2: ∇ψLML (D,ψ) ≈ C∑ k=2 ∑ (x,y)∈Dk ∇ψlog p ( y ⏐⏐⏐x,w(k−1) s ,ψ ) . (4) This can be seen as the sum of the out-of-sample losses for each subnetwork w(k) s . The scheme is illustrated in Figure 1. For details of how the updates are scheduled in our experiments, see Appendix I. Note that, while we could incorporate the gradient of the ﬁrst term from Eq. 1 corresponding to Eq0(w)[log p(D1|w,ψ)] in Eq. 4, we chose to leave it out. Hence, the gradient of Eq. 4 is of an estimate that can be viewed as an approximation to the conditional marginal likelihood log p(D2:C|D1,ψ). Conditional marginal likelihood has been shown to have many desirable properties for model selection and, in many cases, can be a better proxy for generalization (Lotﬁ et al., 2022). Weights: w = (w1,w2,w3) Alternate: Optimize parameters: log p ( D1 |(w1, ˆw2, ˆw3)   Subnet. 1 ,ψ ) w.r.t. w1 log p ( D1:2|(w1,w2, ˆw3)   Subnet. 2 ,ψ ) w.r.t. w2 log p ( D1:3|(w1,w2,w3)   Subnet. 3 ,ψ ) w.r.t. w3 Optimize hyper parameters ψon: log p ( D2|(w1, ˆw2, ˆw3)   Subnet. 1 ,ψ ) + logp ( D3|(w1,w2, ˆw3)   Subnet. 2 ,ψ ) Figure 1: Best viewed in colour. Illustration of the partitioning scheme for a single hidden layer perceptron with C = 3chunks. This procedure, inspired by the marginal likelihood, has several desirable properties compared to prior work. 1) Our objective is computationally efﬁcient, with a computational cost roughly corresponding to evaluating subnetworks on the training set. There is no need to compute nor invert a Hessian with 4e.g., ˆwi could be the value of the weights at initialization, or ˆwi = 0 corresponding to pruning those parameters and obtaining a proper subnetwork. 5Since a) the mapping from parameters to functions is not bijective and b) neural networks are highly overparameterised and can be heavily pruned while retaining performance (Frankle and Carbin, 2018), obtaining a good ﬁt to a subset of the training data with a subset of the model parameters should be possible. Furthermore, “scaling laws” indicate that the beneﬁt of having more parameters becomes apparent mostly for larger dataset sizes (Kaplan et al., 2020), thus it is reasonable for subnetworks ﬁt to more data to have more learnable parameters. 6Optionally with an added negative log-prior regularization term log p(w(k) s ). 4Published as a conference paper at ICLR 2023 respect to the weights, as in the Laplace approximation (Immer et al., 2021; 2022). 2) Our objective is readily amenable to optimization by stochastic gradient descent; we do not have to iterate over the entire training set to compute a single gradient update for the hyperparameters. 3) Compared to the training speed objective (Lyle et al., 2020), in our method, the training of the weights in each subnetwork progresses independently of the data in future chunks. Hence, it can be seen as more truthfully measuring the generalization capability of a model using a given set of hyperparameters. Partitioning Schemes There are several ways in which the neural network weights can be partitioned. In our experiments in Section 5, we partition the weights before beginning training by assigning a ﬁxed proportion of weights in each layer to a given partition at random. For each subnetwork, for the weight partitions corresponding to future chunks, we use the values of the weights at initialisation. For a discussion of partitioning schemes, see Appendix C. 4 R ELATED WORKS Hyperparameter optimization in deep learning Many works have tackled the challenge of op- timizing hyperparameters in deep learning. Works on implicit differentiation, such as the one by Lorraine et al. (2019), allow for optimizing training hyperparameters such as the learning rate, weight- decay, or other hyperparameters that affect the ﬁnal neural network weights only through the training routine. Other works have proposed ways to parameterize and optimize data-augmentations (Cubuk et al., 2018; Li et al., 2020), search-spaces for neural network architectures, as well as methods to optimize architectures using gradient-based optimization (Liu et al., 2018; Elsken et al., 2019). All of the above works have primarily relied on optimizing hyperparameters on a separate validation set and are compatible with the objective deﬁned in this work. Several works have also aimed to cast learning data augmentations as an invariance learning problem. They do so by parameterizing the model itself with data augmentations, and frame invariance learning as a model selection problem (van der Wilk et al., 2018; Benton et al., 2020; Schw¨obel et al., 2021; Nabarro et al., 2022; Immer et al., 2022). We compare against Benton et al. (2020) (“Augerino”) and Immer et al. (2022) (“Differentiable Laplace”) on this task in the experimental section. Hyperparameter optimization without a validation set A limited number of works consider learning hyperparameters without a validation set in a deep learning context. Benton et al. (2020) propose a simple method for learning invariances without a validation set by regularising invariance hyperparameters to those resulting in higher invariance. They show that the invariances found tend to be insensitive to the regularisation strength, determined by another hyperparameter. However, the method relies on being able to a priori deﬁne which hyperparameters lead to higher invariance through a suitable regularisation function. In more complex invariance learning settings, deﬁning the regulariser can be challenging. For example, if data-augmentation transformations were to be parameterized by a neural network (as proposed in Lorraine et al. (2019)), it is non-trivial to devise an adequate regulariser. We show that our method can be applied to such settings. Other works focus on deriving tractable approximations to the marginal likelihood for deep neural networks. Schw ¨obel et al. (2021) propose only marginalising-out the parameters in the last layer of the neural network by switching it out for a Gaussian Process. They treat the preceding layer effectively as a hyperparameter, and optimize invariance parameters using the marginal likelihood. Although they show promising results on MNIST, they found they “were unable to learn invariances for CIFAR-10” (Schw¨obel et al., 2021, §7) and highlighted the need to marginalise lower layers as well. In contrast, our objective can be seen as being inspired by marginal likelihood where arbitrary network layers can be “marginalised”, and works on datasets like CIFAR-10. Immer et al. (2022) have adapted the Laplace approximation (Immer et al., 2021) to make it tractable for learning data augmentations. In contrast to Schw¨obel et al. (2021), they approximately marginalize out all the network parameters, and performs favourably. Their approximation, however, requires approximations to a Hessian w.r.t. all network parameters; for that reason, their work reports results for architectures only up to a ResNet-14, whereas our method can easily scale to larger architectures. Hyperparameter optimization in FL Improving hyperparameter optimization is especially rele- vant to FL. Given the potential system level constraints (Wang et al., 2021), methods that optimize the hyperparameters and parameters in a single training run are preferred. On this note, Khodak et al. (2021) introduced FedEx and showed that it can successfully optimize the client optimizer 5Published as a conference paper at ICLR 2023 hyperparameters. FedEx relies on a training/validation split on the client level and uses a REIN- FORCE type of gradient (Williams, 1992) estimator, which usually exhibits high variance and needs baselines to reduce it (Mohamed et al., 2020). This is in contrast to partitioned networks, which use standard, low-variance backpropagation for the hyperparameters and no separate validation set per client. To optimize the other hyperparameters, Khodak et al. (2021) wrapped FedEx with a traditional hyperparameter optimization strategy, the successive halving algorithm. This is orthogonal to our method and could be applied to partitioned networks as well. In Zhou et al. (2021), the authors perform a hyperparameter search independently on each client with some off-the-shelf methods and then aggregate the results of the search at the server once in order to identify the best hyperparameter setting. The main drawback of this method compared to partitioned networks is that when the local client datasets are small, a client-speciﬁc validation set is not informative, and the aggregation happens only once. Finally, there is also the recent work from Seng et al. (2022) which performs hyperparameter optimization and neural architecture search in the federated setting. Similarly to prior works, it requires client-speciﬁc validation data in order to optimize the hyperparameters. 5 E XPERIMENTS 1 5 10 15 20 25 30 Num. inputs 0.7 0.8 0.9 1.0 Accuracy 0.5 0.4 0.3 0.2 0.1 0.0 Average Log-likelihoodPosthoc Diagonal Laplace Train T est 1 5 10 15 20 25 30 Num. inputs 103 102 Log Marginal Likelihood Estimate Partitioned (a) 0 400080001200016000200002400028000 Iteration 0 5 10 15 20 25Input Mask Element 0.00 0.25 0.50 0.75 1.00 Mask Probability  (b) Figure 2: (a) Demonstrating the ability of the marginal-likelihood inspired objective LML to identify the correct model on a toy input selection task. We plot the hyperparameter objective, train and test set accuracy, and train and test set log-likelihood with the partitioned networks method (left), and the post-hoc diagonal Laplace method (Immer et al., 2021) (right). (b) Mask over input features learned by partitioned networks over time. The ﬁrst 15 features are correctly identiﬁed. Input Selection To demonstrate that LML is a good objective for model selection that captures the desirable properties of the marginal likelihood, we ﬁrst deploy our method on the toy model selection task of Lyle et al. (2020): there the ﬁrst 15 features are informative, and the remaining15 are spurious y∼Bern (1 2 ) x = [ y+ ϵ1,...,y + ϵ15   Informative ,ϵ16,...,ϵ 30   Spurious ]⊺ ϵ1,...,ϵ 30 iid ∼N(0,1). We specify a ﬁxed mask over the inputs prior to training, where the ﬁrst Kinputs remain unmasked, and the remainder is masked. We expect that, given multiple models with different (ﬁxed) masks over the inputs, the proposed objective will be able to identify the correct one — i.e., the one that keeps only the informative features. We train multiple fully connected neural networks (MLPs) on a training set of 1000 examples using our method and compare the ﬁnal values of the LML objective. The results are shown in Figure 2a. LML correctly identiﬁes 15 input features as the optimum, and correlates well with test accuracy and log-likelihood. Training loss and training accuracy, on the other hand, cannot alone disambiguate whether to use 15 or more input features. Differentiable input selection We further show that we can learn the correct mask over the inputs in a differentiable manner using our method during a single training run. We parameterize a learnable mask over the inputs with a concrete Bernoulli distribution (Maddison et al., 2016) and treat the parameters of the mask distribution as a hyperparameter. We optimize them with respect to the proposed objective using our method. The evolution of the learned mask during training is shown in Figure 2b, where we see that we can correctly identify the ﬁrst 15 informative features. 6Published as a conference paper at ICLR 2023 Learning invariances through data-augmentations Following previous literature on learning soft invariances through learning data augmentations (Nabarro et al., 2022; van der Wilk et al., 2018; Benton et al., 2020; Schw ¨obel et al., 2021; Immer et al., 2022), we show that we can learn useful afﬁne image augmentations, resulting in gains in test accuracy. We specify afﬁne data augmentations as part of a probabilistic model as done by van der Wilk et al. (2018), averaging over multiple data augmentation samples during training and inference. This allows us to treat the data-augmentation distribution as a model hyperparameter rather than a training hyperparameter. For datasets, we consider MNIST, CIFAR10, TinyImagenet along with rotCIFAR10 and rotTinyImagenet, variants where the datapoints are randomly rotated at the beginning of training by angles sampled uniformly from [−π,π] (Immer et al., 2022). Experimental setup details are provided in Appendix I. For the CIFAR10 and rotCIFAR10 datasets, we consider as baselines standard training with no augmentations, Augerino (Benton et al., 2020) and Differentiable Laplace (Immer et al., 2022). Following Immer et al. (2022), we use ﬁxupResNets (Zhang et al., 2019) for the architectures. The results can be seen in Table 1. There, we observe that partitioned networks outperform all baselines in the case of CIFAR10 for both ResNet variants we consider. On RotCIFAR10, we observe that partitioned networks outperform the baseline and Augerino, but it is slightly outperformed by Differentiable Laplace, which optimizes additional prior hyperparameters. To demonstrate the scalability of partitioned networks, for the (rot)TinyImagenet experiments we consider a ResNet-50 architecture with GroupNorm(2). In Table 1 we observe that in both cases, partitioned networks learn invariances successfully and improve upon the baseline. Relative to Augerino, we observe that partitioned networks either improve (TinyImagenet) or are similar (rotTinyImagenet). Table 1: Test accuracy with learning afﬁne augmentations on (rot)CIFAR10 and (rot)TinyImagenet. Method Dataset Architecture Baseline Augerino Diff. Laplace Partitioned RotCIFAR10 ﬁxupResNet-8 54.2±0.4 75.4±0.2 79.5±0.6 79.1±0.0 CIFAR10 ﬁxupResNet-8 74.1±0.5 79.0±1.0 84.2±0.8 86.1±0.4 ﬁxupResNet-14 79.5±0.3 83.0±0.1 88.1±0.2 89.1±0.8 RotTinyImagenet ResNet-50 31.5±0.6 44.5±0.2 OOM7 43.9±0.3 TinyImagenet ResNet-50 44.2±0.5 41.1±0.2 OOM 48.6±0.0 Imbuing a model with useful invariances is particularly useful in the low-data regime, due to better data efﬁciency. To show that, we perform experiments where we artiﬁcially reduce the size of the training dataset. The results can be seen in Figure 3. We see that by learning augmentations with partitioned networks, we can drastically improve performance in the low-data regime upon a baseline that does not learn augmentations, while performing favorably against prior works in most cases. On MNIST, our method outperforms the last-layer marginal-likelihood method (last-layer ML) by Schw¨obel et al. (2021) in the large data regime but underperforms in the low-data regime. That is likely to be expected, as their work ﬁts a Gaussian Process (GP) at the last layer (Wilson et al., 2016), which is better tailored for the low-data regime and results into a more ﬂexible model (due to the GP corresponding to an additional, inﬁnite width, layer). Since the MNIST-CNN is sufﬁciently small to ﬁt multiple networks into memory, we also compare to a variant of our method where, instead of partitioning a single network, we train Cdifferent networks where network kis ﬁt on data D1:k. This serves as an upper bound on the performance of the partitioned networks. We see that by partitioning a single network, we can achieve almost equivalent accuracy. On CIFAR10, partitioned networks outperform all other works on all data sizes we considered. On RotCIFAR10, partitioned networks perform again favourably, but they are marginally outperformed by differentiable Laplace in the low-data regime. Compared to partitioned networks where we only optimize augmentations, differentiable Laplace also optimizes the precision of a Gaussian prior over the weights, which better combats overﬁtting in the low-data regime. On both the TinyImagenet and rotTinyImagenet experiments we observe that partitioned networks either outperform or are similar to the baselines on all data sizes considered. 7Out of memory error on a 32GB Nvidia V100. 7Published as a conference paper at ICLR 2023 5000 20000 60000 Dataset Size 0.98 0.99T est Accuracy Baseline Last-layer ML Augerino Diff. Laplace Partitioned (Ens.) Partitioned (a) MNIST 0.25 0.50 0.75 1 5 10 20 50 Dataset Size (x1000) 0.25 0.50 0.75  (b) (rot)CIFAR10 0.25 0.50 10 50 100 Dataset Size (x1000) 0.2 0.4  (c) (rot)TinyImagenet Figure 3: Learning afﬁne data augmentations on subsets of data. (b) uses a ﬁxupResNet-8 architecture whereas (c) a ResNet-50 architecture. (b,c) Top: normal dataset, bottom: rotated dataset. Comparisons to traditional training / validation split We further perform comparisons between partitioned networks and the more traditional training/validation split (denoted as validation set optimization) with additional ﬁnetuning to the task of learning data augmentations. This is realized as follows; we partition 20kCIFAR10 examples into training and validation data of speciﬁc proportions. We then either train a partitioned network (along with the hyperparameters on LML) on these two chunks of data or train a standard network on the training set while using the validation set loss to obtain gradients for the data augmentation hyperparameters. For the validation set optimization baseline, once the hyperparameters are optimized, the resulting network is ﬁnetuned on the whole dataset for 20 epochs. The results for varying chunk proportions are provided in Table 2. Table 2: Learning afﬁne augmentations with ﬁxupResNet-14 on subset of CIFAR-10 (20kexamples). NaN denotes that a run crashed. Chunk Proportions Method [0.3,0.7] [0 .5,0.5] [0 .7,0.3] [0 .8,0.2] [0 .9,0.1] Partitioned 82.9%±0.3 83.0%±0.01 83.7%±0.2 84.0%±0.6 84.6%±0.05 Validation set optim. NaN 78.9%±0.04 81.5%±0.2 82.6%±0.1 83.4%±0.1 +Finetune NaN 81.3%±0.09 82.5%±0.2 83.5%±0.1 83.8%±0.3 Table 3: Learning a feature extractor (ﬁrst 2 out of 3 stages of a Wide ResNet-20) as a hyperparameter on CIFAR10. Method Chunk Proportions Test accuracy Validation set optim. [0.9,0.1] 59 .6%±0.6 Partitioned [0.1,0.8,0.1] 87.3%±0.8 We can see that partitioned net- works (that do not employ ad- ditional ﬁnetuning) outperform validation set optimization with ﬁnetuning in all settings we tried. The gap does get smaller when we move to the more tra- ditional 90/10 splits for train- ing/validation: a 10% proportion for validation data is enough to optimize a handful of hyper- parameters (just 6 scalars). To corroborate this claim, we set up an additional experiment; we use a Wide ResNet-20 on the full CIFAR10 dataset, where the ﬁrst two out of the three stages (13 convolution layers) are considered as hyperparameters. The results for this setting can be seen in Table 3. We see that 10% validation data are not enough, and the validation set optimization baseline performs poorly. This is in contrast to partitioned networks, where with three chunks, we can learn all of these hyperparameters successfully. Note that, compared to Augerino, applying partitioned networks to this setting is straightforward. To apply Augerino, one would have to come up with a metric that can be used to regularize the feature extractor towards “higher invariance”. Partitioned networks for federated learning We consider federated learning (FL) (McMahan et al., 2017), a setting where data is distributed across many clients. In this setting, there are system properties that make hyperparameter optimization especially challenging (Wang et al., 2021). More speciﬁcally, obtaining a validation set and performing multiple training runs with different 8Published as a conference paper at ICLR 2023 hyperparameter settings might not be possible due to the additional communication and computation costs, and transient client availability (clients join and leave the training process at any time). Optimizing hyperparameters together with the model parameters in a single run is therefore especially beneﬁcial (Wang et al., 2021), and partitioned networks are a good ﬁt for FL. We extend our centralized experimental setup to FL by splitting all N clients into Cnon-overlapping chunks, such that each chunk is understood as the union of all clients’ data shards that belong to that chunk. During federated training, a client belonging to chunk ksequentially optimizes partitions wk:C through sub-networks w(k:C) s and computes a gradient wrt. the hyperparameters ψ. Note that partitions w1:k remain unchanged and do not need to be communicated back to the server. This reduction in upload costs is a welcome property for FL, where upload costs can bottleneck system design. The server receives the (hyper-) parameter updates, averages them, and applies the result as a “gradient” to the server-side model in the traditional federated manner (Reddi et al., 2020). For partitioned networks, the hyperparameters that we optimize are the data augmentation parameters and, since we also include dropout in these architectures, the dropout rates (with the concrete relaxation from Maddison et al. (2016)). As a baseline, we consider the standard federated training without learning hyperparameters (denoted as FedAvg) as well as learning the augmentation parameters with Augerino Benton et al. (2020). Please see Appendix J for a detailed explanation of our FL setup. Table 4 summarizes our results using different sub-sets and variations of MNIST and CIFAR10, where we also included rotMNIST Larochelle et al. (2007) as another dataset. We can see that partitioned networks allow training models that generalize better than both FedAvg and FedAvg with Augerino, at reduced communication costs. Especially when the true data-generating process and underlying source of non-i.i.d.-ness are explicitly accounted for — here in the form of rotation — the beneﬁts of learning the augmentations with partitioned networks become apparent. For example, we observe that on the rotated datasets, partitioned networks learn to correctly increase the rotation angle. Table 4: Validation accuracy averaged over the last10 evaluations, each 10 rounds apart; standard- error is computed across 4 random seeds. All datasets are adapted to the federated setting and are synthetically split to be non-i.i.d. sampled as described in Appendix J.2. Dataset & size ↑MNIST ↑RotMNIST ↓Upload Method 1.25k 5k 50k 1.25k 5k 50k [%] FedAvg 95.4%±0.1 97.4%±0.1 99.0%±0.1 80.5%±0.0 90.4%±0.5 96.8%±0.1 100 FedAvg + Augerino 94.2%±0.5 96.4%±0.1 99.1%±0.0 79.5%±0.3 89.0%±2.0 95.3%±0.2 100 FedAvg + Partitioned97.0%±0.1 98.3%±0.0 99.2%±0.1 85.7%±0.9 93.5%±0.6 97.8%±0.1 77 ↑CIFAR10 ↑RotCIFAR10 ↓Upload 1.25k 5k 45k 1.25k 5k 45k [%] FedAvg 50.2%±0.4 64.5%±0.3 79.2%±0.7 35.6%±0.3 45.2%±0.1 53.9%±1.1 100 FedAvg + Augerino 49.9%±0.8 65.0%±0.2 79.9%±0.4 36.1%±0.2 45.0%±0.2 56.4%±0.7 100 FedAvg + Partitioned50.8%±1.0 64.8%±0.4 81.5%±0.5 37.1%±0.2 45.3%±0.3 60.6%±0.2 91 6 D ISCUSSION We propose partitioned networks as a new method for hyperparameter optimization inspired by the marginal likelihood objective. It provides a general and scalable solution to ﬁnding hyperparameters in a single training run without requiring access to a validation set while introducing less additional overhead to the training task than existing approaches. We showed that partitioned networks are applicable on a wide range of tasks; they can identify the correct model on illustrative toy examples, they can learn data augmentations in a way that improves data efﬁciency, they can optimize general feature extractors as hyperparameters and they can also optimize dropout rates. In the federated setting, partitioned networks allow us to overcome practical challenges, reduce the communication overhead and obtain better models. The notion of partitioned networks we propose in this work is novel to the literature and an orthogonal approach to many existing hyperparameter tuning algorithms. Like any other method, partitioned networks come with their own limitations, e.g., needing a partitioning strategy. We expand upon them in appendix H. We hope to see our method successfully reducing the need to perform hyperparameter search through repeated training and thereby contribute to the community’s effort to reduce its carbon footprint. 9Published as a conference paper at ICLR 2023 REFERENCES Gregory Benton, Marc Finzi, and Andrew G Wilson. Augerino, github, com- mit=fd542eb90ac6b1c0959156c1f6ad2ba8719d8572. https://github.com/g-benton/ learning-invariances/. (on page 18) Gregory Benton, Marc Finzi, Pavel Izmailov, and Andrew G Wilson. Learning invariances in neural networks from training data. Advances in neural information processing systems, 33:17605–17616, 2020. (on page 2, 5, 7, 9, 16, 18, 20, 24, 25) Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment: Learning augmentation policies from data. arXiv preprint arXiv:1805.09501, 2018. (on page 5) Kamal Dys. Cifar10 resnet: 90+% accuracy;less than 5 min. https://www.kaggle.com/code/ kmldas/cifar10-resnet-90-accuracy-less-than-5-min . Accessed: 2022-09- 17. (on page 26) Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural architecture search: A survey. The Journal of Machine Learning Research, 20(1):1997–2017, 2019. (on page 5) Edwin Fong and Chris C Holmes. On the marginal likelihood and cross-validation. Biometrika, 107 (2):489–496, 2020. (on page 2) Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. arXiv preprint arXiv:1803.03635, 2018. (on page 4) Yarin Gal, Jiri Hron, and Alex Kendall. Concrete dropout.Advances in neural information processing systems, 30, 2017. (on page 2) Pascal Germain, Francis Bach, Alexandre Lacoste, and Simon Lacoste-Julien. Pac-bayesian theory meets bayesian inference. Advances in Neural Information Processing Systems, 29, 2016. (on page 2) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectiﬁers: Surpassing human-level performance on imagenet classiﬁcation. In Proceedings of the IEEE international conference on computer vision, pages 1026–1034, 2015. (on page 23) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In European conference on computer vision , pages 630–645. Springer, 2016. (on page 23) Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016. (on page 23) Alexander Immer and Tycho F. A. van der Ouderaa. Learning invariances with laplace ap- proximations (lila), github, commit=c0c4a09a109ed2f55e887def7d854b8a3a2330ef. https: //github.com/tychovdo/lila. (on page 17) Alexander Immer, Matthias Bauer, Vincent Fortuin, Gunnar R¨atsch, and Khan Mohammad Emtiyaz. Scalable marginal likelihood estimation for model selection in deep learning. In International Conference on Machine Learning, pages 4563–4573. PMLR, 2021. (on page 2, 5, 6, 24) Alexander Immer, Tycho F. A. van der Ouderaa, Gunnar R¨atsch, Vincent Fortuin, and Mark van der Wilk. Invariance learning in deep neural networks with differentiable laplace approximations, 2022. URL https://arxiv.org/abs/2202.10638. (on page 2, 5, 7, 15, 16, 17, 18, 22, 23, 24, 25) Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning, pages 448–456. PMLR, 2015. (on page 23) Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. (on page 4) 10Published as a conference paper at ICLR 2023 Mikhail Khodak, Renbo Tu, Tian Li, Liam Li, Maria-Florina F Balcan, Virginia Smith, and Ameet Talwalkar. Federated hyperparameter tuning: Challenges, baselines, and connections to weight- sharing. Advances in Neural Information Processing Systems, 34:19184–19197, 2021. (on page 5, 6) Hugo Larochelle, Dumitru Erhan, Aaron Courville, James Bergstra, and Yoshua Bengio. An empirical evaluation of deep architectures on problems with many factors of variation. In Proceedings of the 24th international conference on Machine learning, pages 473–480, 2007. (on page 9) Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge. CS 231N, 7(7):3, 2015. (on page 24) Yonggang Li, Guosheng Hu, Yongtao Wang, Timothy Hospedales, Neil M Robertson, and Yongxin Yang. Dada: Differentiable automatic data augmentation. arXiv preprint arXiv:2003.03780, 2020. (on page 5) Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. arXiv preprint arXiv:1806.09055, 2018. (on page 2, 5) Jonathan Lorraine, Paul Vicol, and David Duvenaud. Optimizing millions of hyperparameters by implicit differentiation. CoRR, abs/1911.02590, 2019. URL http://arxiv.org/abs/ 1911.02590. (on page 1, 2, 5) Sanae Lotﬁ, Pavel Izmailov, Gregory Benton, Micah Goldblum, and Andrew Gordon Wilson. Bayesian model selection, the marginal likelihood, and generalization. In Kamalika Chaud- huri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine Learning , volume 162 of Pro- ceedings of Machine Learning Research , pages 14223–14247. PMLR, 17–23 Jul 2022. URL https://proceedings.mlr.press/v162/lotfi22a.html. (on page 4) Clare Lyle, Lisa Schut, Robin Ru, Yarin Gal, and Mark van der Wilk. A bayesian perspective on training speed and model selection. Advances in Neural Information Processing Systems , 33: 10396–10408, 2020. (on page 2, 3, 5, 6) David JC MacKay. Information theory, inference and learning algorithms. Cambridge university press, 2003. (on page 2) Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous relaxation of discrete random variables. arXiv preprint arXiv:1611.00712, 2016. (on page 6, 9, 26, 27) David A McAllester. Some pac-bayesian theorems. In Proceedings of the eleventh annual conference on Computational learning theory, pages 230–234, 1998. (on page 2) Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efﬁcient learning of deep networks from decentralized data. In Artiﬁcial intelli- gence and statistics, pages 1273–1282. PMLR, 2017. (on page 2, 8) Shakir Mohamed, Mihaela Rosca, Michael Figurnov, and Andriy Mnih. Monte carlo gradient estimation in machine learning. J. Mach. Learn. Res., 21(132):1–62, 2020. (on page 6) Seth Nabarro, Stoil Ganev, Adri`a Garriga-Alonso, Vincent Fortuin, Mark van der Wilk, and Laurence Aitchison. Data augmentation in bayesian neural networks and the cold posterior effect. In Uncertainty in Artiﬁcial Intelligence, pages 1434–1444. PMLR, 2022. (on page 5, 7) Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and R. Gar- nett, editors, Advances in Neural Information Processing Systems 32, pages 8024–8035. Curran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/9015-pytorch-an- imperative-style-high-performance-deep-learning-library .pdf. (on page 22) 11Published as a conference paper at ICLR 2023 Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Kone ˇcn`y, Sanjiv Kumar, and H Brendan McMahan. Adaptive federated optimization. arXiv preprint arXiv:2003.00295, 2020. (on page 9, 26, 27) Robin Ru, Clare Lyle, Lisa Schut, Miroslav Fil, Mark van der Wilk, and Yarin Gal. Speedy performance estimation for neural architecture search. Advances in Neural Information Processing Systems, 34:4079–4092, 2021. (on page 3) Akiyoshi Sannai, Masaaki Imaizumi, and Makoto Kawano. Improved generalization bounds of group invariant/equivariant deep networks via quotient feature spaces. In Uncertainty in Artiﬁcial Intelligence, pages 771–780. PMLR, 2021. (on page 3) Pola Schw¨obel, Martin Jørgensen, Sebastian W. Ober, and Mark van der Wilk. Last layer marginal likelihood for invariance learning, 2021. URL https://arxiv.org/abs/2106.07512. (on page 2, 5, 7, 15, 16, 23, 24, 26, 27) Jonas Seng, Pooja Prasad, Devendra Singh Dhami, and Kristian Kersting. Hanf: Hyperparameter and neural architecture search in federated learning. arXiv preprint arXiv:2206.12342, 2022. (on page 6) Jure Sokolic, Raja Giryes, Guillermo Sapiro, and Miguel Rodrigues. Generalization error of invariant classiﬁers. In Artiﬁcial Intelligence and Statistics, pages 1094–1103. PMLR, 2017. (on page 3) Mark van der Wilk, Matthias Bauer, ST John, and James Hensman. Learning invariances using the marginal likelihood. Advances in Neural Information Processing Systems, 31, 2018. (on page 2, 5, 7, 16) Bastiaan S Veeling, Jasper Linmans, Jim Winkens, Taco Cohen, and Max Welling. Rotation equiv- ariant cnns for digital pathology. In International Conference on Medical image computing and computer-assisted intervention, pages 210–218. Springer, 2018. (on page 15) Jianyu Wang, Zachary Charles, Zheng Xu, Gauri Joshi, H Brendan McMahan, Maruan Al-Shedivat, Galen Andrew, Salman Avestimehr, Katharine Daly, Deepesh Data, et al. A ﬁeld guide to federated optimization. arXiv preprint arXiv:2107.06917, 2021. (on page 5, 8, 9) Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3):229–256, 1992. (on page 6) Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, and Eric P Xing. Deep kernel learning. In Artiﬁcial intelligence and statistics, pages 370–378. PMLR, 2016. (on page 7) Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European conference on computer vision (ECCV), pages 3–19, 2018. (on page 23, 26) Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016. (on page 23) Hongyi Zhang, Yann N Dauphin, and Tengyu Ma. Fixup initialization: Residual learning without normalization. arXiv preprint arXiv:1901.09321, 2019. (on page 7, 18, 23) Yi Zhou, Parikshit Ram, Theodoros Salonidis, Nathalie Baracaldo, Horst Samulowitz, and Heiko Ludwig. Flora: Single-shot hyper-parameter optimization for federated learning. arXiv preprint arXiv:2112.08524, 2021. (on page 6) 12Published as a conference paper at ICLR 2023 A LML IS A LOWER -BOUND TO THE MARGINAL LIKELIHOOD In this section, we show that the objective in equation 2 is a lower-bound on the marginal likelihood, under a mild assumption on each approximate posterior qk(w). The aim is to approximate: log p(D|ψ) = C∑ k=1 log p(Dk|D1:k−1,ψ) (5) Our partitioned approximation is given by: C∑ k=1 Eqk−1(w) [log p(Dk|w,ψ)] (6) We can get the equation for the gap between quantities in 5 and 6: gap = C∑ k=1 log p(Dk|D1:k−1,ψ) − C∑ k=1 Eqk−1(w) [log p(Dk|w,ψ)] (7) = C∑ k=1 Eqk−1(w) [log p(Dk|D1:k−1,ψ) −log p(Dk|w,ψ)] (8) = C∑ k=1 Eqk−1(w) [ log p(Dk|D1:k−1,ψ) p(Dk|w,ψ) ] (9) = C∑ k=1 Eqk−1(w)  log p(w,Dk|D1:k−1)    p(w|D1:k,ψ)p(Dk|D1:k−1,ψ)p(w|D1:k−1,ψ) p(w|D1:k,ψ)p(Dk|w,ψ)p(w|D1:k−1,ψ)   p(w,Dk|D1:k−1)   (10) = C∑ k=1 Eqk−1(w) [ log p(w|D1:k−1,ψ) p(w|D1:k,ψ) ] (11) = C∑ k=1 DKL [qk−1(w)∥p(w|D1:k,ψ)] −DKL [qk−1(w)∥p(w|D1:k−1,ψ)] (12) We now make two assumptions • DKL [qk−1(w)∥p(w|D1:k,ψ)] ≥DKL [qk(w)∥p(w|D1:k,ψ)]. This is motivated from the fact that qk(w) is trained on all data chunks D1:k so it is expected to be a better approxima- tion to the posterior p(w|D1:k), compared to qk−1(w) which is only trained on D1:k−1. • DKL [qC−1(w)∥p(w|D1:C,ψ)] ≥DKL [q0(w)∥p(w)]. Since we are free to choose the approximate posterior before seeing any data — q0(w)—, we can set it to be equal to the prior p(w) which, together with the positivity of the KL divergence, trivially satisﬁes this assumption. Therefore, by rearranging Eq. 12 and using our two assumptions we have that the gap is positive gap =−DKL [q0(w)∥p(w)] +DKL [qC−1(w)∥p(w|D1:C,ψ)] + C∑ k=1 DKL [qk−1(w)∥p(w|D1:k,ψ)] −DKL [qk(w)∥p(w|D1:k,ψ)] ≥0, (13) and our approximation is a lower bound to the marginal likelihood, i.e., log p(D|ψ) ≥ C∑ k=1 Eqk−1(w) [log p(Dk|w,ψ)] . (14) 13Published as a conference paper at ICLR 2023 B P ARTITIONED NETWORKS AS A SPECIFIC APPROXIMATION TO THE MARGINAL LIKELIHOOD In this section of the appendix, we show that the partitioned neural networks we presented in the paper are a particular instance of the approximation to the marginal likelihood shown in equation 2. Consider a dataset Dcomprised of C shards, i.e. D= (D1,..., DC), along with a model, e.g., a neural network, with parameters w ∈RDw, a prior p(w) = ∏Dw j=1 N(wj|0,λ) and a likelihood p(D|w,ψ) with hyperparameters ψ. Assuming a sequence over the dataset chunks, we can write out the true marginal likelihood as log p(D|ψ) = ∑ k log p(Dk|D1:k−1,ψ) = ∑ k log Ep(w|D1:k−1,ψ) [p(Dk|w,ψ)] (15) ≥ ∑ k Ep(w|D1:k−1,ψ) [log p(Dk|w,ψ)] . (16) Since the true posteriors p(w|D1:j,ψ) for j ∈{1,...,C }are intractable, we can use variational inference to approximate them with qφj(w) for j ∈{1,...,C }, with φj being the to-be-optimized parameters of the j’th variational approximation. Based on the result from Appendix A, whenqφj(w) are optimized to match the respective posteriors p(w|D1:j,ψ), we can use them to approximate the marginal likelihood as log p(D|ψ) ≥ ∑ k Eqφk−1 (w) [log p(Dk|w,ψ)] . (17) Partitioned networks correspond to a speciﬁc choice for the sequence of approximating distribution families qφk(w). Speciﬁcally, we partition the parameter space w into Cchunks, i.e., wk ∈RDwk, such that ∑ kDwk = Dw, and we associate each parameter chunk wk with a data shard Dk. Let rφk(wk) be base variational approximations over wk with parameters φk. Each approximate distribution qφk(w) is then deﬁned in terms of these base approximations, i.e., qφk(w) =   k−1∏ j=1 rφj(wj)  rφk(wk) ( K∏ m=k+1 r0(wm) ) (18) where r0(·) is some base distribution with no free parameters. In accordance with the assumptions in appendix A, we can then ﬁt each qφk(w) by minimising the KL-divergence to p(w|D1:k,ψ) – the posterior after seeing kchunks: DKL [qφk(w)∥p(w|D1:k,ψ)] =−Eqφk(w)[log p(D1:k|w,ψ)] +DKL [qφk(w)∥p(w)] + logp(D1:k|ψ) (19) (20) Finding the optimum with respect to φk: arg min φk DKL [qφk(w)∥p(w|D1:k,ψ)] = (21) = arg min φk −Eqφk(w)[log p(D1:k|w,ψ)] +DKL [qφk(w)∥p(w)] (22) = arg min φk −Eqφk(w)[log p(D1:k|w,ψ)] + DKL     k−1∏ j=1 rφj(wj)  rφk(wk) ( K∏ m=k+1 r0(wm) ) ∥ K∏ i p(wi)   (23) = arg min φk −Eqφk(w)[log p(D1:k|w,ψ)] +DKL [rφk(wk)∥p(wk)] . (24) We can now obtain partitioned networks by assuming that rφk(wk) = N(wk|φk,νI) for k ∈ {1,...,C }, r0(w) = N(w|ˆw,νI), with ˆw being the parameters at initialization (i.e., before we 14Published as a conference paper at ICLR 2023 update them on data) and taking ν →0, i.e., in machine-precision, the weights are deterministic. As noted in Section I.1, we scale the weight-decay regularizer forφk (whenever used) differently for each partition k, such that it can be interpreted as regularization towards a prior. In the experiments where we do not regularize φk according to p(wk) when we optimize them, this implicitly corresponds to λ→∞ (i.e. the limiting behaviour when the variance of p(w) goes to inﬁnity), which makes the contribution of the regularizer negligible. C P ARTITIONING SCHEMES There are several ways in which we could aim to partition the weights of a neural network. Throughout the experimental section 5, we partition the weights by assigning a ﬁxed proportion of weights in each layer to a given partition at random. We call this approach random weight partitioning. We also experimented with other partitioning schemes. For example, we tried assigning a ﬁxed proportion of a layer’s outputs (e.g., channels in a convolution layer) to each partition. All weights in a given layer that a speciﬁc output depends on would then be assigned to that partition. We call this approach node partitioning. Both approaches are illustrated in Figure 4. One beneﬁt of the node partitioning scheme is that it makes it possible to update multiple partitions with a single batch; This is because we can make a forward pass at each linear or convolutional layer with the full network parameters w, and, instead, mask the appropriate inputs and outputs to the layer to retrieve an equivalent computation to that with w(k) s . The gradients also need to be masked on the backward pass adequately. No such simpliﬁcation is possible with the random weight partitioning scheme; if we were to compute a backward pass for a single batch of examples using different subnetworks for each example, the memory overhead would grow linearly with the number of subnetworks used. In initial experiments, we found both random weight partitioning and node partitioning performed similarly. In the experimental section 5, we focused on the former, as it’s easier to reason about with relation to e.g., dropout. Throughout this work, partitioning happens prior to initiating training, and remains ﬁxed throughout. It might also be possible to partition the network parameters dynamically during training, which we leave for future work.   w11 w12 w13 w14 w15 w21 w22 w23 w24 w25 w31 w32 w33 w34 w35 w41 w42 w43 w44 w45 w51 w52 w53 w54 w55 w61 w62 w63 w64 w65   (a) Random weight partitioned In node assignment    Out node assignment      w11 w12 w13 w14 w15 w21 w22 w23 w24 w25 w31 w32 w33 w34 w35 w41 w42 w43 w44 w45 w51 w52 w53 w54 w55 w61 w62 w63 w64 w65   (b) Node partitioned Figure 4: Figures showing how the weights within a single weight matrix W ∈R6×5 for a linear layer would be partitioned. D S CALABILITY In the paper, we claim that our method is scalable compared to Schw¨obel et al. (2021) and Immer et al. (2022). What constraints the scalability of the mentioned prior works, however, is different. For the Last Layer Marginal Likelihood, although the approach works on small datasets such as PCAM (Veeling et al., 2018) and MNIST, the authors report that they were unable to learn invariances 15Published as a conference paper at ICLR 2023 on larger datasets such as CIFAR10. In (Schw¨obel et al., 2021, section 7), they explore the issue of scalability in more detail, and showcase that last layer marginal likelihood is insufﬁcient. Differentiable Laplace performs well, even on more complex datasets, such as CIFAR10. Their scalability, however, is limited by the computational and memory complexity of their method, which we go into in more detail in the section below. D.1 C OMPLEXITY ANALYSIS First, we consider the scalability of our algorithm in terms of computational and memory complexity. In particular, we show that our method scales much more favourably compared to Differentiable Laplace (Immer et al., 2022). We present our analysis for a feed-forward model of depth L, with layer widths D8. In order to directly compare to Immer et al. (2022) and Benton et al. (2020), we consider the complexities in the invariance learning setup (Benton et al., 2020; van der Wilk et al., 2018) withSaugmentation samples. In other experiments, hyperparameter optimization setups, S can be taken to be 1. The notation is summarized in Table 5. N Number of datapoints in dataset D NB Batch size S Number of augmentation samples9 C Output size (number of classes) D Feedforward network layer widths L Feedforward network depth P Number of parameters (s.t. O(P) =O(LD2 + DC)) Table 5: Notation for complexity analysis. We consider the computational and memory costs of 1) obtaining a gradient with respect to the parameters 2) obtaining a gradient with respect to the hyperparameters, and 3) computing the value of the model/hyperparameter selection objective for each method. All analysis assumes computation on a Monte-Carlo estimate of the objective on a single batch of data. In Tables 6 and 7, we assume that C <D, and hence, for the clarity of comparison, sometimes fold a factor depending Cinto a factor depending on Dif it’s clearly smaller. This hiding of the factors was only done for Differentiable Laplace, which is the worst scaling method. D.1.1 C OMPUTATIONAL COMPLEXITY Param. Backward Hyperparam. Backward Hyperparam. Objective Partitioned O(NBPS) O(NBPS) O(NBPS) Augerino O(NBPS) O(NBPS) O(NBPS) Diff. Laplace O(NBPS) O(NBPS+NCP +NCDLS + LD3) O(NPS + NCP +NCDLS + LD3) Table 6: Computational Complexities. The two terms highlighted for Augerino can be computed in a single backward pass. For Differentiable Laplace, the terms in blue can be amortised over multiple hyperparameter backward passes. That is why, in their method, they propose updating the hyperparameters once every epoch on (possibly) multiple batches of data, rather than once on every batch as is done with Partitioned Networks and Augerino. 8This is for the ease of comparison. Same upper bound complexities will hold for a network of variable sizes Dℓ for ℓ∈[L], where D= maxℓ Dℓ 9Only relevant for invariance learning. 16Published as a conference paper at ICLR 2023 D.1.2 M EMORY COMPLEXITY The memory complexities for Partitioned Networks, Augerino, and Differentiable Laplace are shown in Table 7. Crucially, the memory required to update the hyperparameters for Differentiable Laplace scales as O(NBSLD2 + P), with a term depending on the square of the network widths. This can become prohibitively expensive for larger models, and is likely the reason why their paper only considers experiments on architectures with widths up to a maximum of 256. Param. Backward Hyperparam. Backward Hyperparam. Objective Partitioned O(NBSLD+ P) O(NBSLD+ P) O(NBSD+ P) Augerino O(NBSLD+ P) O(NBSLD+ P) O(NBSD+ P) Diff. Laplace O(NBSLD+ P) O(NBSLD2 + P) O(NBSLD2 + P) Table 7: Memory Complexities. Differences are highlighted in red. D.2 P RACTICAL SCALABILITY A complexity analysis in big- Onotation as provided by us in the previous sections allows to understand scalability in the limit, but constant terms that manifest in practice are still of interest. In this section we aim present real timing measurements for our method in comparison to Augerino and Differential Laplace, and elaborate on what overhead might be expected with respect to standard neural network training. The empirical timings measurements on an NVIDIA RTX 3080-10GB GPU are shown in Table 8. We used a batch-size of 250, 200 for the MNIST and CIFAR10 experiments respectively, and 20 augmentation samples, just like in our main experiments in Table 1 and Figure 3. As can be seen, the overhead from using a partitioned network is fairly negligible compared to a standard forward and backward pass. The one difference compared to Augerino is, however, the fact that a separate forward-backward pass needs to be made to update the hyperparameters and regular parameters. This necessity is something that can be side-stepped with alternative partitioning schemes, as preliminarily mentioned in appendix C, and is an interesting direction for future research. MNIST CIFAR10 Method CNN ﬁxupResNet-8 ﬁxupResNet-14 Augerino ×1 ×1 ×1 Diff. Laplace† Param. ×1 ×1 ×1 Hyperparam. ×2015.6 ×18.2 - Partitioned Param. ×1.08 ×1.17 ×1.21 Hyperparam. ×1.08 ×1.08 ×1.09 Table 8: Relative empirical time increase with respect to a regular parameter update during standard training. †The timing multipliers with respect to the baseline for ﬁxupResNet-8 are taken from the timings reported in (Immer et al., 2022, Appendix D.4). On the ResNet-14, we get an out-of- memory error during the hyperparam. update step with Differentiable Laplace on the NVIDIA RTX 3080-10GB GPU when running with the ofﬁcial codebase (Immer and van der Ouderaa). Memory Overhead Our proposed method’s memory consumption scales in the same way as Augerino or vanilla neural network training. There is a minor constant memory overhead due to having to store the assignment of weights to partitions. In general, only log Cbits per parameter are necessary to store the partition assignments, whereCis the number of chunks. In our implementation, we only consider C <28, and hence store the assignments in byte tensors. This means that the partitioned models require extra 25% memory for storing the parameters (when using 32bit ﬂoats to represent the parameters). 17Published as a conference paper at ICLR 2023 If the “default” weight values (i.e. those denoted ˆwi in Figure 1) are non-zero, there is an additional overhead to storing those as well, which doubles the memory required to store the parameters. We observed there was no difference in performance when setting default weight values to 0 in architectures in which normalisation layers are used (i.e. most modern architectures). As such, we would in general recommend to set the default weight values to 0. However, we found setting default values to the initialised values to be necessary for stability of training deep normalisation-free architectures such as the ﬁxup architectures (Zhang et al., 2019) we used to compare with Differentiable Laplace. As their method is not compatible with BatchNorm, we used these architectures in our experiments, and hence used non-zero default values. Lastly, if the default weight values are set to the (random) initialisation values, it is possible to write a cleverer implementation in which only the random seeds are stored in memory, and the default values are re-generated every time they are need in a forward and a backward pass. This would make the memory overhead from storing the default values negligible. E N OTE ON AUGERINO In replicating Augerino (Benton et al., 2020) within our code-base and experimenting with the implementation, we discovered a pathological behaviour that is partly mirrored by the authors of Immer et al. (2022). In particular, note that the loss function (Benton et al., 2020, Equation (5)) proposed by the authors is problematic in the sense that for any regularization strength λ> 0, the optimal loss value is negative inﬁnity since the regularization term (negative L2-norm) is unbounded. In our experiments we observe that for a sufﬁciently-large value of λand after a sufﬁcient number of iterations, this behaviour indeed appears and training diverges. In practice, using Augerino therefore necessitates either careful tuning of λ, clipping the regularisation term (a method that introduces yet another hyperparameter), or other techniques such as early stopping. In the open-source repository for the submission (Benton et al.), it can be seen that on many experiments the authors use a ”safe” variant of the objective, in which they clip the regulariser (without pass-through of the gradient) once the l∞-norm of any of the hyperparameters becomes larger than an arbitrary threshold. Without using this adjustment, we found that the Augerino experiments on MNIST crashed every time with hyperparameters diverging to inﬁnity. F S ENSITIVITY TO PARTITIONING F.1 S ENSITIVITY IN TERMS OF FINAL PERFORMANCE (a) 2 chunks [8,1] [4,1] [1,1] [1,4] [1,8] Param. Proportions [8,1] [4,1] [1,1] [1,4] [1,8]Chunk Proportions 0.990 0.992 0.994 0.996 T est Accuracy  (b) 3 chunks [8,4,1][3,2,1][1,1,1][1,2,3][1,4,8] Param. Proportions [8,4,1] [3,2,1] [1,1,1] [1,2,3] [1,4,8]Chunk Proportions 0.990 0.992 0.994 0.996 T est Accuracy  (c) 4 chunks [8,4,2,1][4,3,2,1][1,1,1,1][1,2,3,4][1,2,4,8] Param. Proportions [8,4,2,1] [4,3,2,1] [1,1,1,1] [1,2,3,4] [1,2,4,8]Chunk Proportions 0.990 0.992 0.994 0.996 T est Accuracy Figure 5: Learning afﬁne augmentations on MNIST with a CNN ﬁt on all data. x- and y−ticks denote the ratios of parameters/datapoints assigned to each partition/chunk respectively. Partitioned networks allow for learning hyperparameters in a single training run, however, they introduce an additional hyperparameter in doing so: the partitioning scheme. The practitioner needs to choose the number of chunks C, the relative proportions of data in each chunk, and the relative proportions of parameters assigned to each of the Cpartitions wk. We investigate the sensitivity to the partitioning scheme here. We show that our results are fairly robust to partitioning through a grid-search over parameter partitions and chunk proportions on the afﬁne augmentation learning task on MNIST with the CNN architecture we use throughout this work. 18Published as a conference paper at ICLR 2023 (a) 2 chunks [8,1] [4,1] [1,1] [1,4] [1,8] Param. Proportions [8,1] [4,1] [1,1] [1,4] [1,8]Chunk Proportions 0.987 0.988 0.989 0.990 T est Accuracy  (b) 3 chunks [8,4,1][3,2,1][1,1,1][1,2,3][1,4,8] Param. Proportions [8,4,1] [3,2,1] [1,1,1] [1,2,3] [1,4,8]Chunk Proportions 0.987 0.988 0.989 0.990 T est Accuracy  (c) 4 chunks [8,4,2,1][4,3,2,1][1,1,1,1][1,2,3,4][1,2,4,8] Param. Proportions [8,4,2,1] [4,3,2,1] [1,1,1,1] [1,2,3,4] [1,2,4,8]Chunk Proportions 0.987 0.988 0.989 0.990 T est Accuracy Figure 6: Learning afﬁne augmentations on RotMNIST with a CNN ﬁt on all data. x- and y−ticks denote the ratios of parameters/datapoints assigned to each partition/chunk respectively. Figure 5 and Figure 6 show the test accuracy for a choice of chunk and parameter proportions across two, three and four chunks. The proportions are to be read as un-normalized distributions; for example, chunk proportions set to [1,8] denotes that there are 8×as many datapoints assigned to the second compared to the ﬁrst. Each conﬁguration was run with 2 random seeds, and we report the mean across those runs in the ﬁgure. The same architecture used was the same as for the main MNIST experiments in section 5 (see Appendix I.4 for details). We observe that for various partition/dataset-chunking conﬁgurations, all models achieve fairly similar ﬁnal test accuracy. There is a trend for models with a lot of parameters assigned to later chunks, but with few datapoints assigned to later chunks, to perform worse. While these results show a high level of robustness against the choice of additional hyperparameters introduced by our method, these results do show an opportunity or necessity for choosing the right partitioning scheme in order to achieve optimal performance. F.2 S ENSITIVITY IN TERMS OF HYPERPARAMETERS FOUND To compare how the different partitioning schemes qualitatively impact the hyperparameters that the method identiﬁes, we also retrain vanilla models from scratch using the hyperparameter values found using partitioned networks. Namely, we take the ﬁnal value of the hyperparameters learned with partitioned networks with a given partitioning scheme, and plot the ﬁnal test set accuracy of a vanilla neural network model trained from scratch with those hyperparameters. The results are shown in Figures 7 and 8. (a) 2 chunks [8,1] [4,1] [1,1] [1,4] [1,8] Param. Proportions [8,1] [4,1] [1,1] [1,4] [1,8]Chunk Proportions 0.990 0.992 0.994 0.996 T est Accuracy  (b) 3 chunks [8,4,1][3,2,1][1,1,1][1,2,3][1,4,8] Param. Proportions [8,4,1] [3,2,1] [1,1,1] [1,2,3] [1,4,8]Chunk Proportions 0.990 0.992 0.994 0.996 T est Accuracy  (c) 4 chunks [8,4,2,1][4,3,2,1][1,1,1,1][1,2,3,4][1,2,4,8] Param. Proportions [8,4,2,1] [4,3,2,1] [1,1,1,1] [1,2,3,4] [1,2,4,8]Chunk Proportions 0.990 0.992 0.994 0.996 T est Accuracy Figure 7: Standard neural network trained onMNIST with a CNN ﬁt on all data, with hyperparameters found using partitioned networks with chunk and parameter proportions corresponding to those in Figure 5. x- and y−ticks denote the ratios of parameters/datapoints assigned to each partition/chunk respectively. G H OW GOOD ARE THE HYPERPARAMETERS FOUND ? Here we show that the hyperparameters found by partitioned networks are also a good set of hyperparameters for vanilla neural networks retrained from scratch. This section expands on the 19Published as a conference paper at ICLR 2023 (a) 2 chunks [8,1] [4,1] [1,1] [1,4] [1,8] Param. Proportions [8,1] [4,1] [1,1] [1,4] [1,8]Chunk Proportions 0.987 0.988 0.989 0.990 T est Accuracy  (b) 3 chunks [8,4,1][3,2,1][1,1,1][1,2,3][1,4,8] Param. Proportions [8,4,1] [3,2,1] [1,1,1] [1,2,3] [1,4,8]Chunk Proportions 0.987 0.988 0.989 0.990 T est Accuracy  (c) 4 chunks [8,4,2,1][4,3,2,1][1,1,1,1][1,2,3,4][1,2,4,8] Param. Proportions [8,4,2,1] [4,3,2,1] [1,1,1,1] [1,2,3,4] [1,2,4,8]Chunk Proportions 0.987 0.988 0.989 0.990 T est Accuracy Figure 8: Standard neural network trained on RotMNIST with a CNN ﬁt on all data, with hyper- parameters found using partitioned networks with chunk and parameter proportions corresponding to those in Figure 6. x- and y−ticks denote the ratios of parameters/datapoints assigned to each partition/chunk respectively. experiment in section F.2. To validate this claim, we conducted a fairly extensive hyperparameter search on the afﬁne augmentation learning task on RotMNIST; we trained 200 models by ﬁrst sampling a set of afﬁne augmentation parameters uniformly at random from a predeﬁned range 10, and then training a neural network model (that averages across augmentation samples at train and test time, as described in Benton et al. (2020)) with standard neural training with those hyperparameters ﬁxed throughout. In Figure 9, we plot the ﬁnal test-set performance of all the models trained with those hyperparameters sampled from a ﬁxed range. Alongside, we show the hyperparameters and test-set performance of the partitioned networks as they progress throughout training. The partitioned networks consistently achieve ﬁnal test-set performance as good as that of the best hyperparameter conﬁgurations iden- tiﬁed through extensive random sampling of the space. We also show the test-set performance of neural network models, trained through standard training, with hyperparameters ﬁxed to the ﬁnal hyperparameter values identiﬁed by the partitioned networks. The hyperparameters identiﬁed by partitioned networks appear to also be good for regular neural networks; the standard neural networks with hyperparameters identiﬁed through partitioned training also outperform the extensive random sampling of the hyperparameter space. Furthermore, Figure 9 shows that partitioned networks do learn full rotation invariance on the RotMNIST task, i.e. when full rotation invariance is present in the data generating distribution. 0.0 0.2 0.4 Translation X 0.96 0.97 0.98 0.99T est Accuracy 0.0 0.2 0.4 Translation Y 0 /2 Rotation Random Sampling Partitioned Runs Partitioned Runs Final Partitioned Runs Final - Retrained 0.0 0.2 0.4 Scale X 0.0 0.2 0.4 0.6 Scale Y 0.0 0.2 0.4 Shear Figure 9: The test-set performance plotted alongside (1D projections of) afﬁne augmentation hyper- parameters on the RotMNIST task with MNIST-CNN. Final test-set accuracies are shown for the hyperparameters sampled randomly for a neural network model trained through standard training with those hyperparameters ﬁxed (+). For multiple partitioned networks runs, the plot shows the progres- sion of the identiﬁed hyperparameters and the test-set performance through the training run ( ), as well as the ﬁnal hyperparameters and test-set performance ( ). Lastly, the plot also shows the ﬁnal test-set accuracies of models trained through standard training on the ﬁnal hyperparameters identiﬁed through partitioned training ( ). 10The ranges were: Uniform(0,π) for the maximum rotation, and Uniform(0,1 2 ) for all the remaining afﬁne augmentation parameters (maximum shear, maximum x−and y−translation, and maximum x−and y−scale). 20Published as a conference paper at ICLR 2023 H L IMITATIONS As mentioned in the main text, our method improves upon existing work, but also comes with its own limitations. Complexity Inherent to our method — as presented in e.g. Figure 1 — is the necessity for an additional forward-backward pass to update the hyperparameters. Consequently, hyperparameter optimization has additional costs which, however, are signiﬁcantly less than the computational costs of existing work, as we discuss in more detail in Appendix D.1 and the experimental section. Furthermore, empirically, partitioned networks usually require more training iterations to converge. Performance Assuming the optimal hyper-parameters are given, training the full, non-partitioned networks based on those optimal values can be expected to yield better performance compared to the ﬁnal model found by partitioned training. Partitioning the network inherently constrains the network capacity, causing some loss of performance. Opportunities for alleviating this performance loss while still enjoying single-run hyperparameter optimization through partitioned training will be left to future work. These include for example adjusting training rounds or increasing network capacity in the ﬁrst place. Partitioning While partitioned networks allows for automatic optimization of, intuitively, hard to tune hyperparameters, such as augmentation parameters, they come with the additional limitation of requiring to partition both the data and the model. This introduces an additional hyperparameter, namely, the partitioning strategy. While our default strategy of assigning more parameters and data to the ﬁrst chunk works reasonably well on all of the experiments we consider, if one targets obtaining the best possible performance on a given task, the partitioning strategy might need additional tuning. We provide some empirical results about the sensitivity to partitioning in appendix F.1 I E XPERIMENTAL DETAILS I.1 P ARTITIONED TRAINING Partitioned parameter update scheduling The gradient computation of Equation 3, as described in the main text, requires that the data-points for updating a given subnetwork w(k) s come from the appropriate dataset chunks (x,y) ∈D1:k for a chunk k. Depending on the partitioning scheme (Appendix C), evaluating different subnetworks for different chunks can or cannot be done in a single mini-batch. More speciﬁcally, the random weight-partitioning we chose for our experiments requires a separate mini-batch per subnetwork (in order to keep the memory cost the same as for standard neural network training). An immediate question arising from a chunked dataset and several partitions is to deﬁne the order and frequency of updates across subnetworks. In our experiments we deﬁne (non-uniform) splits of the training dataset Dacross the Cchunks, which requires a tailored approach to sampling the data. More speciﬁcally, for a given (normalized) ratio of chunk-sizes [u1,...,u C], each iteration of partitioned training proceeds as follows: 1. Sample a partition index k∼Cat(u1,...,u C) 2. Sample a mini-batch ˜Dof examples uniformly from D1:k. 3. Evaluate log p( ˜D|w(k) s ,ψ) using subnetwork w(k) s and 4. compute the (stochastic) gradient wrt. partition parameters wk (Eq. 3). 5. Update partition parameters wk using an optimizer, such as SGD or Adam. This sampling scheme results in a data-point (x,y) ∈Dk from earlier chunks to be sampled more often. Concretely, the probability that an example in chunk kwill be sampled is ∝∑ i≤kui. This is done so that each partition wk is updated with equal probability on each of the examples in D1:k As a result, we use with replacement sampling for the partitioned network training throughout the experimental section. 21Published as a conference paper at ICLR 2023 Gradient optimization of partitioned parameters A consequence of per-partition updates with the random weight partitioning scheme (appendix C) is that, for a chosen partition wk to update, all other partitions do not receive a gradient update. In other words, the gradient at each iteration is sparse. Consequently, many off-the-shelve momentum-based optimizers will not account correctly. Speciﬁcally, we implement modiﬁcations to the PyTorch Paszke et al. (2019) provided optimizers that allow us to track per-partition momenta, number of steps, etc. Note that this creates a disconnect between the number of iterations across all partitions and the number of iterations per-partition. Doing so, however aligns the computational cost of training the partitioned network parameters with the cost of training regular neural network parameters. Regardless, we do not alter the way learning-rate schedulers behave in our experiments and anneal learning-rates according to the total number of iterations. Similarly, we report the total number of iterations when comparing against baselines that update all network-parameters per iteration. While a simple gradient-accumulation scheme across mini-batches would result in a single gradient across all partitions, this approach inherently clashes with non-uniform partitioning [u1,...,u C]. Instead, we chose to sequentially apply gradients computed on a single partition, as described in the previous paragraphs. A further advantage of this approach is that learning progress made by updating partition wk immediately inﬂuences (and can improve) the prediction of subnetworks w(k) s ,w(k+1) s ,..., w(C) s . Gradient optimization of hyperparameters Our partitioned network scheme makes it easy to compute stochastic gradients of the hyperparameter objective LML in Eq. 4 using batch gradient descent optimization methods. After every update to a randomly sampled network partition (see previous paragraph), we update hyperparamters ψas follows: • sample a dataset chunk index k ∼Cat(u2 Z ,..., uC Z ). Ratios are re-normalized to exclude D1. • sample a mini-batch ˜Dof examples uniformly from Dk (Note the choice of Dk instead of D1:k). • Evaluate log p( ˜D|w(k−1) s ,ψ) using subnetwork w(k−1) s and • compute the (stochastic) gradient wrt. hyperparameters ψ(Eq. 4). • Update partition parameters ψusing an optimizer, such as SGD or Adam. The above sampling procedure yields an unbiased estimate of gradients in eq. 4. The fact that we optimize hyperparameters with gradients based on data from a single chunk at a time is again a consequence of the random weight-partitioning scheme for the partitioned networks. It is possible to compute gradients wrt. ψfor mini-batches with examples from multiple chunks at a time. With the random weight partitioning scheme, this would result in an increased memory overhead. Lastly, we could also accumulate gradients from different chunks, similarly to Immer et al. (2022), and this would likely result in a lower-variance estimate per update . It is also possible to reduce the computational overhead of evaluating two mini-batches per iteration (one for updates to wk, one for ψ) as we do in our experiments by interleaving hyperparameter updates at less frequent intervals. We leave an exploration of these design choices to future work. Throughout all experiments, except those in the federated settings (see section J), we use the same batch-size for the hyperparameter udpates as for the regular parameter updates. Weight-decay For partitioned networks, whenever using weight-decay, we scale the weight decay for earlier partitions with the reciprocal of the number of examples in chunks used to optimize them, following the diagonal Gaussian prior interpretation of weight-decay. This makes the training compatible with the variational interpretation in Appendix B. I.2 P ARTITIONED AFFINE TRANSFORMATIONS In Appendix C we described how we realize partitioned versions of fully-connected and convolutional layers. Design choices for other parameterized network layers used in our experiments are described below. 22Published as a conference paper at ICLR 2023 Normalization layers It is common-place in most architectures to follow a normalization layer (such as BatchNorm (Ioffe and Szegedy, 2015), GroupNorm (Wu and He, 2018)) with an element- wise or channel-wise, afﬁne transformation. Namely, such a transformation multiplies its input h by a scale vector s and adds a bias vector b: o = h ∗s + b. For random weight-partitioned networks, we parameterize such afﬁne transformations by deﬁning separate vectors {s1,..., sC} and {b1,..., bC}for each partition; the actual scale and bias used in a given subnetwork w(k) s are s(k) s = ∏ i∈{1,...,k}si and b(k) s = ∑ i∈{1,...,k}bi respectively. This ensures that the ﬁnal afﬁne transformation for each subnetwork w(k) s depends on the parameters in the previous partitions [1,...,k −1]. Doing so increases the parameter count for the partitioned networks in architectures that use those normalization layers by a negligible amount. Scale and bias in FixUp networks The FixUp paper (Zhang et al., 2019) introduces extra scales and biases into the ResNet architecture that transform the entire output of the layers they follow. We turn these into “partitioned” parameters using the same scheme as that for scales and biases of afﬁne transformations following normalization layers. For partitioned networks, through-out the paper, we match the proportion of parameters assigned to each partition kin each layer to the proportion of data examples in the corresponding chunk Dk. I.3 A RCHITECTURE CHOICES Input selection experiments We use a fully-connected feed-forward neural network with2 hidden layers of size [256,256], and with GeLU (Hendrycks and Gimpel, 2016) activation functions. We initialise the weights using the Kaiming uniform scheme (He et al., 2015). For partitioned networks, we use the random-weight partitioning scheme. Fixup Resnet For all experiments using FixUp ResNets we follow Immer et al. (2022); Zhang et al. (2019), and use a 3-stage ResNet with channel-sizes (16,32,64) per stage, with identity skip- connections for the residual blocks as described in He et al. (2016). The residual stages are followed by average pooling and a ﬁnal linear layer with biases. We use 2D average pooling in the residual branches of the downsampling blocks.We initialize all the parameters as described in Zhang et al. (2019). Wide ResNet For all experiments using a Wide-ResNet-N-D (Zagoruyko and Komodakis, 2016), with N being the depth and D the width multiplier, we use a 3 stage ResNet with channel-sizes (16D,32D,64D). We use identity skip-connections for the residual blocks, as described in He et al. (2016), also sometimes known as ResNetV2. ResNet-50 We use the ”V2” version of Wide ResNet as described in (Zagoruyko and Komodakis, 2016) and replace BatchNormalization with GroupNormalization using 2 groups. We use the ’standard’ with withD= 1and three stages of 8 layers for a 50-layer deep ResNet. We use ReLU activations for all ResNet experiments throughout. MNIST CNN For the MNIST experiments, we use the same architecture as Schw¨obel et al. (2021) illustrated in the replicated Table 9. Table 9: CNN architecture for MNIST experiments Layer Speciﬁcation 2D convolution channels=20, kernel size=(5,5), padding=2, activation=ReLU Max pooling pool size=(2,2), stride=2 2D convolution channels=50, kernel size=(5,5), padding=2, activation=ReLU Max pooling pool size=(2,2), stride=2 Fully connected units=500, activation=ReLU Fully connected units=50, activation=ReLU Fully connected units=10, activation=Softmax 23Published as a conference paper at ICLR 2023 I.4 T RAINING DETAILS Learning afﬁne augmentations For the parametrization of the learnable afﬁne augmentation strategies, we follow prior works for a fair comparison. More speciﬁcally, for our MNIST based setup we follow the parametrization proposed in Schw¨obel et al. (2021) whereas for our CIFAR10 based setup we use the generator parametrization from Immer et al. (2022). Input selection experiments For the model selection (non-differentiable) input selection exper- iments, we train all variants with Adam with a learning rate of 0.001 and a batch-size of 256 for 10000 iterations. For both Laplace and partitioned networks, we do early stopping based on the marginal likelihood objective (LML for partitioned networks). We use weight-decay 0.0003 in both cases. For the post-hoc Laplace method, we use the diagonal Hessian approximation, following the recommendation in (Immer et al., 2021). For partitioned networks, we divide the data and parameters into 8 chunks of uniform sizes. We plot results averaged across 3 runs. Mask learning for input selection experiment We use the same optimizer settings as for the input selection experiment. We train for 30000 iterations, and optimize hyperparameters with Adam with a learning rate of 0.001. We divide the data and parameters into 4 uniform chunks. MNIST experiments We follow Schw¨obel et al. (2021), and optimize all methods with Adam with a learning rate of 0.001, no weight decay, and a batch-size of 200. For the partitioned net- works and Augerino results, we use 20 augmentation samples. We use an Adam optimizer for the hyperparameters with a learning rate of 0.001 (and default beta parameters). For Augerino on MNIST, we use the “safe” variant, as otherwise the hyperparameters and the loss diverge on every training run. We elaborate on this phenomenon in Appendix E. Otherwise, we follow the recommended settings from (Benton et al., 2020) and Immer et al. (2022), namely, a regularization strength of 0.01, and a learning rate for the hyperparameters of 0.05. For both MNIST and CIFAR experiments, we found it beneﬁcial to allocate more data to either the earlier, or the later, chunks. Hence, we use 3 chunks with [80%,10%,10%] split of examples for all MNIST and CIFAR experiments. CIFAR variations experiments We again follow Immer et al. (2022), and optimize all ResNet models with SGD with a learning rate of 0.1 decayed by a factor of 100×using Cosine An- nealing, and momentum of 0.9 (as is standard for ResNet models). We use a batch-size of 250. We again use Adam for hyperparameter optimization with a learning rate of 0.001 (and default beta parameters). We train our method for [2400,8000,12000,20000,40000] iterations on subsets [1000,5000,10000,20000,50000] respectively for CIFAR-10, just as in (Immer et al., 2022). For all methods, we used a weight-decay of 1e−4. For partitioned networks, we increase the weight decay for earlier partitions with the square root of the number of examples in chunks used to optimize them, following the diagonal Gaussian prior interpretation of weight-decay. We use3 chunks with [80%,10%,10%] split of examples. For RotCIFAR-10 results, we noticed our method hasn’t fully converged (based on training loss) in this number of iterations, and so we doubled the number of training iterations for the RotMNIST results. This slower convergence can be explained by the fact that, with our method, we only update a fraction of the network parameters at every iteration. TinyImagenet experiments Our experiments with TinyImagenet (Le and Yang, 2015) closely follow the setting for the CIFAR-10 experiments described above. Images are of size64x64 pixels, to be classiﬁed into one of 200 classes. The training-set consists of 100000 images and we compare our method against baselines on subset of [10000,50000,100000] datapoints. For the standard version of TinyImagenet, we train for [80000,80000,40000] steps respectively and for the rotated version of TinyImagenet we train for 120000 steps for all subset sizes. We tuned no other hyper-parameters compared to the CIFAR-10 setup and report our method’s result for a partitioning with[80%,20%] across 2 chunks after ﬁnding it to perform slightly better than a [80%,10%,10%] split across 3 chunks in a preliminary comparison. 24Published as a conference paper at ICLR 2023 Fine-tuning experiments For the ﬁne-tuning experiments in table 2, we trained a FixUp ResNet-14 on a subset of 20000 CIFAR10 examples, while optimizing afﬁne augmentations (following afﬁne augmentations parameterization in (Benton et al., 2020)). We used the same optimizer settings as for all other CIFAR experiments, and trained for 80000 iterations, decaying the learning rate with Cosine Annealing for the ﬁrst 60000 iterations. For ﬁne-tuning of validation-set optimization models, we used SGD with same settings, overriding only the learning rate to 0.01. We tried a learning rate of 0.01 and 0.001, and selected the one that was most favourable for the baseline based on the test accuracy. We also tried training on the full CIFAR-10 dataset, but found that all methods ended up within a standard error of each other when more than 70% of the data was assigned to the ﬁrst chunk (or training set, in the case of validation set optimization). This indicates that CIFAR-10 is sufﬁciently larger that, when combined with afﬁne augmentation learning and the relatively small ResNet-14 architecture used, using the extra data in the 2nd partition (or the validation set) results in negligible gains. I.5 D ATASETS Input selection synthetic dataset For the input selection dataset, we sample 3000 datapoints for the training set as described in section 5, and we use a fresh sample of 1000 datapoints for the test set. RotMNIST Sometimes in the literature, RotMNIST referes to a speciﬁc subset of 12000 MNIST examples, whereas in other works, the full dataset with 60000 examples is used. In this work, following (Benton et al., 2020; Immer et al., 2022) we use the latter. J F EDERATED PARTITIONED TRAINING In this section, we explain how partitioned networks can be applied to the federated setting, as well as the experimental details. J.1 P ARTITIONED NETWORKS IN FL In order to apply partitioned networks to the federated setting, we randomly choose a partition for each client such that the marginal distribution of partitions follows a pre-determined ratio. A given chunk Dk therefore corresponds to the union of several clients’ datasets. Analogous to how “partitioned training” is discussed in the main text and Appendix I, we desire each partition wk to be updated on chunks D1:k. Equation 3 in the main text explains which data chunks are used to compute gradients wrt. parameter partition wk. An analogous perspective to this objective is visualized by the exemplary algorithm in Figure 1 and asks which partitions are inﬂuenced (i,e., updated) by data from chunk Dk: A data chunk Dk is used to compute gradients wrt. partitions wk:C through subnetworks w(k) s to w(C) s respectively. Consequently, a client whose dataset is assigned to chunkDk can compute gradients for all partitions wk:C. Updating network partitions Due to the weight-partitioned construction of the partitioned neural networks, it is not possible to compute gradients with respect to all partitions in a single batched forward-pass through the network. Additionally, a change to the partition parameters wk directly inﬂuences subnetworks w(k+1) s to w(C) s . In order to avoid the choice of ordering indices kto Cfor the client’s local update computation, we update each partition independently while keeping all other partitions initialised to the server-provided values that the client received in that round t: Denote Di,k as the dataset of client iwhere we keep index kto emphasize the client’s assignment to chunkk. Further denote wt+1 j,i as the partition wt j after having been updated by client ion dataset Di,k. wt+1 j,i = arg max wj log p ( Di,k|(wt 1,..., wt j, ˆwt j+1,..., ˆwt j+C),ψ ) ∀j ∈[k,C], (25) where the details of optimization are explained in the following section. We leave an explo- ration for different sequential updating schemes to future work. The ﬁnal update communi- cated by a client to the server consists of the concatenation of all updated parameter partitions 25Published as a conference paper at ICLR 2023 wt+1 .,i = concat(wt+1 k,i ,..., wt+1 C,i ). Note that partitions (wt 1,..., wt k−1) have not been modiﬁed and need not be communicated to the server. The resulting communication reductions make partitioned networks especially attractive to FL as data upload from client to server poses a signiﬁcant bottleneck. In practice, we expect the beneﬁts of these communication reductions to outweigh the additional computation burden of sequentially computing gradients wrt., to multiple partitions. The server receives wt+1 .,i from all clients that participates in round t, computes the delta’s with the global model and proceeds to average them to compute the server-side gradient in the typical federated learning fashion (Reddi et al., 2020). Updating hyperparameters The computation of gradients on a clientiwrt. ψis a straight-forward extension of equation 4 and the exemplary algorithm of Figure 1: ∇ψLML (Di,k,ψ) ≈∇ψlog p ( Di,k|w(t+1),(k−1) s,i ,ψ ) , (26) where Di,k corresponds to client i’s local dataset which is assigned to chunk k and w(t+1),(k−1) s corresponds to the (k−1)’th subnetwork after incorporating all updated partitionsw(t+1),(k−1) s,i = concat(wt 1,..., wt k−1,wt+1 k,i ,..., wt+1 C,i ). Note that we compute a full-batch update to ψin MNIST experiments and use a batch-size equal to the batch-size for the partitioned parameter updates for CIFAR10. Upon receiving these gradients from all clients in this round, the server averages them to form a server-side gradient. Conceptually, this approach to updating ψcorresponds to federated SGD. J.2 F EDERATED SETUP Non-i.i.d. partitioning For our federated experiments, we split the 50kMNIST and 45kCIFAR10 training data-points across 100 clients in a non-i.i.d. way to create the typical challenge to federated learning experiments. In order to simulate label-skew, we follow the recipe proposed in Reddi et al. (2020) with α= 1.0 for CIFAR10 and α= 0.1 for MNIST. Note that with α= 0.1, most clients have data corresponding to only a single digit. For our experiments on rotated versions of CIFAR10 and MNIST, we sample a degree of rotation per data-point and keep it ﬁxed during training. In order to create a non-i.i.d partitioning across the clients, we bin data-points according to their degree of rotation into 10 bins and sample using the same technique as for label-skew with α = 0.1 for both datasets. Learning curves are computed using the 10k MNIST and 5k CIFAR10 validation data-points respectively. For the rotated dataset experiments, we rotate the validation set in the same manner as the training set. Architectures and experimental setup We use the convolutional network provided at Schw¨obel et al. (2021) for MNIST and the ResNet-9 (Dys) model for CIFAR10 but with group normaliza- tion (Wu and He, 2018) instead of batch normalization. We include (learnable) dropout using the continuous relaxation proposed at Maddison et al. (2016) between layers for both architectures. We select 3 chunks for MNIST with a [0.7,0.2,0.1] ratio for both, client-assignments and parameter- partition sizes. For CIFAR10, we found a [0.9,0.1] split across 2 sub-networks to be beneﬁcial. In addition to dropout logits, ψencompasses parameters for afﬁne transformations, i.e., shear, trans- lation, scale and rotation. We report results after 2kand 5krounds, respectively, and the expected communication costs as a percentage of the non-partitioned baseline. Shared setting In order to elaborate on the details to reproduce our results, we ﬁrst focus on the settings that apply across all federated experiments. We randomly sample the corresponding subset of 1.25k, 5kdata-points from the full training set and keep that selection ﬁxed across experiments (i,e., baselines and partitioned networks) as well as seeds. The subsequent partitioning across clients as detailed in the previous paragraph is equally kept ﬁxed across experiments and seeds. Each client computes updates for one epoch of its local dataset, which, for the low data regimes of 1.25k data-points globally, results in single update per client using the entire local dataset. We averaged over 10 augmentation samples for the forward pass in both training and inference. MNIST & RotMNIST For 5k data-points and correspondingly 50 data-points on average per client, most clients perform a single update step. A small selection of clients with more than 64 data- 26Published as a conference paper at ICLR 2023 points performs two updates per round. For the experiments using the full dataset and a mini-batch size of 64, each client performs multiple updates per round. After initial exploration on the baseline FedAvg task, we select a local learning-rate of 5e−2 and apply standard SGD. The server performs Adam Reddi et al. (2020) with a learning rate of 1e−3 for the model parameters. We keep the other parameters of Adam at their standard PyTorch values. We ﬁnd this setting to generalize to the partitioned network experiments but found a higher learning rate of3e−3 for the hyper-parameters to be helpful. We chose the convolutional network from Schw¨obel et al. (2021) with (learned) dropout added between layers. The model’s dropout layers are initialized to drop10% of hidden activations. For the baseline model we keep the dropout-rate ﬁxed and found 10% to be more stable than 30%. CIFAR10 & RotCIFAR10 We ﬁx a mini-batch size of 32, leading to multiple updates per client per round in both, the full dataset regime as well as the5kdata-points setting. Similarly to the MNIST setting, we performed an initial exploration of hyperparameters on the baseline FedAvg task and use the same ones on partitioned networks. We used dropout on the middle layer of each block which was initialized to 0.1 for both the baseline and partitioned networks and whereas partitioned networks optimized it with LML and the concrete relaxation from Maddison et al. (2016), the baseline kept it ﬁxed. For the server side optimizer we used Adam with the default betas and a learning rate of 1e−2, whereas for the hyperparameters we used Adam with the default betas and a learning rate of 1e−3. In both cases we used an ϵ= 1e−7. For the local optimizer we used SGD with a learning rate of 10−0.5 and no momentum. J.3 MNIST LEARNING CURVES In Figure 10 we show learning curves for the three considered dataset sizes on the standard MNIST task. Each learning curve is created by computing a moving average across 10 evaluations, each of which is performed every 10 communication rounds, for each seed. We then compute the average and standard-error across sees and plot those values on the y-axis. On the x-axis we denote the total communication costs (up- and download) to showcase the partitioned networks reduction in communication overhead. We see that especially for the low dataset regime, training has not converged yet and we expect performance to improve for an increased number of iterations. Figure 10: Learning curves for MNIST experiments on 1.25k, 5kand 50kdata-points respectively. 27",
      "meta_data": {
        "arxiv_id": "2304.14766v1",
        "authors": [
          "Bruno Mlodozeniec",
          "Matthias Reisser",
          "Christos Louizos"
        ],
        "published_date": "2023-04-28T11:24:41Z",
        "pdf_url": "https://arxiv.org/pdf/2304.14766v1.pdf"
      }
    },
    {
      "title": "DP-HyPO: An Adaptive Private Framework for Hyperparameter Optimization",
      "abstract": "Hyperparameter optimization, also known as hyperparameter tuning, is a widely\nrecognized technique for improving model performance. Regrettably, when\ntraining private ML models, many practitioners often overlook the privacy risks\nassociated with hyperparameter optimization, which could potentially expose\nsensitive information about the underlying dataset. Currently, the sole\nexisting approach to allow privacy-preserving hyperparameter optimization is to\nuniformly and randomly select hyperparameters for a number of runs,\nsubsequently reporting the best-performing hyperparameter. In contrast, in\nnon-private settings, practitioners commonly utilize ``adaptive''\nhyperparameter optimization methods such as Gaussian process-based\noptimization, which select the next candidate based on information gathered\nfrom previous outputs. This substantial contrast between private and\nnon-private hyperparameter optimization underscores a critical concern. In our\npaper, we introduce DP-HyPO, a pioneering framework for ``adaptive'' private\nhyperparameter optimization, aiming to bridge the gap between private and\nnon-private hyperparameter optimization. To accomplish this, we provide a\ncomprehensive differential privacy analysis of our framework. Furthermore, we\nempirically demonstrate the effectiveness of DP-HyPO on a diverse set of\nreal-world datasets.",
      "full_text": "DP-HyPO: An Adaptive Private Hyperparameter Optimization Framework Hua Wang∗ Sheng Gao† Huanyu Zhang‡ Weijie J. Su§ Milan Shen¶ November 28, 2023 Abstract Hyperparameter optimization, also known as hyperparameter tuning, is a widely recognized technique for improving model performance. Regrettably, when training private ML models, many practitioners often overlook the privacy risks associated with hyperparameter optimization, which could potentially expose sensitive information about the underlying dataset. Currently, the sole existing approach to allow privacy-preserving hyperparameter optimization is to uniformly and randomly select hyperparameters for a number of runs, subsequently reporting the best- performing hyperparameter. In contrast, in non-private settings, practitioners commonly utilize “adaptive” hyperparameter optimization methods such as Gaussian process-based optimization, which select the next candidate based on information gathered from previous outputs. This substantial contrast between private and non-private hyperparameter optimization underscores a critical concern. In our paper, we introduce DP-HyPO, a pioneering framework for “adaptive” private hyperparameter optimization, aiming to bridge the gap between private and non-private hyperparameter optimization. To accomplish this, we provide a comprehensive differential privacy analysis of our framework. Furthermore, we empirically demonstrate the effectiveness of DP-HyPO on a diverse set of real-world datasets. 1 Introduction In recent decades, modern deep learning has demonstrated remarkable advancements in various applications. Nonetheless, numerous training tasks involve the utilization of sensitive information pertaining to individuals, giving rise to substantial concerns regarding privacy [31, 7]. To address these concerns, the concept of differential privacy (DP) was introduced by [13, 14]. DP provides a mathematically rigorous framework for quantifying privacy leakage, and it has gained widespread acceptance as the most reliable approach for formally evaluating the privacy guarantees of machine learning algorithms. ∗Department of Statistics and Data Science, University of Pennsylvania, Philadelphia, PA 19104, USA. Email: wanghua@wharton.upenn.edu. †Department of Statistics and Data Science, University of Pennsylvania, Philadelphia, PA 19104, USA. Email: shenggao@wharton.upenn.edu. ‡Meta Platforms, Inc., New York, NY 10003, USA. Email:huanyuzhang@meta.com. §Department of Statistics and Data Science, University of Pennsylvania, Philadelphia, PA 19104, USA. Email: suw@wharton.upenn.edu. ¶Meta Platforms, Inc., Menlo Park, CA 94025, USA. Email:milanshen@gmail.com. 1 arXiv:2306.05734v2  [cs.LG]  27 Nov 2023When training deep learning models, the most popular method to ensure privacy is noisy (stochastic) gradient descent (DP-SGD) [4, 37]. DP-SGD typically resembles non-private gradient- based methods; however, it incorporates gradient clipping and noise injection. More specifically, each individual gradient is clipped to ensure a boundedℓ2 norm. Gaussian noise is then added to the average gradient which is utilized to update the model parameters. These adjustments guarantee a bounded sensitivity of each update, thereby enforcing DP through the introduction of additional noise. In both non-private and private settings, hyperparameter optimization (HPO) plays a crucial role in achieving optimal model performance. Commonly used methods for HPO include grid search (GS), random search (RS), and Bayesian optimization (BO). GS and RS approaches are typically non-adaptive, as they select the best hyperparameter from a predetermined or randomly selected set. While these methods are straightforward to implement, they can be computationally expensive and inefficient when dealing with large search spaces. As the dimensionality of hyperparameters increases, the number of potential trials may grow exponentially. To address this challenge, adaptive HPO methods such as Bayesian optimization have been introduced [36, 15, 42]. BO leverages a probabilistic model that maps hyperparameters to objective metrics, striking a balance between exploration and exploitation. BO quickly emerged as the default method for complex HPO tasks, offering improved efficiency and effectiveness compared to non-adaptive methods. While HPO is a well-studied problem, the integration of a DP constraint into HPO has received little attention. Previous works on DP machine learning often neglect to account for the privacy cost associated with HPO [1, 41, 44, 44]. These works either assume that the best parameters are known in advance or rely on a supplementary public dataset that closely resembles the private dataset distribution, which is not feasible in most real-world scenarios. Only recently have researchers turned to the important concept of honest HPO [30], where the privacy cost during HPO cannot be overlooked. Private HPO poses greater challenges compared to the non-private case for two primary reasons. First, learning with DP-SGD introduces additional hyperparameters (e.g., clipping norm, the noise scale, and stopping time), which hugely adds complexity to the search for optimal hyperparameters. Second, DP-SGD is more sensitive to the selection of hyperparameter combinations, with its performance largely influenced by this choice [30, 11, 33]. To tackle this challenging question, previous studies such as [26, 34] propose running the base algorithm with different hyperparameters a random number of times. They demonstrate that this approach significantly benefits privacy accounting, contrary to the traditional scaling of privacy guarantees with the square root of the number of runs (based on the composition properties from [21]). While these papers make valuable contributions, their approaches only allow for uniformly random subsampling from a finite and pre-fixed set of candidate hyperparameters at each run. As a result, any advanced technique from HPO literature that requires adaptivity is either prohibited or necessitates a considerable privacy cost (polynomially dependent on the number of runs), creating a substantial gap between non-private and private HPO methods. Given these considerations, a natural question arises:Can private hyperparameter optimization be adaptive, without a huge privacy cost?In this paper, we provide an affirmative answer to this question. 1.1 Our Contributions • We introduce the pioneering adaptive private hyperparameter optimization frame- work, DP-HyPO, which enables practitioners to adapt to previous runs and focus on 2potentially superior hyperparameters. DP-HyPO permits the flexible use of non-DP adaptive hyperparameter optimization methods, such as Gaussian process, for enhanced efficiency, while avoiding the substantial privacy costs due to composition. In contrast to the non-adaptive methods presented in [34, 26], our adaptive framework, DP-HyPO, effectively bridges the gap between private and non-private hyperparameter optimization. Importantly, our framework not only encompasses the aforementioned non-adaptive methods as special cases, but also seamlessly integrates virtually all conceivable adaptive methods into the framework. • We provide sharp DP guarantees for the adaptive private hyperparameter optimiza- tion. Specifically, when the training procedure is executed multiple times, with each iteration being DP on its own, outputting the best repetition is DP ensured by the composition property. However, applying composition results in excessively loose privacy guarantees. Prior work in [26, 34] presents bounds that are either independent of the number of repetitions or depend logarithmically on it. Nevertheless, these results require that the hyperparameter selection for each iteration follows a uniform sampling distribution. In contrast, DP-HyPO allows arbitrary adaptive sampling distributions based on previous runs. Utilizing the Rényi DP framework, we offer a strict generalization of those uniform results by providing an accurate characterization of the Rényi divergence between the adaptive sampling distributions of neighboring datasets, without any stability assumptions. • Empirically, we observe that the Gaussian process-based DP-HyPO algorithm outperforms its uniform counterpart across several practical scenarios.Gener- ally, practitioners can integrate any non-private adaptive HPO methods into the DP-HyPO framework, opening up a vast range of adaptive private HPO algorithm possibilities. Further- more, DP-HyPO grants practitioners the flexibility to determine the privacy budget allocation for adaptivity, empowering them to balance between the adaptivity and privacy loss when confronting various hyperparameter optimization challenges. 1.2 Our Contributions • We introduce the pioneering adaptive private hyperparameter optimization frame- work, DP-HyPO, which enables practitioners to adapt to previous runs and focus on potentially superior hyperparameters. DP-HyPO permits the flexible use of non-DP adaptive hyperparameter optimization methods, such as Gaussian process, for enhanced efficiency, while avoiding the substantial privacy costs due to composition. In contrast to the non-adaptive methods presented in [34, 26], our adaptive framework, DP-HyPO, effectively bridges the gap between private and non-private hyperparameter optimization. Importantly, our framework not only encompasses the aforementioned non-adaptive methods as special cases, but also seamlessly integrates virtually all conceivable adaptive methods into the framework. • We provide sharp DP guarantees for the adaptive private hyperparameter optimiza- tion. Specifically, when the training procedure is executed multiple times, with each iteration being DP on its own, outputting the best repetition is DP ensured by the composition property. However, applying composition results in excessively loose privacy guarantees. Prior work in [26, 34] presents bounds that are either independent of the number of repetitions or depend logarithmically on it. Nevertheless, these results require that the hyperparameter selection for each iteration follows a uniform sampling distribution. In contrast, DP-HyPO allows arbitrary 3adaptive sampling distributions based on previous runs. Utilizing the Rényi DP framework, we offer a strict generalization of those uniform results by providing an accurate characterization of the Rényi divergence between the adaptive sampling distributions of neighboring datasets, without any stability assumptions. • Empirically, we observe that the Gaussian process-based DP-HyPO algorithm out- performs its uniform counterpartacross several practical scenarios. Generally, practitioners can integrate any non-private adaptive HPO methods into the DP-HyPO framework, opening up a vast range of adaptive private HPO algorithm possibilities. Furthermore, DP-HyPO grants practitioners the flexibility to determine the privacy budget allocation for adaptivity, empowering them to balance between the adaptivity and privacy loss when confronting various hyperparameter optimization challenges. 2 Preliminaries 2.1 Differential Privacy and Hyperparameter Optimization Differential Privacy is a mathematically rigorous framework for quantifying privacy leakage. A DP algorithm promises that an adversary with perfect information about the entire private dataset in use – except for a single individual – would find it hard to distinguish between its presence or absence based on the output of the algorithm [13]. Formally, forε >0, and0 ≤ δ <1, we consider a (randomized) algorithmM : Zn → Ythat takes as input a dataset. Definition 2.1(Differential privacy). A randomized algorithmM is (ε, δ)-DP if for any neighboring dataset D, D′ ∈ Zn differing by an arbitrary sample, and for any eventE, we have P[M(D) ∈ E] ⩽ eε · P \u0002 M \u0000 D′\u0001 ∈ E \u0003 + δ. Here, ε and δ are privacy parameters that characterize the privacy guarantee of algorithmM. One of the fundamental properties of DP is composition. When multiple DP algorithms are sequentially composed, the resulting algorithm remains private. The total privacy cost of the composition scales approximately with the square root of the number of compositions [21]. We now formalize the problem of hyperparameter optimization with DP guarantees, which builds upon the finite-candidate framework presented in [26, 34]. Specifically, we consider a set of base DP algorithms Mλ : Zn → Y, whereλ ∈ Λ represents a set of hyperparameters of interest,Zn is the domain of datasets, andY denotes the range of the algorithms. This setΛ may be any infinite set, e.g., the cross product of the learning rateη and clipping normR in DP-SGD. We require that the set Λ is a measure space with an associated measureµ. Common choices forµ include the counting measure or Lebesgue measure. We make a mild assumption thatµ(Λ) < ∞. Based on the previous research [34], we make two simplifying assumptions. First, we assume that there is a total ordering on the rangeY, which allows us to compare two selected models based on their “performance measure”, denoted byq. Second, we assume that, for hyperparameter optimization purposes, we output the trained model, the hyperparameter, and the performance measure. Specifically, for any input datasetD and hyperparameterλ, the return value ofMλ is (x, q) ∼ Mλ(D), wherex represents the combination of the model parameters and the hyperparameter λ, andq is the (noisy) performance measure of the model. 42.2 Related Work In this section, we focus on related work concerning private HPO, while deferring the discussion on non-private HPO to Appendix F. Historically, research in DP machine learning has neglected the privacy cost associated with HPO [1, 41, 44]. It is only recently that researchers have begun to consider the honest HPO setting [30], in which the cost is taken into account. A direct approach to addressing this issue involves composition-based analysis. If each training run of a hyperparameter satisfies DP, the entire HPO procedure also complies with DP through composition across all attempted hyperparameter values. However, the challenge with this method is that the privacy guarantee derived from accounting can be excessively loose, scaling polynomially with the number of runs. Chaudhuri et al. [8] were the first to enhance the DP bounds for HPO by introducing additional stability assumptions on the learning algorithms. [26] made significant progress in enhancing DP bounds for HPO without relying on any stability properties of the learning algorithms. They proposed a simple procedure where a hyperparameter was randomly selected from a uniform distribution for each training run. This selection process was repeated a random number of times according to a geometric distribution, and the best model obtained from these runs was outputted. They showed that this procedure satisfied(3ε, 0)-DP as long as each training run of a hyperparameter was (ε, 0)-DP. Building upon this, [34] extended the procedure to accommodate negative binomial or Poisson distributions for the repeated uniform selection. They also offered more precise Rényi DP guarantees for this extended procedure. Furthermore, [9] explored a generalization of the procedure for top-k selection, considering (ε, δ)-DP guarantees. In a related context, [30] explored a setting that appeared superficially similar to ours, as their title mentioned “adaptivity.” However, their primary focus was on improving adaptive optimizers such as DP-Adam, which aimed to reduce the necessity of hyperparameter tuning, rather than the adaptive HPO discussed in this paper. Notably, in terms of privacy accounting, their approach only involved composing the privacy cost of each run without proposing any new method. Another relevant area of research is DP selection, which encompasses well-known methods such as the exponential mechanism [27] and the sparse vector technique [14], along with subsequent studies (e.g., [6] and [17]). However, this line of research always assumes the existence of a low- sensitivity score function for each candidate, which is an unrealistic assumption for hyperparameter optimization. 3 DP-HyPO: General Framework for Private Hyperparameter Op- timization The obvious approach to the problem of differentially private hyperparameter optimization would be to run each base algorithm and simply return the best one. However, running such an algorithm on large hyperparameter space is not feasible due to the privacy cost growing linearly in the worst case. While [26, 34] have successfully reduced the privacy cost for hyperparameter optimization from linear to constant, there are still two major drawbacks. First, none of the previous methods considers the case when the potential number of hyperparameter candidates is infinite, which is common in most hyperparameter optimization scenarios. In fact, we typically start with a range of hyperparameters that we are interested in, rather than a discrete set of candidates. Furthermore, prior methods are 5limited to the uniform sampling scheme over the hyperparameter domainΛ. In practice, this setting is unrealistic since we want to “adapt” the selection based on previous results. For instance, one could use Gaussian process to adaptively choose the next hyperparameter for evaluation, based on all the previous outputs. However, no adaptive hyperparameter optimization method has been proposed or analyzed under the DP constraint. In this paper, we bridge this gap by introducing the first DP adaptive hyperparameter optimization framework. 3.1 DP-HyPO Framework To achieve adaptive hyperparameter optimization with differential privacy, we propose the DP-HyPO framework. Our approach keeps an adaptive sampling distributionπ at each iteration that reflects accumulated information. Let Q(D, π) be the procedure that randomly draws a hyperparameterλ from the distribution1 π ∈ D(Λ) , and then returns the output fromMλ(D). We allow the sampling distribution to depend on both the dataset and previous outputs, and we denote asπ(j) the sampling distribution at thej-th iteration on datasetD. Similarly, the sampling distribution at thej-th iteration on the neighborhood dataset D′ is denoted asπ′(j). We now present the DP-HyPO framework, denoted asA(D, π(0), T , C, c), in Framework 1. The algorithm takes a prior distributionπ(0) ∈ D(Λ) as input, which reflects arbitrary prior knowledge about the hyperparameter space. Another input is the distributionT of the total repetitions of training runs. Importantly, we require it to be a random variable rather than a fixed number to preserve privacy. The last two inputs areC and c, which are upper and lower bounds of the density of any posterior sampling distributions. A finiteC and a positivec are required to bound the privacy cost of the entire framework. Framework 1DP-HyPO A(D, π(0), T , C, c) Initialize π(0), a prior distribution overΛ. Initialize the result setA = {} Draw T ∼ T for j = 0 to T − 1 do (x, q) ∼ Q(D, π(j)) A = A ∪ {(x, q)} Update π(j+1) based onA according to any adaptive algorithm such that for allλ ∈ Λ, c ≤ π(j+1)(λ) π(0)(λ) ≤ C Output (x, q) from A with the highestq Note that we intentionally leave the update rule forπ(j+1) unspecified in Framework 1 to reflect the fact that any adaptive update rule that leverages information from previous runs can be used. However, for a non-private adaptive HPO update rule, the requirement of bounded adaptive density c ≤ π(j+1)(λ) π(0)(λ) ≤ C may be easily violated. In Section 3.2, We provide a simple projection technique 1Here, D(Λ) represents the space of probability densities onΛ. 6to privatize any non-private update rules. In Section 4, we provide an instantiation of DP-HyPO using Gaussian process. We now state our main privacy results for this framework in terms of Rényi Differential Privacy (RDP) [29]. RDP is a privacy measure that is more general than the commonly used(ε, δ)-DP and provides tighter privacy bounds for composition. We defer its exact definition to Definition A.2 in the appendix. We note that different distributions of the number of selections (iterations),T , result in very different privacy guarantees. Here, we showcase the key idea for deriving the privacy guarantee of DP-HyPO framework by considering a special case whenT follows a truncated negative binomial distribution2 NegBin(θ, γ) (the same assumption as in [34]). In fact, as we show in the proof of Theorem 1 in Appendix A, the privacy bounds only depend onT directly through its probability generating function, and therefore one can adapt the proof to obtain the corresponding privacy guarantees for other probability families, for example, the Possion distribution considered in [34]. From here and on, unless otherwise specified, we will stick withT = NegBin(θ, γ) for simplicity. We also assume for simplicity that the prior distributionπ(0) is a uniform distribution overΛ. We provide more detailed discussion of handling informed prior other than uniform distribution in Appendix D. Theorem 1. Suppose thatT follows truncated negative Binomial distributionT ∼ NegBin(θ, γ). Let θ ∈ (−1, ∞), γ ∈ (0, 1), and 0 < c≤ C. Suppose for allMλ : Zn → Yover λ ∈ Λ, the base algorithms satisfy(α, ε)-RDP and(ˆα, ˆε)-RDP for someε, ˆε ≥ 0, α∈ (1, ∞), and ˆα ∈ [1, ∞). Then the DP-HyPO algorithmA(D, π(0), NegBin(θ, γ), C, c) satisfies (α, ε′)-RDP where ε′ = ε + (1 +θ) · \u0012 1 − 1 ˆα \u0013 ˆε + \u0012 α α − 1 + 1 +θ \u0013 log C c + (1 + θ) · log(1/γ) ˆα + log E[T] α − 1 . To prove Theorem 1, one of our main technical contributions is Lemma A.4, which quantifies the Rényi divergence of the sampling distribution at each iteration between the neighboring datasets. We then leverage this crucial result and the probability generating function ofT to bound the Rényi divergence in the output ofA. We defer the detailed proof to Appendix A. Next, we present the case with pure DP guarantees. Recall the fact that(ε, 0)-DP is equivalent to (∞, ε)-RDP [29]. When bothα and ˆα tend towards infinity, we easily obtain the following theorem in terms of(ε, 0)-DP. Theorem 2. Suppose thatT follows truncated negative Binomial distributionT ∼ NegBin(θ, γ). Let θ ∈ (−1, ∞) and γ ∈ (0, 1). If all the base algorithmsMλ satisfies (ε, 0)-DP, then the DP-HyPO algorithm A(D, π(0), NegBin(θ, γ), C, c) satisfies \u0000 (2 + θ) \u0000 ε + log C c \u0001 , 0 \u0001 -DP. Theorem 1 and Theorem 2 provide practitioners the freedom to trade off between allocating more DP budget to enhance the base algorithm or to improve adaptivity. In particular, a higher value ofC c signifies greater adaptivity, while a largerε improves the performance of base algorithms. 3.1.1 Uniform Optimization Method as a Special Case We present the uniform hyperparameter optimization method [34, 25] in Algorithm 2, which is a special case of our general DP-HyPO Framework withC = c = 1. Essentially, this algorithm never updates the sampling distributionπ. 2Truncated negative binomial distribution is a direct generalization of the geometric distribution. See Appendix B for its definition. 7Algorithm 2Uniform Hyperparameter OptimizationU(D, θ, γ,Λ) Let π = Unif({1, ...,|Λ|}), andA = {} Draw T ∼ NegBin(θ, γ) for j = 0 to T − 1 do (x, q) ∼ Q(D, π) A = A ∪ {(x, q)} Output (x, q) from A with the highestq Our results in Theorem 1 and Theorem 2 generalize the main technical results of [34, 26]. Specifically, whenC = c = 1 and Λ is a finite discrete set, our Theorem 1 precisely recovers Theorem 2 in [34]. Furthermore, when we setθ = 1, the truncated negative binomial distribution reduces to the geometric distribution, and our Theorem 2 recovers Theorem 3.2 in [26] . 3.2 Practical Recipe to Privatize HPO Algorithms In the DP-HyPO framework, we begin with a prior and adaptively update it based on the accumulated information. However, for privacy purposes, we require the densityπ(j) to be bounded by some constants c and C, which is due to the potential privacy leakage when updatingπ(j) based on the history. It is crucial to note that this distributionπ(j) can be significantly different from the distribution π′(j) if we were given a different input datasetD′. Therefore, we require the probability mass/density function to satisfy c µ(Λ) ≤ π(j)(λ) ≤ C µ(Λ) for allλ ∈ Λ to control the privacy loss due to adaptivity. This requirement is not automatically satisfied and typically necessitates modifications to current non-private HPO methods. To address this challenge, we propose a general recipe to modify any non-private method. The idea is quite straightforward: throughout the algorithm, we maintain a non-private version of the distribution densityπ(j). When sampling from the spaceΛ, we perform a projection from π(j) to the space consisting of bounded densities. Specifically, we define the space of essentially bounded density functions bySC,c = {f ∈ ΛR+ : ess supf ≤ C µ(Λ), ess inff ≥ c µ(Λ), R α∈Λ f(α)dα = 1}. For such a space to be non-empty, we require thatc ≤ 1 ≤ C, where µ is the measure onΛ. This condition is well-defined as we assumeµ(Λ) < ∞. To privatizeπ(j) at thej-th iteration, we project it into the spaceSC,c, by solving the following convex functional programming problem: min f ∥f − π(j)∥2, s.t. f ∈ SC,c. (3.1) Note that this is a convex program sinceSC,c is convex and closed. We denote the output from this optimization problem byPSC,c(π(j)). Theoretically, problem(3.1) allows the hyperparameter space Λ to be general measurable space with arbitrary topological structure. However, empirically, practitioners need to discretizeΛ to some extent to make the convex optimization computationally feasible. Compared to the previous work, our formulation provides the most general characterization of the problem and allows pratitioners toadaptively and iteratively choose a proper discretization as needed. Framework 1 tolerates a much finer level of discretization than the previous method, as the performance of latter degrades fast when the number of candidates increases. We also provide 8examples using CVX to solve this problem in Section 4.2. In Appendix C, we discuss about its practical implementation, and the connection to information projection. 4 Application: DP-HyPO with Gaussian Process In this section, we provide an instantiation of DP-HyPO using Gaussian process (GP) [40]. GPs are popular non-parametric Bayesian models frequently employed for hyperparameter optimization. At the meta-level, GPs are trained to generate surrogate models by establishing a probability distribution over the performance measureq. While traditional GP implementations are not private, we leverage the approach introduced in Section 3.2 to design a private version that adheres to the bounded density contraint. We provide the algorithmic description in Section 4.1 and the empircal evaluation in Section 4.2. 4.1 Algorithm Description The following Algorithm (AGP) is a private version of Gaussian process for hyperparameter tuning. In Algorithm 3, we utilize GP to construct a surrogate model that generates probability distributions Algorithm 3DP-HyPO with Gaussian processAGP(D, θ, γ, τ, β,Λ, C, c) Initialize π(0) = Unif(Λ), andA = {} Draw T ∼ NegBin(θ, γ) for t = 0 to T − 1 do Truncate the density of currentπ(t) to be bounded into the range of[c, C] by projecting to SC,c. ˜π(t) = PSC,c(π(t)). Sample (x, q) ∼ Q(D, ˜π(j)), and updateA = A ∪ {(x, q)} Update mean estimation and variance estimation of the Gaussian processµλ, σ2 λ, and get the score assλ = µλ + τσλ. Update true (untruncated) posteriorπ(t+1) with softmax, byπ(t+1)(λ) = exp(β·sλ)R λ′∈Λ exp(β·s′ λ). Output (x, q) from A with the highestq for the performance measureq. By estimating the mean and variance, we assign a “score” to each hyperparameter λ, known as the estimated upper confidence bound (UCB). The weight factorτ controls the balance between exploration and exploitation, where larger weights prioritize exploration by assigning higher scores to hyperparameters with greater uncertainty. To transform these scores into a sampling distribution, we apply the softmax function across all hyperparameters, incorporating the parameterβ as the inverse temperature. A higher value ofβ signifies increased confidence in the learned scores for each hyperparameter. 4.2 Empirical Evaluations We now evaluate the performance of our GP-based DP-HyPO (referred to as “GP”) in various settings. Since DP-HyPO is the first adaptive private hyperparameter optimization method of its kind, we compare it to the special case of Uniform DP-HyPO (Algorithm 2), referred to as 9“Uniform”, as proposed in [26, 34]. In this demonstration, we consider two pragmatic privacy configurations: the white-box setting and the black-box setting, contingent on whether adaptive HPO algorithms incur extra privacy cost. In the white-box scenario (Section 4.2.1 and 4.2.2), we conduct experiments involving training deep learning models on both the MNIST dataset and CIFAR-10 dataset. Conversely, when considering the black-box setting (Section 4.2.3), our attention shifts to a real-world Federated Learning (FL) task from the industry. These scenarios provide meaningful insights into the effectiveness and applicability of our GP-based DP-HyPO approach. 4.2.1 MNIST Simulation We begin with the white-box scenario, in which the data curator aims to provide overall protection to the published model. In this context, to accommodate adaptive HPO algorithms, it becomes necessary to reduce the budget allocated to the base algorithm. In this section, we consider the MNIST dataset, where we employ DP-SGD to train a standard CNN. The base algorithms in this case are different DP-SGD models with varying hyperparameters, and we evaluate each base algorithm based on its accuracy. Our objective is to identify the best hyperparameters that produce the most optimal model within a given total privacy budget. Specifically, we consider two variable hyperparameters: the learning rateη and clipping normR, while keeping the other parameters fixed. We ensure that both the GP algorithm and the Uniform algorithm operate under the same total privacy budget, guaranteeing a fair comparison. Due to constraints on computational resources, we conduct a semi-real simulation using the MNIST dataset. For both base algorithms (with different noise multipliers), we cache the mean accuracy of5 independently trained models for each discretized hyperparameter and treat that as a proxy for the “actual accuracy” of the hyperparameter. Each time we sample the accuracy of a hyperparameter, we add a Gaussian noise with a standard deviation of0.1 to the cached mean. We evaluate the performance of the output model based on the “actual accuracy” corresponding to the selected hyperparameter. Further details on the simulation and parameter configuration can be found in Appendix E.1. In the left panel of Figure 1, we demonstrated the comparison of performance of the Uniform and GP methods with total privacy budgetε = 153 and δ = 1e − 5. The accuracy reported is the actual accuracy of the output hyperparameter. From the figure, we see that whenT is very small(T <8), GP method is slightly worse than Uniform method as GP spendslog(C/c) budget less than Uniform method for each base algorithm (the cost of adaptivity). However, we see that after a short period of exploration, GP consistently outperform Uniform, mostly due to the power of being adaptive. The superiority of GP is further demonstrated in Table 1, aggregating over geometric distribution. 4.2.2 CIFAR-10 Simulation When examining the results from MNIST, a legitimate critique arises: our DP-Hypo exhibits only marginal superiority over its uniform counterpart, which questions the assertion that adaptivity holds significant value. Our conjecture is that the hyperparameter landscape of MNIST is relatively uncomplicated, which limits the potential benefits of adaptive algorithms. 3The ε values are seemingly very large. Nonetheless, the reported privacy budget encompasses the overall cost of the entire HPO, which is typically overlooked in the existing literature. Given that HPO roughly incurs three times the privacy cost of the base algorithm, anε as high as15 could be reported as only5 in many other works. 10Figure 1: Left: The accuracy of the output hyperparameter in MNIST semi-real simulation, with ε = 15, δ = 0.00001. Middle: The accuracy of the output hyperparameter in CIFAR-10, with ε = 12, δ = 0.00001. Right: The loss of the output hyperparameter in FL. Error bars stands for95% confidence. Curves for GP are calculated by averaging400 independent runs, and curves for Uniform are calculated by averaging10000 independent runs. For a clearer demonstration, we compare the performance for each fixed value ofT, and recognize that the actual performance is a weighted average across different values ofT. To test the hypothesis, we conduct experiments on the CIFAR-10 dataset, with a setup closely mirroring the previous experiment: we employ the same CNN model for training, and optimize the same set of hyperparameters, which are the learning rateη and clipping normR. The primary difference lies in how we generate the hyperparameter landscape. Given that a single run on CIFAR-10 is considerably more time-consuming than on MNIST, conducting multiple runs for every hyperparameter combination is unfeasible. To address this challenge, we leverage BoTorch [3], an open-sourced library for HPO, to generate the landscape. Since we operate in the white-box setting, where the base algorithms have distinct privacy budgets for the uniform and adaptive scenarios, we execute 50 runs and generate the landscape for each case, including the mean (µλ) and standard error (σλ) of accuracy for each hyperparameter combinationλ. When the algorithm (GP or Uniform) visits a specificλ, our oracle returns a noisy scoreq(λ) drawn from a normal distribution of N(µλ, σλ). A more detailed description of our landscapes and parameter configuration can be found in Appendix E.2. In the middle of Figure 1, we showcase a performance comparison between the Uniform and GP methods with a total privacy budget ofε = 12 and δ = 1e − 5. Clearly, GP consistently outperforms the Uniform method, with the largest performance gap occurring when the number of runs is around 10. 4.2.3 Federated Learning In this section, we move to the black-box setting, where the privacy budget allocated to the base algorithm remains fixed, while we allow extra privacy budget for HPO. That being said, the adaptivity can be achieved without compromising the utility of the base algorithm. We explore another real-world scenario: a Federated Learning (FL) task conducted on a propri- etary dataset4 from industry. Our aim is to determine the optimal learning rates for the central server (using AdaGrad) and the individual users (using SGD). To simulate this scenario, we once again rely on the landscape generated by BoTorch [3], as shown in Figure 3 in Appendix E.3. 4We have to respect confidentiality constraints that limit our ability to provide extensive details about this dataset. 11Under the assumption that base algorithms are black-box models with fixed privacy costs, we proceed with HPO while varying the degree of adaptivity. The experiment results are visualized in the right panel of Figure 1, and Table 2 presents the aggregated performance data. We consistently observe that GP outperforms Uniform in the black-box setting. Furthermore, our findings suggest that allocating a larger privacy budget to the GP method facilitates the acquisition of adaptive information, resulting in improved performance in HPO. This highlights the flexibility of GP in utilizing privacy resources effectively. Geometric(γ) 0.001 0.002 0.003 0.005 0.01 0.02 0.025 0.03 GP 0.946 0.948 0.948 0.947 0.943 0.937 0.934 0.932 Uniform 0.943 0.945 0.945 0.944 0.940 0.935 0.932 0.929 Table 1:Accuracy of MNIST using Geometric Distribution with various different values ofγ for Uniform and GP methods. Each number is the mean of200 runs. Geometric(γ) 0.001 0.002 0.003 0.005 0.01 0.02 0.025 0.03 GP (C = 1.25) 0.00853 0.0088 0.00906 0.00958 0.0108 0.0129 0.0138 0.0146 GP (C = 1.33) 0.00821 0.00847 0.00872 0.00921 0.0104 0.0123 0.0132 0.0140 GP (C = 1.5) 0.00822 0.00848 0.00872 0.00920 0.0103 0.0123 0.0131 0.0130 Uniform 0.0104 0.0106 0.0109 0.0113 0.0123 0.0141 0.0149 0.0156 Table 2:Loss of FL using Geometric Distribution with various different values ofγ for Uniform and GP methods with different choice ofC and c = 1/C. Each number is the mean of200 runs. 5 Conclusion In conclusion, this paper presents a novel framework, DP-HyPO. As the first adaptive HPO framework with sharp DP guarantees, DP-HyPO effectively bridges the gap between private and non-private HPO. Our work encompasses the random search method by [26, 34] as a special case, while also granting practitioners the ability to adaptively learn better sampling distributions based on previous runs. Importantly, DP-HyPO enables the conversion of any non-private adaptive HPO algorithm into a private one. Our framework proves to be a powerful tool for professionals seeking optimal model performance and robust DP guarantees. The DP-HyPO framework presents two interesting future directions. One prospect involves an alternative HPO specification which is practically more favorable. Considering the extensive literature on HPO, there is a significant potential to improve the empirical performance by leveraging more advanced HPO methods. Secondly, there is an interest in establishing a theoretical utility guarantee for DP-HyPO. By leveraging similar proof methodologies to those in Theorem 3.3 in [26], it is feasible to provide basic utility guarantees for the general DP-HyPO, or for some specific configurations within DP-HyPO. 126 Acknowledgements The authors would like to thank Max Balandat for his thoughtful comments and insights that helped us improve the paper. References [1] Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep learning with differential privacy. InProceedings of the 2016 ACM SIGSAC conference on computer and communications security, pages 308–318, 2016. [2] Martin S Andersen, Joachim Dahl, Lieven Vandenberghe, et al. Cvxopt: A python package for convex optimization.Available at cvxopt. org, 54, 2013. [3] Maximilian Balandat, Brian Karrer, Daniel Jiang, Samuel Daulton, Ben Letham, Andrew G Wilson, and Eytan Bakshy. Botorch: A framework for efficient monte-carlo bayesian optimization. Advances in neural information processing systems, 33:21524–21538, 2020. [4] Raef Bassily, Adam Smith, and Abhradeep Thakurta. Private empirical risk minimization: Efficient algorithms and tight error bounds. In2014 IEEE 55th annual symposium on foundations of computer science, pages 464–473. IEEE, 2014. [5] James Bergstra, Rémi Bardenet, Yoshua Bengio, and Balázs Kégl. Algorithms for hyper- parameter optimization. Advances in neural information processing systems, 24, 2011. [6] Mark Bun, Gautam Kamath, Thomas Steinke, and Steven Z Wu. Private hypothesis selection. Advances in Neural Information Processing Systems, 32, 2019. [7] Nicholas Carlini, Chang Liu, Úlfar Erlingsson, Jernej Kos, and Dawn Song. The secret sharer: Evaluating and testing unintended memorization in neural networks. InUSENIX Security Symposium, volume 267, 2019. [8] Kamalika Chaudhuri and Staal A Vinterbo. A stability-based validation procedure for differ- entially private machine learning.Advances in Neural Information Processing Systems, 26, 2013. [9] Edith Cohen, Xin Lyu, Jelani Nelson, Tamás Sarlós, and Uri Stemmer. Generalized private selection and testing with high confidence.arXiv preprint arXiv:2211.12063, 2022. [10] Imre Csiszár and Frantisek Matus. Information projections revisited.IEEE Transactions on Information Theory, 49(6):1474–1490, 2003. [11] Soham De, Leonard Berrada, Jamie Hayes, Samuel L Smith, and Borja Balle. Unlock- ing high-accuracy differentially private image classification through scale. arXiv preprint arXiv:2204.13650, 2022. [12] Jinshuo Dong, Aaron Roth, and Weijie J Su. Gaussian differential privacy.Journal of the Royal Statistical Society Series B: Statistical Methodology, 84(1):3–37, 2022. 13[13] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. InTheory of Cryptography: Third Theory of Cryptography Conference, TCC 2006, New York, NY, USA, March 4-7, 2006. Proceedings 3, pages 265–284. Springer, 2006. [14] Cynthia Dwork, Moni Naor, Omer Reingold, Guy N Rothblum, and Salil Vadhan. On the complexity of differentially private data release: efficient algorithms and hardness results. In Proceedings of the forty-first annual ACM symposium on Theory of computing, pages 381–390, 2009. [15] Matthias Feurer and Frank Hutter. Hyperparameter optimization.Automated machine learning: Methods, systems, challenges, pages 3–33, 2019. [16] Yonatan Geifman and Ran El-Yaniv. Deep active learning with a neural architecture search. Advances in Neural Information Processing Systems, 32, 2019. [17] Sivakanth Gopi, Gautam Kamath, Janardhan Kulkarni, Aleksandar Nikolov, Zhiwei Steven Wu, and Huanyu Zhang. Locally private hypothesis selection. InConference on Learning Theory, pages 1785–1816. PMLR, 2020. [18] Xin He, Kaiyong Zhao, and Xiaowen Chu. Automl: A survey of the state-of-the-art.Knowledge- Based Systems, 212:106622, 2021. [19] Andrew Hundt, Varun Jain, and Gregory D Hager. sharpdarts: Faster and more accurate differentiable architecture search.arXiv preprint arXiv:1903.09900, 2019. [20] Frank Hutter, Holger H Hoos, and Kevin Leyton-Brown. Sequential model-based optimization for general algorithm configuration. InLearning and Intelligent Optimization: 5th International Conference, LION 5, Rome, Italy, January 17-21, 2011. Selected Papers 5, pages 507–523. Springer, 2011. [21] Peter Kairouz, Sewoong Oh, and Pramod Viswanath. The composition theorem for differential privacy. InInternational conference on machine learning, pages 1376–1385. PMLR, 2015. [22] Kirthevasan Kandasamy, Willie Neiswanger, Jeff Schneider, Barnabas Poczos, and Eric P Xing. Neural architecture search with bayesian optimisation and optimal transport.Advances in neural information processing systems, 31, 2018. [23] Rajiv Khanna, Joydeep Ghosh, Rusell Poldrack, and Oluwasanmi Koyejo. Information projection and approximate inference for structured sparse variables. InArtificial Intelligence and Statistics, pages 1358–1366. PMLR, 2017. [24] Liam Li and Ameet Talwalkar. Random search and reproducibility for neural architecture search. InUncertainty in artificial intelligence, pages 367–377. PMLR, 2020. [25] Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. Hyper- band: A novel bandit-based approach to hyperparameter optimization.The Journal of Machine Learning Research, 18(1):6765–6816, 2017. [26] Jingcheng Liu and Kunal Talwar. Private selection from private candidates. InProceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing, pages 298–309, 2019. 14[27] Frank McSherry and Kunal Talwar. Mechanism design via differential privacy. In48th Annual IEEE Symposium on Foundations of Computer Science (FOCS’07), pages 94–103. IEEE, 2007. [28] Hector Mendoza, Aaron Klein, Matthias Feurer, Jost Tobias Springenberg, and Frank Hutter. Towards automatically-tuned neural networks. InWorkshop on automatic machine learning, pages 58–65. PMLR, 2016. [29] Ilya Mironov. Rényi differential privacy. In2017 IEEE 30th computer security foundations symposium (CSF), pages 263–275. IEEE, 2017. [30] Shubhankar Mohapatra, Sajin Sasy, Xi He, Gautam Kamath, and Om Thakkar. The role of adaptive optimizers for honest private hyperparameter selection. InProceedings of the aaai conference on artificial intelligence, volume 36, pages 7806–7813, 2022. [31] Milad Nasr, Reza Shokri, and Amir Houmansadr. Comprehensive privacy analysis of deep learning: Passive and active white-box inference attacks against centralized and federated learning. In 2019 IEEE symposium on security and privacy (SP), pages 739–753. IEEE, 2019. [32] Renato Negrinho, Matthew Gormley, Geoffrey J Gordon, Darshan Patil, Nghia Le, and Daniel Ferreira. Towards modular and programmable architecture search.Advances in neural informa- tion processing systems, 32, 2019. [33] Ashwinee Panda, Xinyu Tang, Vikash Sehwag, Saeed Mahloujifar, and Prateek Mittal. Dp-raft: A differentially private recipe for accelerated fine-tuning.arXiv preprint arXiv:2212.04486, 2022. [34] Nicolas Papernot and Thomas Steinke. Hyperparameter tuning with renyi differential privacy. In International Conference on Learning Representations, 2021. [35] Carl Edward Rasmussen. Gaussian processes in machine learning. In Advanced Lectures on Machine Learning: ML Summer Schools 2003, Canberra, Australia, February 2-14, 2003, Tübingen, Germany, August 4-16, 2003, Revised Lectures, pages 63–71. Springer, 2004. [36] Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P Adams, and Nando De Freitas. Taking the human out of the loop: A review of bayesian optimization.Proceedings of the IEEE, 104(1):148–175, 2015. [37] Shuang Song, Kamalika Chaudhuri, and Anand D Sarwate. Stochastic gradient descent with differentially private updates. In 2013 IEEE global conference on signal and information processing, pages 245–248. IEEE, 2013. [38] Salil Vadhan. The complexity of differential privacy.Tutorials on the Foundations of Cryptogra- phy: Dedicated to Oded Goldreich, pages 347–450, 2017. [39] Hua Wang, Sheng Gao, Huanyu Zhang, Milan Shen, and Weijie J Su. Analytical composition of differential privacy via the edgeworth accountant.arXiv preprint arXiv:2206.04236, 2022. [40] Christopher KI Williams and Carl Edward Rasmussen.Gaussian processes for machine learning, volume 2. MIT press Cambridge, MA, 2006. 15[41] Da Yu, Huishuai Zhang, Wei Chen, Jian Yin, and Tie-Yan Liu. Large scale private learning via low-rank reparametrization. In International Conference on Machine Learning, pages 12208–12218. PMLR, 2021. [42] Tong Yu and Hong Zhu. Hyper-parameter optimization: A review of algorithms and applications. arXiv preprint arXiv:2003.05689, 2020. [43] Arber Zela, Aaron Klein, Stefan Falkner, and Frank Hutter. Towards automated deep learning: Efficient joint neural architecture and hyperparameter search.arXiv preprint arXiv:1807.06906, 2018. [44] Huanyu Zhang, Ilya Mironov, and Meisam Hejazinia. Wide network learning with differential privacy. arXiv preprint arXiv:2103.01294, 2021. 16A Proofs of the technical results A.1 Proof of Main Results First, we define Rényi divergence as follows. Definition A.1(Rényi Divergences). Let P and Q be probability distributions on a common space Ω. Assume thatP is absolutely continuous with respect toQ - i.e., for all measurableE ⊂ Ω, if Q(E) = 0, thenP(E) = 0. Let P(x) and Q(x) denote the densities ofP and Q respectively. The KL divergence fromP to Q is defined as D1(P∥Q) := E X←P \u0014 log \u0012P(X) Q(X) \u0013\u0015 = Z Ω P(x) log \u0012P(x) Q(x) \u0013 dx. The max divergence fromP to Q is defined as D∞(P∥Q) := sup \u001a log \u0012P(E) Q(E) \u0013 : P(E) > 0 \u001b . For α ∈ (1, ∞), the Rényi divergence fromP to Q of orderα is defined as Dα(P∥Q) := 1 α − 1 log   E X←P \"\u0012P(X) Q(X) \u0013α−1#! = 1 α − 1 log \u0012 E X←Q \u0014\u0012P(X) Q(X) \u0013α\u0015\u0013 = 1 α − 1 log \u0012Z Q P(x)αQ(x)1−αdx \u0013 . We now present the definition of Rényi DP (RDP) in [29]. Definition A.2(Rényi Differential Privacy). A randomized algorithmM : Xn → Yis (α, ε)-Rényi differentially private if, for all neighbouring pairs of inputsD, D′ ∈ Xn, Dα (M(x)∥M (x′)) ≤ ε. We define some additional notations for the sake of the proofs. In algorithm 1, for any1 ≤ j ≤ T, and neighboring datasetD and D′, we define the following notations for anyy = (x, q) ∈ Y, the totally ordered range set. Pj(y) = P˜y∼Q(D,π(j))(˜y = y) and P′ j(y) = P˜y∼Q(D′,π′(j))(˜y = y) Pj(≤ y) = P˜y∼Q(D,π(j))(˜y ≤ y) and P′ j(≤ y) = P˜y∼Q(D′,π′(j))(˜y ≤ y) Pj(< y) = P˜y∼Q(D,π(j))(˜y < y) and P′ j(< y) = P˜y∼Q(D′,π′(j))(˜y < y). By these definitions, we havePj(≤ y) = Pj(< y) + Pj(y), andP′ j(≤ y) = P′ j(< y) + P′ j(y). And additionally, we have Pj(y) P′ j(y) = R λ∈Λ P(Mλ(D) = y)π(j)(λ)dλR λ∈Λ P(Mλ(D′) = y)π′(j)(λ)dλ ≤ sup λ∈Λ P(Mλ(D) = y)π(j)(λ) P(Mλ(D′) = y)π′(j)(λ) ≤ C c · sup λ∈Λ P(Mλ(D) = y) P(Mλ(D′) = y). (A.1) 17Here, the first inequality follows from the simple property of integration, and the second inequality follows from the fact thatπ(j) has bounded density betweenc and C. Similarly, we have Pj(≤ y) P′ j(≤ y) ≤ C c · sup λ∈Λ P(Mλ(D) ≤ y) P(Mλ(D′) ≤ y), (A.2) and Pj(< y) P′ j(< y) ≤ C c · sup λ∈Λ P(Mλ(D) < y) P(Mλ(D′) < y). (A.3) Note thatD and D′ are neighboring datasets, andMλ satisfies some DP guarantees. So the ratio P(Mλ(D)∈E) P(Mλ(D′)∈E) for any eventE can be bounded. For simplicity, we define the inner product of a distribution π with the vector M(D) = (P(Mλ(D) = y) : λ ∈ Λ) as π · M(D) := Z λ∈Λ P(Mλ(D) = y)π(λ)dλ. (A.4) Now, we define additional notations to bound the probabilities. RecallSC,s is given by{f ∈ ΛR+ : ess supf ≤ C, ess inff ≥ c, R α∈Λ f(α)dα = 1.}. It is straightforward to see this is a compact set as it is the intersection of three compact sets. We define P+(y) := sup π∈SC,c Z λ∈Λ P(Mλ(D) = y)π(j)(λ)dλ = π+ · M(D), (A.5) where π+ is the distribution that achieves the supreme in the compact setSC,c. Similarly, we define P′−(y) for D′ as given by P′−(y) := inf π∈SC,c Z λ∈Λ P(Mλ(D′) = y) · π′(j)(λ)dλ = π′− · M. (A.6) Similarly, we can defineP′+(y) and P−(y) accordingly. From the definition, we know that P−(y) ≤ Pj(y) ≤ P+(y) and P′−(y) ≤ P′ j(y) ≤ P′+(y). (A.7) We also have P+(y) P′−(y) = π∗ · M(D) π′− · M(D′) ≤ sup λ P(Mλ(D) = y) P(Mλ(D′) = y) · C c . (A.8) It is similar to define P+(≤ y) := sup π∈SC,c Z λ∈Λ P(Mλ(D) ≤ y) and P′+(≤ y) := sup π∈SC,c Z λ∈Λ P(Mλ(D′) ≤ y) P−(≤ y) := inf π∈SC,c Z λ∈Λ P(Mλ(D) ≤ y) and P′−(≤ y) := inf π∈SC,c Z λ∈Λ P(Mλ(D′) ≤ y) P+(< y) := sup π∈SC,c Z λ∈Λ P(Mλ(D) < y) and P′+(< y) := sup π∈SC,c Z λ∈Λ P(Mλ(D′) < y) 18P−(< y) := inf π∈SC,c Z λ∈Λ P(Mλ(D) < y) and P′−(< y) := inf π∈SC,c Z λ∈Λ P(Mλ(D′) < y). Following the exact same proof, we have P−(≤ y) ≤ Pj(≤ y) ≤ P+(≤ y) and P′−(≤ y) ≤ P′ j(≤ y) ≤ P′+(≤ y) (A.9) P−(< y) ≤ Pj(< y) ≤ P+(< y) and P′−(< y) ≤ P′ j(< y) ≤ P′+(< y) (A.10) P+(≤ y) P′−(≤ y) ≤ sup λ P(Mλ(D) ≤ y) P(Mλ(D′) ≤ y) · C c and P+(< y) P′−(< y) ≤ sup λ P(Mλ(D) < y) P(Mλ(D′) < y) · C c . (A.11) It is also straightforward to verify from the definition that P+(≤ y) = P+(< y) + P+(y) and P′+(≤ y) = P′+(< y) + P′+(y) (A.12) P+ − (≤ y) = P−(< y) + P−(y) and P′−(≤ y) = P′−(< y) + P′−(y). (A.13) Lemma A.3.Suppose ifaλ, bλ are non-negative andcλ, c′ λ are positive for allλ. Then we have P λ aλcλP λ bλc′ λ ≤ P λ aλP λ bλ · sup λ,λ′ \f\f\f\f cλ c′ λ \f\f\f\f. Proof of Lemma A.3.This lemma is pretty straight forward by comparing the coefficient for each term in the full expansion. Specifically, we re-write the inequality as X λ aλcλ X λ′ b′ λ ≤ X λ aλ X λ′ b′ λc′ λ · sup λ,λ′ \f\f\f\f cλ c′ λ \f\f\f\f. (A.14) For each termaλb′ λ, its coefficient on the left hand side of(A.14) is cλ, but its coefficient on the right hand side of(A.14) is c′ λ · supλ,λ′ \f\f\fcλ c′ λ \f\f\f. Since we always havec′ λ · supλ,λ′ \f\f\fcλ c′ λ \f\f\f ≥ cλ, andaλb′ λ ≥ 0, we know the inequality (A.14) holds. Next, in order to present our results in terms of RDP guarantees, we prove the following lemma. Lemma A.4.The Rényi divergence betweenP+ and P− is be bounded as follows: Dα(P+∥P′−) ≤ α α − 1 log C c + sup λ∈Λ Dα \u0000 Mλ(D)∥Mλ(D′) \u0001 Proof of Lemma A.4.We write that e(α−1)Dα(P+∥P′−) = X y∈Y P+(y)α · P′−(y)1−α = X y∈Y (P λ π+(λ)P(Mλ(D) = y))α (P λ π′−(λ)P(Mλ(D′) = y))α−1 (A.15) Here, π+ and π′− are defined in(A.5) and (A.6), so they are essentiallyπ+ y and π′− y as they depend on the value ofy. Therefore, we need to “remove” this dependence ony to leverage the RDP guarantees for each base algorithmMλ. We accomplish this task by bridging viaπ, the uniform 19density onΛ (that isπ(λ) = π(λ′) for anyλ, λ′ ∈ Λ). Specifically, we defineaλ = π(λ)P(Mλ(D) = y), bλ = π(λ)P(Mλ(D′) = y), cλ = π+ y (λ) π(λ) , andc′ λ = π′− y (λ) π(λ) . We see that sup λ,λ′ \f\f\f\f cλ c′ λ \f\f\f\f = sup λ,λ′ \f\f\f\f\f π+ y (λ)/π(λ) π′−y (λ′)/π(λ′) \f\f\f\f\f = sup λ,λ′ \f\f\f\f\f π+ y (λ)) π′−y (λ′) \f\f\f\f\f ≤ C/c, (A.16) since π is the uniform, andπ+ y and π′− y belongs toSC,c. We now apply Lemma A.3 with the above notations for eachy to (A.15), and we have X y∈Y (P λ π+(λ)P(Mλ(D) = y))α (P λ π′−(λ)P(Mλ(D′) = y))α−1 = X y∈Y \u0010P λ π(λ)P(Mλ(D) = y) · π+(λ) π(λ) \u0011α−1 \u0010P λ π(λ)P(Mλ(D) = y) · π+(λ) π(λ) \u0011 \u0010P λ π(λ)P(Mλ(D′) = y) · π′−(λ) π(λ) \u0011α−1 = X y∈Y (P λ aλ · cλ)α−1 \u0010P λ π(λ)P(Mλ(D) = y) · π+(λ) π(λ) \u0011 (P λ bλ · c′ λ)α−1 ≤ X y∈Y sup λ,λ′ \f\f\f\f cλ c′ λ \f\f\f\f α−1 (P λ aλ)α−1 \u0010P λ π(λ)P(Mλ(D) = y) · π+(λ) π(λ) \u0011 (P λ bλ)α−1 = X y∈Y sup λ,λ′ \f\f\f\f cλ c′ λ \f\f\f\f α−1 (P λ aλ)α−1 (P λ aλ · cλ) (P λ bλ)α−1 ≤ X y∈Y sup λ,λ′ \f\f\f\f cλ c′ λ \f\f\f\f α−1 (P λ aλ)α−1 (P λ aλ) · supλ cλ (P λ bλ)α−1 ≤ X y∈Y \u0012C c \u0013α−1 (P λ aλ)α−1 (P λ aλ) · \u0000C c \u0001 (P λ bλ)α−1 = X y∈Y \u0012C c \u0013α · (P λ π(λ)P(Mλ(D) = y))α (P λ π(λ)P(Mλ(D′) = y))α−1 The first inequality is due to Lemma A.3, the second inequality is becauseaλ are non-negative, and the last inequality is because of(A.16) and the fact that bothπ+(λ) and π(λ) are defined inSC,c, and thus their ratio is upper bounded byC c for anyλ. Now we only need to prove that for any fixed distributionπ that doesn’t depend on valuey, we have X y∈Y (P λ π(λ)P(Mλ(D) = y))α (P λ π(λ)P(Mλ(D′) = y))α−1 ≤ sup λ∈Λ e(α−1)Dα(Mλ(D)∥Mλ(D′)). (A.17) With this result, we immediately know the result holds for uniform distributionπ as a special case. To prove this result, we first observe that the functionf(u, v) = uαv1−α is a convex function. This 20is because the Hessian off is \u0012α(α − 1)uα−2v1−α −α(α − 1)uα−1v−α −α(α − 1)uα−1v−α α(α − 1)uαv−α−1 \u0013 , which is easy to see to be positive semi-definite. And now, consider any distributionπ, denote u(λ) = P(Mλ(D) = y) and v(λ) = P(Mλ(D′) = y) by Jensen’s inequality, we have f( X λ π(λ)u(λ), X λ π(λ)v(λ)) ≤ X λ π(λ)f(u(λ), v(λ)). By adding the summation overy on both side of the above inequality, we have X y∈Y (P λ π(λ)P(Mλ(D) = y))α (P λ π(λ)P(Mλ(D′) = y))α−1 ≤ X y∈Y X λ π(λ) P(Mλ(D) = y)α P(Mλ(D′) = y)α−1 = X λ X y∈Y π(λ) P(Mλ(D) = y)α P(Mλ(D′) = y)α−1 ≤ sup λ X y∈Y P(Mλ(D) = y)α P(Mλ(D′) = y)α−1 . The first equality is due to Fubini’s theorem. And the second inequality is straight forward as one observe π(λ) only depends onλ. This concludes the proof as we know that e(α−1)Dα(P+∥P′−) ≤ \u0012C c \u0013α sup λ X y∈Y P(Mλ(D) = y)α P(Mλ(D′) = y)α−1 = \u0012C c \u0013α sup λ e(α−1)Dα(Mλ(D)∥Mλ(D′) or equivalently, Dα(P+∥P′−) ≤ α α − 1 log C c + sup λ∈Λ Dα \u0000 Mλ(D)∥Mλ(D′) \u0001 . We now present our crucial technical lemma for adaptive hyperparameter tuing with any distribution on the number of repetitionsT. This is a generalization from [34]. Lemma A.5.Fix α >1. LetT be a random variable supported onN≥0. Letf : [0, 1] → R be the probability generating function ofK, that is,f(x) = P∞ k=0 P[T = k]xk. Let Mλ and M′ λ be the base algorithm forλ ∈ Λ on Y on D and D′ respectively. Define A1 := A(D, π(0), T , C, c), andA2 := A(D′, π(0), T , C, c). Then Dα (A1∥A2) ≤ sup λ Dα \u0000 Mλ∥M′ λ \u0001 + α α − 1 log C c + 1 α − 1 log \u0010 f′(q)α · f′ \u0000 q′\u00011−α\u0011 , where applying the same postprocessing to the bounding probabilitiesP+ and P′− gives probabilitiesq and q′ respectively. This means that, there exist a function setg : Y →[0, 1] such thatq = E X←P+ [g(X)] and q′ = E X′←P′− [g (X′)]. 21Proof of Lemma A.5.We consider the event thatA1 outputs y. By definition, we have A1(y) = ∞X k=1 P(T = k)[ kY j=1 Pj(≤ y) − kY i=1 Pj(< y)] = ∞X k=1 P(T = k)[ kX i=1 Pi(y) i−1Y j=1 Pj(< y) · kY j=i+1 Pj(≤ y)] ≤ ∞X k=1 P(T = k)[ kX i=1 P+(y) i−1Y j=1 P+(< y) · kY j=i+1 P+(≤ y)] = ∞X k=1 P(T = k)[ kX i=1 P+(y) · P+(< y)i−1 · P+(≤ y)k−i] = ∞X k=1 P(T = k)[P+(≤ y)k − P+(< y)k] = f(P+(≤ y)) − f(P+(< y)) = P+(y) · E X←Uniform([P+(<y),P+(≤y)]) [f′(X)]. The second equality is by partitioning on the events of the first time of gettingy, we usei to index such a time. The third inequality is using(A.7), (A.9), and(A.10). The third to last equality is by (A.12) and algebra. The second to last equality is by definition of the probability generating function f. The last equality follows from definition of integral. Similarly, we have A2(y) ≥ ∞X k=1 P(T = k)[P′−(≤ y)k − P′−(< y)k] = P′−(y) · E X←Uniform([P′−(<y),P′−(≤y)]) [f′(X)]. The rest part of the proof is standard and follows similarly as in [34]. Specifically, we have e(α−1)Dα(A1∥A2) = X y∈Y A1(y)α · A2(y)1−α ≤ X y∈Y P+(y)α · P′−(y)1−α · E X←[P+(<y),P+(≤y)] \u0002 f′(X) \u0003α · E X′←[P′−(<y),P′−(≤y)] \u0002 f′ \u0000 X′\u0001\u00031−α ≤ X y∈Y P+(y)α · P′−(y)1−α · E X←[P+(<y),P+(≤y)] X′←[P′−(<y),P′−(≤y)] h f′(X)α · f′ \u0000 X′\u00011−αi ≤ \u0012C c \u0013α sup λ e(α−1)Dα(Mλ(D)∥Mλ(D′)) · max y∈Y E X←[P+(<y),P+(≤y)] X′←[P′−(<y),P′−(≤y)] h f′(X)α · f′ \u0000 X′\u00011−αi . The last inequality follows from Lemma A.4. The second inequality follows from the fact that, for any α ∈ R, the functionh : (0, ∞)2 → (0, ∞) given byh(u, v) = uα · v1−α is convex. Therefore, E[U]αE[V ]1−α = h(E[(U, V)]) ≤ E[h(U, V)] = E \u0002 Uα · V 1−α\u0003 all positive random variables(U, V). Note that X and X′ are required to be uniform separately, but their joint distribution can be 22arbitrary. As in [34], we will couple them so thatX−P+(<y) P+(y) = X′−P′−(<y) P′−(y) . In particular, this implies that, for eachy ∈ Y, there exists somet ∈ [0, 1] such that E X←[P+(<y),P+(≤y)] X′←[P′−(<y),P′−(≤y)] h f′(X)α · f′ \u0000 X′\u00011−αi ≤ f′(P+(< y)+t·P+(y))α ·f′ \u0000 P′−(< y) + t · P′−(y) \u00011−α Therefore, we have Dα (A1∥A2) ≤sup λ Dα \u0000 Mλ∥M′ λ \u0001 + α α − 1 log C c + 1 α − 1 log  max y∈Y t∈[0,1] f′(P+(< y) + t · P+(y))α · f′ \u0000 P′−(< y) + t · P′−(y) \u00011−α  . To prove the result, we simply fixy∗ ∈ Yand t∗ ∈ [0, 1] achieving the maximum above and define g(y) :=    1 if y < y∗ t∗ if y = y∗ 0 if y > y∗ The result directly follows by settingq = E X←P+ [g(X)] and q′ = E X′←P′− [g (X′)]. Now we can prove Theorem 1, given the previous technical lemma. The proof share similarity to the proof of Theorem 2 in [34] with the key difference from the different form in Lemma A.5. We demonstrate this proof as follows for completeness. Proof of Theorem 1.We first specify the probability generating function of the truncated negative binomial distribution f(x) = E T∼NegBin(θ,γ) \u0002 xT \u0003 = ((1−(1−γ)x)−θ−1 γ−θ−1 if θ ̸= 0 log(1−1−γ)x) log(γ) if θ = 0 Therefore, f′(x) = (1 − (1 − γ)x)−θ−1 · (θ·(1−γ) γ−θ−1 if θ ̸= 0 1−γ log(1/γ) if θ = 0 = (1 − (1 − γ)x)−θ−1 · γθ+1 · E[T]. By Lemma A.5, for appropriate valuesq, q′ ∈ [0, 1] and for allα >1 and all ˆα >1, we have Dα (A1∥A2) ≤ sup λ Dα \u0000 Mλ∥M′ λ \u0001 + α α − 1 log C c + 1 α − 1 log \u0010 f′(q)α · f′ \u0000 q′\u00011−α\u0011 ≤ ε + α α − 1 log C c + 1 α − 1 log \u0010 γθ+1 · E[T] · (1 − (1 − γ)q)−α(θ+1) · \u0000 1 − (1 − γ)q′\u0001−(1−α)(θ+1)\u0011 = ε + α α − 1 log C c 23+ 1 α − 1 log \u0010 γθ+1 · E[T] · \u0010 (γ + (1 − γ)(1 − q))1−ˆα · \u0000 γ + (1 − γ) \u0000 1 − q′\u0001\u0001ˆα\u0011ν · (γ + (1 − γ)(1 − q))u \u0011 (Here, we letˆαν = (α − 1)(1 + θ) and (1 − ˆα)ν + u = −α(θ + 1)) ≤ ε + α α − 1 log C c + 1 α − 1 log \u0010 γθ+1 · E[T] · \u0010 γ + (1 − γ) · e(ˆα−1)Dˆα(P+∥P−)\u0011ν · (γ + (1 − γ)(1 − q))u \u0011 (Here,1 − q and 1 − q′ are postprocessings of someP+ and P′− respectively ande(ˆα−1)Dˆα(·∥·) is convex) ≤ ε + α α − 1 log C c + 1 α − 1 log \u0010 γθ+1 · E[T] · \u0010 γ + (1 − γ) · e(ˆα−1) supλ Dˆα(Mλ∥M′ λ)+ˆα log C c \u0011ν · (γ + (1 − γ)(1 − q))u \u0011 (By Lemma A.4) ≤ ε + α α − 1 log C c + 1 α − 1 log \u0010 γθ+1 · E[T] · \u0010 γ + (1 − γ) · e(ˆα−1) supλ Dˆα(Mλ∥M′ λ)+ˆα log C c \u0011ν · (γ + (1 − γ)(1 − q))u \u0011 ≤ ε + α α − 1 log C c + 1 α − 1 log \u0010 γθ+1 · E[T] · \u0010 γ + (1 − γ) · e(ˆα−1) supλ Dˆα(Mλ∥M′ λ)+ˆα log C c \u0011ν · γu \u0011 (Here γ ≤ γ + (1 − γ)(1 − q) and u ≤ 0) = ε + α α − 1 log C c + ν α − 1 log \u0010 γ + (1 − γ) · e(ˆα−1) supλ Dˆα(Mλ∥M′ λ)+ˆα log C c \u0011 + 1 α − 1 log \u0010 γθ+1 · E[T] · γu \u0011 = ε + α α − 1 log C c + ν α − 1 \u0012 (ˆα − 1) sup λ Dˆα \u0000 Mλ∥M′ λ \u0001 + ˆα log C c + log \u0010 1 − γ · \u0010 1 − e−(ˆα−1) supλ Dˆα(Mλ∥M′ λ)+ˆα log C c \u0011\u0011\u0011 + 1 α − 1 log \u0010 γu+θ+1 · E[T] \u0011 = ε + α α − 1 log C c + (1 +θ) \u0012 1 − 1 ˆα \u0013 sup λ Dˆα \u0000 Mλ∥M′ λ \u0001 + (1 +θ) log C c + 1 + θ ˆα log \u0010 1 − γ · \u0010 1 − e−(ˆα−1) supλ Dˆα(Mλ∥M′ λ)+ˆα log C c \u0011\u0011 + log(E[T]) α − 1 + 1 + θ ˆα log(1/γ) (Here we haveν = (α − 1)(1 + θ) ˆα and u = −(1 + θ) \u0012α − 1 ˆα + 1 \u0013 ) = ε + α α − 1 log C c + (1 +θ) \u0012 1 − 1 ˆα \u0013 sup λ Dˆα \u0000 Mλ∥M′ λ \u0001 + (1 +θ) log C c + 1 + θ ˆα log \u00121 γ − 1 + e−(ˆα−1) supλ Dˆα(Mλ∥M′ λ)−ˆα log C c \u0013 + log(E[T]) α − 1 ≤ ε + α α − 1 log C c + (1 +θ) \u0012 1 − 1 ˆα \u0013 ˆε + (1 +θ) log C c + 1 + θ ˆα log \u00121 γ \u0013 + log(E[T]) α − 1 , which completes the proof. B Truncated Negative Binomial Distribution We introduce the definition of truncated negative binomial distribution [34] in this section. Definition B.1.(Truncated Negative Binomial Distribution [34]). Let γ ∈ (0, 1) and θ ∈ (−1, ∞). Define a distribution NegBin(θ, γ) on N+ as follows: 24• If θ ̸= 0 and T is drawn from NegBin(θ, γ), then ∀k ∈ N P [T = k] = (1 − γ)k γ−θ − 1 · k−1Y ℓ=0 \u0012ℓ + θ ℓ + 1 \u0013 and E[T] = θ·(1−γ) γ·(1−γθ). Note that whenθ = 1, it reduces to the geometric distribution with parameter γ. • If θ = 0 and T is drawn from NegBin(0, γ), then P[T = k] = (1 − γ)k k · log(1/γ) and E[T] = 1/γ−1 log(1/γ). C Privatization of Sampling Distribution C.1 General Functional Projection Framework In section 3.2, we define the projection onto a convex setSC,c as an optimization in terms ofℓ2 loss. More generally, we can perform the following general projection at thej-th iteration by considering an additional penalty term, with a constantν: min f ∥f − π(j)∥2 + νKL(π(j), f) (C.1) s.t. f ∈ SC,c. When ν = 0, we recover the originalℓ2 projection. Moreover, it’s worth noting that our formulation has implications for the information projection literature [10, 23]. Specifically, as the penalty term parameterν approaches infinity, the optimization problem evolves into a minimization of KL divergence, recovering the objective function of information projection (in this instance, moment projection). However, the constraint sets in the literature of information projection are generally much simpler than our setSC,c, making it infeasible to directly borrow methods from its field. To the best of our knowledge, our framework is the first to address this specific problem in functional projection and establish a connection to information projection in the DP community. C.2 Practical Implementation of Functional Projection Optimization program (3.1) is essentially a functional programming sincef is a function onΛ. However, whenΛ represents a non-discrete parameter space, such functional minimization is typically difficulttosolveanalytically. Evenwithintheliteratureofinformationprojection, noneofthemethods considers our constraint setSC,c, which can be viewed as the intersections of uncountable single-point constraints onf. To obtain a feasible solution to the optimization problem, we leverage the idea of discretization. Instead of viewing(3.1) as a functional projection problem, we manually discretize Λ and solve(3.1) as a minimization problem over a discrete set. Note that such approximation is unavoidable in numerical computations since computers can only manage discrete functions, even when we solve the functional projection analytically. Moreover, we also have the freedom of choosing 25the discretization grid without incurring extra privacy loss since the privacy cost is independent of the size of parameter space. By convertingSC,c into a set of finite constraints, we are able to solve the discrete optimization problem efficiently using CVXOPT [2]. D DP-HyPO with General Prior Distribution In the main manuscript, we assumeπ(0) follows a uniform distribution over the parameter spaceΛ for simplicity. In practice, informed priors can be used when we want to integrate knowledge about the parameter space into sampling distribution, which is common in the Bayesian optimization framework. We now present the general DP-HyPO framework under the informed prior distribution. To begin with, we define the space of essentially bounded density functions with respect toπ(0) as SC,c(π(0)) = {f ∈ ΛR+ : ess supf/π(0) ≤ C, ess inff/π(0) ≥ c, Z α∈Λ f(α)dα = 1, f≪ π(0)}. When π(0) = 1 µ(λ), we recover the original definition ofSC,c. Note that heref ≪ π(0) means thatf is absolute continuous with respect to the prior distributionπ(0) and this ensures thatSC,c(π(0)) is non-empty. Note that such condition is automatically satisfied whenπ(0) is the uniform prior over the entire parameter space. To define the projection of a density at thej-th iteration, π(j), into the spaceSC,c(π(0)), we consider the following functional programming problem: min f ∥f − π(j)∥2 s.t. f ∈ SC,c(π(0)), which is a direct generalization of Equation (3.1). As before,SC,c(π(0)) is also convex and closed and the optimization program can be solved efficiently via discretization onΛ. E Experiment Details E.1 MNIST Simulation We now provide the detailed description of the experiment in Section 4.2.1. As specified therein, we consider two variable hyperparameters: the learning rateη and clipping normR, while keeping all the other hyperparameters fixed. We set the training batch size to be256, and the total number of epoch to be10. The value ofσ is determined based on the allocatedε budget for each base algorithm. Specifically,σ = 0.71 for GP andσ = 0.64 for Uniform. For demonstration purposes, we set C to 2 andc to 0.75 in the GP method, so each base algorithm of Uniform haslog C/c more privacy budget than base algorithms in GP method. In Algorithm 3, we setτ to 0.1 andβ to 1. To facilitate the implementation of both methods, we discretize the learning rates and clipping norms as specified in the following setting to allow simple implementation of sampling and projection for Uniform and GP methods. Setting E.1.we set a log-spaced grid discretization onη in the range[0.0001, 10] with a multiplicative factor of 3√ 10, resulting in16 observations forη. We also set a linear-spaced grid discretization onR 26in the range[0.3, 6] with an increment of0.3, resulting in20 observations forR. This gives a total of 320 hyperparameters over the search region. We specify the network structure we used in the simulation as below. It is the standard CNN in Tensorflow Privacy and Opacus. class ConvNet(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 16, 8, 2, padding=3) self.conv2 = nn.Conv2d(16, 32, 4, 2) self.fc1 = nn.Linear(32 * 4 * 4, 32) self.fc2 = nn.Linear(32, 10) def forward(self, x): x = F.relu(self.conv1(x)) x = F.max_pool2d(x, 2, 1) x = F.relu(self.conv2(x)) x = F.max_pool2d(x, 2, 1) x = x.view(-1, 32 * 4 * 4) x = F.relu(self.fc1(x)) x = self.fc2(x) return x Despite the simple nature of MNIST, the simulation of training CNN with the two methods over each different fixedT still take significant computation resources. Due to the constraints on computational resources, we conduct a semi-real simulation using the MNIST dataset. We cache the mean accuracy of5 independently trained models for each discretized hyperparameter and treat that as a proxy for the “actual accuracy” of the hyperparameter. Each time we sample the accuracy of a hyperparameter, we add Gaussian noise with a standard deviation of0.1 to the cached mean. We evaluate the performance of the output model based on the “actual accuracy” corresponding to the selected hyperparameter. E.2 CIFAR-10 Simulation We also provide a description of the experiment in Section 4.2.2. We set the training batch size to be 256, and the total number of epoch to be10. The value ofσ is determined based on the allocatedε budget for each base algorithm. Specifically,σ = 0.65 for GP andσ = 0.6 for Uniform. Regarding our GP method, we adopt the same set of hyperparameters as used in our MNIST experiments, which includeC = 2, c = 0.75, τ = 0.1, andβ = 1. As usual, we discretize the learning rates and clipping norms as specified in the following Setting. Setting E.2.we set a log-spaced grid discretization onη in the range[0.0001, 1] with a multiplicative factor of100.1, resulting in50 observations forη. We also set a linear-spaced grid discretization on R in the range[0, 100] with an increment of2, resulting in50 observations forR. This gives a total of 2500 hyperparameter combinations over the search region. We follow the same CNN model architecture with our MNIST experiments. 27In Figure 2, we provide the hyperparameter landscape forσ = 0.65, as generated by BoTorch [3]. Figure 2: Mean and standard error of the accuracy of DP-SGD over the two hyperparameters for σ = 0.65. The learning rate (log-scale) ranges from0.00001 (left) to 1 (right) while the clipping norm ranges from 0 (top) to 100 (bottom). The landscape forσ = 0.6 is similar, with a better accuracy. E.3 Federated Learning Simulation Figure 3: Mean and Standard Error of the loss of the FL over the two hyperparameters. We now provide the detailed description of the experiment in Section 4.2.3. As specified therein, we considered a FL task on a proprietary dataset5. Our objective is to determine the optimal learning rates for the central server (using AdaGrad) and the individual users (using SGD). To simulate this scenario, we utilize the landscape generated by BoTorch [3], as illustrated in Figure 3, and consider it as our reference landscape for both mean and standard deviation of the loss for each hyperparameter. When the algorithm (GP or Uniform) visits a specific hyperparameterλ, our oracle returns a noisy scoreq(λ) drawn from a normal distributionN(µλ, σλ). Figure 3 displays a heatmap that presents the mean (µλ) and standard error (σλ) structure of the loss over these two hyperparameters, providing insights into the landscape’s characteristics. 5We are unable to report a lot of detail about the proprietary dataset due to confidentiality. 28F Additional Related Work In this section, we delve into a more detailed review of the pertinent literature. We begin with non-private Hyperparameter Optimization, a critical topic in the realm of Auto- mated Machine Learning (AutoML) [18]. The fundamental inquiry revolves around the generation of high-performing models within a specific search space. In historical context, two types of optimiza- tions have proven significant in addressing this inquiry: architecture optimization and hyperparameter optimization. Architecture optimization pertains to model-specific parameters such as the number of neural network layers and their interconnectivity, while hyperparameter optimization concerns training-specific parameters, including the learning rate and minibatch size. In our paper, we incorpo- rate both types of optimizations within our HPO framework. Practically speaking,Λ can encompass various learning rates and network architectures for selection. For HPO, elementary methods include grid search and random search [24, 19, 16]. Progressing beyond non-adaptive random approaches, surrogate model-based optimization presents an adaptive method, leveraging information from preceding results to construct a surrogate model of the objective function [28, 43, 22, 32]. These methods predominantly employs Bayesian optimization techniques, including Gaussian process [35], Random Forest [20], and tree-structured Parzen estimator [5]. Another important topic in this paper is Differential Privacy (DP). DP offers a mathematically robust framework for measuring privacy leakage. A DP algorithm promises that an adversary with perfect information about the entire private dataset in use – except for a single individual – would find it hard to distinguish between its presence or absence based on the output of the algorithm [13]. Historically, DP machine learning research has overlooked the privacy cost associated with HPO [1, 41, 44]. The focus has only recently shifted to the “honest HPO” setting, where this cost is factored in [30]. Addressing this issue directly involves employing a composition-based analysis. If each training run of a hyperparameter upholds DP, then the overall HPO procedure adheres to DP through composition across all attempted hyperparameter values. A plethora of literature on the composition of DP mechanisms attempts to quantify a better DP guarantee of the composition. Vadhan et al. [38] demonstrated that though(ε, δ)-DP possesses a simple mathematical form, deriving the precise privacy parameters of a composition is #-P hard. Despite this obstacle, numerous advanced techniques are available to calculate a reasonably accurate approximation of the privacy parameters, such as Moments Accountant [1], GDP Accountant [12], and Edgeworth Accountant [39]. The efficacy of these accountants is attributed to the fact that it is easier to reason about the privacy guarantees of compositions within the framework of Rényi differential privacy [29] or f-differential privacy [12]. These methods have found widespread application in DP machine learning. For instance, when training deep learning models, one of the most commonly adopted methods to ensure DP is via noisy stochastic gradient descent (noisy SGD) [4, 37], which uses Moments Accountant to better quantify the privacy guarantee. Although using composition for HPO is a simple and straightforward approach, it carries with it a significant challenge. The privacy guarantee derived from composition accounting can be excessively loose, scaling polynomially with the number of runs. Chaudhuri et al. [8] were the first to enhance the DP bounds for HPO by introducing additional stability assumptions on the learning algorithms. [26] made significant progress in enhancing DP bounds for HPO without relying on any stability properties of the learning algorithms. They proposed a simple procedure where a hyperparameter was randomly selected from a uniform distribution for each training run. This selection process was repeated a random number of times according to a geometric distribution, and the best model obtained from these runs was outputted. They showed that this procedure satisfied 29(3ε, 0)-DP as long as each training run of a hyperparameter was(ε, 0)-DP. Building upon this, [34] extended the procedure to accommodate negative binomial or Poisson distributions for the repeated uniform selection. They also offered more precise Rényi DP guarantees for this extended procedure. Furthermore, [9] explored a generalization of the procedure for top-k selection, considering (ε, δ)-DP guarantees. In a related context, [30] explored a setting that appeared superficially similar to ours, as their title mentioned “adaptivity.” However, their primary focus was on improving adaptive optimizers such as DP-Adam, which aimed to reduce the necessity of hyperparameter tuning, rather than the adaptive HPO discussed in this paper. Notably, in terms of privacy accounting, their approach only involved composing the privacy cost of each run without proposing any new method. Another relevant area of research is DP selection, which encompasses well-known methods such as the exponential mechanism [27] and the sparse vector technique [14], along with subsequent studies (e.g., [6] and [17]). However, this line of research always assumes the existence of a low- sensitivity score function for each candidate, which is an unrealistic assumption for hyperparameter optimization. 30",
      "meta_data": {
        "arxiv_id": "2306.05734v2",
        "authors": [
          "Hua Wang",
          "Sheng Gao",
          "Huanyu Zhang",
          "Weijie J. Su",
          "Milan Shen"
        ],
        "published_date": "2023-06-09T07:55:46Z",
        "pdf_url": "https://arxiv.org/pdf/2306.05734v2.pdf"
      }
    },
    {
      "title": "Differentiable Expected Hypervolume Improvement for Parallel Multi-Objective Bayesian Optimization",
      "abstract": "In many real-world scenarios, decision makers seek to efficiently optimize\nmultiple competing objectives in a sample-efficient fashion. Multi-objective\nBayesian optimization (BO) is a common approach, but many of the\nbest-performing acquisition functions do not have known analytic gradients and\nsuffer from high computational overhead. We leverage recent advances in\nprogramming models and hardware acceleration for multi-objective BO using\nExpected Hypervolume Improvement (EHVI)---an algorithm notorious for its high\ncomputational complexity. We derive a novel formulation of q-Expected\nHypervolume Improvement (qEHVI), an acquisition function that extends EHVI to\nthe parallel, constrained evaluation setting. qEHVI is an exact computation of\nthe joint EHVI of q new candidate points (up to Monte-Carlo (MC) integration\nerror). Whereas previous EHVI formulations rely on gradient-free acquisition\noptimization or approximated gradients, we compute exact gradients of the MC\nestimator via auto-differentiation, thereby enabling efficient and effective\noptimization using first-order and quasi-second-order methods. Our empirical\nevaluation demonstrates that qEHVI is computationally tractable in many\npractical scenarios and outperforms state-of-the-art multi-objective BO\nalgorithms at a fraction of their wall time.",
      "full_text": "Differentiable Expected Hypervolume Improvement for Parallel Multi-Objective Bayesian Optimization Samuel Daulton Facebook sdaulton@fb.com Maximilian Balandat Facebook balandat@fb.com Eytan Bakshy Facebook ebakshy@fb.com Abstract In many real-world scenarios, decision makers seek to efﬁciently optimize multiple competing objectives in a sample-efﬁcient fashion. Multi-objective Bayesian opti- mization (BO) is a common approach, but many of the best-performing acquisition functions do not have known analytic gradients and suffer from high computational overhead. We leverage recent advances in programming models and hardware acceleration for multi-objective BO using Expected Hypervolume Improvement (EHVI )—an algorithm notorious for its high computational complexity. We derive a novel formulation of q-Expected Hypervolume Improvement (qEHVI ), an acqui- sition function that extends EHVI to the parallel, constrained evaluation setting. qEHVI is an exact computation of the joint EHVI of qnew candidate points (up to Monte-Carlo (MC) integration error). Whereas previous EHVI formulations rely on gradient-free acquisition optimization or approximated gradients, we compute exact gradients of the MC estimator via auto-differentiation, thereby enabling efﬁ- cient and effective optimization using ﬁrst-order and quasi-second-order methods. Our empirical evaluation demonstrates that qEHVI is computationally tractable in many practical scenarios and outperforms state-of-the-art multi-objective BO algorithms at a fraction of their wall time. 1 Introduction The problem of optimizing multiple competing objectives is ubiquitous in scientiﬁc and engineering applications. For example in automobile design, an automaker will want to maximize vehicle durability and occupant safety, while using lighter materials that afford increased fuel efﬁciency and lower manufacturing cost [44, 72]. Evaluating the crash safety of an automobile design experimentally is expensive due to both the manufacturing time and the destruction of a vehicle. In such a scenario, sample efﬁciency is paramount. For a different example, video streaming web services commonly use adaptive control policies to determine the bitrate as the stream progresses in real time [47]. A decision maker may wish to optimize the control policy to maximize the quality of the video stream, while minimizing the stall time. Policy evaluation typically requires using the suggested policy on segments of live trafﬁc, which is subject to opportunity costs. If long evaluation times are the limiting factor, multiple designs may be evaluated in parallel to signiﬁcantly decrease end-to-end optimization time. For example, an automaker could manufacture multiple vehicle designs in parallel or a web service could deploy several control policies to different segments of trafﬁc at the same time. 1.1 Background Multi-Objective Optimization: In this work, we address the problem of optimizing a vector-valued objective f(x) : Rd →RM with f(x) = ( f(1)(x),...,f (M)(x) ) over a bounded set X ⊂Rd. We consider the scenario in which the f(i) are expensive-to-evaluate black-box functions with 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada. arXiv:2006.05078v3  [stat.ML]  23 Oct 2020no known analytical expression, and no observed gradients. Multi-objective (MO) optimization problems typically do not have a single best solution; rather, the goal is to identify the set of Pareto optimal solutions such that any improvement in one objective means deteriorating another. Without loss of generality, we assume the goal is to maximize all objectives. We say a solution f(x) Pareto dominates another solution f(x′) if f(m)(x) ≥ f(m)(x′) ∀m = 1 ,...,M and there exists m′ ∈{1,...,M }such that f(m′)(x) > f(m′)(x′). We write f(x) ≻f(x′). Let P∗= {f(x) s.t. ∄ x′∈X : f(x′) ≻f(x)}and X∗= {x ∈X s.t. f(x) ∈P∗}denote the set of Pareto optimal solutions and Pareto optimal inputs, respectively. Provided with the Pareto set, decision-makers can select a solution with an objective trade-off according to their preferences. A common approach for solving MO problems is to use evolutionary algorithms (e.g. NSGA-II), which are robust multi-objective optimizers, but require a large number of function evaluations [14]. Bayesian optimization (BO) offers a far more sample-efﬁcient alternative [57]. Bayesian Optimization: BO [38] is an established method for optimizing expensive-to-evaluate black-box functions. BO relies on a probabilistic surrogate model, typically a Gaussian Process (GP) [55], to provide a posterior distribution P(f|D) over the true function values f given the observed data D= {(xi,yi)}n i=1. An acquisition function α : Xcand ↦→R employs the surrogate model to assign a utility value to a set of candidates Xcand = {xi}q i=1 to be evaluated on the true function. While the true f may be expensive-to-evaluate, the surrogate-based acquisition function is not, and can thus be efﬁciently optimized to yield a set of candidates Xcand to be evaluated on f. If gradients of α(Xcand) are available, gradient-based methods can be utilized. If not, gradients are either approximated (e.g. with ﬁnite differences) or gradient-free methods (e.g. DIRECT [ 37] or CMA-ES [32]) are used. 1.2 Limitations of current approaches In the single-objective (SO) setting, a large body of work focuses on practical extensions to BO for supporting parallel evaluation and outcome constraints [49, 30, 66, 25, 43]. Less attention has been given to such extensions in the MO setting. Moreover, the existing constrained and parallel MO BO options have limitations: 1) many rely on scalarizations to transform the MO problem into a SO one [40]; 2) many acquisition functions are computationally expensive to compute [52, 21, 6, 71]; 3) few have known analytical gradients or are differentiable [19, 62, 33]; 4) many rely on heuristics to extend sequential algorithms to the parallel setting [27, 62]. A natural acquisition function for MO BO is Expected Hypervolume Improvement (EHVI ). Max- imizing the hypervolume ( HV) has been shown to produce Pareto fronts with excellent cover- age [73, 12, 69]. However, there has been little work on EHVI in the parallel setting, and the work that has been done resorts to approximate methods [71, 28, 62]. A vast body of literature has focused on efﬁcient EHVI computation [34, 20, 67], but the time complexity for computing EHVI is exponential in the number of objectives—in part due the hypervolume indicator itself incurring a time complexity that scales super-polynomially with the number of objectives [68]. Our core insight is that by exploiting advances in auto-differentiation and highly parallelized hardware [51], we can make EHVI computations fast and practical. 1.3 Contributions In this work, we derive a novel formulation of the parallelq-Expected Hypervolume Improvement acquisition function (qEHVI ) that is exact up to Monte-Carlo (MC) integration error. We compute the exact gradient of the MC estimator of qEHVI using auto-differentiation, which allows us to employ efﬁcient and effective gradient-based optimization methods. Rather than using ﬁrst-order gradient methods, we instead leverage the sample average approximation (SAA) approach from [5] to use higher-order deterministic optimization methods, and we prove theoretical convergence guarantees under the SAA approach. Our formulation of qEHVI is embarrassingly parallel, and despite its computational cost would achieve constant time complexity given inﬁnite processing cores. We demonstrate that, using modern GPU hardware and computing exact gradients, optimizing qEHVI is faster than existing state-of-the art methods in many practical scenarios. Moreover, we extend qEHVI to support auxiliary outcome constraints, making it practical in many real-world scenarios. Lastly, we demonstrate how modern auto-differentiation can be used to compute exact gradients of analytic EHVI , which has never been done before for M >2 objectives. Our empirical evaluation 2shows that qEHVI outperforms state-of-the-art multi-objective BO algorithms while using only a fraction of their wall time. 2 Related Work Yang et al. [69] is the only previous work to consider exact gradients of EHVI, but the authors only derive an analytical gradient for the unconstrained two-objective, sequential optimization setting. All other works either do not optimize EHVI (e.g. they use it for pre-screening candidates [ 18]), optimize it with gradient-free methods [68], or using approximate gradients [62]. In contrast, we use exact gradients and demonstrate that optimizing EHVI using this gradient information is far more efﬁcient. There are many alternatives to EHVI for MO BO. For example, ParEGO [ 40] and TS-TCH [50] randomly scalarize the objectives and use Expected Improvement [38] and Thompson Sampling [61], respectively. SMS-EGO [53] uses HV in a UCB-based acquisition function and is more scalable than EHVI [54]. ParEGO and SMS-EGO have only been considered for the q= 1, unconstrained setting. Predictive entropy search for MO BO (PESMO) [ 33] has been shown to be another competitive alternative and has been extended to handle constraints [ 26] and parallel evaluations [ 27]. MO max-value entropy search (MO-MES) has been shown to achieve superior optimization performance and faster wall times than PESMO, but is limited to q= 1. Wilson et al. [65] empirically and theoretically show that sequential greedy selection of qcandidates achieves performance comparable to jointly optimizing qcandidates for many acquisition functions (including [63, 66]). The sequential greedy approach integrates over the posterior of the unobserved outcomes corresponding to the previously selected candidates in the q-batch. Sequential greedy optimization often yields better empirical results because the optimization problem has a lower dimension: din each step, rather than qdin the joint problem. Most prior works in the MO setting use a sequential greedy approximation or heuristics [62, 71, 28, 10], but impute the unobserved outcomes with the posterior mean rather than integrating over the posterior [30]. For many joint acquisition functions involving expectations, this shortcut sacriﬁces the theoretical error bound on the sequential greedy approximation because the exact joint acquisition function overx1,..., xi, 1 ≤i≤qrequires integration over the joint posterior P(f(x1),..., f(xq)|D) and is not computed for i> 1. Garrido-Merchán and Hernández-Lobato [27] and Wada and Hino [62] jointly optimize the qcandi- dates and, noting the difﬁculty of the optimization, both papers focus on deriving gradients to aid in the optimization. Wada and Hino [62] deﬁned the qEHVI acquisition function, but after ﬁnding it challenging to optimize qcandidates jointly (without exact gradients), the authors propose optimizing an alternative acquisition function instead of exactqEHVI . In contrast, our novel qEHVI formulation allows for gradient-based parallel and sequential greedy optimization, with proper integration over the posterior for the latter. Feliot et al. [22] and Abdolshah et al. [1] proposed extensions of EHVI to the constrained q = 1 setting, but neither considers the batch setting and both rely on gradient-free optimization. 3 Differentiable q-Expected Hypervolume Improvement In this section, we review HVI and EHVI computation by means of box decompositions, and explain our novel formulation for the parallel setting. Deﬁnition 1. Given a reference point r ∈RM , the hypervolume indicator (HV) of a ﬁnite approxi- mate Pareto set Pis the M-dimensional Lebesgue measure λM of the space dominated by Pand bounded from below byr: HV(P,r) = λM (⋃|P| i=1[r,yi] ) , where [r,yi] denotes the hyper-rectangle bounded by vertices r and yi. Deﬁnition 2. Given a Pareto set Pand reference point r, the hypervolume improvement (HVI) of a set of points Yis: HVI (Y,P,r) = HV(P∪Y ,r) −HV(P,r).1 EHVI is the expectation of HVI over the posterior P(f,D): αEHVI (Xcand) = E [ HVI (f(Xcand)) ] . In the sequential setting, and assuming the objectives are independent and modeled with independent 1In this work, we omit the arguments Pand rwhen referring to HVI for brevity. 3GPs, EHVI can be expressed in closed form [69]. In other settings, EHVI can be approximated with MC integration. Following previous work, we assume that the reference point is known and speciﬁed by the decision maker [69] (see Appendix E.1.1 for additional discussion). 3.1 A review of hypervolume improvement computation using box decompositions Deﬁnition 3. For a set of objective vectors {f(xi)}q i=1, a reference point r ∈RM , and a non- dominated set P, let ∆({f(xi)}q i=1,P,r) ⊂RM denote the set of points (i) are dominated by {f(xi)}q i=1, dominate r, and are not dominated by P. Given P,r, the HVI of a new point f(x) is the HV of the intersection of space dominated by P∪{ f(x)}and the non-dominated space. Figure 1b illustrates this for one new point f(x) for M = 2. The yellow region is ∆({f(x)},P,r) and the hypervolume improvement is the volume covered by ∆({f(x)},P,r). Since ∆({f(x)},P,r) is often a non-rectangular polytope, HVI is typically computed by partitioning the non-dominated space into disjoint axis-parallel rectangles [12, 68] (see Figure 1a) and using piece-wise integration [18]. Let {Sk}K k=1 be a partitioning the of non-dominated space into disjoint hyper-rectangles, where each Sk is deﬁned by a pair of lower and upper vertices lk ∈RM and uk ∈RM ∪{∞}. The high level idea is to sum the HV ofSk ∩∆({f(x)},P,r) over all Sk. For each hyper-rectangle Sk, the intersec- tion of Sk and ∆({f(x)},P,r) is a hyper-rectangle where the lower bound vertex islk and the upper bound vertex is the component-wise minimum ofuk and the new pointf(x): zk := min [ uk,f(x) ] . r l3 S3 u3u2 S2 l2 l4 S4 u4u1 l1 S1 f (2)(x) f (1)(x) (a) f(x1) r l3 S3 u3u2 S2 l2 l4 S4 u4u1 l1 S1 f(2)(x ) f(1)(x ) (b) f(x1) f(2)(x ) f(1)(x )r l3 S3 u3u2 S2 l2 l4 S4 u4u1 l1 S1 f(x2)  (c) Figure 1: For M=2, (a) the dominated space (red) and the non-dominated space partitioned into disjoint boxes (white), (b) the HVI of one new point f(x), and (c) the HVI of two new points f(x1),f(x2). Hence, the HVI of a single outcome vector f(x) within Sk is given by HVI k ( f(x),lk,uk ) = λM ( Sk ∩∆({f(x)},P,r) ) = ∏M m=1 [ z(m) k −l(m) k ] +, where u(m) k ,l(m) k ,f(m)(x), and z(m) k denote the mth component of the corresponding vector and [·]+ denotes the min(·,0) operation. Summing over rectangles yields HVI ( f(x) ) = K∑ k=1 HVIk ( f(x),lk,uk ) = K∑ k=1 M∏ m=1 [ z(m) k −l(m) k ] + (1) 3.2 Computing q-Hypervolume Improvement via the Inclusion-Exclusion Principle Figure 1c illustrates the HVI in the q = 2 setting. Given q new points {f(xi)}q i=1, let Ai := ∆({f(xi)},P,r) for i= 1,...,q be the space dominated by f(xi) but not dominated by P, independently of the other q−1 points. Note that λM (Ai) = HVI (f(xi)). The union of the subsets Ai is the space dominated jointly by the qnew points: ⋃q i=1 Ai = ⋃q i=1 ∆({f(xi)},P,r), and the Lebesgue measure λM (⋃q i=1 Ai ) is the joint HVI from the qnew points. Since each subspace Ai is bounded, the restricted Lebesgue measure is ﬁnite and we may compute λM (⋃q i=1 Ai ) using the inclusion-exclusion principle [13, 59]: HVI ({f(xi)}q i=1) = λM ( q⋃ i=1 Ai ) = q∑ j=1 (−1)j+1 ∑ 1≤i1≤...≤ij≤q λM ( Ai1 ∩···∩ Aij ) (2) 4Since {Sk}K k=1 is a disjoint partition, λM (Ai1 ∩···∩ Aij ) = ∑K k=1 λM (Sk ∩Ai1 ∩···∩ Aij ), we can compute λM (Ai1 ∩···∩ Aij ) in a piece-wise fashion across the Khyper-rectangles {Sk}K k=1 as the HV of the intersection of Ai1 ∩···∩ Aij with each hyper-rectangle Sk. The inclusion- exclusion principle has been proposed for computingHV (not HVI) [45], but it is rarely used because complexity scales exponentially with the number of elements. However, the inclusion-exclusion principle is practical for computing the joint HVI of qpoints since typically q <<|P|. This formulation has three advantages. First, while the new dominated space Ai can be a non-rectangular polytope, the intersection Ai ∩Sk is a rectangular polytope, which simpliﬁes computation of overlapping hypervolume. Second, the vertices deﬁning the hyper-rectangle Sk ∩Ai1 ∩···∩ Aij are easily derived. The lower bound is simply the lk lower bound of Sk, and the upper bound is the component-wise minimum zk,i1,...ij := min [ uk,f(xi1 ),..., f(xij ) ] . Third, computation can be across all intersections of subsets Ai1 ∩···∩ Aij for 1 ≤ij ≤... ≤ij ≤qand across all Khyper-rectangles can be performed in parallel. Explicitly, the HVI is computed as: HVI ({f(xi)}q i=1) = K∑ k=1 q∑ j=1 ∑ Xj∈Xj (−1)j+1 M∏ m=1 [ z(m) k,Xj −l(m) k ] + (3) where Xj := {Xj ⊂ Xcand : |Xj| = j}is the superset of all subsets of Xcand of size j, and z(m) k,Xj := z(m) k,i1,...ij for Xj = {xi1 ,..., xij }. See Appendix A for further details of the derivation. 3.3 Computing Expected q-Hypervolume Improvement The above approach for computing HVI assumes that we know the true objective values f(Xcand) = {f(xi)}q i=1. In BO, we instead compute qEHVI as the expectation over the posterior model posterior: αqEHVI (Xcand) = E [ HVI (f(Xcand)) ] = ∫ ∞ −∞ HVI(f(Xcand))df. (4) Since no known analytical form is known [ 70] for q > 1 (or in the case of correlated out- comes), we estimate (4) using MC integration with samples from the joint posterior {ft(xi)}q i=1 ∼ P ( f(x1),..., f(xq)|D ) ,t = 1,...N . Let z(m) k,Xj,t := min [ uk,minx′∈Xj ft(x′) ] . Then, ˆαN qEHVI (Xcand) = 1 N N∑ t=1 HVI(ft(Xcand)) = 1 N N∑ t=1 K∑ k=1 q∑ j=1 ∑ Xj∈Xj (−1)j+1 M∏ m=1 [ z(m) k,Xj,t −l(m) k ] + (5) Provided that {Sk}K k=1 is an exact partitioning, (5) is an exact computation of qEHVI up to the MC estimation error, which scales as 1/ √ N when using iidMC samples regardless of the dimension of the search space [18]. In practice, we use randomized quasi MC methods [8] to reduce the variance and empirically observe low estimation error (see Figure 5a in the Appendix for a comparison of analytic EHVI and (quasi-)MC-based qEHVI). qEHVI requires computing the volume of 2q −1 hyper-rectangles (the number of subsets of q) for each of Khyper-rectangles and N MC samples. Given posterior samples, the time complexity on a single-threaded machine is: T1 = O(MNK(2q −1)). In the two-objective case, K = |P|+ 1, but K is super-polynomial in M [68]. The number of boxes required for a decomposition of the non-dominated space is unknown for M ≥4 [68]. qEHVI is agnostic to the partitioning algorithm used, and in F.4, we demonstrate using qEHVI in higher-dimensional objective spaces using an approximate box decomposition algorithm [11]. Despite the daunting workload, the critical work path—the time complexity of the smallest non-parallelizable unit—is constant: T∞= O(1).2 On highly-threaded many-core hardware (e.g. GPUs), our formulation achieves tractable wall times in many practical scenarios: as is shown in Figure 11 in the Appendix, the computation time is nearly constant with increasing quntil an inﬂection point at which the workload saturates the available cores. For additional discussion of both time and memory complexity of qEHVI see Appendix A.4. 3.4 Outcome Constraints Our proposed qEHVI acquisition function is easily extended to constraints on auxiliary outcomes. We consider the scenario where we receive observations of M objectives f(x) ∈RM and V constraints 2As evident from (5), the critical path consists of 3 multiplications and 5 summations. 5c(v) ∈RV , all of which are assumed to be “black-box”. We assume w.l.o.g. that c(v) is feasible iff c(v) ≥0. In the constrained optimization setting, we aim to identify the feasible Pareto set: Pfeas = {f(x) s.t. c(x) ≥0, ∄ x′ : c(x′) ≥0, f(x′) ≻f(x)}. The natural improvement measure in the constrained setting is feasible HVI, which we deﬁne for a single candidate point x as HVI C(f(x),c(x)) := HVI [f(x)] ·1 [c(x) ≥0]. Taking expectations, the constrained expected HV can be seen to be the HV weighted by the probability of feasibility. In Appendix A.3, we detail how performing feasibility-weighting on the sample-level allows us to include such auxiliary outcome constraints into our MC formulation in a straightforward way. 4 Optimizing q-Expected Hypervolume Improvement 4.1 Differentiability While an analytic formula for the gradient of EHVI exists for the M = 2 objective case in the unconstrained, sequential ( q = 1 ) setting, no such formula is known in 1) the case of M > 2 objectives, 2) the constrained setting, and 3) for q >1. Leveraging the re-parameterization trick [39, 64] and auto-differentiation, we are able to automatically compute exact gradients of the MC- estimator qEHVI in all of the above settings, as well as the gradient of analytic EHVI for M ≥2 (see Figure 5b in the Appendix for a comparison of the exact gradients of EHVI and the sample average gradients of qEHVI for M = 3).3,4 4.2 Optimization via Sample Average Approximation We show in Appendix C that if mean and covariance function of the GP are sufﬁciently regular, the gradient of the MC estimator (5) is an unbiased estimate of the gradient of the exact acquisition function (4). To maximize qEHVI , we could therefore directly apply stochastic optimization methods, as has previously been done for single-outcome acquisition functions [ 64, 66]. Instead, we opt to use the sample average approximation (SAA) approach from Balandat et al. [5], which allows us to employ deterministic, higher-order optimizers to achieve faster convergence rates. Informally (see Appendix C for the formal statement), if ˆx∗ N ∈arg maxx∈X ˆαN qEHVI (x), we can show under some regularity conditions that, as N →∞, (i) ˆαN qEHVI (ˆx∗ N ) →maxx∈XαqEHVI (x) a.s., and (ii) dist ( ˆx∗ N ,arg maxx∈XαqEHVI (x) ) →0 a.s.. These results hold for any covariance function satisfying the regularity conditions, including such ones that model correlation between outcomes. In particular, our results do not require the outputs to be modeled by independent GPs. Figure 2a demonstrates the importance of using exact gradients for efﬁciently and effectively op- timizing EHVI and qEHVI by comparing the following optimization methods: L-BFGS-B with exact gradients, L-BFGS-B with gradients approximated via ﬁnite differences, and CMA-ES (without gradients). The cumulative time spent optimizing the acquisition function is an order of magnitude less when using exact gradients rather than approximate gradients or zeroth order methods. 4.3 Sequential Greedy and Joint Batch Optimization Jointly optimizing qcandidates increases in difﬁculty with qbecause the problem dimension is dq. An alternative is to sequentially and greedily select candidates and condition the acquisition function on the previously selected pending points when selecting the next point [65]. Using a submodularity argument similar to that in Wilson et al. [64], the sequential greedy approximation of qEHVI enjoys regret of no more than 1 e α∗ qEHVI , where α∗ qEHVI is the optima of αqEHVI [23] (see Appendix B). Although sequential greedy approaches have been considered for many acquisition functions [65], no previous work has proposed a proper sequential greedy approach (with integration over the posterior) for parallel EHVI , as this would require computing the Pareto front under each sample ft from the joint posterior before computing the hypervolume improvement. These operations would be computationally expensive for even modest N and non-differentiable. qEHVI avoids determining the Pareto set for each sample by using inclusion-exclusion principle to compute the joint HVI over the pending points x1,..., xi−1 and the new candidate xi for each MC sample. Figure 2b empirically 3Technically,min and max are only sub-differentiable, but are known to be well-behaved [64]. In our MC setting with GP posteriors, qEHVI is differentiable w.p. 1 if xcontains no repeated points. 4For the constrained case, we replace the indicator with a differentiable sigmoid approximation. 60 5000 10000 15000 20000 Average/uni00A0Cumulative/uni00A0Acquisition/uni00A0Optimization/uni00A0Wall/uni00A0Time/uni00A0(s) 0.40 0.35 0.30 0.25 0.20 0.15 0.10 log/uni00A0HV/uni00A0difference EHVI/uni00A0/uni00AD/uni00A0Exact/uni00A0Gradient EHVI/uni00A0/uni00AD/uni00A0Approx./uni00A0Gradient EHVI/uni00A0/uni00AD/uni00A0Gradient/uni00A0Free qEHVI/uni00A0(q=2)/uni00A0/uni00AD/uni00A0Exact/uni00A0Gradient qEHVI/uni00A0(q=2)/uni00A0/uni00AD/uni00A0Approx./uni00A0Gradient qEHVI/uni00A0(q=2)/uni00A0/uni00AD/uni00A0Gradient/uni00A0Free (a) 0 20 40 60 80 100 Batch/uni00A0Iteration 1.8 1.6 1.4 1.2 1.0 0.8 0.6 0.4 log/uni00A0HV/uni00A0difference qEHVI/uni00A0Joint/uni00A0q=2 qEHVI/uni00A0Joint/uni00A0q=4 qEHVI/uni00A0Joint/uni00A0q=8 qEHVI/uni00A0Post./uni00A0Mean/uni00A0q=2 qEHVI/uni00A0Post./uni00A0Mean/uni00A0q=4 qEHVI/uni00A0Post./uni00A0Mean/uni00A0q=8 qEHVI/uni00A0Seq./uni00A0Greedy/uni00A0q=2 qEHVI/uni00A0Seq./uni00A0Greedy/uni00A0q=4 qEHVI/uni00A0Seq./uni00A0Greedy/uni00A0q=8 qEHVI/uni00A0q=1 (b) Figure 2: (a) A comparison of EHVI and qEHVI (q = 2) optimized with L-BFGS-B using exact gradients, L-BFGS-B using gradients approximated using ﬁnite differences, and CMA-ES, a gradient- free method. (b) A comparison of joint optimization, sequential greedy optimization with proper integration at the pending points, and sequential greedy using the posterior mean. Both plots show optimization performance on a DTLZ2 problem (d= 6,M = 2) with a budget of 100 evaluations (plus the initial quasi-random design). We report means and 2 standard errors across 20 trials. demonstrates the improved optimization performance from properly integrating over the unobserved outcomes rather than using the posterior mean or jointly optimizing the qcandidates. 5 Benchmarks We empirically evaluate qEHVI on synthetic and real world optimization problems. We compare qEHVI 5 against existing state-of-the-art methods including SMS-EGO6, PESMO6, TS-TCH5, and analytic EHVI [68] with gradients5. Additionally, we compare against a novel extension of ParEGO [40] that supports parallel evaluation and constraints (neither of which have been done before to our knowledge); we call this method qPAREGO5. Additionally, we include a quasi-random baseline that selects candidates from a scrambled Sobol sequence. See Appendix E.1 for details on all baseline algorithms. Synthetic Benchmarks We evaluate optimization performance on four benchmark problems in terms of log hypervolume difference, which is deﬁned as the difference between the hypervolume of the true (feasible) Pareto front and the hypervolume of the approximate (feasible) Pareto front based on the observed data; in the case that the true Pareto front is unknown (or not easily approximated), we evaluate the hypervolume indicator. All references points and search spaces are provided in Appendix E.2. For synthetic problems, we consider the Branin-Currin problem ( d = 2,M = 2, convex Pareto front) [6] and the C2-DTLZ2 (d= 12,M = 2,V = 1, concave Pareto front), which is a standard constrained benchmark from the MO literature [16] (see Appendix F.1 for additional synthetic benchmarks). Real-World Benchmarks Structural Optimization in Automobile Safety Design (VEHICLE SAFETY ): Vehicle crash safety is an important consideration in the structural design of automobiles. A lightweight car is preferable because of its potentially lower manufacturing cost and better fuel economy, but lighter material can fare worse than sturdier alternatives in a collision, potentially leading to increased vehicle damage and more severe injury to the vehicle occupants [72]. We consider the problem designing the thickness of 5 reinforced parts of the frontal frame of a vehicle that considerably affect crash safety. The goal is to minimize: 1) the mass of the vehicle; 2) the collision acceleration in a full frontal crash—a proxy for bio-mechanical trauma to the vehicle occupants from the acceleration; and 3) the toe-board 5Acquisition functions are available as part of the open-source library BoTorch [ 5]. Code is available at https://github.com/pytorch/botorch. 6We leverage existing implementations from the Spearmint library. The code is available at https:// github.com/HIPS/Spearmint/tree/PESM. 70 20 40 60 80 100 Function/uni00A0Evaluations 0.0 0.5 1.0 1.5log/uni00A0HV/uni00A0DifferenceSobol EHVI qEHVI qParEGO TS/uni00ADTCH PESMO SMS/uni00ADEGO (a) 0 20 40 60 80 100 Function/uni00A0Evaluations 1.0 0.9 0.8 0.7 0.6 0.5 0.4 log/uni00A0HV/uni00A0Difference Sobol qEHVI qParEGO (b) 0 20 40 60 80 100 Function/uni00A0Evaluations 0.0 0.5 1.0 1.5 2.0log/uni00A0HV/uni00A0DifferenceSobol EHVI qEHVI qParEGO TS/uni00ADTCH PESMO SMS/uni00ADEGO (c) 0 20 40 60 80 100 Function/uni00A0Evaluations 3.0 3.1 3.2 3.3 3.4 3.5 3.6HV 1e6 Sobol EHVI qEHVI qParEGO TS/uni00ADTCH PESMO SMS/uni00ADEGO (d) Figure 3: Sequential optimization performance on (a) on the Branin-Currin problem (q= 1), (b) the C2-DTLZ2 problem, (c) the vehicle crash safety problem (q= 1), and (d) the ABR control problem (q= 1). We report the means and 2 standard errors across 20 trials. intrusion—a measure of the most extreme mechanical damage to the vehicle in an off-frontal collision [44]. For this problem, we optimize the surrogate from Tanabe and Ishibuchi [60]. Policy Optimization for Adaptive Bitrate Control(ABR ): Many web services adapt video playback quality adaptively based on the receiver’s network bandwith to maintain steady, high quality stream with minimal stalls and buffer periods [47]. Previous works have proposed controllers with different scalarized objective functions [46], but in many cases, engineers may prefer to learn the set of optimal trade-offs between their metrics of interest, rather than specifying a scalarized objective in advance. In this problem, we decompose the objective function proposed in Mao et al. [46] into its constituent metrics and optimize 4 parameters of an ABR control policy on the Park simulator [48] to maximize video quality (bitrate) and minimize stall time. See Appendix E.2 for details. 5.1 Results Figure 3 shows thatqEHVI outperforms all baselines in terms of sequential optimization performance on all evaluated problems. Table 1 shows that qEHVI achieves wall times that are an order of magnitude smaller than those of PESMO on a CPU in sequential optimization, and maintains competitive wall times even relative toqPAREGO (which has a signiﬁcantly smaller workload) for large q on a GPU. TS-TCH has by far the fastest wall time, but this comes at the cost of inferior optimization performance. Figure 4 illustrates optimization performance of parallel acquisition functions for varying batch sizes. Increasing the level of parallelism leads to faster convergence for all algorithms (Figure 4a). In contrast with other algorithms, qEHVI ’s sample complexity does not deteriorate substantially when high levels of parallelism are used (Figure 4b). 8Table 1: Acquisition Optimization wall time in seconds on a CPU (2x Intel Xeon E5-2680 v4 @ 2.40GHz) and a GPU (Tesla V100-SXM2-16GB). We report the mean and 2 standard errors across 20 trials. NA indicates that the algorithm does not support constraints. CPU BRANIN CURRIN C2DTLZ2 ABR V EHICLE SAFETY PESMO ( q=1) 249.16 (±19.35) NA 214.16 (±18.38) 492 .64 (±58.98) SMS-EGO ( q=1) 146.1 (±8.57) NA 89.54 (±5.79) 115 .11 (±8.21) TS-TCH ( q=1) 2.82 (±0.03) NA 17.22 (±0.04) 47 .46 (±0.05) qPAREGO ( q=1) 1.56 (±0.16) 4 .01 (±0.77) 7 .47 (±0.67) 1 .74 (±0.27) EHVI ( q=1) 3.04 (±0.16) NA 2.48 (±0.19) 15 .18 (±2.24) qEHVI ( q=1) 3.63 (±0.23) 5 .4 (±1.18) 6 .15 (±0.71) 67 .54 (±10.45) GPU BRANIN CURRIN C2DTLZ2 ABR V EHICLE SAFETY TS-TCH ( q=1) 0.07 (±0.00) NA 0.16 (±0.00) 0 .32 (±0.0) TS-TCH ( q=2) 0.07 (±0.00) NA 0.15 (±0.00) 0 .34 (±0.01) TS-TCH ( q=4) 0.09 (±0.01) NA 0.15 (±0.00) 0 .31 (±0.01) TS-TCH ( q=8) 0.08 (±0.00) NA 0.16 (±0.00) 0 .34 (±0.01) qPAREGO ( q=1) 3.2 (±0.37) 3 .85 (±0.91) 9 .64 (±0.96) 3 .44 (±0.51) qPAREGO ( q=2) 7.12 (±0.81) 12 .1 (±2.77) 21 .19 (±1.53) 7 .32 (±0.97) qPAREGO ( q=4) 15.34 (±1.69) 39 .71 (±7.40) 35 .46 (±2.32) 17 .2 (±2.29) qPAREGO ( q=8) 32.11 (±4.14) 99 .58 (±15.20) 72 .52 (±5.04) 39 .72 (±7.13) EHVI ( q=1) 4.53 (±0.23) NA 6.82 (±0.55) 8 .95 (±0.64) qEHVI ( q=1) 5.98 (±0.28) 3 .36 (±0.94) 7 .71 (±0.67) 10 .43 (±0.64) qEHVI ( q=2) 11.37 (±0.56) 21 .56 (±3.45) 18 .32 (±1.48) 17 .67 (±1.54) qEHVI ( q=4) 25.29 (±1.51) 89 .18 (±10.86) 44 .44 (±3.53) 54 .25 (±4.17) qEHVI ( q=8) 102.46 (±9.22) 215 .74 (±15.85) 100 .64 (±7.22) 255 .72 (±23.73) 0 20 40 60 80 100 Batch/uni00A0Iteration 3.0 3.1 3.2 3.3 3.4 3.5 3.6HV 1e6 Sobol/uni00A0q=8 TS/uni00A0q=1 TS/uni00A0q=2 TS/uni00A0q=4 TS/uni00A0q=8 qEHVI/uni00A0q=1 qEHVI/uni00A0q=2 qEHVI/uni00A0q=4 qEHVI/uni00A0q=8 qParEGO/uni00A0q=1 qParEGO/uni00A0q=2 qParEGO/uni00A0q=4 qParEGO/uni00A0q=8 (a) 0 20 40 60 80 100 Function/uni00A0Evaluations 3.0 3.1 3.2 3.3 3.4 3.5 3.6HV 1e6 Sobol/uni00A0q=8 TS/uni00A0q=1 TS/uni00A0q=2 TS/uni00A0q=4 TS/uni00A0q=8 qEHVI/uni00A0q=1 qEHVI/uni00A0q=2 qEHVI/uni00A0q=4 qEHVI/uni00A0q=8 qParEGO/uni00A0q=1 qParEGO/uni00A0q=2 qParEGO/uni00A0q=4 qParEGO/uni00A0q=8 (b) Figure 4: Parallel optimization performance on the ABR problem with varying batch sizes (q) by (a) batch BO iterations and (b) function evaluations. 6 Discussion We present a practical and efﬁcient acquisition function, qEHVI , for parallel, constrained multi- objective Bayesian optimization. Leveraging differentiable programming, modern parallel hardware, and the Sample Average Approximation, we efﬁciently optimizeqEHVI via quasi second-order meth- ods and provide theoretical convergence guarantees for our approach. Empirically, we demonstrate that our method out-performs state-of-the-art multi-objective Bayesian optimization methods. One limitation of our approach is that it currently assumes noiseless observations, which, to our knowledge, is the case with all formulations of EHVI . Integrating over the uncertainty around the previous observations [43] by using MC samples over the new candidates and the training points, one may be able to account for the noise.Another limitation of qEHVI is that its scalability is limited the partitioning algorithm, precluding its use in high-dimensional objective spaces. More scalable partitioning algorithms, either approximate algorithms (e.g. the algorithm proposed by Couckuyt et al. [11], which we examine brieﬂy in Appendix F.4) or more efﬁcient exact algorithms that result in fewer disjoint hyper-rectangles (e.g. [41, 17, 69]), will improve the scalability and computation time of of qEHVI . We hope this work encourages researchers to consider more improvements from applying modern computational paradigms and tooling to Bayesian optimization. 97 Statement of Broader Impact Optimizing a single outcome commonly comes at the expense of other secondary outcomes. In some cases, decision makers may be able to form a scalarization of their objectives in advance, but in the researcher’s experience, formulating such trade-offs in advance is difﬁcult for most. Improvements to the optimization performance and practicality of multi-objective Bayesian optimization have the potential to allow decision makers to better understand and make more informed decisions across multiple trade-offs. We expect these directions to be particularly important as Bayesian optimization is increasingly used for applications such as recommender systems [42], where auxiliary goals such as fairness must be accounted for. Of course, at the end of the day, exactly what objectives decision makers choose to optimize, and how they balance those trade-offs (and whether that is done in equitable fashion) is up to the individuals themselves. Acknowledgments We would like to thank Daniel Jiang for helpful discussions around our theoretical results. References [1] M. Abdolshah, A. Shilton, S. Rana, S. Gupta, and S. Venkatesh. Expected hypervolume improvement with constraints. In 2018 24th International Conference on Pattern Recognition (ICPR), pages 3238–3243, 2018. [2] Arash Asadpour, Hamid Nazerzadeh, and Amin Saberi. Stochastic submodular maximization. In Christos Papadimitriou and Shuzhong Zhang, editors, Internet and Network Economics. Springer Berlin Heidelberg, 2008. [3] R. Astudillo and P. Frazier. Bayesian optimization of composite functions. Forthcoming, in Proceedings of the 35th International Conference on Machine Learning, 2019. [4] Anne Auger, Johannes Bader, Dimo Brockhoff, and Eckart Zitzler. Theory of the hypervolume indicator: Optimal mu-distributions and the choice of the reference point. In Proceedings of the Tenth ACM SIGEVO Workshop on Foundations of Genetic Algorithms, FOGA ’09, page 87–102, New York, NY , USA, 2009. Association for Computing Machinery. [5] Maximilian Balandat, Brian Karrer, Daniel R. Jiang, Samuel Daulton, Benjamin Letham, Andrew Gordon Wilson, and Eytan Bakshy. BoTorch: A Framework for Efﬁcient Monte-Carlo Bayesian Optimization. In Advances in Neural Information Processing Systems 33, 2020. [6] Syrine Belakaria, Aryan Deshwal, and Janardhan Rao Doppa. Max-value entropy search for multi-objective bayesian optimization. In Advances in Neural Information Processing Systems 32, 2019. [7] Eric Bradford, Artur Schweidtmann, and Alexei Lapkin. Efﬁcient multiobjective optimization employing gaussian processes, spectral sampling and a genetic algorithm. Journal of Global Optimization, 71, 02 2018. doi: 10.1007/s10898-018-0609-2. [8] Russel E Caﬂisch. Monte carlo and quasi-monte carlo methods. Acta numerica, 7:1–49, 1998. [9] Mauro Cerasoli and Aniello Fedullo. The inclusion-exclusion principle. Journal of Interdisciplinary Mathematics, 5(2):127–141, 2002. [10] Anirban Chaudhuri, Raphael Haftka, Peter Ifju, Kelvin Chang, Christopher Tyler, and Tony Schmitz. Experimental ﬂapping wing optimization and uncertainty quantiﬁcation using limited samples. Structural and Multidisciplinary Optimization, 51, 11 2014. doi: 10.1007/s00158-014-1184-x. [11] I. Couckuyt, D. Deschrijver, and T. Dhaene. Towards efﬁcient multiobjective optimization: Multiobjective statistical criterions. In 2012 IEEE Congress on Evolutionary Computation, pages 1–8, 2012. [12] Ivo Couckuyt, Dirk Deschrijver, and Tom Dhaene. Fast calculation of multiobjective probability of improvement and expected improvement criteria for pareto optimization. J. of Global Optimization, 60(3): 575–594, November 2014. [13] Daniel A. da Silva. Proprietades geraes. J. de l’Ecole Polytechnique, cah. 30. I, 1854. 10[14] K. Deb, A. Pratap, S. Agarwal, and T. Meyarivan. A fast and elitist multiobjective genetic algorithm: Nsga-ii. IEEE Transactions on Evolutionary Computation, 6(2):182–197, 2002. [15] Kalyan Deb, L. Thiele, Marco Laumanns, and Eckart Zitzler. Scalable multi-objective optimization test problems. volume 1, pages 825–830, 06 2002. ISBN 0-7803-7282-4. doi: 10.1109/CEC.2002.1007032. [16] Kalyanmoy Deb. Constrained Multi-objective Evolutionary Algorithm, pages 85–118. Springer Interna- tional Publishing, Cham, 2019. [17] Kerstin Dächert, Kathrin Klamroth, Renaud Lacour, and Daniel Vanderpooten. Efﬁcient computation of the search region in multi-objective optimization. European Journal of Operational Research, 260(3):841 – 855, 2017. [18] M. T. M. Emmerich, K. C. Giannakoglou, and B. Naujoks. Single- and multiobjective evolutionary opti- mization assisted by gaussian random ﬁeld metamodels. IEEE Transactions on Evolutionary Computation, 10(4):421–439, 2006. [19] M. T. M. Emmerich, A. H. Deutz, and J. W. Klinkenberg. Hypervolume-based expected improvement: Monotonicity properties and exact computation. In 2011 IEEE Congress of Evolutionary Computation (CEC), pages 2147–2154, 2011. [20] Michael Emmerich, Kaifeng Yang, André Deutz, Hao Wang, and Carlos M. Fonseca. A Multicriteria Generalization of Bayesian Global Optimization, pages 229–242. Springer International Publishing, 2016. [21] Michael T. M. Emmerich and Carlos M. Fonseca. Computing hypervolume contributions in low dimensions: Asymptotically optimal algorithm and complexity results. In Ricardo H. C. Takahashi, Kalyanmoy Deb, Elizabeth F. Wanner, and Salvatore Greco, editors, Evolutionary Multi-Criterion Optimization , pages 121–135, Berlin, Heidelberg, 2011. Springer Berlin Heidelberg. [22] Paul Feliot, Julien Bect, and Emmanuel Vazquez. A bayesian approach to constrained single- and multi- objective optimization. Journal of Global Optimization, 67(1-2):97–133, Apr 2016. ISSN 1573-2916. doi: 10.1007/s10898-016-0427-3. URL http://dx.doi.org/10.1007/s10898-016-0427-3 . [23] M. L. Fisher, G. L. Nemhauser, and L. A. Wolsey. An analysis of approximations for maximizing submodular set functions—II , pages 73–87. Springer Berlin Heidelberg, Berlin, Heidelberg, 1978. [24] Tobias Friedrich and Frank Neumann. Maximizing submodular functions under matroid constraints by multi-objective evolutionary algorithms. In Thomas Bartz-Beielstein, Jürgen Branke, Bogdan Filipiˇc, and Jim Smith, editors, Parallel Problem Solving from Nature – PPSN XIII , pages 922–931, Cham, 2014. Springer International Publishing. ISBN 978-3-319-10762-2. [25] Jacob Gardner, Matt Kusner, Zhixiang, Kilian Weinberger, and John Cunningham. Bayesian optimization with inequality constraints. In Proceedings of the 31st International Conference on Machine Learning, volume 32 of Proceedings of Machine Learning Research, pages 937–945, Beijing, China, 22–24 Jun 2014. PMLR. [26] Eduardo C Garrido-Merchán and Daniel Hernández-Lobato. Predictive entropy search for multi-objective bayesian optimization with constraints. Neurocomputing, 361:50–68, 2019. [27] Eduardo C Garrido-Merchán and Daniel Hernández-Lobato. Parallel predictive entropy search for multi- objective bayesian optimization with constraints, 2020. [28] David Gaudrie, Rodolphe Le Riche, Victor Picheny, Benoît Enaux, and Vincent Herbert. Targeting solutions in bayesian multi-objective optimization: sequential and batch versions. Annals of Mathematics and Artiﬁcial Intelligence, 88(1-3):187–212, Aug 2019. ISSN 1573-7470. doi: 10.1007/s10472-019-09644-8. URL http://dx.doi.org/10.1007/s10472-019-09644-8 . [29] Michael A. Gelbart, Jasper Snoek, and Ryan P. Adams. Bayesian optimization with unknown constraints. In Proceedings of the 30th Conference on Uncertainty in Artiﬁcial Intelligence, UAI, 2014. [30] David Ginsbourger, Rodolphe Le Riche, and Laurent Carraro. Kriging Is Well-Suited to Parallelize Optimization, pages 131–162. Springer Berlin Heidelberg, Berlin, Heidelberg, 2010. [31] P. Glasserman. Performance continuity and differentiability in monte carlo optimization. In 1988 Winter Simulation Conference Proceedings, pages 518–524, 1988. [32] Nikolaus Hansen. The CMA Evolution Strategy: A Comparing Review, volume 192, pages 75–102. 06 2007. doi: 10.1007/3-540-32494-1_4. 11[33] Daniel Hernández-Lobato, José Miguel Hernández-Lobato, Amar Shah, and Ryan P. Adams. Predictive entropy search for multi-objective bayesian optimization, 2015. [34] Iris Hupkens, Andre Deutz, Kaifeng Yang, and Michael Emmerich. Faster exact algorithms for computing expected hypervolume improvement. In Antonio Gaspar-Cunha, Carlos Henggeler Antunes, and Car- los Coello Coello, editors, Evolutionary Multi-Criterion Optimization, pages 65–79. Springer International Publishing, 2015. [35] Hisao Ishibuchi, Naoya Akedo, and Yusuke Nojima. A many-objective test problem for visually examining diversity maintenance behavior in a decision space. In Proceedings of the 13th Annual Conference on Genetic and Evolutionary Computation , GECCO ’11, page 649–656, New York, NY , USA, 2011. Association for Computing Machinery. ISBN 9781450305570. doi: 10.1145/2001576.2001666. URL https://doi.org/10.1145/2001576.2001666. [36] Hisao Ishibuchi, Ryo Imada, Yu Setoguchi, and Yusuke Nojima. How to specify a reference point in hypervolume calculation for fair performance comparison. Evol. Comput., 26(3):411–440, September 2018. [37] Donald Jones, C. Perttunen, and B. Stuckman. Lipschitzian optimisation without the lipschitz constant. Journal of Optimization Theory and Applications, 79:157–181, 01 1993. doi: 10.1007/BF00941892. [38] Donald R. Jones, Matthias Schonlau, and William J. Welch. Efﬁcient global optimization of expensive black-box functions. Journal of Global Optimization, 13:455–492, 1998. [39] Diederik P Kingma and Max Welling. Auto-Encoding Variational Bayes. arXiv e-prints , page arXiv:1312.6114, Dec 2013. [40] J. Knowles. Parego: a hybrid algorithm with on-line landscape approximation for expensive multiobjective optimization problems. IEEE Transactions on Evolutionary Computation, 10(1):50–66, 2006. [41] Renaud Lacour, Kathrin Klamroth, and Carlos M. Fonseca. A box decomposition algorithm to compute the hypervolume indicator. Computers & Operations Research, 79:347 – 360, 2017. [42] Benjamin Letham and Eytan Bakshy. Bayesian optimization for policy search via online-ofﬂine experimen- tation. Journal of Machine Learning Research, 20(145):1–30, 2019. URL http://jmlr.org/papers/ v20/18-225.html. [43] Benjamin Letham, Brian Karrer, Guilherme Ottoni, and Eytan Bakshy. Constrained bayesian optimization with noisy experiments. Bayesian Analysis, 14(2):495–519, 06 2019. doi: 10.1214/18-BA1110. [44] Xingtao Liao, Qing Li, Xujing Yang, Weigang Zhang, and Wei Li. Multiobjective optimization for crash safety design of vehicles using stepwise regression model. Structural and Multidisciplinary Optimization, 35:561–569, 06 2008. doi: 10.1007/s00158-007-0163-x. [45] Edgar Manoatl Lopez, Luis Miguel Antonio, and Carlos A. Coello Coello. A gpu-based algorithm for a faster hypervolume contribution computation. In António Gaspar-Cunha, Carlos Henggeler Antunes, and Carlos Coello Coello, editors, Evolutionary Multi-Criterion Optimization, pages 80–94. Springer International Publishing, 2015. [46] Hongzi Mao, Ravi Netravali, and Mohammad Alizadeh. Neural adaptive video streaming with pensieve. In Proceedings of the Conference of the ACM Special Interest Group on Data Communication, SIGCOMM ’17, page 197–210, New York, NY , USA, 2017. Association for Computing Machinery. ISBN 9781450346535. doi: 10.1145/3098822.3098843. URL https://doi.org/10.1145/3098822.3098843. [47] Hongzi Mao, Shannon Chen, Drew Dimmery, Shaun Singh, Drew Blaisdell, Yuandong Tian, Mohammad Alizadeh, and Eytan Bakshy. Real-world video adaptation with reinforcement learning. 2019. [48] Hongzi Mao, Parimarjan Negi, Akshay Narayan, Hanrui Wang, Jiacheng Yang, Haonan Wang, Ryan Marcus, Ravichandra Addanki, Mehrdad Khani Shirkoohi, Songtao He, Vikram Nathan, Frank Cangialosi, Shaileshh Bojja Venkatakrishnan, Wei-Hung Weng, Shu-Wen Han, Tim Kraska, and Mohammad Alizadeh. Park: An open platform for learning-augmented computer systems. In NeurIPS, 2019. [49] Sébastien Marmin, Clément Chevalier, and David Ginsbourger. Differentiating the multipoint expected improvement for optimal batch design. In Panos Pardalos, Mario Pavone, Giovanni Maria Farinella, and Vincenzo Cutello, editors, Machine Learning, Optimization, and Big Data , pages 37–48, Cham, 2015. Springer International Publishing. [50] B. Paria, K. Kandasamy, and B. Póczos. A Flexible Multi-Objective Bayesian Optimization Approach using Random Scalarizations. ArXiv e-prints, May 2018. 12[51] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in PyTorch. 2017. [52] Victor Picheny. Multiobjective optimization using gaussian process emulators via stepwise uncertainty reduction. Statistics and Computing, 25, 10 2013. doi: 10.1007/s11222-014-9477-x. [53] Wolfgang Ponweiser, Tobias Wagner, Dirk Biermann, and Markus Vincze. Multiobjective optimization on a limited budget of evaluations using model-assisted s-metric selection. In Günter Rudolph, Thomas Jansen, Nicola Beume, Simon Lucas, and Carlo Poloni, editors, Parallel Problem Solving from Nature – PPSN X, pages 784–794, Berlin, Heidelberg, 2008. Springer Berlin Heidelberg. [54] Alma A. M. Rahat, Richard M. Everson, and Jonathan E. Fieldsend. Alternative inﬁll strategies for expensive multi-objective optimisation. In Proceedings of the Genetic and Evolutionary Computation Con- ference, GECCO ’17, page 873–880, New York, NY , USA, 2017. Association for Computing Machinery. ISBN 9781450349208. [55] Carl Edward Rasmussen. Gaussian Processes in Machine Learning , pages 63–71. Springer Berlin Heidelberg, Berlin, Heidelberg, 2004. [56] Jerry Segercrantz. Inclusion-exclusion and characteristic functions.Mathematics Magazine, 71(3):216–218, 1998. ISSN 0025570X, 19300980. URL http://www.jstor.org/stable/2691209. [57] B. Shahriari, K. Swersky, Z. Wang, R. P. Adams, and N. de Freitas. Taking the human out of the loop: A review of bayesian optimization. Proceedings of the IEEE, 104(1):148–175, 2016. [58] Niranjan Srinivas, Andreas Krause, Sham Kakade, and Matthias Seeger. Gaussian process optimization in the bandit setting: No regret and experimental design. In Proceedings of the 27th International Conference on International Conference on Machine Learning, ICML’10, page 1015–1022, Madison, WI, USA, 2010. Omnipress. ISBN 9781605589077. [59] J. Sylvester. Note sur la théorème de legendre. Comptes Rendus Acad. Sci., 96:463–465, 1883. [60] Ryoji Tanabe and Hisao Ishibuchi. An easy-to-use real-world multi-objective optimization problem suite. Applied Soft Computing, 89:106078, 2020. ISSN 1568-4946. doi: https://doi.org/10.1016/j.asoc.2020. 106078. [61] William R. Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 25(3/4):285–294, 1933. [62] Takashi Wada and Hideitsu Hino. Bayesian optimization for multi-objective optimization and multi-point search, 2019. [63] Jialei Wang, Scott C. Clark, Eric Liu, and Peter I. Frazier. Parallel bayesian global optimization of expensive functions, 2016. [64] J. T. Wilson, R. Moriconi, F. Hutter, and M. P. Deisenroth. The reparameterization trick for acquisition functions. ArXiv e-prints, December 2017. [65] James Wilson, Frank Hutter, and Marc Deisenroth. Maximizing acquisition functions for bayesian optimization. In Advances in Neural Information Processing Systems 31, pages 9905–9916. 2018. [66] Jian Wu and Peter I. Frazier. The parallel knowledge gradient method for batch bayesian optimization. In Proceedings of the 30th International Conference on Neural Information Processing Systems, NIPS’16, page 3134–3142, Red Hook, NY , USA, 2016. Curran Associates Inc. ISBN 9781510838819. [67] Kaifeng Yang, Michael Emmerich, André Deutz, and Carlos M. Fonseca. Computing 3-d expected hypervolume improvement and related integrals in asymptotically optimal time. In 9th International Conference on Evolutionary Multi-Criterion Optimization - Volume 10173, EMO 2017, page 685–700, Berlin, Heidelberg, 2017. Springer-Verlag. [68] Kaifeng Yang, Michael Emmerich, André H. Deutz, and Thomas Bäck. Efﬁcient computation of expected hypervolume improvement using box decomposition algorithms. CoRR, abs/1904.12672, 2019. [69] Kaifeng Yang, Michael Emmerich, André Deutz, and Thomas Bäck. Multi-objective bayesian global optimization using expected hypervolume improvement gradient. Swarm and Evolutionary Computation, 44:945 – 956, 2019. ISSN 2210-6502. doi: https://doi.org/10.1016/j.swevo.2018.10.007. URL http: //www.sciencedirect.com/science/article/pii/S2210650217307861. 13[70] Kaifeng Yang, Pramudita Palar, Michael Emmerich, Koji Shimoyama, and Thomas Bäck. A multi- point mechanism of expected hypervolume improvement for parallel multi-objective bayesian global optimization. pages 656–663, 07 2019. doi: 10.1145/3321707.3321784. [71] Kaifeng Yang, Pramudita Satria Palar, Michael Emmerich, Koji Shimoyama, and Thomas Bäck. A multi-point mechanism of expected hypervolume improvement for parallel multi-objective bayesian global optimization. In Proceedings of the Genetic and Evolutionary Computation Conference, GECCO ’19, page 656–663, New York, NY , USA, 2019. Association for Computing Machinery. ISBN 9781450361118. doi: 10.1145/3321707.3321784. URL https://doi.org/10.1145/3321707.3321784. [72] R. J. Yang, N. Wang, C. H. Tho, J. P. Bobineau, and B. P. Wang. Metamodeling Development for Vehicle Frontal Impact Simulation. Journal of Mechanical Design, 127(5):1014–1020, 01 2005. [73] E. Zitzler, L. Thiele, M. Laumanns, C. M. Fonseca, and V . G. da Fonseca. Performance assessment of multiobjective optimizers: an analysis and review. IEEE Transactions on Evolutionary Computation, 7(2): 117–132, 2003. 14Appendix to: Differentiable Expected Hypervolume Improvement for Parallel Multi-Objective Bayesian Optimization A Derivation of q-Expected Hypervolume Improvement A.1 Hypervolume Improvement via the Inclusion-Exclusion Principle The hypervolume improvement of f(x) within the hyper-rectangle Sk is the volume of Sk ∩∆({f(x)},P,r) and is given by: HVIk ( f(x),lk,uk ) = λM ( Sk ∩∆({f(x)},P,r) ) = M∏ m=1 [ z(m) k −l(m) k ] +, where u(m) k ,l(m) k ,f(m)(x), and z(m) k denote the mth component of the corresponding vector and [·]+ denotes the min(·,0) operation. Summing over all Sk gives the total hypervolume improvement: HVI ( f(x) ) = K∑ k=1 HVIk ( f(x),lk,uk ) = K∑ k=1 λM ( Sk ∩∆({f(x)},P,r) ) = K∑ k=1 M∏ m=1 [ z(m) k −l(m) k ] +. We can extend the HVI computation to the q >1 case using the inclusion-exclusion principle. Principle 1. The inclusion-exclusion principle[13, 59, 9] Given a ﬁnite measure space (B,A,µ) and a ﬁnite sequence of potentially empty or overlapping sets {Ai}i = 1n where Ai ∈A and µ(B) <∞, then, λM ( p⋃ i=1 Ai ) = p∑ j=1 (−1)j+1 ∑ 1≤i1≤...≤ij≤p λM ( Ai1 ∩...∩Aij ) In the context of computing the joint HVI of q new points{f(xi)}q i=1, each subset Ai for i = 1,...,q is the set of points contained in ∆({f(xi)},P,r) — independently of the other q−1 points. λM (Ai) is the hypervolume improvement from the new point f(xi): λM (Ai) = HVI(f(xi)). The union of these subsets is the set of points in the new space dominated by the qnew points: ⋃q i=1 Ai = ⋃q i=1 ∆({f(xi)},P,r). The hypervolume of ⋃q i=1 ∆({f(xi)},P,r) is the hypervolume improvement from the qnew points: HVI({f(xi)}q i=1) = λM ( q⋃ i=1 Ai ) = q∑ j=1 (−1)j+1 ∑ 1≤i1≤...≤ij≤q λM ( Ai1 ∩···∩ Aij ) To compute λM (Ai1 ∩···∩ Aij ), we partition the space covered by Ai1 ∩···∩ Aij across the K hyper- rectangles {Sk}K k=1 and compute the hypervolume of the overlapping space of Ai1 ∩···∩ Aij with each Sk independently. Since {Sk}K k=1 is a disjoint partition, summing overKgives the hypervolume ofAi1 ∩···∩ Aij : λM ( Ai1 ∩···∩ Aij ) = K∑ k=1 λM ( Sk ∩Ai1 ∩···∩ Aij ) This has two advantages. First, the new dominated space Ai can be a non-rectangular polytope, but the intersection Ai ∩Sk is a rectangular polytope, which simpliﬁes computation of overlapping hypervolume. 15Second, the vertices deﬁning the hyper-rectangle encapsulated by Sk ∩Ai1 ∩···∩ Aij are easily derived. The lower bound is simply the lk lower bound of Sk and the upper bound is the component-wise minimum zk,i1,...ij = min [ uk,f(xi1 ),..., f(xij ) ] . Importantly, this is computationally tractable because this speciﬁc approach enables parallelizing computation across all intersections of subsets Ai1 ∩···∩ Aij for 1 ≤ij ≤... ≤ij ≤qand across all Khyper-rectangles. Explicitly, the HVI is computed as: HVI({f(xi)}q i=1) = λM ( p⋃ i=1 Ai ) = q∑ j=1 ∑ 1≤i1≤...≤ij≤q (−1)j+1λM ( Ai1 ∩···∩ Aij ) = K∑ k=1 q∑ j=1 ∑ 1≤i1≤...≤ij≤q (−1)j+1λM ( Sk ∩Ai1 ∩···∩ Aij ) = K∑ k=1 q∑ j=1 ∑ 1≤i1≤...≤ij≤q (−1)j+1λM ( Sk ∩∆({f(xi1 )},P,r) ∩... ∩∆({f(xij )},P,r) ) = K∑ k=1 q∑ j=1 ∑ 1≤i1≤...≤ij≤q (−1)j+1 M∏ m=1 [ z(m) k,i1,...ij −l(m) k ] + = K∑ k=1 q∑ j=1 ∑ Xj∈Xj (−1)j+1 M∏ m=1 [ z(m) k,Xj −l(m) k ] + where Xj is the superset all subsets of Xcand of size j: Xj = {Xj ⊂Xcand : |Xj|= j}and z(m) k,Xj = z(m) k,i1,...ij for Xj = {xi1 ,..., xij }. A.2 Computing Expected Hypervolume Improvement The above approach for computing HVI assumes we know the true objective values {f(xi)}q i=1. Since we do not know the true function values {f(xi)}q i=1, we compute qEHVI as the expectation over the GP posterior. αqEHVI = E [ HVI({f(xi)}q i=1) ] = ∫ RM HVI({f(xi)}q i=1)df (6) In the sequential setting and under the assumption of independent outcomes, qEHVI is simply EHVI and can be expressed in closed form [ 69]. However when q > 1, there is no known analytical formulation [70]. Instead, we estimate the expectation in (6) using MC integration with samples from the joint posterior P ( f(x1),..., f(xq)|D): αqEHVI = E [ HVI({f(xi)}q i=1) ] ≈ 1 N N∑ t=1 HVI({ft(xi)}q i=1) (7) = 1 N N∑ t=1 K∑ k=1 q∑ j=1 ∑ Xj∈Xj (−1)j+1 M∏ m=1 [ z(m) k,Xj,t −l(m) k ] + (8) where {ft(xi)}q i=1 ∼P ( f(x1),..., f(xq)|X,Y ) is the tth sample from the joint posterior over Xcand and z(m) k,Xj,t = min [ uk,minx′∈Xj ft(x′) ] . A.3 Supporting Outcome Constraints Recall that we deﬁned the constrained hypervolume improvement as HVI C (f(x),c(x)) = HVI[f(x)] ·1 [c(x) ≥0]. (9) For q= 1 and assuming independence of the objectives and the constraints, the expected HVI C is the product of the expected HVI and the probability of feasibility (the expectation of 1 [c(x) ≥0]) [22]. However, requiring objectives and constraints to be independent is unnecessary when estimating the expectation with MC integration using samples from the joint posterior. 16In the parallel setting, if all constraints are satisﬁed for all q candidates Xcand = {xi}q i=1, HVI C is simply HVI . If a subset V⊂X cand,V̸= ∅ of the candidates violate at least one of the constraints, then the feasible HVI is the HVI of the set of feasible candidates: HVI C (Xcand) = HVI(Xcand \\V). That is, the hypervolume contribution (i.e. the marginal HVI) of an infeasible point is zero. In our formulation, HVI can be computed by multiplying (5) with an additional factor ∏ x′∈Xj ∏V v=1 1 [c(v)(x′) ≥0]: HVI C ({f(xi),c(xi)}q i=1) = K∑ k=1 q∑ j=1 ∑ Xj∈Xj (−1)j+1 [( M∏ m=1 [ z(m) k,Xj −l(m) k ] + ) ∏ x′∈Xj V∏ v=1 1 [c(v)(x′) ≥0] ] . (10) The additional factor ∏ x′∈Xj ∏V v=1 1 [c(v)(xa) ≥0] indicates whether all constraints are satisﬁed for all candidates in a given subset Xj. Thus HVI C can be computed in the same fashion as HVI , but with the additional step of setting the HV of all subsets containing x′to zero if x′violates any constraint. We can now again perform MC integration as in (5) to compute the expected constrained hypervolume improvement. In this formulation, the marginal hypervolume improvement from a candidate is weighted by the probability that the candidate is feasible. The marginal hypervolume improvements are highly dependent on the outcomes of the other candidates. Importantly, the MC-based approach enables us to properly estimate the marginal hypervolume improvements across candidates by sampling from the joint posterior. Note that while the expected constrained hypervolume E [ HVI C ({f(xi),c(xi)}q i=1) ] is differentiable, we may not differentiate inside the expectation (hence we cannot expect simply differentiating (10) on the sample-level to provide proper gradients). We therefore replace the indicator with a sigmoid function with temperature parameter ϵ, which provides a differentiable relaxation 1 [c(v)(x′) ≥0] ≈s(c(v)(x′); ϵ) := 1 1 + exp(−c(v)(x′)/ϵ) (11) that becomes exact in the limit ϵ↘0. As in the unconstrained parallel scenario, there is no known analytical expression for the expected feasible hypervolume improvement. Therefore, we again use MC integration to approximate the expectation: αqEHVI C (x) = E [ HVI C ({f(xi),c(xi)}q i=1) ] (12a) ≈ 1 N N∑ t=1 HVI C ({ft(xi),ct(xi)}q i=1) (12b) ≈ 1 N N∑ t=1 K∑ k=1 q∑ j=1 ∑ Xj∈Xj (−1)j+1 [( M∏ m=1 [ z(m) k,Xj,t −l(m) k ] + ) ∏ x′∈Xj V∏ v=1 s(c(v)(x′); ϵ) ] (12c) A.3.1 Inclusion Exclusion principle for HVI C Equation (10) holds when the indicator function because HVI C is equivalent to HVI with the subset of feasible points. However, the sigmoid approximation can result in non-zero error. The error function ε: 2Xcand →R can be expressed as ε(X) = ∏ x′∈X V∏ v=1 1 [c(x′) >0] − ∏ x′∈X V∏ v=1 s(c(x′),ϵ) The error function gives a value to each to each element of 2Xcand . Weight functions have been studied in conjunction with the inclusion-exclusion principle [56], but under the assumption of that the weight of a set is the sum of the weights of its elements: w(A) = ∑ a∈A w(a). In our case, the weight function of a set Ais the product the weights of its elements. There, it is not obvious whether the inclusion-exclusion principle will hold in this case. Theorem 1. Given a feasible Pareto frontPfeas, a partitioning {(lk,uk}K k=1 of the objective space RM that is not dominated by the Pfeas, then for a set of points Xcand with objective values f(Xcand) and constraint values c(Xcand), HVI C (f(Xcand),c(Xcand),P,r) = HVI(f′(Xcand),P′,r′) where f′(Xcand) is the set of objective-constraint vectors for each candidate point f′(x) ∈RM+V , P′is the set of vectors [f(1)(x),...,f (M)(x),0V ] ∈RM+V , and r′= [r(1),...,r (M),0V ] ∈RM+V . Proof. Recall equation 10, HVI C ({f(xi),c(xi)}q i=1) = K∑ k=1 q∑ j=1 ∑ Xj∈Xj (−1)j+1 [( M∏ m=1 [ z(m) k,Xj −l(m) k ] + ) ∏ x′∈Xj V∏ v=1 1 [c(v)(x′) ≥0] ] . 17Note that the constraint product ∏ x′∈Xj V∏ v=1 1 [c(v)(x′) ≥0] = V∏ v=1 ∏ x′∈Xj 1 [c(v)(x′) ≥0] = V∏ v=1 min x′∈Xj 1 [c(v)(x′) ≥0] = V∏ v=1 min [ 1, min x′∈Xj 1 [c(v)(x′) ≥0] ] = V∏ v=1 [ min [ 1, min x′∈Xj 1 [c(v)(x′) ≥0] ] −0 ] . (13) For v = 1 ,...,V , k = 1 ,...K, let l(M+v) k = 0 and u(M+v) k = 1 . Then, substituting into the following expression from Equation 13 gives min [ 1, min x′∈Xj 1 [c(v)(x′) ≥0] ] = min [ u(M+v) k , min x′∈Xj 1 [c(v)(x′) ≥0] ] Recall from Section 4, that zis deﬁned as: zk := min [ uk,f(x) ] . The high-level idea is that if we consider the indicator of the slack constraints 1 [c(v)(x′) ≥0] as objectives, then the above expression is consistent with the deﬁnition of zat the beginning of section 4. For v= 1,...,V , z(M+v) k,Xj = min [ 1, min x′∈Xj 1 [c(v)(x′) ≥0] ] Thus, ∏ x′∈Xj V∏ v=1 1 [c(v)(x′) ≥0] = V∏ v=1 [ min [ 1, min x′∈Xj 1 [c(v)(x′) ≥0] ] −0 ] = V∏ v=1 [ z(M+v) k,Xj −l(M+v) k ] + Returning to the HVI C equation, we have HVI C ({f(xi),c(xi)}q i=1) = K∑ k=1 q∑ j=1 ∑ Xj∈Xj (−1)j+1 [( M∏ m=1 [ z(m) k,Xj −l(m) k ] + ) ∏ x′∈Xj V∏ v=1 1 [c(v)(x′) ≥0] ] = K∑ k=1 q∑ j=1 ∑ Xj∈Xj (−1)j+1 [( M∏ m=1 [ z(m) k,Xj −l(m) k ] + ) M+V∏ v=M+1 [ z(v) k,Xj −l(M+v) k ] + ] = K∑ k=1 q∑ j=1 ∑ Xj∈Xj (−1)j+1 [M+V∏ m=1 [ z(m) k,Xj −l(m) k ] + ] (14) Now consider the case when a sigmoid approximation 1 [c(v)(x′) ≥0] ≈s(c(v)(x′); ϵ) is used. The only change to Equation 14 is that z(m) k,Xj ≈ˆz(m) k,Xj = min [ u(M+v) k , min x′∈Xj S[c(v)(x′),ϵ] ] . If S[c(v)(x′),ϵ] = 1 [c(v)(x′) ≥0] for all v,x′, then HVI is computed exactly without approximation error. If S[c(v)(x′),ϵ]1 [c(v)(x′) ≥0] for any v,x′, then there is approximation error: the hypervolume improvement from all subsets containing x′is proportional to ∏V v=1 minx′∈X s(c(x′),ϵ). Since the constraint outcomes are directly considered as components in the hypervolume computation, the inclusion-exclusion principle incorporates the approximate indicator properly. 18A.4 Complexity Recall from Section 3.3 that, given posterior samples, the time complexity on a single-threaded machine is T1 = O(MNK(2q −1)). The space complexity required for maximum parallelism is also is T1 (ignoring the space required by the models), which does limit scalability to larger M and q, but difﬁculty scaling to large M is a known limitaiton of EHVI [69]. To reduce memory load, rectangles could be materialized and processed in chunks at the cost of additional runtime. In addition, our implementation of qEHVI uses the box decomposition algorithm from Couckuyt et al. [11], but we emphasize qEHVI is agnostic to the choice of partitioning algorithm and using a more efﬁcient partitioning algorithm (e.g. [69, 17, 41]) may signiﬁcantly improve memory footprint on GPU and enable larger using qin many scenarios. B Error Bound on Sequential Greedy Approximation If the acquisition function L(Xcand) is a normalized, monotone, submodular set function (where submodular means that the increase in L(Xcand) is non-increasing as elements are added to Xcand and normalized means that L(∅) = 0), then the sequential greedy approximation of Lenjoys regret of no more than 1 e L∗, where L∗is the optima of L[23]. We have αqEHVI (Xcand) = L(Xcand) = Ef ( HVI [ f(Xcand) ]) . Since HVI is a submodular set function [24] and the expectation of a stochastic submodular function is also submodular [ 2], αqEHVI (Xcand) is also submodular and therefore its sequential greedy approximation enjoys regret of no more than 1 e α∗ qEHVI . Using the result from Wilson et al. [65], the MC-based approximation ˆαqEHVI (Xcand) = ∑N t=1 HVI [ ft(Xcand) ] also enjoys the same regret bound since HVI is a normalized submodular set function.7 C Convergence Results For the purpose of stating our convergence results, we recall some concepts and notation from Balandat et al. [5]. First, consider a sample {ft(x1)}q i=1 from the multi-output posterior of the GP surrogate model. Let x∈Rqd be the stacked set of candidates Xcand and let ft(x) := [ft(x1)T ,...,f t(xq)T ]T be the stacked set of corresponding objective vectors. It is well known that, using the reparameterization trick, we can write ft(x) = µ(x) + L(x)ϵt, (15) where µ: Rqd →RqM is the mean function of the multi-output GP, L(x) ∈RqM×qM is a root decomposition (typically the Cholesky decomposition) of the multi-output GP’s posterior covarianceΣ(x) ∈RqM×qM , and ϵt ∈RqM with ϵt ∼N(0,IqM ). For x∈X , consider the MC-approximation ˆαN qEHVI (x) from (5). Denote by ∇x ˆαN qEHVI (x) the gradient of ˆαN qEHVI (x), obtained by averaging the gradients on the sample-level: ∇x ˆαN qEHVI (x) := 1 N N∑ t=1 ∇xHVI({ft(xi)}q i=1) (16) Let α∗ qEHVI := max x∈XαqEHVI (x) denote the maximum of the true acquisition function qEHVI , and let X∗:= arg maxx∈XαqEHVI (x) denote the set of associated maximizers. Theorem 2. Suppose that Xis compact and thatfhas a Multi-Output Gaussian Process prior with continuously differentiable mean and covariance functions. If the base samples {ϵt}N t=1 are drawn i.i.d. from N(0,IqM ), and if ˆx∗ N ∈arg maxx∈X ˆαN qEHVI (x), then (1) αqEHVI (ˆx∗ N ) →α∗ qEHVI a.s. (2) dist(ˆx∗ N ,X∗) →0 a.s. In addition to the almost sure convergence in Theorem 2, deriving a result on the convergence rate of the optimizer, similar to the one obtained in [5], should be possible. We leave this to future work. Moreover, the results in Theorem 2 can also be extended to the situation in which the base samples are generated using a particular class of randomized QMC methods (see similar results in [5]). Proof. We consider the setting from Balandat et al. [5, Section D.5]. Let ϵ ∼N(0,IqM ), so that we can write the posterior over outcome mat xas the random variable f(m)(x,ϵ) = S{ij,m}(µ(x) + L(x)ϵ), where µ(x) 7As noted in Wilson et al. [65], submodularity technically requires the search space Xto be ﬁnite, whereas in BO, it will typically be inﬁnite. Wilson et al. [65] note that in similar scenarios, submodularity has been extended to inﬁnite sets X(e.g. Srinivas et al. [58]). 19and L(x) are the (vector-valued) posterior mean and the Cholesky factor of posterior covariance, respectively, and S{ij,m}is an appropriate selection matrix (in particular, ∥S{ij,m}∥∞≤1 for all ij and m). Let A(x,ϵ) = K∑ k=1 q∑ j=1 ∑ Xj∈Xj (−1)j+1 M∏ m=1 [ z(m) k,Xj (ϵ) −l(m) k ] + where z(m) k,Xj (ϵ) = min [ u(m) k ,f(m)(xi1 ,ϵ),...,f (m)(xij ,ϵ) ] and Xj = {xi1 ,..., xij }. Following [5, Theorem 3], we need to show that there exists an integrable function ℓ: Rq×M ↦→R such that for almost every ϵand all x,y⊆X,x,y∈Rq×d, |A(x,ϵ) −A(y,ϵ)|≤ ℓ(ϵ)∥x−y∥. (17) Let us deﬁne ˜akmjXj (x,ϵ) := [ min [ u(m) k ,f(m)(xi1 ,ϵ),...,f (m)(xij ,ϵ) ] −l(m) k ] + . Linearity implies that it sufﬁces to show that this condition holds for ˜A(x,ϵ) := M∏ m=1 ˜akmjXj (x,ϵ) = M∏ m=1 [ min [ u(m) k ,f(m)(xi1 ,ϵ),...,f (m)(xij ,ϵ) ] −l(m) k ] + (18) for all k, j, and Xj. Observe that ˜akmjXj (x,ϵ) ≤ ⏐⏐⏐min [ u(m) k ,f(m)(xi1 ,ϵ),...,f (m)(xij ,ϵ) ] −l(m) k ⏐⏐⏐ ≤|l(m) k |+ ⏐⏐⏐min [ u(m) k ,f(m)(xi1 ,ϵ),...,f (m)(xij ,ϵ) ]⏐⏐⏐. Note that if u(m) k = ∞, then min[u(m) k ,f(x,ϵ)(m) i1 ,...f(m)(xij ,ϵ)] = min[f(m)(xi1 ,ϵ),...f(m)(xij ,ϵ)]. If u(m) k < ∞, then min[u(m) k ,f(m)(xi1 ,ϵ),...f(m)(xij ,ϵ)] < ⏐⏐min[f(m)(xi1 ,ϵ),...f(m)(xij ,ϵ)] ⏐⏐+ ⏐⏐u(m) k ⏐⏐. Let w(m) k = u(m) k if u(m) k <∞and 0 otherwise. Then ˜akmjXj (x,ϵ) ≤|l(m) k |+ |w(m) k |+ ⏐⏐min [ f(m)(xi1 ,ϵ),...,f (m)(xij ,ϵ) ]⏐⏐ ≤|l(m) k |+ |w(m) k |+ ∑ i1,...,ij ⏐⏐f(m)(xij ,ϵ) ⏐⏐. We therefore have that |˜akmjXj (x,ϵ)|≤| l(m) k |+ |w(m) k |+ |Xj| ( ∥µ(m)(x)∥+ ∥L(m)(x)∥∥ϵ∥ ) for all k,m,j,X j, where |Xj|denotes the cardinality of the set Xj. Under our assumptions (compactness of X, continuous differentiability of mean and covariance function), both µ(x) and L(x), as well as their respective gradients w.r.t. x, are uniformly bounded. In particular there exist C1,C2 <∞such that |˜akmjXj (x,ϵ)|≤ C1 + C2∥ϵ∥ for all k,m,j,X j. Dropping indices k,j,X j for simplicity, observe that ⏐⏐˜A(x,ϵ) −˜A(y,ϵ) ⏐⏐= ⏐⏐˜a1(x,ϵ)˜a2(x,ϵ) −˜a1(y,ϵ)˜a2(y,ϵ) ⏐⏐ (19a) = ⏐⏐˜a1(x,ϵ) ( ˜a2(x,ϵ) −˜a2(y,ϵ) ) + ˜a2(y,ϵ) ( ˜a1(x,ϵ) −˜a1(y,ϵ) )⏐⏐ (19b) ≤|˜a1(x,ϵ)| ⏐⏐˜a2(x,ϵ) −˜a2(y,ϵ) ⏐⏐+ |˜a2(y,ϵ)| ⏐⏐˜a1(x,ϵ) −˜a1(y,ϵ) ⏐⏐. (19c) Furthermore, |˜akmjXj (x,ϵ) −˜akmjXj (y,ϵ)|≤ ∑ i1,...,ij ⏐⏐S{ij,m}(µ(x) + L(x)ϵ) −S{ij,m}(µ(y) + L(y)ϵ) ⏐⏐ ≤|Xj| ( ∥µ(x) −µ(y)∥+ ∥L(x) −L(y)∥∥ϵ∥ ) . Since µand Lhave uniformly bounded gradients, they are Lipschitz. Therefore, there exist C3,C4 <∞such that |˜akmjXj (x,ϵ) −˜akmjXj (y,ϵ)|≤ (C3 + C4∥ϵ∥)∥x−y∥ 20for all x,y,k,m,j,X j. Plugging this into (19) above, we ﬁnd that ⏐⏐˜A(x,ϵ) −˜A(y,ϵ) ⏐⏐≤2 ( C1C3 + (C1C4 + C2C3)∥ϵ∥+ C2C4∥ϵ∥2 ) ∥x−y∥ for all x,yand ϵ. For M > 2 we generalize the idea from (19), making sure to telescope the respective expressions. It is not hard to see that with this, there exist C <∞such that ⏐⏐˜A(x,ϵ) −˜A(y,ϵ) ⏐⏐≤C M∑ m=1 ∥ϵ∥m∥x−y∥ Letting ℓ(ϵ) := C∑M m=1 ∥ϵ∥m, we observe that ℓ(ϵ) is integrable (since all absolute moments exist for the Normal distribution). The result now follows from in Balandat et al. [5, Theorem 3]. Besides the above convergence result, we can also show that the sample average gradient of the MC approximation of qEHVI is an unbiased estimator of the true gradient of qEHVI: Proposition 1. Suppose that the GP mean and covariance function are continuously differentiable. Suppose further that the candidate set xhas no duplicates, and that the sample-level gradients ∇xHVI({ft(xi)}q i=1) are obtained using the reparameterization trick as in [5]. Then E [ ∇x ˆαN qEHVI (x) ] = ∇xαqEHVI (x), (20) that is, the averaged sample-level gradient is an unbiased estimate of the gradient of the true acquisition function. Proof. This proof follows the arguments Wang et al.[63, Theorem 1], which leverages Glasserman[31, Theorem 1]. We verify the conditions of Glasserman [31, Theorem 1] below. Using the arguments from [5], we know that, under the assumption of differentiable mean and covariance functions, the samples ft(x) are continuously differentiable w.r.t. x(since there are no duplicates, and thus the covariance Σ(x) is non-singular). Hence, Glasserman [31, A1] is satisﬁed. Furthermore, it is easy to see from(1) that HVI({f(xi)}q i=1) is a.s. continuous and is differentiable w.r.t. ft(x) on RM , except on the edges of the hyper-rectangle decomposition {Sk}K k=1 of the non-dominated space, which satisﬁes [31, A3]. The set of points deﬁned by the union of these edges clearly has measure zero under any non-degenerate (non-singular covariance) GP posterior on RM , so Glasserman [31, A4] holds. Therefore Glasserman [31, Lemma 2] holds, so HVI({f(xi)}q i=1) is a.s. piece-wise differentiable w.r.t. x. Lastly, we need to show that the result in Glasserman [31, Lemma 3] holds: E [ sup xci /∈˜D |A′(x,ϵ)| ] <∞. As in Wang et al.[63, Theorem 1], we ﬁx xexcept for xci where xci is the cth component of the ith point, We need to show that E [ supxci /∈˜D |A′(x,ϵ)| ] <∞. By linearity, it sufﬁces to show that E [ supxci /∈˜D |˜A′(x,ϵ)| ] <∞. We have E [ sup xci /∈˜D |˜A′(x,ϵ)| ] = E [ sup xci /∈˜D ⏐⏐⏐⏐ ∂˜A(x,ϵ) ∂xci ⏐⏐⏐⏐ ] . Consider the M = 2 case. We have ˜A(x,ϵ) = a1(x,ϵ)a2(x,ϵ), where am(x,ϵ) = [ min [ u(m) k ,f(m)(xi1 ,ϵ),...,f (m)(xij ,ϵ) ] −l(m) k ] + . The partial derivative of ˜A(x,ϵ) with respect to xci is ∂˜A(x,ϵ) ∂xci = ∂a1(x,ϵ) ∂xci a2(x,ϵ) + a1(x,ϵ)∂a2(x,ϵ) ∂xci , and therefore ⏐⏐⏐∂˜A(x,ϵ) ∂xci ⏐⏐⏐≤ ⏐⏐⏐∂a1(x,ϵ) ∂xci ⏐⏐⏐· ⏐⏐⏐a2(x,ϵ) ⏐⏐⏐+ ⏐⏐⏐a1(x,ϵ) ⏐⏐⏐· ⏐⏐⏐∂a2(x,ϵ) ∂xci ⏐⏐⏐ Since we are only concerned with xci /∈ ˜D, am(x,ϵ) = [ min [ f(m)(xi1 ,ϵ),...,f (m)(xij ,ϵ) ] −l(1) k ] + . 21As in the proof of Theorem 2, we write the posterior over outcome mat xas the random variable f(m)(x,ϵ) = S{ij,m}(µ(x) + L(x)ϵ), where ϵ ∼N(0,IqM ) and S{ij,m}is an appropriate selection matrix. With this, am(x,ϵ) = [ min [ S{i1,1} ( µ(x) + L(x)ϵ ) ,...,S {ij,1} ( µ(x) + L(x)ϵ )] −l(1) k ] + . Since the interval X is compact and the mean, covariance, and Cholesky factor of the covariance µ(x),C(x),L(x) are continuously differentiable, for all mwe have sup xci ⏐⏐⏐⏐ ∂µ(m)(xa) ∂xci ⏐⏐⏐⏐= µ∗,(m) a <∞, sup xci ⏐⏐⏐⏐ ∂L(m)(x) ∂xci ⏐⏐⏐⏐= L∗,(m) ca <∞. Let µ(m) ∗∗ = maxa µ∗,(m) a , L(m) ∗∗ = maxa,b L∗,(m) ab (x), where L(m) ab is the element at row a, column bin L(m), the Cholesky factor for outcome m. Let ϵ(m) ∈Rq denote the vector of i.i.d. N(0,1) samples corresponding to outcome m. Then we have⏐⏐⏐⏐ ∂ ∂xci [ [min [ S{i1,1} ( µ(x) + L(x)ϵ ) ,...,S {ij,1} ( µ(x) + L(x)ϵ )] −l(1) k ] + ⏐⏐⏐⏐ ≤ ⏐⏐⏐ [ µ(m) ∗∗ + L(m) ∗∗ ||ϵ(m)||1 −l(m) k ] + ⏐⏐⏐ ≤ ⏐⏐⏐µ(m) ∗∗ + L(m) ∗∗ ||ϵ(m)||1 ⏐⏐⏐+ ⏐⏐⏐l(m) k ⏐⏐⏐. Under our assumptions (compactness of X, continuous differentiability of mean and covariance function) both µ(x) and L(x), as well as their respective gradients, are uniformly bounded. In particular there exist C(m) 1 ,C(m) 2 <∞such that ⏐⏐S{a,m} ( µ(x) + L(x)ϵ ) −l(m) k ⏐⏐≤C(m) 1 + C(m) 2 ||ϵ(m)||1 for all a= i1,...,i j. Hence, ⏐⏐⏐⏐ ∂˜A(x,ϵ) ∂xci ⏐⏐⏐⏐≤ [⏐⏐⏐µ(1) ∗∗ + C(1) ∗∗||ϵ(1)||1 ⏐⏐⏐+ ⏐⏐⏐l(1) k ⏐⏐⏐ ][ C(2) 1 + C(2) 2 ||ϵ(2)||1 ] + [ C(1) 1 + C(1) 2 ||ϵ(1)||1 ][⏐⏐⏐µ(2) ∗∗ + C(2) ∗∗||ϵ(2)||1 ⏐⏐⏐+ ⏐⏐⏐l(2) k ⏐⏐⏐ ] Since ϵis absolutely integrable, E (⏐⏐⏐⏐ ∂˜A(x,ϵ) ∂xci ⏐⏐⏐⏐ ) <∞. Hence, E [ supxci /∈˜D |A′(x,ϵ)| ] <∞. This can be extended to M >2 in the same manner using the product rule to obtain E (∂˜A(x,ϵ) ∂xci ) ≤ M∑ m=1 ([⏐⏐⏐µ(m) ∗∗ + C(m) ∗∗ E[||ϵ(m)||1] ⏐⏐⏐+ ⏐⏐⏐l(1) k ⏐⏐⏐ ] M∏ n=1,n̸=m [ C(n) 1 + C(n) 2 E[||ϵ(n)||1] ]) ≤ M∑ m=1 ([⏐⏐⏐µ(m) ∗∗ + π 2 qC(m) ∗∗ ⏐⏐⏐+ ⏐⏐⏐l(1) k ⏐⏐⏐ ] M∏ n=1,n̸=m [ C(n) 1 + π 2 qC(n) 2 ] ]) . Hence, E [ supxci /∈˜D |A′(x,ϵ)| ] <∞for M ≥2 and Glasserman [31, Theorem 1] holds. 22D Monte-Carlo Approximation Figure 5b shows the gradient of analytic EHVI and the MC estimator qEHVI on slice of a 3-objective problem. Even using only N = 32 QMC samples, the average sample gradient has very low variance. Moreover, ﬁxing the base samples also greatly reduces the variance without introducing bias. 0.0 0.1EHVI MC,/uni00A0N=32 analytic qMC,/uni00A0N=32 analytic 0.00 0.25 0.50 0.75 1.00 0.0 0.1EHVI MC,/uni00A0N=32/uni00A0(fixed) analytic 0.00 0.25 0.50 0.75 1.00 qMC,/uni00A0N=32/uni00A0(fixed) analytic (a) A comparison of the analytic EHVI acquisition function and the MC-based qEHVI for q= 1. 0.1 0.0 0.1 /uni00A0EHVI MC,/uni00A0N=32 analytic qMC,/uni00A0N=32 analytic 0.00 0.25 0.50 0.75 1.00 0.1 0.0 0.1 /uni00A0EHVI MC,/uni00A0N=32/uni00A0(fixed) analytic 0.00 0.25 0.50 0.75 1.00 qMC,/uni00A0N=32/uni00A0(fixed) analytic (b) A comparison of the exact gradient of analytic EHVI and the exact sample average gradient of the MC-based qEHVI for q= 1. Figure 5: A comparison of (a) the analytic EHVI and MC-based qEHVI for q = 1 and (b) a comparison of the exact gradient ∇αEHVI of analytic EHVI and average sample gradient of the MC-estimator ∇ˆαqEHVI over a slice of the input space on a DTLZ2 problem (q= 1, M = 3, d= 6) [15]. x(0) is varied across 0 ≤λ≤1, while x(i) for 1,...D are held constant. In each of (a) and (b), the top row show qEHVI where the (quasi-)standard normal base samples are resampled for each value of x(0). The solid line is one sample average (across (q)MC samples) and the shaded area is the mean plus 2 standard errors across 50 repetitions. The bottom row uses the same base samples for evaluating each test point and the sample average for each of 50 repetitions is plotted. E Experiment Details E.1 Algorithms For TS-TCH, we draw a sample from the joint posterior over a discrete set of 1000dpoints sampled from a scrambled Sobol sequence. For PESMO, we follow [27] and use a Pareto set of size 10 for each sampled GP, which is optimized over a discrete set of 1000dpoints sampled from a scrambled Sobol sequence. The current 23Table 2: Reference points for all benchmark problems. Assuming minimization. In our benchmarks, equivalently maximize the negative objectives and multiply the reference points by -1. PROBLEM REFERENCE POINT BRANIN CURRIN (18.0, 6.0) DTLZ2 (1.1,..., 1.1) ∈RM ABR (-150.0, 3500.0, 5.1) VEHICLE CRASH SAFETY (1864.72022, 11.81993945, 0.2903999384) CONSTRAINED BRANIN CURRIN (90.0, 10.0) C2-DTLZ2 (1.1,..., 1.1) ∈RM Pareto front is approximated by optimizing the posterior means over a grid as is done in Garrido-Merchán and Hernández-Lobato [26, 27]. For SMS-EGO, we use the observed Pareto front. All acquisition functions are optimized with L-BFGS-B (with a maximum of 200 iterations); SMS-EGO [53] and PESMO [26] use gradients approximated by ﬁnite differences and all other methods use exact gradients. For all methods, each outcome is modeled with an independent Gaussian process with a Matern 5/2 ARD kernel. The methods implemented in Spearmint use a fully Bayesian treatment of the hyperparameters with 10 samples from posterior over the hyperparamters, and the methods implemented in BoTorch use maximum a posteriori estimates of the GP hyperparameters. All methods are initialized with 2(d+ 1)points from a scrambled Sobol sequence. qPAREGO and qEHVI use N = 128 QMC samples. E.1.1 Reference point speciﬁcation There is a large body of literature on the effects of reference point speciﬁcation [4, 35, 36]. The hypervolume indicator is sensitive to speciﬁed the reference point: a reference point that is far away from the Pareto front will favor extreme points, where as reference point that is close to the Pareto front gives more weight to less extreme points [36]. Sensitivity to the reference point is affects both the evaluation of different MO methods and the utility function for methods that rely HV. In practice, a decision maker may be able to specify a reference point that satisﬁes their preference with domain knowledge. If a reference point is provided by the decision maker, previous work has suggested heuristics for choosing reference points for use in an algorithm’s utility function [35, 53]. We follow previous work [69, 68] and assume that the reference point is known. We also considered (but did not use in our experiments) a dynamic reference point strategy where at each BO iteration, the reference point is selected to be a point slightly worse than the nadir (component-wise minimum) point of the current observed Pareto front for computing the acquisition function: r = ynadir −0.1 ·|ynadir| where ynadir = ( miny(1)∈D(1) y(1),..., miny(m)∈D(m) y(m)) . This reference point is used in SMS-EMOA in Ishibuchi et al. [35]), and we ﬁnd similar average performance (but higher variance) on problems to using a known reference point with continuous Pareto fronts. If the Pareto front is discontinuous, then it is possible not all sections of the Pareto front will be reached. E.1.2 qPAREGO Previous work has only considered unconstrained sequential optimization with ParEGO [40, 7] and ParEGO is often optimized with gradient-free methods [ 53]. To the best of our knowledge, qPAREGO is the ﬁrst to support parallel and constrained optimization. Moreover, we compute exact gradients via auto-differentiation for acquisition optimization. ParEGO is typically implemented by applying augmented Chebyshev scalarization and modeling the scalarized outcome [40]. However, recent work has shown that composite objectives offer improved optimization performance [3]. qPAREGO uses a MC-based Expected Improvement [38] acquisition function, where the objectives are modeled independently and the augmented Chebyshev scalarization [40] is applied to the posterior samples as a composite objective. This approach enables the use of sequential greedy optimization of qcandidates with proper integration over the posterior at the pending points. Importantly, the sequential greedy approach allows for using different random scalarization weights for selecting each of the q candidates. qPAREGO is extended to the constrained setting by weighting the EI by the probability of feasibility [25]. We estimate the probability of feasiblity using the posterior samples and approximate the indicator function with a sigmoid to maintain differentiablity as in constrained qEHVI . qPAREGO is trivially extended to the noisy setting using Noisy Expected Improvement [43, 5], but we use Expected Improvement in our experiments as all of the problems are noiseless. E.2 Benchmark Problems The details for the benchmark problems below assume minimization of all objectives. Table 2 provides the reference points used for all benchmark problems. 24Branin-Currin f(1)(x′ 1,x′ 2) = (x2 −5.1 4π2 x2 1 + 5 πx1 −r)2 + 10(1 − 1 8π) cos(x1) + 10 f(2)(x1,x2) = [ 1 −exp ( − 1 (2x2) )]2300x3 1 + 1900x2 1 + 2092x1 + 60 100x3 1 + 500x2 1 + 4x1 + 20 where x1,x2 ∈[0,1], x′ 1 = 15x1 −5, and x′ 2 = 15x2. The constrained Branin-Currin problem uses the following disk constraint from [29]: c(x′ 1,x′ 2) = 50 −(x′ 1 −2.5)2 −(x′ 2 −7.5)2) ≥0 DTLZ2 The objectives are given by [15]: f1(x) = (1 + g(xM )) cos (π 2 x1 ) ··· cos (π 2 xM−2 ) cos (π 2 xM−1 ) f2(x) = (1 + g(xM )) cos (π 2 x1 ) ··· cos (π 2 xM−2 ) sin (π 2 xM−1 ) f3(x) = (1 + g(xM )) cos (π 2 x1 ) ··· sin (π 2 xM−2 ) ... fM (x) = (1 + g(xM )) sin (π 2 x1 ) where g(x) = ∑ xi∈xM (xi −0.5)2,x∈[0,1]d,and xM represents the last d−M + 1 elements of x. The C2-DTLZ2 problem adds the following constraint [16]: c(x) = −min [ M min i=1 ( (fi(x) −1)2 + M∑ j=1,j=i (f2 j −r2) ) , ( M∑ i=1 ( (fi(x) − 1√ M )2 −r2))] ≥0 Vehicle Crash Safety The objectives are given by [60]: f1(x) = 1640.2823 + 2.3573285x1 + 2.3220035x2 + 4.5688768x3 + 7.7213633x4 + 4.4559504x5 f2(x) = 6.5856 + 1.15x1 −1.0427x2 + 0.9738x3 + 0.8364x4 −0.3695x1x4 + 0.0861x1x5 + 0.3628x2x4 + 0.1106x2 1 −0.3437x2 3 + 0.1764x2 4 f3(x) = −0.0551 + 0.0181x1 + 0.1024x2 + 0.0421x3 −0.0073x1x2 + 0.024x2x3 −0.0118x2x4 −0.0204x3x4 −0.008x3x5 −0.0241x2 2 + 0.0109x2 4 where x∈[1,3]5. Policy Optimization for Adaptive Bitrate Control The controller is given by: at = x0 ˆzbd,t + x2zbf,t + x3, where ˆzbd,t = ∑ ti<t zbd,ti exp(−x1ti) ∑ ti<t exp(−x1ti) is estimated bandwidth at time tusing an exponential moving average, zbf,t is the buffer occupancy at time t, and x0,...x3 are the parameters we seek to optimize. We evaluate each policy on a set of 400 videos, where the number of time steps (chunks) in each video stream trajectory depends on the size of the video. 25Table 3: Acquisition Optimization wall time in seconds on a CPU (2x Intel Xeon E5-2680 v4 @ 2.40GHz) and on a GPU (Tesla V100-SXM2-16GB). The mean and two standard errors are reported. NA indicates that the algorithm does not support constraints. CPU CONSTRAINED BRANIN CURRIN DTLZ2 PESMO ( q=1) NA 278.53 (±25.66) SMS-EGO ( q=1) NA 104.26 (±7.66) TS-TCH ( q=1) NA 52.55 (±0.06) qPAREGO ( q=1) 2.4 (±0.37) 4 .68 (±0.46) EHVI ( q=1) NA 3.58 (±0.28) qEHVI ( q=1) 5.69 (±0.43) 5 .95 (±0.45) GPU CONSTRAINED BRANIN CURRIN DTLZ2 TS-TCH ( q=1) NA 0.25 (±0.00) TS-TCH ( q=2) NA 0.27 (±0.00) TS-TCH ( q=4) NA 0.28 (±0.00) TS-TCH ( q=8) NA 0.32 (±0.01) qPAREGO ( q=1) 3.52 (±0.34) 9 .04 (±0.93) qPAREGO ( q=2) 6.0 (±0.56) 14 .23 (±1.55) qPAREGO ( q=4) 12.07 (±0.98) 40 .5 (±3.21) qPAREGO ( q=8) 33.1 (±3.32) 84 .15 (±6.9) EHVI ( q=1) NA 84.15 (±6.9) qEHVI ( q=1) 5.61 (±0.17) 10 .21 (±0.58) qEHVI ( q=2) 19.06 (±5.88) 17 .75 (±0.97) qEHVI ( q=4) 29.26 (±2.01) 40 .41 (±2.78) qEHVI ( q=8) 91.56 (±5.51) 106 .51 (±7.69) F Additional Empirical Results F.1 Additional Sequential Optimization Results We include results for an additional synthetic benchmark: the DTLZ2 problem from the MO literature [ 15] (d= 6,M = 2). Figure 6 shows that qEHVI outperforms all other baseline algorithms on the DTLZ2 in terms of sequential optimization performance with competitive wall times as shown in 3. 0 20 40 60 80 100 Function/uni00A0Evaluations 1.8 1.6 1.4 1.2 1.0 0.8 0.6 0.4 log/uni00A0HV/uni00A0DifferenceSobol EHVI qEHVI qParEGO TS/uni00ADTCH PESMO SMS/uni00ADEGO Figure 6: Optimization performance on the DTLZ2 synthetic function (d= 6,M = 2). F.2 Performance with Increasing Parallelism Figure 7 shows that that the performance of qEHVI performance does not degrade substantially, whereas performance does degrade for qPAREGO and TS-TCH on some benchmark problems. We include results for all problems in Section 5 and Appendix F.1 as well as a Constrained Branin-Currin problem (which is described in Appendix E.2). 260 20 40 60 80 100 Batch/uni00A0Iteration 0.0 0.5 1.0 1.5 2.0log/uni00A0HV/uni00A0Difference Sobol/uni00A0q=8 TS/uni00A0q=1 TS/uni00A0q=2 TS/uni00A0q=4 TS/uni00A0q=8 qEHVI/uni00A0q=1 qEHVI/uni00A0q=2 qEHVI/uni00A0q=4 qEHVI/uni00A0q=8 qParEGO/uni00A0q=1 qParEGO/uni00A0q=2 qParEGO/uni00A0q=4 qParEGO/uni00A0q=8 (a) VEHICLE SAFETY 0 20 40 60 80 100 Function/uni00A0Evaluations 0.0 0.5 1.0 1.5 2.0log/uni00A0HV/uni00A0Difference Sobol/uni00A0q=8 TS/uni00A0q=1 TS/uni00A0q=2 TS/uni00A0q=4 TS/uni00A0q=8 qEHVI/uni00A0q=1 qEHVI/uni00A0q=2 qEHVI/uni00A0q=4 qEHVI/uni00A0q=8 qParEGO/uni00A0q=1 qParEGO/uni00A0q=2 qParEGO/uni00A0q=4 qParEGO/uni00A0q=8 (b) VEHICLE SAFETY 0 20 40 60 80 100 Batch/uni00A0Iteration 1.1 1.0 0.9 0.8 0.7 0.6 0.5 0.4 log/uni00A0HV/uni00A0Difference Sobol/uni00A0q=8 qEHVI/uni00A0q=1 qEHVI/uni00A0q=2 qEHVI/uni00A0q=4 qEHVI/uni00A0q=8 qParEGO/uni00A0q=1 qParEGO/uni00A0q=2 qParEGO/uni00A0q=4 qParEGO/uni00A0q=8 (c) C2DTLZ2 0 20 40 60 80 100 Function/uni00A0Evaluations 1.1 1.0 0.9 0.8 0.7 0.6 0.5 0.4 log/uni00A0HV/uni00A0Difference Sobol/uni00A0q=8 qEHVI/uni00A0q=1 qEHVI/uni00A0q=2 qEHVI/uni00A0q=4 qEHVI/uni00A0q=8 qParEGO/uni00A0q=1 qParEGO/uni00A0q=2 qParEGO/uni00A0q=4 qParEGO/uni00A0q=8 (d) C2DTLZ2 0 20 40 60 80 100 Batch/uni00A0Iteration 0.0 0.5 1.0 1.5log/uni00A0HV/uni00A0Difference Sobol/uni00A0q=8 TS/uni00A0q=1 TS/uni00A0q=2 TS/uni00A0q=4 TS/uni00A0q=8 qEHVI/uni00A0q=1 qEHVI/uni00A0q=2 qEHVI/uni00A0q=4 qEHVI/uni00A0q=8 qParEGO/uni00A0q=1 qParEGO/uni00A0q=2 qParEGO/uni00A0q=4 qParEGO/uni00A0q=8 (e) BRANIN CURRIN 0 20 40 60 80 100 Function/uni00A0Evaluations 0.0 0.5 1.0 1.5log/uni00A0HV/uni00A0Difference Sobol/uni00A0q=8 TS/uni00A0q=1 TS/uni00A0q=2 TS/uni00A0q=4 TS/uni00A0q=8 qEHVI/uni00A0q=1 qEHVI/uni00A0q=2 qEHVI/uni00A0q=4 qEHVI/uni00A0q=8 qParEGO/uni00A0q=1 qParEGO/uni00A0q=2 qParEGO/uni00A0q=4 qParEGO/uni00A0q=8 (f) BRANIN CURRIN Figure 7: Optimization performance of parallel acquisition functions over batch BO iterations (left) and function evaluations (right) for benchmark problems in Section 5. 270 20 40 60 80 100 Batch/uni00A0Iteration 0.5 1.0 1.5 2.0 2.5log/uni00A0HV/uni00A0Difference Sobol/uni00A0q=8 qEHVI/uni00A0q=1 qEHVI/uni00A0q=2 qEHVI/uni00A0q=4 qEHVI/uni00A0q=8 qParEGO/uni00A0q=1 qParEGO/uni00A0q=2 qParEGO/uni00A0q=4 qParEGO/uni00A0q=8 (a) CONSTRAINED BRANIN CURRIN 0 20 40 60 80 100 Function/uni00A0Evaluations 0.5 1.0 1.5 2.0 2.5log/uni00A0HV/uni00A0Difference Sobol/uni00A0q=8 qEHVI/uni00A0q=1 qEHVI/uni00A0q=2 qEHVI/uni00A0q=4 qEHVI/uni00A0q=8 qParEGO/uni00A0q=1 qParEGO/uni00A0q=2 qParEGO/uni00A0q=4 qParEGO/uni00A0q=8 (b) CONSTRAINED BRANIN CURRIN 0 20 40 60 80 100 Batch/uni00A0Iteration 1.8 1.6 1.4 1.2 1.0 0.8 0.6 0.4 log/uni00A0HV/uni00A0Difference Sobol/uni00A0q=8 TS/uni00A0q=1 TS/uni00A0q=2 TS/uni00A0q=4 TS/uni00A0q=8 qEHVI/uni00A0q=1 qEHVI/uni00A0q=2 qEHVI/uni00A0q=4 qEHVI/uni00A0q=8 qParEGO/uni00A0q=1 qParEGO/uni00A0q=2 qParEGO/uni00A0q=4 qParEGO/uni00A0q=8 (c) DTLZ2 ( M = 2,d = 6) 0 20 40 60 80 100 Function/uni00A0Evaluations 1.8 1.6 1.4 1.2 1.0 0.8 0.6 0.4 log/uni00A0HV/uni00A0Difference Sobol/uni00A0q=8 TS/uni00A0q=1 TS/uni00A0q=2 TS/uni00A0q=4 TS/uni00A0q=8 qEHVI/uni00A0q=1 qEHVI/uni00A0q=2 qEHVI/uni00A0q=4 qEHVI/uni00A0q=8 qParEGO/uni00A0q=1 qParEGO/uni00A0q=2 qParEGO/uni00A0q=4 qParEGO/uni00A0q=8 (d) DTLZ2 ( M = 2,d = 6) Figure 8: Optimization performance of parallel acquisition functions over batch BO iterations (left) and function evaluations (right) for additional benchmark problems. 28F.3 Noisy Observations Although neither qEHVI nor any variant of expected hypervolume improvement (to our knowledge) directly account for noisy observations, noisy observations are a practical challenge. We empirically evaluate the performance of all algorithms on a Branin-Currin function where observations have additive, zero-mean, iid Gaussian noise; the unknown standard deviation of the noise is set to be 1% of the range of each objective. Fig 9 shows that qEHVI performs favorably in the presence of noise, besting all algorithms including Noisy qPAREGO (qNParego) (described in Appendix E.1.2), PESMO and TS-TCH, all of which account for noise. 0 20 40 60 80 100 Function/uni00A0Evaluations 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8log/uni00A0HV/uni00A0Difference Sobol EHVI qEHVI qParEGO qNParEGO TS/uni00ADTCH Figure 9: Sequential optimization performance on a noisy Branin-Currin problem. F.4 Approximate Box Decompositions EHVI becomes prohibitively computationally expensive in many scenarios with ≥4 objectives because of the wall time of partitioning the non-dominated space into disjoint rectangles [ 11]. Therefore, in addition to providing an exact binary partitioning algorithm, Couckuyt et al. [11] propose an approximation that terminates the partitioning algorithm when the new additional set of hyper-rectangles in the partitioning has a total hypervolume of less than a predetermined fraction ζof the hypervolume dominated by the Pareto front. While qEHVI is guaranteed to be exact when an exact partitioning of the non-dominated space is used, qEHVI is agnostic to the partitioning algorithm used and is compatible with more scalable approximate methods. We evaluate the performance ofqEHVI with approximation of various ﬁdelities ζon DTLZ2 problems with 3 and 4 objectives (with d = 6 ). ζ = 0 corresponds to an exact partitioning and the approximation is monotonically worse as ζincreases. Larger values of ζdegrade optimization performance (Figure 10), but can result in substantial speedups (Table 4). Even with coarser levels of approximation, qEHVI () performs better than qPAREGO with respect to log hypervolume difference, while achieving wall time improvements of 2-7x compared to exact qEHVI. 0 20 40 60 80 100 Function/uni00A0Evaluations 1.0 0.8 0.6 0.4 0.2 log/uni00A0HV/uni00A0Difference qEHVI/uni00A0( = 10 3) qEHVI/uni00A0( = 10 4) qEHVI/uni00A0( = 10 5) qEHVI/uni00A0( = 10 6) qEHVI/uni00A0(exact) qParEGO (a) 0 20 40 60 80 100 Function/uni00A0Evaluations 0.7 0.6 0.5 0.4 0.3 0.2 0.1 log/uni00A0HV/uni00A0DifferenceqEHVI/uni00A0( = 10 3) qEHVI/uni00A0( = 10 4) qEHVI/uni00A0( = 10 5) qEHVI/uni00A0( = 10 6) qEHVI/uni00A0(exact) qParEGO (b) Figure 10: Optimization performance on DTLZ2 problems (d= 6) with approximate partitioning using various approximation levels ζfor (a) M = 3 objectives and (b) M = 4 objectives. 29CPU DTLZ2 ( M = 3) DTLZ2 ( M = 4) qPAREGO 5.86 (±0.51) 5 .6 (±0.53) qEHVI ( ζ = 10−3) 6.89 (±0.41) 9 .53 (±0.49) qEHVI ( ζ = 10−4) 9.83 (±0.9) 17 .47 (±1.2) qEHVI ( ζ = 10−5) 18.99 (±2.72) 60 .27 (±3.57) qEHVI ( ζ = 10−6) 37.9 (±7.47) 136 .15 (±12.88) qEHVI ( EXACT ) 45.52 (±9.83) 459 .33 (±77.95) Table 4: Acquisition function optimization wall time with approximate hypervolume computation, in seconds on a CPU (2x Intel Xeon E5-2680 v4 @ 2.40GHz). The mean and two standard errors are reported. F.5 Acquisition Computation Time Figure 11 show the acquisition computation time for different M and q. The inﬂection points corresponds to available processor cores becoming saturated. For large M an qon the GPU, memory becomes an issue, but we discuss ways of mitigating the issue in Appendix A.4. 2.5 5.0 7.5 10.0 12.5 15.0 17.5 q 0 1 2 3 4 5 6Acquisition/uni00A0Computation/uni00A0Time/uni00A0(s) M=2/uni00A0(CPU) M=3/uni00A0(CPU) M=4/uni00A0(CPU) M=2/uni00A0(GPU) M=3/uni00A0(GPU) M=4/uni00A0(GPU) Figure 11: Acquisition computation time for different batch sizes qand numbers of objectives M (this excludes the time required to compute the acquisition function given box decomposition of the non-dominated space). This uses N = 512 MC samples, d= 6, |P|= 10, and 20 training points. CPU time was measured on 2x Intel Xeon E5-2680 v4 @ 2.40GHz and GPU time was measured on a Tesla V100-SXM2-16GB GPU using 64-bit ﬂoating point precision. The mean and 2 standard errors over 1000 trials are reported. 30",
      "meta_data": {
        "arxiv_id": "2006.05078v3",
        "authors": [
          "Samuel Daulton",
          "Maximilian Balandat",
          "Eytan Bakshy"
        ],
        "published_date": "2020-06-09T06:57:47Z",
        "venue": "Advances in Neural Information Processing Systems 33, 2020",
        "pdf_url": "https://arxiv.org/pdf/2006.05078v3.pdf"
      }
    },
    {
      "title": "Multi-fidelity Bayesian Optimization with Max-value Entropy Search and its Parallelization",
      "abstract": "In a standard setting of Bayesian optimization (BO), the objective function\nevaluation is assumed to be highly expensive. Multi-fidelity Bayesian\noptimization (MFBO) accelerates BO by incorporating lower fidelity observations\navailable with a lower sampling cost. In this paper, we focus on the\ninformation-based approach, which is a popular and empirically successful\napproach in BO. For MFBO, however, existing information-based methods are\nplagued by difficulty in estimating the information gain. We propose an\napproach based on max-value entropy search (MES), which greatly facilitates\ncomputations by considering the entropy of the optimal function value instead\nof the optimal input point. We show that, in our multi-fidelity MES (MF-MES),\nmost of additional computations, compared with usual MES, is reduced to\nanalytical computations. Although an additional numerical integration is\nnecessary for the information across different fidelities, this is only in one\ndimensional space, which can be performed efficiently and accurately. Further,\nwe also propose parallelization of MF-MES. Since there exist a variety of\ndifferent sampling costs, queries typically occur asynchronously in MFBO. We\nshow that similar simple computations can be derived for asynchronous parallel\nMFBO. We demonstrate effectiveness of our approach by using benchmark datasets\nand a real-world application to materials science data.",
      "full_text": "Multi-ﬁdelity Bayesian Optimization with Max-value Entropy Search and its parallelization Shion Takeno1, Hitoshi Fukuoka2, Yuhki Tsukada3, Toshiyuki Koyama4, Motoki Shiga5, Ichiro Takeuchi6, and Masayuki Karasuyama7 1,6,7Nagoya Institute of Technology 2,3,4Nagoya University 3,5,7Japan Science and Technology Agency 5Gifu University 6,7National Institute for Material Science 5,6RIKEN Center for Advanced Intelligence Project takeno.s.mllab.nit@gmail.com, fukuoka.hitoshi@j.mbox.nagoya-u.ac.jp, {tsukada.yuhki,koyama.toshiyuki}@material.nagoya-u.ac.jp, shiga m@gifu-u.ac.jp, {takeuchi.ichiro,karasuyama}@nitech.ac.jp Abstract In a standard setting of Bayesian optimization (BO), the objective function evaluation is assumed to be highly expensive. Multi-ﬁdelity Bayesian optimization (MFBO) accelerates BO by incorporating lower ﬁdelity observations available with a lower sampling cost. In this paper, we focus on the information- based approach, which is a popular and empirically successful approach in BO. For MFBO, however, existing information-based methods are plagued by diﬃculty in estimating the information gain. We propose an approach based on max-value entropy search (MES), which greatly facilitates computations by considering the entropy of the optimal function value instead of the optimal input point. We show that, in our multi-ﬁdelity MES (MF-MES), most of additional computations, compared with usual MES, is reduced to analytical computations. Although an additional numerical integration is necessary for the information across diﬀerent ﬁdelities, this is only in one dimensional space, which can be performed eﬃciently and accurately. Further, we also propose parallelization of MF-MES. Since there exist a variety of diﬀerent sampling costs, queries typically occur asynchronously in MFBO. We show that similar simple computations can be derived for asynchronous parallel MFBO. We demonstrate eﬀectiveness of our approach by using benchmark datasets and a real-world application to materials science data. 1 arXiv:1901.08275v2  [stat.ML]  13 Feb 20201 Introduction Bayesian optimization (BO) is a popular machine-learning technique for the black-box optimization problem. Eﬃciency of BO has been widely shown in a variety of application areas such as scientiﬁc experiments (Wigley et al., 2016), simulation calculations (Ramprasad et al., 2017), and tuning of machine-learning methods (Snoek et al., 2012). In these scenarios, observing an objective function value is usually quite expensive and thus achieving the optimal value with low querying cost is strongly demanded. Although standard BO only considers directly querying to an objective function f(x), in many practical problems, lower ﬁdelity approximations of the original objective function can be observed. For example, theoretical computations of physical processes often have multiple levels of approximations by which the trade-oﬀ between the computational cost and accuracy can be controlled. A goal of multi-ﬁdelity Bayesian optimization (MFBO) is to accelerate BO by utilizing those lower ﬁdelity observations to reduce the total cost of the optimization. In this paper, we focus on the information-based approach. For usual BO without multi-ﬁdelity, which we call single ﬁdelity BO , seminal works of this direction are entropy search (ES) and predictive entropy search (PES) proposed by Hennig & Schuler (2012) and Hern´ andez-Lobato et al. (2014), respectively. They deﬁne acquisition functions by using information gain for the optimal solution x∗:= argmaxx f(x). Unlike classical evaluation measures such as expected improvement, the information-based criterion is a measure of global utility which does not require any additional exploit-explore trade-oﬀ parameter. The superior performance of information-based methods have been shown empirically, and then, the same approach has also been extended to the multi-ﬁdelity setting (Swersky et al., 2013; Zhang et al., 2017). Even in the case of single ﬁdelity BO, however, accurately evaluating information gain is notoriously diﬃcult, which often requires complicated numerical approximations. For MFBO, evaluating information across multiple ﬁdelities is further diﬃcult. To overcome this diﬃculty, we consider a novel information-based approach to MFBO, which is based on a variant of ES called max-value entropy search (MES), proposed by Wang & Jegelka (2017). MES considers the information gain for f∗:= maxx f(x) instead of x∗. This greatly facilitates the computation of the information gain because f∗is in one dimensional space unlike x∗, and they showed superior performance of MES compared with ES/PES. Our method, called multi-ﬁdelity MES (MF-MES), can evaluate the information gain for f∗from an observation of an arbitrary ﬁdelity, and we show that additional expressions, compared with MES, can be derived analytically except for one dimensional integral, which can be calculated accurately and eﬃciently by using standard numerical integration techniques. This enables us to obtain more reliable evaluation of information gain easily unlike existing information-based MFBO methods because they contain approximations which are diﬃcult to justify. Our MF-MES is also advantageous to other measures of global utility for MFBO, such as the knowledge gradient-based method (Poloczek et al., 2017), because they are often computationally extremely complicated. Section 5 discusses related studies in more detail. Further, we also propose parallelization of MF-MES. Since objective functions have a variety of sampling 2costs, queries naturally occur asynchronously in MFBO. We extend our information gain so that points currently being queried can be taken into consideration. Similarly in the case of MF-MES, we show that a required numerical integration in addition to the sampling of f∗ is also reduced to one dimensional space through the integration by substitution. This allows us to obtain the reliable evaluation of the information gain for the parallel extension of MF-MES. Our main contributions are summarized as follows: 1. We develop an information-theoretic eﬃcient MFBO method. Na¨ ıve formulation and implementation of this problem raise computationally challenging issues that need to be addressed by carefully-tuned and time-consuming approximate computations. By using several computational tricks mainly inspired by MES (Wang & Jegelka, 2017), we show that this computational bottleneck can be nicely avoided without additional assumptions or approximations. 2. We develop an information-theoretic asynchronous parallel MFBO method. To our knowledge, there are no existing works in this topic — We believe that our method is useful in many practical experimental design and black-box optimization tasks with multiple information sources with diﬀerent ﬁdelities and its parallel evaluation. We empirically demonstrate eﬀectiveness of our approach by using benchmark functions and a real-world application to materials science data. 2 Preliminary In this section, we ﬁrst brieﬂy review a multi-ﬁdelity extension of Gaussian process regression (GPR). Suppose that y(1) x ,...,y (M) x are the observations at x ∈X ⊂Rd with M diﬀerent ﬁdelities in which y(M) x is the highest ﬁdelity and y(1) x is the lowest ﬁdelity. Each observation is modeled as y(m) x = f(m) x + ϵ in which a random noise ϵ ∼N(0,σ2 noise) is added to the underlying true function f(m) x : X→ R. The training data set Dn = {(xi,y(mi) xi ,mi)}i∈[n] contains a set of triplets consisting of an input xi, ﬁdelity mi ∈[M], and an output y(mi) xi , where [n] := {1,...,n }. Throughout the paper, we assume that a set of outputs {f(m) x }for any set of pairs ( x,m) are always modeled as the multi-variate normal distribution. Standard multi-output extensions of GPR such as multi-task GPR (Bonilla et al., 2008), co-kriging (Kennedy & O’Hagan, 2000), and semiparametric latent factor model (SLFM) (Teh et al., 2005), satisfy this condition. We call GPR ﬁtted to observations across multiple ﬁdelities multi-ﬁdelity Gaussian process regression (MF-GPR), in general. MF-GPR deﬁnes a kernel function k((xi,mi),(xj,mj)) for a pair of training instances ( xi,y(mi) xi ,mi) and (xj,y(mj) xj ,mj). An example of this kernel function in the case of SLFM is shown in appendix A.1. By deﬁning a kernel matrix K∈Rn×n in which the i,j element is deﬁned by k((xi,mi),(xj,mj)), all the ﬁdelities f(1),...,f (M) are integrated into a GPR model in which predictive mean and variance are µ(m) x = k(m) n (x)⊤C−1y, and σ2(m) x = k((x,m),(x,m)) −k(m) n (x)⊤C−1k(m) n (x), where C := K+ σ2 noiseI with the identity matrix I, y:= (y(m1) x1 ,...,y (mn) xn )⊤, and k(m) n (x) := (k((x,m),(x1,m1)),...,k ((x,m),(xn,mn)))⊤. 3For later use, we deﬁne σ2(mm′) x as the predictive covariance between ( x,m) and (x,m′), i.e., covariance for the identical xat diﬀerent ﬁdelities: σ2(mm′) x = k((x,m),(x,m′)) −k(m) n (x)⊤C−1k(m′) n (x). 3 Multi-ﬁdelity Bayesian Optimization with Max-value Entropy We consider Bayesian optimization (BO) for maximizing the highest ﬁdelity function f(M) x when M diﬀerent ﬁdelities y(m) x for m = 1,...,M are available to querying. The querying cost is assumed to be known as λ(m), where λ(1) ≤λ(2) ... ≤λ(M). Our goal is to achieve a higher value with smaller accumulated cost of the queryings. We call this problem multi-ﬁdelity Bayesian optimization (MFBO). When M = 1, MFBO is reduced to the usual black box optimization to which we refer as the single ﬁdelity setting, while we refer to the setting M ≥2 as the multi-ﬁdelity setting. We employ the information-based approach, which has been widely used in the single ﬁdelity BO. In particular, our approach is inspired by max-value entropy search (MES) proposed by Wang & Jegelka (2017), which considers information gain about the optimal value maxx∈Xf(x) obtained by a querying. In the case of MFBO, we need to consider the information gain for identifying the maximum of the highest ﬁdelity function f∗:= maxx∈Xf(M) x by observing an arbitrary ﬁdelity observation. We refer to our information-based MFBO as multi-ﬁdelity MES (MF-MES). Although information-based approaches often result in complicated computations, we show that the calculation of our information gain is reduced to simple computations by which stable information evaluation becomes possible. 3.1 Information Gain for Sequential Querying We ﬁrst consider the case that a query is sequentially issued after the previous one is observed, which we refer to as sequential querying. Suppose that we already have a training data set Dt and need to determine next xt+1 and mt+1. We deﬁne an acquisition function a(x,m) := I(f∗; f(m) x |Dt) / λ(m), (1) where I(f∗; f(m) x |Dt) is the mutual information between f∗ and f(m) x conditioned on Dt. By maximizing a(x,m), we obtain a pair of the input xand the ﬁdelity mwhich maximally gains information of the optimal value f∗of the highest ﬁdelity per unit cost. The mutual information can be written as the diﬀerence of the entropy: I(f∗; f(m) x |Dt) = H(f(m) x |Dt) −Ef∗|Dt [ H(f(m) x |f∗,Dt) ] , (2) where H(·|· ) is the conditional entropy of p(·|· ). The ﬁrst term in the right hand side can be derived analytically for any ﬁdelity m: H(f(m) x |Dt) = log ( σ(m) x √ 2πe ) , where e:= exp(1). The second term in (2) takes the expectation over the maximum f∗. Since an analytical formula is not known for this expectation, 4we employ Monte Carlo estimation by sampling f∗from the current GPR: Ef∗|Dt [ H(f(m) x |f∗,Dt) ] ≈ ∑ f∗∈F∗ H(f(m) x |f∗,Dt) |F∗| , (3) where F∗ is a set of sampled f∗. Note that since this sampling approximation is in one dimensional space, accurate approximation can be expected with a small amount of samples. In Section 4, we discuss computational procedures of this sampling. For a given sampled f∗, the entropy of p(f(m) x |f∗,Dt) is needed to calculate in (3). To make the computation tractable, we replace this conditional distribution with p(f(m) x |f(M) x ≤f∗,Dt), i.e., conditioning only on the given x rather than requiring f(M) x ≤f∗ for ∀x ∈X . Note that this simpliﬁcation has been employed by most of entropy-based BO methods (e.g., Hern´ andez-Lobato et al., 2014; Wang & Jegelka, 2017) including MES, and superior performance compared with other approaches has been shown. For any ζ ∈ R, deﬁne γ(m) ζ (x) := ( ζ −µ(m) x )/σ(m) x as a function for scaling. When m = M, the density function p(f(m) x |f(M) x ≤f∗,Dt) is truncated normal distribution. The entropy of truncated normal distribution can be represented as (Michalowicz, 2014) H(f(M) x |f(M) x ≤f∗,Dt)=log (√ 2πeσ(M) x Φ ( γ(M) f∗ (x) )) − γ(M) f∗ (x)φ ( γ(M) f∗ (x) ) 2Φ ( γ(M) f∗ (x) ) , (4) where φ and Φ are the probability density function and the cumulative distribution function of the standard normal distribution. Next, we consider the case of m̸= M. Unlike the case of m= M, the density p(f(m) x |f(M) x ≤f∗,Dt) is not the truncated normal. Since MF-GPR represents all ﬁdelities as one uniﬁed GPR, the joint marginal distribution p(f(M) x ,f(m) x |Dt) can be immediately obtained from the two dimensional predictive distribution, from which we obtain p(f(M) x |f(m) x ,Dt) as f(M) x |f(m) x ,Dt ∼N(u(x),s2(x)), (5) where u(x) = σ2(mM) x ( f(m) x −µ(m) x ) /σ2(m) x + µ(M) x , and s2(x) = σ2(M) x − ( σ2(mM) x )2 /σ2(m) x . By using this conditional distribution, the entropy of p(f(m) x |f(M) x ≤f∗,Dt) can be written as follows: Lemma 3.1. Let Z := 1/σ(m) x Φ(γ(M) f∗ (x)) and Ψ(f(m) x ) := Φ ( (f∗−u(x))/s(x) ) φ ( γ(m) f(m) x (x) ) . Then, for a given f∗, we obtain H(f(m) x |f(M) x ≤f∗,Dt) = − ∫ ZΨ(f(m) x ) log ( ZΨ(f(m) x ) ) df(m) x . (6) See Appendix B for the proof. Lemma 3.1 indicates that the entropy is represented through the one dimensional integral over f(m) x . Since the integral is only on the one dimensional space, standard numerical integration techniques (e.g., quadrature) 5TimeWorkers Query 1 Query 6 Query 3 Query 5 Query 2 Query 4 Query 8 Query 9 Query 7 Figure 1: Asynchronous parallelization in MFBO. Because of diversity of the evaluation cost of objective functions, queries typically occur asynchronously. When a worker becomes available, a next query should be determined while taking queries being evaluated in the other workers into consideration. can provide precise approximation eﬃciently. Consequently, we see that that the entropy H(f(m) x |f∗,Dt) in (3) can be obtained accurately with simple computations. 3.2 Asynchronous Parallelization We consider an extension of MF-MES for the case that multiple queries can be issued in parallel, which we refer to as parallel querying. Suppose that we have q >1 “workers” each one of which can evaluate an objective function value. In the context of parallel BO, the two settings called synchronous and asynchronous parallelizations can be considered. As shown in Figure 1, since MFBO evaluates a variety of diﬀerent costs of objective functions, queries naturally occur asynchronously. Thus, we focus on asynchronous parallelization (See Appendix D.4 for the discussion of the synchronous setting). Suppose that q−1 pairs of the input xand the ﬁdelity m, written as Q:= {(x1,m1),..., (xq−1,mq−1)}, are now being evaluated by using q−1 workers, and an additional query to an available worker needs to be determined. Let fQ:= (f(m1) x1 ,...,f (mq−1) xq−1 )⊤. Then, a natural extension of MF-MES to determine the q-th pair (xq,mq) is apara(x,m) = I(f∗; f(m) x |Dt,fQ) / λ(m). (7) The numerator is the mutual information conditioned on fQwhich is deﬁned by I(f∗; f(m) x |Dt,fQ) := EfQ|Dt [ H(f(m) x |Dt,fQ) ] −EfQ,f∗|Dt [ H(f(m) x |Dt,fQ,f(M) x ≤f∗) ] . (8) Compared with the mutual information in sequential querying (2), this equation additionally takes the expectation over fQwhich is currently under evaluation. Thus, by using (7), we can select a cost eﬀective pair of xand m while the q−1 pairs running on the other workers are taken into consideration. Although (8) contains the |Q|+ 2 dimensional integral at a glance, we show that this can be calculated by at most 2 dimensional numerical integral. Let ΣM∈R2×2 and ΣQ∈Rq−1×q−1 be the predictive covariance 6matrices for M:= {(x,m),(x,M)}and Q, respectively, and ΣQ,M(= Σ⊤ M,Q) ∈Rq−1×2 be the predictive covariance matrix of the rows Qand the columns M. For later use, we deﬁne the conditional distribution p(f(m) x ,f(M) x |Dt,fQ) as follows  f(m) x f(M) x  |Dt,fQ∼N    µ(m) x|fQ µ(M) x|fQ  ,   σ2(m) x|fQ σ2(mM) x|fQ σ2(mM) x|fQ σ2(M) x|fQ    , where  µ(m) x|fQ µ(M) x|fQ  =  µ(m) x µ(M) x  + ΣM,QΣ−1 Q (fQ−µQ), (9)   σ2(m) x|fQ σ2(mM) x|fQ σ2(mM) x|fQ σ2(M) x|fQ  = ΣM−ΣM,QΣ−1 Q ΣQ,M, (10) and µQ:= (µ(m1) x1 ,...,µ (mq−1) xq−1 )⊤. Note that (9) is a random variable vector because it depends on fQ, while all the elements of (10) are constants. By using these equations, the mutual information (8) is re-written as follows: Lemma 3.2. Let ˜f∗:= f∗−µ(M) x|fQ , (11) and ˜f(m) x := f(m) x −µ(m) x|fQ . Then, we obtain I(f∗; f(m) x |Dt,fQ) = log ( σ(m) x|fQ √ 2πe ) −E˜f∗|Dt [∫ η( ˜f∗, ˜f(m) x ) logη( ˜f∗, ˜f(m) x ) d ˜f(m) x ] (12) where η( ˜f∗, ˜f(m) x ):= Φ ( ˜f∗− ( σ2(mM) x|fQ / σ2(m) x|fQ ) ˜f(m) x σ2(M) x|fQ − ( σ2(mM) x|fQ )2 / σ2(m) x|fQ ) φ ( ˜f(m) x σ(m) x|fQ ) σm x|fQ Φ ( ˜f∗ σ(M) x|fQ ) . (13) See Appendix D.1 for the proof. It should be noted that the second term of (12) only contains the integral over two variables ( ˜f(m) x and ˜f∗) unlike the original formulation (8). The ﬁrst term of (12) can be directly calculated because σ(m) x|fQ does not depend on the random vector fQ as shown in (10). We calculate the expectation in the second term of (12) by using the Monte Carlo estimation with sampled ˜f∗: ∑ ˜f∗∈˜F∗ 1 |˜F∗| ∫ η( ˜f∗, ˜f(m) x ) logη( ˜f∗, ˜f(m) x ) d ˜f(m) x (14) where ˜F∗is a set of sampled ˜f∗. The integral in this equation can be easily evaluated by using quadrature because it is on the one dimensional space and η( ˜f∗, ˜f(m) x ) can be analytically calculated from the deﬁnition (13). Further, when m= M, this integral is also can be analytically calculated (See Appendix D.2). 74 Computations Algorithm 1 shows the procedure of MF-MES for sequential querying. As the ﬁrst step in the every iteration, a set of max values F∗ are sampled from p(f∗ |Dt). There are several approaches to sampling the max value. Wang & Jegelka (2017) showed that the eﬀective approximation is possible by using sampling through Gumbel distribution or random feature map (RFM). Gumbel distribution is widely known in extreme value theory (Gumbel, 1958) as one of generalized extreme value distributions. Although the Gumbel approximation is performed under an independent approximation of GPR, Wang & Jegelka (2017) showed the accurate approximation can be obtained. In contrast, RFM (Rahimi & Recht, 2008) can incorporate dependency in the GPR model by using a set of pre-deﬁned basis functions φ(x,m) ∈RD, and the highest ﬁdelity function is represented as f(M) x ≈w⊤φ(x,M), where w∈RD (Appendix A.2 shows an example of an RFM approximation in the case of SLFM). The max value is sampled by maximizing w⊤φ(x,M) with respect to x. For further detail of these two approaches, see (Wang & Jegelka, 2017), in which it is also shown that MES is empirically robust with respect to this sampling, and theoretically, they showed that the regret bound can be guaranteed even only for one sample of f∗. Once F∗is generated, the acquisition function calculation can be analytically performed except for one dimensional numerical integration. Although most complicated process in the algorithm is the calculation of (6) shown in line 15 of Algoirthm 1, this is also quite simple in practice as described below. For a given f∗ and the conditional distribution (5) which is constructed from the two dimension GPR predictive distribution p(f(M) x ,f(m) x |x,Dt), the integral of (6) can be computed by O(1). Further, since (5) does not depend on sampled f∗, it is not required to re-calculate (5) for each one of sampled f∗. For the acquisition function maximization ( argmax in line 4), if the candidate space Xis a discrete set, we simply calculate the acquisition values for all x∈X. For a continuous space, popular approaches such as DIRECT (Jones et al., 1993) and gradient-based optimizers are applicable. Note that our acquisition function is diﬀerentiable, and the derivative of the integral (6) can be calculated by the same one dimensional numerical integral procedure. For the case of parallel querying, the acquisition function maximization is performed when a worker becomes available. To evaluate (14), we need to sample ˜f∗, which is determined through f∗and fQas shown in (11). This can be easily performed through RFM. By calculating w⊤φ(x,m) for ( x,m) ∈Q with the sampled parameter w, we can directly obtain a sample of fQ. For f∗, we maximize w⊤φ(x,M) as in the sequential querying case. The algorithm of Parallel MF-MES is shown in Appendix D.3. Throughout the paper, we use I(f∗; f(m) x ) as the information gain for brevity. I(f∗; y(m) x ), in which noisy observation y(m) x is contained, is also possible to use with the almost same procedure (for details, see Appendix C). Although we mainly focus on the case that we only have the discrete ﬁdelity level m ∈{1,...,M }as an “ordinal scale”, several studies consider the setting in which a ﬁdelity can be deﬁned as a point z in a continuous “ﬁdelity feature” (FF) space Z(Kandasamy et al., 2017). This setting is more restrictive because 8Algorithm 1 MF-MES for sequential querying 1: function MF-MES(D0,M, X,{λ(m)}M m=1) 2: for t= 0,...,T do 3: Generate F∗from current f(M)(x) 4: (xt+1,mt+1) ←argmaxx∈X,m InfoGain(x, m, F∗, Dt) / λ(m) 5: Dt+1 ←Dt ∪(xt+1,y(mt+1)(xt+1),mt+1) 6: end for 7: end function 8: function InfoGain(x, m, F∗, Dt) 9: Calculate µ(m) x and σ(m) x 10: Set H0 ←log ( σ(m) x √ 2πe ) 11: if m= M then 12: Set H1 ←∑ f∗∈F∗ H(f(M) x |f(M) x ≤f∗,Dt) |F∗| by using (4) 13: else 14: Calculate µ(M) x and σ(M) x and σ2(mM) x 15: Set H1 ←∑ f∗∈F∗ H(f(m) x |f(M) x ≤f∗,Dt) |F∗| by using (6) 16: end if 17: Return H0 −H1 18: end function it requires additional side-information z which speciﬁes a degree of ﬁdelity, though this prior knowledge may be able to improve the accuracy. By introducing a kernel function in ﬁdelity space Z, our method can easily adapt to this setting (See appendix E). 5 Related Work Multi-ﬁdelity extension of BO has been widely studied. For example, (Huang et al., 2006; Lam et al., 2015; Picheny et al., 2013) extended the standard EI to the multi-ﬁdelity setting. As with the usual EI, these are local measures of utility unlike the information-based approaches. Gaussian process upper conﬁdence bound (GP-UCB) (Srinivas et al., 2010) is a popular approach in the single ﬁdelity setting, and some studies proposed its multi-ﬁdelity extensions. Kandasamy et al. (2016) proposed multi-ﬁdelity GP-UCB for discrete ﬁdelity m= 1,...,M , and further, Kandasamy et al. (2017) proposed a similar UCB-based approach for the setting with the continuous ﬁdelity space Z. However, the UCB criterion has a trade-oﬀ parameter which balances exploit-exploration. In practice, this parameter needs to be carefully selected to achieve good 9performance. Another approach recently proposed in (Sen et al., 2018) is a multi-ﬁdelity extension of a hierarchical space partitioning (Bubeck et al., 2011). However, this method assumes that the approximation error can be represented as a known function form of cost, and further, they associate ﬁdelity with the depth of hierarchical tree, but the appropriateness of a speciﬁc choice of a pair of a point xand ﬁdelity mis diﬃcult to interpret. Information-based BO has also been studied for the multi-ﬁdelity setting, including entropy search (ES)-based (Swersky et al., 2013; Klein et al., 2017) and predictive entropy search (PES)-based (Zhang et al., 2017; McLeod et al., 2018) methods. Although these methods can measure global utility of the query without introducing any trade-oﬀ parameter, they inherit the computational diﬃculty of the original ES and PES, which consider the entropy of p(x∗), where x∗ := argmaxx f(x) is the optimal solution. PES mitigates computational diﬃculty by using 1) the symmetric property of the mutual information, and 2) several assumptions which simplify involved densities. However, integral with respect to x∗is still necessary though the dimension of x∗can be high, and the complicated approximation procedure including expectation propagation (Minka, 2001) is required. Further, an additional assumption about inter-ﬁdelity diﬀerences are required in the case of (Zhang et al., 2017). Song et al. (2018) proposed another information-based approach, which separates phases of the low-ﬁdelity exploration and the highest ﬁdelity optimization. However, the transition of these phases are controlled by a hyper-parameter which is necessary to set appropriately beforehand. Another approach incorporating a measure of global utility is knowledge gradient (KG)-based methods (Poloczek et al., 2017; Wu & Frazier, 2017). This approach evaluates the max gain of predictive mean maxx∈Xµ(M) x . In particular, misoKG (Poloczek et al., 2017) deals with the discrete ﬁdelity case. However, the acquisition function evaluation requires the expected value of the maximum of the mean function E[maxx′∈Xµ(M) x′ ] after adding y(m) x into training set, meaning that the maximization of the acquisition function is deﬁned as a nested optimization. Although a variety of computational techniques have been studied for KG, this nested optimization process is highly cumbersome to implement and computationally expensive. In contrast, our MF-MES is based on much simpler computations compared with existing information- based methods and other measures of global utility. Original MES calculates the entropy by representing a conditional distribution of fx given f∗as a truncated normal distribution. As we saw in Section 3.1, for the information gain from a lower ﬁdelity, the truncated normal approach is not applicable anymore because lower ﬁdelity functions f(m) x for m= 1,...,M −1 are not truncated for a given f∗. We already show that equations derived in Lemma 3.1 enables us to evaluate the entropy accurately with the only one dimensional additional numerical integration. For further acceleration of MES, Ru et al. (2018) proposed approximating the density of f∗ and f given f∗ by normal distributions, but reliability of these approximations are not clearly understood, and thus we do not employ in this paper. The parallel extension of BO has been widely studied (e.g., Snoek et al., 2012; Desautels et al., 2014). 10As we described in Section 3.2, MFBO is typically asynchronous, while many of existing studies focus on the synchronous setting including PES-based parallel BO (Shah & Ghahramani, 2015). Several papers focus on the asynchronous setting (Kandasamy et al., 2018), but these methods are diﬃcult to apply to the multi-ﬁdelity setting because they do not provide any criterion to select ﬁdelity. To our knowledge, an extension of KG (Wu & Frazier, 2017) is an only parallel method proposed for MFBO. However, this method is only for the synchronous setting, and further, it is only shown for the FF-based setting which is more restrictive as we described in the end of Section 4. We also note that a parallel extension of MES has not been shown even for the single-ﬁdelity setting. About a possible sequential/parallel settings of MF-MES, a summary is shown in Appendix F. 6 Experiments We evaluate eﬀectiveness of MF-MES compared with other existing methods. To evaluate performance, we em- ployed simple regret (SR) and inference regret (IR). SR is deﬁned by maxx∈Xf(M)(x) − maxi∈{i|i∈[t],mi=M}f(M)(xi), indicating the error by the best point queried so far. IR is deﬁned by maxx∈Xf(M)(x) −f(M)(ˆxt), where ˆxt := argmaxx∈Xµ(M) x which is seen as the recommendation from the model at iteration t. If IR is larger than SR at an iteration, we employed the value of SR as IR of that iteration for stable evaluation. For MF-GPR, we used SLFM in GP-based methods, unless otherwise noted. For the kernel function, we used Gaussian kernel with automatic relevance determination (ARD). We used a synthetic function generated by GPR, two benchmark functions, and a real-world dataset from materials science. For the GP-based synthetic function, we generated d= 3 dimensional synthetic functions through an SLFM model which has two ﬁdelity levels. The benchmark functions are called Styblinski-Tang, and HartMann6, which has M = 2, and 3 ﬁdelities, respectively. The sampling cost is set ( λ(1),λ(2)) = (1,5) when M = 2, and ( λ(1),λ(2),λ(3)) = (1 ,3,5) when M = 3. As an example of practical applications, we applied our method to the parameter optimization of a simulation model in materials science. The task is to optimize two material parameters of the model (Tsukada et al., 2014) by minimizing the discrepancy between the precipitate shape predicted by the model and one measured by an electron microscope. The relative cost of the objective function evaluation is determined by the accuracy of the computational model which is speciﬁed beforehand as ( λ(1),λ(2),λ(3)) = (5,10,60). Unlike benchmark functions, the candidate xis ﬁxed beforehand in this dataset (so-called the pooled setting). Each ﬁdelity has 62,500 candidate points. The experiments on the GP-based synthetic function were performed 100 times (10 diﬀerent initialization for each one of 10 generated functions). The other benchmark functions and the material dataset were performed 10 times with diﬀerent initialization. For further detail of the settings, see Appendix G.1. 11100 200 300 400 Total cost 10 1 100 Simple Regret Synthetic Function MF-MES MES MF-PES BOCA MFSKO 50 60 70 80 90 100 Total cost 10 1 100 101 Styblinski-Tang MF-MES MES MF-PES BOCA MFSKO 150 175 200 225 250 275 300 Total cost 10 1 100 HartMann6 MF-MES MES MF-PES BOCA MFSKO 600 800 1000 1200 1400 Total cost 10 2 10 1 100 101 Material MF-MES MES MF-PES BOCA MFSKO (a) Simple regret. 100 200 300 400 Total cost 10 1 100 Inferences Regret Synthetic Function MF-MES MES MF-PES BOCA MFSKO 50 60 70 80 90 100 Total cost 10 3 10 2 10 1 100 101 Styblinski-Tang MF-MES MES MF-PES BOCA MFSKO 150 175 200 225 250 275 300 Total cost 10 1 100 HartMann6 MF-MES MES MF-PES BOCA MFSKO 600 800 1000 1200 1400 Total cost 10 2 10 1 100 Material MF-MES MES MF-PES BOCA MFSKO (b) Inference regret. Figure 2: Performance comparison on sequential querying. 100 200 300 400 Total cost 10 1 100 Simple Regret Synthetic Function Parallel MF-MES MF-MES Parallel MES MES-LP GP-UCB-PE AsyTS 50 60 70 80 90 100 Total cost 10 5 10 4 10 3 10 2 10 1 100 101 Styblinski-Tang Parallel MF-MES MF-MES Parallel MES MES-LP GP-UCB-PE AsyTS 150 175 200 225 250 275 300 Total cost 10 1 100 HartMann6 Parallel MF-MES MF-MES Parallel MES MES-LP GP-UCB-PE AsyTS 600 800 1000 1200 1400 Total cost 10 2 10 1 100 101 Material Parallel MF-MES MF-MES Parallel MES MES-LP GP-UCB-PE AsyTS (a) Simple regret. 100 200 300 400 Total cost 10 2 10 1 100 Inferences Regret Synthetic Function Parallel MF-MES MF-MES Parallel MES MES-LP GP-UCB-PE AsyTS 50 60 70 80 90 100 Total cost 10 4 10 3 10 2 10 1 100 101 Styblinski-Tang Parallel MF-MES MF-MES Parallel MES MES-LP GP-UCB-PE AsyTS 150 175 200 225 250 275 300 Total cost 10 1 100 HartMann6 Parallel MF-MES MF-MES Parallel MES MES-LP GP-UCB-PE AsyTS 600 800 1000 1200 1400 Total cost 10 2 10 1 100 Material Parallel MF-MES MF-MES Parallel MES MES-LP GP-UCB-PE AsyTS (b) Inference regret. Figure 3: Performance comparison on parallel querying. 126.1 Evaluation for Sequential Querying We ﬁrst evaluate the performance for sequential querying. For comparison, we used MF-SKO (Huang et al., 2006), Bayesian optimization with continuous approximations (BOCA) (Kandasamy et al., 2017), and multi-ﬁdelity PES (MF-PES) (Zhang et al., 2017). We also evaluated single ﬁdelity MES which applied to the highest ﬁdelity function f(M)(x). As we see in Section 5, misoKG is another measure of global utility for MFBO. However, we could not employ it as a baseline because it was not straightforward to modify the author implementation for fair comparison (e.g., changing the MF-GPR model), and creating eﬃcient implementation from scratch is also extremely complicated (na¨ ıve implementation of KG can be prohibitively slow). Only BOCA employed the multi-task GPR (MT-GPR) model because the acquisition function assumes MT-GPR. For the sampling of f∗in MES and MF-MES, we employed the RFM-based approach described in Section 4, and sampled 10 f∗s at every iteration. In MF-PES, x∗was also sampled 10 times through RFM as suggested by (Hern´ andez-Lobato et al., 2014). Figure 2 shows SR and IR. In both of SR and IR, MF-MES decreased the regret faster than or comparable with all the other methods. The single-ﬁdelity MES is relatively slow because it cannot use lower-ﬁdelity functions, and we clearly see that MF-MES successfully accelerates MES. For SR of the GP-based synthetic, HartMann6 and material functions, MF-PES was slower than the others. We empirically observed that MF-PES sometime did not aggressively select the highest ﬁdelity samples enough. A possible reason is in an approximation employed by MF-PES which assumes f(m) x ≤f(m) x∗ +cfor m<M , where c is a constant (see Zhang et al., 2017, for the detailed deﬁnition). However, even when x∗is given, this strict inequality relation does not hold obviously (note that x∗is the maximizer only when m= M), and we conjecture that the information gain from lower ﬁdelity functions can be overly estimated because of this artiﬁcial truncation. In the material data, IR was slightly unstable which was caused by noisy observations contained in this real-world dataset. In particular, MF-PES largely ﬂuctuated, and this would also be due to the lack of the highest ﬁdelity samples as we mentioned above. We also evaluate computational time of the acquisition functions in Appendix G.2. 6.2 Evaluation for Parallel Querying Next, we evaluate performance on parallel querying. For comparison, we used MES combined with local penalization (Gonzalez et al., 2016), denoted as MES-LP, Gaussian process upper conﬁdence bound with pure exploration (GP-UCB-PE) (Gonzalez et al., 2016), asynchronous parallel Thompson sampling (AsyTS) (Kandasamy et al., 2018). Here, we would like to note that no existing methods have been proposed for discrete ﬁdelity parallel MFBO, to our knowledge, and extending existing methods to this setting is not straightforward because of discreteness of ﬁdelity levels. We also compare the performance of “sequential” MF-MES (which is same as “MF-MES” in Figure 2), and a parallel extension of single-ﬁdelity MES (shown in Appendix D.4) as baselines. For the sampling of ˜f∗in Parallel MF-MES and Parallel MES, the number of 13samples are set 10 through RFM. The number of workers is set q= 4. Figure 3 shows SR and IR. We see that parallel MF-MES substantially faster than sequential MF-MES and parallel MES. This indicates that parallel MF-MES succeeded in assigning workers across multiple ﬁdelities. Compared with other methods, parallel MF-MES shows rapid or comparable convergence. 7 Conclusion We propose a novel information-based multi-ﬁdelity Bayesian optimization (MFBO). The acquisition function is deﬁned through the information gain for the optimal value f∗of the highest ﬁdelity function. We show that our method called MF-MES (multi-ﬁdelity max-value entropy search) can be reduced to simple computations, which allows reliable evaluation of the entropy. For the asynchronous setting, which naturally arises in MFBO, we further propose parallelization of MF-MES and show that it is also easy to compute. We demonstrate eﬀectiveness of MF-MES by using benchmark functions and a real-world materials science data. 14Acknowledgements This work was supported by MEXT KAKENHI to I.T. (16H06538, 17H00758), M.K. (16H06538, 17H04694) and M.S (16H02866); from JST CREST awarded to I.T. (JPMJCR1302, JPMJCR1502) and PRESTO awarded to M.K. (JPMJPR15N2), M.S (JPMJPR16N6) and Y.T (JPMJPR15NB); from the MI2I project of the Support Program for Starting Up Innovation Hub from JST awarded to I.T., and M.K.; and from RIKEN Center for AIP awarded to M.S. and I.T. 15References Bhattacharjee, T., Mendis, C., Oh-ishi, K., Ohkubo, T., and Hono, K. The eﬀect of ag and ca additions on the age hardening response of mgzn alloys. Materials Science and Engineering: A , 575:231 – 240, 2013. Bonilla, E. V., Chai, K. M., and Williams, C. Multi-task gaussian process prediction. In Advances in Neural Information Processing Systems 20 , pp. 153–160. Curran Associates, Inc., 2008. Bubeck, S., Munos, R., Stoltz, G., and Szepesv´ ari, C. X-armed bandits. Journal of Machine Learning Research, 12:1655–1695, 2011. Desautels, T., Krause, A., and Burdick, J. W. Parallelizing exploration-exploitation tradeoﬀs in Gaussian process bandit optimization. Journal of Machine Learning Research , 15:4053–4103, 2014. G, M. B. and Wilhelm, S. Moments calculation for the doubly truncated multivariate normal density, 2012. Genton, M. G., Keyes, D. E., and Turkiyyah, G. Hierarchical decompositions for the computation of high-dimensional multivariate normal probabilities. Journal of Computational and Graphical Statistics , pp. 268–277, 2017. Genz, A. Numerical computation of multivariate normal probabilities. Journal of Computational and Graphical Statistics, 1:141–150, 1992. Gonzalez, J., Dai, Z., Hennig, P., and Lawrence, N. Batch bayesian optimization via local penalization. In Proceedings of the 19th International Conference on Artiﬁcial Intelligence and Statistics , volume 51, pp. 648–657. PMLR, 2016. Gumbel, E. J. Statistics of Extremes . Columbia University Press, 1958. Hennig, P. and Schuler, C. J. Entropy search for information-eﬃcient global optimization. Journal of Machine Learning Research, 13:1809–1837, 2012. Hern´ andez-Lobato, J. M., Hoﬀman, M. W., and Ghahramani, Z. Predictive entropy search for eﬃcient global optimization of black-box functions. In Advances in Neural Information Processing Systems 27 , pp. 918–926. Curran Associates, Inc., 2014. Huang, D., Allen, T., Notz, W., and Miler, R. Sequential kriging optimization using multiple-ﬁdelity evaluations. Structural and Multidisciplinary Optimization , 32(5):369–382, 2006. Jones, D. R., Perttunen, C. D., and Stuckman, B. E. Lipschitzian optimization without the lipschitz constant. Journal of Optimization Theory and Applications , 79(1):157–181, 1993. Kandasamy, K., Dasarathy, G., Oliva, J., Schneider, J., and P´ oczos, B. Gaussian process bandit optimisation with multi-ﬁdelity evaluations. In Advances in Neural Information Processing Systems 29 , pp. 1000–1008. Curran Associates, Inc., 2016. 16Kandasamy, K., Dasarathy, G., Schneider, J., and P´ oczos, B. Multi-ﬁdelity Bayesian optimisation with continuous approximations. In Proceedings of the 34th International Conference on Machine Learning , pp. 1799–1808, 2017. Kandasamy, K., Krishnamurthy, A., Schneider, J., and Poczos, B. Parallelised bayesian optimisation via Thompson sampling. In Proceedings of the 21st International Conference on Artiﬁcial Intelligence and Statistics, volume 84, pp. 133–142. PMLR, 2018. Kennedy, M. C. and O’Hagan, A. Predicting the output from a complex computer code when fast approxi- mations are available. Biometrika, 87(1):1–13, 2000. Klein, A., Falkner, S., Bartels, S., Hennig, P., and Hutter, F. Fast Bayesian optimization of machine learning hyperparameters on large datasets. In Proceedings of the 20th International Conference on Artiﬁcial Intelligence and Statistics , volume 54, pp. 528–536. PMLR, 2017. Lam, R., Allaire, D. L., and Willcox, K. E. Multiﬁdelity optimization using statistical surrogate modeling for non-hierarchical information sources. In Proceedings of the 56th AIAA/ASCE/AHS/ASC Structures, Struc- tural Dynamics, and Materials Conference , pp. 0143. American Institute of Aeronautics and Astronautics, 2015. McLeod, M., Osborne, M. A., and Roberts, S. J. Practical Bayesian optimization for variable cost objectives. arXiv:1703.04335, 2018. Michalowicz, J. Handbook of Diﬀerential Entropy. Chapman and Hall/CRC, New York, 2014. Minka, T. P. Expectation propagation for approximate Bayesian inference. In Proceedings of the 17th Conference in Uncertainty in Artiﬁcial Intelligence , pp. 362–369. Morgan Kaufmann Publishers Inc., 2001. Picheny, V., Ginsbourger, D., Richet, Y., and Caplin, G. Quantile-based optimization of noisy computer experiments with tunable precision. Technometrics, 55(1):2–13, 2013. Poloczek, M., Wang, J., and Frazier, P. I. Multi-information source optimization. In Advances in Neural Information Processing Systems 30 , pp. 4288–4298. Curran Associates, Inc., 2017. Rahimi, A. and Recht, B. Random features for large-scale kernel machines. In Advances in Neural Information Processing Systems 20, pp. 1177–1184. Curran Associates, Inc., 2008. Ramprasad, R., Batra, R., Pilania, G., Mannodi-Kanakkithodi, A., and Kim, C. Machine learning in materials informatics: recent applications and prospects. npj Computational Materials , 3(54), 2017. Ru, B., Osborne, M. A., Mcleod, M., and Granziol, D. Fast information-theoretic Bayesian optimisation. In Proceedings of the 35th International Conference on Machine Learning , volume 80, pp. 4384–4392. PMLR, 2018. 17Sen, R., Kandasamy, K., and Shakkottai, S. Multi-ﬁdelity black-box optimization with hierarchical partitions. In Proceedings of the 35th International Conference on Machine Learning , volume 80, pp. 4538–4547. PMLR, 2018. Shah, A. and Ghahramani, Z. Parallel predictive entropy search for batch global optimization of expensive objective functions. In Advances in Neural Information Processing Systems 28 , pp. 3330–3338. Curran Associates, Inc., 2015. Snoek, J., Larochelle, H., and Adams, R. P. Practical bayesian optimization of machine learning algorithms. In Advances in Neural Information Processing Systems 25 , pp. 2951–2959. Curran Associates, Inc., 2012. Song, J., Chen, Y., and Yue, Y. A general framework for multi-ﬁdelity Bayesian optimization with gaussian processes. arXiv:1811.00755, 2018. Srinivas, N., Krause, A., Kakade, S., and Seeger, M. Gaussian process optimization in the bandit setting: No regret and experimental design. In Proceedings of the 27th International Conference on International Conference on Machine Learning, pp. 1015–1022. Omnipress, 2010. Swersky, K., Snoek, J., and Adams, R. P. Multi-task Bayesian optimization. In Advances in Neural Information Processing Systems 26 , pp. 2004–2012. Curran Associates, Inc., 2013. Teh, Y. W., Seeger, M. W., and Jordan, M. I. Semiparametric latent factor models. In Proceedings of the 8th International Conference on Artiﬁcial Intelligence and Statistics , 2005. Tsukada, Y., Beniya, Y., and Koyama, T. Equilibrium shape of isolated precipitates in the α-mg phase. Journal of Alloys and Compounds , 603:65 – 74, 2014. Wang, Z. and Jegelka, S. Max-value entropy search for eﬃcient Bayesian optimization. In Proceedings of the 34th International Conference on Machine Learning , volume 70, pp. 3627–3635. PMLR, 2017. Wigley, P. B., Everitt, P. J., van den Hengel, A., Bastian, J. W., Sooriyabandara, M. A., McDonald, G. D., Hardman, K. S., Quinlivan, C. D., Manju, P., Kuhn, C. C. N., Petersen, I. R., Luiten, A. N., Hope, J. J., Robins, N. P., and Hush, M. R. Fast machine-learning online optimization of ultra-cold-atom experiments. Scientiﬁc Reports, 6:25890, 2016. Wu, J. and Frazier, P. Continuous-ﬁdelity Bayesian optimization with knowledge gradient. In NIPS Workshop on Bayesian Optimization , 2017. Zhang, Y., Hoang, T. N., Low, B. K. H., and Kankanhalli, M. Information-based multiﬁdelity Bayesian optimization. In NIPS Workshop on Bayesian Optimization , 2017. 18A Semiparametric Latent Factor Model and its RFM approxima- tion A.1 Model Deﬁnition Semiparametric Latent Factor Model (SLFM) is a Gaussian process based multiple response model (Teh et al., 2005). SLFM represents each output as a sum of C functions having diﬀerent kernel functions k1,...,k C, where kc : x×x→R is a kernel function. Let wmc ∈R be a weight that the m-th output (ﬁdelity) assigns to the c-th function. By introducing an independent term κcm >0, the kernel function is written as k((x,m),(x′,m′)) = C∑ c=1 (wcmwcm′+ κcmδm=m′)kc(x,x′), where δm=m′ = 1 if m = m′, and 0 otherwise. The parameters wcm and κcm which control dependence between multiple outputs are regarded as hyper-parameters, and standard approaches such as marginal likelihood optimization are often used to set them. A.2 RFM for SLFM Let fx := (f(1) x ,...,f (M) x )⊤be the M-dimensional output vector, and cov(fx,fx′) :=   k((x,1),(x′,1)) ··· k((x,1),(x′,M)) ... ... k((x,M),(x′,1)) ··· k((x,M),(x′,M))   be the M ×M covariance matrix of xand x′. By deﬁning wc := (wc1,...,w cM) and κc := (κc1,...,κ cM), this covariance is written as cov(fx,fx′) = C∑ c=1 (wcw⊤ c + diag(κc))kc(x,x′). Since kc(x,x′) is assumed to be one of stationary kernel functions (e.g., Gaussian kernel), RFM can produce a feature vector representation φc which approximates the kernel function as kc(x,x′) ≈φ⊤ c (x)φc(x). To transform wcw⊤ c + diag(κc) into a form of inner product, we use the Cholesky decomposition wcw⊤ c + diag(κc) = LcL⊤ c , where Lc ∈RM×M is a lower triangular matrix. Then, we obtain cov(fx,fx′) ≈ C∑ c=1 LcL⊤ c ( φ⊤ c (x)φc(x′) ) = C∑ c=1 Ψ⊤ c (x)Ψc(x′) where Ψc(x) := L⊤ c ⊗φc(x). Here, in the last line, we use the mixed-product property of Kronecker product. Then, the m-th column of Ψc(x) is deﬁned as the feature of xfor the m-th ﬁdelity φ(x,m). 19B Proof of Lemma 3.1 Using Bayes’ theorem, we obtain p(f(m) x |f(M) x ≤f∗,Dt) = p(f(M) x ≤f∗|f(m) x ,Dt)p(f(m) x |Dt) p(f(M) x ≤f∗|Dt) . (15) The densities p(f(m) x |Dt) and p(f(M) x ≤f∗|Dt) are directly obtained from the predictive distribution: p(f(m) x |Dt) = φ(γ(m) f(m) x (x))/σ(m) x , p(f(M) x ≤f∗|Dt) = Φ(γ(M) f∗ (x)). (16) In addition, from (5), p(f(M) x ≤f∗|f(m) x ,x,Dt) is written as the cumulative distribution of this Gaussian: p(f(M) x ≤f∗|f(m) x ,Dt) = Φ((f∗−u(x))/s(x)). (17) Substituting (16) and (17) into (15), the entropy is obtained. C Information Gain with Noisy Observation Here, we describe calculation of the mutual information between f∗ and noisy observation y(m) x , where y(m) x := y(m)(x) in this section. The mutual information can be written as the diﬀerence of the entropy: I(f∗; y(m) x |x,Dt) = H(y(m) x |x,Dt) −Ep(f∗|x,Dt) [ H(y(m) x |x,f∗,Dt) ] . (18) The ﬁrst term in the right hand side is H(y(m) x |x,Dt) = log (√ 2πe(σ2(m) x + σ2 noise) ) . (19) Using the sampling approximation of f∗, the second term in (18) is Ep(f∗|x,Dt) [ H(y(m) x |x,f∗,Dt) ] ≈ ∑ f∗∈F∗ 1 |F∗|H(y(m) x |x,f∗,Dt). (20) For any ζ ∈R, deﬁne γ(m) ζ (x) := (ζ−µ(m) x )/σ(m) x , and ρ(m) ζ (x) := (ζ−µ(m) x )/ √ σ2(m) x + σ2 noise. In this case, even for the highest ﬁdelity M, the density p(y(m) x |x,f(M) x ≤f∗,Dt) is not the truncated normal because of the noise term. Using Bayes’ theorem, we decompose this density as p(y(m) x |x,f(M) x ≤f∗,Dt) = p(f(M) x ≤f∗|y(m) x ,x,Dt)p(y(m) x |x,Dt) p(f(M) x ≤f∗|x,Dt) . (21) 20The densities p(y(m) x |x,Dt) and p(f(M) x ≤f∗|x,Dt) are directly obtained from the predictive distribution: p(y(m) x |x,Dt) = 1√ σ2(m) x + σ2 noise φ(ρ(m) y(m) x (x)), p(f(M) x ≤f∗|x,Dt) = Φ(γ(M) f∗ (x)). (22) The joint marginal distribution p(f(M) x ,y(m) x |x,Dt) is written as  y(m) x f(M) x  |x,Dt ∼N    µ(m) x µ(M) x  ,  σ2(m) x + σ2 noise σ2(mM) x σ2(mM) x σ2(M) x    , From this distribution, we obtain p(f(M) x |y(m) x ,x,Dt) as f(M) x |y(m) x ,x,Dt ∼N(unoise(x),s2 noise(x)), where unoise(x) = σ2(mM) x ( y(m) x −µ(m) x ) σ2(m) x + σ2 noise + µ(M) x , s2 noise(x) = σ2(M) x − ( σ2(mM) x )2 σ2(m) x + σ2 noise . Thus, p(f(M) x ≤f∗|y(m) x ,x,Dt) is written as the cumulative distribution of this Gaussian: p(f(M) x ≤f∗|y(m) x ,x,Dt) = Φ(γ′ f∗(x)), (23) where, γ′ f∗(x) := (f∗−unoise(x))/snoise(x). Using (15), (16), and (17) in the proof of Lemma 3.1, the entropy is obtained as H(y(m) x |x,f(M) x ≤f∗,Dt) = − ∫ ZΦ ( γ′ f∗(x) ) φ ( ρ(m) y(m) x (x) ) ·log ( ZΦ ( γ′ f∗(x) ) φ ( ρ(m) y(m) x (x) )) dy(m) x , (24) where Z := 1 / √ σ2(m) x + σ2 noiseΦ(γ(M) f∗ (x)). The integral in (24) can be calculated by using numerical integration in the same way as (6). Using I(f∗; y(m) x ) instead of I(f∗; f(m) x ) would be more natural when the observations are assumed to contain the observation noise with large variance σ2 noise, but in practice, diﬀerence of these two formulations would not largely eﬀect on performance of BO when σ2 noise is small. Note that the mutual information of parallel querying I(f∗; f(m) x |Dt,fQ) can be replaced with the noisy observation I(f∗; y(m) x |Dt,fQ) by using same procedure. 21D Additional Information for Parallel Querying D.1 Proof of Lemma 3.2 The ﬁrst term of (8) is EfQ|Dt [ H(f(m) x |Dt,fQ) ] = EfQ|Dt [ log ( σ(m) x|fQ √ 2πe )] = log ( σ(m) x|fQ √ 2πe ) . The last equation holds since σ(m) x|fQ does not depend on fQ. The second term of (8) is written as EfQ,f∗|Dt [ H(f(m) x |Dt,fQ,f(M) x ≤f∗) ] = − ∫ ∫ p(fQ,f∗|Dt) ∫ p(f(m) x |Dt,fQ,f(M) x ≤f∗) logp(f(m) x |Dt,fQ,f(M) x ≤f∗)df(m) x dfQdf∗. (25) For the conditional distribution f(M) x |Dt,fQ,f(m) x ∼N(up(x),s2 p(x)), the mean and the variance function can be written as up(x) = σ2(mM) x|fQ ( f(m) x −µ(m) x|fQ ) σ2(m) x|fQ + µ(M) x|fQ , s2 p(x) = σ2(M) x|fQ − ( σ2(mM) x|fQ )2 / σ2(m) x|fQ . Then, from Bayes’ theorem, we see p(f(m) x |Dt,fQ,f(M) x ≤f∗) = p(f(M) x ≤f∗|Dt,f(m) x ,fQ)p(f(m) x |Dt,fQ) p(f(M) x ≤f∗|Dt,fQ) = Φ ( f∗−up(x) sp(x) ) φ ( f(m) x −µ(m) x|fQ σ(m) x|fQ ) σ(m) x|fQ Φ ( f∗−µ(M) x|fQ σ(M) x|fQ ) . (26) By deﬁning A:= σ2(mM) x|fQ σ2(m) x|fQ , we can re-write ˜f∗−up(x) = ˜f∗−A˜f(m) x , and then, (26) is transformed into Φ (˜f∗−A˜f(m) x sp(x) ) φ ( ˜f(m) x σ(m) x|fQ ) σ(m) x|fQ Φ ( ˜f∗ σ(M) x|fQ ) =: η( ˜f∗, ˜f(m) x ). 22By further deﬁning h( ˜f∗, ˜f(m) x ) := η( ˜f∗, ˜f(m) x ) logη( ˜f∗, ˜f(m) x ), we simplify (25) as follows − ∫ ∫ p(fQ,f∗|Dt) ∫ h( ˜f∗, ˜f(m) x )df(m) x dfQdf∗. (27) This indicates that the most inner integrand can be shown as a function which only depends two random variables ˜f∗and ˜f(m) x . We change the variables of integration from ( f∗,f(m) x ,f⊤ Q)⊤to ( ˜f∗, ˜f(m) x ,f⊤ Q)⊤. J:=   ∂˜f∗ ∂f∗ ∂˜f∗ ∂f(m) x ∂˜f∗ ∂f⊤ Q ∂˜f(m) x ∂f∗ ∂˜f(m) x ∂f(m) x ∂˜f(m) x ∂f⊤ Q ∂fQ ∂f∗ ∂fQ ∂f(m) x ∂fQ ∂f⊤ Q   =  I2 ΣM,QΣ−1 Q 0 I|Q|   where I2 and I|Q|are the identity matrices with size 2 and |Q|, respectively. Note that determinant of J is |J|= 1. Thus, by changing variables of integration and variables of the densities, (27) can be transformed into − ∫ ∫ p(fQ,f∗|Dt) ∫ h( ˜f∗, ˜f(m) x )df(m) x dfQdf∗= − ∫ ∫ p(fQ, ˜f∗|Dt) ∫ h( ˜f∗, ˜f(m) x )d ˜f(m) x dfQd ˜f∗ = − ∫ p( ˜f∗|Dt) ∫ h( ˜f∗, ˜f(m) x )d ˜f(m) x d ˜f∗ = −E˜f∗|Dt [∫ h( ˜f∗, ˜f(m) x )d ˜f(m) x ] . (28) D.2 Analytical Calculation of Entropy for m = M When m= M, the most inner integral in (25) can be further simpliﬁed because it is equal to the entropy of the truncated normal p(f(M) x |Dt,fQ,f(M) x ≤f∗), which is written as − ∫ p(f(M) x |Dt,fQ,f(M) x ≤f∗) logp(f(M) x |Dt,fQ,f(M) x ≤f∗)df(M) x = log  √ 2πeσ(M) x|fQ Φ   ˜f∗ σ(M) x|fQ    − ˜f∗ σ(M) x|fQ φ ( ˜f∗ σ(M) x|fQ ) 2Φ ( ˜f∗ σ(M) x|fQ ) =: ω( ˜f∗), By using the same change of variables as (28), we obtain − ∫ ∫ p(fQ,f∗|Dt)ω( ˜f∗)dfQdf∗= −E˜f∗|Dt [ ω( ˜f∗) ] . D.3 Algorithm As shown in Algorithm 2, the acquisition function maximization is performed when a worker becomes available. The sampling of ˜f∗∈ ˜F∗is performed through an RFM approximation of MF-GPR: w⊤φ(x,m). For the 23Algorithm 2 Parallel MF-MES 1: function Parallel MF-MES(D0,M, X,{λ(m)}M m=1) 2: for t= 0,...,T do 3: Wait for a worker to be available 4: Generate ˜F∗from RFM 5: (xt+1,mt+1) ←argmaxx∈X,m InfoGain(x, m, ˜F∗, Dt) / λ(m) 6: Dt+1 ←Dt ∪(xt+1,y(mt+1)(xt+1),mt+1) 7: end for 8: end function 9: function InfoGain(x, m, F∗, Dt) 10: Calculate µ(m) x|fQ and σ(m) x|fQ 11: Set H0 ←log ( σx|fQ √ 2πe ) 12: if m̸= M then 13: Calculate µ(M) x|fQ ,σ(m) x|fQ , and σ2(mM) x|fQ 14: end if 15: Set H1 ←(14) 16: Return H0 −H1 17: end function entropy calculation in line 19, one dimensional numerical integration is necessary for the integral in (14) when m̸= M, while the analytical formula is available when m= M as shown in Appendix D.2. D.4 Synchronous Parallelization D.4.1 Single-ﬁdelity Setting In the main text, we focus on the asynchronous setting because of the diversity of sampling costs in MFBO. On the other hand, many parallel BO studies on the single-ﬁdelity setting consider the synchronous setting (Figure 4). To our knowledge, a parallel extension of MES has not been studies even in the single-ﬁdelity setting. Our approach is actually applicable to deﬁning the single ﬁdelity acquisition function. Although our main focus is in MFBO, we here show a counterpart of our multi-ﬁdelity acquisition function in the single ﬁdelity setting. Suppose that we need to select q points written as Q= {x1,..., xq}for the single ﬁdelity parallel BO. Unlike the asynchronous setting, q points is needed to be selected simultaneously. By setting fQ:= (fx1 ,...,f xq )⊤, a natural extension of MES for synchronous single-ﬁdelity setting is written as I(f∗; fQ|Dt) := H(fQ|Dt) −EfQ|Dt [H(fQ|fQ≤f∗,Dt)] . (29) 24TimeWorkers Query 1 Query 5 Query 3 Query 4 Query 2 Query 6 Figure 4: Synchronous setting in parallel BO. Note that we impose the condition fQ≤f∗, indicating that all the elements of fQis less than or equal to f∗, instead of fx ≤f∗in the usual MES. The ﬁrst term is the entropy of the q-dimensional Gaussian distribution which can be analytically calculated. The second term is the entropy of the multi-variate truncated normal distribution, for which we show analytical and approximate approaches to the computation. First, we consider the analytical approach. The density p(fQ|Dt) is the predictive distribution of GPR, and we deﬁne µQand ΣQas the mean and covariance matrix, respectively. The truncated normal in the second term is deﬁned through this density as follows p(fQ|fQ≤f∗,Dt) =    p(fQ|Dt)/Z, if fQ≤f∗, 0, otherwise, (30) where Z := ∫ fQ≤f∗ p(fQ|Dt)dfQ. We refer to the truncated normal (30) as TN(µTN Q ,ΣTN Q ), where µTN Q and ΣTN Q are the mean and covariance matrix, respectively. Let ETN be the expectation by the density (30). Then, the entropy in the second term of (29) is re-written as H[fQ|D,fQ≤f∗] = − ∫ fQ≤f∗ p(fQ|Dt) Z log p(fQ|Dt) Z dfQ = −ETN [ log p(fQ|Dt) Z ] = −ETN [ log p(fQ|Dt) −log Z ] = −ETN [ log p(fQ|Dt) ] + logZ = −ETN [ −1 2 log |2πΣQ|− 1 2(fQ−µQ)⊤Σ−1 Q (fQ−µQ) ] + logZ = 1 2 log |2πΣQ|+ 1 2 ETN [ (fQ−µQ)⊤Σ−1 Q (fQ−µQ) ]    =:B + logZ. 25By deﬁning d= µTN Q −µQ, we see B = ETN [ Tr ( Σ−1 Q (fQ−µQ)(fQ−µQ)⊤)] = Tr ( Σ−1 Q ETN [ (fQ−µQ)(fQ−µQ)⊤]) = Tr ( Σ−1 Q ETN [ (fQ−µTN Q + d)(fQ−µTN Q + d)⊤]) = Tr ( Σ−1 Q ETN [ (fQ−µTN Q )(fQ−µTN Q )⊤+ d(fQ−µTN Q )⊤+ (fQ−µTN Q )d⊤+ dd⊤]) . Since ETN[(fQ−µQ)] = 0, we further obtain B = Tr ( Σ−1 Q ETN [ (fQ−µTN Q )(fQ−µTN Q )⊤+ dd⊤]) = Tr ( Σ−1 Q (ΣTN Q + dd⊤) ) Therefore, we obtain H[fQ |D,fQ ≤f∗] = 1 2 ( log |2πΣQ|+ Tr ( Σ−1 Q (ΣTN Q + dd⊤) )) + logZ. If Z, µTN Q , and ΣTN Q are available, the above equation is easily calculated. The normalization term Z is the q-dimensional Gaussian CDF, for which a lot of fast computation algorithms have been proposed (e.g., Genz, 1992; Genton et al., 2017). A method proposed by (Genz, 1992) has been widely used, which requires O(q2) computations. For µTN Q , and ΣTN Q , G & Wilhelm (2012) shows analytical formulas which also depend on the multivariate Gaussian CDF. This needs q times computations of the q−1 dimensional CDF, and q(q−1) times computations of the q−2 dimensional CDF. To avoid many computations of q−1 dimensional CDF, we can introduce approximation of the entropy calculation or greedy selection of Q. As a fast approximation, expectation propagation (EP) can be used to replace the truncated normal distribution with a Gaussian distribution, which makes the entropy calculation analytical. The similar technique is also used in (Hern´ andez-Lobato et al., 2014). For the greedy strategy, we can choose a next point to add Qby maximizing I(f∗; fx |Dt,f˜Q), where ˜Qis a set of ( x,m) already determined to be included in Q. This information can be evaluated by the same way as we saw in the asynchronous setting (8) because the equation has the same form of conditional mutual information. D.4.2 Multi-ﬁdelity Setting Combining the synchronous setting with multi-ﬁdelity functions m= 1,...,M results in a combinatorial selection of Q= {(x1,m1),..., (xq,mq)}because of the discreteness of the ﬁdelity level m. When a simple greedy strategy is employed to select Q, the procedure is reduced to the almost the same procedure as the synchronous single ﬁdelity case described above. This indicates that we can avoid the q dimensional integral by using the technique shown in Section 3.2. 26E Incorporating Fidelity Feature Our proposed method is applicable to the case that the ﬁdelity is deﬁned as a point of a ﬁdelity feature (FF) space Zinstead of the discrete ﬁdelity level 1 ,...,M (Kandasamy et al., 2017). Let f(z) x be the predictive distribution for the ﬁdelity z∈Z. The goal is to solve maxx∈Xf(z∗) x , where z∗∈Z is the highest ﬁdelity to be optimized. For example, in the neural network hyper-parameter optimization, Zcan be a two dimensional space deﬁned by the number of training data and the number of training iterations. In this case, our acquisition function (1) is extended to a(x,z) := I(f∗; f(z) x ) / λ(z), (31) where f∗:= maxx∈Xf(z∗) x in this case, and λ(z) is known cost for z∈Z. As with (Kandasamy et al., 2017), we represent the output f(z) x as a Gaussian process on the direct product space X×Z . Suppose that the observed training data set is written as Dn = {(xi,y(zi)(xi),zi)}n i=1, where y(zi)(xi) is an observation of xi at the ﬁdelity zi. A standard approach to deﬁning a kernel on the joint space X×Z is to use the product form k((xi,zi),(xj,zj)) = kx(xi,xj) kz(zi,zj), where kx : X×X→ R is a kernel for the input space X, and kz : Z×Z→ R is a kernel for the ﬁdelity space Z. Based on this kernel, predictive distribution of GPR can be deﬁned for any pair of ( x,z), and thus the numerator of (31) can be calculated by using the same approach as I(f∗; f(m) x ) which we describe in Section 3.1. Parallelization can also be considered in this FF-based case. For the asynchronous setting, the acquisition function is apara(x,z) = I(f∗; f(z) x |Dt,fQ)/λ(z), in which information gain is conditioned on the set of points currently under evaluation Q= {(x1,m1),..., (xq−1,mq−1)}. As in the sequential case above, the calculation of this acquisition function is almost same as the discrete case in Section 3.2. For the synchronous case, the same discussion as Appendix D.4 also holds. F Summary of Settings in Sequential/Parallel MFBO A possible combination of the single/multiple ﬁdelity and sequential/parallel querying are summarized in Table 1. Our main focus is in FF-free MFBO, and FF-free parallel MFBO with asynchronous querying. In particular, for parallel MFBO, except for the FF-based synchronous querying, no prior works exist to our knowledge. 27Table 1: Summary of possible settings. “FF-based” indicates the setting that the ﬁdelity feature z is available, while “FF-free” does not assume it. Synchronous querying is denoted as ’sync’, and asynchronous querying is denoted as ’asyn’. Fidelity (S)equential/ Our description Note (P)arallel Parallel BO Single P (sync) Appendix D.4.1 - Single P (asyn) Special case of Parallel MF-MES - MFBO Multiple (FF-based) S Appendix E - Multiple (FF-free) S MF-MES described in Section 3.1 - Parallel MFBO Multiple (FF-based) P (sync) Appendix E (Wu & Frazier, 2017) Multiple (FF-based) P (asyn) Appendix E No prior work Multiple (FF-free) P (sync) Appendix D.4.2 No prior work Multiple (FF-free) P (asyn) Parallel MF-MES described in Section 3.2 No prior work G Additional Information of Empirical Evaluation G.1 Other Experimental Settings G.1.1 Settings of Methods We trained the GPR model using normalized training observations (mean 0, and standard deviation 1), other than the GP-based synthetic function. Model hyper-parameters were optimized by marginal-likelihood at every 5 iterations. For the GP-based synthetic function, we set the GPR hyper-parameters as parameters used for sampling the function. For the initial observations, we employed the Latin hypercube approach shown by (Huang et al., 2006). The number of initial training points x∈X⊂ Rd were set as follows: • 5d and 4d for m= 1 and 2, respectively, if M = 2 • 6d, 3d and 2d for m= 1,2 and 3, respectively, if M = 3 • 10d, 7d and 3d for m= 1,2 and 3, respectively, in the material dataset We used the Gaussian kernel k(x,x′) = exp(−∑d i=1(xi −x′ i)2/(2ℓ2 i)) for all kernels. The length scale parameter ℓd was optimized through marginal-likelihood in the following interval: • ℓd ∈[Domain size/10,Domain size ×10] for the GP-based synthetic function and the benchmark functions, here Domain size is the diﬀerence between the maximum and the minimum of the input domain in each dimension. The input domain of each function is shown in Appendix G.1.2. • ℓd ∈[10−3,10−1] for the material dateset • The task kernel in BOCA: ℓd ∈[2,(M −1) ×10] for benchmark functions, and ℓd ∈[10,103] for the material dataset 28The noise parameter of GPR was ﬁxed as σ2 noise = 10−6. The number of kernels in SLFM was C = 2. The hyper-parameters in covariance among diﬀerent output dimension were also optimized through marginal- likelihood in the following interval: • wc1 ∈[ √ 0.75,1] for c= 1,2 • wc2 ∈[− √ 0.25, √ 0.25] for c= 1,2 • κcm ∈[10−3,10−1] for c= 1,2 and m= 1,...,M The number of basis D in RFM was 1000, which was used by MF-MES, MF-PES, MES-LP, and AsyTS. The number of samplings for f∗in MES and PES was 10. For all compared methods, including BOCA, MFSKO, local penalization in MES-LP, GP-UCB-PE, and AsyTS, we followed the settings of hyper-parameters in their original papers. G.1.2 Details of Benchmark Datasets GP-based Synthetic functions We used RFM for SLFM described in Appendix A.2. The input dimension is d = 3 and the domain is xi ∈[0,1]. The parameters are C = 1,w = (0.9,0.9)⊤,κ = (0.1,0.1)⊤, and ℓi = 0.1 for i= 1,2,3. Styblinski-Tang function f(1) = 1 2 2∑ i=1 (0.9x4 i −15x2 i + 6xi), f(2) = 1 2 2∑ i=1 (x4 i −16x2 i + 5xi), xi ∈[−5,5],i = 1,2 29HartMann6 function f(1) = − 4∑ i=1 (αi −0.2) exp ( − 6∑ j=1 Aij(xj −Pij)2 ) , f(2) = − 4∑ i=1 (αi −0.1) exp ( − 6∑ j=1 Aij(xj −Pij)2 ) f(3) = − 4∑ i=1 αiexp ( − 6∑ j=1 Aij(xj −Pij)2 ) α= [1.0,1.2,3.0,3.2]⊤ A=   10 3 17 3 .5 1 .7 8 0.05 10 17 0 .1 8 14 3 3 .5 1 .7 10 17 8 17 8 0 .05 10 0 .1 14   P = 10−4   1312 1696 5569 124 8283 5886 2329 4135 8307 3736 1004 9991 2348 1451 3522 2883 3047 6650 4047 8828 8732 5743 1091 381   xj ∈[0,1],j = 1,..., 6 Materials Data As an example of practical application, we applied our method to the parameter opti- mization of computational simulation model in materials science. There is a computational model (Tsukada et al., 2014) that predicts equilibrium shape of precipitates in the α-Mg phase when material parameters are given. We estimate two material parameters (lattice mismatch and interface energy between the α-Mg and precipitate phases) from experimental data on precipitate shape measured by transmission electron microscopy (TEM) (Bhattacharjee et al., 2013). The objective function is the discrepancy between precipitate shape predicted by the computational model and one measured by TEM. G.2 Measuring Computational Time of Acquisition Functions We measured the computational time for the maximization of the acquisition functions. We assume that the predictive distribution of the GPR model is already obtained, because it is almost common for all the methods. The training dataset is created by the initialization process in our experiment described in Appendix G.1. Figure 5 shows the results on three benchmark dataset, used in the main text. BOCA and MFSKO are relatively easy to compute because they are based on UCB and EI, respectively. Their acquisition function is simple, but diﬃcult to incorporate global utility of the candidate without tuning parameters as we discuss in the main text. MF-MES was much faster than MF-PES. We emphasize that MF-PES employs the approximation based on EP to accelerate the computation, unlike our MF-MES which is almost 30MF-MES MF-PES BOCA MFSKO 0 25 50 75 100 125 150Time (sec) (a) Styblinski-Tang MF-MES MF-PES BOCA MFSKO 0 100 200 300 400 500 600Time (sec) (b) HartMann6 Figure 5: Computational time for acquisition function maximization. analytical. This indicates that MF-MES provides more reliable entropy computation with smaller amount of computations than MF-PES. 31",
      "meta_data": {
        "arxiv_id": "1901.08275v2",
        "authors": [
          "Shion Takeno",
          "Hitoshi Fukuoka",
          "Yuhki Tsukada",
          "Toshiyuki Koyama",
          "Motoki Shiga",
          "Ichiro Takeuchi",
          "Masayuki Karasuyama"
        ],
        "published_date": "2019-01-24T08:20:21Z",
        "pdf_url": "https://arxiv.org/pdf/1901.08275v2.pdf"
      }
    },
    {
      "title": "Parallel Bayesian Optimization of Multiple Noisy Objectives with Expected Hypervolume Improvement",
      "abstract": "Optimizing multiple competing black-box objectives is a challenging problem\nin many fields, including science, engineering, and machine learning.\nMulti-objective Bayesian optimization (MOBO) is a sample-efficient approach for\nidentifying the optimal trade-offs between the objectives. However, many\nexisting methods perform poorly when the observations are corrupted by noise.\nWe propose a novel acquisition function, NEHVI, that overcomes this important\npractical limitation by applying a Bayesian treatment to the popular expected\nhypervolume improvement (EHVI) criterion and integrating over this uncertainty\nin the Pareto frontier. We argue that, even in the noiseless setting,\ngenerating multiple candidates in parallel is an incarnation of EHVI with\nuncertainty in the Pareto frontier and therefore can be addressed using the\nsame underlying technique. Through this lens, we derive a natural parallel\nvariant, $q$NEHVI, that reduces computational complexity of parallel EHVI from\nexponential to polynomial with respect to the batch size. $q$NEHVI is one-step\nBayes-optimal for hypervolume maximization in both noisy and noiseless\nenvironments, and we show that it can be optimized effectively with\ngradient-based methods via sample average approximation. Empirically, we\ndemonstrate not only that $q$NEHVI is substantially more robust to observation\nnoise than existing MOBO approaches, but also that it achieves state-of-the-art\noptimization performance and competitive wall-times in large-batch\nenvironments.",
      "full_text": "Parallel Bayesian Optimization of Multiple Noisy Objectives with Expected Hypervolume Improvement Samuel Daulton Facebook, University of Oxford sdaulton@fb.com Maximilian Balandat Facebook balandat@fb.com Eytan Bakshy Facebook ebakshy@fb.com Abstract Optimizing multiple competing black-box objectives is a challenging problem in many ﬁelds, including science, engineering, and machine learning. Multi-objective Bayesian optimization (MOBO) is a sample-efﬁcient approach for identifying the optimal trade-offs between the objectives. However, many existing methods perform poorly when the observations are corrupted by noise. We propose a novel acquisition function, NEHVI , that overcomes this important practical limitation by applying a Bayesian treatment to the popular expected hypervolume improvement (EHVI ) criterion and integrating over this uncertainty in the Pareto frontier. We argue that, even in the noiseless setting, generating multiple candidates in parallel is an incarnation of EHVI with uncertainty in the Pareto frontier and therefore can be addressed using the same underlying technique. Through this lens, we derive a natural parallel variant, qNEHVI , that reduces computational complexity of parallel EHVI from exponential to polynomial with respect to the batch size. qNEHVI is one-step Bayes-optimal for hypervolume maximization in both noisy and noiseless environments, and we show that it can be optimized effectively with gradient-based methods via sample average approximation. Empirically, we demonstrate not only that qNEHVI is substantially more robust to observation noise than existing MOBO approaches, but also that it achieves state-of-the-art optimization performance and competitive wall-times in large-batch environments. 1 Introduction Black-box optimization problems that involve multiple competing noisy objectives are ubiquitous in science and engineering. For example, a real-time communications service may be interested in tuning the parameters of a control policy to adapt video quality in real time in order to maximize video quality and minimize latency [ 10, 17]. In robotics, scientists may seek to design hardware components that maximize locomotive speed and minimize energy expended [8, 38]. In agriculture, development agencies may seek to balance crop yield and environmental impact [ 28]. For such multi-objective optimization (MOO) problems, there typically is no single solution that is best with respect to all objectives. Rather, the goal is to identify the Pareto frontier: a set of optimal trade-offs such that improving one objective means deteriorating another. In many cases, the objectives are expensive to evaluate. For instance, randomized trials used in agriculture and the internet industry may take weeks or months to conduct and incur opportunity costs, and manufacturing and testing hardware is both costly and time-consuming. Therefore, it is imperative to be able to identify good trade-offs with as few objective evaluations as possible. Bayesian optimization (BO), a method for efﬁcient global black-box optimization, is often used to tackle such problems. BO employs a probabilistic surrogate model in conjunction with an acquisition function to navigate the trade-off between exploration (evaluating designs with high uncertainty) and exploitation (evaluating designs that are believed to be optimal). Although a signiﬁcant number of works have explored multi-objective Bayesian optimization (MOBO), most available methods 35th Conference on Neural Information Processing Systems (NeurIPS 2021) arXiv:2105.08195v2  [cs.LG]  26 Oct 2021[3, 39, 51, 60] do not take into account the fact that, in practice, observations are often subject to noise. For example, results of an A/B test are highly variable due to heterogeneity in the underlying user population and other factors. Agricultural trials are affected by the stochastic nature of plant growth and environmental factors such as soil composition or wind currents. In robotics, devices are subject to manufacturing tolerances, and observations of quantities such as locomotive speed and efﬁciency may be corrupted by measurement error from noisy sensors and environmental factors such as temperature or surface friction. While previous work has shown that a principled treatment of noisy observations can signiﬁcantly improve optimization performance in the single-objective case [24, 37], this issue is understudied in the multi-objective setting. Furthermore, many applications in which evaluations take a long time require evaluating large batches of candidates in parallel in order to achieve reasonable throughput. For example, when ﬁrms optimize systems via A/B tests, it may take several weeks to test any particular conﬁguration. Because of this, large batches of candidate policies are tested simultaneously [36]. In biochemistry and materials design, dozens of tests can be conducted parallel on a single microplate [63]. Even in sophisticated high throughput chemistry settings, these batches may take several hours or days to set up and evaluate [ 42]. Most existing MOBO methods, however, are either designed for purely sequential optimization [3, 51] or do not scale well to large batch sizes [11]. Contributions: In this work, we propose a novel MOBO algorithm, based on expected hypervolume improvement (EHVI), that scales to highly parallel evaluations of noisy objectives. Our approach is made possible by a general-purpose, differentiable, cached box decomposition (CBD) implementation that dramatically speeds up critical computations needed to account for uncertainty introduced by noisy observations and generate new candidate points for highly parallel batch or asynchronous evaluation. In particular, our CBD-based approach solves the fundamental problem of scaling parallel EHVI-based methods to large batch sizes, reducing time and space complexity from exponential to polynomial. Our proposed algorithm, noisy expected hypervolume improvement (NEHVI ), is the one- step Bayes-optimal policy for hypervolume improvement and provides state-of-the-art performance across a variety of benchmarks. To our knowledge, our work provides the most extensive evaluation of noisy parallel MOBO to date. A high-quality implementation of qNEHVI , as well as many of the baselines considered here, will be made available as open-source software upon publication. 2 Preliminaries Our goal is to ﬁnd the set of optimal designs xover a bounded set X⊂ Rd that maximize one or more objectives f(x) ∈RM, with no known analytical expression nor gradient information of f. Multi-Objective Optimization (MOO) aims to identify the set of Pareto optimal objective trade- offs. We say a solution f(x) = [ f(1)(x),...,f (M)(x) ] dominates another solution f(x) ≻f(x′) if f(m)(x) ≥f(m)(x′) for m= 1,...,M and ∃m∈{1,...,M }s.t. f(m)(x) >f (m)(x′). We deﬁne the Pareto frontier as P∗= {f(x) : x∈X, ∄ x′∈X s.t.f(x′) ≻f(x)}, and denote the set of Pareto optimal designs as X∗= {x: f(x) ∈P∗}. Since the Pareto frontier (PF) is often an inﬁnite set of points, MOO algorithms usually aim to identify a ﬁnite approximate PF P. A natural measure of the quality of a PF is the hypervolume of the region of objective space that is dominated by the PF and bounded from below by a reference point. Provided with the approximate PF, the decision-maker can select a particular Pareto optimal trade-off according to their preferences. Bayesian Optimization (BO) is a sample-efﬁcient optimization method that leverages a probabilistic surrogate model to make principled decisions to balance exploration and exploitation [ 19, 50]. Typically, the surrogate is a Gaussian Process (GP), a ﬂexible, non-parametric model known for its well-calibrated predictive uncertainty [47]. To decide which points to evaluate next, BO employs an acquisition function α(·) that speciﬁes the value of evaluating a set of new points xbased on the surrogate’s predictive distribution at . While evaluating the true black-box function f is time- consuming or costly, evaluating the surrogate is cheap and relatively fast; therefore, numerical optimization can be used to ﬁnd the maximizer of the acquisition function x∗= arg maxx∈Xα(x) to evaluate next on the black-box function. BO sequentially selects new points to evaluate and updates the model to incorporate the new observations. Evolutionary algorithms (EAs) such as NSGA-II [12] are a popular choice for solving MOO problems (see Zitzler et al. [67] for a review of various other approaches). However, EAs generally suffer from high sample complexity, rendering them infeasible for optimizing expensive-to-evaluate black-box 2functions. Multi-objective Bayesian optimization (MOBO), which combines a Bayesian surrogate with an acquisition function designed for MOO, provides a much more sample-efﬁcient alternative. 3 Related Work Methods based on hypervolume improvement (HVI) seek to expand the volume of the objective space dominated by the Pareto frontier. Expected hypervolume improvement ( EHVI ) [16] is a natural extension of the popular expected improvement (EI) [29] acquisition function to the MOO setting. Recent work has led to efﬁcient computational paradigms using box decomposition algorithms [59] and practical enhancements such as support for parallel candidate generation and gradient-based acquisition optimization [11, 58]. However, EHVI still suffers from some limitations, including (i) the assumption that observations are noise-free, and (ii) the exponential scaling of its batch variant, qEHVI , in the batch size q, which precludes large-batch optimization. DGEMO [ 39] is a recent method for parallel MOBO that greedily maximizes HVI while balancing the diversity of the design points being sampled. Although DGEMO scales well to large batch sizes, it does not account for noisy observations. TSEMO [5] is a Thompson sampling (TS) heuristic that can acquire batches of points by optimizing a random fourier feature (RFF) [ 46] approximation of a GP surrogate using NSGA-II and selecting a subset of points from the EA’s population to sequentially greedily maximize HVI. This heuristic approach for maximizing HVI currently has no theoretical guarantees and relies on zeroth-order optimization methods, which tend to be slower and exhibit worse optimization performance than gradient-based approaches. Entropy-based methods such as PESMO [ 25], MESMO [3], and PFES [ 51] are an alternative to EHVI . Of these three methods, PESMO is the only one that accounts for observation noise. However, PESMO involves intractable entropy computations and therefore relies on complex approximations, as well as challenging and time-consuming numerical optimization procedures [25]. Garrido-Merchán & Hernández-Lobato [21] recently proposed an extension to PESMO that supports parallel candidate generation. However, the authors of this work provide limited evaluation and have not provided code to reproduce their results.1 MOO can also be cast into a single-objective problem by applying a random scalarization of the objectives. ParEGO maximizes the expected improvement using random augmented Chebyshev scalarizations [32]. MOEA/D-EGO [64] extends ParEGO to the batch setting using multiple random scalarizations and the genetic algorithm MOEA/D [65] to optimize these scalarizations in parallel. Recently, qParEGO, another batch variant of ParEGO was proposed that uses compositional Monte Carlo objectives and sequential greedy candidate selection [11]. Additionally, the authors proposed a noisy variant, qNParEGO, but the empirical evaluation of that variant was limited. TS-TCH [45] combines random Chebyshev scalarizations with Thompson sampling [54], which is naturally robust to noise when the objective is scalarized. Golovin & Zhang [23] propose to use a hypervolume scalarization with the property that the expected value of the scalarization over a speciﬁc distribution of weights is equivalent to the hypervolume indicator. The authors propose a upper conﬁdence bound algorithm using randomly sampled weights, but provide a very limited empirical evaluation. Many prior attempts by the simulation community to handle MOO with noisy observations found that accounting for the noise did not improve optimization performance: Horn et al. [26] suggest that the best approach is to ignore noise, and Koch et al. [33] concluded that further research was needed to determine if modeling techniques such as re-interpolation could improve BO performance with noisy observations. In contrast, we ﬁnd that accounting for noise does substantially improve performance in noisy settings. Lastly, previous works have considered methods for quantifying and monitoring uncertainty in the Pareto frontiers during the optimization [ 4, 7]. In contrast, we provide a solution to performing MOBO in noisy settings, rather than purely reasoning about the uncertainty in the Pareto frontier. 4 Background on Expected Hypervolume Improvement In this section, we review hypervolume, hypervolume improvement, and expected hypervolume improvement as well as efﬁcient methods for computing these metrics using box decompositions. 1We contacted the authors twice asking for code to reproduce their results, but they graciously declined. 3Deﬁnition 1. The hypervolume indicator ( HV) of a ﬁnite approximate Pareto frontier Pis the M-dimensional Lebesgue measure λM of the space dominated by Pand bounded from below by a reference point. r∈RM: HV(P|r) = λM (⋃ v∈P[r,v] ) , where [r,v] denotes the hyper-rectangle bounded by vertices rand v. As in previous work, we assume that the reference point ris known and speciﬁed by the decision maker [58]. Deﬁnition 2. The hypervolume improvement (HVI) of a set of pointsP′w.r.t. an existing approximate Pareto frontierPand reference point ris deﬁned as2 HVI (P′|P,r) = HV(P∪P ′|r) −HV(P|r). Computing HV requires calculating the volume of a typically non-rectangular polytope and is known to have time complexity that is super-polynomial in the number of objectives [ 59]. An efﬁcient approach for computing HV is to (i) decompose the region that is dominated by the Pareto frontier Pand bounded from below by the reference point r into disjoint axis-aligned hyperrectangles [34], (ii) compute the volume of each hyperrectangle in the decomposition, and (iii) sum over all hyperrectangles. So-called box decomposition algorithms have also been applied to partition the region that is not dominated by the Pareto frontier P, which can be used to compute the HVI from a set of new points [15, 59]. See Appendix B for further details. Expected Hypervolume Improvement: Since function values at unobserved points are unknown in black-box optimization, so is the HVI of an out-of-sample point. However, in BO the probabilistic surrogate model provides a posterior distribution p(f(x)|D) over the function values for each x, which can be used to compute the expected hypervolume improvement (EHVI ) acquisition function: αEHVI (x|P) = E [ HVI(f(x)|P) ] . Although αEHVI can be expressed analytically when (i) the objectives are assumed to be conditionally independent given xand (ii) the candidates are generated and evaluated sequentially [58], Monte Carlo (MC) integration is commonly used since it does not require either assumption [16]. The more general parallel variant using MC integration is given by αqEHVI (Xcand|P) ≈ˆαqEHVI (Xcand|P) = 1 N N∑ t=1 HVI ( ˜ft(Xcand)|P), (1) where ˜ft ∼p(f|D) for t = 1 ,...,N and Xcand = {xi}q i=1 [11]. The same box decomposition algorithms used to compute HVI can be used to compute EHVI (either analytic or via MC) using piece-wise integration. EHVI computation is agnostic to the choice of box decomposition algorithm (and can also use approximate methods [ 9]). Similar to EI in the single-objective case, EHVI is a one-step Bayes-optimal algorithm for maximizing hypervolume in the MOO setting under the following assumptions: (i) only a single design will be generated and evaluated, (ii) the observations are noise-free, (iii) the ﬁnal approximate Pareto frontier (and ﬁnal design that will be deployed) will be drawn from the set of observed points [19]. 5 Expected Hypervolume Improvement with Noisy Observations We consider the case that frequently arises in practice where we only receive noisy observations yi = f(xi) + ϵi, ϵi ∼N (0,Σi), where Σi is the noise covariance. In this setting, EHVI is no longer (one-step) Bayes-optimal. This is because we can no longer compute the true Pareto frontier Pn = {f(x) |x∈Xn, ∄ x′∈Xn s.t.f(x′) ≻f(x)}over the previously evaluated points Xn = {xi}n i=1. Simply using the observed Pareto frontier, Yn = {y|y∈Yn, ∄ y′∈Yn s.t.y′≻y,y} where Yn = {yi}n i=1, can have strong detrimental effects on optimization performance. This is illustrated in Figure 1, which shows how EHVI is misled by noisy observations that appear to be Pareto optimal. EHVI proceeds to spend its evaluation budget trying to optimize noise, resulting in a clumped Pareto frontier that lacks diversity. Although the posterior mean could serve as a \"plug-in\" estimate of the true function values at the observed points and provide some regularization [61], we ﬁnd that this heuristic also leads to clustered Pareto frontiers (EHVI-PM in Fig. 1). Similar patterns emerge with DGEMO (which does not account for noise), and other baselines that utilize the posterior mean rather than the observed values when computing hypervolume improvement (see Appendix H). To our knowledge, all previous work onEHVI assumes that observations are noiseless [16, 58] or imputes the unknown true function values with the posterior mean. 2For brevity we omit the reference point r when referring to HVI. 4-15 -10 -5 0 Objective 1 -6 -5 -4 -3 -2 -1Objective 2  NEHVI EHVI EHVI-PM Ref. Point True PF Figure 1: An illustration of the effect of noisy observations on the true noiseless Pareto frontiers identiﬁed by NEHVI (our pro- posed algorithm), EHVI , and EHVI -PM, which uses the modeled posterior mean as point estimate of the true in-sample function values. All algorithms are tested on a BraninCurrin synthetic prob- lem, where observations are corrupted with zero-mean, additive Gaussian noise with a standard deviation of 5% of the range of respective objective. All methods use sequential (q= 1) optimiza- tion. See Appendix G for details. 5.1 A Bayes-optimal algorithm for hypervolume maximization in noisy environments In contrast with EHVI(-PM), we instead approach the problem of hypervolume maximization under noisy observations from a Bayesian perspective and derive a novel one-step Bayes-optimal expected hypervolume improvement criterion that iterates the expectation over the posterior distribution p(f(Xn)|Dn) of the function values at the previously evaluated points Xn given noisy observa- tions Dn = {xi,yi,(Σi)}n i=1. Our acquisition function, noisy expected hypervolume improvement (NEHVI), is deﬁned as αNEHVI (x) = ∫ αEHVI (x|Pn)p(f|Dn)df (2) where Pn denotes the Pareto frontier over f(Xn). By integrating over the uncertainty in the function values at the observed points, NEHVI retains one-step Bayes-optimality in noisy environments (in noiseless environments,NEHVI is equivalent to EHVI ). Empirically, Figure 1 shows that NEHVI is robust to noise and identiﬁes a well-distributed Pareto frontier with no signs of clumping, even under very noisy observations.3 The integral in (2) is analytically intractable, but can easily be approximated using MC integration. Let ˜ft ∼p(f|Dn) for t = 1 ,...N be samples from the posterior, and let Pt = {˜ft(x) |x ∈ Xn, ˜ft(x) ≻ ˜ft(x′) ∀x′∈Xn}be the Pareto frontier over the previously evaluated points under the sampled function ˜ft. Then, αNEHVI (x) ≈ 1 N ∑ N t=1 αEHVI (x|Pt). Using MC integration, we can compute the inner expectation in αEHVI simultaneously using samples from the joint posterior ˜ft(Xn,x) ∼p(f(Xn,x)|Dn) over xand Xn: ˆαNEHVI (x) = 1 N N∑ t=1 HVI ( ˜ft(x)|Pt). (3) See Appendix B for details on computing (3) using box decompositions. Note that this “full-MC” variant of NEHVI does not require objectives to be modeled independently, and supports multi-task covariance functions across correlated objectives. 5.2 Parallel Noisy Expected Hypervolume Improvement Generating and evaluating batches of candidates is imperative to achieving adequate throughput in many real-world scenarios. q NEHVI can naturally be extended to the parallel (asynchronous or batch) setting by evaluating HVI with respect to a batch of qpoints Xcand = {xi}q i=1 αqNEHVI (Xcand) = ∫ αqEHVI (Xcand|Pn)p(f|Dn)df≈ˆαqNEHVI (Xcand) = 1 N N∑ t=1 HVI ( ˜ft(Xcand)|Pt) (4) Since optimizing q candidates jointly is a difﬁcult numerical optimization problem over a qd- dimensional domain, we use a sequential greedy approximation in the parallel setting and solve a sequence of q simpler optimization problems with ddimensions, which been shown empirically to improve optimization performance [57]. While selecting candidates according to a “sequential greedy” policy does not guarantee that the selected batch of candidates is a maximizer of theαqNEHVI , the submodularity of αqNEHVI allows us to bound the regret of this approximation to be no more than 1 eα∗ qNEHVI , where α∗ qNEHVI = maxXcand∈XαqNEHVI (Xcand) (see Appendix F). 3This noise level is 5x greater than the ones considered by previous works that evaluate noisy MOBO [25]. 56 Efﬁcient Evaluation with Cached Box Decompositions Although ˆαNEHVI (x) in (3) has a concise mathematical form, computing it requires determining the Pareto frontier Pt under each sample ˜ft for t= 1,...,N and then partitioning the region that is not dominated by Pt into disjoint hyperrectangles {Skt}Kt kt=1. Optimizing the unbiased MC estimator of αNEHVI would require re-sampling {˜ft}N t=1 at each evaluation of αNEHVI . However, computing the Pareto frontier and performing a box decomposition under each of the N samples during every evaluation of αNEHVI in the inner optimization loop ( x∗ = arg maxx αNEHVI (x|Dn)) would be prohibitively expensive. This is because box decomposition algorithms have super-polynomial time complexity in the number of objectives [59]. We instead propose an efﬁcient alternative computational technique for repeated evaluations of EHVI with uncertain Pareto frontiers. Cached Box Decompositions: For repeated evaluations of the integral in (2), we use a set of ﬁxed samples {˜ft(Xn)}N t=1, which allows us to compute the Pareto frontiers and box decompositions once, and cache them for the entirety of the acquisition function optimization, thereby making those two computationally intensive operations a one-time cost per BO iteration.4 We refer to this approach as using cached box decompositions (CBD ). The method of optimizing over ﬁxed random samples is known as sample average approximation (SAA) [2]. Conditional Posterior Sampling: Under the CBD formulation, computing ˆαNEHVI (x) with joint samples from ˜ft(Xn,x) ∼p(f(Xn,x)|Dn) requires sampling from the conditional distributions ˜ft(x) ∼p ( f(x)|f(Xn) = ˜ft(Xn),Dn ) , (5) where t = 1,...,N and {˜ft(Xn)}N t=1 are the realized samples at the previously evaluated points. For multivariate Gaussian posteriors (as is the case with GP surrogates), we can sample from p(f(Xn)|Dn) via the reparameterization trick [ 30] by evaluating ˜ft(x) = µn + LT nζn,t,where ζn,t ∼N(0,InM), µn ∈RnM is the posterior mean, and Ln ∈RnM×nM is a lower triangular root decomposition of the posterior covariance matrix, typically a Cholesky decomposition. Given Ln, we can obtain a root decomposition L′ n of the covariance matrix of the joint posterior p(f(Xn,x)|Dn) by performing efﬁcient low-rank updates [44]. Given L′ n and the posterior mean of p(f(Xn,x)|Dn), we can sample from (5) via the reparameterization trick by augmenting the existing base samples ζn,t with M new base samples for the new point. 6.1 Efﬁcient Sequential Greedy Batch Selection using CBD The CBD technique addresses the general problem of inefﬁcient repeated evaluations of EHVI with uncertain Pareto frontiers. In this section, we show that sequential greedy batch selection (with both qEHVI and qNEHVI) is an incarnation of EHVI with uncertain Pareto frontiers. The original formulation of parallel EHVI in Daulton et al.[11] uses the inclusion-exclusion principle (IEP ), which involves computing the volume jointly dominated by each of the 2q −1 nonempty subsets of points in Xcand. However, using large batch sizes is not computationally feasible under this formulation because time and space complexity are exponential in qand multiplicative in the number of hyperrectangles in the box decomposition [11] (see Appendix D for a complexity analysis). Although qEHVI is optimized using sequential greedy batch selection, the IEP is used over all candidates x1,..., xi when selecting candidate i. Although the IEP could similarly be used to compute qNEHVI , we instead leverage CBD, which yields a sequential greedy approximation of the joint (noisy) EHVI that is mathematically equivalent to the IEP formulation, but signiﬁcantly reduces computational overhead. That is, the IEP and CBD approaches produce exactly the same acquisition value for a given set of points Xcand, but the IEP and the CBD approaches have exponential and polynomial time complexities in q, respectively. When selecting xi for i ∈{2,...,q }, all xj for which j < ihave already been selected and are therefore held constant. Thus, we can decompose qNEHVI into the qNEHVI from the previously selected candidates x1,..., xi−1 and NEHVI from xi given the previously selected candidates 4For greater efﬁciency, we may also prune Xn to remove points that are dominated with high probability, which we estimate via MC. 60 20 40 60 80 100 q 0 100 200 300Acquisition Optimization Time (s) CBD (CPU) CBD (GPU) IEP (CPU) IEP (GPU) OOM Figure 2: Acquisition optimization wall time under a sequential greedy approximation using L-BFGS-B. CBD enables scaling to much larger batch sizes qthan using the IEP and avoids running out-of-memory (OOM) on a GPU. Independent GPs are used for each outcome. The Pareto frontier of of the 2-objective, 6- dimensional DTLZ2 problem [ 13] is initialized with 20 points. Wall times were measured on a Tesla V100 SXM2 GPU (16GB RAM) and a 2x Intel Xeon 6138 CPU @ 2GHz (251GB RAM). See Appendix H.2 for results with more objectives. ˆαqNEHVI ({xj}i j=1) = 1 N N∑ t=1 HVI ( {˜ft(xj)}i−1 j=1}|Pt ) + 1 N N∑ t=1 HVI ( ˜ft(xi) |Pt∪{˜ft(xj)}i−1 j=1} ) (6) Note that the ﬁrst term on the right hand side is constant, since {xj}i−1 j=1 and {˜ft(xj)}i−1 j=1 are ﬁxed for all t = 1 ,...,N . The second term is ˆαNEHVI (xi), where the NEHVI is taken with respect to the Pareto frontier across f(Xn,x1,..., xi−1) and computed using the ﬁxed samples {˜ft(Xn,x1,...xi−1)}N t=1. To compute the second term when selecting candidate xi, the N Pareto frontiers and CBD s are updated to include {˜ft(Xn,x1,...xi−1)}N t=1. As in the sequential q = 1 setting, the box decompositions are only computed and cached while selecting each candidate point. See Appendix C.2 for a derivation of (6). Although we have focused on qNEHVI in the above, the CBD formulation for qEHVI is obtained by simply replacing Pt with the Pareto frontier over the observed values Yn. Despite computing Nbox decompositions when selecting each candidatexifor i= 2,...,q , the CBD approach reduces the time and space complexity from exponential (under the IEP ) to polynomial in q(see Appendix D for details on time and space complexity). Figure 2 shows the total acquisition optimization time (including box decompositions) for various batch sizes and demonstrates that using CBD allows to scale to batch sizes that are completely infeasible when using IEP. 7 Optimizing NEHVI Differentiability: Importantly, ˆαNEHVI (x) is differentiable w.r.t. x. Although determining the Pareto frontier and computing the box decompositions are non-differentiable operations, these operations do not involve x, even when re-sampling from the joint posterior p(f(Xn,x)|Dn). Exact sample-path gradients of ∇x ˆαNEHVI (x) can easily be computed using auto-differentiation in modern computational frameworks. This enables efﬁcient gradient-based optimization of qNEHVI. 5 SAA Convergence Results: In addition to approximating the outer expectation over f(Xn) with ﬁxed posterior samples, we can similarly ﬁx the base samples used for the new candidate point x. This approach yields a deterministic acquisition function, which enables using (quasi-) higher-order optimization methods to obtain fast convergence rates for acquisition optimization [2]. Importantly, we prove that the theoretical convergence guarantees on acquisition optimization under the SAA approach proposed by Balandat et al. [2] also hold for NEHVI. Theorem 1. Suppose Xis compact and fhas a multi-output GP prior with continuously differen- tiable mean and covariance functions. Let Xn = {xi}n i=1 denote the previously evaluated points and {ζ}N t=1 be base samples ζ ∼N (0,I(n+1)M). Let ˆαNEHVI denote the deterministic acquisi- tion function computed using {ζ}N t=1 as ˆαN NEHVI and deﬁne S∗ := arg maxx∈XαNEHVI (x) to be the set of maximizers of αNEHVI (x) over X. Suppose ˆx∗ N ∈arg maxx∈X ˆαN NEHVI (x). Then (1) ˆαN NEHVI (ˆx∗ N) →αNEHVI (x∗ N) almost surely, and (2) dist(ˆx∗ N,S∗) →0, where dist(ˆx∗ N,S∗) := infx∈S∗||ˆx∗ N −x||is the Euclidean distance between ˆx∗ N and the set S∗. Theorem 1 also holds in the parallel setting, so qNEHVI enjoys the same convergence guarantees as NEHVI on acquisition optimization under the SAA. See Appendix E for further details and proof. 5One can also show that the gradient of the full MC estimator ˆαqNEHVI is an unbiased estimator of the gradient of the true joint noisy expected hypervolume improvement αqNEHVI . However, this result is not necessary for our SAA approach. 78 Approximation of qNEHVI using Approximate GP Sample Paths Although CBD yields polynomial complexity of qNEHVI with respect to q(rather than exponential complexity with the IEP), it still requires computingNbox decompositions and repeatedly evaluating the joint posterior over f(Xn,{xj}i−1 j=1) for selecting each candidate xi for i= 1,...,q . A cheaper alternative is to approximate the integral in (4) using a single approximate GP sample path ˜fi using RFFs when optimizing candidate xi. A single-sample approximation of qNEHVI, which we refer to as qNEHVI-1 , can be computed by using ˜fi as the sampled GP in (6). Since the RFF is a deterministic model, it is much less computationally expensive to evaluate than the GP posterior on out-of-sample points, and exact gradients of qNEHVI-1 with respect to current candidate xi can be computed and used for efﬁcient multi-start optimization of qNEHVI-1 using second-order gradient methods. qNEHVI-1 requires CBD for efﬁcient sequential greedy batch selection and gradient-based optimization, but does not use a sample average approximation for optimizing a new candidatexi; instead, it uses an approximate sample path. See Rahimi & Recht [46] for details on RFFs. qNEHVI-1 is related to TSEMO in that both use sequential greedy batch selection using HVI based on RFF samples. However, TSEMO does not directly maximize HVI when selecting candidate xi, where i= 1,...,q ; rather, it relies on a heuristic approach of running NSGA-II on an RFF sample of each objective to create a discrete population of candidates and then selecting the point from the discrete population that maximizes HVI under the RFF sample. In contrast, qNEHVI-1 directly optimizes HVI under the RFF using exact sample-path gradients, which leads to improved optimiza- tion performance (see Appendix H). Furthermore, we ﬁnd that qNEHVI-1 is signiﬁcantly faster than TSEMO, because rather than using NSGA-II it uses second order gradient methods to optimize HVI (see Appendix H). Gradient-based optimization is only possible because CBD enables scalable, differentiable HVI computation. While the primary goal of this work is to develop a principled, scalable method for parallel EHVI in noisy environments, we include empirical comparisons with qNEHVI-1 throughout the appendix to demonstrate the generalizablility of the CBD approach and practical performance of the qNEHVI-1 approximation. qNEHVI-1 achieves the fastest batch selection timesof any method tested on a GPU on every problem; in many cases, this is an order of magnitude speed-up over qNEHVI . Moreover, qNEHVI-1 has a remarkable ability to scale to large batch sizes when the dimensionality of optimization problem is modest. Further investigation of qNEHVI-1 is needed, but we hope that the readers can recognize the ways in which qNEHVI can create broader opportunities for research into hypervolume improvement based acquisition functions. 9 Experiments We empirically evaluate qNEHVI on a set of synthetic and real-world benchmark problems. We compare it against the following recently proposed methods from the literature: PESMO, MESMO (which we extend to the handle noisy observations using the noisy information gain from Takeno et al. [52]), PFES, DGEMO, MOEA/D-EGO, TSEMO, TS-TCH, qEHVI (and qEHVI-PM-CBD , which uses the posterior mean as a plug-in estimate for the function values at the in-sample points, along with CBD to scale to large batch sizes), and qNParEGO. We optimize all methods using multi-start L-BFGS-B with exact gradients (except for PFES, which uses gradients approximated via ﬁnite differences), including TS-TCH where we optimize approximate function samples using RFFs with 500 basis functions. We model each outcome with an independent GP with a Matérn 5/2 ARD kernel and infer the GP hyperparameters via maximum a posteriori (MAP) estimation. For all problems, we assume that the noise variances are observed (except ABR, where we infer the noise level). See Appendix G for more details on the experiments and acquisition function implementations. We evaluate all methods using the logarithm of the difference in hypervolume between the true Pareto frontier and the approximate Pareto frontier recovered by the algorithm. Since evaluations are noisy, we compute the hypervolume dominated by the noiseless Pareto frontier across the observed points for each method. Synthetic Problems: We consider a noisy variants of the BraninCurrin problem (M = 2,d = 2) and the DTLZ2 problem (M = 2,d = 6) [13], in which observations are corrupted with zero-mean additive Gaussian noise with standard deviation of 5% of the range of each objective forBraninCurrin and 10% for DTLZ2. Adaptive Bitrate (ABR) Control Policy Optimization: ABR controllers are used for real-time communication and media streaming applications. Policies for these controllers must be tuned to 80 50 100 150 200 Function Evaluations 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75Log Hypervolume Difference BraninCurrin 0 50 100 150 200 Function Evaluations -1.10 -1.00 -0.90 -0.80 -0.70 -0.60 -0.50 -0.40 DTLZ2 0 50 100 150 200 Function Evaluations 4.80 5.00 5.20 5.40 5.60 5.80 ABR 0 50 100 150 200 Function Evaluations -1.00 -0.50 0.00 0.50 1.00 VehicleSafety DGEMO MESMO MOEA/D-EGO PESMO PFES TS-TCH TSEMO qEHVI qEHVI-PM-CBD qNEHVI qNEHVI-1 qNParEGO Figure 3: Sequential optimization performance. The shaded region indicates two standard errors of the mean over 100 replications (only 20 replications were feasible for PESMO due to large runtimes). deliver a high quality of experience with respect to multiple objectives [40]. In industry settings, A/B tests with dozens of policies are tested simultaneously since each policy may take days or weeks to evaluate, producing noisy measurements across multiple objectives. In this experiment, we tune policies to maximize video quality (bitrate) and minimize stall time. The policy hasd= 4 parameters, which are detailed in Appendix G. We use the Park simulator [ 41] and sample a random set of 100 traces to obtain noisy measurements of the objectives under a given policy. For comparing the performance of different methods, we estimate the true noiseless objective using mean objectives across 300 traces. We infer a homoskedastic noise level jointly with the GP hyperparameters via MAP estimation. Vehicle Design Optimization: Optimizing the design of the frame an automobile is important to maximizing passenger safety, vehicle durability and fuel efﬁciency. Evaluating a vehicle design is time-consuming, since either a vehicle must manufactured and crashed, or a nonlinear ﬁnite element-based crash analysis must be run to simulate a collision (which can take over 20 hours per run) [62]. Hence, evaluating many designs in parallel is critical for reducing end-to-end optimization time. Observations are often noisy due to manufacturing imperfections, measurement error, or non-deterministic simulations. In this experiment, we tune the d= 5 widths of various components of a vehicle’s frame to minimize proxy metrics for (1) fuel consumption, (2) passenger trauma in a full frontal collison, and (3) vehicle fragility [53]. See Appendix G for details. For this demonstration, we add zero-mean Gaussian noise with a standard deviation of 1% of the objective range, which roughly corresponds to the manufacturing noise level used in previous work [62]. 9.1 Summary of Results: We ﬁnd that qNEHVI and qNEHVI-1 outperform all other methods on the noisy benchmarks, both in the sequential and parallel setting. In the sequential setting (Fig 3), qNEHVI and qNEHVI-1 are followed closely by qEHVI -PM, and in some cases, even qEHVI . TS-TCH is ﬁrmly in the middle of the pack, while information-theoretic acquisition functions appear to perform the worst. This is consistent across noise levels; for experiments where we add noise to the objectives, we consider noise levels ranging from 1% to 10% of the range of each objective (these are magnitudes of the noise often seen in practice). Previous works have only evaluated MOBO algorithms with noise levels of 1% [25]. In Appendix H, we perform a study showing that qNEHVI consistently performs best with increasing noise levels up to 30% of the range of each objective. While parallel evaluation can provide optimization speedups on order of the batch size q, these evaluations do affect the overall sample complexity of the algorithm, since less information is available within the synchronous batch setting compared with fully sequential optimization. We ﬁnd that, by and large, qNEHVI achieves the greatest hyper-volume for increasingly large batch sizes, and scales more elegantly relative to TS-TCH and the ParEGO variants (Fig 4). qNEHVI also consistently outperforms qEHVI -PM-CBD. In Appendix H, we observe that qNEHVI and qNEHVI-1 provides excellent anytime performance all values ofqthat we tested. We provide results on 4 additional test problems in Appendix H.3, and in Appendix H.8, we demonstrate that leveraging CBD and a single sample path approximation, qNEHVI-1 enables scaling to 5-objective problems, which is a ﬁrst for an HVI-based method, to our knowledge. 91 8 16 32 q -0.25 0.00 0.25 0.50 0.75 1.00 1.25 1.50 Log Hypervolume Difference BraninCurrin 1 8 16 32 q -1.10 -1.00 -0.90 -0.80 -0.70 -0.60 -0.50 DTLZ2 1 8 16 32 q 4.70 4.80 4.90 5.00 5.10 5.20 ABR 1 8 16 32 q -1.00 -0.75 -0.50 -0.25 0.00 0.25 0.50 0.75 VehicleSafety DGEMO MOEA/D-EGO TS-TCH TSEMO qEHVI qEHVI-PM-CBD qNEHVI qNEHVI-1 qNParEGO Figure 4: The quality of the ﬁnal Pareto frontier identiﬁed by each method with increasing batch sizes qgiven a budget of 224 function evaluations. qEHVI is only included for q = 1 and q = 8 because the IEP scales exponential with q. DGEMO is omitted on the ABR problem because it was prohibitively slow with time-consuming ABR simulations and on the VehicleSafety problem because DGEMO consistently crashed in the graph cutting algorithm. In our experiments, we ﬁnd that qNEHVI-1 is among the top performers on relatively low- dimensional problems. Given the strong performance of qNEHVI-1 , we examine its performance as the dimensionality of the search space increases in Appendix H.5. We ﬁnd that qNEHVI is more robust than qNEHVI-1 in higher-dimensional search spaces, but further investigation is needed into how the number of the Fourier basis functions affects the performance ofqNEHVI-1 in high- dimensional search spaces. Optimization wall time: Across all experiments, we observe competitive wall times for optimizing qNEHVI and qNEHVI-1 (all wall time comparisons are provided in Appendix H). On a GPU, optimizing qNEHVI-1 incurs the lowest wall time of any method that we tested on every single problem and optimizing qNEHVI is faster than optimizing information-theoretic methods on all problems. Using efﬁcient low-rank Cholesky updates, qNEHVI is often faster than the qNParEGO implementation in BoTorch on a GPU. 10 Discussion We proposed NEHVI , a novel acquisition function that provides a principled approach to parallel and noisy multi-objective Bayesian optimization. NEHVI is a one-step Bayes-optimal policy for maximizing the hypervolume dominated by the Pareto frontier in noisy and noise-free settings. NEHVI is made feasible by a new approach to computing joint hypervolumes ( CBD ), and we demonstrated that CBD enables scalable, parallel candidate generation with both noiseless qEHVI and qNEHVI . We provide theoretical results on optimizing a MC estimator ofqNEHVI using sample average approximation and demonstrate signiﬁcant improvements in optimization performance over state-of-the-art MOBO algorithms. Yet, our work has some limitations. While the information-theoretic acquisition functions tested here perform poorly on our benchmarks, they do allow for decoupled evaluations of different objectives in cases where querying one objective may be more resource-intensive than querying other objectives. Optimizing such acquisition functions is a non-trivial task, and it is possible that with improved procedures, such acquisition functions could yield improved performance and provide a principled approach to selecting evaluation sources on a budget. Although practically fast enough for most Bayesian optimization tasks, exact hypervolume computation has super-polynomial complexity in the number of objectives. Combining qNEHVI with differentiable approximate methods for computing hypervolume (e.g. Couckuyt et al. [9], Golovin & Zhang [23]) could lead to further speed-ups. We hope that the core ideas presented in this work, including the CBD approach, can provide a framework to support the development of new computationally efﬁcient MOBO methods. 10References [1] Asadpour, A., Nazerzadeh, H., and Saberi, A. Stochastic submodular maximization. In Papadimitriou, C. and Zhang, S. (eds.), Internet and Network Economics . Springer Berlin Heidelberg, 2008. [2] Balandat, M., Karrer, B., Jiang, D. R., Daulton, S., Letham, B., Wilson, A. G., and Bakshy, E. BoTorch: A Framework for Efﬁcient Monte-Carlo Bayesian Optimization. In Advances in Neural Information Processing Systems 33, 2020. [3] Belakaria, S., Deshwal, A., and Doppa, J. R. Max-value entropy search for multi-objective bayesian optimization. In Advances in Neural Information Processing Systems 32, 2019. [4] Binois, M., Ginsbourger, D., and Roustant, O. Quantifying uncertainty on pareto fronts with gaussian process conditional simulations. Eur. J. Oper. Res., 243:386–394, 2015. [5] Bradford, E., Schweidtmann, A. M., and Lapkin, A. Efﬁcient multiobjective optimization employing gaussian processes, spectral sampling and a genetic algorithm. Journal of global optimization, 71(2):407–438, 2018. [6] Brockhoff, D., Tusar, T., Auger, A., and Hansen, N. Using well-understood single-objective functions in multiobjective black-box optimization test suites, 2019. [7] Calandra, R. and Peters, J. Pareto front modeling for sensitivity analysis in multi-objective bayesian optimization. 2014. [8] Calandra, R., Seyfarth, A., Peters, J., and Deisenroth, M. P. Bayesian optimization for learning gaits under uncertainty. Annals of Mathematics and Artiﬁcial Intelligence , 76(1):5–23, Feb 2016. ISSN 1573-7470. doi: 10 .1007/s10472-015-9463-9. [9] Couckuyt, I., Deschrijver, D., and Dhaene, T. Towards efﬁcient multiobjective optimization: Multiobjective statistical criterions. In 2012 IEEE Congress on Evolutionary Computation, pp. 1–8, 2012. [10] Daulton, S., Singh, S., Avadhanula, V ., Dimmery, D., and Bakshy, E. Thompson sampling for contextual bandit problems with auxiliary safety constraints. In NeurIPS Workshop on Safety and Robustness in Decision Making, 2019. [11] Daulton, S., Balandat, M., and Bakshy, E. Differentiable expected hypervolume improvement for parallel multi-objective Bayesian optimization. In Advances in Neural Information Processing Systems 33, NeurIPS, 2020. [12] Deb, K., Pratap, A., Agarwal, S., and Meyarivan, T. A fast and elitist multiobjective genetic algorithm: Nsga-ii. IEEE Transactions on Evolutionary Computation, 6(2):182–197, 2002. [13] Deb, K., Thiele, L., Laumanns, M., and Zitzler, E. Scalable multi-objective optimization test problems. volume 1, pp. 825–830, 06 2002. ISBN 0-7803-7282-4. doi: 10 .1109/ CEC.2002.1007032. [14] Deb, K., Gupta, S., Daum, D., Branke, J., Mall, A. K., and Padmanabhan, D. Reliability-based optimization using evolutionary algorithms. IEEE Transactions on Evolutionary Computation, 13(5):1054–1074, 2009. doi: 10 .1109/TEVC.2009.2014361. [15] Dächert, K., Klamroth, K., Lacour, R., and Vanderpooten, D. Efﬁcient computation of the search region in multi-objective optimization. European Journal of Operational Research, 260 (3):841 – 855, 2017. [16] Emmerich, M. T. M., Giannakoglou, K. C., and Naujoks, B. Single- and multiobjective evolutionary optimization assisted by gaussian random ﬁeld metamodels. IEEE Transactions on Evolutionary Computation, 10(4):421–439, 2006. [17] Feng, Q., Letham, B., Bakshy, E., and Mao, H. High-Dimensional Contextual Policy Search with Unknown Context Rewards using Bayesian Optimization. In Advances in Neural Information Processing Systems 33, 2020. [18] Fisher, M. L., Nemhauser, G. L., and Wolsey, L. A. An analysis of approximations for maximizing submodular set functions—II , pp. 73–87. Springer Berlin Heidelberg, Berlin, Heidelberg, 1978. [19] Frazier, P. I. A tutorial on bayesian optimization. arXiv preprint arXiv:1807.02811, 2018. 11[20] Garrido-Merchán, E. C. and Hernández-Lobato, D. Predictive entropy search for multi-objective bayesian optimization with constraints. Neurocomputing, 361:50–68, 2019. [21] Garrido-Merchán, E. C. and Hernández-Lobato, D. Parallel predictive entropy search for multi-objective bayesian optimization with constraints, 2020. [22] Gelbart, M. A., Snoek, J., and Adams, R. P. Bayesian optimization with unknown constraints. In Proceedings of the 30th Conference on Uncertainty in Artiﬁcial Intelligence, UAI, 2014. [23] Golovin, D. and Zhang, Q. Random hypervolume scalarizations for provable multi-objective black box optimization, 2020. [24] Hernández-Lobato, J. M., Hoffman, M. W., and Ghahramani, Z. Predictive entropy search for efﬁcient global optimization of black-box functions. In Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1 , NIPS’14, pp. 918–926, Cambridge, MA, USA, 2014. MIT Press. [25] Hernández-Lobato, D., Hernández-Lobato, J. M., Shah, A., and Adams, R. P. Predictive entropy search for multi-objective bayesian optimization, 2015. [26] Horn, D., Dagge, M., Sun, X., and Bischl, B. First investigations on noisy model-based multi- objective optimization. volume 10173, pp. 298–313, 02 2017. ISBN 978-3-319-54156-3. doi: 10.1007/978-3-319-54157-0_21. [27] Igel, C., Hansen, N., and Roth, S. Covariance matrix adaptation for multi-objective optimization. Evolutionary Computation, 15(1):1–28, 2007. doi: 10 .1162/evco.2007.15.1.1. [28] Jiang, S., Zhang, H., Cong, W., Liang, Z., Ren, Q., Wang, C., Zhang, F., and Jiao, X. Multi- objective optimization of smallholder apple production: Lessons from the bohai bay region. Sustainability, 12(16):6496, 2020. [29] Jones, D. R., Schonlau, M., and Welch, W. J. Efﬁcient global optimization of expensive black-box functions. Journal of Global Optimization, 13:455–492, 1998. [30] Kingma, D. P. and Welling, M. Auto-Encoding Variational Bayes. arXiv e-prints , pp. arXiv:1312.6114, Dec 2013. [31] Klamroth, K., Lacour, R., and Vanderpooten, D. On the representation of the search region in multi-objective optimization. European Journal of Operational Research, 245(3):767–778, Sep 2015. ISSN 0377-2217. doi: 10 .1016/j.ejor.2015.03.031. URL http://dx.doi.org/ 10.1016/j.ejor.2015.03.031. [32] Knowles, J. Parego: a hybrid algorithm with on-line landscape approximation for expensive multiobjective optimization problems. IEEE Transactions on Evolutionary Computation, 10(1): 50–66, 2006. [33] Koch, P., Wagner, T., Emmerich, M. T., Back, T., and Konen, W. Efﬁcient multi-criteria optimization on noisy machine learning problems. Appl. Soft Comput., 29(C):357–370, April 2015. ISSN 1568-4946. doi: 10 .1016/j.asoc.2015.01.005. URL https://doi.org/10.1016/ j.asoc.2015.01.005. [34] Lacour, R., Klamroth, K., and Fonseca, C. M. A box decomposition algorithm to compute the hypervolume indicator. Computers & Operations Research, 79:347 – 360, 2017. [35] LeCun, Y ., Cortes, C., and Burges, C. Mnist handwritten digit database.ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist, 2, 2010. [36] Letham, B. and Bakshy, E. Bayesian optimization for policy search via online-ofﬂine ex- perimentation. Journal of Machine Learning Research , 20(145):1–30, 2019. URL http: //jmlr.org/papers/v20/18-225.html. [37] Letham, B., Karrer, B., Ottoni, G., and Bakshy, E. Constrained bayesian optimization with noisy experiments. Bayesian Analysis, 14(2):495–519, 06 2019. doi: 10 .1214/18-BA1110. [38] Liao, T., Wang, G., Yang, B., Lee, R., Pister, K., Levine, S., and Calandra, R. Data-efﬁcient learning of morphology and controller for a microrobot. In 2019 International Conference on Robotics and Automation (ICRA), pp. 2488–2494. IEEE, 2019. [39] Lukovic, K. M., Tian, Y ., and Matusik, W. Diversity-guided multi-objective bayesian opti- mization with batch evaluations. Advances in Neural Information Processing Systems , 33, 2020. 12[40] Mao, H., Chen, S., Dimmery, D., Singh, S., Blaisdell, D., Tian, Y ., Alizadeh, M., and Bakshy, E. Real-world video adaptation with reinforcement learning. 2019. [41] Mao, H., Negi, P., Narayan, A., Wang, H., Yang, J., Wang, H., Marcus, R., Addanki, R., Shirkoohi, M. K., He, S., Nathan, V ., Cangialosi, F., Venkatakrishnan, S. B., Weng, W.-H., Han, S.-W., Kraska, T., and Alizadeh, M. Park: An open platform for learning-augmented computer systems. In NeurIPS, 2019. [42] Mennen, S. M., Alhambra, C., Allen, C. L., Barberis, M., Berritt, S., Brandt, T. A., Campbell, A. D., Castañón, J., Cherney, A. H., Christensen, M., Damon, D. B., Eugenio de Diego, J., García-Cerrada, S., García-Losada, P., Haro, R., Janey, J., Leitch, D. C., Li, L., Liu, F., Lobben, P. C., MacMillan, D. W. C., Magano, J., McInturff, E., Monfette, S., Post, R. J., Schultz, D., Sitter, B. J., Stevens, J. M., Strambeanu, I. I., Twilton, J., Wang, K., and Zajac, M. A. The evolution of high-throughput experimentation in pharmaceutical development and perspectives on the future. Organic Process Research & Development, 23(6):1213–1242, 2019. doi: 10.1021/acs.oprd.9b00140. [43] Namkoong, H., Daulton, S., and Bakshy, E. Distilled thompson sampling: Practical and efﬁcient thompson sampling via imitation learning. In NeurIPS Ofﬂine Reinforcement Learning Workshop, 2020. [44] Osborne, M. A. Bayesian gaussian processes for sequential prediction, optimisation and quadrature. 2010. [45] Paria, B., Kandasamy, K., and Póczos, B. A Flexible Multi-Objective Bayesian Optimization Approach using Random Scalarizations. ArXiv e-prints, May 2018. [46] Rahimi, A. and Recht, B. Random features for large-scale kernel machines. In Proceedings of the 20th International Conference on Neural Information Processing Systems, NIPS’07, pp. 1177–1184, Red Hook, NY , USA, 2007. Curran Associates Inc. ISBN 9781605603520. [47] Rasmussen, C. E. Gaussian Processes in Machine Learning , pp. 63–71. Springer Berlin Heidelberg, Berlin, Heidelberg, 2004. [48] Real, E., Aggarwal, A., Huang, Y ., and Le, Q. V . Regularized evolution for image classiﬁer architecture search. Proceedings of the AAAI Conference on Artiﬁcial Intelligence , 33(01): 4780–4789, Jul. 2019. doi: 10 .1609/aaai.v33i01.33014780. URL https://ojs.aaai.org/ index.php/AAAI/article/view/4405. [49] Schuster, M. Speech recognition for mobile devices at google. In Zhang, B.-T. and Orgun, M. A. (eds.), PRICAI 2010: Trends in Artiﬁcial Intelligence, pp. 8–10, Berlin, Heidelberg, 2010. Springer Berlin Heidelberg. ISBN 978-3-642-15246-7. [50] Shahriari, B., Swersky, K., Wang, Z., Adams, R. P., and de Freitas, N. Taking the human out of the loop: A review of bayesian optimization. Proceedings of the IEEE, 104(1):148–175, 2016. [51] Suzuki, S., Takeno, S., Tamura, T., Shitara, K., and Karasuyama, M. Multi-objective Bayesian optimization using pareto-frontier entropy. In III, H. D. and Singh, A. (eds.), Pro- ceedings of the 37th International Conference on Machine Learning , volume 119 of Pro- ceedings of Machine Learning Research , pp. 9279–9288. PMLR, 13–18 Jul 2020. URL http://proceedings.mlr.press/v119/suzuki20a.html. [52] Takeno, S., Fukuoka, H., Tsukada, Y ., Koyama, T., Shiga, M., Takeuchi, I., and Karasuyama, M. Multi-ﬁdelity Bayesian optimization with max-value entropy search and its parallelization. In III, H. D. and Singh, A. (eds.), Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 9334–9345. PMLR, 13–18 Jul 2020. URL http://proceedings.mlr.press/v119/takeno20a.html. [53] Tanabe, R. and Ishibuchi, H. An easy-to-use real-world multi-objective optimization problem suite. Applied Soft Computing, 89:106078, 2020. ISSN 1568-4946. doi: https://doi.org/10.1016/ j.asoc.2020.106078. [54] Thompson, W. R. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 25(3/4):285–294, 1933. [55] Touré, C., Hansen, N., Auger, A., and Brockhoff, D. Uncrowded hypervolume improvement: Como-cma-es and the sofomore framework. In Proceedings of the Genetic and Evolutionary Computation Conference, GECCO ’19, pp. 638–646, New York, NY , USA, 2019. Association for Computing Machinery. ISBN 9781450361118. doi: 10 .1145/3321707.3321852. URL https://doi.org/10.1145/3321707.3321852. 13[56] Wang, R., Xiong, J., Ishibuchi, H., Wu, G., and Zhang, T. On the effect of reference point in moea/d for multi-objective optimization. Applied Soft Computing , 58:25–34, 2017. ISSN 1568-4946. doi: https://doi .org/10.1016/j.asoc.2017.04.002. URL https: //www.sciencedirect.com/science/article/pii/S1568494617301722. [57] Wilson, J., Hutter, F., and Deisenroth, M. Maximizing acquisition functions for bayesian optimization. In Advances in Neural Information Processing Systems 31, pp. 9905–9916. 2018. [58] Yang, K., Emmerich, M., Deutz, A., and Bäck, T. Multi-objective bayesian global optimization using expected hypervolume improvement gradient. Swarm and Evolutionary Computation, 44: 945 – 956, 2019. ISSN 2210-6502. doi: https://doi .org/10.1016/j.swevo.2018.10.007. [59] Yang, K., Emmerich, M., Deutz, A. H., and Bäck, T. Efﬁcient computation of expected hypervolume improvement using box decomposition algorithms. CoRR, abs/1904.12672, 2019. [60] Yang, K., Palar, P., Emmerich, M., Shimoyama, K., and Bäck, T. A multi-point mechanism of expected hypervolume improvement for parallel multi-objective bayesian global optimization. pp. 656–663, 07 2019. doi: 10 .1145/3321707.3321784. [61] Yang, K., Palar, P. S., Emmerich, M., Shimoyama, K., and Bäck, T. A multi-point mech- anism of expected hypervolume improvement for parallel multi-objective bayesian global optimization. In Proceedings of the Genetic and Evolutionary Computation Conference , GECCO ’19, pp. 656–663, New York, NY , USA, 2019. Association for Computing Machinery. ISBN 9781450361118. doi: 10 .1145/3321707.3321784. URL https://doi.org/10.1145/ 3321707.3321784. [62] Youn, B. D., Choi, K., Yang, R.-J., and Gu, L. Reliability-based design optimization for crashworthiness of vehicle side impact. Structural and Multidisciplinary Optimization , 26: 272–283, 02 2004. doi: 10 .1007/s00158-003-0345-0. [63] Zhang, G. and Block, D. E. Using highly efﬁcient nonlinear experimental design methods for optimization of lactococcus lactis fermentation in chemically deﬁned media. Biotechnology progress, 25(6):1587–1597, 2009. [64] Zhang, Q., Liu, W., Tsang, E., and Virginas, B. Expensive multiobjective optimization by moea/d with gaussian process model. IEEE Transactions on Evolutionary Computation, 14(3): 456–474, 2010. doi: 10 .1109/TEVC.2009.2033671. [65] Zhou, A., Zhang, Q., and Zhang, G. A multiobjective evolutionary algorithm based on decom- position and probability model. In 2012 IEEE Congress on Evolutionary Computation, pp. 1–8, 2012. doi: 10 .1109/CEC.2012.6252954. [66] Zitzler, E., Deb, K., and Thiele, L. Comparison of multiobjective evolutionary algorithms: Empirical results. Evol. Comput., 8(2):173–195, June 2000. ISSN 1063-6560. doi: 10 .1162/ 106365600568202. URL https://doi.org/10.1162/106365600568202. [67] Zitzler, E., Deb, K., and Thiele, L. Comparison of multiobjective evolutionary algorithms: Empirical results. Evolutionary computation, 8(2):173–195, 2000. 14Appendix to: Parallel Bayesian Optimization of Multiple Noisy Objectives with Expected Hypervolume Improvement A Potential Societal Impact Bayesian Optimization speciﬁcally aims to increase sample efﬁciency for hard optimization algo- rithms, and consequently can help achieve better solutions without incurring large societal costs. For instance, as demonstrated in this work, automotive design problems may be solved much faster, reducing the amount of computationally costly simulations and thus the energy footprint during development. At the same time, improved solutions mean that high crash safety can be achieved with lighter cars, resulting in fewer resources required for their production and, importantly, improving fuel economy of the whole vehicle ﬂeet. Increased robustness to noisy observations further helps reduce the resources spent on evaluating regions of the search space that are not promising. Improvements to the optimization performance and practicality of multi-objective Bayesian optimization have the potential to allow decision makers to better understand and make more informed decisions across multiple trade-offs. We expect these directions to be particularly important as Bayesian optimization is increasingly used for applications such as recommender systems [36], where auxiliary goals such as fairness must be accounted for. Of course, at the end of the day, exactly what objectives decision makers choose to optimize, and how they balance those trade-offs (and whether that is done in equitable fashion) is up to the individuals themselves. B Computing Hypervolume Improvement with Box Decompositions Deﬁnition 3. For a set of objective vectors {f(xi)}q i=1, a reference point r∈RM, and a Pareto frontier P, let ∆({f(xi)}q i=1,P,r) ⊂ RM denote the set of points (1) that are dominated by {f(xi)}q i=1, (2) that dominate r, and (3) that are not dominated by P. Let {S1,...,S K}be a set ofKdisjoint axis-aligned rectangles where each Sk is deﬁned by a pair of lower and upper vertices lk ∈RM and uk ∈RM ∪{∞}. Figure 5 shows an example decomposition. Such a partitioning allows for efﬁcient piece-wise computation of the hypervolume improvement from a new point f(xi) by computing the volume of the intersection of the region dominated exclusively by the new point with ∆({f(xi),P,r) (and not dominated by the P) with each hyperrectangle Sk. Although ∆(f(xi),P,r) is a non-rectangular polytope, the intersection of∆(f(xi),P,r) with each rectangle Sk is a rectangular polytope and the vertices bounding the hyperrectangle corresponding to ∆(f(xi),P,r) ∩Sk can be easily computed: the lower bound vertex is lk and the upper bound vertex is the component-wise minimum of uk and the new point f(x): zk := min [ uk,f(x) ] . The hypervolume improvement can be computed by summing over the volume of∆(f(xi),P,r) ∩Sk over all Sk HVI ( f(x),P ) = K∑ k=1 HVI k ( f(x),lk,uk ) = K∑ k=1 M∏ m=1 [ z(m) k −l(m) k ] +, (7) where [·]+ denotes the max(·,0) operation. 15f(x) r l3 S3 u3u2 S2 l2 l4 S4 u4u1 l1 S1 f (2)(x) f (1)(x) f(x1) f(x2) f(x3) Figure 5: The hypervolume improvement from a new point f(x) is shown in blue. The current Pareto frontier Pis given by the green points, the green area is the hypervolume of the Pareto frontier Pgiven reference point r. The white rectangles S1,...,S k are a disjoint, box decomposition of the non-dominated space that can be used to efﬁciently compute the hypervolume improvement. C qNEHVI under Different Computational Approaches C.1 Derivation of IEP formulation of qNEHVI From (4), the expected noisy joint hypervolume improvement is given by ˆαqNEHVI (Xcand) = 1 N N∑ t=1 HVI ( ˜ft(Xcand)|Pt) Recall that the joint HVI formulation under the IEP derived by Daulton et al. [11] is given by HVI (f(Xcand)|P) = K∑ k=1 q∑ j=1 ∑ Xj∈Xj (−1)j+1 M∏ m=1 [ z(m) k,Xj −l(m) k ] + (8) where Xj := {Xj ⊆Xcand : |Xj|= j}and z(m) k,t,Xj := min[ u(m) k,t ,f(m)(xi1 ),...,f (m)(xij )] for Xj = {xi1 ,..., xij }. In qNEHVI , the lower and upper bounds and the number of rectangles in each box decomposition depend Pt. Hence, ˆαqNEHVI (Xcand) = 1 N N∑ t=1 Kt∑ k=1 q∑ j=1 ∑ Xj∈Xj (−1)j+1 M∏ m=1 [ z(m) k,t,Xj −l(m) k,t ] + where z(m) k,t,Xj := min[u(m) k,t , ˜f(m) t (xi1 ),..., ˜f(m) t (xij )] for Xj = {xi1 ,..., xij }. C.2 Derivation of CBD formulation of qNEHVI Using Deﬁnition 2, we rewrite (4) as ˆαqNEHVI (Xcand) = 1 N N∑ t=1 HVI ( ˜ft(Xcand)|Pt) = 1 N N∑ t=1 [ HV( ˜f(Xcand) ∪Pt) −HV(Pt) ] Adding and subtracting HV( ˜f({x1),..., ˜f(xq−1)}) ∪Pt) yields ˆαqNEHVI (Xcand) = 1 N N∑ t=1 [ HV( ˜f(Xcand) ∪Pt) −HV( ˜f({x1),..., ˜f(xq−1)}) ∪Pt) + HV({˜f(x1),..., ˜f(xq−1)}∪Pt) −HV(Pt) ] . 16Applying Deﬁnition 2 again leads to (6): ˆαqNEHVI (Xcand) = 1 N N∑ t=1 HVI( ˜f(xq)|{˜f(x1),..., ˜f(xq−1)}∪Pt) + 1 N N∑ t=1 HVI ({˜f(x1),..., ˜f(xq−1)})|Pt). Note that using the method of common random numbers, the CBD formulation is mathematically equivalent to IEP formulation, but the computing qNEHVI with the CBD trick is much more efﬁcient. D Complexity Analysis D.1 Complexity of Computing qNEHVI In this section we study the complexity of computing the acquisition function. For brevity, we omit the cost of posterior sampling, which is the same for the CBD and IEP approaches. 6 The CBD approach requires recomputing box decompositions when generating each new candidate. In the worst case, each new candidate is Pareto optimal under the ﬁxed posterior samples, which leads to a time complexity of O ( N(n+ i)M) for computing the box decompositions in iteration i[59]. Note that there are O ( (n+ i)M) rectangles in each box decomposition. Given box decompositions and posterior samples at the new point, the complexity of computing the acquisition function on a single-threaded machine is O ( MN(n+ i)M) . Hence, the total time complexity for generating q candidates (ignoring potentially additional time complexity for automated gradient computations) is O ( N q∑ i=1 (n+ i)M ) + O ( NoptMN q∑ i=1 (n+ i)M ) = O ( NoptNM(n+ q)Mq ) , (9) O ( NnM) + O ( NoptMNnM q∑ i=1 2i−1 ) = O ( NoptNMnM2qq ) . (10) The second term on the left hand side in both (9) and (10) is the acquisition optimization complexity, which boils down to O(Nopt) given inﬁnite computing cores because the acquisition computation is completely parallelizable. However, as shown in Figure 2, even for relatively small values ofq, CPU cores become saturated and GPU memory limits are reached. Everything else ﬁxed, the asymptotic relative time complexity of using CBD over IEP is therefore q−M2q →∞ as q→∞. Similarly, the space complexity under the CBD formulation, O ( MN(n+ q)M) , is also polynomial in q, whereas the space complexity is exponential in qunder the IEP formulation: O ( MNnMq2q) . Everything else ﬁxed, the asymptotic relative complexity (both in terms of time and space) of using CBD over IEP is therefore qM2−q →0 as q→∞. D.2 Efﬁcient Batched Computation As noted above, using either the IEP or CBD approach, the acquisition computation given the box decompositions is highly parallelizable. However, since the number of hyperrectangles Kt in the box decompoosition can be different under each posterior sample ˜ft, stacking the box decompositions does not result in a rectangular matrix; the matrix is ragged. In order to leverage modern batched 6Sampling from p(f(Xn)|Dn) incurs a one-time cost of O(Mn3) if each of the M outcomes is modeled by an independent GP, as it involves computing a Cholesky decomposition of the n×n posterior covari- ance (at the n observed points) for each. Using low-rank updates of the Cholesky factor to sample from p(f(Xn,x0,..., xi)|Dn) has a time complexity of O(M(n+ i−1)2) for 1 ≤i ≤qsince each triangular solve has quadratic complexity. Sampling is more costly when using a multi-task GP model, as it requires a root decomposition of the Mn ×Mn posterior covariance across data points and tasks. 17tensor computational paradigms, we pad the box decompositions with empty hyperrectangles (e.g. l = 0,u = 0) such that the box decomposition under every posterior sample contains exactly K = max tKt hyperrectangles, which allows us to deﬁne a t×K dimensional matrix of box decompositions for use in batched tensor computation. In the 2-objective case, instead of padding the box decomposition, the Pareto frontier under each posterior sample can be padded instead by repeating a point on the Pareto Frontier such that the padded Pareto frontier under every posterior sample has exactly maxt|Pt|points. This enables computing the box decompositions analytically for all posterior samples in parallel using efﬁcient batched computation. The resulting box decompositions all have K = maxt|Pt|+ 1hyperrectangles (some of which may be empty). E Theoretical Results Let xprev ∈Rnd denote the stacked set of previously evaluated points in Xn: xprev := [xT 1 ,..., xT n]T. Similarly, let xcand ∈Rqd denote the stacked set of candidates in Xcand: xcand := [xT n+1,..., xT n+q]T. Let ˜ft(xprev,xcand) := [ ˜ft(x1)T,..., ˜ft(xn+q)T]T denote the tth sample of the corresponding objec- tives, which we write using the parameterization trick as ft(xprev,xcand) = µ(xprev,xcand) + L(xprev,xcand)ζt, where µ(xprev,xcand) : R(n+q)d → R(n+q)M is the multi-output GP’s posterior mean and L(xcand,xprev) ∈R(n+q)M×(n+q)M is a root decomposition (often a Cholesky decomposition) of the multi-output GP’s posterior covarianceΣ(xcand,xprev) ∈R(n+q)M×(n+q)M, and ζt ∈R(n+q)M with ζt ∼N(0,I(n+q)M).7 Proof of Theorem 1. Since the sequential NEHVI is equivalent to the qNEHVI with q = 1, we prove Theorem 1 for the general q >1 case. Recall from Section C.2, that using the method of common random numbers to ﬁx the base samples, the IEP and CBD formulations are equivalent. Therefore, we proceed only with the IEP formulation for this proof. We closely follow the proof of Theorem 2 in Daulton et al.[11]. We consider the setting from Balandat et al. [2, Section D.5]. Let f(m) t (xi,ζt) = S{i,m}(µ(xcand,xprev) + L(xcand,xprev)ζt) denote the posterior distribution over the mth outcome at xi as a random variable, where S{i,m}denotes the selection matrix (∥S{i,m}∥∞≤1 for all i= 1,...,n + qand m= 1,...,M ), to extract the element corresponding to outcome mfor the point xi. The HVI under a single posterior sample is given by A(xcand,ζt; xprev) = Kt∑ k=1 q∑ j=1 ∑ Xj∈Xj (−1)j+1 M∏ m=1 [ z(m) k,Xj (ζt) −l(m) k ] + where Xj := {Xj = {xi1 ,...xij } ⊆ Xcand : |Xj| = j,n + 1 ≤ i1 ≤ ij ≤ n + q}and z(m) k,Xj (ζt) = min [ u(m) k ,f(m)(xi1 ,ζt),...,f (m)(xij ,ζt) ] . Note that the box decomposition of the non-dominated space {S1,...,S Kt}and the number of rectangles in the box decomposition depend on ζt. Importantly, the number of hyperrectangles Kt in the decomposition is a ﬁnite and bounded by O(|Pt|⌊M 2 ⌋+1) [34, 59], where |Pt|≤ n. To satisfy the conditions of [2, Theorem 3], we need to show that there exists an integrable function ℓ: Rq×M ↦→R such that for almost every ζt and all xcand,ycand ⊆X, |A(xcand,ζt; xprev) −A(ycand,ζt; xprev)|≤ ℓ(ζt)∥xcand −ycand∥. (11) We note that xprev is ﬁxed and omit xprev for brevity, except where necessary. Let ˜ak,m,j,Xj (xcand,ζt) := [ min [ u(m) k,t ,f(m)(xi1 ,ζt),...,f (m)(xij ,ζt) ] −l(m) k,t ] + . 7Theorem 1 can be extended to handle non-iidbase samples from a family of quasi-Monte Carlo methods as in Balandat et al. [2]. 18Because of linearity, it sufﬁces to show that this condition holds for ˜A(xcand,ζt) := M∏ m=1 ˜ak,m,j,Xj (xcand,ζt) = M∏ m=1 [ min [ u(m) k,t ,f(m)(xi1 ,ζt),...,f (m)(xij ,ζt) ] −l(m) k,t ] + (12) for all k,j, and Xj. Note that we can bound ˜ak,m,j,Xj (xcand,ζt) by ˜ak,m,j,Xj (xcand,ζt) ≤ ⏐⏐⏐min [ u(m) k,t ,f(m)(xi1 ,ζt),...,f (m)(xij ,ζt) ] −l(m) k,t ⏐⏐⏐ ≤|l(m) k,t |+ ⏐⏐⏐min [ u(m) k,t ,f(m)(xi1 ,ζt),...,f (m)(xij ,ζt) ]⏐⏐⏐. (13) Consider the case where u(m) k,t = ∞. Then min[u(m) k,t ,f(xi1 ,ζt)(m),...,f (m)(xij ,ζt)] = min[f(m)(xi1 ,ζt),...,f (m)(xij ,ζt)]. Now suppose u(m) k,t <∞. Then min[u(m) k,t ,f(m)(xi1 ,ζt),...f(m)(xij ,ζt)] < ⏐⏐min[f(m)(xi1 ,ζt),...f(m)(xij ,ζt)] ⏐⏐+ ⏐⏐u(m) k,t ⏐⏐. Let w(m) k,t = { u(m) k,t , if u(m) k,t <∞ 0, otherwise. Note that l(m) k,t is ﬁnite and bounded from above and below by r(m) ≤l(m) k,t < u(m) k,t for all k,t,m , where r(m) is the mth dimension of the reference point. Hence, we can express the bound in (13) as ˜ak,m,j,Xj (xcand,ζt) ≤|l(m) k,t |+ |w(m) k,t |+ ⏐⏐min [ f(m)(xi1 ,ζt),...,f (m)(xij ,ζt) ]⏐⏐ ≤|l(m) k,t |+ |w(m) k,t |+ ∑ i1,...,ij ⏐⏐f(m)(xij ,ζt) ⏐⏐. (14) Note that we can bound ∑ i1,...,ij ⏐⏐f(m)(xij ,ζt) ⏐⏐by ∑ i1,...,ij ⏐⏐f(m)(xij ,ζt) ⏐⏐≤|Xj| ( ∥µ(m)(xcand,xprev)∥+ ∥L(m)(xcand,xprev)∥∥ζt∥ ) . Substituting this into (14) yields |˜ak,m,j,Xj (xcand,ζt)|≤| l(m) k,t |+ |w(m) k,t |+ |Xj| ( ∥µ(m)(xcand,xprev)∥+ ∥L(m)(xcand,xprev)∥∥ζt∥ ) (15) for all k,m,j,X j. Because of our assumptions of that X is compact and that the mean and covariance func- tions are continuously differentiable, µ(xcand,xprev),L(xcand,xprev),∇xcand µ(xcand,xprev), and ∇xcand L(xcand,xprev) are uniformly bounded. Hence, there exist C1,C2 <∞such that |˜ak,m,j,Xj (xcand,ζt)|≤ C1 + C2∥ζt∥ for all k,m,j,X j. Consider the M = 2 case. Omitting the indices k,t,j,X j for brevity, we have ⏐⏐˜A(xcand,ζt)−˜A(ycand,ζt) ⏐⏐ = ⏐⏐˜a1(xcand,ζt)˜a2(xcand,ζt) −˜a1(ycand,ζt)˜a2(ycand,ζt) ⏐⏐ = ⏐⏐˜a1(xcand,ζt) ( ˜a2(xcand,ζt) −˜a2(ycand,ζt) ) + ˜a2(ycand,ζt) ( ˜a1(xcand,ζt) −˜a1(ycand,ζt) )⏐⏐ ≤|˜a1(xcand,ζt)| ⏐⏐˜a2(xcand,ζt) −˜a2(ycand,ζt) ⏐⏐+ |˜a2(ycand,ζt)| ⏐⏐˜a1(xcand,ζt) −˜a1(ycand,ζt) ⏐⏐. (16) 19Using (15), we can bound |˜ak,m,j,Xj (xcand,ζt) −˜akmjXj (ycand,ζt)|by |˜ak,t,m,j,Xj (xcand,ζt) −˜ak,t,m,j,Xj (ycand,ζt)| ≤ ∑ i1,...,ij ⏐⏐S{ij,m}(µ(xcand,xprev) + L(xcand,xprev)ζt) −S{ij,m}(µ(ycand,xprev) + L(ycand,xprev)ζt) ⏐⏐ ≤|Xj| ( ∥µ(xcand,xprev) −µ(ycand,xprev)∥+ ∥L(xcand,xprev) −L(ycand,xprev)∥∥ζt∥ ) . Since µand Lhave uniformly bounded gradients with respect to xcand and ycand, they are Lipschitz. Therefore, there exist C3,C4 <∞such that |˜ak,t,m,j,Xj (xcand,ζt) −˜ak,t,m,j,Xj (ycand,ζt)|≤ (C3 + C4∥ζt∥)∥xcand −ycand∥ (17) for all xcand,ycand,k,t,m,j,X j. Substituting (17) into (16), we have ⏐⏐˜A(xcand,ζt) −˜A(ycand,ζt) ⏐⏐≤2 ( C1C3 + (C1C4 + C2C3)∥ζt∥+ C2C4∥ζt∥2 ) ∥xcand −ycand∥ The M >2 is very similar to theM = 2 case in (16) albeit with more complex expansions. Similarly, There exist C <∞such that ⏐⏐˜A(xcand,ζt) −˜A(ycand,ζt) ⏐⏐≤C M∑ m=1 ∥ζt∥m∥xcand −ycand∥ Let us deﬁne ℓ(ζt) := C∑M m=1 ∥ζt∥m. Note that ℓ(ζt) is integrable because all absolute moments exist for the Gaussian distribution. Since this satisﬁes the criteria for Theorem 3 in Balandat et al. [2], the theorem holds for qNEHVI. E.1 Unbiased Gradient estimates from the MC formulation As noted in Section 7, we can show the following (note that this result is not actually required for Theorem 1): Proposition 1. Suppose that the GP mean and covariance function are continuously differentiable. Suppose further that the candidate set Xcand has no duplicates, and that the sample-level gradients ∇xHVI ( ˜ft(x)) are obtained using the reparameterization trick as in Balandat et al. [2]. Then E [ ∇xcand ˆαN qNEHVI (xcand) ] = ∇xcand αqNEHVI (xcand), (18) that is, the averaged sample-level gradient is an unbiased estimate of the gradient of the true acquisition function. The proof of Proposition 1 closely follows the proof of Proposition 1 in Daulton et al. [11]. F Error Bound on Sequential Greedy Approximation for NEHVI If the acquisition function L(Xcand) is a normalized (meaning L(∅) = 0 ), monotone, submodu- lar (meaning that the increase in L(Xcand) is non-increasing as elements are added to Xcand set function), then the sequential greedy approximation ˆLof Lenjoys regret of no more than 1 eL∗, where L∗= maxXcand⊆XL(Xcand) is the optima of L[18]. We have αqNEHVI (Xcand) = L(Xcand) = EP [ αqEHVI (Xcand|P) ] . For a ﬁxed, known P, Daulton et al. [11] showed that αqEHVI is submodu- lar set function. In αqNEHVI , Pis a stochastic, so αqEHVI (Xcand|P) is a stochastic submodular set function. Because the expectation of a stochastic submodular function is submodular [1], αqNEHVI is also submodular. Hence, the sequential greedy approximation of αqNEHVI enjoys regret of no more than 1 eαqNEHVI ∗. Using the result from Wilson et al. [57], the MC-based approximation ˆαqNEHVI (Xcand) = ∑N t=1 HVI [ ft(Xcand)|Pt ] also enjoys the same regret bound because HVI is a normalized submodular set function.8 8Submodularity technically requires a ﬁnite search space X, whereas in BO Xis typically an inﬁnite set. Nevertheless in similar scenarios, submodularity has been extended to inﬁnite sets (e.g. Wilson et al. [57]). 20G Experiment Setup G.1 Implementation / Code used in the experiments Our implementations of qNEHVI , MESMO, PFES are available in the supplementary ﬁles and will be open-sourced under MIT license upon publication. For PESMO, we use the open-source imple- mentation in Spearmint (https://github.com/HIPS/Spearmint/tree/PESM), which is licensed by Harvard. For DGEMO, MOEA/D-EGO, and TSEMO we use the open-source implementations available at https://github.com/yunshengtian/DGEMO/tree/master under the MIT license. For TS-TCH, qEHVI , and qNParEGO we use the open-source implementations in BoTorch, which are available at https://github.com/pytorch/botorch) under the MIT license. For the ABR problem, we use the Park simulator, which is available in open-source at https: //github.com/park-project/park under the MIT license. G.2 Algorithm Details All methods are initialized with2(d+1) points from a scrambled Sobol sequence. All MC acquisition functions uses N = 128 quasi-MC samples [2]. All parallel algorithms using sequential greedy optimization for selecting a batch of candidates points and the base samples are redrawn when selecting candidate xi,i = 1,...,q . For EHVI-based methods, we leverage the two-step trick proposed by [ 59] to perform efﬁcient box decompositions; (i) we ﬁnd the set of local lower bounds for the maximization problem using Algorithm 5 from Klamroth et al. [31]9, and then (ii) using the local lower bounds as a Pareto frontier for the artiﬁcial minimization problem, we compute a box decomposition of the dominated space using Algorithm 1 from Lacour et al. [34]. qEHVI uses the IEP for computing joint EHVI over a set of candidates and computes EHVI with respect to the observed Pareto frontier. qEHVI-PM-CBD uses the Pareto frontier over the posterior means at the previously evaluated points, providing some amount of regularization with respect to the observed values. In addition, qEHVI-PM-CBD uses CBD rather than the IEP , which enables scaling to large batch sizes. qNEHVI-1 uses 500 fourier basis functions. For PFES and MESMO, we use 10 sampled (approximate) functions using RFFs (with 500 basis functions) and optimize each function using 5000 iterations of NSGA-II [12] with a population size of 50. For PFES, we partition the dominated space under each sampled Pareto frontier using the algorithm proposed Lacour et al. [34], which is more efﬁcient and yields fewer hyperrectangles than the Quick Hypervolume algorithm used by the PFES authors [51]. For qNParEGO, we use a similar pruning strategy to that in qNEHVI to only integrate over the function values of in-sample points that have positive probability of being best with respect to the sampled scalarization. We use the off-the-shelf implementation of qNParEGO in BoTorch [2], which does not use low-rank Cholesky updates; however, we do note thatqNPAREGO would likely achieve lower wall times using more efﬁcient linear algebra tricks. For DGEMO, TSEMO, and MOEA/D-EGO, we use the default settings provided in https:// github.com/yunshengtian/DGEMO/tree/master. G.3 Problem Details All benchmark problems are treated as maximization problems; the objectives for minimization problems are multiplied by -1 to obtain an equivalent maximization problem. 9More efﬁcient methods for this step exist (e.g. Dächert et al. [15]), but Klamroth et al. [31] can easily leverage vectorized operations and we ﬁnd it to be efﬁcient in our experiments. 21BraninCurrin (M = 2, d= 2) The BraninCurrin problem involves optimizing two competing functions used in BO benchmarks: Branin and Currin. The goal is minimize both: f(1)(x′ 1,x′ 2) = (x2 −5.1 4π2 x2 1 + 5 πx1 −r)2 + 10(1 − 1 8π) cos(x1) + 10 f(2)(x1,x2) = [ 1 −exp ( − 1 (2x2) )]2300x3 1 + 1900x2 1 + 2092x1 + 60 100x3 1 + 500x2 1 + 4x1 + 20 where x1,x2 ∈[0,1], x′ 1 = 15x1 −5, and x′ 2 = 15x2. DTLZ2 (M = 2,d = 6) DTLZ2 [13] is a standard problem from the multi-objective optimization literature. The two objectives are f1(x) = (1 + g(xM)) cos (π 2 x1 ) ···cos (π 2 xM−2 ) cos (π 2 xM−1 ) f2(x) = (1 + g(xM)) cos (π 2 x1 ) ···cos (π 2 xM−2 ) sin (π 2 xM−1 ) , where g(x) = ∑ xi∈xM (xi −0.5)2,x∈[0,1]d,and xM is the d−M + 1 elements of x. ZDT1 (M = 2, d= 4) ZDT1 is a benchmark problem from the multi-objective optimization literature [66]. The goal is minimize the following two objectives f(1)(x) = x1 f(2)(x) = g(x) ( 1 − √ f(1)(x) g(x) ) where g(x) = 1 + 9 d−1 ∑d i=2 xi and x= [x1,...,x d] ∈[0,1]d. VehicleSafety (M = 3, d= 5) The 3 objectives are based on a response surface model that is ﬁt to data collected from a simulator and are given by [53]: f1(x) = 1640.2823 + 2.3573285x1 + 2.3220035x2 + 4.5688768x3 + 7.7213633x4 + 4.4559504x5 f2(x) = 6.5856 + 1.15x1 −1.0427x2 + 0.9738x3 + 0.8364x4 −0.3695x1x4 + 0.0861x1x5 + 0.3628x2x4 + 0.1106x2 1 −0.3437x2 3 + 0.1764x2 4 f3(x) = −0.0551 + 0.0181x1 + 0.1024x2 + 0.0421x3 −0.0073x1x2 + 0.024x2x3 −0.0118x2x4 −0.0204x3x4 −0.008x3x5 −0.0241x2 2 + 0.0109x2 4 where x ∈ [1,3]5. We seek to (1) minimize mass (a proxy for fuel efﬁciency), (2) minimize acceleration (a proxy for passenger trauma) in a full-frontal collision, and (3) minimize the distance that the toe-board intrudes into the cabin (a proxy for vehicle fragility) [53]. AutoML (M = 2, d= 8) . This experiment considers optimizing predictive performance and latency of a deep neural networks (DNN). Practitioners and researchers across many domains use DNNs for recommendation and recognition tasks in low-latency (e.g. on-device) environments [49], where any increase in prediction time degrades the product experience [43]. Simultaneously, researchers are considering increasingly larger architectures that improve predictive performance [48]. Therefore, a ﬁrm may be interesting understanding the set of optimal trade-offs between prediction latency and predictive performance. For a demonstration, we consider optimizing (d= 8) hyperparameters of DNN (detailed in Table 1) to minimize out-of-sample prediction error and minimize latency on the MNIST data set [ 35]. Using a small randomized test set leads to noisy evaluations of predictive performance and latency measurements are often noisy due to unrelated ﬂuctuations in the testing environment. As in previous works, we minimize a logit transformation of the prediction error and minimize a logarithm of the ratio between the latency of a proposed DNN and the latency of the fastest DNN [20, 21, 25]. For each evaluation, we randomly partition the 60,000 examples from the MNIST training set into a set of 50,000 examples for training and 10,000 examples for evaluation. We train each network for 8 epochs using SGD with momentum with mini-batches of 512 examples. The learning rate is decayed after every 30 mini-batch updates using the speciﬁed 22PARAMETER SEARCH SPACE LEARNING RATE (log10 SCALE ) [-5.0, -1.0] LEARNING RATE DECAY MULTIPLIER [0.01, 1.0] DROPOUT RATE [0.0, 0.7] L1 REGULARIZATION [10−5, 0.1] L2 REGULARIZATION [10−5, 0.1] HIDDEN LAYER 1 SIZE [20, 500] HIDDEN LAYER 2 SIZE [20, 500] HIDDEN LAYER 3 SIZE [20, 500] Table 1: The search space for the AutoML benchmark. decay multiplier. We use randomized rounding on the integer parameters before evaluation. For evaluating the performance of different BO methods, we estimate the noiseless objectives using the mean objectives across 3 replications. DNNs are implemented in PyTorch using ReLU activations and a softmax output layer. Latency measurements are taken on a CPU (2x Intel Xeon E5-2680 v4 @ 2.40GHz). CarSideImpact ( M = 4, d= 7) A side-impact test is common practice under European Enhanced Vehicle-Safety Committee to uphold vehicle safety standards [14]. In constrast with the previous VehicleSafety problem where we considered a full-frontal collision, we now consider the problem of tuning parameters controlling the structural design the of an automobile in the case of a side-impact collision. This problem has been widely used in various works and has previously used stochastic parameters to account for manufacturing error [14]. We use the recent 4-objective version proposed by Tanabe & Ishibuchi[53] where the goal to minimize the weight of the vehicle, passenger trauma (pubic force), and vehicle damage (the average velocity of the V-pillar). The fourth objective is a combination of 10 other measures of the vehicle durability and passenger safety (see [14] for details). The mathematical formulas for a response surface model ﬁt to data collected from a simulator are given below: f(1)(x) = 1.98 + 4.9x1 + 6.67x2 + 6.98x3 + 4.01x4 + 1.78x5 + 10−5x6 + 2.73x7 f(2)(x) = 4.72 −0.5x4 −0.19x2x3 f(3)(x) = 0.5(VMBP(x) + VFD(x)) f(4)(x) = − 10∑ i=1 max[gi(x),0] where g1(x) = 1 −1.16 + 0.3717x2x4 + 0.0092928x3 g2(x) = 0.32 −0.261 + 0.0159x1x2 + 0.06486x1 + 0.019x2x7 −0.0144x3x5 −0.0154464x6 g3(x) = 0.32 −0.214 −0.00817x5 + 0.045195x1 + 0.0135168x1 −0.03099x2x6 + 0.018x2x7 −0.007176x3 −0.023232x3 + 0.00364x5x6 + 0.018x2 2 g4(x) = 0.32 −0.74 + 0.61x2 + 0.031296x3 + 0.031872x7 −0.227x2 2 g5(x) = 32 −28.98 −3.818x3 + 4.2x1x2 −1.27296x6 + 2.68065x7 g6(x) = 32 −33.86 −2.95x3 + 5.057x1x2 + 3.795x2 + 3.4431x7 −1.45728 g7(x) = 32 −46.36 + 9.9x2 + 4.4505x1 g8(x) = 4 −f2(x) g9(x) = 9.9 −VMBP(x) g10(x) = 15.7 −VFD(x) VMBP(x) = 10.58 −0.674x1x2 −0.67275x2 VFD(x) = 16.45 −0.489x3x7 −0.843x5x6 23. The search space is: x1 ∈[0.5,1.5] x2 ∈[0.45,1.35] x3,x4 ∈[0.5,1.5] x5 ∈[0.875,2.625] x6,x7 ∈[0.4,1.2]. As in the VehicleSafety problem, we add zero-mean Gaussian noise to each objective with a standard deviation of 1% the range of each objective. Constrained BraninCurrin (M = 2, V= 2, d= 2) The constrained BraninCurrin problem uses the same objectives as BraninCurrin, but adds the following disk constraint from [22]: c(x′ 1,x′ 2) = 50 −(x′ 1 −2.5)2 −(x′ 2 −7.5)2) ≥0 We add zero-mean Gaussian noise to objectives and the constraint slack observations with a standard deviation of 5% of the range of each outcome. SphereEllipsoidal (M = 2, d= 5) The SphereEllipsoidal problem is deﬁned over x∈[−5,5]d and the objectives are given by [6]: f(1)(x) = d∑ i=1 (xi −x(1) opt,i)2 + f(1) opt f(2)(x) = d∑ i=1 106 i−1 d−1 z2 i + f(2) opt where zi = Tosz(δi) δi = xi −x(2) opt,i Tosz(δi) = sign(δi)e ˆδi+0.049 ( sin [ c1(δi) ˆδi ] +sin [ c2(δi) ˆδi ]) and ˆδi = {log(|δi|), if δi ̸= 0 0, otherwise c1(δi) = {10, if δi ≥0 5.5, otherwise c2(δi) = {7.9, if δi ≥0 3.1, otherwise. We set x(1) opt = [−0.0299,2.1458,−3.2922,−2.9438,−1.5406] x(2) opt = [2.0611,−1.7655,−0.7754,1.8775,−3.7657] f(1) opt = 203.71 f(2) opt = 135.6 . We add zero-mean Gaussian noise to objectives and the constraint slack observations with a standard deviation of 5% of the range of each outcome. 24PROBLEM REFERENCE POINT BRANIN CURRIN [-18.00, -6.00] ZDT1 [-1.10, -1.10] DTLZ2 [−1.10]M VEHICLE SAFETY [-1698.55, -11.21, -0.29] ABR [150.00, -3500.00] AUTO ML [-2.45, 0.60] CARSIDE IMPACT [-45.49, -4.51, -13.34, -10.39] CONSTRAINED BRANIN CURRIN [-80.00, -12.00] SPHERE ELLIPSOIDAL [-261.00, −6.77 ·106] Table 2: The reference points for each benchmark problem. G.4 Evaluation Details To compute the log hypervolume difference metric, we use NSGA-II to estimate the true Pareto frontier (except for the ABR and AutoML problems, where evaluations are time-consuming and we instead take the true Pareto frontier to be the Pareto frontier across the estimated objectives across all methods and replications). Using this Pareto frontier, we compute the hypervolume dominated by the true Pareto frontier in order to calculate the log hypervolume difference. For ZDT1, the hypervolume dominated by the true Pareto frontier can be computed analytically. For Constrained BraninCurrin, we evaluate the logarithm of the difference between the hypervolume dominated by the true feasible Pareto frontier and the feasible in-sample Pareto frontier for each method. For all problems, we selected the reference point based on the component-wise noiseless nadir point fnadir(x) = min x∈Xf(x) and the range of the Pareto frontier for each noiseless objective using the common heuristic [56]: r= fnadir(x) −β∗(fideal(x) −fnadir(x)),where β = 0.1 and fideal(x) = maxx∈Xf(x). H Experiments H.1 Wall Time Results Tables 3 and 4 report the acquisition optimization wall times for each method. On all benchmark problems except CarSideImpact, qNEHVI is faster to optimize than MESMO and PFES on a GPU. The wall times for optimizingqNEHVI are competitive with those forqNParEGO on most benchmark problems and batch sizes; on many problems, qNEHVI is often faster than qNParEGO. On the problems VehicleSafety and CarSideImpact problems which have 3 and 4 objectives respectively, we observed tractable wall times, even when generating q= 32 candidates. The wall time for 3 and 4 objective problems is larger primarily because the box decompositions are more time-consuming to compute and result in more hyperrectangles as the number of objectives increases. Although, qEHVI (-PM) is faster for q = 1 and q = 8 on many problems, it is unable to scale to large batch sizes and ran out of memory for q = 8 on CarSideImpact due to the box decomposition having a large number of hyperrectangles. 25CPU BRANINCURRIN ZDT1 ABR V EHICLE SAFETY MESMO (q=1) 21.24 (±0.02) 19 .76 (±0.03) 23 .24 (±0.04) 28 .39 (±0.07) PFES (q=1) 22.86 (±0.05) 39 .82 (±0.14) 43 .03 (±0.12) 53 .16 (±0.17) TS-TCH (q=1) 0.51 (±0.0) 0 .48 (±0.0) 0 .75 (±0.0) 0 .67 (±0.0) qEHVI-PM-CBD ( q=1) 2.34 (±0.02) 3 .7 (±0.02) 3 .56 (±0.03) 7 .82 (±0.05) qEHVI (q=1) 0.58 (±0.0) 0 .66 (±0.01) 2 .98 (±0.02) 5 .07 (±0.03) qNEHVI (q=1) 40.55 (±0.61) 35 .66 (±0.47) 62 .29 (±0.97) 120 .43 (±1.25) qNPAREGO (q=1) 3.19 (±0.05) 1 .65 (±0.02) 6 .94 (±0.06) 1 .05 (±0.01) qPAREGO (q=1) 0.58 (±0.01) 0 .7 (±0.01) 2 .5 (±0.03) 0 .75 (±0.01) DGEMO (q=1) 65.28(±0.26) 76 .99(±0.35) NA NA DGEMO (q=8) 65.44(±0.63) 86 .97(±0.85) NA NA DGEMO (q=16) 66.44(±0.93) 86 .06(±1.21) NA NA DGEMO (q=32) 66.66(±1.47) 84 .66(±1.66) NA NA TSEMO (q=1) 3.02(±0.01) 2 .98(±0.01) NA 3.61(±0.01) TSEMO (q=8) 3.53(±0.01) 3 .48(±0.01) NA 7.45(±0.1) TSEMO (q=16) 3.77(±0.02) 3 .74(±0.02) NA 11.06(±0.28) TSEMO (q=32) 4.29(±0.03) 4 .22(±0.02) NA 16.3(±0.68) MOEA/D-EGO (q=1) 57.79(±0.17) 58 .1(±0.17) NA 71.0(±0.21) MOEA/D-EGO (q=8) 63.56(±0.18) 63 .57(±0.17) NA 77.56(±0.22) MOEA/D-EGO (q=16) 64.0(±0.18) 63 .99(±0.19) NA 78.03(±0.25) MOEA/D-EGO (q=32) 64.09(±0.26) 63 .9(±0.24) NA 77.77(±0.35) GPU BRANINCURRIN ZDT1 ABR VEHICLE SAFETY MESMO (q=1) 19.9 (±0.04) 19 .92 (±0.04) 21 .54 (±0.08) 24 .57 (±0.09) PFES (q=1) 21.68 (±0.07) 45 .9 (±0.17) 43 .3 (±0.13) 47 .25 (±0.16) TS-TCH (q=1) 0.88 (±0.01) 0 .94 (±0.01) 1 .08 (±0.01) 1 .04 (±0.01) TS-TCH (q=8) 1.85 (±0.03) 2 .01 (±0.03) 2 .99 (±0.04) 2 .32 (±0.05) TS-TCH (q=16) 3.08 (±0.08) 3 .29 (±0.1) 4 .28 (±0.08) 3 .54 (±0.09) TS-TCH (q=32) 5.25 (±0.15) 5 .41 (±0.16) 7 .23 (±0.2) 6 .41 (±0.23) qEHVI-PM-CBD ( q=1) 2.17(±0.01) 2 .12(±0.02) 3 .59(±0.02) 51 .11(±0.28) qEHVI-PM-CBD ( q=8) 39.56(±0.79) 30 .83(±1.35) 36 .2(±0.73) 716 .03(±13.44) qEHVI-PM-CBD ( q=16) 82.91(±2.42) 67 .3(±3.88) 70 .02(±1.64) 1410 .79(±41.72) qEHVI-PM-CBD ( q=32) 147.81(±6.85) 105 .74(±8.55) 251 .97(±12.69) 2570 .95(±116.61) qEHVI (q=1) 0.72 (±0.01) 0 .99 (±0.02) 3 .67 (±0.02) 3 .96 (±0.05) qEHVI (q=8) 18.12 (±1.03) 18 .05 (±0.86) 40 .55 (±0.58) 71 .49 (±2.04) qNEHVI (q=1) 6.15 (±0.06) 5 .75 (±0.04) 7 .72 (±0.09) 20 .81 (±0.11) qNEHVI (q=8) 48.19 (±1.2) 46 .74 (±0.83) 49 .7 (±0.79) 168 .63 (±2.49) qNEHVI (q=16) 102.87 (±4.02) 95 .6 (±2.62) 93 .14 (±1.72) 289 .02 (±5.82) qNEHVI (q=32) 177.56 (±7.81) 190 .59 (±6.07) 181 .97 (±4.77) 546 .83 (±16.09) qNEHVI-1 (q=1) 0.32(±0.0) 0 .24(±0.0) 0 .56(±0.0) 0 .92(±0.0) qNEHVI-1 (q=8) 2.43(±0.02) 2 .11(±0.03) 4 .55(±0.06) 7 .1(±0.1) qNEHVI-1 (q=16) 4.97(±0.07) 3 .73(±0.06) 8 .73(±0.14) 14 .77(±0.31) qNEHVI-1 (q=32) 9.03(±0.18) 8 .18(±0.24) 17 .15(±0.41) 34 .99(±1.46) qNPAREGO (q=1) 2.39 (±0.04) 1 .84 (±0.04) 6 .47 (±0.05) 0 .9 (±0.02) qNPAREGO (q=8) 47.05 (±1.74) 52 .99 (±1.94) 74 .72 (±1.9) 45 .56 (±1.17) qNPAREGO (q=16) 118.73 (±5.53) 116 .68 (±5.51) 116 .79 (±3.19) 91 .3 (±3.83) qNPAREGO (q=32) 306.17 (±17.81) 279 .01 (±17.72) 240 .56 (±6.44) 188 .42 (±13.42) qPAREGO (q=1) 0.81 (±0.02) 1 .05 (±0.03) 4 .39 (±0.05) 0 .79 (±0.02) qPAREGO (q=8) 13.01 (±0.53) 16 .4 (±0.72) 31 .02 (±0.81) 12 .64 (±0.84) qPAREGO (q=16) 34.34 (±2.12) 43 .66 (±3.12) 66 .85 (±3.13) 36 .68 (±4.48) qPAREGO (q=32) 139.73 (±25.22) 108 .25 (±6.94) 122 .37 (±6.12) 107 .34 (±14.76) Table 3: Acquisition function optimization wall time (including box decompositions) in seconds on a CPU (2x Intel Xeon E5-2680 v4 @ 2.40GHz) and a Tesla V100 SXM2 GPU (16GB RAM). The mean and two standard errors are reported. DGEMO, TSEMO, and MOEA/D-EGO are omitted for ABR because they have package requirements that are not easily compatible with our distributed evaluation pipeline and ABR evaluations are prohibitively slow without distributed evaluations. DGEMO is omitted for VehicleSafety because the open-source implementation raises an consistently raises an exception in the graph cutting algorithm with this problem. 26H.2 Scaling to large batch sizes with CBD In Figure 6, we provide results demonstrating the CBD approach enables scaling to large batch sizes, even with 3 or 4 objectives, whereas the IEP wall times grow exponentially with the batch size and the IEP overﬂows GPU memory even with modest batch sizes. 0 25 50 75 100 q 0 200 400 600 800Acquisition Optimization Time (s) 3 Objectives CBD (CPU) CBD (GPU) IEP (CPU) IEP (GPU) OOM 0 25 50 75 100 q 0 500 1000 1500 2000 4 Objectives Figure 6: Acquisition optimization wall time under a sequential greedy approximation using L- BFGS-B for three and four objectives. CBD enables scaling to much larger batch sizes q than using the inclusion-exclusion principle (IEP) and avoids running out-of-memory (OOM) on a GPU. Independent GPs are used for each outcome and are initialized with 20 points from the Pareto frontier of the 6-dimensional DTLZ2 problem [13] with 3 objectives (left) and 4 objectives (right). Wall times were measured on a Tesla V100 SXM2 GPU (16GB GPU RAM) and a Intel Xeon Gold 6138 CPU @ 2GHz CPU (251GB RAM). H.3 Additional Empirical Results The additional optimization performance results in the appendix demonstrate that qNEHVI -based algorithms are consistently the top performer. The only case where qNEHVI (-1) is outperformed is in the sequential setting on the CarSideImpact problem in Figure 9(a), where qEHVI performs best However, as show in Figure 9(b) and Figure 9(c), qNEHVI enables scaling to large batch sizes, whereas qEHVI runs out of memory on a GPU for q = 8. Therefore, in a practical setting where vehicles are manufactured and test in parallel, qNEHVI would be the best choice. Figure 11 shows that qNEHVI achieves solid performance anytime throughout the learning curve in the sequential setting, and Figure 13 shows that qNEHVI -based variants consistently achieves the best performance for variousqwith a ﬁxed budget of 224 function evaluations. Although,qNEHVI-1 does not consistently perform better than qNEHVI , qNEHVI-1 achieves very little degradation of sample complexity as the batch size qincreases. H.4 Better performance from qEHVI with a larger batch size Interestingly, on many test problems qEHVI performs better with q = 8 than q = 1 . In the case of BraninCurrin and ConstrainedBraninCurrin, qParEGO also improves as qincreases. Since this phenomenon is not observed with the noisy acquisition functions (qNEHVI , qNParEGO), we hypothesize that it may be the case that sequential data collection results in a difﬁcult to optimize acquisition surface and that integrating over the in-sample points leads to a smoother acquisition surface that results in improved sequential optimization. The acquisition functions that do not account for noise may be misled by the noise in sequential setting, but using a larger batch size (within limits) may help avoid the issue of not properly accounting for noise. 27H.5 Performance over Higher Dimensional Spaces qNEHVI-1 relies on approximate GP sample paths (RFFs). Although we ﬁnd that qNEHVI-1 performs very well on many low-dimensional problems (see Figure 4), Figures 8 and 7 show that qNEHVI-1 does not perform as well a qNEHVI on higher dimensional problems. We hypothesize that the RFF approximation degrades in higher dimensional search spaces leading to poor optimization performance relative qNEHVI , which uses exact GP samples. It is likely that using 500 Fourier basis functions leads to large approximation error on high dimensional search spaces. Further study is needed to examine whether robust performance can be achieved by increasing the number of basis functions. As shown in Figure 7, qNEHVI consistently outperforms all tested methods regardless of the dimension of the design space. 0 50 100 150 200 Function Evaluations 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40Hypervolume d=5 qNEHVI qNParEGO TS-TCH qNEHVI-1 0 50 100 150 200 Function Evaluations d=10 0 50 100 150 200 Function Evaluations d=15 0 50 100 150 200 Function Evaluations d=20 Figure 7: Sequential optimization performance 2-objective DTLZ2 with σ = 5% problems as the dimension of the search space increases from d= 5 to d= 20. 1 8 16 32 q -1.25 -1.20 -1.15 -1.10 -1.05 -1.00 -0.95 -0.90 -0.85Log Hypervolume Difference d=5 1 8 16 32 q -0.75 -0.72 -0.70 -0.68 -0.65 -0.62 -0.60 -0.57 -0.55 d=10 1 8 16 32 q -0.54 -0.52 -0.50 -0.48 -0.46 -0.44 -0.42 -0.40 -0.38 d=15 1 8 16 32 q -0.42 -0.41 -0.40 -0.39 -0.38 -0.37 d=20 qNEHVI qNEHVI-1 Figure 8: A comparison of the ﬁnal optimization performance of qNEHVI-1 , a single sample path approximation of qNEHVI , and qNEHVI on 2-objective DTLZ2 problems with input dimensions between 5 and 20 under different batch sizes q. qNEHVI-1 is very effective on lower dimensional problems, but does not perform as well as qNEHVI on higher dimensional problems. 28CPU DTLZ2 A UTOML C ARSIDEIMPACT CONSTRAINEDBRANINCURRIN MESMO (q=1) 27.79(±0.07) 37 .86 (±0.08) 33 .31 (±0.06) NA PFES (q=1) 69.85(±0.21) 101 .24 (±0.29) 102 .55 (±0.34) NA TS-TCH (q=1) 0.72(±0.0) 0 .93 (±0.0) 1 .27 (±0.01) NA qEHVI-PM-CBD (q=1) 3.98(±0.02) 5 .76 (±0.05) 83 .14 (±0.74) 10 .27 (±0.06) qEHVI (q=1) 2.74(±0.01) 5 .05 (±0.04) 96 .19 (±0.9) 3 .26 (±0.05) qNEHVI (q=1) 54.04(±0.67) 22 .71 (±0.48) 541 .13 (±6.83) 267 .67 (±4.09) qNPAREGO (q=1) 21.39(±0.2) 5 .6 (±0.07) 3 .38 (±0.05) 12 .05 (±0.17) qPAREGO (q=1) 2.33(±0.02) 3 .06 (±0.03) 1 .91 (±0.02) 1.56 (±0.03) DGEMO (q=1) 84.48(±0.64) NA NA NA DGEMO (q=8) 65.99(±0.51) NA NA NA DGEMO (q=16) 69.57(±0.59) NA NA NA DGEMO (q=32) 72.82(±0.8) NA NA NA TSEMO (q=1) 3.1(±0.01) NA 14.91(±0.16) NA TSEMO (q=8) 3.58(±0.01) NA 100.78(±3.75) NA TSEMO (q=16) 3.87(±0.02) NA 188.44(±9.88) NA TSEMO (q=32) 4.55(±0.03) NA 326.51(±24.03) NA MOEA/D-EGO (q=1) 59.3(±0.18) NA 79.87(±0.16) NA MOEA/D-EGO (q=8) 65.5(±0.18) NA 85.34(±0.21) NA MOEA/D-EGO (q=16) 65.81(±0.2) NA 85.18(±0.3) NA MOEA/D-EGO (q=32) 65.44(±0.3) NA 84.8(±0.39) NA GPU DTLZ2 A UTOML C ARSIDEIMPACT CONSTRAINEDBRANINCURRIN MESMO (q=1) 17.01(±0.13) 28 .83 (±0.2) 26 .0 (±0.06) NA PFES (q=1) 46.63(±0.51) 85 .73 (±0.4) 55 .41 (±0.16) NA TS-TCH (q=1) 0.84(±0.01) 1 .49 (±0.01) 2 .17 (±0.01) NA TS-TCH (q=8) 3.71(±0.07) 3 .64 (±0.1) 4 .15 (±0.07) NA TS-TCH (q=16) 5.85(±0.14) 6 .16 (±0.21) 6 .9 (±0.19) NA TS-TCH (q=32) 9.33(±0.3) 9 .47 (±0.47) 10 .2 (±0.4) NA qEHVI-PM-CBD (q=1) 4.29(±0.01) 5 .26(±0.05) 52 .7(±0.41) 15 .89(±0.08) qEHVI-PM-CBD (q=8) 40.85(±0.41) 39 .55(±0.8) 460 .18(±9.76) 135 .89(±1.43) qEHVI-PM-CBD (q=16) 93.39(±1.87) 71 .8(±1.81) 866 .12(±26.46) 314 .42(±5.44) qEHVI-PM-CBD (q=32) 194.12(±5.6) 143 .83(±4.88) 1682 .28(±72.5) 823 .01(±24.48) qEHVI (q=1) 2.93(±0.02) 4 .67 (±0.1) 9 .63 (±0.05) 5 .69 (±0.11) qEHVI (q=8) 39.64(±0.57) 104 .48 (±1.34) OOM 68.95 (±2.57) qNEHVI (q=1) 4.91(±0.01) 7 .95 (±0.1) 82 .66 (±0.63) 20 .47 (±0.12) qNEHVI (q=8) 39.96(±0.35) 67 .28 (±1.87) 683 .06 (±13.82) 168 .04 (±1.85) qNEHVI (q=16) 74.41(±0.63) 145 .66 (±4.45) 1289 .4 (±36.81) 362 .15 (±9.08) qNEHVI (q=32) 142.18(±1.59) 247.92 (±11.93) 2480.41 (±102.38) 654 .66 (±23.48) qNEHVI-1 (q=1) 0.42(±0.0) 0 .53(±0.0) 2 .11(±0.01) 1 .57(±0.02) qNEHVI-1 (q=8) 3.26(±0.03) 4 .55(±0.09) 16 .88(±0.2) 11 .01(±0.16) qNEHVI-1 (q=16) 6.34(±0.04) 7 .22(±0.19) 34 .78(±0.63) 20 .56(±0.4) qNEHVI-1 (q=32) 14.85(±0.45) 12 .66(±0.54) 67 .27(±1.61) 40 .6(±1.1) qNPAREGO (q=1) 6.41(±0.03) 4 .86 (±0.1) 3 .25 (±0.06) 6 .17 (±0.07) qNPAREGO (q=8) 57.17(±0.43) 48 .62 (±1.18) 30 .65 (±0.92) 38 .66 (±0.95) qNPAREGO (q=16) 114.91(±1.3) 122 .09 (±3.5) 81 .04 (±3.44) 84 .27 (±3.04) qNPAREGO (q=32) 263.56(±4.03) 275 .41 (±8.29) 219 .98 (±9.75) 199 .6 (±9.68) qPAREGO (q=1) 2.62(±0.03) 3 .31 (±0.07) 3 .03 (±0.06) 2 .83 (±0.08) qPAREGO (q=8) 25.75(±0.66) 33 .84 (±1.76) 22 .0 (±0.88) 20 .28 (±1.18) qPAREGO (q=16) 67.89(±2.27) 77 .25 (±3.9) 56 .09 (±3.09) 50 .06 (±3.84) qPAREGO (q=32) 159.98(±7.41) 217.55 (±19.15) 139 .25 (±9.41) 135 .39 (±13.04) Table 4: Acquisition function optimization wall time (including box decompositions) in seconds on a CPU (2x Intel Xeon E5-2680 v4 @ 2.40GHz) and a Tesla V100 SXM2 GPU (16GB RAM). The mean and two standard errors are reported. DGEMO, TSEMO, MOEA/D-EGO are omitted for AutoML because they have package requirements that are not easily compatible with our distributed training and evaluation pipeline, and they are omitted for ConstrainedBraninCurrin because they do not support constraints in the open-source implementation at https://github.com/yunshengtian/DGEMO/ tree/master. DGEMO is omitted on CarSideImpact because the open-source implementation does not support more than 3 objectives. 290 50 100 150 200 Function Evaluations -0.20 -0.10 0.00 0.10 0.20 Log Hypervolume Difference AutoML 0 50 100 150 200 Function Evaluations 1.20 1.40 1.60 1.80 2.00 2.20 2.40 CarSideImpact 0 50 100 150 200 Function Evaluations 1.00 1.20 1.40 1.60 1.80 2.00 2.20 2.40 ConstrainedBraninCurrin 0 50 100 150 200 Function Evaluations -2.00 -1.50 -1.00 -0.50 0.00 ZDT1 DGEMO MESMO MOEA/D-EGO PESMO PFES TS-TCH TSEMO qEHVI qEHVI-PM-CBD qNEHVI qNEHVI-1 qNParEGO (a) Sequential Optimization performance on additional benchmark problems. 0 50 100 150 200 Batch Iteration 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75Log Hypervolume Difference BraninCurrin 0 50 100 150 200 Batch Iteration -1.10 -1.00 -0.90 -0.80 -0.70 -0.60 -0.50 DTLZ2 0 50 100 150 200 Batch Iteration -0.50 0.00 0.50 1.00 VehicleSafety 0 50 100 150 200 Batch Iteration 4.80 5.00 5.20 5.40 5.60 5.80 ABR DGEMO (q=1) DGEMO (q=16) DGEMO (q=32) DGEMO (q=8) MOEA/D-EGO (q=1) MOEA/D-EGO (q=16) MOEA/D-EGO (q=32) MOEA/D-EGO (q=8) TS-TCH (q=1) TS-TCH (q=16) TS-TCH (q=32) TS-TCH (q=8) TSEMO (q=1) TSEMO (q=16) TSEMO (q=32) TSEMO (q=8) qNEHVI (q=1) qNEHVI (q=16) qNEHVI (q=32) qNEHVI (q=8) qNParEGO (q=1) qNParEGO (q=16) qNParEGO (q=32) qNParEGO (q=8) (b) Parallel optimization performance vs batch iterations (1/2). 0 50 100 150 200 Batch Iteration -0.20 -0.10 0.00 0.10 0.20 Log Hypervolume Difference AutoML 0 50 100 150 200 Batch Iteration 1.40 1.60 1.80 2.00 2.20 2.40 CarSideImpact 0 50 100 150 200 Batch Iteration 1.00 1.20 1.40 1.60 1.80 2.00 2.20 2.40 ConstrainedBraninCurrin 0 50 100 150 200 Batch Iteration -2.00 -1.75 -1.50 -1.25 -1.00 -0.75 -0.50 -0.25 0.00 ZDT1 DGEMO (q=1) DGEMO (q=16) DGEMO (q=32) DGEMO (q=8) MOEA/D-EGO (q=1) MOEA/D-EGO (q=16) MOEA/D-EGO (q=32) MOEA/D-EGO (q=8) TS-TCH (q=1) TS-TCH (q=16) TS-TCH (q=32) TS-TCH (q=8) TSEMO (q=1) TSEMO (q=16) TSEMO (q=32) TSEMO (q=8) qNEHVI (q=1) qNEHVI (q=16) qNEHVI (q=32) qNEHVI (q=8) qNParEGO (q=1) qNParEGO (q=16) qNParEGO (q=32) qNParEGO (q=8) (c) Parallel optimization performance vs batch iterations (2/2). Figure 9: Optimization performance on additional problems. (a) Sequential optimization performance. (b) and (c) Optimization performance of batch acquisition functions using various qover the number of BO iterations. To improve readability, we omitqEHVI (-PM) in this ﬁgure because the IEP cannot scale beyond q= 8 because of the exponential time and space complexity (running it on a GPU runs out of memory and running it on a CPU results in prohibitively slow wall times). See Figure 10 for results using qEHVI(-PM). 300 50 100 150 200 Batch Iteration -0.25 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 Log Hypervolume Difference BraninCurrin 0 50 100 150 200 Batch Iteration -1.10 -1.00 -0.90 -0.80 -0.70 -0.60 -0.50 DTLZ2 0 50 100 150 200 Batch Iteration -1.00 -0.50 0.00 0.50 1.00 VehicleSafety 0 50 100 150 200 Batch Iteration 4.80 5.00 5.20 5.40 5.60 5.80 ABR qEHVI-PM-CBD (q=1) qEHVI-PM-CBD (q=8) qNEHVI (q=1) qNEHVI (q=16) qNEHVI (q=32) qNEHVI (q=8) qNEHVI-1 (q=1) qNEHVI-1 (q=16) qNEHVI-1 (q=32) qNEHVI-1 (q=8) (a) 0 50 100 150 200 Batch Iteration -0.20 -0.10 0.00 0.10 0.20 Log Hypervolume Difference AutoML 0 50 100 150 200 Batch Iteration 1.40 1.60 1.80 2.00 2.20 2.40 CarSideImpact 0 50 100 150 200 Batch Iteration 1.00 1.20 1.40 1.60 1.80 2.00 2.20 2.40 ConstrainedBraninCurrin 0 50 100 150 200 Batch Iteration -2.00 -1.50 -1.00 -0.50 0.00 ZDT1 qEHVI-PM-CBD (q=1) qEHVI-PM-CBD (q=16) qEHVI-PM-CBD (q=32) qEHVI-PM-CBD (q=8) qNEHVI (q=1) qNEHVI (q=16) qNEHVI (q=32) qNEHVI (q=8) qNEHVI-1 (q=1) qNEHVI-1 (q=16) qNEHVI-1 (q=32) qNEHVI-1 (q=8) (b) Figure 10: Optimization performance of qNEHVI under various batch sizes qvs qEHVI (-PM ). Note that using the IEP , qEHVI (-PM ) cannot scale beyond q= 8 because of the exponential time and space complexity (running it on a GPU runs out of memory and running it on a CPU results in prohibitively slow wall times). 310 50 100 150 200 Function Evaluations 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75Log Hypervolume Difference BraninCurrin 0 50 100 150 200 Function Evaluations -1.10 -1.00 -0.90 -0.80 -0.70 -0.60 -0.50 DTLZ2 0 50 100 150 200 Function Evaluations -0.50 0.00 0.50 1.00 VehicleSafety 0 50 100 150 200 Function Evaluations 4.80 5.00 5.20 5.40 5.60 5.80 ABR DGEMO (q=1) DGEMO (q=16) DGEMO (q=32) DGEMO (q=8) MOEA/D-EGO (q=1) MOEA/D-EGO (q=16) MOEA/D-EGO (q=32) MOEA/D-EGO (q=8) TS-TCH (q=1) TS-TCH (q=16) TS-TCH (q=32) TS-TCH (q=8) TSEMO (q=1) TSEMO (q=16) TSEMO (q=32) TSEMO (q=8) qNEHVI (q=1) qNEHVI (q=16) qNEHVI (q=32) qNEHVI (q=8) qNParEGO (q=1) qNParEGO (q=16) qNParEGO (q=32) qNParEGO (q=8) (a) 0 50 100 150 200 Function Evaluations -0.20 -0.10 0.00 0.10 0.20 Log Hypervolume Difference AutoML 0 50 100 150 200 Function Evaluations 1.40 1.60 1.80 2.00 2.20 2.40 CarSideImpact 0 50 100 150 200 Function Evaluations 1.00 1.20 1.40 1.60 1.80 2.00 2.20 2.40 ConstrainedBraninCurrin 0 50 100 150 200 Function Evaluations -2.00 -1.75 -1.50 -1.25 -1.00 -0.75 -0.50 -0.25 0.00 ZDT1 DGEMO (q=1) DGEMO (q=16) DGEMO (q=32) DGEMO (q=8) MOEA/D-EGO (q=1) MOEA/D-EGO (q=16) MOEA/D-EGO (q=32) MOEA/D-EGO (q=8) TS-TCH (q=1) TS-TCH (q=16) TS-TCH (q=32) TS-TCH (q=8) TSEMO (q=1) TSEMO (q=16) TSEMO (q=32) TSEMO (q=8) qNEHVI (q=1) qNEHVI (q=16) qNEHVI (q=32) qNEHVI (q=8) qNParEGO (q=1) qNParEGO (q=16) qNParEGO (q=32) qNParEGO (q=8) (b) Figure 11: Anytime optimization performance of batch acquisition functions using various qover the number of function evaluations. To improve readability, we omit qEHVI (-PM) in this ﬁgure because the IEP cannot scale beyond q= 8 because of the exponential time and space complexity (running it on a GPU runs out of memory and running it on a CPU results in prohibitively slow wall times). 320 50 100 150 200 Function Evaluations -0.25 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 Log Hypervolume Difference BraninCurrin 0 50 100 150 200 Function Evaluations -1.10 -1.00 -0.90 -0.80 -0.70 -0.60 -0.50 DTLZ2 0 50 100 150 200 Function Evaluations -1.00 -0.50 0.00 0.50 1.00 VehicleSafety 0 50 100 150 200 Function Evaluations 4.80 5.00 5.20 5.40 5.60 5.80 ABR qEHVI-PM-CBD (q=1) qEHVI-PM-CBD (q=8) qNEHVI (q=1) qNEHVI (q=16) qNEHVI (q=32) qNEHVI (q=8) qNEHVI-1 (q=1) qNEHVI-1 (q=16) qNEHVI-1 (q=32) qNEHVI-1 (q=8) (a) 0 50 100 150 200 Batch Iteration -0.20 -0.10 0.00 0.10 0.20 Log Hypervolume Difference AutoML 0 50 100 150 200 Batch Iteration 1.40 1.60 1.80 2.00 2.20 2.40 CarSideImpact 0 50 100 150 200 Batch Iteration 1.00 1.20 1.40 1.60 1.80 2.00 2.20 2.40 ConstrainedBraninCurrin 0 50 100 150 200 Batch Iteration -2.00 -1.50 -1.00 -0.50 0.00 ZDT1 qEHVI-PM-CBD (q=1) qEHVI-PM-CBD (q=16) qEHVI-PM-CBD (q=32) qEHVI-PM-CBD (q=8) qNEHVI (q=1) qNEHVI (q=16) qNEHVI (q=32) qNEHVI (q=8) qNEHVI-1 (q=1) qNEHVI-1 (q=16) qNEHVI-1 (q=32) qNEHVI-1 (q=8) (b) Figure 12: Anytime optimization performance of batch EHVI-based acquisition functions using various qover the number of function evaluations. 1 8 16 32 q -0.20 -0.10 0.00 0.10 Log Hypervolume Difference AutoML 1 8 16 32 q 1.40 1.60 1.80 2.00 2.20 CarSideImpact 1 8 16 32 q 0.90 1.00 1.10 1.20 1.30 1.40 1.50 1.60 ConstrainedBraninCurrin 1 8 16 32 q -2.25 -2.00 -1.75 -1.50 -1.25 -1.00 -0.75 -0.50 ZDT1 DGEMO MOEA/D-EGO TS-TCH TSEMO qEHVI-PM-CBD qNEHVI qNEHVI-1 qNParEGO Figure 13: Final log hypervolume difference with variousqunder a budget of 224 function evaluations. Smaller log hypervolume differences are better. 33H.6 Optimization Performance under Increasing Noise Levels Figure 14 shows the sequential optimization performance of qNEHVI and qNEHVI-1 relative to qEHVI and qNParEGO under increasing noise levels. qNEHVI-1 achieves the best ﬁnal hy- pervolume when the noise standard deviation σ is less than 15% of the range of each objective, but performs worse than qNEHVI earlier in the optimization. qNEHVI is the top performer in high-noise environments. We observe that all methods degrade as the noise level increases, however qNEHVI consistently exhibits excellent performance relative to other methods and only qNEHVI-1 is competitive and only in the low-noise regime. -2.00 -1.75 -1.50 -1.25 -1.00 -0.75 -0.50Log Hypervolume Difference = 1% qNEHVI qEHVI qNEHVI-1 qNParEGO = 2%  = 3%  = 4% 0 100 200 300 400 Function Evaluations -2.00 -1.75 -1.50 -1.25 -1.00 -0.75 -0.50Log Hypervolume Difference = 5% 0 100 200 300 400 Function Evaluations = 10% 0 100 200 300 400 Function Evaluations = 15% 0 100 200 300 400 Function Evaluations = 20% Figure 14: Sequential optimization performance under increasing noise levels on a DTLZ2 problem (d = 6, M = 2). σis the noise standard deviation, which we deﬁne as a percentage of the range of each objective over the entire search space. A noise level of 20% is very high; for comparison, previous work on noisy MOBO has only considered noise levels of 1% [25]. H.7 Optimization Performance on Noiseless Benchmarks We include a comparison of optimization performance onnoiseless benchmarks. Figure 15 shows that qNEHVI performs competitively with qEHVI (-PM-CBD) and outperforms DGEMO, TS-TCH and qNParEGO across all benchmark problems. qNEHVI-1 is also a top performer on noiseless problems and both qNEHVI and qNEHVI-1 show little degradation in performance with increasing levels of parallelism. 340 50 100 150 200 Function Evaluations -0.50 0.00 0.50 1.00 1.50 Log Hypervolume Difference BraninCurrin 0 50 100 150 200 Function Evaluations -2.25 -2.00 -1.75 -1.50 -1.25 -1.00 -0.75 -0.50 DTLZ2 0 50 100 150 200 Function Evaluations -2.50 -2.00 -1.50 -1.00 -0.50 0.00 ZDT1 0 50 100 150 200 Function Evaluations -1.00 -0.50 0.00 0.50 1.00 VehicleSafety DGEMO TS-TCH qEHVI qEHVI-PM-CBD qNEHVI qNEHVI-1 qNParEGO (a) 1 8 16 32 q -0.75 -0.50 -0.25 0.00 0.25 0.50 0.75 1.00 1.25 Log Hypervolume Difference BraninCurrin 1 8 16 32 q -2.40 -2.20 -2.00 -1.80 -1.60 -1.40 -1.20 -1.00 -0.80 DTLZ2 1 8 16 32 q -2.50 -2.25 -2.00 -1.75 -1.50 -1.25 -1.00 -0.75 ZDT1 1 8 16 32 q -1.25 -1.00 -0.75 -0.50 -0.25 0.00 0.25 0.50 0.75 VehicleSafety DGEMO TS-TCH qEHVI-PM-CBD qNEHVI qNEHVI-1 qNParEGO (b) Figure 15: Sequential (a) and parallel (b) optimization Performance on noiseless benchmarks. H.8 Performance of qNEHVI-1 on 5-Objective Optimization We demonstrate that qNEHVI-1 enables scaling to 5-objective problems. To our knowledge, no previous methods leveraging EHVI or HVI (e.g. DGEMO, TSEMO) considers 5-objective problems because of the super-polynomial complexity of the hypervolume indicator. Nevertheless, we show that using CBD and a single sample path approximation,qNEHVI-1 can be used for 5-objective opti- mization. As shown in Figure 16,qNEHVI-1 outperforms qNParEGO and Sobol search. qNEHVI-1 takes on average 73.53 seconds (with an SEM of 1.74 seconds) to generate each candidate, whereas qNParEGO takes 11.37 seconds (with an SEM of 0.97 seconds). 0 100 200 300 400 Function Evaluations -0.60 -0.50 -0.40 -0.30 -0.20 -0.10 0.00 Log Hypervolume Difference DTLZ2-5 Sobol qNEHVI-1 qNParEGO Figure 16: Optimization performance on a 5-objective DTLZ2 problem ( d= 6) with σ= 5%. 35H.9 Performance compared against a Multi-Objective CMA-ES CMA-ES is an evolutionary strategy that is a strong method in single objective optimization, and many works have proposed extensions of CMA-ES to the multi-objective setting [27, 55]. We compare qNEHVI against the COMO-CMA-ES algorithm, which has been shown to outperform MO-CMA- ES on a variety of problems [55].10. We evaluate performance on the SphereEllipsoidal function from Bi-objective Black-Box Optimization Benchmarking Test Suite [6], and we add zero-mean Gaussian noise to each objective with σ= 5% of the range of each objective. We run COMO-CMA-ES with 5 kernels, the same initial quasi-random design as the BO methods, a population size of 10, and an initial step size of 0.2. As shown in Figure 17, the BO methods vastly outperform COMO-CMA-ES. qNEHVI and qNParEGO perform best and are closely followed by qNEHVI-1. 0 50 100 150 200 Function Evaluations 7.20 7.40 7.60 7.80 8.00 8.20 8.40Log Hypervolume Difference SphereEllipsoidal COMO-CMA-ES Sobol TS-TCH qNEHVI qNEHVI-1 qNParEGO Figure 17: Optimization performance on a 2-objective Sphere-Ellipsoidal problem ( d = 5) with σ= 5%. H.10 Importance of Accounting for Noise in DGEMO -15 -10 -5 0 Objective 1 -8 -7 -6 -5 -4 -3 -2 -1 0 1 Objective 2 DGEMO-PM-NEHVI -15 -10 -5 0 Objective 1 DGEMO -15 -10 -5 0 Objective 1 DGEMO-PM Figure 18: An illustration of the effect of noisy observations on the true noiseless Pareto frontiers identiﬁed by DGEMO (right) DGEMO-PM-NEHVI (left, see Appendix H.10). Both algorithms are tested on a BraninCurrin synthetic problem, where observations are corrupted with zero-mean, additive Gaussian noise with a standard deviation of 5% of the range of respective objective. All methods use sequential (q= 1) optimization. 10COMO-CMA-ES is also the only multi-objective CMA-ES that we could ﬁnd with an open-source Python implementation. We use the implementation available at https://github.com/CMA-ES/pycomocma under the BSD 3-clause license. 36Similar to EHVI, DGEMO relies on the observed (noisy) Pareto frontier for batch selection. The right plot in Figure 18 shows that DGEMO exhibits the same clumping behavior in objective space in the noisy setting as EHVI . While DGEMO’s diversity constraints (with respect to the input parameters) make it slightly more robust to noise, the solutions are clustered and the bottom right corner of the Pareto frontier is not identiﬁed. In an attempt to mitigate these issues, we propose an augmented version of DGEMO, which we call DGEMO-PM-NEHVI, as follows: (i) we use the posterior mean at the previously evaluated points to estimate the in-sample Pareto frontier, which we hope will improve robustness to noise when selecting a discrete set of potential candidates using the DGEMO’s ﬁrst-order approximation of the Pareto frontier, and (ii) we use qNEHVI rather than HVI under the posterior mean as the batch selection criterion over the discrete set, subject to DGEMO’s diversity constraints. We ﬁnd that using qNEHVI to integrate over the uncertainty in the Pareto frontier over the previously evaluated points, results in identifying higher quality Pareto frontiers as shown in Figure 18. We also include DGEMO-PM, which uses (i) but not (ii) for completeness. Not only does DGEMO-PM-NEHVI identify much more diverse solutions that provide better coverage across the Pareto frontier, but DGEMO-PM-NEHVI also identiﬁes much better solutions on the lower right portion of the Pareto frontier than DGEMO and DGEMO-PM. DGEMO-PM performs much better than DGEMO and is competitively with DGEMO-PM-NEHVI, which we speculate is because DGEMO uses a ﬁrst-order approximation of the Pareto frontier (using the observed values or using the posterior mean for -PM variants) to generate a discrete set of candidates. Using the posterior mean in this step is important for regularizing against extreme observed values due to noise. qNEHVI is only used as a ﬁltering criterion for batch selection over that discrete set of candidates, subject to DGEMO’s diversity constraints. Hence, qNEHVI has limited control over the batch selection procedure. DGEMO’s ﬁrst-order approximation fundamentally does not account for uncertainty in the Pareto frontier over previously evaluated points. Although one could integrate over the uncertainty in the in-sample Pareto frontier by generating a ﬁrst-order approximation of the Pareto frontier under different sample paths, the graph cut algorithm would yield different families under each sample path. It is unclear how to set the diversity constraints in that setting. We leave this for future work. I Noisy Outcome Constraints While the focus of this work is on developing a scalable parallel hypervolume-based acquisition function for noisy settings, our MC-based approach naturally lends itself to support for constraints. I.1 Derivation of Constrained NEHVI The NEHVI formulation in (2) can be extended to handle noisy observations of outcome constraints. We consider the scenario where we receive noisy observations ofM objectives f(x) ∈RM and V constraints c(v) ∈RV, all of which are assumed to be “black-box”: Dn = {xi,yi,bi}n i=1 where[ yi bi ] ∼N ([ f(xi) c(xi) ] ,Σi ) , Σi ∈R(M+V)×(M+V). We assume, without loss of generality, that c(v) is feasible iff c(v) ≥0. In the constrained optimization setting, we aim to identify the a ﬁnite approximate feasible Pareto set Pfeas = {f(x) |x∈Xn,c(x) ≥0, ∄ x′: c(x′) ≥0 s.t.f(x′) ≻f(x)} of the true feasible Pareto set P∗ feas = {f(x) s.t. c(x) ≥0, ∄ x′: c(x′) ≥0 s.t.f(x′) ≻f(x)}. The natural improvement measure in the constrained setting is feasible HVI, which we deﬁne for a single candidate point xas HVI C(f(x),c(x)|Pfeas) := HVI [f(x)|Pfeas] ·1 [c(x) ≥0]. Taking the expectation over HVIC gives the constrained expected hypervolume improvement: αEHVI C (x) = ∫ HVI C(f(x),c(x)|Pfeas)p(f,c|D)dfdc (19) 37For brevity, we deﬁne Cn = c(Xn),Fn = f(Xn). The noisy expected hypervolume improvement is then deﬁned as: αNEHVI C (x) = ∫ αEHVI C (x|Pfeas)p(Fn,Cn|Dn)dFndCn. (20) Performing feasibility-weighting on the sample-level allows us to include such auxiliary outcome constraints into the full Monte Carlo formulation given in (3) in a straightforward way: ˆαNEHVI c(x) = 1 N N∑ t=1 Kt∑ k=1 [ M∏ m=1 [ z(m) k,t −l(m) k,t ] + V∏ v=1 1 [c(v) t (x) ≥0] ] where z(m) k,t := min [ u(m) k,t , ˜f(m) t (x) ] and l(m) k,t ,u(m) k,t are the mth dimension of the lower and upper vertices of the rectangle Sk,t in the non-dominated partitioning {S1,t,...,S Kt,t}under the feasible sampled Pareto frontier Pfeas,t = Pfeas = {˜ft(x) |x∈Xn,˜ct(x) ≥0, ∄ x′: ˜ct(x′) ≥0 s.t. ˜ft(x′) ≻ ˜ft(x)}. In this formulation, the ∏V v=1 1 [c(v) t (x) ≥0] indicates feasibility of the t-th sample. To permit gradient-based optimization via exact sample-path gradients, we replace the indicator function (which is non-differentiable) with a differentiable sigmoid approximation with a temperature parameter τ, which becomes exact as τ →∞: 1 [c(v)(x) ≥0] ≈s(c(v)(x); τ) := 1 1 + exp(−c(v)(x)/τ) (21) Hence, ˆαNEHVI c(x) ≈ 1 N N∑ t=1 Kt∑ k=1 [ M∏ m=1 [ z(m) k,t −l(m) k,t ] + V∏ v=1 s(c(v) t (x),τ) ] I.2 Derivation of Parallel, Constrained NEHVI The constrained NEHVI can be extended to the parallel setting in a straightforward fashion. The joint constrained hypervolume improvement of a set of points {xi}q i=1 is given by HVI C({f(xi),c(xi)}q i=1) = K∑ k=1 q∑ j=1 ∑ Xj∈Xj (−1)j+1 [( M∏ m=1 [ z(m) k,Xj −l(m) k ] + ) ∏ x′∈Xj V∏ v=1 1 [c(v)(x′) ≥0] ] . and the constrained qEHVI is [11]: αqEHVI C (Xcand|Pfeas) = ∫ HVI C(f(Xcand),c(Xcand)|Pfeas)p(f,c|Dn)dfdc Hence, the constrained qNEHVI is given by: αqNEHVI c(Xcand) = ∫ αqEHVI C (Xcand|Pfeas)p(Fn,Cn|Dn)dFndCn = ∫ HVI C(f(Xcand),c(Xcand)|Pfeas)p(Fn,Cn|Dn)dFndCn (22) Using MC integration for the integral in (22), we have ˆαqNEHVI c(Xcand) = 1 N N∑ t=1 HVI c( ˜ft(Xcand),˜ct(Xcand)|Pfeas,t). (23) Under the CBD formulation, the constrained qNEHVI is given by ˆαqNEHVI ({x1,..., xi}) = 1 N N∑ t=1 HVI C ( {˜ft(xj),˜ct(xj)}i−1 j=1}|P feas,t ) + 1 N N∑ t=1 HVI C (˜ft(xi),˜ct(xi) |Pfeas,t ∪{ ˜ft(xj),˜ct(xj)}i−1 j=1} ) . (24) 38As in (6), the ﬁrst term is a constant when generating candidate iand the second term is the NEHVI of xi. J Evaluating Methods on Noisy Benchmarks Given noisy observations, we can no longer compute the true Pareto frontier over the in-sample points Xn. Moreover, the subset of Pareto optimal designsX∗ n = {x∈Xn, ∄ x′∈Xns.t.f(x′) ≻f(x)} from the previously evaluated points may not be identiﬁed due to noise. For the previous results reported in this paper, we evaluate each method according to hypervolume dominated by the true unknown Pareto frontier of the noiseless objectives over thein-sample points. In practice, decision- makers would often select one of the in-sample points according to their preferences. If the decision maker only has noisy observations, selecting an in-sample point may be preferable to evaluating a new out-of-sample point according to the model’s beliefs. An alternative evaluation method would be use the model’s posterior mean to identify what it believes is the Pareto optimal set of in-sample designs. The hypervolume dominated by the true Pareto frontier of noiseless objectives over that set of selected designs could be computed and used for comparing the performance of different methods. Results using this procedure are shown in Figure 19. The quality of the Pareto set depends on the model ﬁt. Several methods have worse performance over time (e.g. qEHVI and qPAREGO on the ZDT1 problem), likely due to the collection of outlier observations that degrade the model ﬁt. Nevertheless, qNEHVI consistently has the strongest performance. An alternative to the in-sample evaluation techniques described above would be to use the model to identify the Pareto frontier across the entire search space (in-sample or out-of-sample). For example, Hernández-Lobato et al. [25] used NSGA-II to optimize the model’s posterior mean and identify the model estimated Pareto frontier. For benchmarking purposes on expensive-to-evaluate functions (e.g. in AutoML or ABR), this is prohibitively expensive. Moreover, such a method is less appealing in practice because a decision-maker would have to select out-of-sample points according to the posterior mean and then evaluate a set of preferred designs on the noisy objective to verify that the model predictions are fairly accurate at those out-of-sample designs. Therefore, in this work we evaluate methods based on the in-sample designs. 390 50 100 150 200 Function Evaluations 0.60 0.80 1.00 1.20 1.40 1.60 1.80Log Hypervolume Diﬀerence BraninCurrin 0 50 100 150 200 Function Evaluations -1.60 -1.40 -1.20 -1.00 -0.80 -0.60 -0.40 -0.20 0.00 ZDT1 0 50 100 150 200 Function Evaluations 5.50 5.60 5.70 5.80 5.90 6.00 6.10 Adaptive Bitrate 0 50 100 150 200 Function Evaluations -0.50 -0.25 0.00 0.25 0.50 0.75 1.00 1.25 1.50 VehicleSafety MESMO PFES Sobol TS-TCH qEHVI qEHVI-PM qNEHVI qNParEGO qParEGO (a) 0 50 100 150 200 Function Evaluations -0.05 0.00 0.05 0.10 0.15 0.20 0.25 Log Hypervolume Diﬀerence AutoML 0 50 100 150 200 Function Evaluations 1.40 1.60 1.80 2.00 2.20 2.40 CarSideImpact 0 50 100 150 200 Function Evaluations 1.40 1.60 1.80 2.00 2.20 2.40 2.60 ConstrainedBraninCurrin qNEHVI qParEGO qNParEGO TS-TCH qEHVI qEHVI-PM MESMO PFES (b) Figure 19: Sequential optimization performance using based on the model-identiﬁed Pareto set across in-sample points. 40",
      "meta_data": {
        "arxiv_id": "2105.08195v2",
        "authors": [
          "Samuel Daulton",
          "Maximilian Balandat",
          "Eytan Bakshy"
        ],
        "published_date": "2021-05-17T23:31:42Z",
        "pdf_url": "https://arxiv.org/pdf/2105.08195v2.pdf"
      }
    },
    {
      "title": "Diversity-Guided Multi-Objective Bayesian Optimization With Batch Evaluations"
    },
    {
      "title": "Collaborative Bayesian Optimization with Fair Regret"
    },
    {
      "title": "Multi-Fidelity Bayesian Optimization via Deep Neural Networks",
      "abstract": "Bayesian optimization (BO) is a popular framework to optimize black-box\nfunctions. In many applications, the objective function can be evaluated at\nmultiple fidelities to enable a trade-off between the cost and accuracy. To\nreduce the optimization cost, many multi-fidelity BO methods have been\nproposed. Despite their success, these methods either ignore or over-simplify\nthe strong, complex correlations across the fidelities, and hence can be\ninefficient in estimating the objective function. To address this issue, we\npropose Deep Neural Network Multi-Fidelity Bayesian Optimization (DNN-MFBO)\nthat can flexibly capture all kinds of complicated relationships between the\nfidelities to improve the objective function estimation and hence the\noptimization performance. We use sequential, fidelity-wise Gauss-Hermite\nquadrature and moment-matching to fulfill a mutual information-based\nacquisition function, which is computationally tractable and efficient. We show\nthe advantages of our method in both synthetic benchmark datasets and\nreal-world applications in engineering design.",
      "full_text": "arXiv:2007.03117v4  [cs.LG]  10 Dec 2020 Multi-Fidelity Bayesian Optimization via Deep Neural Networks Shibo Li School of Computing University of Utah Salt Lake City, UT 84112 shibo@cs.utah.edu W ei Xing Scientiﬁc Computing and Imaging Institute University of Utah Salt Lake City, UT 84112 wxing@sci.utah.edu Robert M. Kirby School of Computing University of Utah Salt Lake City, UT 84112 kirby@cs.utah.edu Shandian Zhe School of Computing University of Utah Salt Lake City, UT 84112 zhe@cs.utah.edu Abstract Bayesian optimization (BO) is a popular framework for optim izing black-box functions. In many applications, the objective function ca n be evaluated at mul- tiple ﬁdelities to enable a trade-off between the cost and ac curacy. T o reduce the optimization cost, many multi-ﬁdelity BO methods have b een proposed. De- spite their success, these methods either ignore or over-si mplify the strong, com- plex correlations across the ﬁdelities. While the acquisit ion function is therefore easy and convenient to calculate, these methods can be inefﬁ cient in estimating the objective function. T o address this issue, we propose De ep Neural Network Multi-Fidelity Bayesian Optimization (DNN-MFBO) that can ﬂexibly capture all kinds of complicated relationships between the ﬁdelities t o improve the objective function estimation and hence the optimization performanc e. W e use sequential, ﬁdelity-wise Gauss-Hermite quadrature and moment-matchi ng to compute a mu- tual information based acquisition function in a tractable and highly efﬁcient way. W e show the advantages of our method in both synthetic benchm ark datasets and real-world applications in engineering design. 1 Introduction Bayesian optimization (BO) (Mockus et al., 1978; Snoek et al., 2012) is a general and powerful ap- proach for optimizing black-box functions. It uses a probab ilistic surrogate model (typically Gaus- sian process (GP) (Rasmussen and Williams, 2006)) to estima te the objective function. By repeat- edly maximizing an acquisition function computed with the i nformation of the surrogate model, BO ﬁnds and queries at new input locations that are closer and cl oser to the optimum; meanwhile the new training examples are incorporated into the surrogate m odel to improve the objective estimation. In practice, many applications allow us to query the objecti ve function at different ﬁdelities, where low ﬁdelity queries are cheap yet inaccurate, and high ﬁdeli ty queries more accurate but costly. For example, in physical simulation (Peherstorfer et al., 2018 ), the computation of an objective ( e.g., the elasticity of a part or energy of a system) often involves solving partial differential equations. Running a numerical solver with coarse meshes gives a quick y et rough result; using dense meshes substantially improves the accuracy but dramatically incr eases the computational cost. The multi- ﬁdelity queries enable us to choose a trade-off between the c ost and accuracy. 34th Conference on Neural Information Processing Systems ( NeurIPS 2020), V ancouver, Canada.Accordingly, to reduce the optimization cost, many multi-ﬁ delity BO methods (Huang et al., 2006; Lam et al., 2015; Kandasamy et al., 2016; Zhang et al., 2017; T akeno et al., 2019) have been pro- posed to jointly select the input locations and ﬁdelities to best balance the optimization progress and query cost, i.e., the beneﬁt-cost ratio. Despite their success, these method s often ignore the strong, complex correlations between the function outputs at diffe rent ﬁdelities, and learn an independent GP for each ﬁdelity (Lam et al., 2015; Kandasamy et al., 2016) . Recent works use multi-output GPs to capture the ﬁdelity correlations. However, to avoid intr actable computation of the acquisition function, they have to impose simpliﬁed correlation struct ures. For example, T akeno et al. (2019) assume a linear correlation between the ﬁdelities; Zhang et al. (2017) use kernel convolution to con- struct the cross-covariance function, and have to choose si mple, smooth kernels ( e.g., Gaussian) to ensure a tractable convolution. Therefore, the existing me thods can be inefﬁcient and inaccurate in estimating the objective function, which further lowers th e optimization efﬁciency and increases the cost. T o address these issues, we propose DNN-MFBO, a deep neural n etwork based multi-ﬁdelity Bayesian optimization that is ﬂexible enough to capture all kinds of complex (possibly highly nonlin- ear and nonstationary) relationships between the ﬁdelitie s, and exploit these relationships to jointly estimate the objective function in all the ﬁdelities to impr ove the optimization performance. Speciﬁ- cally, we stack a set of neural networks (NNs) where each NN mo dels one ﬁdelity. In each ﬁdelity, we feed both the original input (to the objective) and output from the previous ﬁdelity into the NN to propagate information throughout and to estimate the com plex relationships across the ﬁdelities. Then, the most challenging part is the calculation of the acq uisition function. For efﬁcient inference and tractable computation, we consider the NN weights in the output layer as random variables and all the other weights as hyper-parameters. W e develop a stoc hastic variational learning algorithm to jointly estimate the posterior of the random weights and h yper-parameters. Next, we sequen- tially perform Gauss-Hermite quadrature and moment matchi ng to approximate the posterior and conditional posterior of the output in each ﬁdelity, based o n which we calculate and optimize an information based acquisition function, which is not only c omputationally tractable and efﬁcient, but also conducts maximum entropy search (W ang and Jegelka, 2017), the state-of-the-art criterion in BO. For evaluation, we examined DNN-MFBO in three benchmark functions and two real-world applica- tions in engineering design that requires physical simulat ions. The results consistently demonstrate that DNN-MFBO can optimize the objective function (in the hi ghest ﬁdelity) more effectively, mean- while with smaller query cost, as compared with state-of-th e-art multi-ﬁdelity and single ﬁdelity BO algorithms. 2 Background Bayesian optimization. T o optimize a black-box objective function f : X → R, BO learns a probabilistic surrogate model to predict the function valu es across the input domain X and quantiﬁes the uncertainty of the predictions. This information is use d to calculate an acquisition function that measures the utility of querying at different input locatio ns, which usually encodes a exploration- exploitation trade-off. By maximizing the acquisition fun ction, BO ﬁnds new input locations at which to query, which are supposed to be closer to the optimum ; meanwhile the new examples are added into the training set to improve the accuracy of the surrogate model. The most commonly used surrogate model is Gaussian process (GP) (Rasmussen an d Williams, 2006). Given the training inputs X = [ x1, . . . , xN ]⊤ and (noisy) outputs y = [ y1, . . . , y N ]⊤, GP assumes the outputs follow a multivariate Gaussian distribution, p(y|X) = N (y|m, K + σ2I) where m are the values of the mean function at the inputs X, K is a kernel matrix on X, [K]ij = k(xi, xj ) (k(·, ·) is the kernel function), and σ2 is the noise variance. The mean function is usually set to the constant function 0 and so m = 0. Due to the multi-variate Gaussian form, given a new input x∗, the posterior distribution of the function output, p ( f(x∗)|x∗, X, y ) is a closed-form conditional Gaussian, and hence is convenient to quantify the uncertainty and calcula te the acquisition function. There are a variety of commonly used acquisition functions, such as expected improvement (EI) (Jones et al., 1998), upper conﬁdent bound (UCB) (Srini vas et al., 2010), entropy search (ES) (Hennig and Schuler, 2012), and predictive entropy sea rch (PES) (Hernández-Lobato et al., 2014). A particularly successful recent addition is the max -value entropy search (MES) (W ang and Jegelka, 2017), which not only enjoys a global util ity measure (like ES and PES), but also is computationally efﬁcient (because it calculates th e entropy of the function output rather than input like in ES/PES). Speciﬁcally, MES maximizes the mutua l information between the function 2value and its maximum f∗ to ﬁnd the next input at which to query, a(x) = I ( f(x), f ∗|D ) = H ( f(x)|D ) − Ep(f∗|D)[H ( f(x)|f∗, D ) ], (1) where I(·, ·) is the mutual information, H(·) the entropy, and D the training examples collected so far. Note that the function values and extremes are consider ed as generated from the posterior in the surrogate model, which includes all the knowledge we have fo r the black-box objective function. Multi-ﬁdelity Bayesian optimization . Many applications allow multi-ﬁdelity queries of the obje c- tive function, {f1(x), . . . , f M (x)}, where the higher (larger) the ﬁdelity m, the more accurate yet costly the query of fm(·). Many studies have extended BO for multi-ﬁdelity settings. For exam- ple, MF-GP-UCB (Kandasamy et al., 2016) starts from the lowe st ﬁdelity ( m = 1 ), and queries the objective at each ﬁdelity until the conﬁdence band excee ds a particular threshold. Despite its effectiveness and theoretical guarantees, MF-GP-UCB lear ns an independent GP surrogate for each ﬁdelity and ignores the strong correlations between the ﬁde lities. Recent works use a multi-output GP to model the ﬁdelity correlations. For example, MF-PES (Z hang et al., 2017) introduces a shared latent function, and uses kernel convolution to derive the c ross-covariance between the ﬁdelities. The most recent work, MF-MES (T akeno et al., 2019) introduces C kernel functions {κc(·, ·)} and, for each ﬁdelity m, C latent features {ωcm}. The covariance function is deﬁned as k ( fm(x), fm′ (x′) ) = ∑ C c=1 (ωcmωcm′ + τcmδmm′ )κc(x, x′), (2) where τcm > 0, δmm′ = 1 if and only if m = m′, and each kernel κc(·, ·) is usually assumed to be stationary, e.g., Gaussian kernel. 3 Multi-Fidelity Modeling with Deep Neural Networks Despite the success of existing multi-ﬁdelity BO methods, they either overlook the strong, complex correlations between different ﬁdelities ( e.g., MF-GP-UCB) or model these correlations with an over-simpliﬁed structure. For example, the convolved GP in MF-PES has to employ simple/smooth kernels (typically Gaussian) for both the latent function a nd convolution operation to obtain an ana- lytical cross-covariance function, which has limited expr essiveness. MF-MES essentially adopts a linear correlation assumption between the ﬁdelities. Acc ording to (2), if we choose each κc as a Gaussian kernel (with amplitude one), we have k ( fm(x), fm′ (x) ) = ω⊤ mωm′ + δmm′ τm where ωm = [ ω1m, . . . , ω Cm ]⊤ and τm = ∑ C c=1 τcm. These correlation structures might be over-simpliﬁed and insufﬁcient to estimate the complicate d relationships between the ﬁdelities ( e.g., highly nonlinear and nonstationary). Hence, they can limit the accuracy of the surrogate model and lower the optimization efﬁciency while increasing the quer y cost. T o address this issue, we use deep neural networks to build a m ulti-ﬁdelity model that is ﬂexible enough to capture all kinds of complicated relationships be tween the ﬁdelities, taking advantage of the relationships to promote the accuracy of the surrogat e model. Speciﬁcally, for each ﬁdelity m > 1, we introduce a neural network (NN) parameterized by {wm, θm}, where wm are the weights in the output layer and θm the weights in all the other layers. Denote the NN input by xm, the output by fm(x) and the noisy observation by ym(x). The model is deﬁned as xm = [ x; fm−1(x)], f m(x) = w⊤ mφθ m (xm), y m(x) = fm(x) + ǫm, (3) where x is the original input to the objective function, φθ m (xm) is the output vector of the second last layer (hence parameterized by θm) which can be viewed as a set of nonlinear basis functions, an d ǫm ∼ N (ǫm|0, σ2 m) is a Gaussian noise. The input xm is obtained by appending the output from the previous ﬁdelity to the original input. Through a series of l inear and nonlinear transformations inside the NN, we obtain the output fm(x). In this way, we digest the information from the lower ﬁdelit ies, and capture the complex relationships between the current a nd previous ﬁdelities by learning a nonlinear mapping fm(x) = h(x, fm−1(x)), where h(·) is fulﬁlled by the NN. When m = 1 , we set xm = x. A graphical representation of our model is given in Fig. 1 of the supplementary material. W e assign a standard normal prior over each wm. Following (Snoek et al., 2015), we con- sider all the remaining NN parameters as hyper-parameters. Given the training set D = {{(xnm, ynm)}Nm n=1}M m=1, the joint probability of our model is p(W, Y|X , Θ , s) = ∏ M m=1 N (wm|0, I) ∏ Nm n=1 N ( ynm|fm(xnm), σ2 m ) , (4) 3where W = {wm}, Θ = {θm}, s = [ σ2 1, . . . , σ 2 M ]⊤, and X , Y are the inputs and outputs in D. In order to obtain the posterior distribution of our model (w hich is in turn used to compute the ac- quisition function), we develop a stochastic variational l earning algorithm. Speciﬁcally, for each wm, we introduce a multivariate Gaussian posterior, q(wm) = N (wm|µm, Σ m). W e further pa- rameterize Σ m with its Cholesky decomposition to ensure the positive deﬁn iteness, Σ m = LmL⊤ m where Lm is a lower triangular matrix. W e assume q(W) = ∏ M m=1 q(wm), and construct a varia- tional model evidence lower bound (ELBO), L ( q(W), Θ , s ) = Eq[log(p(W, Y|X , Θ , s)/q(W))]. W e then maximize the ELBO to jointly estimate the variationa l posterior q(W) and all the other hyper-parameters. The ELBO is analytically intracta ble, and we use the reparameterization trick (Kingma and W elling, 2013) to conduct efﬁcient stocha stic optimization. The details are given in the supplementary material (Sec. 3). 4 Multi-Fidelity Optimization with Max-V alue Entropy Search W e now consider an acquisition function to select both the ﬁd elities and input locations at which we query during optimization. Following (T akeno et al., 2019) , we deﬁne the acquisition function as a(x, m) = 1 λm I (f∗, fm(x)|D) = 1 λm ( H ( fm(x)|D ) − Ep(f∗|D) [ H ( fm(x)|f∗, D )]) (5) where λm > 0 is the cost of querying with ﬁdelity m. In each step, we maximize the acquisition function to ﬁnd a pair of input location and ﬁdelity that prov ides the largest beneﬁt-cost ratio. However, given the model inference result, i.e., p(W|D) ≈ q(W), a critical challenge is to compute the posterior distribution of the output in each ﬁdelity, p(fm(x)|D), and use them to compute the acquisition function. Due to the nonlinear coupling of the o utputs in different ﬁdelities (see (3)), the computation is analytically intractable. T o address th is issue, we conduct ﬁdelity-wise moment matching and Gauss-Hermite quadrature to approximate each p(fm(x)|D) as a Gaussian distribu- tion. 4.1 Computing Output Posteriors Speciﬁcally, we ﬁrst assume that we have obtained the posterior of the output for ﬁdelity m − 1, p ( fm−1(x)|D ) ≈ N ( fm−1|αm−1(x), ηm−1(x) ) . For convenience, we slightly abuse the notation and use fm−1 and fm to denote fm−1(x) and fm(x), respectively. Now we consider calculat- ing p(fm|D). According to (3), we have fm = w⊤ mφθ m ([x; fm−1]). Based on our variational posterior q(wm) = N (wm|µm, LmL⊤ m), we can immediately derive the conditional posterior p(fm|fm−1, D) = N ( fm|u(fm−1, x), γ(fm−1, x) ) where u(fm−1, x) = µ⊤ mφθ m ([x; fm−1]) and γ(fm−1, x) = ∥L⊤ mφθ m ([x; fm−1])∥2. Here ∥ · ∥ 2 is the square norm. W e can thereby read out the ﬁrst and second conditional moments, E[fm|fm−1, D] = u(fm−1, x), E[f2 m|fm−1, D] = γ(fm−1, x) + u(fm−1, x)2. (6) T o obtain the moments, we need to take the expectation of the c onditional moments w .r.t p(fm−1|D) ≈ N ( fm−1|αm−1(x), ηm−1(x) ) . While the conditional moments are nonlinear to fm−1 and their expectation is not analytical, we can use Gauss-He rmite quadrature to give an accu- rate, closed-form approximation, E[fm|D] = Ep(fm−1|D)E[fm|fm−1, D] ≈ ∑ k gk · u(tk, x), E[f2 m|D] = Ep(fm−1|D)E[f2 m|fm−1, D] ≈ ∑ k gk · [γ(tk, x) + u(tk, x)2], (7) where {gk} and {tk} are quadrature weights and nodes, respectively. Note that e ach node tk is determined by αm−1(x) and ηm−1(x). W e then use these moments to construct a Gaus- sian posterior approximation, p(fm|D) ≈ N ( fm|αm(x), ηm(x) ) where αm(x) = E[fm|D] and ηm(x) = E[f2 m|D] − E[fm|D]2. This is called moment matching, which is widely used and ver y successful in approximate Bayesian inference, such as expe ctation-propagation (Minka, 2001). One may concern if the quadrature will give a positive variance. This is guaranteed by the follow lemma. Lemma 4.1. As long as the conditional posterior variance γ(fm−1, x) > 0, the posterior variance ηm(x), computed based on the quadrature in (7), is positive. 4The proof is given in the supplementary material. Following the same procedure, we can compute the posterior of the output in ﬁdelity m + 1. Note that when m = 1 , we do not need quadrature because the input of the NN is the same as the original input, not inclu ding other NN outputs. Hence, we can derive the Gaussian posterior outright from q(w1) — p(f1(x)|D) = N ( f1(x)|α1(x), η1(x) ) , where α1(x) = µ⊤ 1φθ 1 (x) and η1(x) = ∥L⊤ 1φθ 1 (x)∥2. 4.2 Computing Acquisition Function Given the posterior of the NN output in each ﬁdelity,p ( fm(x)|D) ≈ N (fm(x)|αm(x), ηm(x) ) (1 ≤ m ≤ M), we consider how to compute the acquisition function (5). Du e to the Gaussian posterior, the ﬁrst entropy term is straightforward, H ( fm(x)|D ) = 1 2 log ( 2πeηm(x) ) . The second term — a conditional entropy, however, is intractable. Hence, we f ollow (W ang and Jegelka, 2017) to use a Monte-Carlo approximation, Ep(f∗|D)[H ( fm(x)|f∗, D ) ] ≈ 1 |F| ∑ f∗∈F∗ H ( fm(x)|f∗, D ) , where F∗ are a collection of independent samples of the function maxi mums based on the posterior distribution of our model. T o obtain a sample of the function maximum, we ﬁrst generate a posterior sample for each wm, according to q(wm) = N (wm|µm, LmL⊤ m). W e replace each wm by their sample in calculating fM (x) so as to obtain a posterior sample of the objective function. W e then maximize this sample function to obtain one instance of f∗. W e use L-BFGS (Liu and Nocedal, 1989) for optimization. Given f∗, the computation of H ( fm(x)|f∗, D ) = H ( fm(x)| max fM (x) = f∗, D ) is still in- tractable. W e then follow (W ang and Jegelka, 2017) to calcul ate H ( fm(x)|fM (x) ≤ f∗, D ) instead as a reasonable approximation. For m = M, the entropy is based on a truncated Gaussian distribu- tion, p(fM (x)|fM (x) ≤ f∗, D) ∝ N ( fM (x)|αM (x), ηM (x) ) 1(fM (x) ≤ f∗) where 1(·) is the indicator function, and is given by H ( fm(x)|fM (x) ≤ f∗, D ) = log ( √ 2πeηM (x)Φ( β) ) − β · N (β|0, 1)/ ( 2Φ( β) ) , (8) where Φ( ·) is the cumulative density function (CDF) of the standard nor mal distribution, and β =( f∗ − αM (x) ) / √ ηM (x). When m < M , the entropy is based on the conditional distribution p(fm(x)|fM (x) ≤ f∗, D) = 1 Z · p ( fm(x)|D ) p(fM (x) ≤ f∗|fm(x), D) ≈ 1 Z · N ( fm(x)|αm(x), ηm(x) ) p(fM (x) ≤ f∗|fm(x), D). (9) where Z is the normalizer. T o obtain p(fM (x) ≤ f∗|fm(x), D), we ﬁrst consider how to compute p(fM (x)|fm(x), D). According to (3), it is trivial to derive that p(fm+1(x)|fm(x), D) = N ( fm+1|ˆαm+1(x, fm), ˆηm+1(x, fm) ) , where ˆαm+1(x, fm) = µ⊤ m+1φθ m+1 ([x; fm]) and ˆηm+1(x, fm) = ∥L⊤ m+1φθ m+1 ([x; fm])∥2. Note that we again use fm+1 and fm to denote fm+1(x) and fm(x) for convenience. Next, we follow the same method as in Section 4.1 to sequentially obtain the c onditional posterior for each higher ﬁdelity, p(fm+k|fm, D)(1 < k ≤ M − m). In more detail, we ﬁrst base on q(wm+k) to derive the conditional moments E(fm+k|fm+k−1, fm, D) and E(f2 m+k|fm+k−1, fm, D). They are calculated in the same way as in (6), because fm+k are independent to fm conditioned on fm+k−1. Then we take the expectation of the conditional moments w .r.t p(fm+k−1|fm, D) (that is Gaussian) to obtain E(fm+k|fm, D) and E(f2 m+k|fm, D). This again can be done by Gauss-Hermite quadrature. Finally, we use these moments to construct a Gaussian approx imation to the conditional posterior, p(fm+k|fm, D) ≈ N ( fm+k|ˆαm+k(x, fm), ˆηm+k(x, fm) ) , (10) where ˆαm+k(x, fm) = E(fm+k|fm, D) and ˆηm+k(x, fm) = E(f2 m+k|fm, D) − E(fm+k|fm, D)2. According to Lemma 4.1, we guarantee ˆηm+k(x, fm) > 0. Now we can obtain p(fm(x)|fM (x) ≤ f∗, D) ≈ 1 Z · N ( fm|αm(x), ηm(x) ) Φ ( f∗ − ˆαM (x, fm)√ ˆηM (x, fm) ) . (11) 5In order to compute the entropy analytically, we use moment m atching again to approximate this distribution as a Gaussian distribution. T o this end, w e use Gauss-Hermite quadrature to compute three integrals, Z = ∫ R(fm) · N ( fm|αm(x), ηm(x) ) dfm, Z1 = ∫ fmR(fm) · N ( fm|αm(x), ηm(x) ) dfm, and Z2 = ∫ f2 mR(fm) · N ( fm|αm(x), ηm(x) ) dfm, where R(fm) = Φ ( (f∗ − ˆαM (x, fm))/ √ ˆηM (x, fm) ) . Then we can obtain E[fm|fM ≤ f∗, D] = Z1/Z and E[f2 m|fM ≤ f∗, D] = Z2/Z, based on which we approximate p(fm(x)|fM (x) ≤ f∗, D) ≈ N ( fm|Z1/Z, Z2/Z − Z2 1 /Z2) . (12) Following the same idea to prove Lemma 4.1, we can show that th e variance is non-negative. See the details in the supplementary material (Sec. 5). With the Gaussian form, we can analytically compute the entropy, H(fm(x)|fM (x) ≤ f∗, D) = 1 2 log ( 2πe(Z2/Z − Z2 1 /Z2) ) . Although our calculation of the acquisition function is qui te complex, due to the analytical form, we can use automatic differentiation libraries (Baydin et al. , 2017), to compute the gradient efﬁciently and robustly for optimization. In our experiments, we used T ensorFlow (Abadi et al., 2016) and L-BFGS to maximize the acquisition function to ﬁnd the ﬁdeli ty and input location we query at in the next step. Our multi-ﬁdelity Bayesian optimization alg orithm is summarized in Algorithm 1. Algorithm 1 DNN-MFBO ( D, M, T , {λm}M m=1 ) 1: Learn the DNN-based multi-ﬁdelity model (4) on Dwith stochastic variational learning. 2: for t = 1, . . . , T do 3: Generate F∗ from the variational posterior q(W) and the NN output at ﬁdelity M, i.e., fM (x) 4: (xt, m t) = argmaxx∈X ,1≤ m≤ M MutualInfo(x, m, λ m, F∗ , D, M ) 5: D←D∪{ (xt, m t)} 6: Re-train the DNN-based multi-ﬁdelity model on D 7: end for Algorithm 2 MutualInfo(x, m, λm, F∗, D, M) 1: Compute each p(fm(x)|D) ≈N ( fm|α m(x), η m(x) ) (Sec. 4.1) 2: H0 ← 1 2 log(2πeη m(x)), H1 ← 0 3: for f∗ ∈F ∗ do 4: if m = M then 5: Use (8) to compute H(fm|fM ≤ f∗ , D) and add it to H1 6: else 7: Compute p(fm(x)|fM (x), D) following (10) and p(fm(x)|fM (x) ≤ f∗ , D) with (12) 8: H1 ← H1 + 1 2 log ( 2πe (Z2/Z − Z2 1 /Z 2) ) 9: end if 10: end for 11: return (H0 − H1/ |F∗ |)/λ m 5 Related W ork Most surrogate models used in Bayesian optimization (BO) (Mockus, 2012; Snoek et al., 2012) are based on Gaussian processes (GPs) (Rasmussen and Williams, 2006), partly because their closed- form posteriors (Gaussian) are convenient to quantify the u ncertainty and calculate the acquisition functions. However, GPs are known to be costly for training, and the exact inference takes O(N3) time complexity ( N is the number of samples). Recently, Snoek et al. (2015) show ed deep neural networks (NNs) can also be used in BO and performs very well. T he training of NNs are much more efﬁcient ( O(N)). T o conveniently quantify the uncertainty, Snoek et al. (2 015) consider the NN weights in the output layer as random variables and all the ot her weights as hyper-parameters (like the kernel parameters in GPs). They ﬁrst obtain a point estim ation of the hyper-parameters (typically through stochastic training). Then they ﬁx the hyper-param eters and compute the posterior distribu- tion of the random weights (in the last layer) and NN output — t his can be viewed as the inference for Bayesian linear regression. In our multi-ﬁdelity model , we also only consider the NN weights in the output layer of each ﬁdelity as random variables. Howe ver, we jointly estimate the hyper- parameters and posterior distribution of the random weight s. Since the NN outputs in successive ﬁdelities are coupled non-linearly, we use the variational estimation framework (W ainwright et al., 2008). 6Many multi-ﬁdelity BO algorithms have been proposed. For ex ample, Huang et al. (2006); Lam et al. (2015); Picheny et al. (2013) augmented the standa rd EI for the multi-ﬁdelity settings. Kandasamy et al. (2016, 2017) extended GP upper conﬁdence bo und (GP-UCB) (Srinivas et al., 2010). Poloczek et al. (2017); Wu and Frazier (2017) develop ed multi-ﬁdelity BO with knowledge gradients (Frazier et al., 2008). EI is a local measure of the utility and UCB requires us to explicitly tune the exploit-exploration trade-off. The recent works a lso extend the information-based acqui- sition functions to enjoy a global utility for multi-ﬁdelit y optimization, e.g., (Swersky et al., 2013; Klein et al., 2017) using entropy search (ES), (Zhang et al., 2017; McLeod et al., 2017) (PES) using predictive entropy search (PES), and (Song et al., 2019; T ak eno et al., 2019) using max-value en- tropy search (MES). Note that ES and PES are computationally more expensive than MES because the former calculate the entropy of the input (vector) and la tter the output scalar. Despite the great success of the existing methods, they either ignore or overs implify the complex correlations across the ﬁdelities, and hence might hurt the accuracy of the surro gate model and further the optimiza- tion performance. For example, Picheny et al. (2013); Lam et al. (2015); Kandasamy et al. (2016); Poloczek et al. (2017) train an independent GP for each ﬁdeli ty; Song et al. (2019) combined all the examples indiscriminately to train a single GP; Huang et al. (2006); T akeno et al. (2019) assume a linear correlation structure between ﬁdelities, and Zhang et al. (2017) used the convolution opera- tion to construct the covariance and so the involved kernels have to be simple and smooth enough (yet less expressive) to obtain an analytical form. T o overc ome these limitations, we propose an NN-based multi-ﬁdelity model, which is ﬂexible enough to ca pture arbitrarily complex relation- ships between the ﬁdelities and to promote the performance o f the surrogate model. Recently, a NN-based multi-task model (Perrone et al., 2018) was also de veloped for BO and hyper-parameter transfer learning. The model uses an NN to construct a shared feature map ( i.e., bases) across the tasks, and generates the output of each task by a linear combi nation of the latent features. While this model can also be used for multi-ﬁdelity BO (each task co rresponds to one ﬁdelity), it views each ﬁdelity as symmetric and does not reﬂect the monotonici ty of function accuracy/importance along with the ﬁdelities. More important, the model does not capture the correlation between ﬁdeli- ties — given the shared bases, different ﬁdelities are assum ed to be independent. Finally, while a few algorithms deal with continuous ﬁdelities, e.g., (Kandasamy et al., 2017; McLeod et al., 2017; Wu and Frazier, 2017), we focus on discrete ﬁdelities in this work. 6 Experiment 6.1 Synthetic Benchmarks W e ﬁrst evaluated DNN-MFBO in three popular synthetic bench mark tasks. (1) Branin func- tion (Forrester et al., 2008; Perdikaris et al., 2017) with t hree ﬁdelities. The input is two dimensional and ranges from [−5, 10] × [0, 15]. (2) P ark1function (Park, 1991) with two ﬁdelities. The input is four dimensional and each dimension is in [0, 1]. (3) Levy function (Laguna and Martí, 2005), having three ﬁdelities and two dimensional inputs. The doma in is [−10, 10] × [−10, 10]. For each objective function, between ﬁdelities can be nonlinear and /or nonstationary transformations. The detailed deﬁnitions are given in the supplementary materia l (Sec. 1). Competing Methods. W e compared with the following popular and state-of-the-ar t multi- ﬁdelity BO algorithms: (1) Multi-Fidelity Sequential Krig ing (MF-SKO) (Huang et al., 2006) that models the function of the current ﬁdelity as the functi on of the previous ﬁdelity plus a GP , (2) MF-GP-UCB (Kandasamy et al., 2016), (3) Multi- Fidelity Predictive Entropy Search (MF-PES) (Zhang et al., 2017) and (4) Multi-Fidelity Maximum Entropy Search (MF- MES) (T akeno et al., 2019). These algorithms extend the stan dard BO with EI, UCB, PES and MES principles respectively. W e also compared with (5) mult i-task NN based BO (MTNN-BO) by Perrone et al. (2018), where a set of latent bases (generat ed by an NN) are shared across the tasks, and the output of each task ( i.e., ﬁdelity) is predicted by a linear combination of the bases. W e tested the single ﬁdelity BO with MES, named as (6) SF-MES (W a ng and Jegelka, 2017). SF-MES only queries the objective at the highest ﬁdelity. Settings and Results.W e implemented our method and MTNN-BO with T ensorFlow . W e used the original Matlab implementatio n for MF-GP-UCB ( https://github.com/kirthevasank/mf-gp-ucb), MF-PES ( https://github.com/YehongZ/MixedTypeBO) and SF- MES ( https://github.com/zi-w/Max-value-Entropy-Search/ ), and 7500 1000 1500 2000 2500 Total Cos  10−1 100 101 102 103 Simple Regre  DNN -MFBO MF-MES MF-PES MF-SKO MF-GP-UCB SF-MES MTNN-BO (a) Branin 25 50 75 100 125 150 175 200 Total Cost 10−6 10−4 10−2 100 (b) P ark1 500 1000 1500 2000 2500 Total Cost 10 0 10 1 10 2 (c) Levy 100 200 300 400 500 Total Cost 170 180 190190 200 210 220Queried Maximum (d) V ibration Plate 500 1000 1500 2000 2500 Total Cost 10 −2 10 −1 10 0 10 1 10 2 10 3 Inference Regret (e) Branin 25 50 75 100 125 150 175 200 Total Cost 10−6 10−3 10−1 100 101 (f) P ark1 500 1000 1500 2000 2500 Total Cost 10−2 10−1 100 101 102 (g) Levy 100 200 300 400 500 Total cost 1.05 1.10 1.15 1.20 1.25 1.3 × 100 1.35 × 100 1.4 × 100 Queried Minimum (h) Thermal Conductor Figure 1: Simple and Inference regrets on three synthetic benchmark t asks (a-c, e-g) and the optimum queried function values (d, h) along with the query cost. Python/Numpy implementation for MF-MES. MF-SKO was implemented with Python as well. W e used the default settings in their implementations . SF-MES and MF-GP-UCB used the Squared Exponential (SE) kernel. MF-PES used the Automa tic Relevance Determination (ARD) kernel. MF-MES and MF-SKO used the Radial Basis (RBF) k ernel (within each ﬁdelity). For DNN-MFBO and MTNN-BO, we used ReLU activation. T o identi fy the architecture of the neural network in each ﬁdelity and learning rate, we ﬁrst ran the AutoML tool SMAC3 (https://github.com/automl/SMAC3) on the initial training dataset (we randomly split the data into half for training and the other half for test, an d repeated multiple times to obtain a cross-validation accuracy to guide the search) and then man ually tuned these hyper-parameters. The depth and width of each network were chosen from [2, 12] and [32, 512], and the learning rate [10−5, 10−1]. W e used ADAM (Kingma and Ba, 2014) for stochastic training. The number of epochs was set to 5, 000, which is enough for convergence. T o optimize the acquisiti on function, MF-MES and MF-PES ﬁrst run a global optimization algorithm D IRECT (Jones et al., 1993; Gablonsky et al., 2001) and then use the results as the initia lization to run L-BFGS. SF-MES uses a grid search ﬁrst and then runs L-BFGS. DNN-MFBO and MTNN-BO d irectly use L-BFGS with a random initialization. T o obtain the initial training poin ts, we randomly query in each ﬁdelity. For Branin and Levy, we generated 20, 20 and 2 training samples for the ﬁrst, second and third ﬁdelity, respectively. For P ark1, we generated 5 and 2 examples for the ﬁrst and second ﬁdelity. The query costs is (λ1, λ2, λ3) = (1 , 10, 100). W e examined the simple regret (SR) and inference regret (IR ). SR is deﬁned as the difference between the global optimum and the best queried function value so far: maxx∈X fM (x) − maxi∈{i|i∈[t],mi=M} fM (xi); IR is the difference between the global optimum and the optimum estimated by the surrogate model: maxx∈X fM (x) − maxx∈X ˆfM (x) where ˆfM (·) is the estimated objective. W e repeated the experiment for ﬁ ve times, and report on average how the simple and inference regrets vary along with the query cost in Fig. 1 (a-c, e-g). W e also show the standard error bars. As we can see, in all the t hree tasks, DNN-MFBO achieves the best regrets with much smaller or comparable querying co sts. The best regrets obtained by our method are much smaller (often orders of magnitude) than the baselines. In particular, DNN-MFBO almost achieved the global optimum after querying one point (IR < 10−6) (Fig. 1f). These results demonstrate our DNN based surrogate model is more ac curate in estimating the objective. Furthermore, our method spends less or comparable cost to ac hieve the best regrets, showing a much better beneﬁt/cost ratio. 6.2 Real-W orld Applications in Engineering Design Mechanical Plate Vibration Design.W e aim to optimize three material properties, Y oung’s modu- lus (in [1 × 1011, 5 × 1011]), Poisson’s ratio (in [0.2, 0.6]) and mass density (in [6 × 103, 9 × 103]), to maximize the fourth vibration mode frequency of a 3-D simp ly supported, square, elastic plate, of size 10 × 10 × 1. T o evaluate the frequency, we need to run a numerical solver on the discretized 8DNN-MFBO MF-MES MF-GP-UCB MF-PESMF-SKOMTNN-BO 0 50 100 150 200Time (seconds) (a) Branin DNN-MFBO MF-MES MF-GP-UCB MF-PESMF-SKOMTNN-BO 0 50 100 150 200 250Time (seconds) (b) P ark1 DNN-MFBO MF-MES MF-GP-UCB MF-PESMF-SKOMTNN-BO 0 50 100 150 200Time (seconds) (c) Levy DNN-MFBO MF-MES MF-GP-UCB MF-PESMF-SKOMTNN-BO 0 50 100 150 200Time (seconds) (d) V ibration Plate DNN-MFBO MF-MES MF-GP-UCB MF-PESMF-SKOMTNN-BO 0 50 100 150 200Time (seconds) (e) Heat Conductor Figure 2: The average query time on three synthetic tasks (a- c) and two real-world applications (d-e). plate. W e considered two ﬁdelities, one with a coarse mesh an d the other a dense mesh. The details about the settings of the solvers are provided the supplemen tary document. Thermal Conductor Design. Given the property of a particular thermal conductor, our go al is to optimize the shape of the central hole where we install/ﬁx th e conductor to make the heat conduction (from left to right) to be as as fast as possible. The shape of t he hole (an ellipse) is described by three parameters: x-radius, y-radius and angle. W e used the time to reach 70 degrees as the objective function value and we want to minimize the objective. W e need to run numerical solvers to calculate the objective. W e considered two ﬁdelities. The details are given in the supplementary material. For both problems, we randomly queried at 20 and 5 inputs in th e low and high ﬁdelities respectively, at the beginning. The query cost is (λ1, λ2) = (1 , 10). W e then ran each algorithm until convergence. W e repeated the experiments for ﬁve times. Since we do not kno w the ground-truth of the global optimum, we report how the average of the best function value s queried improves along with the cost. The results are shown in Fig. 1d and h. As we can see, in bo th applications, DNN-MFBO reaches the maximum/minimum function values with a smaller cost than all the competing methods, which is consistent with results in the synthetic benchmark tasks. Finally, we examined the average query time of each multi-ﬁd elity BO method, which is spent in calculating and optimizing the acquisition function to ﬁnd new inputs and ﬁdelities to query at in each step. For a fair comparison, we ran all the methods on a Li nux workstation with a 16-core Intel(R) Xeon(R) CPU E5-2670 and 16GB RAM. As shown in Fig. 2, DNN-MFBO spends much less time than MF-MES and MF-PES that are based on multi-outp ut GPs, and the speed of DNN- MFBO is close or comparable to MF-GP-UCB and MF-SKO, which us e independent and additive GPs for each ﬁdelity, respectively. On average, DNN-MFBO ac hieves 25x and 60x speedup over MF-MES and MF-PES. One reason might be that DNN-MFBO simply a dopts a random initializa- tion for L-BFGS rather than runs an expensive global optimiz ation (so does MTNN-BO). However, as we can see from Fig. 1, DNN-MFBO still obtains new input and ﬁdelities that achieve much better beneﬁt/cost ratio. On the other hand, the close speed to MF-GP-UCB and MF-SKO also demonstrate that our method is efﬁcient in acquisition func tion calculation, despite its seemingly complex approximations. 7 Conclusion W e have presented DNN-MFBO, a deep neural network based mult i-ﬁdelity Bayesian optimization algorithm. Our DNN surrogate model is ﬂexible enough to capt ure the strong and complicated relationships between ﬁdelities and promote objective est imation. Our information based acquisition function not only enjoys a global utility measure, but also i s computationally tractable and efﬁcient. Acknowledgments This work has been supported by DARP A TRADES A ward HR0011-17 -2-0016 and NSF IIS- 1910983. Broader Impact This work can be used in a variety of engineering design probl ems that involve intensive computa- tion, e.g., ﬁnite elements or differences. Hence, the work has potentia l positive impacts in the society if it is used to design passenger aircrafts, biomedical devi ces, automobiles, and all the other devices 9or machines that can beneﬁt human lives. At the same time, thi s work may have some negative consequences if it is used to design weapons or weapon parts. References Abadi, M., Barham, P ., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M., Ghemawat, S., Irving, G., Isard, M., et al. (2016). T ensorﬂow: A system for large-s cale machine learning. In 12th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 16) , pages 265–283. Baydin, A. G., Pearlmutter, B. A., Radul, A. A., and Siskind, J. M. (2017). Automatic differentiation in machine learning: a survey. The Journal of Machine Learning Research, 18(1):5595–5637. Forrester, A., Sobester, A., and Keane, A. (2008). Engineering design via surrogate modelling: a practical guide. John Wiley & Sons. Frazier, P . I., Powell, W . B., and Dayanik, S. (2008). A knowl edge-gradient policy for sequential information collection. SIAM Journal on Control and Optimization, 47(5):2410–2439. Gablonsky, J. M. et al. (2001). Modiﬁcations of the DIRECT Algorithm. PhD thesis. Hennig, P . and Schuler, C. J. (2012). Entropy search for info rmation-efﬁcient global optimization. Journal of Machine Learning Research, 13(Jun):1809–1837. Hernández-Lobato, J. M., Hoffman, M. W ., and Ghahramani, Z. (2014). Predictive entropy search for efﬁcient global optimization of black-box functions. I n Advances in neural information processing systems, pages 918–926. Huang, D., Allen, T . T ., Notz, W . I., and Miller, R. A. (2006). Sequential kriging optimization using multiple-ﬁdelity evaluations. Structural and Multidisciplinary Optimization, 32(5):369–382. Incropera, F . P ., Lavine, A. S., Bergman, T . L., and DeWitt, D . P . (2007). Fundamentals of heat and mass transfer. Wiley. Jones, D. R., Perttunen, C. D., and Stuckman, B. E. (1993). Li pschitzian optimization without the lipschitz constant. Journal of optimization Theory and Applications, 79(1):157–181. Jones, D. R., Schonlau, M., and W elch, W . J. (1998). Efﬁcient global optimization of expensive black-box functions. Journal of Global optimization, 13(4):455–492. Kandasamy, K., Dasarathy, G., Oliva, J. B., Schneider, J., a nd Póczos, B. (2016). Gaussian process bandit optimisation with multi-ﬁdelity evaluations. In Advances in Neural Information Processing Systems, pages 992–1000. Kandasamy, K., Dasarathy, G., Schneider, J., and Póczos, B. (2017). Multi-ﬁdelity bayesian optimi- sation with continuous approximations. In Proceedings of the 34th International Conference on Machine Learning-V olume70, pages 1799–1808. JMLR. org. Kingma, D. P . and Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980. Kingma, D. P . and W elling, M. (2013). Auto-encoding variati onal bayes. arXiv preprint arXiv:1312.6114. Klein, A., Falkner, S., Bartels, S., Hennig, P ., and Hutter, F . (2017). Fast bayesian optimization of machine learning hyperparameters on large datasets. In Artiﬁcial Intelligence and Statistics, pages 528–536. Laguna, M. and Martí, R. (2005). Experimental testing of adv anced scatter search designs for global optimization of multimodal functions. Journal of Global Optimization, 33(2):235–255. Lam, R., Allaire, D. L., and Willcox, K. E. (2015). Multiﬁdel ity optimization using statistical surrogate modeling for non-hierarchical information sour ces. In 56th AIAA/ASCE/AHS/ASC Structures, Structural Dynamics, and Materials Conference, page 0143. 10Liu, D. C. and Nocedal, J. (1989). On the limited memory bfgs m ethod for large scale optimization. Mathematical programming, 45(1-3):503–528. McLeod, M., Osborne, M. A., and Roberts, S. J. (2017). Practi cal bayesian optimization for variable cost objectives. arXiv preprint arXiv:1703.04335. Minka, T . P . (2001). Expectation propagation for approxima te bayesian inference. In Proceedings of the Seventeenth conference on Uncertainty in artiﬁcial intelligence, pages 362–369. Mockus, J. (2012). Bayesian approach to global optimization: theory and applications, volume 37. Springer Science & Business Media. Mockus, J., Tiesis, V ., and Zilinskas, A. (1978). The applic ation of Bayesian methods for seeking the extremum. T owards global optimization, 2(117-129):2. Park, J. S. (1991). Tuning complex computer codes to data and optimal designs. Peherstorfer, B., Willcox, K., and Gunzburger, M. (2018). S urvey of multiﬁdelity methods in uncer- tainty propagation, inference, and optimization. Siam Review, 60(3):550–591. Perdikaris, P ., Raissi, M., Damianou, A., Lawrence, N., and Karniadakis, G. E. (2017). Nonlinear in- formation fusion algorithms for data-efﬁcient multi-ﬁdel ity modelling. Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences, 473(2198):20160751. Perrone, V ., Jenatton, R., Seeger, M. W ., and Archambeau, C. (2018). Scalable hyperparameter transfer learning. In Advances in Neural Information Processing Systems, pages 6845–6855. Picheny, V ., Ginsbourger, D., Richet, Y ., and Caplin, G. (20 13). Quantile-based optimization of noisy computer experiments with tunable precision. T echnometrics, 55(1):2–13. Poloczek, M., W ang, J., and Frazier, P . (2017). Multi-infor mation source optimization. In Advances in Neural Information Processing Systems, pages 4288–4298. Rasmussen, C. E. and Williams, C. K. I. (2006). Gaussian Processes for Machine Learning. MIT Press. Snoek, J., Larochelle, H., and Adams, R. P . (2012). Practica l bayesian optimization of machine learning algorithms. In Advances in neural information processing systems, pages 2951–2959. Snoek, J., Rippel, O., Swersky, K., Kiros, R., Satish, N., Su ndaram, N., Patwary, M., Prabhat, M., and Adams, R. (2015). Scalable bayesian optimization us ing deep neural networks. In International conference on machine learning, pages 2171–2180. Song, J., Chen, Y ., and Y ue, Y . (2019). A general framework fo r multi-ﬁdelity bayesian optimization with gaussian processes. In The 22nd International Conference on Artiﬁcial Intelligence and Statistics, pages 3158–3167. Srinivas, N., Krause, A., Kakade, S., and Seeger, M. (2010). Gaussian process optimization in the bandit setting: no regret and experimental design. In Proceedings of the 27th International Conference on International Conference on Machine Learning, pages 1015–1022. Swersky, K., Snoek, J., and Adams, R. P . (2013). Multi-task b ayesian optimization. In Advances in neural information processing systems, pages 2004–2012. T akeno, S., Fukuoka, H., Tsukada, Y ., Koyama, T ., Shiga, M., T akeuchi, I., and Karasuyama, M. (2019). Multi-ﬁdelity bayesian optimization with max-val ue entropy search. arXiv preprint arXiv:1901.08275. W ainwright, M. J., Jordan, M. I., et al. (2008). Graphical mo dels, exponential families, and varia- tional inference. Foundations and Trends® in Machine Learning, 1(1–2):1–305. W ang, Z. and Jegelka, S. (2017). Max-value entropy search fo r efﬁcient bayesian optimization. In Proceedings of the 34th International Conference on Machine Learning-V olume 70 , pages 3627– 3635. JMLR. org. 11Wu, J. and Frazier, P . I. (2017). Continuous-ﬁdelity bayesi an optimization with knowledge gradient. In NIPS W orkshop on Bayesian Optimization. Zhang, Y ., Hoang, T . N., Low , B. K. H., and Kankanhalli, M. (20 17). Information-based multi- ﬁdelity bayesian optimization. In NIPS W orkshop on Bayesian Optimization. Zienkiewicz, O. C., T aylor, R. L., Zienkiewicz, O. C., and T a ylor, R. L. (1977). The ﬁnite element method, volume 36. McGraw-hill London. 12Supplementary Material . . .x x x f1(x) f2(x) fM (x) Figure 3: Graphical representation of the DNN based multi-ﬁdelity su rrogate model. The output in each ﬁdelity fm(x) (1 ≤ m ≤ M) is fulﬁlled by a (deep) neural network. 1 Deﬁnitions of Synthetic Benchmark Functions In the experiments, we used three synthetic benchmark tasksto evaluate our method. The deﬁnitions of the objective functions are given as follows. 1.1 Branin Function The input is two dimensional, x = [ x1, x2] ∈ [−5, 10] × [0, 15]. W e have three ﬁdelities to query the function, which, from high to low , are given by f3(x) = − ( −1.275x2 1 π2 + 5x1 π + x2 − 6 ) 2 − ( 10 − 5 4π ) cos(x1) − 10, f2(x) = −10 √ −f3(x − 2) − 2(x1 − 0.5) + 3(3 x2 − 1) + 1 , f1(x) = −f2 ( 1.2(x + 2) ) + 3x2 − 1. (13) W e can see that between ﬁdelities are nonlinear transformat ions and non-uniform scaling and shifts. The global maximum is -0.3979 at (−π, 12.275), (π, 2.275) and (9.425, 2.475). 1.2 Park1 Function The input is four dimensional, x = [ x1, x2, x3, x4] ∈ [0, 1]4. W e have two ﬁdelities, f2(x) = x1 2 [ √ 1 + ( x2 + x2 3)x4 x2 1 − 1 ] + (x1 + 3x4) exp[1 + sin( x3)], f1(x) = [ 1 + sin(x1) 10 ] f2(x) − 2x1 + x2 2+ x2 3+ 0.5. (14) The global maximum is at 25.5893 at (1.0, 1.0, 1.0, 1.0). 1.3 Levy Function The input is two dimensional, x = [ x1, x2] ∈ [−10, 10]2. The query has three ﬁdelities, f3(x) = − sin2(3πx1) − (x1 − 1)2[1 + sin 2(3πx2)] − (x2 − 1)2[1 + sin 2(2πx2)], f2(x) = − exp(0.1 · √ −f3(x)) − 0.1 · √ 1 + f2 3 (x), f1(x) = − √ 1 + f2 3 (x). (15) The global maximum is 0.0 at (1.0, 1.0). 2 Details of Real-W orld Applications 2.1 Mechanical Plate Vibration Design In this application, we want to make a 3-D simply supported, square, elastic plate, of size 10×10×1, as shown in Fig. 4. The goal is to ﬁnd materials that can maximi ze the fourth vibration mode 13frequency (so as to avoid resonance with other parts which ca uses damages). The materials are parameterized by three properties, Y oung’s modulus (in [1 × 1011, 5 × 1011]), Poisson’s ratio (in [0.2, 0.6]) and mass density (in [6 × 103, 9 × 103]). T o compute the frequency, we discretize the plate with quadr atic tetrahedral elements (see Fig. 4). W e consider two ﬁdelities. The low-ﬁdelity solution is o btained from setting a maximum mesh edge length to 1.2, while the high-ﬁdelity 0.6. W e then use the ﬁnite ﬁnite element method (Zienkiewicz et al., 1977) to solve for the ﬁrst 4th vibratio n mode and compute the frequency as our objective. Figure 4: The plate discretized with quadratic tetrahedral elements (the maximum mesh edge length is 1. 2). 2.2 Thermal Conductor Design In the second application, we consider the design of a thermal conductor, shown in Fig. 5a. The heat source is on the left, where the temperature is zero at th e beginning and ramps to 100 degrees in 0.5 seconds. The heat runs through the conductor to the right end . The size and properties of the conductor are ﬁxed: the thermal conductivity and mass densi ty are both 1. W e need to bore a hole in the centre to install the conductor. The edges on the top, bot tom and inside the hole are all insulated, i.e., no heat is transferred across these edges. Note that the size and the angle of the hole determine the speed of the heat transfusion. The hole in general is an el lipse, described by three parameters, x-radius, y-radius and angle. The goal is to make the heat con duction (from left to right) as fast as possible. Hence, we use the time to reach 70 degrees on the r ight end as the objective function value. T o compute the time, we discretize the conductor with quadratic tetrahedral elements, and apply the ﬁnite element methods to solve a transient heat tra nsfer problem (Incropera et al., 2007) to obtain a response heat curve on the right edge. An example is g iven in Fig. 5b. The response curve is a function of time, from which we can calculate when the tem perature reaches 70 degrees. W e consider queries of two ﬁdelities. The low ﬁdelity queries a re computed with the maximum mesh edge length being 0.8 in solving the heat transfer problem; t he high ﬁdelity queries are computed with the maximum mesh edge length being 0.2. 3 Details of Stochastic V ariational Learning W e develop a stochastic variational learning algorithm to j ointly estimate the posterior of W = {wm} — the NN weights in the output layer in each ﬁdelity, and the hy perparameters, including all the other NN weights Θ = {θm} and noise variance s = [ σ2 1 , . . . , σ 2 M ]⊤. T o this end, we assume q(W) = ∏ M m=1 q(wm) where each q(wm) = N (wm|µm, Σ m). W e parameterize Σ m with its Cholesky decomposition to ensure the positive deﬁniteness , Σ m = LmL⊤ mwhere Lm is a lower triangular matrix. W e then construct a variational model ev idence lower bound (ELBO) from the 14-0.5 0 0.5 -0.8 -0.6 -0.4 -0.2 0 0.2 0.4 0.6 88 90 92 94 96 98 100 (a) Conductor 0 1 2 3 4 5 Time (seconds) -20 0 20 40 60 80 100Temperature (degrees-Celsius) (b) Heat Response Curve Figure 5: The thermal conductor with one transient heat solution (a), and the heat responsive curve on the right edge (b). The white triangles in (a) are the ﬁnite eleme nts used to discretize the conductor to compute the solution. joint probability of our model (see (4) of the main paper), L ( q(W), Θ , s ) = Eq [ log(p(W, Y|X , Θ , s) q(W) ] = − M∑ m=1 KL ( q(wm)∥p(wm) ) + M∑ m=1 Nm∑ n=1 Eq [ log ( N (ynm|fm(xnm), σ2 m) )] , (16) where p(wm) = N (wm|0, I) and KL (·∥·) is the Kullback Leibler divergence. W e maximize L to estimate q(W), Θ and s jointly. However, since the NN outputs fm(·) in each ﬁdelity are coupled in a highly nonlinear way (see (3) of the main paper), the expect ation terms in L is analytical intractable. T o address this issue, we apply stochastic optimization. Sp eciﬁcally, we use the reparameterization trick (Kingma and W elling, 2013) and for each wm generate parameterized samples from their variational posterior, ˆwm = µm + Lmǫ where ǫ ∼ N (·|0, I). W e then substitute each sample ˆwm for wm in computing all log ( N (ynm|fm(xnm), σ2 m) ) in (16) and remove the expectation in front of them. W e therefore obtain ˆL, an unbiased estimate of ELBO, which is analytically tracta ble. Next, we compute ∇ ˆL, which is an unbiased estimate of the ∇L and hence can be used to maximize L. W e can use any stochastic optimization algorithm. 4 Proof of Lemma 4.1 Lemma 4.1.As long as the conditional posterior variance γ(fm−1, x) > 0, the posterior variance ηm(x), computed based on the quadrature in (7) of the main paper , ispositive. Proof. First, for brevity, we denote u(tk, x) and γ(tk, x) in (7) of the main paper by uk and γk, respectively. Then from the quadrature results, we compute the variance V ar(fm|D) = ∑ k gkγk + ∑ k gku2 k− ( ∑ k gkuk)2. 15Since γk > 0, the ﬁrst summation ∑ k gkγk > 0. Note that the quadrature weights have all gk > 0 and ∑ k gk = 1 . W e deﬁne ¯u = ∑ k gkuk. Next, we derive that ∑ k gku2 k− ( ∑ k gkuk)2 = ∑ k gku2 k− ¯u2 = ∑ k gku2 k+ ¯u2 − 2¯u2 = ∑ k gku2 k+ ∑ k gk ¯u2 − 2¯u2 = ∑ k gku2 k+ ∑ k gk ¯u2 − 2 ∑ k gkuk ¯u = ∑ k gk(u2 k+ ¯u2 − 2uk ¯u) = ∑ k gk(uk − ¯u)2 ≥ 0. (17) Therefore, V ar(fm|D) > 0. 5 Proof of Nonnegative V ariance in (12) of the Main Paper W e show the variance in (12) of the main paper, computed by qua drature, is non-negative. The proof is very similar to that of Lemma 4.1 (Section 4). W e denote the quadrature weights and nodes by {gk} and {tk}. Then we have Z = ∑ k gkR(tk), Z 1 = ∑ k gktkR(tk), Z 2 = ∑ k gkt2 kR(tk). (18) Therefore, Z1 Z = ∑ k tk gkR(tk)∑ j gjR(tj ) = ∑ k tkνk, Z2 Z = ∑ k t2 k gkR(tk) ∑ j gjR(tj ) = ∑ k t2 kνk (19) where νk = gk R(tk) ∑ j gj R(tj ) > 0 and ∑ k νk = 1 . Following the same derivation as in (17), we can immediately show that the variance Z2/Z − Z2 1 /Z2 = ∑ k νk(tk − ¯t)2 ≥ 0 where ¯t = Z1/Z = ∑ k tkνk. 16",
      "meta_data": {
        "arxiv_id": "2007.03117v4",
        "authors": [
          "Shibo Li",
          "Wei Xing",
          "Mike Kirby",
          "Shandian Zhe"
        ],
        "published_date": "2020-07-06T23:28:40Z",
        "pdf_url": "https://arxiv.org/pdf/2007.03117v4.pdf"
      }
    },
    {
      "title": "Efficient Hyperparameter Optimization with Adaptive Fidelity Identification"
    },
    {
      "title": "Batch Multi-Fidelity Bayesian Optimization with  Deep Auto-Regressive Networks",
      "abstract": "Bayesian optimization (BO) is a powerful approach for optimizing black-box,\nexpensive-to-evaluate functions. To enable a flexible trade-off between the\ncost and accuracy, many applications allow the function to be evaluated at\ndifferent fidelities. In order to reduce the optimization cost while maximizing\nthe benefit-cost ratio, in this paper, we propose Batch Multi-fidelity Bayesian\nOptimization with Deep Auto-Regressive Networks (BMBO-DARN). We use a set of\nBayesian neural networks to construct a fully auto-regressive model, which is\nexpressive enough to capture strong yet complex relationships across all the\nfidelities, so as to improve the surrogate learning and optimization\nperformance. Furthermore, to enhance the quality and diversity of queries, we\ndevelop a simple yet efficient batch querying method, without any combinatorial\nsearch over the fidelities. We propose a batch acquisition function based on\nMax-value Entropy Search (MES) principle, which penalizes highly correlated\nqueries and encourages diversity. We use posterior samples and moment matching\nto fulfill efficient computation of the acquisition function and conduct\nalternating optimization over every fidelity-input pair, which guarantees an\nimprovement at each step. We demonstrate the advantage of our approach on four\nreal-world hyperparameter optimization applications.",
      "full_text": "Batch Multi-Fidelity Bayesian Optimization with Deep Auto-Regressive Networks Shibo Li, Robert M. Kirby, and Shandian Zhe School of Computing, University of Utah Salt Lake City, UT 84112 shibo@cs.utah.edu, kirby@cs.utah.edu, zhe@cs.utah.edu Abstract Bayesian optimization (BO) is a powerful approach for optimizing black-box, expensive-to-evaluate functions. To enable a ﬂexible trade-off between the cost and accuracy, many applications allow the function to be evaluated at different ﬁdelities. In order to reduce the optimization cost while maximizing the beneﬁt- cost ratio, in this paper we propose Batch Multi-ﬁdelity Bayesian Optimization with Deep Auto-Regressive Networks (BMBO-DARN). We use a set of Bayesian neural networks to construct a fully auto-regressive model, which is expressive enough to capture strong yet complex relationships across all the ﬁdelities, so as to improve the surrogate learning and optimization performance. Furthermore, to enhance the quality and diversity of queries, we develop a simple yet efﬁcient batch querying method, without any combinatorial search over the ﬁdelities. We propose a batch acquisition function based on Max-value Entropy Search (MES) principle, which penalizes highly correlated queries and encourages diversity. We use posterior samples and moment matching to fulﬁll efﬁcient computation of the acquisition function, and conduct alternating optimization over every ﬁdelity-input pair, which guarantees an improvement at each step. We demonstrate the advantage of our approach on four real-world hyperparameter optimization applications. 1 Introduction Many applications demand we optimize a complex function of an unknown form that is expensive to evaluate. Bayesian optimization (Mockus, 2012; Snoek et al., 2012) is a powerful approach to optimize such functions. The key idea is to use a probabilistic surrogate model, typically Gaussian processes (Rasmussen and Williams, 2006), to iteratively approximate the target function, integrate the posterior information to compute and maximize an acquisition function so as to generate new inputs at which to query, update the model with new examples, and meanwhile approach the optimum. In practice, to enable a ﬂexible trade-off between the computational cost and accuracy, many appli- cations allow us to evaluate the target function at different ﬁdelities. For example, to evaluate the performance of the hyperparameters for a machine learning model, we can train the model thoroughly, i.e., with sufﬁcient iterations/epochs, to obtain the accurate evaluation (high-ﬁdelity yet often costly) or just run a few iterations/epochs to obtain a rough estimate (low-ﬁdelity but much cheaper). Many multi-ﬁdelity BO algorithms (Lam et al., 2015; Kandasamy et al., 2016; Zhang et al., 2017; Song et al., 2019; Takeno et al., 2019) have therefore been proposed to identify both the ﬁdelities and inputs at which to query, so as to reduce the cost and achieve a good beneﬁt-cost balance. Notwithstanding their success, these methods often overlook the strong yet complex relationships between different ﬁdelities or adopt an over-simpliﬁed assumption, (partly) for the sake of convenience in calculating/maximizing the acquisition function considering ﬁdelities. This, however, can restrict the performance of the surrogate model, impair the optimization efﬁciency and increase the cost. For 35th Conference on Neural Information Processing Systems (NeurIPS 2021). arXiv:2106.09884v2  [cs.LG]  25 Oct 2021example, Lam et al. (2015); Kandasamy et al. (2016) learned an independent GP for each ﬁdelity, Zhang et al. (2017) used multitask GPs with a convolved kernel for multi-ﬁdelity modeling and have to use a simple smoothing kernel (e.g., Gaussian) for tractable convolutions. The recent work (Takeno et al., 2019) imposes a linear correlation across different ﬁdelities. In addition, the standard one-by- one querying strategy needs to sequentially run each query and cannot utilize parallel computing resources to accelerate, e.g., multi-core CPUs/GPUs and clusters. While incrementally absorbing more information, it does not explicitly account for the correlation between different queries, hence still has a risk to bring in highly correlated examples that includes redundant information. To address these issues, we propose BMBO-DARN, a novel batch multi-ﬁdelity Bayesian optimization method. First, we develop a deep auto-regressive model to integrate training examples at various ﬁdelities. Each ﬁdelity is modeled by a Bayesian neural network (NN), where the output predicts the objective function value at that ﬁdelity and the input consists of the original inputs and the outputs of all the previous ﬁdelities. In this way, our model is adequate to capture the complex, strong correlations ( e.g., nonstationary, highly nonlinear) across all the ﬁdelities to enhance the surrogate learning. We use Hamiltonian Monte-Carlo (HMC) sampling for posterior inference. Next, to improve the quality of the queries, we develop a simple yet efﬁcient method to jointly fetch a batch of inputs and ﬁdelities. Speciﬁcally, we propose a batch acquisition function based on the state-of-the-art Max-value Entropy Search (MES) principle (Wang and Jegelka, 2017). The batch acquisition function explicitly penalizes highly correlated queries and encourages diversity. To efﬁciently compute the acquisition function, we use the posterior samples of the NN weights and moment matching to construct a multi-variate Gaussian posterior for all the ﬁdelity outputs and the function optimum. To prevent a combinatorial search over multiple ﬁdelities in maximizing the acquisition function, we develop an alternating optimization algorithm to cyclically update each pair of input and ﬁdelity, which is much more efﬁcient and guarantees an improvement at each step. For evaluation, we examined BMBO-DARN in both synthetic benchmarks and real-world applications. The synthetic benchmark tasks show that given a small number of training examples, our deep auto- regressive model can learn a more accurate surrogate of the target function than other state-of-the-art multi-ﬁdelity BO models. We then evaluated BMBO-DARN on four popular machine learning models (CNN, online LDA, XGBoost and Physics informed NNs) for hyperparameter optimization. BMBO-DARN can ﬁnd more effective hyperparameters leading to superior predictive performance, and meanwhile spends smaller total evaluation costs, as compared with state-of-the-art multi-ﬁdelity BO algorithms and other popular hyperparameter tuning methods. 2 Background Bayesian Optimization (BO) (Mockus et al., 1978; Snoek et al., 2012) is a popular approach for optimizing black-box functions that are often costly to evaluate and cannot provide exact gradient information. BO learns a probabilistic surrogate model to predict the function value across the input space and quantiﬁes the predictive uncertainty. At each step, we use this information to compute an acquisition function to measure the utility of querying at different inputs. By maximizing the acquisition function, we ﬁnd the next input at which to query, which is supposed to be closer to the optimum. Then we add the new example into the training set to improve the accuracy of the surrogate model. The procedure is repeated until we ﬁnd the optimal input or the maximum number of queries have been ﬁnished. There are a variety of acquisition functions, such as Expected Improvement (EI) (Mockus et al., 1978) and Upper Conﬁdence Bound (UCB) (Srinivas et al., 2010). The recent state-of-the-art addition is Maximum-value Entropy Search (MES) (Wang and Jegelka, 2017), a(x) = I ( f(x),f∗|D ) , (1) where I(·,·) is the mutual information, f(x) is the objective function value at x, f∗the minimum, and Dthe training data collected so far for the surrogate model. Note that both f(x) and f∗are considered as generated by the posterior of the surrogate model given D; they are random variables. The most commonly used class of surrogate models is Gaussian process (GP) (Rasmussen and Williams, 2006). Given the training dataset X = [ x⊤ 1 ,..., x⊤ N]⊤and y = [ y1,...,y N]⊤, a GP assumes the outputs y follow a multivariate Gaussian distribution, p(y|X) = N(y|m,K + vI), where m is the mean function values at the inputs X, often set to 0, vis the noise variance, and K is a kernel matrix on X. Each [K]ij = κ(xi,xj), where κ(·,·) is a kernel function. For example, a popular one is the RBF kernel,κ(xi,xj) = exp ( −β−1∥xi −xj∥2) . An important advantage of GPs 2f1(θ) f2(θ) . . . f3(θ) fM (θ)θ Figure 1: Graphical representation of the deep auto-regressive model in BMBO-DARN. The output at each ﬁdelity fm(x) (1 ≤ m ≤ M) is calculated by a (deep) neural network. is their convenience in uncertainty quantiﬁcation. Since GPs assume any ﬁnite set of function values follow a multi-variate Gaussian distribution, given a test input ˆx, we can compute the predictive (or posterior) distribution p(f(ˆx)|ˆx,X,y) via a conditional Gaussian distribution, which is simple and analytical. Multi-Fidelity BO. Since evaluating the exact value of the object function is often expensive, many practical applications provide multi-ﬁdelity evaluations {f1(x),...,f M(x)}to allow us to choose a trade-off between the accuracy and cost. Accordingly, many multi-ﬁdelity BO algorithms have been developed to select both the inputs and ﬁdelities to reduce the cost and to achieve a good balance between the optimization progress and cost, i.e., the beneﬁt-cost ratio. For instance, MF- GP-UCB (Kandasamy et al., 2016) sequentially queries at each ﬁdelity (from the lowest one, i.e., m= 1) until the conﬁdence band is over a given threshold. In spite of its great success and guarantees in theory, MF-GP-UCB uses a set of independent GPs to estimate the objective at each ﬁdelity, and hence ignores the valuable correlations between different ﬁdelities. MF-PES (Zhang et al., 2017) uses a multi-task GP surrogate where each task corresponds to one ﬁdelity, and convolves a smoothing kernel with the kernel of a shared latent function to obtain the cross-covariance. The recent MF-MES (Takeno et al., 2019) also builds a multi-task GP surrogate, where the covariance function is κ(fm(x),fm′ (x′)) = ∑d j=1 (umjum′j + 1(m= m′) ·αmj) ρj(x1,x2), (2) where αmj >0, 1(·) is the indicator function, {umj}d j=1 is dlatent features for each ﬁdelity m, and {ρj(·,·)}are dbases kernels, usually chosen as a commonly used stationary kernel, e.g., RBF. 3 Deep Auto-Regressive Model for Multi-Fidelity Surrogate Learning Notwithstanding the elegance and success of the existing multi-ﬁdelity BO methods, they often ignore or oversimplify the complex, strong correlations between different ﬁdelities, and hence can be inefﬁcient for surrogate learning, which might further lower the optimization efﬁciency and incur more expenses. For example, the state-of-the-art methods MF-GP-UCB (Kandasamy et al., 2016) estimate a GP surrogate for each ﬁdelity independently; MF-PES (Zhang et al., 2017) has to adopt a simple form for both the smoothing and latent function kernel (e.g., Gaussian and delta) to achieve an analytically tractable convolution, which might limit the expressivity in estimating the cross-ﬁdelity covariance; MF-MES (Takeno et al., 2019) essentially imposes a linear correlation structure between different ﬁdelities — for any input x, κ(fm(x),fm′ (x)) = u⊤ m1 um2 + αm where um = [um1,...,u md] and ˜αm = ∑d j=1 αmj if we use a RBF basis kernel (see (2)). To overcome this limitation, we develop a deep auto-regressive model for multi-ﬁdelity surrogate learning. Our model is expressive enough to capture the strong, possibly very complex (e.g., highly nonlinear, nonstationary) relationships between all the ﬁdelities to improve the prediction (at the highest ﬁdelity). As such, our model can more effectively integrate multi-ﬁdelity training information to better estimate the objective function. Speciﬁcally, given M ﬁdelities, we introduce a chain of M neural networks, each of which models one ﬁdelity and predicts the target function at that ﬁdelity. Denote by xm, Wm, and ψWm(·) the NN input, parameters and output mapping at each ﬁdelity m. Our model is deﬁned as follows, xm = [x; f1(x); ... ; fm−1(x)], fm(x) = ψWm(xm), y m(x) = fm(x) + ϵm, (3) where x1 = x, fm(x) is the prediction (i.e., NN output) at the m-th ﬁdelity, ym(x) is the observed function value, and ϵm is a random noise, ϵm ∼N (ϵm|0,τ−1 m ). We can see that each input xm 3consists of not only the original input x of the objective function, but also the outputs from all the previous ﬁdelities. Via a series of linear projection and nonlinear activation from the NN, we obtain the output at ﬁdelity m. In this way, our model fully exploits the information from the lower ﬁdelities and can ﬂexibly capture arbitrarily complex relationships between the current and all the previous ﬁdelities by learning an NN mapping, fm(x) = ψWm ( xm,f1(x),...,f m−1(x) ) . We assign a standard Gaussian prior distribution over each element of the NN parameters W= {W1,..., WM}, and a Gamma prior over each noise precision, p(τm) = Gam(τm|a0,b0). Given the dataset D= {{(xnm,ynm)}Nm n=1}M m=1, the joint probability of our model is given by p(W,τ, Y,S|X) = N(vec(W)|0,I) M∏ m=1 Gam(τm|a0,b0) M∏ m=1 Nm∏ n=1 N ( ynm|fm(xnm),τ−1 m ) , (4) where τ = [ τ1,...,τ M], X = {xnm}, Y = {ynm}, and vec(·) is vectorization. The graphical representation of our model is given in Fig. 1. We use Hamiltonian Monte Carlo (HMC) (Neal et al., 2011) sampling to perform posterior inference due to its unbiased, high-quality uncertainty quantiﬁcation, which is critical to calculate the acquisition function. However, our method allows us to readily switch to other approximate inference approaches as needed (see Sec. 4), e.g., stochastic gradient HMC (Chen et al., 2014) used in the excellent work of Springenberg et al. (2016). 4 Batch Acquisition for Multi-Fidelity Optimization Given the posterior of our model, we aim to compute and optimize an acquisition function to identify the input and ﬁdelity at which to query next. Popular BO methods query at one input each time and then update the surrogate model. While successful, this one-by-one strategy has to run each query sequentially and cannot take advantage of parallel computing resources (that are often available in practice) to further accelerate, such as multi-core CPU and GPU workstations and computer clusters. In addition, the one-by-one strategy although gradually integrates more data information, it lacks an explicit mechanism to take into account the correlation across different queries, hence still has a risk to bring in highly correlated examples with redundant information, especially in the multi-ﬁdelity setting, e.g., querying at the same input with another ﬁdelity. To allow parallel query and to improve the query quality and diversity, we develop a batch acquiring approach to jointly identify a set of inputs and ﬁdelities at a time, presented as follows. 4.1 Batch Acquisition Function We ﬁrst propose a batch acquisition function based on the MES principle (Zhang et al., 2017) (see (1)). Denote by Bthe batch size and by {λ1,...,λ M}the cost of querying at M ﬁdelities. We want to jointly identify Bpairs of inputs and ﬁdelities (x1,m1),..., (xB,mB) at which to query. The batch acquisition function is given by abatch(X,m) = I({fm1 (x1),...,f mB (xB)},f∗|D)∑B k=1 λmk , (5) where X = {x1,..., xB}and m = [m1,...,m B]. As we can see, our batch acquisition function explicitly penalizes highly correlated queries, encouraging joint effectiveness and diversity — if between the outputs {fmk(xk)}B k=1 are high correlations, the mutual information in the numerator will decrease. Furthermore, by dividing the total querying cost in (5), the batch acquisition function expresses a balance between the beneﬁt of these queries (in probing the optimum) and the price, i.e., beneﬁt-cost ratio. When we set B = 1, our batch acquisition function is reduced to the single one used in (Takeno et al., 2019). 4.2 Efﬁcient Computation Given X and m, the computation of (5) is challenging, because it involves the mutual information between a set of NN outputs and the function optimum. To address this challenge, we use posterior samples and moment matching to approximate p(f,f∗|D) as a multi-variate Gaussian distribution, where f = [fm1 (x1),...,f mB (xB)]. Speciﬁcally, we ﬁrst draw a posterior sample of the NN weights Wfrom our model. We then calculate the output at each input and ﬁdelity to obtain a sample of f, and maximize (or minimize) fM(·) to obtain a sample of f∗. We use L-BFGS (Liu and Nocedal, 41989) for optimization. After we collect Lindependent samples {(ˆf1, ˆf∗ 1 ),..., (ˆfL, ˆf∗ L)}, we can estimate the ﬁrst and second moments of h = [f; f∗], namely, mean and covariance matrix, µ = 1 L L∑ j=1 ˆhj, Σ = 1 L−1 L∑ j=1 (ˆhj −µ)(ˆhj −µ)⊤, where each ˆhj = [ˆfj; ˆf∗ j]. We then use these moments to match a multivariate Gaussian posterior, p(h|D) ≈N(h|µ,Σ). Then the mutual information can be computed with a closed form, I(f,f∗|D) = H(f|D) + H(f∗|D) −H(f,f∗|D) ≈1 2 log |Σﬀ |+ 1 2 log σ∗∗−1 2 log |Σ|, (6) where Σﬀ = Σ[1 : B,1 : B], i.e., the ﬁrst B×B sub-matrix along the diagonal, which is the posterior covariance of f, and σ∗∗= Σ[B+ 1,B + 1], i.e., the posterior variance of f∗. The batch acquisition function is therefore calculated from abatch(X,m) ≈ 1 2 ∑B k=1 λmk (log |Σﬀ |+ logσ∗∗−log |Σ|) . (7) Note that Σ is a function of the inputs X and ﬁdelities m and hence so are its submatrix and elements, Σﬀ and σ∗∗. To obtain a reliable estimate of the moments, we set L= 100 in our experiments. Note that our method can be applied along with any posterior inference algorithm, such as variational inference and SGHMC (Chen et al., 2014), as long as we can generate posterior samples of the NN weights, not restricted to the HMC adopted in our paper. 4.3 Optimizing a Batch of Fidelities and Inputs Now, we consider maximizing (7) to identify Binputs X and their ﬁdelities m at which to query. However, since the optimization involves a mix of continuous inputs and discrete ﬁdelities, it is quite challenging. A straightforward approach would be to enumerate all possible conﬁgurations of m, for each particular conﬁguration, run a gradient based optimization algorithm to ﬁnd the optimal inputs, and then pick the conﬁguration and its optimal inputs that give the largest value of the acquisition function. However, doing so is essentially conducting a combinatorial search over Bﬁdelities, and the search space grows exponentially with B, i.e., O(MB) = O(eBlog M). Hence, it will be very costly, even infeasible for a moderate choice of B. To address this issue, we develop an alternating optimization algorithm. Speciﬁcally, we ﬁrst initialize all the Bqueries, Q= {(x1,m1),..., (xB,mB)}, say, randomly. Then each time, we only optimize one pair of the input and ﬁdelity (xk,mk)(1 ≤k≤B), while ﬁxing the others. We cyclically update each pair, where each update is much cheaper but guarantees to increase abatch. Speciﬁcally, each time, we maximize abatch,k(x,m) = I(F¬k ∪{fm(x)},f∗|D) λm + ∑ j̸=kλmj , (8) where F¬k = {fmj (xj)|j ̸= k}. Note that the computation of(8) still follows(7). We set(xk,mk) to the optimum (x∗,m∗), and then proceed to optimize the next input location and ﬁdelity(xk+1,mk+1) in Qwith the others ﬁxed. We continues this until we ﬁnish updating all the queries in Q, which corresponds to one iteration. We can keep running iterations until the increase of the batch acquisition function is less than a tolerance level or a maximum number of iterations has been done. Suppose we ran Giterations, the time complexity is O(GMB), which is linear in the number of ﬁdelities and batch size, and hence is much more efﬁcient than the naive combinatorial search. Our multi-ﬁdelity BO approach is summarized in Algorithm 1. 5 Related Work Most Bayesian optimization (BO) (Mockus, 2012; Snoek et al., 2012) methods are based on Gaussian processes (GPs) and a variety of acquisition functions, such as (Mockus et al., 1978; Auer, 2002; Srinivas et al., 2010; Hennig and Schuler, 2012; Hernández-Lobato et al., 2014; Wang and Jegelka, 2017; Kandasamy et al., 2017b; Garrido-Merchán and Hernández-Lobato, 2020). Snoek et al. (2015) showed Bayesian neural networks (NNs) can also be used as a general surrogate model, and has 5Algorithm 1 BMBO-DARN (D, B, M, T, {λm}M m=1 ) Learn the deep auto-regressive model (4) on Dwith HMC. for t= 1,...,T do Collect a batch of Bqueries, Q= {(xk,mk)}B k=1, with Algorithm 2. Query the objective function value at each input xk and ﬁdelity mk in Q D←D∪{ (xk,yk,mk)|1 ≤k≤B}. Re-train the deep auto-regressive model on Dwith HMC. end for Algorithm 2 BatchAcquisition({λm}, B, L, G, ξ) Initialize Q= {(x1,m1),..., (xB,mB)}randomly. Collect Lindependent posterior samples of the NN weights. repeat for k= 1,...,B do Use the posterior samples to calculate and optimize (8), (x∗,m∗) = argmax x∈Ω,1≤m≤M abatch,k(x,m), where Ω is the input domain. (xk,mk) ←(x∗,m∗). end for until Giterations are done or the increase of abatch in (7) is less than ξ Return Q. excellent performance. Moreover, the training of NNs is scalable, not suffering from O(N3) time complexity (N is the number of examples) of training exact GPs. Springenberg et al. (2016) further used scale adaption to develop a robust stochastic gradient HMC for the posterior inference in the NN based BO. Recent works that deal with discrete inputs (Baptista and Poloczek, 2018) or mixed discrete and continuous inputs (Daxberger et al., 2019) use an explicit nonlinear feature mapping and Bayesian linear regression, which can be viewed as one-layer Bayesian NNs. There have been many studies in multi-ﬁdelity (MF) BO, e.g., (Huang et al., 2006; Swersky et al., 2013; Lam et al., 2015; Picheny et al., 2013; Kandasamy et al., 2016, 2017a; Poloczek et al., 2017; McLeod et al., 2017; Wu and Frazier, 2017). While successful, these methods either ignore or oversimplify the strong, complex correlations between different ﬁdelities, and hence might be inefﬁcient in surrogate learning. For example, Picheny et al. (2013); Lam et al. (2015); Kandasamy et al. (2016); Poloczek et al. (2017) learned an independent GP for each ﬁdelity; Song et al. (2019) used all the examples without discrimination to train one single GP; Huang et al. (2006); Takeno et al. (2019) imposed a linear correlation across ﬁdelities, while Zhang et al. (2017) constructed a convolutional kernel as the cross-ﬁdelity covariance and so the involved kernels in the convolution must be simple and smooth enough (yet less expressive) to obtain a closed form. Recently, Perrone et al. (2018) developed an NN-based multi-task BO method for hyper-parameter transfer learning. Their model constructs an NN feature mapping shared by all the tasks, and uses an independent linear combination of the mapped features to predict each task output. While we can consider each task as evaluating the objective at a particular ﬁdelity, the model does not explicitly capture and exploit the correlations across different tasks — given the shared (latent) features, the predictions of these tasks (ﬁdelities) are independent. The most recent work (Li et al., 2020) also developed an NN-based multi-ﬁdelity BO method, which differs from our work in that (1) their model only estimates the relationship between successive ﬁdelities, and hence has less capacity, (2) their work uses a recursive one-dimensional quadrature to calculate the acquisition function, and is difﬁcult to extend to batch acquisitions. In a high level, the chain structure of Li et al. (2020)’s model also resembles deep GP based multi-ﬁdelity models (Perdikaris et al., 2017; Cutajar et al., 2019). Quite a few batch BO algorithms have been developed, such as (González et al., 2016; Wu and Frazier, 2016; Hernández-Lobato et al., 2017; Kandasamy et al., 2017b). However, they work with single-ﬁdelity queries and are not easily extended to multi-ﬁdelity optimization tasks. Takeno et al. (2019) proposed two batch querying strategies for their MF-BO framework. Both strategies 6leverage the property that the covariance of a conditional Gaussian does not rely on the values of the conditioned variables; so, there is no need to worry about conditioning on function values that are still in query. The asynchronous version generates new queries conditioned on different sets of function values in query (asynchronously). However, if the conditional parts are signiﬁcantly overlapping, which might not be uncommon in practice, there is a risk of generating redundant or even collapsed queries. Takeno et al. (2019) also talked about a synchronous version. While they discussed how to compute the information gain between the function maximum and a batch of function values, they did not provide an effective way to optimize it with the multi-ﬁdelity querying costs. Instead, they suggested a simple heuristics to sequentially ﬁnd each query by conditioning on the generated ones. However, there is no guarantee about this heuristics. While in our experiments, we mainly use hyperparameter optimization to evaluate our multi-ﬁdelity BO approach, there are many other excellent works speciﬁcally designed for hyperparameter tuning or selection, e.g., the non-Bayesian, random search based method Hyberband (Li et al., 2017) which also reﬂects the multi-ﬁdelity idea: it starts using few training iterations/epochs (low ﬁdelity) to evaluate many candidates, rank them, iteratively selects the top-ranked ones, and further evaluate them with more iterations/epochs (high ﬁdelity). BOHB (Falkner et al., 2018) is a hybrid of KDE based BO (Bergstra et al., 2011) and Hyperband. Li et al. (2018) further developed an asynchronous successive halving algorithm for parallel random search over hyperparameters. Domhan et al. (2015); Klein et al. (2017b) propose to estimate the learning curves, and early halt the evaluation of ominous hyperparameters according to the learning curve predictions. Swersky et al. (2014) introduced a kernel about the training steps, and developed Freeze-thaw BO (Swersky et al., 2014) that can temporarily pause the model training and explore several promising hyperparameter settings for a while and then continue on to the most promising one. The work in (Klein et al., 2017a) jointly estimates the cost as a function of the data size and training steps, which can be viewed as continuous ﬁdelities, like in (Kandasamy et al., 2017a; Wu and Frazier, 2017). 6 Experiment 6.1 Surrogate Learning Performance We ﬁrst examined if BMBO-DARN can learn a more accurate surrogate of the objective. We used two popular benchmark functions: (1) Levy (Laguna and Martí, 2005) with two-ﬁdelity evaluations, and (2) Branin (Forrester et al., 2008; Perdikaris et al., 2017) with three-ﬁdelity evaluations. Throughout different ﬁdelities are nonlinear/nonstationary transforms. We provide the details in the Appendix. Levy nRMSE MNLL MF-GP-UCB 0.831 ± 0.195 1 .824 ± 0.276 MF-MES 0.581 ± 0.032 1 .401 ± 0.031 SHTL 0.443 ± 0.009 1 .208 ± 0.026 DNN-MFBO 0.365 ± 0.035 1 .081 ± 0.011 BMBO-DARN 0.348 ± 0.021 1 .072 ± 0.016 Branin MF-GP-UCB 0.846 ± 0.147 1 .976 ± 0.208 MF-MES 0.719 ± 0.099 1 .796 ± 0.128 SHTL 0.835 ± 0.218 1 .958 ± 0.646 DNN-MFBO 0.182 ± 0.022 0 .973 ± 0.013 BMBO-DARN 0.158 ± 0.016 0 .965 ± 0.005 Table 1: Surrogate learning performance on Branin function with three-ﬁdelity training examples and Levy function with two-ﬁdelity examples: normalized root- mean-square-error (nRMSE) and mean-negative-log- likelihood (MNLL). The results were averaged over ﬁve runs. Methods. We compared with the following multi-ﬁdelity learning models used in the state- of-the-art BO methods: (1) MF-GP-UCB (Kan- dasamy et al., 2016) that learns an independent GP for each ﬁdelity. (2) MF-MES (Takeno et al., 2019) that uses a multi-output GP with a linear correlation structure across different outputs (ﬁ- delities), (3) Scalable Hyperparameter Transfer Learning (SHTL) (Perrone et al., 2018) that uses an NN to generate latent bases shared by all the tasks (ﬁdelities) and predicts the output of each task with a linear combination of the bases. (4) Deep Neural Network Multi-Fidelity BO (DNN- MFBO) (Li et al., 2020) that uses a chain of NNs to model each ﬁdelity, but only estimates the relationship between successive ﬁdelities. Settings. We implemented our model with PyTorch (Paszke et al., 2019) and HMC sam- pling based on the Hamiltorch library (Cobb and Jalaian, 2021) (https://github.com/ AdamCobb/hamiltorch). For each ﬁdelity, we used two hidden layers with 40 neurons and tanh activation. We ran HMC for 5K steps to reach burn in (by looking at the trace plots) and then produced 200 posterior samples with every 10 steps. To generate each sample proposal, we ran 10 leapfrog steps, and the step size was chosen as 0.012. 7We implemented DNN-MFBO and SHTL with PyTorch as well. For DNN-MFBO, we used the same NN architecture as in BMBO-DARN for each ﬁdelity, and ran HMC with the same setting for model estimation. For SHTL, we used two hidden layers with 40 neurons and an output layer with 32 neurons to generate the shared bases. We used ADAM (Kingma and Ba, 2014) to estimate the model parameters, and the learning rate was chosen from {10−4,5 ×10−4,10−3,5 ×10−3,10−2}. We ran 1K epochs, which are enough for convergence. Note that we also attempted to use L-BFGS to train SHTL, but it often runs into numerical issues. ADAM is far more stable. We used a Python implementation of MF-MES and MF-GP-UCB, both of which use the RBF kernel (consistent with the original papers). Results. We randomly generated{130,65}examples for Levy function at the two increasing ﬁdelities, and {320,130,65}examples for Branin function at its three increasing ﬁdelities. After training, we examined the prediction accuracy of all the models with 100 test samples uniformly sampled from the input space. We calculated the normalized root-mean-square-error (nRMSE) and mean- negative-log-likelihood (MNLL). We repeated the experiment for 5 times, and report their average and standard deviations in Table. 1. As we can see, for both benchmark functions, BMBO-DARN outperforms all the competing models, conﬁrming the advantage of our deep auto-regressive model in surrogate learning. Note that despite using a similar chain structure, DNN-MFBO is still inferior to BMBO-DARN, implying that our fully auto-regressive modeling (see (3)) can better estimate the relationships between the ﬁdelities to facilitate surrogate estimation. 6.2 Real-World Applications Next, we used BMBO-DARN to optimize the hyperparameters of four popular machine learning models: Convolutional Neural Networks (CNN) (Fukushima and Miyake, 1982; LeCun et al., 1990) for image classiﬁcation, Online Latent Dirichlet Allocation (LDA) (Hoffman et al., 2010) for text mining, XGBoost (Chen and Guestrin, 2016) for diabetes diagnosis, and Physics-Informed Neural Networks (PINN) (Raissi et al., 2019) for solving partial differential equations (PDE). Methods and Setting. We compared with the state-of-the-art multi-ﬁdelity BO algorithms men- tioned in Sec. 6.1, (1) MF-GP-UCB, (2) MF-MES, (3) SHTL, and (4) DNN-MFBO. In ad- dition, we compared with (5) MF-MES-Batch (Takeno et al., 2019), the (asynchronous) paral- lel version of MF-MES, (6) SF-Batch (Kandasamy et al., 2017b) ( https://github.com/ kirthevasank/gp-parallel-ts), a single-ﬁdelity GP-based BO that optimizes posterior samples of the objective function to obtain a batch of queries, (7) SMAC3 ( https://github. com/automl/SMAC3), BO based on random forests, (8) Hyperband (Li et al., 2017) ( https: //github.com/automl/HpBandSter) that conducts multi-ﬁdelity random search over the hy- perparameters, (9) BOHB (Falkner et al., 2018) that uses Tree Parzen Estimator (TPE) (Bergstra et al., 2011) to generate hyperparameter candidates in Hyperband iterations. We also tested our method that queries at one input and ﬁdelity each time (B = 1), which we denote by BMBO-DARN-1. We used the same setting as in Sec. 6.1 for all the multi-ﬁdelity methods, except that for SHTL, we ran 2K epochs in surrogate training to ensure the convergence. For our method, we set the maximum number of iterations in optimizing the batch acquisition function (see Algorithm 8) to 100 and tolerance level to 10−3. For the remaining methods, e.g., SMAC3 and Hyperband, we used their original implementations and default settings. For all the batch querying methods, we set the batch size to 5. All the single ﬁdelity methods queried at the highest ﬁdelity. Convolutional Neural Network (CNN). Our ﬁrst application is to train a CNN for image classiﬁca- tion. We used CIFAR-10 dataset (https://www.cs.toronto.edu/~kriz/cifar.html), from which we used 10K images for training and another 10K for evaluation. To optimize the hyperparameters, we considered three ﬁdelities, i.e., training with 1, 10, 50 epochs. We used the average negative log-loss (nLL) to evaluate the prediction accuracy of each method. We considered optimizing the following hyperparameters: # convolutional layers ranging from [1,4], # channels in the ﬁrst ﬁlter ([8, 136]), depth of the dense layers ([1, 8]), width of the dense layers ([32, 2080]), pooling type ([“max”, “average”]), and dropout rate ([10−3, 0.99]). We optimized the dropout rate in the log domain, and used a continuous relaxation of the discrete parameters. Initially, we queried at 10 random hyperparameter settings at each ﬁdelity. All the methods started with these evaluation results and repeatedly identiﬁed new hyperparameters. We used the average running time at each training ﬁdelity as the cost: λ1 : λ2 : λ3 = 1 : 10 : 50 . After each query, we evaluated the performance of the new hyperparameters at the highest 80 1000 2000 3000 4000 5000 6000 7000 8000 Accumulated Cost (Time in seconds) 1.0 1.5 2.0Negative Log Loss (a) CNN 0 200 400 600 800 1000 1200 Accumulated Cost (Time in seconds) 600 800 1000 1200 1400Perplexity (b) Online LDA 0 5 10 15 20 25 30 35 Accumulated Cost (Time in seconds) −0.4 −0.3 −0.2 −0.1Log nRMSE (c) XGBoost 0 1000 2000 3000 4000 5000 6000 7000 8000 Accumulated Cost (Time in seconds) −6 −4 −2 0 Log nRMSE (d) PINN Figure 2: Performance vs. accumulated cost (running time) in Hyperparameter optimization tasks. For fairness, all the batch methods queried new examples sequentially, i.e., no parallel querying was employed. The results were averaged over ﬁve runs. Note that MF-GP-UCB, MF-MES and MF-MES-Batch often obtained very close results and their curves overlap much. training level. We ran each method until 100 queries were issued. We repeated the exper- iment for 5 times and in Fig. 2a report the average accuracy (nLL) and its standard devi- ation for the hyperparameters found by each method throughout the optimization procedure. 0 250 500 750 1000 1250 1500 1750Time(seconds)-Per-Query BMBO-DARN BMBO-DARN-1 DNN-MFBO MF-GP-UCB MF-MES MF-MES-Batch    MF-GP-UCB MF-MES MF-MES-Batch    (a) CNN 0 250 500 750 1000 1250 1500 1750Time(seconds)-Per-Query (b) Online LDA Figure 3: Average time to generate queries (including surrogate training). Online Latent Dirichlet Allocation (LDA). Our second task is to train online LDA (Hoffman et al., 2010) to extract topics from 20NewsGroups corpus ( http://qwone.com/~jason/ 20Newsgroups/). We used 5K documents for training, and 2K for evaluation. We used the implement from the scikit-learn library (https: //scikit-learn.org/stable/). We considered optimizing the following hyperparam- eters: document topic prior α ∈ [10−3,1], topic word prior η ∈ [10−3,1], learn- ing decay κ ∈ [0.51,1], learning offset τ0 ∈[1,2,5,10,20,50,100,200], E-step stopping tolerance ϵ∈[10−5,10−1], document batch size in [2,4,8,16,32,64,128,256], and topic number K ∈[1,64]. We optimized α, η, κand ϵin the log domain, and used a continuous relaxation of the discrete parameters. We considered three ﬁdelities — training with 1, 10 and 50 epochs, and randomly queried 10 examples at each ﬁdelity to start each method. We evaluated the performance of the selected hyperparameters in terms of perplexity (the smaller, the better). In Fig. 2 b, we reported the average perplexity (and its standard deviation) of each method after ﬁve runs of the hyperparameter optimization. XGBoost. Third, we trained an XGBoost model (Chen and Guestrin, 2016) to predict a quantitative measure of the diabetes progression ( https://archive.ics.uci.edu/ml/datasets/ diabetes). The dataset includes 442 examples. We used two-thirds for training and the remaining one-third for evaluation. We used the implementation from the scikit-learn library. We optimized the following hyperparameters: Huber loss parameter α∈[0.01,0.1], the non-negative complexity pruning parameter ([0.01,100]), fraction of samples used to ﬁt individual base learners ( [0.1,1]), 9fraction of features considered to split the tree ( [0.01,1]), splitting criterion ([“MAE”, “MSE”]), minimum number of samples required to split an internal node ( [2,9]), and the maximum depth of individual trees ( [1,16]). The hyperparameter space is 12 dimensional. We considered three ﬁdelities — training XGBoost with 2, 10 and 100 weak learners (trees). The querying cost is therefore λ1 : λ2 : λ3 = 1 : 5 : 50. We started with 10 random queries at each ﬁdelity. We used the log of nRMSE to evaluate the performance. We ran 5 times and report the average log-nRMSE of the identiﬁed hyperparameters by each method in Fig. 2c. Physics-informed Neural Networks (PINN). Our fourth application is to learn a PINN to solve PDEs (Raissi et al., 2019). The key idea of PINN is to use boundary points to construct the training loss, and meanwhile use a set of collocation points in the domain to regularize the NN solver to respect the PDE. With appropriate choices of hyperparameters, PINNs can obtain very accurate solutions. We used PINNs to solve Burger’s equation (Morton and Mayers, 2005) with the viscosity 0.01/π. The solution becomes sharper with bigger time variables (see the Appendix) and hence the learning is quite challenging. We followed (Raissi et al., 2019) to use fully connected networks and L-BFGS for training. The hyperparameters include NN depth ([1,8]), width ([1,64]), and activations (8 choices: Relu, tanh, sigmoid, their variants, etc.). Following (Raissi et al., 2019), we used 100 boundary points as the training set and 10K collocation points for regularization. We used 10K points for evaluation. We chose 3 training ﬁdelities, running L-BFGS with 10, 100, 50K maximum iterations. The querying cost (average training time) is λ1 : λ2 : λ3 = 1 : 10 : 50 . Note that in ﬁdelity 3, L-BFGS usually converged before running 50K iterations. We initially issued 10 random queries at each ﬁdelity. We ran each method for 5 times and reported the average log nRMSE after each step in Fig. 2d. Results. As we can see, in all the applications, BMBO-DARN used the smallest cost (i.e., running time) to ﬁnd the hyperparameters that gives the best learning performance. In general, BMBO-DARN identiﬁed better hyperparameters with the same cost, or equally good hyperparameters with the smallest cost. BMBO-DARN-1 outperformed all the one-by-one querying methods, except that for online LDA (Fig. 2b) and PINN (Fig. 2d), it was worse than DNN-MFBO and Hyperband at the early stage, but ﬁnally obtained better learning performance. We observed that the GP based baselines (MF-MES, MF-GP-UCB, SF-Batch, etc.) are often easier to be stuck in suboptimal hyperparameters, this might because these models are not effective enough to integrate information of multiple ﬁdelities to obtain a good surrogate. Together these results have shown the advantage of our method, especially in our batch querying strategy. Finally, we show the average query generation time of BMBO-DARN for CNN and Online LDA in Fig. 3 (including surrogate training). It turns out BMBO-DARN spends much less time than MF-MES using the global optimization method DIRECT (Jones et al., 1998), and comparable to MF-GP-UCB and DNN-MFBO. Therefore, BMBO-DARN is efﬁcient to update the surrogate model and generate new queries. 7 Conclusion We have presented BMBO-DARN, a batch multi-ﬁdelity Bayesian optimization method. Our deep auto-regressive model can serve as a better surrogate of the black-box objective. Our batch query- ing method not only is efﬁcient, avoiding combinatorial search over discrete ﬁdelities, but also signiﬁcantly reduces the cost while improving the optimization performance. Acknowledgments This work has been supported by MURI AFOSR grant FA9550-20-1-0358. References Auer, P. (2002). Using conﬁdence bounds for exploitation-exploration trade-offs. Journal of Machine Learning Research, 3(Nov):397–422. Baptista, R. and Poloczek, M. (2018). Bayesian optimization of combinatorial structures. In International Conference on Machine Learning, pages 462–471. Bergstra, J., Bardenet, R., Bengio, Y ., and Kégl, B. (2011). Algorithms for hyper-parameter op- timization. In 25th annual conference on neural information processing systems (NIPS 2011), 10volume 24. Neural Information Processing Systems Foundation. Chen, T., Fox, E., and Guestrin, C. (2014). Stochastic gradient Hamiltonian Monte Carlo. In International conference on machine learning, pages 1683–1691. PMLR. Chen, T. and Guestrin, C. (2016). Xgboost: A scalable tree boosting system. In Proceedings of the 22nd acm SigKDD international conference on knowledge discovery and data mining, pages 785–794. Chung, T. (2010). Computational ﬂuid dynamics. Cambridge university press. Cobb, A. D. and Jalaian, B. (2021). Scaling Hamiltonian Monte Carlo inference for Bayesian neural networks with symmetric splitting. Uncertainty in Artiﬁcial Intelligence. Cutajar, K., Pullin, M., Damianou, A., Lawrence, N., and González, J. (2019). Deep gaussian processes for multi-ﬁdelity modeling. arXiv preprint arXiv:1903.07320. Daxberger, E., Makarova, A., Turchetta, M., and Krause, A. (2019). Mixed-variable Bayesian optimization. arXiv preprint arXiv:1907.01329. Domhan, T., Springenberg, J. T., and Hutter, F. (2015). Speeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves. In Twenty-fourth international joint conference on artiﬁcial intelligence. Falkner, S., Klein, A., and Hutter, F. (2018). BOHB: Robust and efﬁcient hyperparameter optimization at scale. In International Conference on Machine Learning, pages 1437–1446. PMLR. Forrester, A., Sobester, A., and Keane, A. (2008). Engineering design via surrogate modelling: a practical guide. John Wiley & Sons. Fukushima, K. and Miyake, S. (1982). Neocognitron: A self-organizing neural network model for a mechanism of visual pattern recognition. In Competition and cooperation in neural nets, pages 267–285. Springer. Garrido-Merchán, E. C. and Hernández-Lobato, D. (2020). Dealing with categorical and integer- valued variables in Bayesian optimization with gaussian processes. Neurocomputing, 380:20–35. González, J., Dai, Z., Hennig, P., and Lawrence, N. (2016). Batch Bayesian optimization via local penalization. In Artiﬁcial intelligence and statistics, pages 648–657. PMLR. Hennig, P. and Schuler, C. J. (2012). Entropy search for information-efﬁcient global optimization. Journal of Machine Learning Research, 13(Jun):1809–1837. Hernández-Lobato, J. M., Hoffman, M. W., and Ghahramani, Z. (2014). Predictive entropy search for efﬁcient global optimization of black-box functions. In Advances in neural information processing systems, pages 918–926. Hernández-Lobato, J. M., Requeima, J., Pyzer-Knapp, E. O., and Aspuru-Guzik, A. (2017). Parallel and distributed Thompson sampling for large-scale accelerated exploration of chemical space. In International conference on machine learning, pages 1470–1479. PMLR. Hoffman, M., Bach, F. R., and Blei, D. M. (2010). Online learning for latent dirichlet allocation. In advances in neural information processing systems, pages 856–864. Citeseer. Huang, D., Allen, T. T., Notz, W. I., and Miller, R. A. (2006). Sequential kriging optimization using multiple-ﬁdelity evaluations. Structural and Multidisciplinary Optimization, 32(5):369–382. Jones, D. R., Schonlau, M., and Welch, W. J. (1998). Efﬁcient global optimization of expensive black-box functions. Journal of Global optimization, 13(4):455–492. Kandasamy, K., Dasarathy, G., Oliva, J. B., Schneider, J., and Póczos, B. (2016). Gaussian process bandit optimisation with multi-ﬁdelity evaluations. In Advances in Neural Information Processing Systems, pages 992–1000. 11Kandasamy, K., Dasarathy, G., Schneider, J., and Póczos, B. (2017a). Multi-ﬁdelity Bayesian optimisation with continuous approximations. In Proceedings of the 34th International Conference on Machine Learning-V olume70, pages 1799–1808. JMLR. org. Kandasamy, K., Krishnamurthy, A., Schneider, J., and Poczos, B. (2017b). Asynchronous parallel Bayesian optimisation via Thompson sampling. arXiv preprint arXiv:1705.09236. Kingma, D. P. and Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980. Klein, A., Falkner, S., Bartels, S., Hennig, P., and Hutter, F. (2017a). Fast Bayesian optimization of machine learning hyperparameters on large datasets. In Artiﬁcial Intelligence and Statistics, pages 528–536. PMLR. Klein, A., Falkner, S., Springenberg, J. T., and Hutter, F. (2017b). Learning curve prediction with Bayesian neural networks. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net. Kutluay, S., Bahadir, A., and Özdecs, A. (1999). Numerical solution of one-dimensional burgers equation: explicit and exact-explicit ﬁnite difference methods. Journal of Computational and Applied Mathematics, 103(2):251–261. Laguna, M. and Martí, R. (2005). Experimental testing of advanced scatter search designs for global optimization of multimodal functions. Journal of Global Optimization, 33(2):235–255. Lam, R., Allaire, D. L., and Willcox, K. E. (2015). Multiﬁdelity optimization using statistical surrogate modeling for non-hierarchical information sources. In 56th AIAA/ASCE/AHS/ASC Structures, Structural Dynamics, and Materials Conference, page 0143. LeCun, Y ., Boser, B. E., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W. E., and Jackel, L. D. (1990). Handwritten digit recognition with a back-propagation network. In Advances in neural information processing systems, pages 396–404. Li, L., Jamieson, K., DeSalvo, G., Rostamizadeh, A., and Talwalkar, A. (2017). Hyperband: A novel bandit-based approach to hyperparameter optimization. The Journal of Machine Learning Research, 18(1):6765–6816. Li, L., Jamieson, K., Rostamizadeh, A., Gonina, E., Hardt, M., Recht, B., and Talwalkar, A. (2018). Massively parallel hyperparameter tuning. arXiv preprint arXiv:1810.05934. Li, S., Xing, W., Kirby, R., and Zhe, S. (2020). Multi-ﬁdelity Bayesian optimization via deep neural networks. In Advances in Neural Information Processing Systems. Liu, D. C. and Nocedal, J. (1989). On the limited memory BFGS method for large scale optimization. Mathematical programming, 45(1-3):503–528. McLeod, M., Osborne, M. A., and Roberts, S. J. (2017). Practical Bayesian optimization for variable cost objectives. arXiv preprint arXiv:1703.04335. Mockus, J. (2012). Bayesian approach to global optimization: theory and applications, volume 37. Springer Science & Business Media. Mockus, J., Tiesis, V ., and Zilinskas, A. (1978). The application of Bayesian methods for seeking the extremum. Towards global optimization, 2(117-129):2. Morton, K. W. and Mayers, D. F. (2005). Numerical solution of partial differential equations: an introduction. Cambridge university press. Nagel, K. (1996). Particle hopping models and trafﬁc ﬂow theory. Physical review E, 53(5):4655. Neal, R. M. et al. (2011). Mcmc using Hamiltonian dynamics. Handbook of markov chain monte carlo, 2(11):2. 12Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., and Chintala, S. (2019). Pytorch: An imperative style, high- performance deep learning library. In Wallach, H., Larochelle, H., Beygelzimer, A., d'Alché-Buc, F., Fox, E., and Garnett, R., editors, Advances in Neural Information Processing Systems 32, pages 8024–8035. Curran Associates, Inc. Perdikaris, P., Raissi, M., Damianou, A., Lawrence, N., and Karniadakis, G. E. (2017). Nonlinear information fusion algorithms for data-efﬁcient multi-ﬁdelity modelling. Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences, 473(2198):20160751. Perrone, V ., Jenatton, R., Seeger, M. W., and Archambeau, C. (2018). Scalable hyperparameter transfer learning. In Advances in Neural Information Processing Systems, pages 6845–6855. Picheny, V ., Ginsbourger, D., Richet, Y ., and Caplin, G. (2013). Quantile-based optimization of noisy computer experiments with tunable precision. Technometrics, 55(1):2–13. Poloczek, M., Wang, J., and Frazier, P. (2017). Multi-information source optimization. In Advances in Neural Information Processing Systems, pages 4288–4298. Raissi, M., Perdikaris, P., and Karniadakis, G. E. (2017). Physics informed deep learning (part i): Data-driven solutions of nonlinear partial differential equations. arXiv preprint arXiv:1711.10561. Raissi, M., Perdikaris, P., and Karniadakis, G. E. (2019). Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of Computational Physics, 378:686–707. Rasmussen, C. E. and Williams, C. K. I. (2006). Gaussian Processes for Machine Learning. MIT Press. Shah, A., Xing, W., and Triantafyllidis, V . (2017). Reduced-order modelling of parameter-dependent, linear and nonlinear dynamic partial differential equation models. Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences, 473(2200):20160809. Snoek, J., Larochelle, H., and Adams, R. P. (2012). Practical Bayesian optimization of machine learning algorithms. In Advances in neural information processing systems, pages 2951–2959. Snoek, J., Rippel, O., Swersky, K., Kiros, R., Satish, N., Sundaram, N., Patwary, M., Prabhat, M., and Adams, R. (2015). Scalable Bayesian optimization using deep neural networks. In International conference on machine learning, pages 2171–2180. Song, J., Chen, Y ., and Yue, Y . (2019). A general framework for multi-ﬁdelity Bayesian optimization with Gaussian processes. In The 22nd International Conference on Artiﬁcial Intelligence and Statistics, pages 3158–3167. Springenberg, J. T., Klein, A., Falkner, S., and Hutter, F. (2016). Bayesian optimization with robust Bayesian neural networks. In Advances in neural information processing systems, volume 29, pages 4134–4142. Srinivas, N., Krause, A., Kakade, S., and Seeger, M. (2010). Gaussian process optimization in the bandit setting: no regret and experimental design. In Proceedings of the 27th International Conference on International Conference on Machine Learning, pages 1015–1022. Sugimoto, N. (1991). Burgers equation with a fractional derivative; hereditary effects on nonlinear acoustic waves. Journal of ﬂuid mechanics, 225:631–653. Swersky, K., Snoek, J., and Adams, R. P. (2013). Multi-task Bayesian optimization. In Advances in neural information processing systems, pages 2004–2012. Swersky, K., Snoek, J., and Adams, R. P. (2014). Freeze-thaw Bayesian optimization. arXiv preprint arXiv:1406.3896. Takeno, S., Fukuoka, H., Tsukada, Y ., Koyama, T., Shiga, M., Takeuchi, I., and Karasuyama, M. (2019). Multi-ﬁdelity Bayesian optimization with max-value entropy search. arXiv preprint arXiv:1901.08275. 13Wang, Z. and Jegelka, S. (2017). Max-value entropy search for efﬁcient Bayesian optimization. In Proceedings of the 34th International Conference on Machine Learning-V olume70, pages 3627– 3635. JMLR. org. Wu, J. and Frazier, P. (2016). The parallel knowledge gradient method for batch Bayesian optimization. Advances in Neural Information Processing Systems, 29:3126–3134. Wu, J. and Frazier, P. I. (2017). Continuous-ﬁdelity Bayesian optimization with knowledge gradient. In NIPS Workshop on Bayesian Optimization. Zhang, Y ., Hoang, T. N., Low, B. K. H., and Kankanhalli, M. (2017). Information-based multi-ﬁdelity Bayesian optimization. In NIPS Workshop on Bayesian Optimization. Appendix 8 Synthetic Benchmark Functions 8.1 Branin Function The input is two dimensional, x = [x1,x2] ∈[−5,10] ×[0,15]. We have three ﬁdelities to evaluate the function, which, from high to low, are given by f3(x) = − (−1.275x2 1 π2 + 5x1 π + x2 −6 )2 − ( 10 − 5 4π ) cos(x1) −10, f2(x) = −10 √ −f3(x−2) −2(x1 −0.5) + 3(3x2 −1) + 1, f1(x) = −f2 ( 1.2(x + 2) ) + 3x2 −1. (9) We can see that between ﬁdelities are nonlinear transformations, nonuniform scaling, and shifts. 8.2 Levy Function The input is two dimensional, x = [x1,x2] ∈[−10,10]2. We have two ﬁdelities, f2(x) = −sin2(3πx1) −(x1 −1)2[1 + sin2(3πx2)] −(x2 −1)2[1 + sin2(2πx2)], f1(x) = − √ 1 + f2 2 (x). (10) 9 Details about Physics Informed Neural Networks Burgers’ equation is a canonical nonlinear hyperbolic PDE, and widely used to characterize a variety of physical phenomena, such as nonlinear acoustics (Sugimoto, 1991), ﬂuid dynamics (Chung, 2010), and trafﬁc ﬂows (Nagel, 1996). Since the solution can develop discontinuities (i.e., shock waves) based on a normal conservation equation, Burger’s equation is often used as a nontrivial benchmark test for numerical solvers and surrogate models (Kutluay et al., 1999; Shah et al., 2017; Raissi et al., 2017). We used physics informed neural networks (PINN) to solve the viscosity version of Burger’s equation, ∂u ∂t + u∂u ∂x = ν∂2u ∂x2 , (11) where uis the volume, xis the spatial location, tis the time, and ν is the viscosity. Note that the smaller ν, the sharper the solution of u. In our experiment, we set ν = 0.01 π , x ∈[−1,1], and t∈[0,1]. The boundary condition is given by u(0,x) = −sin(πx), u(t,−1) = u(t,1) = 0. We use an NNuWto represent the solution. To estimate the NN, we collectedN training points in the boundary, D= {(ti,xi,ui)}N i=1, and M collocation (input) points in the domain, C= {(ˆti,ˆxi)}M i=1. We then minimize the following loss function to estimate uW, L(W) = 1 N N∑ i=1 (uW(ti,xi) −ui)2 + 1 M M∑ i=1 (⏐⏐ψ(uW)(ˆti,ˆxi) ⏐⏐2) , 14where ψ(·) is a functional constructed from the PDE, ψ(u) = ∂u ∂t + u∂u ∂x −ν∂2u ∂x2 . Obviously, the loss consists of two terms, one is the training loss, and the other is a regularization term that enforces the NN solution to respect the PDE. 15",
      "meta_data": {
        "arxiv_id": "2106.09884v2",
        "authors": [
          "Shibo Li",
          "Robert M. Kirby",
          "Shandian Zhe"
        ],
        "published_date": "2021-06-18T02:55:48Z",
        "pdf_url": "https://arxiv.org/pdf/2106.09884v2.pdf"
      }
    },
    {
      "title": "Supervising the Multi-Fidelity Race of Hyperparameter Configurations",
      "abstract": "Multi-fidelity (gray-box) hyperparameter optimization techniques (HPO) have\nrecently emerged as a promising direction for tuning Deep Learning methods.\nHowever, existing methods suffer from a sub-optimal allocation of the HPO\nbudget to the hyperparameter configurations. In this work, we introduce DyHPO,\na Bayesian Optimization method that learns to decide which hyperparameter\nconfiguration to train further in a dynamic race among all feasible\nconfigurations. We propose a new deep kernel for Gaussian Processes that embeds\nthe learning curve dynamics, and an acquisition function that incorporates\nmulti-budget information. We demonstrate the significant superiority of DyHPO\nagainst state-of-the-art hyperparameter optimization methods through\nlarge-scale experiments comprising 50 datasets (Tabular, Image, NLP) and\ndiverse architectures (MLP, CNN/NAS, RNN).",
      "full_text": "Supervising the Multi-Fidelity Race of Hyperparameter Configurations Martin Wistuba∗† Amazon Web Services, Berlin, Germany marwistu@amazon.com Arlind Kadra∗ University of Freiburg, Freiburg, Germany kadraa@cs.uni-freiburg.de Josif Grabocka University of Freiburg, Freiburg, Germany grabocka@cs.uni-freiburg.de Abstract Multi-fidelity (gray-box) hyperparameter optimization techniques (HPO) have recently emerged as a promising direction for tuning Deep Learning methods. However, existing methods suffer from a sub-optimal allocation of the HPO bud- get to the hyperparameter configurations. In this work, we introduce DyHPO, a Bayesian Optimization method that learns to decide which hyperparameter config- uration to train further in a dynamic race among all feasible configurations. We propose a new deep kernel for Gaussian Processes that embeds the learning curve dynamics, and an acquisition function that incorporates multi-budget information. We demonstrate the significant superiority of DyHPO against state-of-the-art hy- perparameter optimization methods through large-scale experiments comprising 50 datasets (Tabular, Image, NLP) and diverse architectures (MLP, CNN/NAS, RNN). 1 Introduction Hyperparameter Optimization (HPO) is arguably an acute open challenge for Deep Learning (DL), especially considering the crucial impact HPO has on achieving state-of-the-art empirical results. Unfortunately, HPO for DL is a relatively under-explored field and most DL researchers still optimize their hyperparameters via obscure trial-and-error practices. On the other hand, traditional Bayesian Optimization HPO methods [Snoek et al., 2012, Bergstra et al., 2011] are not directly applicable to deep networks, due to the infeasibility of evaluating a large number of hyperparameter configurations. In order to scale HPO for DL, three main directions of research have been recently explored. (i) Online HPO methods search for hyperparameters during the optimization process via meta-level controllers [Chen et al., 2017, Parker-Holder et al., 2020], however, this online adaptation can not accommodate all hyperparameters (e.g. related to architectural changes). (ii) Gradient-based HPO techniques, on the other hand, compute the derivative of the validation loss w.r.t. hyperparameters by reversing the training update steps [Maclaurin et al., 2015, Franceschi et al., 2017, Lorraine et al., 2020], however, the reversion is not directly applicable to all cases (e.g. dropout rate). The last direction, (iii) Gray-box HPO techniques discard sub-optimal configurations after evaluating them on lower budgets [Li et al., 2017, Falkner et al., 2018]. In contrast to the online and gradient-based alternatives, gray-box approaches can be deployed in an off-the-shelf manner to all types of hyperparameters and architectures. The gray-box concept is based on the intuition that a poorly-performing hyperparameter configuration can be identified and ∗equal contribution †work does not relate to position at Amazon 36th Conference on Neural Information Processing Systems (NeurIPS 2022). arXiv:2202.09774v2  [cs.LG]  1 Jun 2023terminated by inspecting the validation loss of the first few epochs, instead of waiting for the full convergence. The most prominent gray-box algorithm is Hyperband [Li et al., 2017], which is based on successive halving. It runs random configurations at different budgets (e.g. number of epochs) and successively halves these configurations by keeping only the top performers. Follow-up works, such as BOHB [Falkner et al., 2018] or DEHB [Awad et al., 2021], replace the random sampling of Hyperband with a sampling based on Bayesian optimization or differentiable evolution. Despite their great practical potential, gray-box methods suffer from a major issue. The low-budget (few epochs) performances are not always a good indicator for the full-budget (full convergence) performances. For example, a properly regularized network converges slower in the first few epochs, however, typically performs better than a non-regularized variant after the full convergence. In other words, there can be a poor rank correlation of the configurations’ performances at different budgets. Figure 1: Top: The learning curve for different hyperparameter configurations. The darker the learning curve, the later it was evaluated during the search. Bottom: The hyperparameter indices in a temporal order as evaluated during the opti- mization and their corresponding curves. We introduceDYHPO, a Bayesian Optimization (BO) approach based on Gaussian Processes (GP), that proposes a novel treatment to the multi-budget (a.k.a. multi-fidelity) setup. In this perspective, we propose a deep kernel GP that captures the learning dynamics. As a result, we train a kernel capable of capturing the similarity of a pair of hyperparameter configurations, even if the pair’s configurations are evaluated at differ- ent budgets. Furthermore, we extend Expected Improvement [Jones et al., 1998] to the multi- budget case, by introducing a new mechanism for the incumbent configuration of a budget. We illustrate the differences between our rac- ing strategy and successive halving with the experiment of Figure 1, where, we showcase the HPO progress of two different methods on the \"Helena\" dataset from the LCBench bench- mark [Zimmer et al., 2021]. Hyperband [Li et al., 2017] is a gray-box approach that stati- cally pre-allocates the budget for a set of candidates (Hyperband bracket) according to a predefined policy. However, DYHPO dynamically adapts the allocation of budgets for configurations after every HPO step (a.k.a. a dynamic race). As a result, DYHPO invests only a small budget on configurations that show little promise as indicated by the intermediate scores. The joint effect of modeling a GP kernel across budgets together with a dedicated acquisition function leads to DYHPO achieving a statistically significant empirical gain against state-of-the-art gray-box baselines [Falkner et al., 2018, Awad et al., 2021], including prior work on multi-budget GPs [Kandasamy et al., 2017, 2020] or neural networks [Li et al., 2020b]. We demonstrate the performance of DYHPO in three diverse deep learning architectures (MLP, CNN/NAS, RNN) and 50 datasets of three diverse modalities (tabular, image, natural language processing). We believe our method is a step forward toward making HPO for DL practical and feasible. Overall, our contributions can be summarized as follows: • We introduce a novel Bayesian surrogate for gray-box HPO optimization. Our novel surrogate model predicts the validation score of a machine learning model based on both the hyperparameter configuration, the budget information, and the learning curve. • We derive a simple yet robust way to combine this surrogate model with Bayesian opti- mization, reusing most of the existing components currently used in traditional Bayesian optimization methods. • Finally, we demonstrate the efficiency of our method for HPO and neural architecture search tasks compared to the current state-of-the-art methods in HPO, by outperforming seven strong HPO baselines with a statistically significant margin. As an overarching goal, we believe our method is an important step toward scaling HPO for DL. 22 Related Work on Gray-box HPO Multi-Fidelity Bayesian Optimization and Bandits. Bayesian optimization is a black-box func- tion optimization framework that has been successfully applied in optimizing hyperparameter and neural architectures alike [Snoek et al., 2012, Kandasamy et al., 2018, Bergstra et al., 2011]. To further improve Bayesian optimization, several works propose low-fidelity data approximations of hyperparameter configurations by training on a subset of the data [Swersky et al., 2013, Klein et al., 2017a], or by terminating training early [Swersky et al., 2014]. Additionally, several methods extend Bayesian optimization to multi-fidelity data by engineering new kernels suited for this problem [Swer- sky et al., 2013, 2014, Poloczek et al., 2017]. Kandasamy et al. [2016] extends GP-UCB [Srinivas et al., 2010] to the multi-fidelity setting by learning one Gaussian Process (GP) with a standard kernel for each fidelity. Their later work improves upon this method by learning one GP for all fidelities that enables the use of continuous fidelities [Kandasamy et al., 2017]. The work by Takeno et al. [2020] follows a similar idea but proposes to use an acquisition function based on information gain instead of UCB. While most of the works rely on GPs to model the surrogate function, Li et al. [2020b] use a Bayesian neural network that models the complex relationship between fidelities with stacked neural networks, one for each fidelity. Hyperband [Li et al., 2017] is a bandits-based multi-fidelity method for hyperparameter optimization that selects hyperparameter configurations at random and uses successive halving [Jamieson and Tal- walkar, 2016] with different settings to early-stop less promising training runs. Several improvements have been proposed to Hyperband with the aim to replace the random sampling of hyperparameter configurations with a more guided approach [Bertrand et al., 2017, Wang et al., 2018, Wistuba, 2017]. BOHB [Falkner et al., 2018] uses TPE [Bergstra et al., 2011] and builds a surrogate model for every fidelity adhering to a fixed-fidelity selection scheme. DEHB [Awad et al., 2021] samples candidates using differential evolution which handles large and discrete search spaces better than BOHB. Mendes et al. [2021] propose a variant of Hyperband which allows to skip stages. Learning Curve Prediction A variety of methods attempt to extrapolate a partially observed learning curve in order to estimate the probability that a configuration will improve over the current best solution. Domhan et al. [2015] propose to ensemble a set of parametric functions to extrapolate a partial learning curve. While this method is able to extrapolate with a single example, it requires a relatively long learning curve to do so. The work by Klein et al. [2017b] build upon the idea of using a set of parametric functions. The main difference is that they use a heteroscedastic Bayesian model to learn the ensemble weights. Baker et al. [2018] propose to use support vector machines (SVM) as an auto-regressive model. The SVM predicts the next value of a learning curve, the original learning curve is augmented by this value and we keep predicting further values. The work by Gargiani et al. [2019] use a similar idea but makes prediction based on the last K observations only and uses probabilistic models. Wistuba and Pedapati [2020] propose to learn a prediction model across learning curves from different tasks to avoid the costly learning curve collection. In contrast to DYHPO , none of these methods selects configuration but is limited to deciding when to stop a running configuration. Multi-Fidelity Acquisition Functions Klein et al. [2017a] propose an acquisition function which allows for selecting hyperparameter configurations and the dataset subset size. The idea is to reduce training time by considering only a smaller part of the training data. In contrast to EIMF, this acquisition function is designed to select arbitrary subset sizes whereas EIMF is intended to slowly increase the invested budget over time. Mendes et al. [2020] extend the work of Klein et al. [2017a] to take business constraints into account. Deep Kernel Learning with Bayesian Optimization. We are among the first to use deep kernel learning with Bayesian optimization and to the best of our knowledge the first to use it for multi- fidelity Bayesian optimization. Rai et al. [2016] consider the use of a deep kernel instead of a manually designed kernel in the context of standard Bayesian optimization, but, limit their experimentation to synthetic data and do not consider its use for hyperparameter optimization. Perrone et al. [2018], Wistuba and Grabocka [2021] use a pre-trained deep kernel to warm start Bayesian optimization with meta-data from previous optimizations. The aforementioned approaches are multi-task or transfer learning methods that require the availability of meta-data from related tasks. 3In contrast to prior work, we propose a method that introduces deep learning to multi-fidelity HPO with Bayesian Optimization, and captures the learning dynamics across fidelities/budgets, combined with an acquisition function that is tailored for the gray-box setup. 3 Dynamic Multi-Fidelity HPO 3.1 Preliminaries Gray-Box Optimization. The gray-box HPO setting allows querying configurations with a smaller budget compared to the total maximal budget B. Thus, we can query from the response function f : X ×N → R where fi,j = f(xi, j) is the response after spending a budget ofj on configuration xi. As before, these observations are noisy and we observe yi,j = f(xi, j) +εj where εj ∼ N(0, σ2 j,n). Please note, we assume that the budget required to query fi,j+b after having queried fi,j is only b. Furthermore, we use the learning curve Yi,j−1 = (yi,1, . . . , yi,j−1) when predicting fi,j. Gaussian Processes (GP). Given a training data set D = {(xi, yi)}n i=1, the Gaussian Process assumption is that yi is a random variable and the joint distribution of all yi is assumed to be multivariate Gaussian distributed as y ∼ N(m (X) , k(X, X)) . Furthermore, f∗ for test instances x∗ are jointly Gaussian with y as:\u0014 y f∗ \u0015 ∼ N \u0012 m (X, x∗) , \u0012 Kn K∗ KT ∗ K∗∗ \u0013\u0013 . (1) The mean function m is often set to 0 and its covariance function k depends on parameters θ. For notational convenience, we use Kn = k (X, X|θ) +σ2 nI, K∗ = k (X, X∗|θ) and K∗∗ = k (X∗, X∗|θ) to define the kernel matrices. We can derive the posterior predictive distribution with mean and covariance as follows: E[f∗|X, y, X∗] =KT ∗ K−1 n y, cov [f∗|X, X∗] =K∗∗ − KT ∗ K−1 n K∗ (2) Often, the kernel function is manually engineered, one popular example is the squared exponential kernel. However, in this work, we make use of the idea of deep kernel learning [Wilson et al., 2016]. The idea is to model the kernel as a neural network φ and learn the best kernel transformation K (θ, w) :=k(φ(x, w), φ(x′; w)|θ), which allows us to use convolutional operations in our kernel. 3.2 Deep Multi-Fidelity Surrogate We propose to use a Gaussian Process surrogate model that infers the value of fi,j based on the hyperparameter configuration xi, the budget j as well as the past learning curve Yi,j−1. For this purpose, we use a deep kernel as: K (θ, w) :=k(φ(xi, Yi,j−1, j; w), φ(xi′ , Yi′,j′−1, j′; w); θ) (3) xi j Yi,j−1 · 1B P P P Convolution max P P P Figure 2: The feature extractor φ of our kernel. We use a squared exponential kernel for k and the neural network φ is composed of linear and convolutional layers as shown in Figure 2. We normalize the budget j to a range between 0 and 1 by dividing it by the maximum budget B. Afterward, it is concatenated with the hyper- parameter configuration xi and fed to a linear layer. The learning curve Yi,j−1 is transformed by a one-dimensional convolution followed by a global max pooling layer. Finally, both repre- sentations are fed to another linear layer. Its output will be the input to the kernel function k. Both, the kernel k and the neural network φ consist of trainable parameters θ and w, respectively. We find their optimal values by computing the maximum likelihood estimates as: ˆθ, ˆw = arg max θ,w p(y|X, Y, θ, w) ∝ arg min θ,w yTK (θ, w)−1 y + log|K (θ, w)| (4) 4In order to solve this optimization problem, we use gradient descent and Adam [Kingma and Ba, 2015] with a learning rate of 0.1. Given the maximum likelihood estimates, we can approximate the predictive posterior through p \u0010 fi,j|xi, Yi,j−1, j,D, ˆθ, ˆw \u0011 , and ultimately compute the mean and covariance of this Gaussian using Equation 2. 3.3 Multi-Fidelity Expected Improvement Expected improvement [Jones et al., 1998] is a commonly used acquisition function and is defined as: EI(x|D) =E[max {f(x) − ymax, 0}] , (5) where ymax is the largest observed value of f. We propose a multi-fidelity version of it as: EIMF(x, j|D) =E \u0002 max \b f(x, j) − ymax j , 0 \t\u0003 , (6) where: ymax j = \u001amax {y | ((x, ·, j), y) ∈ D} if ((x, ·, j), y) ∈ D max {y | (·, y) ∈ D} otherwise (7) Simply put, ymax j is the largest observed value of f for a budget of j if it exists already, otherwise, it is the largest observed value for any budget. If there is only one possible budget, the multi-fidelity expected improvement is identical to expected improvement. 3.4 The D YHPO Algorithm The DYHPO algorithm looks very similar to many black-box Bayesian optimization algorithms as shown in Algorithm 1. The big difference is that at each step we dynamically decide which candidate configuration to train for a small additional budget. Algorithm 1 DYHPO Algorithm 1: b(x) = 0∀x ∈ X 2: while not converged do 3: xi ← arg maxx∈X EIMF (x, b(x) + 1)(Sec. 3.3) 4: Observe yi,b(xi)+1. 5: b(xi) ← b(xi) + 1 6: D ← D ∪ \b ((xi, Yi,b(xi)−1, b(xi)), yi,b(xi)) \t 7: Update the surrogate on D. (Sec. 3.2) return xi with largest yi,·. Possible candidates are previously un- considered configurations as well as configurations that did not reach the maximum budget. In Line 2, the most promising candidate is chosen using the acquisition function introduced in Section 3.3 and the surrogate model’s predictions. It is important to high- light that we do not maximize the ac- quisition function along the budget di- mensionality. Instead, we set the bud- get b such that it is by exactly one higher than the budget used to eval- uate xi before. This ensures that we explore configurations by slowly increasing the budget. After the candidate and the corresponding budget are selected, the function f is evaluated and we observe yi,j (Line 3). This additional data point is added to D in Line 4. Then in Line 5, the surrogate model is updated according to the training scheme described in Section 3.2. 4 Experimental Protocol 4.1 Experimental Setup We evaluate DYHPO in three different settings on hyperparameter optimization for tabular, text, and image classification against several competitor methods, the details of which are provided in the following subsections. We ran all of our experiments on an Amazon EC2 M5 Instance (m5.xlarge). In our experiments, we report the mean of ten repetitions and we report two common metrics, the regret and the average rank. The regret refers to the absolute difference between the score of the solution found by an optimizer compared to the best possible score. If we report the regret as an aggregate result over multiple datasets, we report the mean over all regrets. The average rank is the metric we use to aggregate rank results over different datasets. We provide further implementation and training details in Appendix A.4. Our implementation of DYHPO is publicly available.3 3https://github.com/releaunifreiburg/DyHPO 54.2 Benchmarks In our experiments, we use the following benchmarks. We provide more details in Appendix A.1. LCBench: A learning curve benchmark [Zimmer et al., 2021] that evaluates neural network architec- tures for tabular datasets. LCBench contains learning curves for 35 different datasets, where 2,000 neural networks per dataset are trained for 50 epochs with Auto-PyTorch. TaskSet: A benchmark that features diverse tasks Metz et al. [2020] from different domains and includes 5 search spaces with different degrees of freedom, where, every search space includes 1000 hyperparameter configurations. In this work, we focus on a subset of NLP tasks (12 tasks) and we use the Adam8p search space with 8 continuous hyperparameters. NAS-Bench-201: A benchmark consisting of 15625 hyperparameter configurations representing different architectures on the CIFAR-10, CIFAR-100 and ImageNet datasets Dong and Yang [2020]. NAS-Bench-201 features a search space of 6 categorical hyperparameters and each architecture is trained for 200 epochs. 4.3 Baselines Random Search: A random/stochastic black-box search method for HPO. HyperBand: A multi-arm bandit method that extends successive halving by multiple brackets with different combinations of the initial number of configurations, and their initial budget [Li et al., 2017]. BOHB: An extension of Hyperband that replaces the random sampling of the initial configurations for each bracket with recommended configurations from a model-based approach [Falkner et al., 2018]. BOHB builds a model for every fidelity that is considered. DEHB: A method that builds upon Hyperband by exploiting differential evolution to sample the initial candidates of a Hyperband bracket [Awad et al., 2021]. ASHA: An asynchronous version of successive halving (or an asynchronous version of Hyperband if multiple brackets are run). ASHA Li et al. [2020a] does not wait for all configurations to finish inside a successive halving bracket, but, instead promotes configurations to the next successive halving bracket in real-time. MF-DNN: A multi-fidelity Bayesian optimization method that uses deep neural networks to capture the relationships between different fidelities Li et al. [2020b]. Dragonfly: We compare against BOCA [Kandasamy et al., 2017] by using the Dragonfly library Kan- dasamy et al. [2020]. This method suggests the next hyperparameter configuration as well as the budget it should be evaluated for. 4.4 Research Hypotheses and Associated Experiments Hypothesis 1: DYHPO achieves state-of-the-art results in multi-fidelity HPO. Experiment 1: We compare DYHPO against the baselines of Section 4.3 on the benchmarks of Section 4.2 with the experimental setup of Section 4.1. For TaskSet we follow the authors’ recommendation and report the number of steps (every 200 iterations). Hypothesis 2: DYHPO’s runtime overhead has a negligible impact on the quality of results. Experiment 2: We compare DYHPO against the baselines of Section 4.3 over the wallclock time. The wallclock time includes both (i) the optimizer’s runtime overhead for recommending the next hyperparameter configuration, plus (ii) the time needed to evaluate the recommended configuration. In this experiment, we consider all datasets where the average training time per epoch is at least 10 seconds, because, for tasks where the training time is short, there is no practical justification for complex solutions and their overhead. In these cases, we recommend using a random search. We don’t report results for TaskSet because the benchmark lacks training times. Hypothesis 3: DYHPO uses the computational budget more efficiently than baselines. Experiment 3: To further verify that DYHPO is efficient compared to the baselines, we investigate whether competing methods spend their budgets on qualitative candidates. Concretely we: i) calculate 60 500 1000 Number of Epochs 10 2 10 1 Mean Regret LCBench 0 500 1000 Number of Steps 10 1 TaskSet 0 2000 4000 Number of Epochs 10 1 ImageNet16-120 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO Figure 3: The mean regret for the different benchmarks over the number of epochs or steps (every 200 iterations). The results are aggregated over 35 different datasets for LCBench and aggregated over 12 different NLP tasks for TaskSet. the precision of the top (w.r.t. ground truth) performing configurations that were selected by each method across different budgets, ii) compute the average regret of the selected configurations across budget, and iii) we compare the fraction of top-performing configurations at a given budget that were not top performers at lower budgets, i.e. measure the ability to handle the poor correlation of performances across budgets. 5 Results Experiment 1: DYHPO achieves state-of-the-art results. In our first experiment, we evaluate the various methods on the benchmarks listed in Section 4.2. We show the aggregated results in Figure 3, the results show that DYHPO manages to outperform competitor methods over the set of considered benchmarks by achieving a better mean regret across datasets. Not only does DYHPO achieve a better final performance, it also achieves strong anytime results by converging faster than the competitor methods. For the extended results, related to the performance of all methods on a dataset level, we refer the reader to Appendix B. 12345678 Random Dragonfly Hyperband MF-DNN ASHA BOHB DEHB DyHPO LCBench@50% 12345678 Random Hyperband MF-DNN Dragonfly DEHB BOHB ASHA DyHPO LCBench@100% 12345678 Random MF-DNN ASHA Dragonfly Hyperband DEHB BOHB DyHPO TaskSet@50% 12345678 Random MF-DNN ASHA Dragonfly DEHB Hyperband BOHB DyHPO TaskSet@100% Figure 4: Critical difference diagram for LCBench and TaskSet in terms of the number of HPO steps. The results correspond to results after 500 and 1,000 epochs. Connected ranks via a bold bar indicate that performances are not significantly different (p >0.05). In Figure 4, we provide further evidence thatDYHPO’s improvement over the baselines is statistically significant. The critical difference diagram presents the ranks of all methods and provides information on the pairwise statistical difference between all methods for two fractions of the number of HPO steps (50% and 100%). We included the LCBench and TaskSet benchmarks in our significance plots. NAS-Bench-201 was omitted because it has only 3 datasets and the statistical test cannot be applied. Horizontal lines indicate groupings of methods that are not significantly different. As suggested by the best published practices Demsar [2006], we use the Friedman test to reject the null hypothesis followed by a pairwise post-hoc analysis based on the Wilcoxon signed-rank test (α = 0.05). For LCBench, DYHPO already outperforms the baselines significantly after 50% of the search budget, with a statistically significant margin. As the optimization procedure continues, DYHPO manages to extend its gain in performance and is the only method that has a statistically significant improvement against all the other competitor methods. Similarly, for TaskSet, DYHPO manages to outperform all methods with a statistically significant margin only halfway through the optimization procedure and achieves the best rank over all methods. However, as the optimization procedure continues, BOHB manages to decrease the performance gap with DYHPO , although, it still achieves a worse rank across all datasets. Considering the empirical results, we conclude that Hypothesis 1 is validated and that DYHPO achieves state-of-the-art results on multi-fidelity HPO. Experiment 2: On the impact of DYHPO ’s overhead on the results. We present the results of our second experiment in Figure 5 (left), where, as it can be seen, DYHPO still outperforms the other methods when its overhead is considered. For LCBench, DYHPO manages to get an advantage 70.2 0.4 0.6 0.8 1.0 Normalized Wallclock Time 10 1 Mean Regret LCBench 103 105 Wallclock Time in Seconds 10 1 ImageNet16-120 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 12345678 Random Dragonfly ASHA Hyperband MF-DNN BOHB DEHB DyHPO LCBench@50% 12345678 Random Dragonfly ASHA Hyperband MF-DNN BOHB DEHB DyHPO LCBench@100% Figure 5: Left: The regret over time for all methods during the optimization procedure for the LCBench benchmark and the ImageNet dataset from the NAS-Bench-201 benchmark. The normalized wall clock time represents the actual run time divided by the total wall clock time ofDYHPO including the overhead of fitting the deep GP. Right: The critical difference diagram for LCBench halfway through the HPO wall-clock time, and in the end. Connected ranks via a bold bar indicate that differences are not significant (p >0.05). fairly quickly and it only increases the gap in performance with the other methods as the optimization process progresses. Similarly, in the case of ImageNet from NAS-Bench-201, DYHPO manages to gain an advantage earlier than other methods during the optimization procedure. Although in the end DYHPO still performs better than all the other methods, we believe most of the methods converge to a good solution and the differences in the final performance are negligible. For the extended results, related to the performance of all methods on a dataset level over time, we refer the reader to the plots in Appendix B. Additionally, in Figure 5 (right), we provide the critical difference diagrams for LCBench that present the ranks and the statistical difference of all methods halfway through the optimization procedure, and in the end. As it can be seen, DYHPO has a better rank with a significant margin with only half of the budget used and it retains the advantage until the end. Experiment 3: On the efficiency of DYHPO . In Figure 6 (left), we plot the precision of every method for different budgets during the optimization procedure, which demonstrates that DYHPO effectively explores the search space and identifies promising candidates. The precision at an epoch i is defined as the number of top 1% candidates that are trained, divided by the number of all candidates trained, both trained for at least i epochs. The higher the precision, the more relevant candidates were considered and the less computational resources were wasted. For small budgets, the precision is low since DYHPO spends budget to consider various candidates, but then, promising candidates are successfully identified and the precision quickly increases. This argument is further supported in Figure 6 (middle), where we visualize the average regret of all the candidates trained for at least the specified number of epochs on the x-axis. In contrast to the regret plots, here we do not show the regret of the best configuration, but the mean regret of all the selected configurations. The analysis deduces a similar finding, our method DYHPO selects more qualitative hyperparameter configurations than all the baselines. An interesting property of multi-fidelity HPO is the phenomenon of poor rank correlations among the validation performance of candidates at different budgets. In other words, a configuration that achieves a poor performance at a small budget can perform better at a larger budget. To analyze this phenomenon, we measure the percentage of \"good\" configurations at a particular budget, that were \"bad\" performers in at least one of the smaller budgets. We define a \"good\" performance at a budget B when a configuration achieves a validation accuracy ranked among the top 1/3 of the validation accuracies belonging to all the other configurations that were run until that budget B. In Figure 6 (right), we analyze the percentage of \"good\" configurations at each budget denoted by the x-axis, that were \"bad\" performers in at least one of the lower budgets. Such a metric is a proxy for the degree of the promotion of \"bad\" configurations towards higher budgets. We present the analysis for all the competing methods of our experimental protocol from Section 4. We have additionally included the ground-truth line annotated as \"Baseline\", which represents the fraction of past poor performers among all the feasible configurations in the search space. In contrast, the respective methods compute the fraction of promotions only among the configurations that those methods have considered (i.e. selected within their HPO trials) until the budget indicated by the x-axis. We see that there is a high degree of \"good\" configurations that were \"bad\" at a previous budget, with fractions of the ground-truth \"Baseline\" going up to 40% for the LCBench benchmark and up to 80% for the NAS-Bench-201 benchmark. 80 10 20 30 40 50 Number of Epochs 0.0 0.1 0.2 0.3 0.4Precision of Top Candidates LCBench Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 10 20 30 40 50 Number of Epochs 0.05 0.10 0.15Average Regret LCBench Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 10 20 30 40 50 Number of Epochs 0.0 0.2 0.4 0.6 0.8 Fraction of Poor Performer Promotions LCBench Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO Baseline 0 50 100 150 200 Number of Epochs 0.05 0.10 0.15Precision of Top Candidates NAS-Bench-201 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 50 100 150 200 Number of Epochs 0.04 0.06 0.08 0.10 0.12Average Regret NAS-Bench-201 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 50 100 150 200 Number of Epochs 0.0 0.2 0.4 0.6 0.8 1.0 Fraction of Poor Performer Promotions NAS-Bench-201 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO Baseline Figure 6: The efficiency of DYHPO as the optimization progresses. Left: The fraction of top- performing candidates from all candidates that were selected to be trained. Middle: The average regret for the configurations that were selected to be trained at a given budget. Right: The percentage of configurations that belong to the top 1/3 configurations at a given budget and that were in the top bottom 2/3 of the configurations at a previous budget. All of the results are from the LCBench and NAS-Bench-201 benchmark. On the other hand, the analysis demonstrates that our method DYHPO has promoted more \"good\" configurations that were \"bad\" in a lower budget, compared to all the rival methods. In particular, more than 80% of selected configurations from the datasets belonging to either benchmark were \"bad\" performers at a lower budget. The empirical evidence validates Hypothesis 3 and demonstrates that DYHPO efficiently explores qualitative candidates. We provide the results of our analysis for DYHPO’s efficiency on the additional benchmarks (Taskset) in Appendix B. Ablating the impact of the learning curve 102 103 104 105 Training Time in Seconds 10 1 Regret ImageNet16-120 DyHPO DyHPO w/o CNN Random Search Figure 7: Ablating the impact of the learning curve on DYHPO. One of the main differences between DYHPO and similar methods Kandasamy et al. [2017], is that the learning curve is an input to the kernel function. For this reason, we investigate the impact of this design choice. We consider a variation of DYHPO w/o CNN, which is simply DYHPO without the learning curve. It is worth emphasizing that both variants (with and without the learning curve) are multi-fidelity surrogates and both receive the budget informa- tion through the inputted index j in Equation 3. The only difference is that DYHPO additionally incorporates the pattern of the learning curve. We run the ablation on the NAS-Bench-201 benchmark and report the results for ImageNet, the largest dataset in our collection. The ablation results are shown in Figure 7, while the remaining results on the other datasets are shown in Figure 8 of the appendix. Based on the results from our learning curve ablation, we conclude that the use of an explicit learning curve representation leads to significantly better results. 6 Limitations of Our Method Although DYHPO shows a convincing and statistically significant reduction of the HPO time on diverse Deep Learning (DL) experiments, we cautiously characterized our method only as a \"step 9towards\" scaling HPO for DL. The reason for our restrain is the lack of tabular benchmarks for HPO on very large deep learning models, such as Transformers-based architectures [Devlin et al., 2019]. Additionally, the pause and resume part of our training procedure can only be applied when tuning the hyperparameters of parametric models, otherwise, the training of a hyperparameter configuration would have to be restarted. Lastly, for small datasets that can be trained fast, the overhead of model-based techniques would make an approach like random search more appealing. 7 Conclusions In this work, we present DYHPO , a new Bayesian optimization (BO) algorithm for the gray-box setting. We introduced a new surrogate model for BO that uses a learnable deep kernel and takes the learning curve as an explicit input. Furthermore, we motivated a variation of expected improvement for the multi-fidelity setting. Finally, we compared our approach on diverse benchmarks on a total of 50 different tasks against the current state-of-the-art methods on gray-box hyperparameter optimization (HPO). Our method shows significant gains and has the potential to become the de facto standard for HPO in Deep Learning. Acknowledgments Josif Grabocka and Arlind Kadra would like to acknowledge the grant awarded by the Eva-Mayr-Stihl Stiftung. In addition, this research was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under grant number 417962828 and grant INST 39/963-1 FUGG (bwForCluster NEMO). In addition, Josif Grabocka acknowledges the support of the BrainLinks- BrainTools center of excellence. References Noor H. Awad, Neeratyoy Mallik, and Frank Hutter. DEHB: evolutionary hyberband for scalable, robust and efficient hyperparameter optimization. In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI 2021, Virtual Event / Montreal, Canada, 19-27 August 2021, pages 2147–2153, 2021. doi: 10.24963/ijcai.2021/296. URL https://doi.org/ 10.24963/ijcai.2021/296. Bowen Baker, Otkrist Gupta, Ramesh Raskar, and Nikhil Naik. Accelerating neural architecture search using performance prediction. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Workshop Track Proceedings, 2018. URL https://openreview.net/forum?id=HJqk3N1vG. James Bergstra, Rémi Bardenet, Yoshua Bengio, and Balázs Kégl. Algorithms for hyper-parameter optimization. In Advances in Neural Information Processing Systems 24: 25th Annual Conference on Neural Information Processing Systems 2011. Proceedings of a meeting held 12-14 December 2011, Granada, Spain, pages 2546–2554, 2011. URL https://proceedings.neurips.cc/ paper/2011/hash/86e8f7ab32cfd12577bc2619bc635690-Abstract.html. Hadrien Bertrand, Roberto Ardon, Matthieu Perrot, and Isabelle Bloch. Hyperparameter optimization of deep neural networks: Combining hyperband with bayesian model selection. In Conférence sur l’Apprentissage Automatique, 2017. Yutian Chen, Matthew W. Hoffman, Sergio Gomez Colmenarejo, Misha Denil, Timothy P. Lillicrap, Matthew Botvinick, and Nando de Freitas. Learning to learn without gradient descent by gradient descent. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pages 748–756, 2017. URL http://proceedings. mlr.press/v70/chen17e.html. Janez Demsar. Statistical comparisons of classifiers over multiple data sets. J. Mach. Learn. Res., 7: 1–30, 2006. URL http://jmlr.org/papers/v7/demsar06a.html. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of 10the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 4171–4186, 2019. doi: 10.18653/v1/n19-1423. URL https://doi.org/ 10.18653/v1/n19-1423. Tobias Domhan, Jost Tobias Springenberg, and Frank Hutter. Speeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves. In Qiang Yang and Michael J. Wooldridge, editors, Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence, IJCAI 2015, Buenos Aires, Argentina, July 25-31, 2015, pages 3460–3468. AAAI Press, 2015. URL http://ijcai.org/Abstract/15/487. Xuanyi Dong and Yi Yang. Nas-bench-201: Extending the scope of reproducible neural architecture search. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020, 2020. URL https://openreview.net/forum?id=HJxyZkBKDr. Stefan Falkner, Aaron Klein, and Frank Hutter. BOHB: robust and efficient hyperparameter optimiza- tion at scale. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, pages 1436–1445, 2018. URL http://proceedings.mlr.press/v80/falkner18a.html. Luca Franceschi, Michele Donini, Paolo Frasconi, and Massimiliano Pontil. Forward and reverse gradient-based hyperparameter optimization. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pages 1165–1173, 2017. URL http://proceedings.mlr.press/v70/franceschi17a.html. Jacob R. Gardner, Geoff Pleiss, Kilian Q. Weinberger, David Bindel, and Andrew Gordon Wil- son. Gpytorch: Blackbox matrix-matrix gaussian process inference with GPU acceleration. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural In- formation Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada , pages 7587–7597, 2018. URL https://proceedings.neurips.cc/paper/2018/hash/ 27e8e17134dd7083b050476733207ea1-Abstract.html. Matilde Gargiani, Aaron Klein, Stefan Falkner, and Frank Hutter. Probabilistic rollouts for learning curve extrapolation across hyperparameter settings. CoRR, abs/1910.04522, 2019. URL http: //arxiv.org/abs/1910.04522. Kevin G. Jamieson and Ameet Talwalkar. Non-stochastic best arm identification and hyperparameter optimization. In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, AISTATS 2016, Cadiz, Spain, May 9-11, 2016 , pages 240–248, 2016. URL http://proceedings.mlr.press/v51/jamieson16.html. Donald R. Jones, Matthias Schonlau, and William J. Welch. Efficient global optimization of ex- pensive black-box functions. J. Global Optimization , 13(4):455–492, 1998. doi: 10.1023/A: 1008306431147. URL https://doi.org/10.1023/A:1008306431147. Kirthevasan Kandasamy, Gautam Dasarathy, Junier B. Oliva, Jeff G. Schneider, and Barn- abás Póczos. Gaussian process bandit optimisation with multi-fidelity evaluations. In Advances in Neural Information Processing Systems 29: Annual Conference on Neu- ral Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain , pages 992–1000, 2016. URL https://proceedings.neurips.cc/paper/2016/hash/ 605ff764c617d3cd28dbbdd72be8f9a2-Abstract.html. Kirthevasan Kandasamy, Gautam Dasarathy, Jeff G. Schneider, and Barnabás Póczos. Multi-fidelity bayesian optimisation with continuous approximations. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pages 1799–1808, 2017. URL http://proceedings.mlr.press/v70/kandasamy17a.html. Kirthevasan Kandasamy, Willie Neiswanger, Jeff Schneider, Barnabás Póczos, and Eric P. Xing. Neural architecture search with bayesian optimisation and optimal transport. In Ad- vances in Neural Information Processing Systems 31: Annual Conference on Neural Infor- mation Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada , pages 2020–2029, 2018. URL https://proceedings.neurips.cc/paper/2018/hash/ f33ba15effa5c10e873bf3842afb46a6-Abstract.html. 11Kirthevasan Kandasamy, Karun Raju Vysyaraju, Willie Neiswanger, Biswajit Paria, Christopher R. Collins, Jeff Schneider, Barnabás Póczos, and Eric P. Xing. Tuning hyperparameters without grad students: Scalable and robust bayesian optimisation with dragonfly. J. Mach. Learn. Res., 21: 81:1–81:27, 2020. URL http://jmlr.org/papers/v21/18-223.html. Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1412.6980. Aaron Klein, Stefan Falkner, Simon Bartels, Philipp Hennig, and Frank Hutter. Fast bayesian optimization of machine learning hyperparameters on large datasets. In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, AISTATS 2017, 20-22 April 2017, Fort Lauderdale, FL, USA, pages 528–536, 2017a. URL http://proceedings.mlr.press/ v54/klein17a.html. Aaron Klein, Stefan Falkner, Jost Tobias Springenberg, and Frank Hutter. Learning curve prediction with bayesian neural networks. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings , 2017b. URL https://openreview.net/forum?id=S11KBYclx. Liam Li, Kevin G. Jamieson, Afshin Rostamizadeh, Ekaterina Gonina, Jonathan Ben-tzur, Moritz Hardt, Benjamin Recht, and Ameet Talwalkar. A system for massively parallel hyperparameter tuning. In Inderjit S. Dhillon, Dimitris S. Papailiopoulos, and Vivienne Sze, editors,Proceedings of Machine Learning and Systems 2020, MLSys 2020, Austin, TX, USA, March 2-4, 2020. mlsys.org, 2020a. URL https://proceedings.mlsys.org/book/303.pdf. Lisha Li, Kevin G. Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. Hyper- band: A novel bandit-based approach to hyperparameter optimization. J. Mach. Learn. Res., 18: 185:1–185:52, 2017. URL http://jmlr.org/papers/v18/16-558.html. Shibo Li, Wei Xing, Robert M. Kirby, and Shandian Zhe. Multi-fidelity bayesian optimization via deep neural networks. In Advances in Neural Information Processing Systems 33: An- nual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual , 2020b. URL https://proceedings.neurips.cc/paper/2020/hash/ 60e1deb043af37db5ea4ce9ae8d2c9ea-Abstract.html. Jonathan Lorraine, Paul Vicol, and David Duvenaud. Optimizing millions of hyperparameters by implicit differentiation. In The 23rd International Conference on Artificial Intelligence and Statistics, AISTATS 2020, 26-28 August 2020, Online [Palermo, Sicily, Italy], pages 1540–1552, 2020. URL http://proceedings.mlr.press/v108/lorraine20a.html. Dougal Maclaurin, David Duvenaud, and Ryan P. Adams. Gradient-based hyperparameter opti- mization through reversible learning. In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015 , pages 2113–2122, 2015. URL http://proceedings.mlr.press/v37/maclaurin15.html. Pedro Mendes, Maria Casimiro, Paolo Romano, and David Garlan. Trimtuner: Efficient optimization of machine learning jobs in the cloud via sub-sampling. In 28th International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems, MASCOTS 2020, Nice, France, November 17-19, 2020, pages 1–8. IEEE, 2020. doi: 10.1109/MASCOTS50786. 2020.9285971. URL https://doi.org/10.1109/MASCOTS50786.2020.9285971. Pedro Mendes, Maria Casimiro, and Paolo Romano. Hyperjump: Accelerating hyperband via risk modelling. CoRR, abs/2108.02479, 2021. URL https://arxiv.org/abs/2108.02479. Luke Metz, Niru Maheswaranathan, Ruoxi Sun, C. Daniel Freeman, Ben Poole, and Jascha Sohl- Dickstein. Using a thousand optimization tasks to learn hyperparameter search strategies. CoRR, abs/2002.11887, 2020. URL https://arxiv.org/abs/2002.11887. Jack Parker-Holder, Vu Nguyen, and Stephen J. Roberts. Provably efficient online hyperparameter optimization with population-based bandits. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/ hash/c7af0926b294e47e52e46cfebe173f20-Abstract.html. 12Valerio Perrone, Rodolphe Jenatton, Matthias W. Seeger, and Cédric Archambeau. Scalable hyper- parameter transfer learning. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada, pages 6846–6856, 2018. URL https://proceedings.neurips.cc/ paper/2018/hash/14c879f3f5d8ed93a09f6090d77c2cc3-Abstract.html. Matthias Poloczek, Jialei Wang, and Peter I. Frazier. Multi-information source optimiza- tion. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA , pages 4288–4298, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/ df1f1d20ee86704251795841e6a9405a-Abstract.html. Akshara Rai, Ruta Desai, and Siddharth Goyal. Bayesian optimization with a neural network kernel, 2016. URL http://www.cs.cmu.edu/~rutad/files/BO_NN.pdf. Jasper Snoek, Hugo Larochelle, and Ryan P. Adams. Practical bayesian optimization of machine learn- ing algorithms. In Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United States, pages 2960–2968, 2012. URL https://proceedings. neurips.cc/paper/2012/hash/05311655a15b75fab86956663e1819cd-Abstract.html. Niranjan Srinivas, Andreas Krause, Sham M. Kakade, and Matthias W. Seeger. Gaussian process optimization in the bandit setting: No regret and experimental design. In Proceedings of the 27th International Conference on Machine Learning (ICML-10), June 21-24, 2010, Haifa, Israel, pages 1015–1022, 2010. URL https://icml.cc/Conferences/2010/papers/422.pdf. Kevin Swersky, Jasper Snoek, and Ryan Prescott Adams. Multi-task bayesian optimization. In Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States, pages 2004–2012, 2013. URL https://proceedings.neurips. cc/paper/2013/hash/f33ba15effa5c10e873bf3842afb46a6-Abstract.html. Kevin Swersky, Jasper Snoek, and Ryan Prescott Adams. Freeze-thaw bayesian optimization. CoRR, abs/1406.3896, 2014. URL http://arxiv.org/abs/1406.3896. Shion Takeno, Hitoshi Fukuoka, Yuhki Tsukada, Toshiyuki Koyama, Motoki Shiga, Ichiro Takeuchi, and Masayuki Karasuyama. Multi-fidelity bayesian optimization with max-value entropy search and its parallelization. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, pages 9334–9345, 2020. URLhttp://proceedings. mlr.press/v119/takeno20a.html. Jiazhuo Wang, Jason Xu, and Xuejun Wang. Combination of hyperband and bayesian optimization for hyperparameter optimization in deep learning. CoRR, abs/1801.01596, 2018. URL http: //arxiv.org/abs/1801.01596. Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, and Eric P. Xing. Deep kernel learning. In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, AISTATS 2016, Cadiz, Spain, May 9-11, 2016, pages 370–378, 2016. URL http://proceedings.mlr. press/v51/wilson16.html. Martin Wistuba. Bayesian optimization combined with incremental evaluation for neural network architecture optimization. In AutoML@PKDD/ECML, 2017. Martin Wistuba and Josif Grabocka. Few-shot bayesian optimization with deep kernel surrogates. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021, 2021. URL https://openreview.net/forum?id=bJxgv5C3sYc. Martin Wistuba and Tejaswini Pedapati. Learning to rank learning curves. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 10303–10312. PMLR, 2020. URL http://proceedings.mlr.press/v119/wistuba20a.html. Lucas Zimmer, Marius Lindauer, and Frank Hutter. Auto-pytorch: Multi-fidelity metalearning for efficient and robust autodl. IEEE Trans. Pattern Anal. Mach. Intell., 43(9):3079–3090, 2021. doi: 10.1109/TPAMI.2021.3067763. URL https://doi.org/10.1109/TPAMI.2021.3067763. 13Checklist 1. For all authors... (a) Do the main claims made in the abstract and introduction accurately reflect the paper’s contributions and scope? [Yes] (b) Did you describe the limitations of your work? [Yes] See Section 6. (c) Did you discuss any potential negative societal impacts of your work? [Yes] See Section “Societal Implications” in the Appendix. (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] 2. If you are including theoretical results... (a) Did you state the full set of assumptions of all theoretical results? [N/A] (b) Did you include complete proofs of all theoretical results? [N/A] 3. If you ran experiments... (a) Did you include the code, data, and instructions needed to reproduce the main experi- mental results (either in the supplemental material or as a URL)? [No] We provide our main algorithm in Section 3 and we additionally provide the detailed implementation details in Appendix A for all methods and benchmarks. We will release the code for the camera-ready version of our work. (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] Please see Appendix A. (c) Did you report error bars (e.g., with respect to the random seed after running experi- ments multiple times)? [Yes] We report the statistical significance of the performance difference between our method and the baselines in Section 5 (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See Section 4.1. 4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... (a) If your work uses existing assets, did you cite the creators? [Yes] See Section 4.2 and Section 4.3. (b) Did you mention the license of the assets? [Yes] See Appendix A.1 and A.5 where we provide references to the assets where the license is included. (c) Did you include any new assets either in the supplemental material or as a URL? [No] (d) Did you discuss whether and how consent was obtained from people whose data you’re using/curating? [N/A] The benchmarks and baselines are open-sourced. (e) Did you discuss whether the data you are using/curating contains personally identi- fiable information or offensive content? [N/A] The data does not contain personally identifiable information or offensive content. 5. If you used crowdsourcing or conducted research with human subjects... (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A] 14Societal Implications In our work, we use only publicly available data with no privacy concerns. Furthermore, our algorithm reduces the overall time for fitting deep networks, therefore, saving computational resources and yielding a positive impact on the environment. Moreover, our method can help smaller research organizations with limited access to resources to be competitive in the deep learning domain, which reduces the investment costs on hardware. Although our method significantly reduces the time taken for optimizing a machine learning algorithm that achieves peak performance, we warn against running our method for an extended time only to achieve marginal gains in performance, unless it is mission-critical. Last but not least, in order to save energy, we invite the community to create sparse benchmarks with surrogates, instead of dense tabular ones. A Experimental Setup A.1 Benchmarks LCBench. LCBench4 is a feedforward neural network benchmark on tabular data which consists of 2000 configuration settings for each of the 35 datasets. The configurations were evaluated during HPO runs with AutoPyTorch. LCBench features a search space of 7 numerical hyperparameters, where every hyperparameter configuration is trained for 50 epochs. The objective is to optimize seven different hyperparameters of funnel-shaped neural networks, i.e., batch size, learning rate, momentum, weight decay, dropout, number of layers, and maximum number of units per layer. TaskSet. TaskSet5 is a benchmark that features over 1162 diverse tasks from different domains and includes 5 search spaces. In this work, we focus on NLP tasks and we use the Adam8p search space with 8 continuous hyperparameters. We refer to Figure 11 for the exact task names considered in our experiments. The learning curves provided in TaskSet report scores after every 200 iterations. We refer to those as \"steps\". The objective is to optimize eight hyperparameters for a set of different recurrent neural networks (RNN) that differ in embedding size, RNN cell, and other architectural features. The set of hyperparameters consists of optimizer-specific hyperparameters, such as the learning rate, the exponential decay rate of the first and second momentum of Adam, β1 and β2, and Adam’s constant for numerical stability ε. Furthermore, there are two hyperparameters controlling linear and exponential learning rate decays, as well as L1 and L2 regularization terms. NAS-Bench-201. NAS-Bench-2016 is a benchmark that has precomputed about 15,600 archi- tectures trained for 200 epochs for the image classification datasets CIFAR-10, CIFAR-100, and ImageNet. The objective is to select for each of the six operations within the cell of the macro architecture one of five different operations. All other hyperparameters such as learning rate and batch size are kept fixed. NAS-Bench-201 features a search space of 6 categorical hyperparameters and each architecture is trained for 200 epochs. A.2 Preprocessing In the following, we describe the preprocessing applied to the hyperparameter representation. For LCBench, we apply a log-transform to batch size, learning rate, and weight decay. For TaskSet, we apply it on the learning rate, L1 and L2 regularization terms, epsilon, linear and exponential decay of the learning rate. All continuous hyperparameters are scaled to the range between 0 and 1 using sklearn’s MinMaxScaler. If not mentioned otherwise, we use one-hot encoding for the categorical hyperparameters. As detailed in subsection A.5, some baselines have a specific way of dealing with them. In that case, we use the method recommended by the authors. A.3 Framework The framework contains the evaluated hyperparameters and their corresponding validation curves. The list of candidate hyperparameters is passed to the baseline-specific interface, which in turn, 4https://github.com/automl/LCBench 5https://github.com/google-research/google-research/tree/master/task_set 6https://github.com/D-X-Y/NAS-Bench-201 15optimizes and queries the framework for the hyperparameter configuration that maximizes utility. Our framework in turn responds with the validation curve and the cost of the evaluation. In case a hyperparameter configuration has been evaluated previously up to a budget b and a baseline requires the response for budget b + 1, the cost is calculated accordingly only for the extra budget requested. A.4 Implementation Details We implement the Deep Kernel Gaussian Process using GPyTorch 1.5 [Gardner et al., 2018]. We use an RBF kernel and the dense layers of the transformation functionφ have 128 and 256 units. We used a convolutional layer with a kernel size of three and four filters. All parameters of the Deep Kernel are estimated by maximizing the marginal likelihood. We achieve this by using gradient ascent and Adam [Kingma and Ba, 2015] with a learning rate of 0.1 and batch size of 64. We stop training as soon as the training likelihood is not improving for 10 epochs in a row or we completed 1,000 epochs. For every new data point, we start training the GP with its old parameters to reduce the required effort for training. A.5 Baselines Random Search & Hyperband. Random search and Hyperband sample hyperparameter config- urations at random and therefore the preprocessing is irrelevant. We have implemented both from scratch and use the recommended hyperparameters for Hyperband, i.e. η = 3. BOHB. For our experiments with BOHB, we use version 0.7.4 of the officially-released code7. DEHB. For our experiments with DEHB, we use the official public implementation8. We devel- oped an interface that communicates between our framework and DEHB. In addition to the initial preprocessing common for all methods, we encode categorical hyperparameters with a numerical value in the interval [0, 1]. For a categorical hyperparameter xi, we take Ni equal-sized intervals, where Ni represents the number of unique categorical values for hyperparameter xi and we assign the value for a categorical value n ∈ Ni to the middle of the interval [n, n+ 1]as suggested by the authors. For configuring the DEHB algorithm we used the default values from the library. Dragonfly. We use the publicly available code of Dragonfly9. No special treatment of categorical hyperparameters is required since Dragonfly has its own way to deal with them. We use version 0.1.6 with default settings. MF-DNN. We use the official implementation of MF-DNN by the authors10. Initially, we tried to use multiple incremental fidelity levels like for DYHPO, however, the method runtime was too high and it could not achieve competitive results. For that reason, we use only a few fidelity levels like the authors do in their work Li et al. [2020b]. We use the same fidelity levels as for Hyperband, DEHB, and BOHB to have a fair comparison between the baselines. We also use the same number of initial points as for the other methods to have the same maximal resource allocated for every fidelity level. ASHA-HB. We use the public implementation from the well-known optuna library (version2.10.0). We used the same eta, minimum and maximal budget as for HB, DEHB, and BOHB in our experi- ments, to have a fair comparison. B Additional Plots In Figure 8, we ablate the learning curve input in our kernel, to see the effect it has on performance for the CIFAR-10 and CIFAR-100 datasets from the NAS-Bench-201 benchmark. The results indicate that the learning curve plays an important role in achieving better results by allowing faster convergence and a better anytime performance. 7https://github.com/automl/HpBandSter 8https://github.com/automl/DEHB/ 9https://github.com/dragonfly/dragonfly 10https://github.com/shib0li/DNN-MFBO 16102 103 104 105 Training Time in Seconds 10 2 10 1 Regret cifar10 DyHPO DyHPO w/o CNN Random Search 102 103 104 105 Training Time in Seconds 10 2 10 1 Regret cifar100 DyHPO DyHPO w/o CNN Random Search Figure 8: The learning curve ablation for the CIFAR-10 and CIFAR-100 tasks of NAS-Bench-201. 0 1000 2000 3000 4000 Number of Epochs 10 2 10 1 Regret cifar10 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 1000 2000 3000 4000 Number of Epochs 10 2 10 1 Regret cifar100 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO Figure 9: NAS-Bench-201 regret results over the number of epochs spent during the optimization. Additionally, in Figure 9, we show the performance comparison over the number of epochs of every method for the CIFAR-10 and CIFAR-100 datasets in the NAS-Bench-201 benchmark. While, in Figure 10, we present the performance comparison over time. As can be seen, DYHPO converges faster and has a better performance compared to the other methods over the majority of the time or steps, however, towards the end although it is the optimal method or close to the optimal method, the difference in regret is not significant anymore. Furthermore, Figure 11 shows the performance comparison for the datasets chosen from TaskSet over the number of steps. Looking at the results, DYHPO is outperforming all methods convincingly on the majority of datasets by converging faster and with significant differences in the regret evaluation metric. In Figure 12 and 13, we show the performance comparison for all the datasets from LCBench regarding regret over the number of epochs. Similarly, in Figure 14 and 15, we show the same performance comparison, however, over time. As can be seen, DYHPO manages to outperform the other competitors in the majority of the datasets, and in the datasets that it does not, it is always close to the top-performing method, and the difference between methods is marginal. In Figure 16 we provide the extended results of Experiment 3 for TaskSet. We show the precision, average regret, and promotion percentage for poor-performing configurations for DYHPO and the other competitor methods. Lastly, we explore the behavior of DYHPO after finding the configuration which is returned at the end of the optimization as the best configuration. In Figure 17, we show how the budget is distributed on the configurations considered during that part of the optimization. Clearly, DYHPO is spending very little budget on most configurations. Furthermore, we investigated how many new configurations are considered during this phase. For LCBench, 76.98% of considered configurations are new demonstrating that DYHPO is investigating most of the budget into exploration. These are even more extreme for TaskSet (93.16% and NAS-Bench-201 (97.51%). 17102 103 104 Wallclock Time in Seconds 10 2 10 1 Regret cifar10 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 102 103 104 Wallclock Time in Seconds 10 2 10 1 Regret cifar100 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO Figure 10: NAS-Bench-201 regret results over the total optimization time. The total time includes the method overhead time and the hyperparameter configuration evaluation time. 0 200 400 600 800 1000 Number of Steps 10 3 10 2 10 1 Regret FixedTextRNNClassification imdb_patch128_LSTM128_avg_bs64 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Steps 10 3 10 2 10 1 Regret FixedTextRNNClassification imdb_patch128_LSTM128_bs64 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Steps 10 1 Regret FixedTextRNNClassification imdb_patch128_LSTM128_embed128_bs64 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Steps 10 1 6 × 10 2 2 × 10 1 3 × 10 1 Regret FixedTextRNNClassification imdb_patch32_GRU128_bs128 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Steps 10 1 Regret FixedTextRNNClassification imdb_patch32_GRU64_avg_bs128 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Steps 10 1 Regret FixedTextRNNClassification imdb_patch32_IRNN64_relu_avg_bs128 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Steps 10 1 100 Regret FixedTextRNNClassification imdb_patch32_IRNN64_relu_last_bs128 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Steps 10 2 10 1 Regret FixedTextRNNClassification imdb_patch32_LSTM128_E128_bs128 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Steps 10 2 10 1 Regret FixedTextRNNClassification imdb_patch32_LSTM128_bs128 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Steps 10 1 2 × 10 1 Regret FixedTextRNNClassification imdb_patch32_VRNN128_tanh_bs128 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Steps 10 1 Regret FixedTextRNNClassification imdb_patch32_VRNN64_relu_avg_bs128 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Steps 10 3 10 2 10 1 Regret FixedTextRNNClassification imdb_patch32_VRNN64_tanh_avg_bs128 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO Figure 11: Performance comparison over the number of steps on a dataset level for TaskSet. 180 200 400 600 800 1000 Number of Epochs 10 2 Regret APSFailure Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 1 3 × 10 2 4 × 10 2 6 × 10 2 Regret Amazon_employee_access Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 3 10 2 10 1 Regret Australian Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 3 10 2 10 1 Regret Fashion-MNIST Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret KDDCup09_appetency Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 4 10 3 10 2 10 1 Regret MiniBooNE Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 3 10 2 10 1 Regret adult Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 3 10 2 Regret airlines Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 3 10 2 10 1 Regret albert Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret bank-marketing Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret blood-transfusion-service-center Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 1 Regret car Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret christine Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 100 Regret cnae-9 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret connect-4 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 4 10 3 10 2 10 1 Regret covertype Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 1 Regret credit-g Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 4 10 2 100 Regret dionis Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO Figure 12: Performance comparison over the number of steps on a dataset level for LCBench. 190 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret fabert Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 1 Regret helena Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret higgs Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret jannis Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret jasmine Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 4 10 3 10 2 10 1 Regret jungle_chess_2pcs_raw_endgame_complete Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 1 Regret kc1 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret kr-vs-kp Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret mfeat-factors Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 4 10 3 10 2 10 1 Regret nomao Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 Regret numerai28.6 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret phoneme Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 1 Regret segment Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 1 Regret shuttle Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 4 10 2 Regret sylvine Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 3 10 2 10 1 Regret vehicle Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret volkert Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO Figure 13: Performance comparison over the number of steps on a dataset level for LCBench (cont.). 200 250 500 750 1000 1250 Wallclock Time in Seconds 10 2 Regret APSFailure Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 500 1000 1500 Wallclock Time in Seconds 10 1 3 × 10 2 4 × 10 2 6 × 10 2 Regret Amazon_employee_access Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 100 200 300 Wallclock Time in Seconds 10 3 10 2 10 1 Regret Australian Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 1000 2000 3000 4000 5000 Wallclock Time in Seconds 10 2 10 1 Regret Fashion-MNIST Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 500 1000 1500 2000 2500 Wallclock Time in Seconds 10 2 10 1 Regret KDDCup09_appetency Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 500 1000 1500 Wallclock Time in Seconds 10 1 Regret MiniBooNE Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 500 1000 1500 2000 Wallclock Time in Seconds 10 3 10 2 10 1 Regret adult Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 2500 5000 7500 10000 12500 Wallclock Time in Seconds 10 3 10 2 Regret airlines Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 2000 4000 6000 8000 10000 Wallclock Time in Seconds 10 2 Regret albert Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 500 1000 1500 2000 Wallclock Time in Seconds 10 2 10 1 Regret bank-marketing Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Wallclock Time in Seconds 10 2 10 1 Regret blood-transfusion-service-center Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Wallclock Time in Seconds 10 1 Regret car Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 5000 10000 15000 Wallclock Time in Seconds 10 2 10 1 Regret christine Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 500 1000 1500 2000 2500 Wallclock Time in Seconds 10 2 10 1 100 Regret cnae-9 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 500 1000 1500 2000 2500 Wallclock Time in Seconds 10 2 10 1 Regret connect-4 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 1000 2000 3000 4000 5000 Wallclock Time in Seconds 10 1 Regret covertype Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Wallclock Time in Seconds 10 1 Regret credit-g Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 2000 4000 6000 8000 Wallclock Time in Seconds 10 1 Regret dionis Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO Figure 14: Performance comparison over time on a dataset level for LCBench with the overhead included. 210 1000 2000 3000 Wallclock Time in Seconds 10 2 10 1 Regret fabert Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Wallclock Time in Seconds 10 1 Regret helena Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Wallclock Time in Seconds 10 1 Regret higgs Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 1000 2000 3000 Wallclock Time in Seconds 10 2 10 1 Regret jannis Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 250 500 750 1000 1250 Wallclock Time in Seconds 10 2 10 1 Regret jasmine Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 500 1000 1500 2000 Wallclock Time in Seconds 10 2 10 1 Regret jungle_chess_2pcs_raw_endgame_complete Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Wallclock Time in Seconds 10 1 Regret kc1 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 250 500 750 1000 Wallclock Time in Seconds 10 2 10 1 Regret kr-vs-kp Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 250 500 750 1000 1250 Wallclock Time in Seconds 10 2 10 1 Regret mfeat-factors Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 500 1000 1500 2000 Wallclock Time in Seconds 10 2 10 1 Regret nomao Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Wallclock Time in Seconds 10 2 Regret numerai28.6 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 250 500 750 1000 1250 Wallclock Time in Seconds 10 2 10 1 Regret phoneme Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Wallclock Time in Seconds 10 1 Regret segment Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 500 1000 1500 2000 Wallclock Time in Seconds 10 1 Regret shuttle Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 250 500 750 1000 1250 Wallclock Time in Seconds 10 4 10 2 Regret sylvine Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Wallclock Time in Seconds 10 2 10 1 Regret vehicle Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 1000 2000 3000 Wallclock Time in Seconds 10 2 10 1 Regret volkert Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO Figure 15: Performance comparison over time on a dataset level for LCBench with the overhead included. (cont.). 220 10 20 30 40 50 Number of Epochs 0.0 0.1 0.2 0.3Precision of Top Candidates Taskset Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 10 20 30 40 50 Number of Epochs 0.10 0.15 0.20 0.25 0.30Average Regret Taskset Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 10 20 30 40 50 Number of Steps 0.0 0.2 0.4 0.6 0.8 Fraction of Poor Performer Promotions Taskset Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO Baseline Figure 16: The efficiency of DYHPO as the optimization progresses. Left: The fraction of top- performing candidates from all candidates that were selected to be trained. Middle: The average regret for the configurations that were selected to be trained at a given budget. Right: The percentage of configurations that belong to the top 1/3 configurations at a given budget and that were in the top bottom 2/3 of the configurations at a previous budget. All of the results are from the Taskset benchmark. 0 10 20 30 40 50 Epochs 0.0 0.2 0.4 0.6Relative Number of Candidates LCBench 0 50 100 150 200 Epochs 0.0 0.2 0.4 0.6 0.8Relative Number of Candidates NAS-Bench-201 0 10 20 30 40 50 Steps 0.0 0.2 0.4 0.6 0.8Relative Number of Candidates TaskSet Figure 17: These plots shed light on how DYHPO behaves after the configuration it finally returns as the best. The plots show how many epochs are spent per candidate. As we can see, for most candidates only a small budget was considered, indicating that DYHPO is mostly exploring at this point. 23",
      "meta_data": {
        "arxiv_id": "2202.09774v2",
        "authors": [
          "Martin Wistuba",
          "Arlind Kadra",
          "Josif Grabocka"
        ],
        "published_date": "2022-02-20T10:28:02Z",
        "pdf_url": "https://arxiv.org/pdf/2202.09774v2.pdf"
      }
    },
    {
      "title": "Supervising the Multi-Fidelity Race of Hyperparameter Configurations",
      "abstract": "Multi-fidelity (gray-box) hyperparameter optimization techniques (HPO) have\nrecently emerged as a promising direction for tuning Deep Learning methods.\nHowever, existing methods suffer from a sub-optimal allocation of the HPO\nbudget to the hyperparameter configurations. In this work, we introduce DyHPO,\na Bayesian Optimization method that learns to decide which hyperparameter\nconfiguration to train further in a dynamic race among all feasible\nconfigurations. We propose a new deep kernel for Gaussian Processes that embeds\nthe learning curve dynamics, and an acquisition function that incorporates\nmulti-budget information. We demonstrate the significant superiority of DyHPO\nagainst state-of-the-art hyperparameter optimization methods through\nlarge-scale experiments comprising 50 datasets (Tabular, Image, NLP) and\ndiverse architectures (MLP, CNN/NAS, RNN).",
      "full_text": "Supervising the Multi-Fidelity Race of Hyperparameter Configurations Martin Wistuba∗† Amazon Web Services, Berlin, Germany marwistu@amazon.com Arlind Kadra∗ University of Freiburg, Freiburg, Germany kadraa@cs.uni-freiburg.de Josif Grabocka University of Freiburg, Freiburg, Germany grabocka@cs.uni-freiburg.de Abstract Multi-fidelity (gray-box) hyperparameter optimization techniques (HPO) have recently emerged as a promising direction for tuning Deep Learning methods. However, existing methods suffer from a sub-optimal allocation of the HPO bud- get to the hyperparameter configurations. In this work, we introduce DyHPO, a Bayesian Optimization method that learns to decide which hyperparameter config- uration to train further in a dynamic race among all feasible configurations. We propose a new deep kernel for Gaussian Processes that embeds the learning curve dynamics, and an acquisition function that incorporates multi-budget information. We demonstrate the significant superiority of DyHPO against state-of-the-art hy- perparameter optimization methods through large-scale experiments comprising 50 datasets (Tabular, Image, NLP) and diverse architectures (MLP, CNN/NAS, RNN). 1 Introduction Hyperparameter Optimization (HPO) is arguably an acute open challenge for Deep Learning (DL), especially considering the crucial impact HPO has on achieving state-of-the-art empirical results. Unfortunately, HPO for DL is a relatively under-explored field and most DL researchers still optimize their hyperparameters via obscure trial-and-error practices. On the other hand, traditional Bayesian Optimization HPO methods [Snoek et al., 2012, Bergstra et al., 2011] are not directly applicable to deep networks, due to the infeasibility of evaluating a large number of hyperparameter configurations. In order to scale HPO for DL, three main directions of research have been recently explored. (i) Online HPO methods search for hyperparameters during the optimization process via meta-level controllers [Chen et al., 2017, Parker-Holder et al., 2020], however, this online adaptation can not accommodate all hyperparameters (e.g. related to architectural changes). (ii) Gradient-based HPO techniques, on the other hand, compute the derivative of the validation loss w.r.t. hyperparameters by reversing the training update steps [Maclaurin et al., 2015, Franceschi et al., 2017, Lorraine et al., 2020], however, the reversion is not directly applicable to all cases (e.g. dropout rate). The last direction, (iii) Gray-box HPO techniques discard sub-optimal configurations after evaluating them on lower budgets [Li et al., 2017, Falkner et al., 2018]. In contrast to the online and gradient-based alternatives, gray-box approaches can be deployed in an off-the-shelf manner to all types of hyperparameters and architectures. The gray-box concept is based on the intuition that a poorly-performing hyperparameter configuration can be identified and ∗equal contribution †work does not relate to position at Amazon 36th Conference on Neural Information Processing Systems (NeurIPS 2022). arXiv:2202.09774v2  [cs.LG]  1 Jun 2023terminated by inspecting the validation loss of the first few epochs, instead of waiting for the full convergence. The most prominent gray-box algorithm is Hyperband [Li et al., 2017], which is based on successive halving. It runs random configurations at different budgets (e.g. number of epochs) and successively halves these configurations by keeping only the top performers. Follow-up works, such as BOHB [Falkner et al., 2018] or DEHB [Awad et al., 2021], replace the random sampling of Hyperband with a sampling based on Bayesian optimization or differentiable evolution. Despite their great practical potential, gray-box methods suffer from a major issue. The low-budget (few epochs) performances are not always a good indicator for the full-budget (full convergence) performances. For example, a properly regularized network converges slower in the first few epochs, however, typically performs better than a non-regularized variant after the full convergence. In other words, there can be a poor rank correlation of the configurations’ performances at different budgets. Figure 1: Top: The learning curve for different hyperparameter configurations. The darker the learning curve, the later it was evaluated during the search. Bottom: The hyperparameter indices in a temporal order as evaluated during the opti- mization and their corresponding curves. We introduceDYHPO, a Bayesian Optimization (BO) approach based on Gaussian Processes (GP), that proposes a novel treatment to the multi-budget (a.k.a. multi-fidelity) setup. In this perspective, we propose a deep kernel GP that captures the learning dynamics. As a result, we train a kernel capable of capturing the similarity of a pair of hyperparameter configurations, even if the pair’s configurations are evaluated at differ- ent budgets. Furthermore, we extend Expected Improvement [Jones et al., 1998] to the multi- budget case, by introducing a new mechanism for the incumbent configuration of a budget. We illustrate the differences between our rac- ing strategy and successive halving with the experiment of Figure 1, where, we showcase the HPO progress of two different methods on the \"Helena\" dataset from the LCBench bench- mark [Zimmer et al., 2021]. Hyperband [Li et al., 2017] is a gray-box approach that stati- cally pre-allocates the budget for a set of candidates (Hyperband bracket) according to a predefined policy. However, DYHPO dynamically adapts the allocation of budgets for configurations after every HPO step (a.k.a. a dynamic race). As a result, DYHPO invests only a small budget on configurations that show little promise as indicated by the intermediate scores. The joint effect of modeling a GP kernel across budgets together with a dedicated acquisition function leads to DYHPO achieving a statistically significant empirical gain against state-of-the-art gray-box baselines [Falkner et al., 2018, Awad et al., 2021], including prior work on multi-budget GPs [Kandasamy et al., 2017, 2020] or neural networks [Li et al., 2020b]. We demonstrate the performance of DYHPO in three diverse deep learning architectures (MLP, CNN/NAS, RNN) and 50 datasets of three diverse modalities (tabular, image, natural language processing). We believe our method is a step forward toward making HPO for DL practical and feasible. Overall, our contributions can be summarized as follows: • We introduce a novel Bayesian surrogate for gray-box HPO optimization. Our novel surrogate model predicts the validation score of a machine learning model based on both the hyperparameter configuration, the budget information, and the learning curve. • We derive a simple yet robust way to combine this surrogate model with Bayesian opti- mization, reusing most of the existing components currently used in traditional Bayesian optimization methods. • Finally, we demonstrate the efficiency of our method for HPO and neural architecture search tasks compared to the current state-of-the-art methods in HPO, by outperforming seven strong HPO baselines with a statistically significant margin. As an overarching goal, we believe our method is an important step toward scaling HPO for DL. 22 Related Work on Gray-box HPO Multi-Fidelity Bayesian Optimization and Bandits. Bayesian optimization is a black-box func- tion optimization framework that has been successfully applied in optimizing hyperparameter and neural architectures alike [Snoek et al., 2012, Kandasamy et al., 2018, Bergstra et al., 2011]. To further improve Bayesian optimization, several works propose low-fidelity data approximations of hyperparameter configurations by training on a subset of the data [Swersky et al., 2013, Klein et al., 2017a], or by terminating training early [Swersky et al., 2014]. Additionally, several methods extend Bayesian optimization to multi-fidelity data by engineering new kernels suited for this problem [Swer- sky et al., 2013, 2014, Poloczek et al., 2017]. Kandasamy et al. [2016] extends GP-UCB [Srinivas et al., 2010] to the multi-fidelity setting by learning one Gaussian Process (GP) with a standard kernel for each fidelity. Their later work improves upon this method by learning one GP for all fidelities that enables the use of continuous fidelities [Kandasamy et al., 2017]. The work by Takeno et al. [2020] follows a similar idea but proposes to use an acquisition function based on information gain instead of UCB. While most of the works rely on GPs to model the surrogate function, Li et al. [2020b] use a Bayesian neural network that models the complex relationship between fidelities with stacked neural networks, one for each fidelity. Hyperband [Li et al., 2017] is a bandits-based multi-fidelity method for hyperparameter optimization that selects hyperparameter configurations at random and uses successive halving [Jamieson and Tal- walkar, 2016] with different settings to early-stop less promising training runs. Several improvements have been proposed to Hyperband with the aim to replace the random sampling of hyperparameter configurations with a more guided approach [Bertrand et al., 2017, Wang et al., 2018, Wistuba, 2017]. BOHB [Falkner et al., 2018] uses TPE [Bergstra et al., 2011] and builds a surrogate model for every fidelity adhering to a fixed-fidelity selection scheme. DEHB [Awad et al., 2021] samples candidates using differential evolution which handles large and discrete search spaces better than BOHB. Mendes et al. [2021] propose a variant of Hyperband which allows to skip stages. Learning Curve Prediction A variety of methods attempt to extrapolate a partially observed learning curve in order to estimate the probability that a configuration will improve over the current best solution. Domhan et al. [2015] propose to ensemble a set of parametric functions to extrapolate a partial learning curve. While this method is able to extrapolate with a single example, it requires a relatively long learning curve to do so. The work by Klein et al. [2017b] build upon the idea of using a set of parametric functions. The main difference is that they use a heteroscedastic Bayesian model to learn the ensemble weights. Baker et al. [2018] propose to use support vector machines (SVM) as an auto-regressive model. The SVM predicts the next value of a learning curve, the original learning curve is augmented by this value and we keep predicting further values. The work by Gargiani et al. [2019] use a similar idea but makes prediction based on the last K observations only and uses probabilistic models. Wistuba and Pedapati [2020] propose to learn a prediction model across learning curves from different tasks to avoid the costly learning curve collection. In contrast to DYHPO , none of these methods selects configuration but is limited to deciding when to stop a running configuration. Multi-Fidelity Acquisition Functions Klein et al. [2017a] propose an acquisition function which allows for selecting hyperparameter configurations and the dataset subset size. The idea is to reduce training time by considering only a smaller part of the training data. In contrast to EIMF, this acquisition function is designed to select arbitrary subset sizes whereas EIMF is intended to slowly increase the invested budget over time. Mendes et al. [2020] extend the work of Klein et al. [2017a] to take business constraints into account. Deep Kernel Learning with Bayesian Optimization. We are among the first to use deep kernel learning with Bayesian optimization and to the best of our knowledge the first to use it for multi- fidelity Bayesian optimization. Rai et al. [2016] consider the use of a deep kernel instead of a manually designed kernel in the context of standard Bayesian optimization, but, limit their experimentation to synthetic data and do not consider its use for hyperparameter optimization. Perrone et al. [2018], Wistuba and Grabocka [2021] use a pre-trained deep kernel to warm start Bayesian optimization with meta-data from previous optimizations. The aforementioned approaches are multi-task or transfer learning methods that require the availability of meta-data from related tasks. 3In contrast to prior work, we propose a method that introduces deep learning to multi-fidelity HPO with Bayesian Optimization, and captures the learning dynamics across fidelities/budgets, combined with an acquisition function that is tailored for the gray-box setup. 3 Dynamic Multi-Fidelity HPO 3.1 Preliminaries Gray-Box Optimization. The gray-box HPO setting allows querying configurations with a smaller budget compared to the total maximal budget B. Thus, we can query from the response function f : X ×N → R where fi,j = f(xi, j) is the response after spending a budget ofj on configuration xi. As before, these observations are noisy and we observe yi,j = f(xi, j) +εj where εj ∼ N(0, σ2 j,n). Please note, we assume that the budget required to query fi,j+b after having queried fi,j is only b. Furthermore, we use the learning curve Yi,j−1 = (yi,1, . . . , yi,j−1) when predicting fi,j. Gaussian Processes (GP). Given a training data set D = {(xi, yi)}n i=1, the Gaussian Process assumption is that yi is a random variable and the joint distribution of all yi is assumed to be multivariate Gaussian distributed as y ∼ N(m (X) , k(X, X)) . Furthermore, f∗ for test instances x∗ are jointly Gaussian with y as:\u0014 y f∗ \u0015 ∼ N \u0012 m (X, x∗) , \u0012 Kn K∗ KT ∗ K∗∗ \u0013\u0013 . (1) The mean function m is often set to 0 and its covariance function k depends on parameters θ. For notational convenience, we use Kn = k (X, X|θ) +σ2 nI, K∗ = k (X, X∗|θ) and K∗∗ = k (X∗, X∗|θ) to define the kernel matrices. We can derive the posterior predictive distribution with mean and covariance as follows: E[f∗|X, y, X∗] =KT ∗ K−1 n y, cov [f∗|X, X∗] =K∗∗ − KT ∗ K−1 n K∗ (2) Often, the kernel function is manually engineered, one popular example is the squared exponential kernel. However, in this work, we make use of the idea of deep kernel learning [Wilson et al., 2016]. The idea is to model the kernel as a neural network φ and learn the best kernel transformation K (θ, w) :=k(φ(x, w), φ(x′; w)|θ), which allows us to use convolutional operations in our kernel. 3.2 Deep Multi-Fidelity Surrogate We propose to use a Gaussian Process surrogate model that infers the value of fi,j based on the hyperparameter configuration xi, the budget j as well as the past learning curve Yi,j−1. For this purpose, we use a deep kernel as: K (θ, w) :=k(φ(xi, Yi,j−1, j; w), φ(xi′ , Yi′,j′−1, j′; w); θ) (3) xi j Yi,j−1 · 1B P P P Convolution max P P P Figure 2: The feature extractor φ of our kernel. We use a squared exponential kernel for k and the neural network φ is composed of linear and convolutional layers as shown in Figure 2. We normalize the budget j to a range between 0 and 1 by dividing it by the maximum budget B. Afterward, it is concatenated with the hyper- parameter configuration xi and fed to a linear layer. The learning curve Yi,j−1 is transformed by a one-dimensional convolution followed by a global max pooling layer. Finally, both repre- sentations are fed to another linear layer. Its output will be the input to the kernel function k. Both, the kernel k and the neural network φ consist of trainable parameters θ and w, respectively. We find their optimal values by computing the maximum likelihood estimates as: ˆθ, ˆw = arg max θ,w p(y|X, Y, θ, w) ∝ arg min θ,w yTK (θ, w)−1 y + log|K (θ, w)| (4) 4In order to solve this optimization problem, we use gradient descent and Adam [Kingma and Ba, 2015] with a learning rate of 0.1. Given the maximum likelihood estimates, we can approximate the predictive posterior through p \u0010 fi,j|xi, Yi,j−1, j,D, ˆθ, ˆw \u0011 , and ultimately compute the mean and covariance of this Gaussian using Equation 2. 3.3 Multi-Fidelity Expected Improvement Expected improvement [Jones et al., 1998] is a commonly used acquisition function and is defined as: EI(x|D) =E[max {f(x) − ymax, 0}] , (5) where ymax is the largest observed value of f. We propose a multi-fidelity version of it as: EIMF(x, j|D) =E \u0002 max \b f(x, j) − ymax j , 0 \t\u0003 , (6) where: ymax j = \u001amax {y | ((x, ·, j), y) ∈ D} if ((x, ·, j), y) ∈ D max {y | (·, y) ∈ D} otherwise (7) Simply put, ymax j is the largest observed value of f for a budget of j if it exists already, otherwise, it is the largest observed value for any budget. If there is only one possible budget, the multi-fidelity expected improvement is identical to expected improvement. 3.4 The D YHPO Algorithm The DYHPO algorithm looks very similar to many black-box Bayesian optimization algorithms as shown in Algorithm 1. The big difference is that at each step we dynamically decide which candidate configuration to train for a small additional budget. Algorithm 1 DYHPO Algorithm 1: b(x) = 0∀x ∈ X 2: while not converged do 3: xi ← arg maxx∈X EIMF (x, b(x) + 1)(Sec. 3.3) 4: Observe yi,b(xi)+1. 5: b(xi) ← b(xi) + 1 6: D ← D ∪ \b ((xi, Yi,b(xi)−1, b(xi)), yi,b(xi)) \t 7: Update the surrogate on D. (Sec. 3.2) return xi with largest yi,·. Possible candidates are previously un- considered configurations as well as configurations that did not reach the maximum budget. In Line 2, the most promising candidate is chosen using the acquisition function introduced in Section 3.3 and the surrogate model’s predictions. It is important to high- light that we do not maximize the ac- quisition function along the budget di- mensionality. Instead, we set the bud- get b such that it is by exactly one higher than the budget used to eval- uate xi before. This ensures that we explore configurations by slowly increasing the budget. After the candidate and the corresponding budget are selected, the function f is evaluated and we observe yi,j (Line 3). This additional data point is added to D in Line 4. Then in Line 5, the surrogate model is updated according to the training scheme described in Section 3.2. 4 Experimental Protocol 4.1 Experimental Setup We evaluate DYHPO in three different settings on hyperparameter optimization for tabular, text, and image classification against several competitor methods, the details of which are provided in the following subsections. We ran all of our experiments on an Amazon EC2 M5 Instance (m5.xlarge). In our experiments, we report the mean of ten repetitions and we report two common metrics, the regret and the average rank. The regret refers to the absolute difference between the score of the solution found by an optimizer compared to the best possible score. If we report the regret as an aggregate result over multiple datasets, we report the mean over all regrets. The average rank is the metric we use to aggregate rank results over different datasets. We provide further implementation and training details in Appendix A.4. Our implementation of DYHPO is publicly available.3 3https://github.com/releaunifreiburg/DyHPO 54.2 Benchmarks In our experiments, we use the following benchmarks. We provide more details in Appendix A.1. LCBench: A learning curve benchmark [Zimmer et al., 2021] that evaluates neural network architec- tures for tabular datasets. LCBench contains learning curves for 35 different datasets, where 2,000 neural networks per dataset are trained for 50 epochs with Auto-PyTorch. TaskSet: A benchmark that features diverse tasks Metz et al. [2020] from different domains and includes 5 search spaces with different degrees of freedom, where, every search space includes 1000 hyperparameter configurations. In this work, we focus on a subset of NLP tasks (12 tasks) and we use the Adam8p search space with 8 continuous hyperparameters. NAS-Bench-201: A benchmark consisting of 15625 hyperparameter configurations representing different architectures on the CIFAR-10, CIFAR-100 and ImageNet datasets Dong and Yang [2020]. NAS-Bench-201 features a search space of 6 categorical hyperparameters and each architecture is trained for 200 epochs. 4.3 Baselines Random Search: A random/stochastic black-box search method for HPO. HyperBand: A multi-arm bandit method that extends successive halving by multiple brackets with different combinations of the initial number of configurations, and their initial budget [Li et al., 2017]. BOHB: An extension of Hyperband that replaces the random sampling of the initial configurations for each bracket with recommended configurations from a model-based approach [Falkner et al., 2018]. BOHB builds a model for every fidelity that is considered. DEHB: A method that builds upon Hyperband by exploiting differential evolution to sample the initial candidates of a Hyperband bracket [Awad et al., 2021]. ASHA: An asynchronous version of successive halving (or an asynchronous version of Hyperband if multiple brackets are run). ASHA Li et al. [2020a] does not wait for all configurations to finish inside a successive halving bracket, but, instead promotes configurations to the next successive halving bracket in real-time. MF-DNN: A multi-fidelity Bayesian optimization method that uses deep neural networks to capture the relationships between different fidelities Li et al. [2020b]. Dragonfly: We compare against BOCA [Kandasamy et al., 2017] by using the Dragonfly library Kan- dasamy et al. [2020]. This method suggests the next hyperparameter configuration as well as the budget it should be evaluated for. 4.4 Research Hypotheses and Associated Experiments Hypothesis 1: DYHPO achieves state-of-the-art results in multi-fidelity HPO. Experiment 1: We compare DYHPO against the baselines of Section 4.3 on the benchmarks of Section 4.2 with the experimental setup of Section 4.1. For TaskSet we follow the authors’ recommendation and report the number of steps (every 200 iterations). Hypothesis 2: DYHPO’s runtime overhead has a negligible impact on the quality of results. Experiment 2: We compare DYHPO against the baselines of Section 4.3 over the wallclock time. The wallclock time includes both (i) the optimizer’s runtime overhead for recommending the next hyperparameter configuration, plus (ii) the time needed to evaluate the recommended configuration. In this experiment, we consider all datasets where the average training time per epoch is at least 10 seconds, because, for tasks where the training time is short, there is no practical justification for complex solutions and their overhead. In these cases, we recommend using a random search. We don’t report results for TaskSet because the benchmark lacks training times. Hypothesis 3: DYHPO uses the computational budget more efficiently than baselines. Experiment 3: To further verify that DYHPO is efficient compared to the baselines, we investigate whether competing methods spend their budgets on qualitative candidates. Concretely we: i) calculate 60 500 1000 Number of Epochs 10 2 10 1 Mean Regret LCBench 0 500 1000 Number of Steps 10 1 TaskSet 0 2000 4000 Number of Epochs 10 1 ImageNet16-120 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO Figure 3: The mean regret for the different benchmarks over the number of epochs or steps (every 200 iterations). The results are aggregated over 35 different datasets for LCBench and aggregated over 12 different NLP tasks for TaskSet. the precision of the top (w.r.t. ground truth) performing configurations that were selected by each method across different budgets, ii) compute the average regret of the selected configurations across budget, and iii) we compare the fraction of top-performing configurations at a given budget that were not top performers at lower budgets, i.e. measure the ability to handle the poor correlation of performances across budgets. 5 Results Experiment 1: DYHPO achieves state-of-the-art results. In our first experiment, we evaluate the various methods on the benchmarks listed in Section 4.2. We show the aggregated results in Figure 3, the results show that DYHPO manages to outperform competitor methods over the set of considered benchmarks by achieving a better mean regret across datasets. Not only does DYHPO achieve a better final performance, it also achieves strong anytime results by converging faster than the competitor methods. For the extended results, related to the performance of all methods on a dataset level, we refer the reader to Appendix B. 12345678 Random Dragonfly Hyperband MF-DNN ASHA BOHB DEHB DyHPO LCBench@50% 12345678 Random Hyperband MF-DNN Dragonfly DEHB BOHB ASHA DyHPO LCBench@100% 12345678 Random MF-DNN ASHA Dragonfly Hyperband DEHB BOHB DyHPO TaskSet@50% 12345678 Random MF-DNN ASHA Dragonfly DEHB Hyperband BOHB DyHPO TaskSet@100% Figure 4: Critical difference diagram for LCBench and TaskSet in terms of the number of HPO steps. The results correspond to results after 500 and 1,000 epochs. Connected ranks via a bold bar indicate that performances are not significantly different (p >0.05). In Figure 4, we provide further evidence thatDYHPO’s improvement over the baselines is statistically significant. The critical difference diagram presents the ranks of all methods and provides information on the pairwise statistical difference between all methods for two fractions of the number of HPO steps (50% and 100%). We included the LCBench and TaskSet benchmarks in our significance plots. NAS-Bench-201 was omitted because it has only 3 datasets and the statistical test cannot be applied. Horizontal lines indicate groupings of methods that are not significantly different. As suggested by the best published practices Demsar [2006], we use the Friedman test to reject the null hypothesis followed by a pairwise post-hoc analysis based on the Wilcoxon signed-rank test (α = 0.05). For LCBench, DYHPO already outperforms the baselines significantly after 50% of the search budget, with a statistically significant margin. As the optimization procedure continues, DYHPO manages to extend its gain in performance and is the only method that has a statistically significant improvement against all the other competitor methods. Similarly, for TaskSet, DYHPO manages to outperform all methods with a statistically significant margin only halfway through the optimization procedure and achieves the best rank over all methods. However, as the optimization procedure continues, BOHB manages to decrease the performance gap with DYHPO , although, it still achieves a worse rank across all datasets. Considering the empirical results, we conclude that Hypothesis 1 is validated and that DYHPO achieves state-of-the-art results on multi-fidelity HPO. Experiment 2: On the impact of DYHPO ’s overhead on the results. We present the results of our second experiment in Figure 5 (left), where, as it can be seen, DYHPO still outperforms the other methods when its overhead is considered. For LCBench, DYHPO manages to get an advantage 70.2 0.4 0.6 0.8 1.0 Normalized Wallclock Time 10 1 Mean Regret LCBench 103 105 Wallclock Time in Seconds 10 1 ImageNet16-120 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 12345678 Random Dragonfly ASHA Hyperband MF-DNN BOHB DEHB DyHPO LCBench@50% 12345678 Random Dragonfly ASHA Hyperband MF-DNN BOHB DEHB DyHPO LCBench@100% Figure 5: Left: The regret over time for all methods during the optimization procedure for the LCBench benchmark and the ImageNet dataset from the NAS-Bench-201 benchmark. The normalized wall clock time represents the actual run time divided by the total wall clock time ofDYHPO including the overhead of fitting the deep GP. Right: The critical difference diagram for LCBench halfway through the HPO wall-clock time, and in the end. Connected ranks via a bold bar indicate that differences are not significant (p >0.05). fairly quickly and it only increases the gap in performance with the other methods as the optimization process progresses. Similarly, in the case of ImageNet from NAS-Bench-201, DYHPO manages to gain an advantage earlier than other methods during the optimization procedure. Although in the end DYHPO still performs better than all the other methods, we believe most of the methods converge to a good solution and the differences in the final performance are negligible. For the extended results, related to the performance of all methods on a dataset level over time, we refer the reader to the plots in Appendix B. Additionally, in Figure 5 (right), we provide the critical difference diagrams for LCBench that present the ranks and the statistical difference of all methods halfway through the optimization procedure, and in the end. As it can be seen, DYHPO has a better rank with a significant margin with only half of the budget used and it retains the advantage until the end. Experiment 3: On the efficiency of DYHPO . In Figure 6 (left), we plot the precision of every method for different budgets during the optimization procedure, which demonstrates that DYHPO effectively explores the search space and identifies promising candidates. The precision at an epoch i is defined as the number of top 1% candidates that are trained, divided by the number of all candidates trained, both trained for at least i epochs. The higher the precision, the more relevant candidates were considered and the less computational resources were wasted. For small budgets, the precision is low since DYHPO spends budget to consider various candidates, but then, promising candidates are successfully identified and the precision quickly increases. This argument is further supported in Figure 6 (middle), where we visualize the average regret of all the candidates trained for at least the specified number of epochs on the x-axis. In contrast to the regret plots, here we do not show the regret of the best configuration, but the mean regret of all the selected configurations. The analysis deduces a similar finding, our method DYHPO selects more qualitative hyperparameter configurations than all the baselines. An interesting property of multi-fidelity HPO is the phenomenon of poor rank correlations among the validation performance of candidates at different budgets. In other words, a configuration that achieves a poor performance at a small budget can perform better at a larger budget. To analyze this phenomenon, we measure the percentage of \"good\" configurations at a particular budget, that were \"bad\" performers in at least one of the smaller budgets. We define a \"good\" performance at a budget B when a configuration achieves a validation accuracy ranked among the top 1/3 of the validation accuracies belonging to all the other configurations that were run until that budget B. In Figure 6 (right), we analyze the percentage of \"good\" configurations at each budget denoted by the x-axis, that were \"bad\" performers in at least one of the lower budgets. Such a metric is a proxy for the degree of the promotion of \"bad\" configurations towards higher budgets. We present the analysis for all the competing methods of our experimental protocol from Section 4. We have additionally included the ground-truth line annotated as \"Baseline\", which represents the fraction of past poor performers among all the feasible configurations in the search space. In contrast, the respective methods compute the fraction of promotions only among the configurations that those methods have considered (i.e. selected within their HPO trials) until the budget indicated by the x-axis. We see that there is a high degree of \"good\" configurations that were \"bad\" at a previous budget, with fractions of the ground-truth \"Baseline\" going up to 40% for the LCBench benchmark and up to 80% for the NAS-Bench-201 benchmark. 80 10 20 30 40 50 Number of Epochs 0.0 0.1 0.2 0.3 0.4Precision of Top Candidates LCBench Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 10 20 30 40 50 Number of Epochs 0.05 0.10 0.15Average Regret LCBench Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 10 20 30 40 50 Number of Epochs 0.0 0.2 0.4 0.6 0.8 Fraction of Poor Performer Promotions LCBench Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO Baseline 0 50 100 150 200 Number of Epochs 0.05 0.10 0.15Precision of Top Candidates NAS-Bench-201 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 50 100 150 200 Number of Epochs 0.04 0.06 0.08 0.10 0.12Average Regret NAS-Bench-201 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 50 100 150 200 Number of Epochs 0.0 0.2 0.4 0.6 0.8 1.0 Fraction of Poor Performer Promotions NAS-Bench-201 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO Baseline Figure 6: The efficiency of DYHPO as the optimization progresses. Left: The fraction of top- performing candidates from all candidates that were selected to be trained. Middle: The average regret for the configurations that were selected to be trained at a given budget. Right: The percentage of configurations that belong to the top 1/3 configurations at a given budget and that were in the top bottom 2/3 of the configurations at a previous budget. All of the results are from the LCBench and NAS-Bench-201 benchmark. On the other hand, the analysis demonstrates that our method DYHPO has promoted more \"good\" configurations that were \"bad\" in a lower budget, compared to all the rival methods. In particular, more than 80% of selected configurations from the datasets belonging to either benchmark were \"bad\" performers at a lower budget. The empirical evidence validates Hypothesis 3 and demonstrates that DYHPO efficiently explores qualitative candidates. We provide the results of our analysis for DYHPO’s efficiency on the additional benchmarks (Taskset) in Appendix B. Ablating the impact of the learning curve 102 103 104 105 Training Time in Seconds 10 1 Regret ImageNet16-120 DyHPO DyHPO w/o CNN Random Search Figure 7: Ablating the impact of the learning curve on DYHPO. One of the main differences between DYHPO and similar methods Kandasamy et al. [2017], is that the learning curve is an input to the kernel function. For this reason, we investigate the impact of this design choice. We consider a variation of DYHPO w/o CNN, which is simply DYHPO without the learning curve. It is worth emphasizing that both variants (with and without the learning curve) are multi-fidelity surrogates and both receive the budget informa- tion through the inputted index j in Equation 3. The only difference is that DYHPO additionally incorporates the pattern of the learning curve. We run the ablation on the NAS-Bench-201 benchmark and report the results for ImageNet, the largest dataset in our collection. The ablation results are shown in Figure 7, while the remaining results on the other datasets are shown in Figure 8 of the appendix. Based on the results from our learning curve ablation, we conclude that the use of an explicit learning curve representation leads to significantly better results. 6 Limitations of Our Method Although DYHPO shows a convincing and statistically significant reduction of the HPO time on diverse Deep Learning (DL) experiments, we cautiously characterized our method only as a \"step 9towards\" scaling HPO for DL. The reason for our restrain is the lack of tabular benchmarks for HPO on very large deep learning models, such as Transformers-based architectures [Devlin et al., 2019]. Additionally, the pause and resume part of our training procedure can only be applied when tuning the hyperparameters of parametric models, otherwise, the training of a hyperparameter configuration would have to be restarted. Lastly, for small datasets that can be trained fast, the overhead of model-based techniques would make an approach like random search more appealing. 7 Conclusions In this work, we present DYHPO , a new Bayesian optimization (BO) algorithm for the gray-box setting. We introduced a new surrogate model for BO that uses a learnable deep kernel and takes the learning curve as an explicit input. Furthermore, we motivated a variation of expected improvement for the multi-fidelity setting. Finally, we compared our approach on diverse benchmarks on a total of 50 different tasks against the current state-of-the-art methods on gray-box hyperparameter optimization (HPO). Our method shows significant gains and has the potential to become the de facto standard for HPO in Deep Learning. Acknowledgments Josif Grabocka and Arlind Kadra would like to acknowledge the grant awarded by the Eva-Mayr-Stihl Stiftung. In addition, this research was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under grant number 417962828 and grant INST 39/963-1 FUGG (bwForCluster NEMO). In addition, Josif Grabocka acknowledges the support of the BrainLinks- BrainTools center of excellence. References Noor H. Awad, Neeratyoy Mallik, and Frank Hutter. DEHB: evolutionary hyberband for scalable, robust and efficient hyperparameter optimization. In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI 2021, Virtual Event / Montreal, Canada, 19-27 August 2021, pages 2147–2153, 2021. doi: 10.24963/ijcai.2021/296. URL https://doi.org/ 10.24963/ijcai.2021/296. Bowen Baker, Otkrist Gupta, Ramesh Raskar, and Nikhil Naik. Accelerating neural architecture search using performance prediction. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Workshop Track Proceedings, 2018. URL https://openreview.net/forum?id=HJqk3N1vG. James Bergstra, Rémi Bardenet, Yoshua Bengio, and Balázs Kégl. Algorithms for hyper-parameter optimization. In Advances in Neural Information Processing Systems 24: 25th Annual Conference on Neural Information Processing Systems 2011. Proceedings of a meeting held 12-14 December 2011, Granada, Spain, pages 2546–2554, 2011. URL https://proceedings.neurips.cc/ paper/2011/hash/86e8f7ab32cfd12577bc2619bc635690-Abstract.html. Hadrien Bertrand, Roberto Ardon, Matthieu Perrot, and Isabelle Bloch. Hyperparameter optimization of deep neural networks: Combining hyperband with bayesian model selection. In Conférence sur l’Apprentissage Automatique, 2017. Yutian Chen, Matthew W. Hoffman, Sergio Gomez Colmenarejo, Misha Denil, Timothy P. Lillicrap, Matthew Botvinick, and Nando de Freitas. Learning to learn without gradient descent by gradient descent. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pages 748–756, 2017. URL http://proceedings. mlr.press/v70/chen17e.html. Janez Demsar. Statistical comparisons of classifiers over multiple data sets. J. Mach. Learn. Res., 7: 1–30, 2006. URL http://jmlr.org/papers/v7/demsar06a.html. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of 10the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 4171–4186, 2019. doi: 10.18653/v1/n19-1423. URL https://doi.org/ 10.18653/v1/n19-1423. Tobias Domhan, Jost Tobias Springenberg, and Frank Hutter. Speeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves. In Qiang Yang and Michael J. Wooldridge, editors, Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence, IJCAI 2015, Buenos Aires, Argentina, July 25-31, 2015, pages 3460–3468. AAAI Press, 2015. URL http://ijcai.org/Abstract/15/487. Xuanyi Dong and Yi Yang. Nas-bench-201: Extending the scope of reproducible neural architecture search. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020, 2020. URL https://openreview.net/forum?id=HJxyZkBKDr. Stefan Falkner, Aaron Klein, and Frank Hutter. BOHB: robust and efficient hyperparameter optimiza- tion at scale. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, pages 1436–1445, 2018. URL http://proceedings.mlr.press/v80/falkner18a.html. Luca Franceschi, Michele Donini, Paolo Frasconi, and Massimiliano Pontil. Forward and reverse gradient-based hyperparameter optimization. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pages 1165–1173, 2017. URL http://proceedings.mlr.press/v70/franceschi17a.html. Jacob R. Gardner, Geoff Pleiss, Kilian Q. Weinberger, David Bindel, and Andrew Gordon Wil- son. Gpytorch: Blackbox matrix-matrix gaussian process inference with GPU acceleration. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural In- formation Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada , pages 7587–7597, 2018. URL https://proceedings.neurips.cc/paper/2018/hash/ 27e8e17134dd7083b050476733207ea1-Abstract.html. Matilde Gargiani, Aaron Klein, Stefan Falkner, and Frank Hutter. Probabilistic rollouts for learning curve extrapolation across hyperparameter settings. CoRR, abs/1910.04522, 2019. URL http: //arxiv.org/abs/1910.04522. Kevin G. Jamieson and Ameet Talwalkar. Non-stochastic best arm identification and hyperparameter optimization. In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, AISTATS 2016, Cadiz, Spain, May 9-11, 2016 , pages 240–248, 2016. URL http://proceedings.mlr.press/v51/jamieson16.html. Donald R. Jones, Matthias Schonlau, and William J. Welch. Efficient global optimization of ex- pensive black-box functions. J. Global Optimization , 13(4):455–492, 1998. doi: 10.1023/A: 1008306431147. URL https://doi.org/10.1023/A:1008306431147. Kirthevasan Kandasamy, Gautam Dasarathy, Junier B. Oliva, Jeff G. Schneider, and Barn- abás Póczos. Gaussian process bandit optimisation with multi-fidelity evaluations. In Advances in Neural Information Processing Systems 29: Annual Conference on Neu- ral Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain , pages 992–1000, 2016. URL https://proceedings.neurips.cc/paper/2016/hash/ 605ff764c617d3cd28dbbdd72be8f9a2-Abstract.html. Kirthevasan Kandasamy, Gautam Dasarathy, Jeff G. Schneider, and Barnabás Póczos. Multi-fidelity bayesian optimisation with continuous approximations. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pages 1799–1808, 2017. URL http://proceedings.mlr.press/v70/kandasamy17a.html. Kirthevasan Kandasamy, Willie Neiswanger, Jeff Schneider, Barnabás Póczos, and Eric P. Xing. Neural architecture search with bayesian optimisation and optimal transport. In Ad- vances in Neural Information Processing Systems 31: Annual Conference on Neural Infor- mation Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada , pages 2020–2029, 2018. URL https://proceedings.neurips.cc/paper/2018/hash/ f33ba15effa5c10e873bf3842afb46a6-Abstract.html. 11Kirthevasan Kandasamy, Karun Raju Vysyaraju, Willie Neiswanger, Biswajit Paria, Christopher R. Collins, Jeff Schneider, Barnabás Póczos, and Eric P. Xing. Tuning hyperparameters without grad students: Scalable and robust bayesian optimisation with dragonfly. J. Mach. Learn. Res., 21: 81:1–81:27, 2020. URL http://jmlr.org/papers/v21/18-223.html. Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1412.6980. Aaron Klein, Stefan Falkner, Simon Bartels, Philipp Hennig, and Frank Hutter. Fast bayesian optimization of machine learning hyperparameters on large datasets. In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, AISTATS 2017, 20-22 April 2017, Fort Lauderdale, FL, USA, pages 528–536, 2017a. URL http://proceedings.mlr.press/ v54/klein17a.html. Aaron Klein, Stefan Falkner, Jost Tobias Springenberg, and Frank Hutter. Learning curve prediction with bayesian neural networks. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings , 2017b. URL https://openreview.net/forum?id=S11KBYclx. Liam Li, Kevin G. Jamieson, Afshin Rostamizadeh, Ekaterina Gonina, Jonathan Ben-tzur, Moritz Hardt, Benjamin Recht, and Ameet Talwalkar. A system for massively parallel hyperparameter tuning. In Inderjit S. Dhillon, Dimitris S. Papailiopoulos, and Vivienne Sze, editors,Proceedings of Machine Learning and Systems 2020, MLSys 2020, Austin, TX, USA, March 2-4, 2020. mlsys.org, 2020a. URL https://proceedings.mlsys.org/book/303.pdf. Lisha Li, Kevin G. Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. Hyper- band: A novel bandit-based approach to hyperparameter optimization. J. Mach. Learn. Res., 18: 185:1–185:52, 2017. URL http://jmlr.org/papers/v18/16-558.html. Shibo Li, Wei Xing, Robert M. Kirby, and Shandian Zhe. Multi-fidelity bayesian optimization via deep neural networks. In Advances in Neural Information Processing Systems 33: An- nual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual , 2020b. URL https://proceedings.neurips.cc/paper/2020/hash/ 60e1deb043af37db5ea4ce9ae8d2c9ea-Abstract.html. Jonathan Lorraine, Paul Vicol, and David Duvenaud. Optimizing millions of hyperparameters by implicit differentiation. In The 23rd International Conference on Artificial Intelligence and Statistics, AISTATS 2020, 26-28 August 2020, Online [Palermo, Sicily, Italy], pages 1540–1552, 2020. URL http://proceedings.mlr.press/v108/lorraine20a.html. Dougal Maclaurin, David Duvenaud, and Ryan P. Adams. Gradient-based hyperparameter opti- mization through reversible learning. In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015 , pages 2113–2122, 2015. URL http://proceedings.mlr.press/v37/maclaurin15.html. Pedro Mendes, Maria Casimiro, Paolo Romano, and David Garlan. Trimtuner: Efficient optimization of machine learning jobs in the cloud via sub-sampling. In 28th International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems, MASCOTS 2020, Nice, France, November 17-19, 2020, pages 1–8. IEEE, 2020. doi: 10.1109/MASCOTS50786. 2020.9285971. URL https://doi.org/10.1109/MASCOTS50786.2020.9285971. Pedro Mendes, Maria Casimiro, and Paolo Romano. Hyperjump: Accelerating hyperband via risk modelling. CoRR, abs/2108.02479, 2021. URL https://arxiv.org/abs/2108.02479. Luke Metz, Niru Maheswaranathan, Ruoxi Sun, C. Daniel Freeman, Ben Poole, and Jascha Sohl- Dickstein. Using a thousand optimization tasks to learn hyperparameter search strategies. CoRR, abs/2002.11887, 2020. URL https://arxiv.org/abs/2002.11887. Jack Parker-Holder, Vu Nguyen, and Stephen J. Roberts. Provably efficient online hyperparameter optimization with population-based bandits. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/ hash/c7af0926b294e47e52e46cfebe173f20-Abstract.html. 12Valerio Perrone, Rodolphe Jenatton, Matthias W. Seeger, and Cédric Archambeau. Scalable hyper- parameter transfer learning. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada, pages 6846–6856, 2018. URL https://proceedings.neurips.cc/ paper/2018/hash/14c879f3f5d8ed93a09f6090d77c2cc3-Abstract.html. Matthias Poloczek, Jialei Wang, and Peter I. Frazier. Multi-information source optimiza- tion. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA , pages 4288–4298, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/ df1f1d20ee86704251795841e6a9405a-Abstract.html. Akshara Rai, Ruta Desai, and Siddharth Goyal. Bayesian optimization with a neural network kernel, 2016. URL http://www.cs.cmu.edu/~rutad/files/BO_NN.pdf. Jasper Snoek, Hugo Larochelle, and Ryan P. Adams. Practical bayesian optimization of machine learn- ing algorithms. In Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United States, pages 2960–2968, 2012. URL https://proceedings. neurips.cc/paper/2012/hash/05311655a15b75fab86956663e1819cd-Abstract.html. Niranjan Srinivas, Andreas Krause, Sham M. Kakade, and Matthias W. Seeger. Gaussian process optimization in the bandit setting: No regret and experimental design. In Proceedings of the 27th International Conference on Machine Learning (ICML-10), June 21-24, 2010, Haifa, Israel, pages 1015–1022, 2010. URL https://icml.cc/Conferences/2010/papers/422.pdf. Kevin Swersky, Jasper Snoek, and Ryan Prescott Adams. Multi-task bayesian optimization. In Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States, pages 2004–2012, 2013. URL https://proceedings.neurips. cc/paper/2013/hash/f33ba15effa5c10e873bf3842afb46a6-Abstract.html. Kevin Swersky, Jasper Snoek, and Ryan Prescott Adams. Freeze-thaw bayesian optimization. CoRR, abs/1406.3896, 2014. URL http://arxiv.org/abs/1406.3896. Shion Takeno, Hitoshi Fukuoka, Yuhki Tsukada, Toshiyuki Koyama, Motoki Shiga, Ichiro Takeuchi, and Masayuki Karasuyama. Multi-fidelity bayesian optimization with max-value entropy search and its parallelization. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, pages 9334–9345, 2020. URLhttp://proceedings. mlr.press/v119/takeno20a.html. Jiazhuo Wang, Jason Xu, and Xuejun Wang. Combination of hyperband and bayesian optimization for hyperparameter optimization in deep learning. CoRR, abs/1801.01596, 2018. URL http: //arxiv.org/abs/1801.01596. Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, and Eric P. Xing. Deep kernel learning. In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, AISTATS 2016, Cadiz, Spain, May 9-11, 2016, pages 370–378, 2016. URL http://proceedings.mlr. press/v51/wilson16.html. Martin Wistuba. Bayesian optimization combined with incremental evaluation for neural network architecture optimization. In AutoML@PKDD/ECML, 2017. Martin Wistuba and Josif Grabocka. Few-shot bayesian optimization with deep kernel surrogates. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021, 2021. URL https://openreview.net/forum?id=bJxgv5C3sYc. Martin Wistuba and Tejaswini Pedapati. Learning to rank learning curves. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 10303–10312. PMLR, 2020. URL http://proceedings.mlr.press/v119/wistuba20a.html. Lucas Zimmer, Marius Lindauer, and Frank Hutter. Auto-pytorch: Multi-fidelity metalearning for efficient and robust autodl. IEEE Trans. Pattern Anal. Mach. Intell., 43(9):3079–3090, 2021. doi: 10.1109/TPAMI.2021.3067763. URL https://doi.org/10.1109/TPAMI.2021.3067763. 13Checklist 1. For all authors... (a) Do the main claims made in the abstract and introduction accurately reflect the paper’s contributions and scope? [Yes] (b) Did you describe the limitations of your work? [Yes] See Section 6. (c) Did you discuss any potential negative societal impacts of your work? [Yes] See Section “Societal Implications” in the Appendix. (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] 2. If you are including theoretical results... (a) Did you state the full set of assumptions of all theoretical results? [N/A] (b) Did you include complete proofs of all theoretical results? [N/A] 3. If you ran experiments... (a) Did you include the code, data, and instructions needed to reproduce the main experi- mental results (either in the supplemental material or as a URL)? [No] We provide our main algorithm in Section 3 and we additionally provide the detailed implementation details in Appendix A for all methods and benchmarks. We will release the code for the camera-ready version of our work. (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] Please see Appendix A. (c) Did you report error bars (e.g., with respect to the random seed after running experi- ments multiple times)? [Yes] We report the statistical significance of the performance difference between our method and the baselines in Section 5 (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See Section 4.1. 4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... (a) If your work uses existing assets, did you cite the creators? [Yes] See Section 4.2 and Section 4.3. (b) Did you mention the license of the assets? [Yes] See Appendix A.1 and A.5 where we provide references to the assets where the license is included. (c) Did you include any new assets either in the supplemental material or as a URL? [No] (d) Did you discuss whether and how consent was obtained from people whose data you’re using/curating? [N/A] The benchmarks and baselines are open-sourced. (e) Did you discuss whether the data you are using/curating contains personally identi- fiable information or offensive content? [N/A] The data does not contain personally identifiable information or offensive content. 5. If you used crowdsourcing or conducted research with human subjects... (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A] 14Societal Implications In our work, we use only publicly available data with no privacy concerns. Furthermore, our algorithm reduces the overall time for fitting deep networks, therefore, saving computational resources and yielding a positive impact on the environment. Moreover, our method can help smaller research organizations with limited access to resources to be competitive in the deep learning domain, which reduces the investment costs on hardware. Although our method significantly reduces the time taken for optimizing a machine learning algorithm that achieves peak performance, we warn against running our method for an extended time only to achieve marginal gains in performance, unless it is mission-critical. Last but not least, in order to save energy, we invite the community to create sparse benchmarks with surrogates, instead of dense tabular ones. A Experimental Setup A.1 Benchmarks LCBench. LCBench4 is a feedforward neural network benchmark on tabular data which consists of 2000 configuration settings for each of the 35 datasets. The configurations were evaluated during HPO runs with AutoPyTorch. LCBench features a search space of 7 numerical hyperparameters, where every hyperparameter configuration is trained for 50 epochs. The objective is to optimize seven different hyperparameters of funnel-shaped neural networks, i.e., batch size, learning rate, momentum, weight decay, dropout, number of layers, and maximum number of units per layer. TaskSet. TaskSet5 is a benchmark that features over 1162 diverse tasks from different domains and includes 5 search spaces. In this work, we focus on NLP tasks and we use the Adam8p search space with 8 continuous hyperparameters. We refer to Figure 11 for the exact task names considered in our experiments. The learning curves provided in TaskSet report scores after every 200 iterations. We refer to those as \"steps\". The objective is to optimize eight hyperparameters for a set of different recurrent neural networks (RNN) that differ in embedding size, RNN cell, and other architectural features. The set of hyperparameters consists of optimizer-specific hyperparameters, such as the learning rate, the exponential decay rate of the first and second momentum of Adam, β1 and β2, and Adam’s constant for numerical stability ε. Furthermore, there are two hyperparameters controlling linear and exponential learning rate decays, as well as L1 and L2 regularization terms. NAS-Bench-201. NAS-Bench-2016 is a benchmark that has precomputed about 15,600 archi- tectures trained for 200 epochs for the image classification datasets CIFAR-10, CIFAR-100, and ImageNet. The objective is to select for each of the six operations within the cell of the macro architecture one of five different operations. All other hyperparameters such as learning rate and batch size are kept fixed. NAS-Bench-201 features a search space of 6 categorical hyperparameters and each architecture is trained for 200 epochs. A.2 Preprocessing In the following, we describe the preprocessing applied to the hyperparameter representation. For LCBench, we apply a log-transform to batch size, learning rate, and weight decay. For TaskSet, we apply it on the learning rate, L1 and L2 regularization terms, epsilon, linear and exponential decay of the learning rate. All continuous hyperparameters are scaled to the range between 0 and 1 using sklearn’s MinMaxScaler. If not mentioned otherwise, we use one-hot encoding for the categorical hyperparameters. As detailed in subsection A.5, some baselines have a specific way of dealing with them. In that case, we use the method recommended by the authors. A.3 Framework The framework contains the evaluated hyperparameters and their corresponding validation curves. The list of candidate hyperparameters is passed to the baseline-specific interface, which in turn, 4https://github.com/automl/LCBench 5https://github.com/google-research/google-research/tree/master/task_set 6https://github.com/D-X-Y/NAS-Bench-201 15optimizes and queries the framework for the hyperparameter configuration that maximizes utility. Our framework in turn responds with the validation curve and the cost of the evaluation. In case a hyperparameter configuration has been evaluated previously up to a budget b and a baseline requires the response for budget b + 1, the cost is calculated accordingly only for the extra budget requested. A.4 Implementation Details We implement the Deep Kernel Gaussian Process using GPyTorch 1.5 [Gardner et al., 2018]. We use an RBF kernel and the dense layers of the transformation functionφ have 128 and 256 units. We used a convolutional layer with a kernel size of three and four filters. All parameters of the Deep Kernel are estimated by maximizing the marginal likelihood. We achieve this by using gradient ascent and Adam [Kingma and Ba, 2015] with a learning rate of 0.1 and batch size of 64. We stop training as soon as the training likelihood is not improving for 10 epochs in a row or we completed 1,000 epochs. For every new data point, we start training the GP with its old parameters to reduce the required effort for training. A.5 Baselines Random Search & Hyperband. Random search and Hyperband sample hyperparameter config- urations at random and therefore the preprocessing is irrelevant. We have implemented both from scratch and use the recommended hyperparameters for Hyperband, i.e. η = 3. BOHB. For our experiments with BOHB, we use version 0.7.4 of the officially-released code7. DEHB. For our experiments with DEHB, we use the official public implementation8. We devel- oped an interface that communicates between our framework and DEHB. In addition to the initial preprocessing common for all methods, we encode categorical hyperparameters with a numerical value in the interval [0, 1]. For a categorical hyperparameter xi, we take Ni equal-sized intervals, where Ni represents the number of unique categorical values for hyperparameter xi and we assign the value for a categorical value n ∈ Ni to the middle of the interval [n, n+ 1]as suggested by the authors. For configuring the DEHB algorithm we used the default values from the library. Dragonfly. We use the publicly available code of Dragonfly9. No special treatment of categorical hyperparameters is required since Dragonfly has its own way to deal with them. We use version 0.1.6 with default settings. MF-DNN. We use the official implementation of MF-DNN by the authors10. Initially, we tried to use multiple incremental fidelity levels like for DYHPO, however, the method runtime was too high and it could not achieve competitive results. For that reason, we use only a few fidelity levels like the authors do in their work Li et al. [2020b]. We use the same fidelity levels as for Hyperband, DEHB, and BOHB to have a fair comparison between the baselines. We also use the same number of initial points as for the other methods to have the same maximal resource allocated for every fidelity level. ASHA-HB. We use the public implementation from the well-known optuna library (version2.10.0). We used the same eta, minimum and maximal budget as for HB, DEHB, and BOHB in our experi- ments, to have a fair comparison. B Additional Plots In Figure 8, we ablate the learning curve input in our kernel, to see the effect it has on performance for the CIFAR-10 and CIFAR-100 datasets from the NAS-Bench-201 benchmark. The results indicate that the learning curve plays an important role in achieving better results by allowing faster convergence and a better anytime performance. 7https://github.com/automl/HpBandSter 8https://github.com/automl/DEHB/ 9https://github.com/dragonfly/dragonfly 10https://github.com/shib0li/DNN-MFBO 16102 103 104 105 Training Time in Seconds 10 2 10 1 Regret cifar10 DyHPO DyHPO w/o CNN Random Search 102 103 104 105 Training Time in Seconds 10 2 10 1 Regret cifar100 DyHPO DyHPO w/o CNN Random Search Figure 8: The learning curve ablation for the CIFAR-10 and CIFAR-100 tasks of NAS-Bench-201. 0 1000 2000 3000 4000 Number of Epochs 10 2 10 1 Regret cifar10 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 1000 2000 3000 4000 Number of Epochs 10 2 10 1 Regret cifar100 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO Figure 9: NAS-Bench-201 regret results over the number of epochs spent during the optimization. Additionally, in Figure 9, we show the performance comparison over the number of epochs of every method for the CIFAR-10 and CIFAR-100 datasets in the NAS-Bench-201 benchmark. While, in Figure 10, we present the performance comparison over time. As can be seen, DYHPO converges faster and has a better performance compared to the other methods over the majority of the time or steps, however, towards the end although it is the optimal method or close to the optimal method, the difference in regret is not significant anymore. Furthermore, Figure 11 shows the performance comparison for the datasets chosen from TaskSet over the number of steps. Looking at the results, DYHPO is outperforming all methods convincingly on the majority of datasets by converging faster and with significant differences in the regret evaluation metric. In Figure 12 and 13, we show the performance comparison for all the datasets from LCBench regarding regret over the number of epochs. Similarly, in Figure 14 and 15, we show the same performance comparison, however, over time. As can be seen, DYHPO manages to outperform the other competitors in the majority of the datasets, and in the datasets that it does not, it is always close to the top-performing method, and the difference between methods is marginal. In Figure 16 we provide the extended results of Experiment 3 for TaskSet. We show the precision, average regret, and promotion percentage for poor-performing configurations for DYHPO and the other competitor methods. Lastly, we explore the behavior of DYHPO after finding the configuration which is returned at the end of the optimization as the best configuration. In Figure 17, we show how the budget is distributed on the configurations considered during that part of the optimization. Clearly, DYHPO is spending very little budget on most configurations. Furthermore, we investigated how many new configurations are considered during this phase. For LCBench, 76.98% of considered configurations are new demonstrating that DYHPO is investigating most of the budget into exploration. These are even more extreme for TaskSet (93.16% and NAS-Bench-201 (97.51%). 17102 103 104 Wallclock Time in Seconds 10 2 10 1 Regret cifar10 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 102 103 104 Wallclock Time in Seconds 10 2 10 1 Regret cifar100 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO Figure 10: NAS-Bench-201 regret results over the total optimization time. The total time includes the method overhead time and the hyperparameter configuration evaluation time. 0 200 400 600 800 1000 Number of Steps 10 3 10 2 10 1 Regret FixedTextRNNClassification imdb_patch128_LSTM128_avg_bs64 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Steps 10 3 10 2 10 1 Regret FixedTextRNNClassification imdb_patch128_LSTM128_bs64 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Steps 10 1 Regret FixedTextRNNClassification imdb_patch128_LSTM128_embed128_bs64 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Steps 10 1 6 × 10 2 2 × 10 1 3 × 10 1 Regret FixedTextRNNClassification imdb_patch32_GRU128_bs128 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Steps 10 1 Regret FixedTextRNNClassification imdb_patch32_GRU64_avg_bs128 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Steps 10 1 Regret FixedTextRNNClassification imdb_patch32_IRNN64_relu_avg_bs128 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Steps 10 1 100 Regret FixedTextRNNClassification imdb_patch32_IRNN64_relu_last_bs128 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Steps 10 2 10 1 Regret FixedTextRNNClassification imdb_patch32_LSTM128_E128_bs128 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Steps 10 2 10 1 Regret FixedTextRNNClassification imdb_patch32_LSTM128_bs128 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Steps 10 1 2 × 10 1 Regret FixedTextRNNClassification imdb_patch32_VRNN128_tanh_bs128 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Steps 10 1 Regret FixedTextRNNClassification imdb_patch32_VRNN64_relu_avg_bs128 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Steps 10 3 10 2 10 1 Regret FixedTextRNNClassification imdb_patch32_VRNN64_tanh_avg_bs128 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO Figure 11: Performance comparison over the number of steps on a dataset level for TaskSet. 180 200 400 600 800 1000 Number of Epochs 10 2 Regret APSFailure Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 1 3 × 10 2 4 × 10 2 6 × 10 2 Regret Amazon_employee_access Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 3 10 2 10 1 Regret Australian Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 3 10 2 10 1 Regret Fashion-MNIST Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret KDDCup09_appetency Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 4 10 3 10 2 10 1 Regret MiniBooNE Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 3 10 2 10 1 Regret adult Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 3 10 2 Regret airlines Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 3 10 2 10 1 Regret albert Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret bank-marketing Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret blood-transfusion-service-center Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 1 Regret car Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret christine Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 100 Regret cnae-9 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret connect-4 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 4 10 3 10 2 10 1 Regret covertype Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 1 Regret credit-g Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 4 10 2 100 Regret dionis Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO Figure 12: Performance comparison over the number of steps on a dataset level for LCBench. 190 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret fabert Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 1 Regret helena Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret higgs Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret jannis Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret jasmine Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 4 10 3 10 2 10 1 Regret jungle_chess_2pcs_raw_endgame_complete Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 1 Regret kc1 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret kr-vs-kp Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret mfeat-factors Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 4 10 3 10 2 10 1 Regret nomao Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 Regret numerai28.6 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret phoneme Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 1 Regret segment Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 1 Regret shuttle Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 4 10 2 Regret sylvine Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 3 10 2 10 1 Regret vehicle Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret volkert Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO Figure 13: Performance comparison over the number of steps on a dataset level for LCBench (cont.). 200 250 500 750 1000 1250 Wallclock Time in Seconds 10 2 Regret APSFailure Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 500 1000 1500 Wallclock Time in Seconds 10 1 3 × 10 2 4 × 10 2 6 × 10 2 Regret Amazon_employee_access Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 100 200 300 Wallclock Time in Seconds 10 3 10 2 10 1 Regret Australian Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 1000 2000 3000 4000 5000 Wallclock Time in Seconds 10 2 10 1 Regret Fashion-MNIST Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 500 1000 1500 2000 2500 Wallclock Time in Seconds 10 2 10 1 Regret KDDCup09_appetency Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 500 1000 1500 Wallclock Time in Seconds 10 1 Regret MiniBooNE Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 500 1000 1500 2000 Wallclock Time in Seconds 10 3 10 2 10 1 Regret adult Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 2500 5000 7500 10000 12500 Wallclock Time in Seconds 10 3 10 2 Regret airlines Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 2000 4000 6000 8000 10000 Wallclock Time in Seconds 10 2 Regret albert Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 500 1000 1500 2000 Wallclock Time in Seconds 10 2 10 1 Regret bank-marketing Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Wallclock Time in Seconds 10 2 10 1 Regret blood-transfusion-service-center Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Wallclock Time in Seconds 10 1 Regret car Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 5000 10000 15000 Wallclock Time in Seconds 10 2 10 1 Regret christine Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 500 1000 1500 2000 2500 Wallclock Time in Seconds 10 2 10 1 100 Regret cnae-9 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 500 1000 1500 2000 2500 Wallclock Time in Seconds 10 2 10 1 Regret connect-4 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 1000 2000 3000 4000 5000 Wallclock Time in Seconds 10 1 Regret covertype Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Wallclock Time in Seconds 10 1 Regret credit-g Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 2000 4000 6000 8000 Wallclock Time in Seconds 10 1 Regret dionis Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO Figure 14: Performance comparison over time on a dataset level for LCBench with the overhead included. 210 1000 2000 3000 Wallclock Time in Seconds 10 2 10 1 Regret fabert Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Wallclock Time in Seconds 10 1 Regret helena Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Wallclock Time in Seconds 10 1 Regret higgs Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 1000 2000 3000 Wallclock Time in Seconds 10 2 10 1 Regret jannis Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 250 500 750 1000 1250 Wallclock Time in Seconds 10 2 10 1 Regret jasmine Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 500 1000 1500 2000 Wallclock Time in Seconds 10 2 10 1 Regret jungle_chess_2pcs_raw_endgame_complete Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Wallclock Time in Seconds 10 1 Regret kc1 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 250 500 750 1000 Wallclock Time in Seconds 10 2 10 1 Regret kr-vs-kp Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 250 500 750 1000 1250 Wallclock Time in Seconds 10 2 10 1 Regret mfeat-factors Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 500 1000 1500 2000 Wallclock Time in Seconds 10 2 10 1 Regret nomao Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Wallclock Time in Seconds 10 2 Regret numerai28.6 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 250 500 750 1000 1250 Wallclock Time in Seconds 10 2 10 1 Regret phoneme Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Wallclock Time in Seconds 10 1 Regret segment Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 500 1000 1500 2000 Wallclock Time in Seconds 10 1 Regret shuttle Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 250 500 750 1000 1250 Wallclock Time in Seconds 10 4 10 2 Regret sylvine Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Wallclock Time in Seconds 10 2 10 1 Regret vehicle Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 1000 2000 3000 Wallclock Time in Seconds 10 2 10 1 Regret volkert Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO Figure 15: Performance comparison over time on a dataset level for LCBench with the overhead included. (cont.). 220 10 20 30 40 50 Number of Epochs 0.0 0.1 0.2 0.3Precision of Top Candidates Taskset Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 10 20 30 40 50 Number of Epochs 0.10 0.15 0.20 0.25 0.30Average Regret Taskset Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 10 20 30 40 50 Number of Steps 0.0 0.2 0.4 0.6 0.8 Fraction of Poor Performer Promotions Taskset Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO Baseline Figure 16: The efficiency of DYHPO as the optimization progresses. Left: The fraction of top- performing candidates from all candidates that were selected to be trained. Middle: The average regret for the configurations that were selected to be trained at a given budget. Right: The percentage of configurations that belong to the top 1/3 configurations at a given budget and that were in the top bottom 2/3 of the configurations at a previous budget. All of the results are from the Taskset benchmark. 0 10 20 30 40 50 Epochs 0.0 0.2 0.4 0.6Relative Number of Candidates LCBench 0 50 100 150 200 Epochs 0.0 0.2 0.4 0.6 0.8Relative Number of Candidates NAS-Bench-201 0 10 20 30 40 50 Steps 0.0 0.2 0.4 0.6 0.8Relative Number of Candidates TaskSet Figure 17: These plots shed light on how DYHPO behaves after the configuration it finally returns as the best. The plots show how many epochs are spent per candidate. As we can see, for most candidates only a small budget was considered, indicating that DYHPO is mostly exploring at this point. 23",
      "meta_data": {
        "arxiv_id": "2202.09774v2",
        "authors": [
          "Martin Wistuba",
          "Arlind Kadra",
          "Josif Grabocka"
        ],
        "published_date": "2022-02-20T10:28:02Z",
        "pdf_url": "https://arxiv.org/pdf/2202.09774v2.pdf"
      }
    },
    {
      "title": "Conformal Inference is (almost) Free for Neural Networks Trained with Early Stopping",
      "abstract": "Early stopping based on hold-out data is a popular regularization technique\ndesigned to mitigate overfitting and increase the predictive accuracy of neural\nnetworks. Models trained with early stopping often provide relatively accurate\npredictions, but they generally still lack precise statistical guarantees\nunless they are further calibrated using independent hold-out data. This paper\naddresses the above limitation with conformalized early stopping: a novel\nmethod that combines early stopping with conformal calibration while\nefficiently recycling the same hold-out data. This leads to models that are\nboth accurate and able to provide exact predictive inferences without multiple\ndata splits nor overly conservative adjustments. Practical implementations are\ndeveloped for different learning tasks -- outlier detection, multi-class\nclassification, regression -- and their competitive performance is demonstrated\non real data.",
      "full_text": "Conformal inference is (almost) free for neural networks trained with early stopping Ziyi Liang∗, Yanfei Zhou†, Matteo Sesia† June 28, 2023 Abstract Early stopping based on hold-out data is a popular regularization technique designed to mitigate overfitting and increase the predictive accuracy of neural networks. Models trained with early stopping often provide relatively accurate predictions, but they generally still lack precise statistical guarantees unless they are further calibrated using independent hold-out data. This paper addresses the above limitation with conformalized early stopping : a novel method that combines early stopping with conformal calibration while efficiently recycling the same hold-out data. This leads to models that are both accurate and able to provide exact predictive inferences without multiple data splits nor overly conservative adjustments. Practical implementations are developed for different learning tasks—outlier detection, multi-class classification, regression— and their competitive performance is demonstrated on real data. 1 Introduction Deep neural networks can detect complex data patterns and leverage them to make accurate pre- dictions in many applications, including computer vision, natural language processing, and speech recognition, to name a few examples. These models can sometimes even outperform skilled hu- mans [1], but they still make mistakes. Unfortunately, the severity of these mistakes is compounded by the fact that the predictions computed by neural networks are often overconfident [2], partly due to overfitting [3, 4]. Several training strategies have been developed to mitigate overfitting, including dropout [5], batch normalization [6], weight normalization [7], data augmentation [8], and early stopping [9]; the latter is the focus of this paper. Early stopping consists of continuously evaluating after each batch of stochastic gradient updates (or epoch) the predictive performance of the current model on hold-out independent data. After a large number of gradient updates, only the intermediate model achieving the best performance on the hold-out data is utilized to make predictions. This strategy is often effective at mitigat- ing overfitting and can produce relatively accurate predictions compared to fully trained models, but it does not fully resolve overconfidence because it does not lead to models with finite-sample guarantees. ∗Department of Mathematics, University of Southern California, Los Angeles, CA, USA. †Department of Data Sciences and Operations, University of Southern California, Los Angeles, CA, USA. 1 arXiv:2301.11556v2  [stat.ML]  26 Jun 2023A general framework for quantifying the predictive uncertainty of anyblack-box machine learning model is that of conformal inference [10]. The idea is to apply a pre-trained model to a calibration set of hold-out observations drawn at random from the target population. If the calibration data are exchangeable with the test point of interest, the model performance on the calibration set can be translated into statistically rigorous predictive inferences. This framework is flexible and can accommodate different learning tasks, including out-of-distribution testing [11], classification [12], and regression [10]. For example, in the context of classification, conformal inference can give prediction sets that contain the correct label for a new data point with high probability. In theory, the quality of the trained model has no consequence on the average validity of conformal inferences, but it does affect their reliability and usefulness on a case-by-case level. In particular, conformal uncertainty estimates obtained after calibrating an overconfident model may be too conservative for some test cases and too optimistic for others [13]. The goal of this paper is to combine conformal calibration with standard early stopping training techniques as efficiently as possible, in order to produce more reliable predictive inferences with a finite amount of available data. Achieving high accuracy with deep learning often requires large training sets [14], and conformal inference makes the overall pipeline even more data-intensive. As high-quality observations can be expensive to collect, in some situations practitioners may naturally wonder whether the advantage of having principled uncertainty estimates is worth a possible reduction in predictive accuracy due to fewer available training samples. This concern is relevant because the size of the calibration set cannot be too small if one wants stable and reliable conformal inferences [15, 16]. In fact, very large calibration sets may be necessary to obtain stronger conformal inferences that are valid not only on average but also conditionally on some important individual features; see Vovk et al. [12], Romano et al. [17], and Barber et al. [18]. This paper resolves the above dilemma by showing that conformal inferences for deep learning models trained with early stopping can be obtained almost “for free”—without spending more precious data. More precisely, we present an innovative method that blends model training with early stopping and conformal calibration using the same hold-out samples, essentially obtaining rigorous predictive inferences at no additional data cost compared to standard early stopping. It is worth emphasizing this result is not trivial. In fact, naively applying existing conformal calibration methods using the same hold-out samples utilized for early stopping would not lead to theoretically valid inferences, at least not without resorting to very conservative corrections. The paper is organized as follows. Section 2 develops our conformalized early stopping (CES) method, starting from outlier detection and classification, then addressing regression. Section 3 demonstrates the advantages of CES through numerical experiments. Section 4 concludes with a discussion and some ideas for further research. Additional details and results, including a theoretical analysis of the naive benchmark mentioned above, can be found in the Appendices, along with all mathematical proofs. Related Work Conformal inference [10, 19, 20] has become a very rich and active area of research [21–24]. Many prior works studied the computation of efficient conformal inferences starting from pre-trainedblack- box models, including for example in the context of outlier detection [11, 25–27], classification [12, 13, 28–30], and regression [10, 22, 31]. Other works have studied the general robustness of conformal inferences to distribution shifts [32, 33] and, more broadly, to failures of the data exchangeability 2assumption [34, 35]. Our research is orthogonal, as we look inside the black-box model and develop a novel early-stopping training technique that is naturally integrated with conformal calibration. Nonetheless, the proposed method could be combined with those described in the aforementioned papers. Other recent research has explored different ways of bringing conformal inference into the learning algorithms [36–39], and some of those works apply standard early stopping techniques, but they do not address our problem. This paper is related to Yang and Kuchibhotla [40], which proposed a general theoretical ad- justment for conformal inferences computed after model selection. That method could be utilized to account for early stopping without further data splits, as explained in Section 2.2. However, we will demonstrate that even an improved version of such analysis remains overly conservative in the context of model selection via early stopping, and the alternative method developed in this paper performs much better in practice. Our solution is inspired by Mondrian conformal inference [12] as well as by the integrative conformal method of Liang et al. [26]. The latter deals with the problem of selecting the best model from an arbitrary machine learning toolbox to obtain the most pow- erful conformal p-values for outlier testing. The idea of Liang et al. [26] extends naturally to the early stopping problem in the special cases of outlier detection and classification, but the regression setting requires substantial technical innovations. The work of Liang et al. [26] is also related to Marandon et al. [41], although the latter is more distant from this paper because it focuses on theoretically controlling the false discovery rate [42] in multiple testing problems. Finally, this paper draws inspiration from Kim et al. [43], which shows that models trained with bootstrap (or bagging) techniques can also lead to valid conformal inferences essentially for free. 2 Methods 2.1 Standard Conformal Inference and Early Stopping Consider n data points, Zi for i ∈ D= [n] = {1, . . . , n}, sampled exchangeably (e.g., i.i.d.) from an unknown distribution PZ with support on some space Z. Consider also an additional test sample, Zn+1. In the context of outlier detection, one wishes to test whether Zn+1 was sampled exchangeably from PZ. In classification or regression, one can write Zi = (Xi, Yi), where Xi is a feature vector while Yi is a discrete category or a continuous response, and the goal is to predict the unobserved value of Yn+1 given Xn+1 and the data in D. The standard pipeline begins by randomly splitting the data in D into three disjoint subsets: Dtrain, Des, Dcal ⊂ [n]. The samples in Dtrain are utilized to train a model M via stochastic gradient descent, in such a way as to (approximately) minimize the desired loss L, while the observations in Des and Dcal are held out. We denote by Mt the model learnt after t epochs of stochastic gradient descent, for any t ∈ [tmax], where tmax is a pre-determined maximum number of epochs. For sim- plicity, L is assumed to be an additive loss, in the sense that its value calculated on the training data after t epochs is Ltrain(Mt) = P i∈Dtrain ℓ(Mt; Zi), for some appropriate function ℓ. For example, a typical choice for regression would be the squared-error loss: ℓ(Mt; Zi) = [Yi − ˆµ(Xi; Mt)]2, where ˆµ(Xi; Mt) indicates the value of the regression function at Xi, as estimated by Mt. Similarly, the loss evaluated onDes is denoted as Les(Mt) = P i∈Des ℓ(Mt; Zi). After training for tmax epochs, early stopping selects the model ˆMes that minimizes the loss on Des: ˆMes = arg minMt : 0≤t≤tmax Les(Mt). Conformal calibration of ˆMes is then conducted using the independent hold-out data set Dcal, as sketched in Figure 1 (a). This pipeline requires a three-way data split because: (i) Dtrain and Des 3must be disjoint to ensure the early stopping criterion is effective at mitigating overfitting; and (ii) Dcal must be disjoint from Dtrain ∪ Des to ensure the performance of the selected model ˆMes on the calibration data gives us an unbiased preview of its future performance at test time, enabling valid conformal inferences. data set D Dtrain candidate models Des Dcal selected model test point conformal inference (a) Conformal inference after early stopping. data set D Dtrain candidate models Des-cal selected model test point conformal inference (b) Conformalized early stopping (CES). Figure 1: Schematic visualization of rigorous conformal inferences for models trained with early stopping. (a) Conventional pipeline requiring a three-way sample split. (b) Conformalized early stopping, requiring only a two-way split. 2.2 The Limitations of a Naive Benchmark An intuitive alternative to the standard approach described in the previous section is to naively perform standard conformal inference using the same hold-out samples utilized for early stopping, as sketched in Figure 2. This approach, which we call the naive benchmark, may seem appealing as it avoids a three-way data split, but it does not provide rigorous inferences. In fact, the necessary exchangeability with the test point is broken if the same hold-out data are used twice—first to evaluate the early stopping criterion and then to perform conformal calibration. In principle, the issue could be corrected by applying a conservative adjustment to the nominal significance level of the conformal inferences, as studied by Yang and Kuchibhotla [40] and reviewed in Appendix A1. However, this leads to overly conservative inferences in practice when applied with the required theoretical correction, as demonstrated by the numerical experiments summarized in Figure 3, even if a tighter adjustment developed in Appendix A1 is utilized instead of that of Yang and Kuchibhotla [40]. Intuitively, the problem is that there tend to be complicated dependencies among the candidate models provided to the early stopping algorithm, but the available analyses are not equipped to handle such intricacies and must therefore take a pessimistic viewpoint of the model selection process. Thus, the naive benchmark remains unsatisfactory, although it can serve as an informative benchmark for the novel method developed in this paper. Interestingly, we will see empirically that the naive benchmark applied without the required theoretical corrections often performs similarly to the rigorous method proposed in this paper, especially for large data sets. 2.3 Preview of our Contribution This paper develops a novel conformalized early stopping (CES) method to jointly carry out both early stopping and conformal inference using a single hold-out data set, denoted in the following as Des-cal. The advantage of this approach is that it avoids a three-way data split, so that more samples can be allocated to Dtrain, without breaking the required data exchangeability. As a result, 4data set D Dtrain candidate models Des-cal selected model test point conformal inference Figure 2: Schematic visualization of heuristic conformal inferences based on a naive benchmark that utilizes the same hold-out data twice. Coverage (marginal) Coverage (conditional) Width 200 500 1000 2000 200 500 1000 2000 200 500 1000 2000 16 17 18 19 20 21 0.7 0.8 0.9 1.0 0.7 0.8 0.9 1.0 Sample size Method CES Naive + theory Data splitting Full training Figure 3: Average performance, as a function of the sample size, of conformal inferences based on neural networks trained and calibrated with different methods, on the bio regression data [44]. Ideally, the coverage of the conformal prediction intervals should be close to 90% and their width should be small. All methods shown here guarantee 90% marginal coverage. See Table A3 for more detailed results and standard errors. CES often leads to relatively more reliable and informative conformal inferences compared to other existing approaches; e.g., see for example the empirical performance preview shown in Figure 3. The CES method is based on the following idea inspired by Liang et al. [26]. Valid conformal inferences can be obtained by calibrating ˆMes using the same data set Des-cal used for model selection, as long as the test sample Zn+1 is also involved in the early stopping rule exchangeably with all other samples in Des-cal. This concept, sketched in Figure 1 (b), is not obvious to translate into a practical method, however, for two reasons. Firstly, the ground truth for the test point (i.e., its outlier status or its outcome label) is unknown. Secondly, the method may need to be repeatedly applied for a large number of distinct test points in a computationally efficient way, and one cannot re-train the model separately for each test point. In the next section, we will explain how to overcome these challenges in the special case of early stopping for outlier detection; then, the solution will be extended to the classification and regression settings. 2.4 CES for Outlier Detection Consider testing whether Zn+1, is an inlier, in the sense that it was sampled from PZ exchangeably with the data in D. Following the notation of Section 2.1, consider a partition ofD into two subsets, Dtrain and Des-cal, chosen at random independently of everything else, such thatD = Dtrain ∪Des-cal. The first step of CES consists of training a deep one-class classifier M using the data in Dtrain via 5stochastic gradient descent for tmax epochs, storing all parameters characterizing the intermediate model after each τ epochs. We refer to τ ∈ [tmax] as the storage period, a parameter pre-defined by the user. Intuitively, a smaller τ increases the memory cost of CES but may also lead to the selection of a more accurate model. While the memory cost of this approach is higher compared to that of standard early-stopping training techniques, which only require storing one model at a time, it is not prohibitively expensive. In fact, the candidate models do not need to be kept in precious RAM memory but can be stored on a relatively cheap hard drive. As reasonable choices of τ may typically be in the order of T = ⌊tmax/τ⌋ ≈100, the cost of CES is not excessive in many real-world situations. For example, it takes approximately 100 MB to store a pre-trained standard ResNet50 computer vision model, implying that CES would require approximately 10 GB of storage in such applications—today this costs less than $0.25/month in the cloud. After pre-training and storing T candidate models, namely Mt1, . . . , MtT for some sub-sequence (t1, . . . , tT ) of [ tmax], the next step is to select the appropriate early-stopped model based on the hold-out data in Des-cal as well as the test point Zn+1. Following the notation of Section 2.1, define the value of the one-class classification loss L for model Mt, for any t ∈ [T], evaluated on Des-cal as: Les-cal(Mt) = P i∈Des-cal ℓ(Mt; Zi). Further, for any z ∈ Z, define also L+1 es-cal(Mt, z) as: L+1 es-cal(Mt, z) = Les-cal(Mt) + ℓ(Mt; z). (1) Therefore, L+1 es-cal(Mt, Zn+1) can be interpreted as the cumulative value of the loss function calcu- lated on an augmented hold-out data set including alsoZn+1. Then, we select the model ˆMces(Zn+1) minimizing L+1 es-cal(Mt, Zn+1): ˆMces(Zn+1) = arg min Mtj : 1≤j≤T L+1 es-cal(Mtj , Zn+1). (2) Note that the computational cost of evaluating (2) is negligible compared to that of training the models. Next, the selected model ˆMces(Zn+1) is utilized to compute a conformal p-value [27] to test whether Zn+1 is an inlier. In particular, ˆMces(Zn+1) is utilized to compute nonconformity scores ˆSi(Zn+1) for all samples i ∈ Des-cal ∪{n+1}. These scores rank the observations in Des-cal ∪{n+1} based on how the one-class classifier ˆMces(Zn+1) perceives them to be similar to the training data; by convention, a smaller value of ˆSi(Zn+1) suggests Zi is more likely to be an outlier. Suitable scores are typically included in the output of standard one-class classification models, such as those provided by the Python library PyTorch. For simplicity, we assume all scores are almost-surely distinct; otherwise, ties can be broken at random by adding a small amount of independent noise. Then, the conformal p-value ˆu0(Zn+1) is given by the usual formula: ˆu0(Zn+1) = 1 + |i ∈ Des-cal : ˆSi ≤ ˆSn+1| 1 + |Des-cal| , (3) making the dependence of ˆSi on Zn+1 implicit in the interest of space. This method, outlined by Algorithm 1, gives p-values that are exactly valid in finite samples, in the sense that they are stochastically dominated by the uniform distribution under the null hypothesis. 6Algorithm 1 Conformalized early stopping for outlier detection 1: Input: Exchangeable data points Z1, . . . , Zn; test point Zn+1. 2: Input: Maximum number of training epochs tmax; storage period hyper-parameter τ. 3: Input: One-class classifier trainable via (stochastic) gradient descent. 4: Randomly split the exchangeable data points into Dtrain and Des-cal. 5: Train the one-class classifier for tmax epochs and save the intermediate models Mt1, . . . , MtT . 6: Pick the most promising model ˆMces(Zn+1) based on (2), using the data in Des-cal ∪ {n + 1}. 7: Compute nonconformity scores ˆSi(Zn+1) for all i ∈ Des-cal ∪ {n + 1} using model ˆMces(Zn+1). 8: Output: Conformal p-value ˆu0(Zn+1) given by (3). Theorem 1. Assume Z1, . . . , Zn, Zn+1 are exchangeable random samples, and let ˆu0(Zn+1) be the output of Algorithm 1, as given in (3). Then, P[ˆu0(Zn+1) ≤ α] ≤ α for any α ∈ (0, 1). 2.5 CES for Classification The above CES method will now be extended to deal with K-class classification problems, for any K ≥ 2. Consider n exchangeable pairs of observations ( Xi, Yi), for i ∈ D= [n], and a test point (Xn+1, Yn+1) whose label Yn+1 ∈ [K] has not yet been observed. The goal is to construct an informative prediction set for Yn+1 given the observed features Xn+1 and the rest of the data, assuming (Xn+1, Yn+1) is exchangeable with the observations indexed byD. An ideal goal would be to construct the smallest possible prediction set with guaranteedfeature-conditional coverageat level 1 − α, for any fixed α ∈ (0, 1). Formally, a prediction set ˆCα(Xn+1) ⊆ [K] has feature-conditional coverage at level 1 − α if P[Yn+1 ∈ ˆCα(Xn+1) | Xn+1 = x] ≥ 1 − α, for any x ∈ X, where X is the feature space. Unfortunately, perfect feature-conditional coverage is extremely difficult to achieve unless the feature space X is very small [18]. Therefore, in practice, one must be satisfied with obtaining relatively weaker guarantees, such as label-conditional coverage and marginal coverage. Formally, ˆCα(Xn+1) has 1−α label-conditional coverage if P[Yn+1 ∈ ˆCα(Xn+1) | Yn+1 = y] ≥ 1−α, for any y ∈ [K], while marginal coverage corresponds to P[Yn+1 ∈ ˆCα(Xn+1)] ≥ 1 − α. Label- conditional coverage is stronger than marginal coverage, but both criteria are useful because the latter is easier to achieve with smaller (and hence more informative) prediction sets. We begin by focusing on label-conditional coverage, as this follows most easily from the results of Section 2.4. This solution will be extended in Appendix A2 to target marginal coverage. The first step of CES consists of randomly splitting D into two subsets, Dtrain and Des-cal, as in Section 2.4. The samples in Des-cal are further divided into subsets Dy es-cal with homogeneous labels; that is, Dy es-cal = {i ∈ Des-cal : Yi = y} for each y ∈ [K]. The data in Dtrain are utilized to train a neural network classifier via stochastic gradient descent, storing the intermediate candidate models Mt after each τ epochs. This is essentially the same approach as in Section 2.4, with the only difference being that the neural network is now designed to perform K-class classification rather than one-class classification. Therefore, this neural network should have a soft-max layer with K nodes near its output, whose values corresponding to an input data point with features x are denoted as ˆπy(x), for all y ∈ [K]. Intuitively, we will interpret ˆπy(x) as approximating (possibly inaccurately) the true conditional data-generating distribution; i.e., ˆπy(x) ≈ P[Y = y | X = x]. For any model Mt, any x ∈ X, and any y ∈ [K], define the augmented loss L+1 es-cal(Mt, x, y) as: L+1 es-cal(Mt, x, y) = Les-cal(Mt) + ℓ(Mt; x, y). (4) 7Concretely, a typical choice for ℓ is the cross-entropy loss: ℓ(Mt; x, y) = −log ˆπt y(x), where ˆπt denotes the soft-max probability distribution estimated by model Mt. Intuitively, L+1 es-cal(Mt, x, y) is the cumulative value of the loss function calculated on an augmented hold-out data set including also the imaginary test sample ( x, y). Then, for any y ∈ [K], CES selects the model ˆMces(Xn+1, y) minimizing L+1 es-cal(Mt, Xn+1, y) among the T stored models: ˆMces(Xn+1, y) = arg min Mtj : 1≤j≤T L+1 es-cal(Mtj , Xn+1, y). (5) The selected model ˆMces(Xn+1, y) is then utilized to compute a conformal p-value for testing whether Yn+1 = y. In particular, we compute nonconformity scores ˆSy i (Xn+1) for alli ∈ Dy es-cal∪{n+ 1}, imagining that Yn+1 = y. Different types of nonconformity scores can be easily accommodated, but in this paper, we follow the adaptive strategy of Romano et al. [13]. The computation of these nonconformity scores based on the selected model ˆMces is reviewed in Appendix A3. Here, we simply note the p-value is given by: ˆuy(Xn+1) = 1 + |i ∈ Dy es-cal : ˆSy i ≤ ˆSy n+1| 1 + |Dy es-cal| , (6) again making the dependence of ˆSy i on Xn+1 implicit. Finally, the prediction set ˆCα(Xn+1) is constructed by including all possible labels for which the corresponding null hypothesis cannot be rejected at level α: ˆCα(Xn+1) = {y ∈ [K] : ˆuy(Xn+1) ≥ α}. (7) This method, outlined by Algorithm 2, guarantees label-conditional coverage at level 1 − α. Algorithm 2 Conformalized early stopping for multi-class classification 1: Input: Exchangeable data points ( X1, Y1), . . . ,(Xn, Yn) with labels Yi ∈ [K]. 2: Input: Test point with features Xn+1. Desired coverage level 1 − α. 3: Input: Maximum number of training epochs tmax; storage period hyper-parameter τ. 4: Input: K-class classifier trainable via (stochastic) gradient descent. 5: Randomly split the exchangeable data points into Dtrain and Des-cal. 6: Train the K-class classifier for tmax epochs and save the intermediate models Mt1, . . . , MtT . 7: for y ∈ [K] do 8: Define Dy es-cal = {i ∈ Des-cal : Yi = y} and imagine Yn+1 = y. 9: Pick the model ˆMces(Xn+1, y) according to (5), using the data in Des-cal ∪ {n + 1}. 10: Compute scores ˆSy i (Xn+1), ∀i ∈ Dy es-cal ∪ {n + 1}, using ˆMces(Xn+1, y); see Appendix A3. 11: Compute the conformal p-value ˆuy(Xn+1) according to (6). 12: end for 13: Output: Prediction set ˆCα(Xn+1) given by (7). Theorem 2. Assume (X1, Y1), . . . ,(Xn+1, Yn+1) are exchangeable, and let ˆCα(Xn+1) be the output of Algorithm 2, as given in (7), for any given α ∈ (0, 1). Then, P[Yn+1 ∈ ˆCα(Xn+1) | Yn+1 = y] ≥ 1 − α for any y ∈ [K]. 82.6 CES for Regression This section extends CES to regression problems with a continuous outcome. As in the previous sections, consider a data set containing n exchangeable observations (Xi, Yi), for i ∈ D= [n], and a test point (Xn+1, Yn+1) with a latent label Yn+1 ∈ R. The goal is to construct a reasonably narrow prediction interval ˆCα(Xn+1) for Yn+1 that is guaranteed to have marginal coverage above some level 1 − α, i.e., P[Yn+1 ∈ ˆCα(Xn+1)] ≥ 1 − α, and can also practically achieve reasonably high feature- conditional coverage. Developing a CES method for this problem is more difficult compared to the classification case studied in Section 2.5 due to the infinite number of possible values for Yn+1. In fact, a naive extension of Algorithm 2 would be computationally unfeasible in the regression setting, for the same reason why full-conformal prediction [10] is generally impractical. The novel solution described below is designed to leverage the particular structure of an early stopping criterion based on the squared-error loss evaluated on hold-out data. Focusing on the squared-error loss makes CES easier to implement and explain using classical absolute residual nonconformity scores [10, 45]. However, similar ideas could also be repurposed to accommodate other scores, such as those based on quantile regression [31], conditional distributions [46, 47], or conditional histograms [48]. As usual, we randomly split D into Dtrain and Des-cal. The data in Dtrain are utilized to train a neural network via stochastic gradient descent, storing the intermediate models Mt after each τ epoch. The approach is similar to those in Sections 2.4–2.5, although now the output of a model Mt applied to a sample with features x is denoted by ˆµt(x) and is designed to approximate (possibly inaccurately) the conditional mean of the unknown data-generating distribution; i.e., ˆ µt(x) ≈ E[Y | X = x]. (Note that we will omit the superscript t unless necessary to avoid ambiguity). For any model Mt and any x ∈ X, y ∈ R, define L+1 es-cal(Mt, x, y) = Les-cal(Mt) + [y − ˆµt(x)]2. (8) Consider now the following optimization problem, ˆMces(Xn+1, y) = arg min Mtj : 1≤j≤T L+1 es-cal(Mtj , Xn+1, y), (9) which can be solved simultaneously for all y ∈ R thanks to the amenable form of (8). In fact, each L+1 es-cal(Mt, x, y) is a simple quadratic function of y; see the sketch in Figure 4. This implies ˆMces(Xn+1, y) is a step function, whose parameters can be computed at cost O(T log T) with an efficient divide-and-conquer algorithm designed to find the lower envelope of a family of parabolas [49, 50]; see Appendix A4. Therefore, ˆMces(Xn+1, y) has L distinct steps, for some L = O(T log T) that may depend on Xn+1, and it can be written as a function of y as: ˆMces(Xn+1, y) = LX l=1 ml(Xn+1)1 [y ∈ (kl−1, kl]] , (10) where ml(Xn+1) ∈ [T] represents the best model selected within the interval ( kl−1, kl] such that ml(Xn+1) ̸= ml−1(Xn+1) for all l ∈ [L]. Above, k1 ≤ k2 ≤ ··· ≤kL denote the knots of ˆMces(Xn+1, y), which also depend on Xn+1 and are defined as the boundaries in the domain of y between each consecutive pair of steps, with the understanding that k0 = −∞ and kL+1 = +∞. Then, for each step l ∈ [L], let Bl indicate the interval Bl = (kl−1, kl] and, for all i ∈ Des-cal, eval- 9y L +1 es-cal ( M t ,x ,y ) M 1 M 2 M 3 𝑘!𝑘\"𝑘# Figure 4: Squared-error loss on test-augmented hold-out data for three alternative regression models M1, M2 and M3, as a function of the place-holder outcome y for the test point. The CES method utilizes the best model for each possible value of y, which is identified by the lower envelope of these three parabolas. In this case, the lower envelope has two finite knots at k1 and k3. uate the nonconformity score ˆSi(Xn+1, Bl) for observation ( Xi, Yi) based on the regression model indicated by ml(Xn+1); i.e., ˆSi(Xn+1, Bl) = |Yi − ˆµml(Xn+1)(Xi)|. (11) Let ˆQ1−α(Xn+1, Bl) denote the ⌈(1 − α)(1 + |Des-cal|)⌉-th smallest value among all nonconfor- mity scores ˆSi(Xn+1, Bl), assuming for simplicity that there are no ties; otherwise, ties can be broken at random. Then, define the interval ˆCα(Xn+1, Bl) as that obtained by applying the stan- dard conformal prediction method with absolute residual scores based on the regression model ˆµml(Xn+1)(Xn+1): ˆCα(Xn+1, Bl) = ˆµml(Xn+1)(Xn+1) ± ˆQ1−α(Xn+1, Bl). (12) Finally, the prediction interval ˆCα(Xn+1) is given by: ˆCα(Xn+1) = Convex \u0010 ∪L l=1{Bl ∩ ˆCα(Xn+1, Bl)} \u0011 , (13) where Convex(·) denotes the convex hull of a set. This procedure is summarized in Algorithm 3 and it is guaranteed to produce prediction sets with valid marginal coverage. 10Algorithm 3 Conformalized early stopping for regression 1: Input: Exchangeable data points ( X1, Y1), . . . ,(Xn, Yn) with outcomes Yi ∈ R. 2: Input: Test point with features Xn+1. Desired coverage level 1 − α. 3: Input: Maximum number of training epochs tmax; storage period hyper-parameter τ. 4: Input: Regression model trainable via (stochastic) gradient descent. 5: Randomly split the exchangeable data points into Dtrain and Des-cal. 6: Train the regression model for tmax epochs and save the intermediate models Mt1, . . . , MtT . 7: Evaluate ˆMces(Xn+1, y) as in (10), using Algorithm A9. 8: Partition the domain of Y into L intervals Bl, for l ∈ [L], based on knots of ˆMces(Xn+1, y). 9: for l ∈ [L] do 10: Evaluate nonconformity scores ˆSi(Xn+1, Bl) for all i ∈ Des-cal as in (11). 11: Compute ˆQ1−α(Xn+1, Bl): the ⌈(1 −α)(1 +|Des-cal|)⌉-th smallest value among ˆSi(Xn+1, Bl). 12: Construct the interval ˆCα(Xn+1, Bl) according to (12). 13: end for 14: Output: Prediction interval ˆCα(Xn+1) given as a function of { ˆCα(Xn+1, Bl)}L l=1 by (13). Theorem 3. Assume (X1, Y1), . . . ,(Xn+1, Yn+1) are exchangeable, and let ˆCα(Xn+1) be the output of Algorithm 3, as given by (13), for any given α ∈ (0, 1). Then, P[Yn+1 ∈ ˆCα(Xn+1)] ≥ 1 − α. The intuition behind the above method is as follows. Each intermediate interval ˆCα(Xn+1, Bl), for l ∈ [L], may be thought of as being computed by applying, under the null hypothesis that Yn+1 ∈ Bl, the classification method from Section 2.5 for a discretized version of our problem based on the partition {Bl}L l=1. Then, leveraging the classical duality between confidence intervals and p-values, it becomes clear that taking the intersection of Bl and ˆCα(Xn+1, Bl) essentially amounts to including the “label” Bl in the output prediction if the null hypothesis Yn+1 ∈ Bl cannot be rejected. The purpose of the final convex hull operation is to generate a contiguous prediction interval, which is what we originally stated to seek. One may intuitively be concerned that this method may output excessively wide prediction interval if the location of {Bl ∩ ˆCα(Xn+1, Bl)} is extremely large in absolute value. However, our numerical experiments will demonstrate that, as long as the number of calibration data points is not too small, the selected models in general provide reasonably concentrated predictions around the true test response regardless of the placeholder value y. Therefore, the interval ˆCα(Xn+1, Bl) tends to be close to the true y even if Bl is far away, in which case ˆCα(Xn+1, Bl) ∩ Bl = ∅ does not expand the final prediction interval ˆCα(Xn+1). Although it is unlikely, Algorithm 3 may sometimes produce an empty set, which is an uninfor- mative and potentially confusing output. A simple solution consists of replacing any empty output with the naive conformal prediction interval computed by Algorithm A7 in Appendix A1, which leverages an early-stopped model selected by looking at the original calibration data set without the test point. This approach is outlined by Algorithm A11 in Appendix A5. As the intervals given by Algorithm A11 always contain those output by Algorithm 3, it follows that Algorithm A11 also enjoys guaranteed coverage; see Corollary A3. 2.7 CES for Quantile Regression The CES method for regression described in Section 2.6 relies on classical nonconformity scores [10, 45] that are not designed to deal efficiently with heteroscedastic data [16, 31]. However, the idea 11can be extended to accommodate other nonconformity scores, including those based on quantile regression [31], conditional distributions [46, 47], or conditional histograms [48]. The reason why we have so far focused on the classical absolute residual scores is that they are more intuitive to apply in conjunction with an early stopping criterion based on the squared-error loss. In this section, we extend CES to the conformalized quantile regression (CQR) method of Romano et al. [31]. A review of the CQR method is provided in Appendix A6. As in the previous section, consider a data set containing n exchangeable observations (Xi, Yi), for i ∈ D= [n], and a test point ( Xn+1, Yn+1) with a latent label Yn+1 ∈ R. We first randomly split D into two subsets, Dtrain and Des-cal. The data in Dtrain are utilized to train a neural network quantile regression model [51] by seeking to minimize the pinball loss instead of the squared error loss, for each target level β = βlow and β = βhigh (e.g., βlow = α/2 and βhigh = 1 − α/2). Note that the same neural network, with two separate output nodes, can be utilized to estimate conditional quantiles at two different levels; e.g., as in Romano et al. [31]. For anyt ∈ [tmax], let Mβ,t denote the intermediate neural network model stored after t epochs of stochastic gradient descent, following the same notation as in Section 2.6. For each target level β and any x ∈ X, let ˆqβ,t(x) denote the approximate β−th conditional quantile of the unknown conditional distribution P(Y | X = x) estimated by Mβ,t. Similarly to Section 2.6, for any model Mβ,t and any x ∈ X, y ∈ R, define the augmented loss evaluated on the calibration data including also a dummy test point ( x, y) as: L+1 es-cal(Mβ,t, x, y) = Les-cal(Mβ,t) + L(Mβ,t, x, y) = X i∈Des-cal ρβ(Yi, ˆqβ,t(Xi)) + ρβ(y, ˆqβ,t(x)), (14) where ρβ denotes the pinball loss function defined in (A31). For any model Mβ,t, the augmented loss is equal to a constant plus a convex function of y, namely ρβ(y, ˆqβ,t(x)). Therefore, for any fixed x, the quantity in (14) can be sketched as a function of Mβ,t and y as shown in Figure 5. This is analogous to Figure 4 from Section 2.6, with the difference that now the quadratic functions have been replaced by piece-wise linear “pinball” functions. After pre-training and storing T candidate models, namely Mβ,t1, . . . , Mβ,tT for some sub- sequence (t1, . . . , tT ) of [tmax], consider the following optimization problem, ˆMβ,ces(Xn+1, y) = arg min Mβ,tj : 1≤j≤T L+1 es-cal(Mβ,tj , Xn+1, y). (15) This problem is equivalent to identifying the lower envelope of a family of shifted pinball loss func- tions, similarly to Section 2.6; see Figure 5 for a schematic visualization. Again, this lower envelope can be found at computational cost O(T log T), with the same divide-and-conquer algorithm de- scribed in Appendix A4. In particular, ˆMβ,ces(Xn+1, y) is a step function with respect to y with L distinct steps, for some L = O(T log T), and it can be written as: ˆMβ,ces(Xn+1, y) = LX l=1 mβ,l(Xn+1)1 h y ∈ (kβ l−1, kβ l ] i , (16) where mβ,l(Xn+1) ∈ [T] represents the best model selected within the interval ( kβ l−1, kβ l ] such that mβ,l(Xn+1) ̸= mβ,l−1(Xn+1) for all l ∈ [L]. Above, kβ 1 ≤ kβ 2 ≤ ··· ≤kβ L denote the knots of 12y L +1 es-cal ( M \u0000 ,t ,x ,y ) M \u0000 , 1 M \u0000 , 2 M \u0000 , 3 !\"!! Figure 5: Pinball loss functions on test-augmented hold-out data for three alternative regression models, M1, M2 and M3, as a function of the place-holder outcome y for the test point. The CES method utilizes the best model for each possible value of y, which is identified by the lower envelope of these three pinball loss functions. In this case, the lower envelope has a single finite knot at k2. ˆMβ,ces(Xn+1, y), which also depend on Xn+1 and are defined as the boundaries in the domain of y between each consecutive pair of steps, with the understanding that kβ 0 = −∞ and kβ L+1 = +∞; see Figure 5 for a schematic visualization. After computing ˆMβ,ces(Xn+1, y) in (15) for both βlow and βhigh, we concatenate the respective knots klow 1 , . . . , klow L1 , khigh 1 , . . . , khigh L2 and sort them into k1 ≤ k2 ≤ kL1+L2, so that within each interval Bl = (kl−1, kl] for step l ∈ [L1 +L2], there exist exactly one best model for βlow and exactly one best model for βhigh. Then, for each interval Bl = (kl−1, kl] associated with step l ∈ [L1 + L2], evaluate the nonconformity score ˆEi(Xn+1, Bl) for all i ∈ Des-cal, based on the regression model indicated by mβlow,l(Xn+1) and mβhigh,l(Xn+1); i.e., ˆEi(Xn+1, Bl) = max n ˆqmβlow,l(Xn+1)(Xi) − Yi, Yi − ˆqmβhigh,l(Xn+1)(Xi) o . (17) Let ˆQ1−α(Xn+1, Bl) denote the ⌈(1−α)(1+ |Des-cal|)⌉-th smallest value among all nonconformity scores ˆEi(Xn+1, Bl), assuming for simplicity that there are no ties; otherwise, ties can be broken at random. Then, define the interval ˆCα(Xn+1, Bl) as that obtained by applying the conformal prediction method of Romano et al. [31] with nonconformity scores (17) based on the estimated conditional quantiles ˆqmβlow,l(Xn+1)(Xn+1) and ˆqmβhigh,l(Xn+1)(Xn+1); that is, ˆCα(Xn+1, Bl) = [ˆqmβlow,l(Xn+1)(Xn+1) − ˆQ1−α(Xn+1, Bl), ˆqmβhigh,l(Xn+1)(Xn+1) + ˆQ1−α(Xn+1, Bl)]. (18) 13Finally, the output prediction interval ˆCα(Xn+1) is given by: ˆCα(Xn+1) = Convex \u0010 ∪L l=1{Bl ∩ ˆCα(Xn+1, Bl)} \u0011 . (19) This procedure, summarized in Algorithm 4, guarantees valid marginal coverage. Algorithm 4 Conformalized early stopping for quantile regression 1: Input: Exchangeable data points ( X1, Y1), . . . ,(Xn, Yn) with outcomes Yi ∈ R. 2: Input: Test point with features Xn+1. Desired coverage level 1 − α. 3: Input: Maximum number of training epochs tmax; storage period hyper-parameter τ. 4: Input: Trainable quantile regression model with target quantiles [ βlow, βhigh]. 5: Randomly split the exchangeable data points into Dtrain and Des-cal. 6: Train for tmax epochs and save the intermediate models Mβlow,t1, . . . , Mβlow,tT , Mβhigh,t1, . . . , Mβhigh,tT . 7: Evaluate ˆMβlow,ces(Xn+1, y) and ˆMβhigh,ces(Xn+1, y) as in (16), using Algorithm A10. 8: Partition the domain of Y into L1 + L2 intervals Bl, for l ∈ [L1 + L2], based on the knots of ˆMβlow,ces(Xn+1, y) and ˆMβhigh,ces(Xn+1, y). 9: for l ∈ [L1 + L2] do 10: Evaluate nonconformity scores ˆEi(Xn+1, Bl) for all i ∈ Des-cal as in (17). 11: Compute ˆQ1−α(Xn+1, Bl): the ⌈(1 − α)(1 +|Des-cal|)⌉-th smallest value among ˆEi(Xn+1, Bl). 12: Construct the interval ˆCα(Xn+1, Bl) according to (18). 13: end for 14: Output: Prediction interval ˆCα(Xn+1) given as a function of { ˆCα(Xn+1, Bl)}L l=1 by (19). Theorem 4. Assume (X1, Y1), . . . ,(Xn+1, Yn+1) are exchangeable, and let ˆCα(Xn+1) be the output of Algorithm 4, as given by (19), for any given α ∈ (0, 1). Then, P[Yn+1 ∈ ˆCα(Xn+1)] ≥ 1 − α. Similarly to Section 2.6, it is possible (although unlikely) that Algorithm 4 may sometimes produce an empty prediction set. Therefore, we present Algorithm A12 in Appendix A7, which extends Algorithm 4 in such a way as to explicitly avoid returning empty prediction intervals. As the intervals given by Algorithm A12 always contain those output by Algorithm 4, it follows from Theorem 4 that Algorithm A12 also enjoys guaranteed coverage; see Corollary A4. 2.8 Implementation Details and Computational Cost Beyond the cost of training the neural network (which is relatively expensive but does not need to be repeated for different test points) and the storage cost associated with saving the candidate models (which we have argued to be feasible in many applications), CES is quite computationally efficient. Firstly, CES treats all test points individually and could process them in parallel, although many operations do not need to be repeated. In particular, one can recycle the evaluation of the calibration loss across different test points; e.g., see (4). Thus, the model selection component can be easily implemented at cost O((nes-cal +ntest)·T +ntest ·nes-cal) for classification (of which outlier detection is a special case) and O((nes-cal + ntest) · T · log T + ntest · nes-cal) for regression, where nes-cal = |Des-cal| and T is the number of candidate models. Note that the T · log T dependence in the regression setting comes from the divide-and-conquer algorithm explained in Appendix A4. 14It is possible that the cost of CES may become a barrier in some applications, particularly if T is very large, despite the slightly more than linear scaling. Hence, we recommend employing moderate values of T (e.g., 100 or 1000). 3 Numerical Experiments 3.1 Outlier Detection The use of CES for outlier detection is demonstrated using the CIFAR10 data set [52], a collec- tion of 60,000 32-by-32 RGB images from 10 classes including common objects and animals. A convolutional neural network with ReLU activation functions is trained on a subset of the data to minimize the cross-entropy loss. The maximum number of epochs is set to be equal to 50. The trained classification model is then utilized to compute conformity scores for outlier detection with the convention that cats are inliers and the other classes are outliers. In particular, a nonconformity score for each Zn+1 is defined as 1 minus the output of the soft-max layer corresponding to the label “cat”. This can be interpreted as an estimated probability of Zn+1 being an outlier. After translating these scores into a conformal p-value ˆu0(Zn+1), the null hypothesis that Zn+1 is a cat is rejected if ˆu0(Zn+1) ≤ α = 0.1. The total number of samples utilized for training, early stopping, and conformal calibration is varied between 500 and 2000. In each case, CES is applied using 75% of the samples for training and 25% for early stopping and calibration. Note that the calibration step only utilizes inliers, while the other data subsets also contain outliers. The empirical performance of CES is measured in terms of the probability of falsely rejecting a true null hypothesis—the false positive rate (FPR)—and the probability of correctly rejecting a false null hypothesis—the true positive rate (TPR). The CES method is compared to three benchmarks. The first benchmark is naive early stopping with the best (hybrid) theoretical correction for the nominal coverage level described in Appendix A1.2. The second benchmark is early stopping based on data splitting, which utilizes 50% of the available samples for training, 25% for early stopping, and 25% for calibration. The third benchmark is full training without early stopping, which simply selects the model obtained after the last epoch. The test set consists of 100 independent test images, half of which are outliers. All results are averaged over 100 trials based on independent data subsets. FPR TPR 500 1000 2000 500 1000 2000 0.1 0.2 0.3 0.4 0.00 0.05 0.10 0.15 Sample size Method CES Naive + theory Data splitting Full training Figure 6: Average performance, as a function of the sample size, of conformal inferences for outlier detection based on neural networks trained and calibrated with different methods, on the CIFAR10 data [52]. Ideally, the TPR should be as large as possible while maintaining the FPR below 0.1. See Table A1 for more detailed results and standard errors. Figure 6 summarizes the performance of the four methods as a function of the total sample 15size; see Table A1 in Appendix A8 for the corresponding standard errors. All methods control the FPR below 10%, as expected, but CES achieves the highest TPR. The increased power of CES compared to data splitting is not surprising, as the latter relies on a less accurate model trained on less data. By contrast, the naive benchmark trains a model more similar to that of CES, but its TPR is not as high because the theoretical correction for the naive conformal p-values is overly pessimistic. Finally, full training is the least powerful competitor for large sample sizes because its underlying model becomes more and more overconfident as the training set grows. Note that Table A1 also includes the results obtained with the naive benchmark detailed in Appendix A1, applied without the theoretical correction necessary to guarantee marginal coverage. Remarkably, the results show that the naive benchmark performs similarly to the CES method, even though only the latter has the advantage of enjoying rigorous finite-sample guarantees. 3.2 Multi-class Classification The same CIFAR10 data [52] are utilized to demonstrate the performance of CES for a 10-class classification task. These experiments are conducted similarly to those in Section 3.1, with the difference that now the soft-max output of the convolutional neural network is translated into conformal prediction sets, as explained in Appendix A3, instead of conformal p-values. The CES method is compared to the same three benchmarks from Section 3.1. All prediction sets guarantee 90% marginal coverage, and their performances are evaluated based on average cardinality. Coverage (marginal) Cardinality 500 1000 2000 500 1000 2000 6 7 8 9 0.7 0.8 0.9 1.0 Sample size Method CES Naive + theory Data splitting Full training Figure 7: Average performance, as a function of the sample size, of conformal prediction sets for multi-class classification based on neural networks trained and calibrated with different methods, on the CIFAR10 data [52]. Ideally, the coverage should be close to 90% and the cardinality should be small. See Table A2 for more detailed results and standard errors. Figure 7 summarizes the results averaged over 100 independent realizations of these experiments, while Table A2 in Appendix A8 reports on the corresponding standard errors. While all approaches always achieve the nominal coverage level, the CES method is able to do so with the smallest, and hence most informative, prediction sets. As before, the more disappointing performance of the data splitting benchmark can be explained by the more limited amount of data available for training, that of the naive benchmark by the excessive conservativeness of its theoretical correction, and that of the full training benchmark by overfitting. Table A2 also includes the results obtained with the naive benchmark without the theoretical correction, which again performs similarly to CES. 3.3 Regression We now apply the CES method from Section 2.6 to the following 3 public-domain regression data sets from the UCI Machine Learning repository [53]: physicochemical properties of protein tertiary 16structure (bio) [44], hourly and daily counts of rental bikes ( bike) [54], and concrete compressive strength (concrete) [55]. These data sets were previously also considered by Romano et al. [31], to which we refer for further details. As in the previous sections, we compare CES to the usual three benchmarks: naive early stopping with the hybrid theoretical correction for the nominal coverage level, early stopping based on data splitting, and full model training without early stopping. All methods utilize the same neural network with two hidden layers of width 128 and ReLU activation functions, trained for up to 1000 epochs. The models are calibrated in such a way as to produce conformal prediction sets with guaranteed 90% marginal coverage for a test set of 100 independent data points. The total sample size available for training, early stopping and calibration is varied between 200 and 2000. These data are allocated for specific training, early-stopping, and calibration operations as in Sections 3.1–3.2. The performance of each method is measured in terms of marginal coverage, worst-slab conditional coverage [56]—estimated as described in Sesia and E. J. Cand` es [16]—and average width of the prediction intervals. All results are averaged over 100 independent experiments, each based on a different random sample from the original raw data sets. Figure 3 summarizes the performance of the four alternative methods on the bio data, as a function of the total sample size; see Table A12 in Appendix A8 for the corresponding standard errors. These results show that all methods reach 90% marginal coverage in practice, as anticipated by the mathematical guarantees, although the theoretical correction for the naive early stopping method appears to be overly conservative. The CES method clearly performs best, in the sense that it leads to the shortest prediction intervals while also achieving approximately valid conditional coverage. By contrast, the conformal prediction intervals obtained without early stopping have significantly lower conditional coverage, which is consistent with the prior intuition that fully trained neural networks can sometimes suffer from overfitting. More detailed results from these experiments can be found in Table A3 in Appendix A8. Analogous results corresponding to thebike and concrete data sets can be found in Figures A12–A13 and Tables A4–A5 in Appendix A8. Tables A3–A5 also include the results obtained with the naive benchmark applied without the necessary theoretical correction, which performs similarly to CES. Finally, it must be noted that the widths of the prediction intervals output by the CES method in these experiments are very similar to those of the corresponding intervals produced by naively applying early stopping without data splitting and without the theoretical correction described in Appendix A1. This naive approach was not taken as a benchmark because it does not guarantee valid coverage, unlike the other methods. Nonetheless, it is interesting to note that the rigorous theoretical properties of the CES method do not come at the expense of a significant loss of power compared to this very aggressive heuristic, and in this sense, one may say that the conformal inferences computed by CES are “almost free”. 3.4 Quantile Regression We apply the CES quantile regression method from Section 2.7 to the following publicly avail- able and commonly investigated regression data sets from the UCI Machine Learning repository [53]: medical expenditure panel survey number 21 ( MEPS 21) [57]; blog feedback ( blog data) [58]; Tennessee’s student teacher achievement ratio ( STAR) [59]; community and crimes ( community) [60]; physicochemical properties of protein tertiary structure ( bio) [44]; house sales in King County (homes) [61]; and hourly and daily counts of rental bikes (bike) [54]. These data sets were previously also considered by Romano et al. [31]. 17As in the previous sections, we compare CES to the usual three benchmarks, now implemented based on quantile regression: naive early stopping with the hybrid theoretical correction for the nominal coverage level, early stopping based on data splitting and full model training without early stopping. We follow the same model architecture and data preprocessing steps as in Romano et al. [31]. To be specific, the input features are standardized to have zero mean and unit variance, and the response values are rescaled by diving the absolute mean of the training responses. All methods utilize the same neural network with three hidden layers and ReLU activation functions between layers, trained for up to 2000 epochs. The parameters are trained minimizing the pinball loss function (see Appendix A6) with Adam optimizer [64], minibatches of size 25, 0 weight decay and dropout, and fixed learning rate (0.001 for STAR, homes, bike, and bio , 0.0001 for community, and 0.00005 for MEPS 21 and blog data). The models are calibrated in such a way as to produce conformal prediction sets with guaranteed 90% marginal coverage for a test set of 1000 independent data points. The total sample size available for training, early stopping and calibration is varied between 200 and 2000 (200 and 1000 for small data sets such as community and STAR). These data are allocated for specific training, early- stopping, and calibration operations as in Sections 3.1–3.2. Again, the performance of each method is measured in terms of marginal coverage, worst-slab conditional coverage [56], and average width of the prediction intervals. All results are averaged over 25 independent experiments, each based on a different random sample from the original raw data sets. Figure 8 summarizes the performance of the four alternative methods on the homes data, as a function of the total sample size; The error bar corresponding to standard errors are plotted around each data point. These results show that all methods reach 90% marginal coverage in practice, as anticipated by the mathematical guarantees, although the theoretical correction for the naive early stopping method appears to be overly conservative. Full training, though producing the smallest prediction bands, has very low conditional coverage, which indicates that fully trained neural network models can suffer from overfitting and therefore is not appealing. Data splitting method beats full training as it gives higher approximated conditional coverage, and CES further beats data splitting in terms of conditional coverage, meanwhile producing prediction intervals of similar length as data splitting. These patterns hold true in general for additional data sets, as illustrated by Figures A14–A19 and by Tables A6–A12. Tables A6–A12 also include the results obtained with the naive benchmark applied without the necessary theoretical correction, which performs similarly to CES. Coverage (marginal) Coverage (conditional) Width 200 500 1000 2000 200 500 1000 2000 200 500 1000 2000 1 2 3 0.7 0.8 1.0 0.7 0.8 1.0 Sample size Method CES Naive + theory Data splitting Full training Figure 8: Average performance, as a function of the sample size, of conformal prediction sets for quantile regression based on neural networks trained and calibrated with different methods, on the homes data [61]. The marginal coverage is theoretically guaranteed to be above 90%. Ideally, the conditional coverage should high and the prediction intervals should be tight. See Table A6 for additional details and standard errors. 184 Discussion This paper has focused on early stopping and conformal calibration because these are two popular techniques, respectively designed to mitigate overfitting and reduce overconfidence, that were pre- viously combined without much thought. However, the relevance of our methodology extends well beyond the problem considered in this paper. In fact, related ideas have already been utilized in the context of outlier detection to tune hyper-parameters and select the most promising candidate from an arbitrary toolbox of machine learning models [26]. The techniques developed in this paper also allow one to calibrate, without further data splits, the most promising model selected in a data- driven way from an arbitrary machine learning toolbox in the context of multi-class classification and regression. As mentioned in Section 2.2 and detailed in Appendix A1, the naive benchmark that uses the same hold-out data twice, both for standard early stopping and standard conformal calibration, is not theoretically valid without conservative corrections. Nonetheless, our numerical experiments have shown that this naive approach often performs similarly to CES in practice. Of course, the naive benchmark may sometimes fail, and thus we would advise practitioners to apply the theoretically principled CES whenever its additional memory costs are not prohibitive. However, the empirical evidence suggests the naive benchmark may not be a completely unreasonable heuristic when CES is not applicable. Software implementing the algorithms and data experiments are available online at https: //github.com/ZiyiLiang/Conformalized_early_stopping. Acknowledgements The authors thank the Center for Advanced Research Computing at the University of Southern California for providing computing resources to carry out numerical experiments. The authors are also grateful to three anonymous reviewers for their insightful comments and suggestions. M. S. and Y. Z. are supported by NSF grant DMS 2210637. M. S. is also supported by an Amazon Research Award. References [1] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, et al. “Mastering the game of Go with deep neural networks and tree search”. In: Nature 529.7587 (2016), pp. 484–489. [2] C. Guo, G. Pleiss, Y. Sun, and K. Q. Weinberger. “On calibration of modern neural networks”. In: Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org. 2017, pp. 1321–1330. [3] S. Thulasidasan, G. Chennupati, J. A. Bilmes, T. Bhattacharya, and S. Michalak. “On mixup training: Improved calibration and predictive uncertainty for deep neural networks”. In: Adv. Neural. Inf. Process. Syst. 2019, pp. 13888–13899. [4] Y. Ovadia, E. Fertig, J. Ren, Z. Nado, D. Sculley, S. Nowozin, J. Dillon, B. Lakshminarayanan, and J. Snoek. “Can you trust your model’s uncertainty? evaluating predictive uncertainty under dataset shift”. In: Advances in Neural Information Processing Systems 32 (2019). 19[5] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. “Dropout: A Simple Way to Prevent Neural Networks from Overfitting”. In: J. Mach. Learn. Res. 15 (2014), pp. 1929–1958. [6] S. Ioffe and C. Szegedy. “Batch normalization: Accelerating deep network training by reducing internal covariate shift”. In: International conference on machine learning . PMLR. 2015, pp. 448–456. [7] T. Salimans and D. P. Kingma. “Weight normalization: A simple reparameterization to ac- celerate training of deep neural networks”. In: Advances in neural information processing systems 29 (2016). [8] C. Shorten and T. M. Khoshgoftaar. “A survey on image data augmentation for deep learn- ing”. In: Journal of Big Data 6.1 (2019), pp. 1–48. [9] L. Prechelt. “Automatic early stopping using cross validation: quantifying the criteria”. In: Neural networks 11.4 (1998), pp. 761–767. [10] V. Vovk, A. Gammerman, and G. Shafer. Algorithmic learning in a random world . Springer, 2005. [11] J. Smith, I. Nouretdinov, R. Craddock, C. Offer, and A. Gammerman. “Conformal anomaly detection of trajectories with a multi-class hierarchy”. In: International symposium on sta- tistical learning and data sciences . Springer. 2015, pp. 281–290. [12] V. Vovk, D. Lindsay, I. Nouretdinov, and A. Gammerman. Mondrian Confidence Machine . Technical Report. On-line Compression Modelling project. Royal Holloway, University of London, 2003. [13] Y. Romano, M. Sesia, and E. J. Cand` es. “Classification with Valid and Adaptive Coverage”. In: Advances in Neural Information Processing Systems 33 (2020). [14] G. Marcus. “Deep learning: A critical appraisal”. In: arXiv preprint arXiv:1801.00631 (2018). [15] V. Vovk. “Conditional Validity of Inductive Conformal Predictors”. In: Proceedings of the Asian Conference on Machine Learning . Vol. 25. 2012, pp. 475–490. [16] M. Sesia and E. J. Cand` es. “A comparison of some conformal quantile regression methods”. In: Stat 9.1 (2020). [17] Y. Romano, R. F. Barber, C. Sabatti, and E. Cand` es. “With Malice Toward None: Assessing Uncertainty via Equalized Coverage”. In: Harvard Data Science Review (2020). [18] R. F. Barber, E. J. Cand` es, A. Ramdas, and R. J. Tibshirani. “The limits of distribution-free conditional predictive inference”. In: Information and Inference 10.2 (2021), pp. 455–482. [19] C. Saunders, A. Gammerman, and V. Vovk. “Transduction with confidence and credibility”. In: IJCAI. 1999. [20] V. Vovk, A. Gammerman, and C. Saunders. “Machine-learning applications of algorithmic randomness”. In: International Conference on Machine Learning . 1999, pp. 444–453. [21] J. Lei, J. Robins, and L. Wasserman. “Distribution-Free Prediction Sets”. In: J. Am. Stat. Assoc. 108.501 (2013), pp. 278–287. [22] J. Lei and L. Wasserman. “Distribution-free prediction bands for non-parametric regression”. In: J. R. Stat. Soc. (B) 76.1 (2014), pp. 71–96. [23] J. Lei, M. G’Sell, A. Rinaldo, R. J. Tibshirani, and L. Wasserman. “Distribution-free predic- tive inference for regression”. In: J. Am. Stat. Assoc. 113.523 (2018), pp. 1094–1111. 20[24] R. F. Barber, E. J. Cand` es, A. Ramdas, R. J. Tibshirani, et al. “Predictive inference with the jackknife+”. In: Ann. Stat. 49.1 (2021), pp. 486–507. [25] L. Guan and R. Tibshirani. “Prediction and outlier detection in classification problems”. In: J. R. Stat. Soc. (B) 84.2 (2022), pp. 524–546. [26] Z. Liang, M. Sesia, and W. Sun. “Integrative conformal p-values for powerful out-of-distribution testing with labeled outliers”. In: arXiv preprint arXiv:2208.11111 (2022). [27] S. Bates, E. Cand` es, L. Lei, Y. Romano, and M. Sesia. “Testing for outliers with conformal p-values”. In: Ann. Stat. 51.1 (2023), pp. 149–178. [28] Y. Hechtlinger, B. P´ oczos, and L. Wasserman. Cautious Deep Learning . arXiv:1805.09460. 2018. [29] A. N. Angelopoulos, S. Bates, M. Jordan, and J. Malik. “Uncertainty Sets for Image Classifiers using Conformal Prediction”. In:International Conference on Learning Representations. 2021. [30] S. Bates, A. Angelopoulos, L. Lei, J. Malik, and M. Jordan. “Distribution-free, risk-controlling prediction sets”. In: Journal of the ACM (JACM) 68.6 (2021), pp. 1–34. [31] Y. Romano, E. Patterson, and E. J. Cand` es. “Conformalized quantile regression”. In: Ad- vances in Neural Information Processing Systems . 2019, pp. 3538–3548. [32] R. J. Tibshirani, R. Foygel Barber, E. Candes, and A. Ramdas. “Conformal prediction under covariate shift”. In: Advances in neural information processing systems 32 (2019). [33] M. Sesia, S. Favaro, and E. Dobriban. “Conformal Frequency Estimation with Sketched Data under Relaxed Exchangeability”. In: arXiv preprint arXiv:2211.04612 (2022). [34] R. F. Barber, E. J. Cand` es, A. Ramdas, and R. J. Tibshirani. “Conformal prediction beyond exchangeability”. In: arXiv preprint arXiv:2202.13415 (2022). [35] I. Gibbs and E. Cand` es. “Conformal inference for online prediction with arbitrary distribution shifts”. In: arXiv preprint arXiv:2208.08401 (2022). [36] N. Colombo and V. Vovk. “Training conformal predictors”. In: Conformal and Probabilistic Prediction and Applications. PMLR. 2020, pp. 55–64. [37] A. Bellotti. “Optimized conformal classification using gradient descent approximation”. In: arXiv preprint arXiv:2105.11255 (2021). [38] D. Stutz, K. Dvijotham, A. T. Cemgil, and A. Doucet. “Learning Optimal Conformal Clas- sifiers”. In: arXiv preprint arXiv:2110.09192 (2021). [39] B.-S. Einbinder, Y. Romano, M. Sesia, and Y. Zhou. “Training Uncertainty-Aware Classifiers with Conformalized Deep Learning”. In: Adv. Neural Inf. Process. Syst. Vol. 35. 2022. [40] Y. Yang and A. K. Kuchibhotla. “Finite-sample efficient conformal prediction”. In: arXiv preprint arXiv:2104.13871 (2021). [41] A. Marandon, L. Lei, D. Mary, and E. Roquain. “Machine learning meets false discovery rate”. In: arXiv preprint arXiv:2208.06685 (2022). [42] Y. Benjamini and Y. Hochberg. “Controlling the false discovery rate: a practical and powerful approach to multiple testing”. In: J. R. Stat. Soc. (B) 57.1 (1995), pp. 289–300. [43] B. Kim, C. Xu, and R. Barber. “Predictive inference is free with the jackknife+-after- bootstrap”. In: Advances in Neural Information Processing Systems 33 (2020), pp. 4138– 4149. 21[44] Physicochemical properties of protein tertiary structure data set . https://archive.ics. uci.edu/ml/datasets/Physicochemical+Properties+of+Protein+Tertiary+Structure . Accessed: July, 2019. [45] J. Lei, M. G’Sell, A. Rinaldo, R. J. Tibshirani, and L. Wasserman. “Distribution-free predic- tive inference for regression”. In: J. Am. Stat. Assoc. 113.523 (2018), pp. 1094–1111. [46] V. Chernozhukov, K. W¨ uthrich, and Y. Zhu. “Distributional conformal prediction”. In: Pro- ceedings of the National Academy of Sciences 118.48 (2021), e2107794118. [47] R. Izbicki, G. Shimizu, and R. Stern. “Flexible distribution-free conditional predictive bands using density estimators”. In: International Conference on Artificial Intelligence and Statis- tics. PMLR. 2020, pp. 3068–3077. [48] M. Sesia and Y. Romano. “Conformal Prediction using Conditional Histograms”. In: Advances in Neural Information Processing Systems 34 (2021). [49] O. Devillers and M. J. Golin. “Incremental algorithms for finding the convex hulls of cir- cles and the lower envelopes of parabolas”. In: Information Processing Letters 56.3 (1995), pp. 157–164. [50] F. Nielsen and M. Yvinec. “An output-sensitive convex hull algorithm for planar objects”. In: International Journal of Computational Geometry & Applications 8.01 (1998), pp. 39–65. [51] J. W. Taylor. “A quantile regression neural network approach to estimating the conditional density of multiperiod returns”. In: Journal of Forecasting 19.4 (2000), pp. 299–311. [52] A. Krizhevsky. Learning Multiple Layers of Features from Tiny Images . 2009. [53] H. K. Pınar T¨ ufekci. UCI Machine Learning Repository. 2012. [54] Bike sharing dataset . https : / / archive . ics . uci . edu / ml / datasets / bike + sharing + dataset. Accessed: July, 2019. [55] Concrete compressive strength data set . http://archive.ics.uci.edu/ml/datasets/ concrete+compressive+strength. Accessed: July, 2019. [56] M. Cauchois, S. Gupta, and J. C. Duchi. “Knowing what You Know: valid and validated confidence sets in multiclass and multilabel prediction.” In: J. Mach. Learn. Res. 22 (2021), pp. 81–1. [57] Medical Expenditure Panel Survey, Panel 21 . https://meps.ahrq.gov/mepsweb/data_ stats/download_data_files_detail.jsp?cboPufNumber=HC- 192 . Accessed: January, 2019. [58] BlogFeedback dataset. https://github.com/xinbinhuang/feature-selection_blogfeedback. Accessed: Mar, 2023. [59] C. Achilles, H. P. Bain, F. Bellott, J. Boyd-Zaharias, J. Finn, J. Folger, J. Johnston, and E. Word. Tennessee’s Student Teacher Achievement Ratio (STAR) project. Version V1. 2008. [60] Communities and crime dataset . https://github.com/vbordalo/Communities- Crime . Accessed: Mar, 2023. [61] House prices from King County dataset. https://www.kaggle.com/datasets/shivachandel/ kc-house-data?select=kc_house_data.csv. Accessed: Mar, 2023. [62] R. Koenker and G. Bassett. “Regression Quantiles”. In: Econometrica 46.1 (1978), pp. 33–50. [63] I. Steinwart and A. Christmann. “Estimating conditional quantiles with the help of the pinball loss”. In: Bernoulli 17.1 (2011), pp. 211–225. 22[64] D. P. Kingma and J. Ba. “Adam: A method for stochastic optimization”. In: preprint at arXiv:1412.6980 (2014). [65] M. Abramowitz and I. A. Stegun. Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables. ninth Dover printing, tenth GPO printing. Dover, 1964. 23A1 Naive Early Stopping Benchmarks A1.1 Detailed Implementation of the Naive Benchmarks We detail here the implementation of the naive benchmark discussed in Section 2.2. This approach can serve as an informative benchmark and it becomes useful in Appendix A5 to extend our rigorous conformalized early stopping method for regression problems in such as way as to explicitly avoid returning empty prediction intervals. For completeness, we present the implementation of the naive benchmark separately for outlier detection, multi-class classification, and regression, respectively in Algorithms A5, A6 and A7. Note that Algorithm A6 also allows for the possibility of computing prediction sets seeking (approximate) marginal coverage instead of (approximate) label-conditional coverage for multi-class classification problems; see Appendix A2 for further details on multi-class classification with marginal coverage. Algorithm A5 Naive conformal outlier detection benchmark with greedy early stopping 1: Input: Exchangeable data points Z1, . . . , Zn; test point Zn+1. 2: Input: Maximum number of training epochs tmax; storage period hyper-parameter τ. 3: Input: One-class classifier trainable via (stochastic) gradient descent. 4: Randomly split the exchangeable data points into Dtrain and Des-cal. 5: Train the one-class classifier for tmax epochs and save the intermediate models Mt1, . . . , MtT . 6: Pick the most promising model t∗ ∈ [T] minimizing Les-cal(Mt) in (1), based on Des-cal. 7: Compute nonconformity scores ˆSi(Zn+1) for all i ∈ Des-cal ∪ {n + 1} using model t∗. 8: Output: Naive conformal p-value ˆunaive 0 (Zn+1) given by (3). 24Algorithm A6 Naive conformal multi-class classification benchmark with greedy early stopping 1: Input: Exchangeable data points ( X1, Y1), . . . ,(Xn, Yn) with labels Yi ∈ [K]. 2: Input: Test point with features Xn+1. Desired coverage level 1 − α. 3: Input: Maximum number of training epochs tmax; storage period hyper-parameter τ. 4: Input: K-class classifier trainable via (stochastic) gradient descent. 5: Randomly split the exchangeable data points into Dtrain and Des-cal. 6: Train the K-class classifier for tmax epochs and save the intermediate models Mt1, . . . , MtT . 7: Pick the most promising model t∗ ∈ [T] minimizing Les-cal(Mt) in (4), based on Des-cal. 8: for y ∈ [K] do 9: if Label-conditional coverage is desired then 10: Define Dy es-cal = {i ∈ Des-cal : Yi = y}. 11: Compute scores ˆSy i (Xn+1) for all i ∈ Dy es-cal ∪ {n + 1} using model t∗; see Appendix A3. 12: Compute the naive conformal p-value ˆunaive y (Xn+1) according to (6). 13: else 14: Compute scores ˆSy i (Xn+1) for all i ∈ Des-cal ∪ {n + 1} using model t∗; see Appendix A3. 15: Compute the naive conformal p-value ˆunaive y (Xn+1) according to ˆunaive y (Xn+1) = 1 + |i ∈ Des-cal : ˆSy i (Xn+1) ≤ ˆSy n+1(Xn+1)| 1 + |Des-cal| . 16: end if 17: end for 18: Output: Naive prediction set ˆCnaive α (Xn+1) given by (7). Algorithm A7 Naive conformal regression benchmark with greedy early stopping 1: Input: Exchangeable data points ( X1, Y1), . . . ,(Xn, Yn) with outcomes Yi ∈ R. 2: Input: Test point with features Xn+1. Desired coverage level 1 − α. 3: Input: Maximum number of training epochs tmax; storage period hyper-parameter τ. 4: Input: Regression model trainable via (stochastic) gradient descent. 5: Randomly split the exchangeable data points into Dtrain and Des-cal. 6: Train the regression model for tmax epochs and save the intermediate models Mt1, . . . , MtT . 7: Pick the most promising model t∗ ∈ [T] minimizing Les-cal(Mt) in (8). 8: Evaluate nonconformity scores ˆSi(Xn+1) = |Yi − ˆµt∗(Xi)| for all i ∈ Des-cal. 9: Compute ˆQ1−α(Xn+1) = ⌈(1 − α)(1 + |Des-cal|)⌉-th smallest value in ˆSi(Xn+1) for i ∈ Des-cal. 10: Output: Prediction interval ˆCnaive α (Xn+1) = ˆµt∗(Xn+1) ± ˆQ1−α(Xn+1). A1.2 Theoretical Analysis of the Naive Benchmark Although the naive benchmarks described above often perform similarly to CES in practice, they do not enjoy the same desirable theoretical guarantees. Nonetheless, we can study their behaviour in sufficient detail as to prove that their inferences are too far from being valid. Unfortunately, as demonstrated in Section 3, these theoretical results are still not tight enough to be very useful in practice. For simplicity, we will begin by focusing on outlier detection. Review of existing results based on the DKW inequality. Yang and Kuchibhotla [40] have recently studied the finite-sample coverage rate of a conformal 25prediction interval formed by naively calibrating a model selected among T possible candidates based on its performance on the calibration data set itself, which we denote by Des-cal. Although Yang and Kuchibhotla [40] focus on conformal prediction intervals, here we find it easier to explain their ideas in the context of conformal p-values for outlier detection. Let ˆSi(Zn+1; t), for all i ∈ Des-cal and t ∈ [T], denote the nonconformity scores corresponding to model t, and denote the ⌊α(1 +|Des-cal|)⌋-th smallest value in ˆSi(Xn+1; t) as ˆQα(Zn+1; t). Let t∗ indicate the selected model. As we are interested in constructing a conformal p-value ˆunaive 0 (Zn+1), the goal is to bound from above the tail probability P \u0000 ˆunaive 0 (Zn+1) > α \u0001 = E h P \u0010 ˆSi(Xn+1; t∗) > ˆQα(Zn+1; t∗) | Des-cal \u0011i . (A20) Intuitively, if nes-cal = |Des-cal| is sufficiently large, the conditional probability inside the expected value on the right-hand-side above can be well-approximated by the following empirical quantity: 1 n X i∈Des-cal 1 n ˆSi(Xn+1; t∗) > ˆQα(Zn+1; t∗) o = ⌈(1 + nes-cal)(1 − α)⌉ nes-cal ≥ \u0012 1 + 1 nes-cal \u0013 (1 − α). The quality of this approximation in finite samples can be bound by the DKW inequality, which holds for any ε ≥ 0: P  sup s∈R \f\f\f\f\f\f 1 nes-cal X i∈Des-cal 1 n ˆSi(Xn+1; t∗) > s o − P \u0010 ˆSi(Xn+1; t∗) > s| Des-cal \u0011 \f\f\f\f\f\f > ε   ≤ 2e−2nes-calε2 . (A21) Starting from this, Theorem 1 in Yang and Kuchibhotla [40] shows that P(ˆunaive 0 (Zn+1) > α) ≥ \u0012 1 + 1 nes-cal \u0013 (1 − α) − p log(2T)/2 + c(T)√nes-cal , (A22) where c(T) is a constant that can be computed explicitly and is generally smaller than 1 /3. Intu- itively, the [ p log(2T)/2 +c(T)]/√nes-cal term above can be interpreted as the worst-case approxi- mation error among all possible models t ∈ [T]. One limitation with this result is that is gives a worst-case correction that does not depend on the chosen level α, and one would intuitively expect this bound to be tighter for α = 1/2 and overly conservative for the small α values (e.g., α = 0.1) that are typically interesting in practice. (This intuition will be confirmed empirically in Figure A10.) This observation motivates the following alternative analysis, which can often give tighter results. Alternative probabilistic bound based on Markov’s inequality. Define Wt = P \u0002 ˆunaive 0 (Zn+1; t) > α| Des-cal \u0003 . Lemma 3 in Vovk [15] tells us that Wt follows a Beta distribution, assuming exchangeability among Des-cal and the test point. That is, Wt ∼ Beta(nes-cal + 1 − l, l), l = ⌊α(nes-cal + 1)⌋. In the following, we will denote the corresponding inverse Beta cumulative distribution function as I−1(x; nes-cal + 1 − l, l). This result can be used to derive an alternative upper bound for P(ˆunaive 0 (Zn+1) > α) using the Markov’s inequality. 26Proposition A1. Assume Z1, . . . , Zn, Zn+1 are exchangeable random samples, and let ˆunaive 0 (Zn+1) be the output of Algorithm A5, for any given α ∈ (0, 1). Then, for any fixed α ∈ (0, 1) and any b >1, letting l = ⌊α(nes-cal + 1)⌋, P \u0002 ˆunaive 0 (Zn+1) > α \u0003 ≥ I−1 \u0012 1 bT ; nes-cal + 1 − l, l \u0013 · (1 − 1/b). Note that this bound depends on α in a more complex way compared to that of Yang and Kuchibhotla [40]. However, its asymptotic behaviour in the large-T limit remains similar, as shown below. Lemma A1. Denote I−1(x; nes-cal + 1 − l, l) as the inverse Beta cumulative distribution function. For any fixed b >1 and α ∈ (0, 1) , letting l = ⌊α(nes-cal + 1)⌋, for sufficiently large T and nes-cal, we have: I−1 \u0012 1 bT ; nes-cal + 1 − l, l \u0013 = (1 − α) − s α(1 − α) nes-cal + 1 · p 2 log(bT) + O   1p nes-cal log(T) ! . In simpler terms, Lemma A1 implies that the coverage lower bound in Proposition A1 is ap- proximately equal to  (1 − α) − s α(1 − α) nes-cal + 1 · p 2 log(bT)   · \u0012 1 − 1 b \u0013 , which displays an asymptotic behaviour similar to that of the bound from Yang and Kuchibhotla [40]. Further, the Markov bound is easy to compute numerically and often turns out to be tighter as long as b is moderately large (e.g., b = 100), as we shall see below. Naturally, the same idea can also be applied to bound the coverage of naive conformal prediction sets or intervals output by Algorithm A6 or Algorithm A7, respectively. Corollary A1. Assume (X1, Y1), . . . ,(Xn, Yn), (Xn+1, Yn+1) are exchangeable random sample, and let ˆCnaive α (Xn+1) be the output of Algorithm A6, for any given α ∈ (0, 1). Then, for any b > 1, letting l = ⌊α(nes-cal + 1)⌋, P h Yn+1 ∈ ˆCnaive α (Xn+1) i ≥ I−1 \u0012 1 bT ; nes-cal + 1 − l, l \u0013 · (1 − 1/b). Corollary A2. Assume (X1, Y1), . . . ,(Xn, Yn), (Xn+1, Yn+1) are exchangeable random samples, and let ˆCnaive α (Xn+1) be the output of Algorithm A7, for any α ∈ (0, 1). Then, for any b > 1, letting l = ⌊α(nes-cal + 1)⌋, P h Yn+1 ∈ ˆCnaive α (Xn+1) i ≥ I−1 \u0012 1 bT ; nes-cal + 1 − l, l \u0013 · (1 − 1/b). Hybrid probabilistic bound. Since neither the DKW nor the Markov bound described above always dominate the other for all possible combinations of T, nes-cal, and α, it makes sense to combine them to obtain a uniformly tighter hybrid bound. For any fixed b >1 and any T, nes-cal, and α, let M(T, nes-cal, α) = I−1 (1/bT; nes-cal + 1 − l, l) · (1 − 1/b) denote the Markov bound and D(T, nes-cal, α) = (1 + 1/(nes-cal)) (1− α) − ( p log(2T)/2 +c(T))/√nes-cal denote the DKW bound, 27Figure A9: Numerical comparison of different theoretical lower bounds for the marginal coverage of conformal prediction sets computed with a naive early stopping benchmark (e.g., Algorithm A6). Left: lower bounds for the marginal coverage as a function of the number of candidate models T, when α = 0.1 and nes-cal = 8000. Right: lower bounds for the marginal coverage as a function of the number of hold-out data points, nes-cal, when α = 0.1 and T = 100. Higher values correspond to tighter bounds. define H(T, nes-cal, α) as H(T, nes-cal, α) = max {M(T, nes-cal, α), D(T, nes-cal, α)}. It then follows immediately from Yang and Kuchibhotla [40] and Proposition A1 that, under the same conditions of Proposition A1, for any fixed b >1, P \u0002 ˆunaive 0 (Zn+1) > α \u0003 ≥ H(T, nes-cal, α). Of course, the same argument can also be utilized to tighten the results of Corollaries A1–A2. Numerical comparison of different probabilistic bounds. Figure A9 compares the three probabilistic bounds described above ( DKW, Markov, and hybrid) as a function of the number of candidate models T and of the number of hold-out data points nes-cal, in the case of α = 0.1. For simplicity, the Markov and hybrid bounds are evaluated by setting b = 100, which may not be the optimal choice but appears to work reasonably well. These results show that Markov bound tends to be tighter than the DKW bound for large values of T and for small values of nes-cal, while the hybrid bound generally achieves the best of both worlds. Lastly, Figure A10 demonstrates that the Markov bound tends to be tighter when α is small. The Markov and hybrid bounds here are also evaluated using b = 100. A2 Classification with Marginal Coverage The conformalized early stopping method presented in Section 2.5 can be easily modified to produce prediction sets with marginal rather than label-conditional coverage, as outlined in Algorithm A8. The difference between Algorithm 2 and Algorithm A8 is that the latter utilizes all calibration data in Des-cal to compute each conformal p-value ˆuy(Xn+1), not only the samples with true label y. An advantage of this approach is that conformal p-values based on a larger calibration samples are less aleatoric [27] and require less conservative finite-sample corrections (i.e., the “+1” term the 28Figure A10: Numerical comparison of different theoretical lower bounds for the marginal coverage of conformal prediction sets computed with a naive early stopping benchmark (e.g., Algorithm A6), as a function of the nominal significance level α. Left: lower bounds for the marginal coverage as a function of α, when T = 1000 and nes-cal = 1000. Right: theoretically corrected significance level necessary needed to achieve the marginal coverage guarantees expected at the nominal α level, as a function of α when T = 1000 and nes-cal = 1000. The dashed grey lines indicate the ideal values corresponding to standard conformal inferences based on calibration data that are independent of those used for early stopping. Higher values correspond to tighter bounds. numerator of the p-value formula becomes more negligible as the calibration set size increases). In turn, this tends to lead to smaller prediction sets with potentially more stable coverage conditional on the calibration data [16, 27] Of course, the downside of these prediction sets is that they can only be guaranteed to provide marginal coverage, although they can sometimes also perform well empirically in terms of label-conditional coverage [13]. Theorem A5. Assume (X1, Y1), . . . ,(Xn, Yn), (Xn+1, Yn+1) are exchangeable random samples, and let ˆCα(Xn+1) be the output of Algorithm A8, for any α ∈ (0, 1). Then, P[Yn+1 ∈ ˆCα(Xn+1)] ≥ 1−α. 29Algorithm A8 Conformalized early stopping for multi-class classification with marginal coverage 1: Input: Exchangeable data points ( X1, Y1), . . . ,(Xn, Yn) with labels Yi ∈ [K]. 2: Input: Test point with features Xn+1. Desired coverage level 1 − α. 3: Input: Maximum number of training epochs tmax; storage period hyper-parameter τ. 4: Input: K-class classifier trainable via (stochastic) gradient descent. 5: Randomly split the exchangeable data points into Dtrain and Des-cal. 6: Train the K-class classifier for tmax epochs and save the intermediate models Mt1, . . . , MtT . 7: for y ∈ [K] do 8: Imagine Yn+1 = y. 9: Pick the model ˆMces(Xn+1, y) according to (5), using the data in Des-cal ∪ {n + 1}. 10: Compute scores ˆSi(Xn+1, y) for all i ∈ Des-cal ∪{n+1} using ˆMces(Xn+1, y); see Appendix A3. 11: Compute the conformal p-value ˆumarg y (Xn+1) according to ˆumarg y (Xn+1) = 1 + |i ∈ Des-cal : ˆSy i (Xn+1) ≤ ˆSy n+1(Xn+1)| 1 + |Des-cal| . (A23) 12: end for 13: Output: Prediction set ˆCα(Xn+1) given by (7), with ˆumarg y (Xn+1) instead of ˆuy(Xn+1). A3 Review of Nonconformity Scores for Classification This section reviews the relevant background on the adaptive nonconformity scores for classification developed by Romano et al. [13]. For any x ∈ Xand y ∈ [K], let ˆπy(x) denote any (possibly very inaccurate) estimate of the true P[Y = y | X = x] corresponding to the unknown data-generating distribution. Concretely, a typical choice of ˆπ may be given by the output of the final softmax layer of a neural network classifier, for example. For any x ∈ Xand τ ∈ [0, 1], define the generalized conditional quantile function L, with input x, ˆπ, τ, as: L(x; ˆπ, τ) = min{k ∈ [K] : ˆ π(1)(x) + ˆπ(2)(x) + . . .+ ˆπ(k)(x) ≥ τ}, (A24) where ˆπ(1)(x) ≤ ˆπ(2)(x) ≤ . . .ˆπ(K)(x) are the order statistics of ˆπ1(x) ≤ ˆπ2(x) ≤ . . .ˆπK(x). Intu- itively, L(x; ˆπ, τ) gives the size of the smallest possible subset of labels whose cumulative probability mass according to ˆπ is at least τ. Define also a function S with input x, u ∈ (0, 1), ˆπ, and τ that computes the set of most likely labels up to (but possibly excluding) the one identified byL(x; ˆπ, τ): S(x, u; ˆπ, τ) = ( ‘y’ indices of the L(x; ˆπ, τ) − 1 largest ˆπy(x), if u ≤ V (x; ˆπ, τ), ‘y’ indices of the L(x; ˆπ, τ) largest ˆπy(x), otherwise, (A25) where V (x; ˆπ, τ) = 1 ˆπ(L(x;ˆπ,τ))(x)   L(x;ˆπ,τ)X k=1 ˆπ(k)(x) − τ  . 30Then, define the generalized inverse quantile nonconformity score function s, with input x, y, u; ˆπ, as: s(x, y, u; ˆπ) = min {τ ∈ [0, 1] : y ∈ S(x, u; ˆπ, τ)}. (A26) Intuitively, s(x, y, u; ˆπ) is the smallest value of τ for which the set S(x, u; ˆπ, τ) contains the label y. Finally, the nonconformity score for a data point ( Xi, Yi) is given by: ˆSi = s(Xi, Yi, Ui; ˆπ), (A27) where Ui is a uniform random variable independent of anything else. Note that this can also be equivalently written more explicitly as: ˆSi = ˆπ(1)(Xi) + ˆπ(2)(Xi) + . . .+ ˆπ(r(Yi,ˆπ(Xi)))(Xi) − Ui · ˆπ(r(Yi,ˆπ(Xi)))(Xi), (A28) where r(Yi, ˆπ(Xi)) is the rank of Yi among the possible labels y ∈ [K] based on ˆπy(Xi), so that r(y, ˆπ(Xi)) = 1 if ˆπy(Xi) = ˆπ(1)(Xi). The idea motivating this construction is that the nonconfor- mity score ˆSi defined above is guaranteed to be uniformly distributed on [0 , 1] conditional on X if the model ˆπ estimates the true unknown P[Y = y | X = x] accurately for all x ∈ X. This is a desirable property in conformal inference because it leads to statistically efficient prediction sets that can often achieve relatively high feature-conditional coverage in practice, even if the true data- generating distribution is such that some observations are much noisier than others; see Romano et al. [13] for further details. Finally, we conclude this appendix by noting that the nonconformity scores in Section 2.5 are written as ˆSi(Xn+1, y), instead of the more compact notation ˆSi adopted here, simply to emphasize that they are computed based on class probabilities ˆ π estimated by a data-driven model ˆM that depends on the test features Xn+1 as well as on the placeholder label y for Yn+1. A4 Efficient Computation of the Lower Envelope This section explains how to implement a computationally efficient divide-and-conquer algorithm for finding the lower envelope of a family of T parabolas or a family of shifted pinball loss functions at cost O(T log T) [49, 50]. This solution, outlined in Algorithm A9 and Algorithm A10, is useful to implement the proposed CES method for regression problems, as detailed in Algorithm 3 and Algorithm 4. 31Algorithm A9 Divide-and-conquer algorithm for finding the lower envelope of many parabolas 1: Input: A set of parabolas L = {l1, l2, . . . , lT } of forms li = aix2 + bix + ci for i = 1, . . . , T. 2: Randomly split L into two subsets. Repeat splitting until each subset only contains one parabola or is empty. 3: For each subset with only one parabola, set the parabola itself as the lower envelope and set the initial breakpoint list to [ −∞, +∞]. 4: for each interval constructed by adjacent breakpoints do 5: Within the interval, identify the two parabolas contributing to the previous lower envelopes, denoted as P1, P2. 6: Evaluate P1 and P2 at the current interval endpoints. 7: Calculate the intersection point p of P1 and P2. There exists at most one such p because ai = 1, ∀i, by (8). 8: if p not exists or p exists but lies outside the current interval then 9: Set the new lower envelope as the parabola with smaller values computed at the interval endpoints. 10: else 11: Add p as a breakpoint. 12: Within the current interval, set the new lower envelope below and above p based on eval- uations of the parabolas at the interval endpoints. 13: end if 14: Update and sort the breakpoint list and update the new lower envelope. 15: end for 16: Recursively merge two lower envelopes to form a new lower envelope by repeating Lines 4–15. 17: Output: A sorted dictionary of breakpoints and parabola indices characterizing the lower envelope of L. 32Algorithm A10 Divide-and-conquer algorithm for finding the lower envelope of many pinball loss functions 1: Input: A set of shifted pinball loss functions L = {l1, l2, . . . , lT } of forms li = ci + ρβ(y, ˆy) for i = 1, . . . , T. 2: Randomly split L into two subsets. Repeat splitting until each subset only contains one pinball loss function or is empty. 3: For each subset with only one pinball loss function, set the function itself as the lower envelope and set the initial breakpoint list to [ −∞, +∞]. 4: for each interval constructed by adjacent breakpoints do 5: Within the interval, identify the two pinball loss functions contributing to the previous lower envelopes; i.e., P1, P2. 6: Evaluate P1 and P2 at the current interval endpoints. 7: Calculate the intersection point p of P1 and P2. There exists at most one such p because β is the same ∀i, by (14). 8: if p not exists or p exists but lies outside the current interval then 9: Set the new lower envelope as the pinball loss function with smaller values computed at the interval endpoints. 10: else 11: Add p as a breakpoint. 12: Within the current interval, set the new lower envelope below and above p based on eval- uations of the pinball loss functions at the interval endpoints. 13: end if 14: Update and sort the breakpoint list and update the new lower envelope. 15: end for 16: Recursively merge two lower envelopes to form a new lower envelope by repeating Lines 4–15. 17: Output: A sorted dictionary of breakpoints and pinball loss function indices characterizing the lower envelope of L. A5 Avoiding Empty Predictions in CES for Regression This section presents Algorithm A11, which extends Algorithm 3 from Section 2.6 in such a way as to explicitly avoid returning empty prediction intervals. Algorithm A11 Conformalized early stopping for regression, avoiding empty predictions 1: Input: Exchangeable data points ( X1, Y1), . . . ,(Xn, Yn) with outcomes Yi ∈ R. 2: Input: Test point with features Xn+1. Desired coverage level 1 − α. 3: Input: Maximum number of training epochs tmax; storage period hyper-parameter τ. 4: Input: Regression model trainable via (stochastic) gradient descent. 5: Randomly split the exchangeable data points into Dtrain and Des-cal. 6: Train the regression model for tmax epochs and save the intermediate models Mt1, . . . , MtT . 7: Evaluate ˆCα(Xn+1) using Algorithm 3. 8: if ˆCα(Xn+1) = ∅ then 9: Evaluate ˆCnaive α (Xn+1) using Algorithm A7. Set ˆCα(Xn+1) = ˆCnaive α (Xn+1). 10: end if 11: Output: A non-empty prediction interval ˆCα(Xn+1). 33Corollary A3. Assume (X1, Y1), . . . ,(Xn, Yn), (Xn+1, Yn+1) are exchangeable random samples, and let ˆCα(Xn+1) be the output of Algorithm A11, for any α ∈ (0, 1). Then, P[Yn+1 ∈ ˆCα(Xn+1)] ≥ 1 − α. A6 Review of Conformalized Quantile Regression This section reviews the relevant background on conditional quantile regression [62] and conformal- ized quantile regression (CQR) [31]. In contrast to the classical regression models that estimate the conditional mean of the test response Yn+1 given the test feature Xn+1 = x, quantile regression estimates the conditional quantile qβ of Yn+1 given Xn+1 = x, which is defined as qβ(x) = inf{y ∈ R : P(Yn+1 ≤ y|Xn+1 = x) ≥ β}. (A29) This can be formulated as solving the optimization problem: ˆqβ(x) = f(x, ˆθ), ˆθ = arg min θ 1 n nX i=1 ρβ(Yi, f(Xi, θ)), (A30) where f(x, θ) represents the quantile regression function [62] and ρβ is the convex “pinball loss” function [63], illustrated in Figure A11 and mathematically defined as ρβ(y, ˆy) = ( β(y − ˆy), if y − ˆy >0, (1 − β)(ˆy − y), otherwise. (A31) y− ˆy ρβ(y,ˆy) (1 − β)(ˆy− y) β(y− ˆy) Figure A11: Visualization of the pinball loss function defined in (A31). To construct an efficient prediction interval ˆC(Xn+1) whose length is adaptive to the local variability of Xn+1, CQR operates as follows. As in split conformal prediction, firstly the available data are randomly split into a proper training set, indexed by I1, and a calibration set, indexed by I2. Given any quantile regression algorithm A, two conditional quantile functions, ˆqαlo and ˆqαhi, are fitted on I1, where αlo = α/2 and αhi = 1 − α/2: {ˆqαlo, ˆqαhi} ← A({(Xi, Yi) : i ∈ I1}). (A32) 34Then, conformity scores are computed on the calibration data set I2 as: Ei = max{ˆqαlo(Xi) − Yi, Yi − ˆqαhi(Xi)} for i ∈ I2. (A33) The conformity score in (A33) can account both for possible under-coverage and over-coverage of the quantile regression model [31]. If Yi is outside the interval [ˆqαlo(Xi), ˆqαhi(Xi)], then Ei is the (positive) distance of Yi from the closest endpoint of the interval. Otherwise, if Yi is inside the interval [ˆqαlo(Xi), ˆqαhi(Xi)], then Ei is the negative of the distance of Yi from the closest endpoint of the interval. Therefore, if the quantile regression model is well-calibrated, approximately 90% of the calibration data points should have Ei ≤ 0 [31]. Finally, CQR constructs the prediction interval for the test response value Yn+1 through ˆC(Xn+1) = [ˆqαlo(Xn+1) − ˆQ1−α(E, I2), ˆqαhi(Xn+1) + ˆQ1−α(E, I2)], (A34) where ˆQ1−α(E, I2) is the (1 − α)(1 + 1/|I2|)-th empirical quantile of {Ei : i ∈ I2}. A7 Avoiding Empty Predictions for Regression with CQR In this section, we introduce Algorithm A12 as an extension of Algorithm 4 in the main text to address the rare possibility of generating empty prediction sets. Algorithm A12 ensures that the intervals it produces are always non-empty, while still encompassing the intervals obtained from Algorithm 4. Consequently, Algorithm A12 maintains the guaranteed coverage provided by Theorem 4, as indicated in Corollary A4. Algorithm A12 Conformalized early stopping for quantile regression, avoiding empty predictions 1: Input: Exchangeable data points ( X1, Y1), . . . ,(Xn, Yn) with outcomes Yi ∈ R. 2: Input: Test point with features Xn+1. Desired coverage level 1 − α. 3: Input: Maximum number of training epochs tmax; storage period hyper-parameter τ. 4: Input: Trainable quantile regression model with target quantiles [ βlow, βhigh]. 5: Randomly split the exchangeable data points into Dtrain and Des-cal. 6: Train for tmax epochs and save the intermediate models Mβlow,t1, . . . , Mβlow,tT , Mβhigh,t1, . . . , Mβhigh,tT . 7: Evaluate ˆCα(Xn+1) using Algorithm 4. 8: if ˆCα(Xn+1) = ∅ then 9: Evaluate ˆCnaive α (Xn+1) using Algorithm A13. Set ˆCα(Xn+1) = ˆCnaive α (Xn+1). 10: end if 11: Output: A non-empty prediction interval ˆCα(Xn+1). Corollary A4. Assume (X1, Y1), . . . ,(Xn, Yn), (Xn+1, Yn+1) are exchangeable random samples, and let ˆCα(Xn+1) be the output of Algorithm A12, for any α ∈ (0, 1). Then, P[Yn+1 ∈ ˆCα(Xn+1)] ≥ 1 − α. 35A7.1 Implementation of the Naive Benchmark Algorithm A13 Naive conformal quantile regression benchmark with greedy early stopping 1: Input: Exchangeable data points ( X1, Y1), . . . ,(Xn, Yn) with outcomes Yi ∈ R. 2: Input: Test point with features Xn+1. Desired coverage level 1 − α. 3: Input: Maximum number of training epochs tmax; storage period hyper-parameter τ. 4: Input: Trainable quantile regression model with target quantiles [ βlow, βhigh]. 5: Randomly split the exchangeable data points into Dtrain and Des-cal. 6: Train for tmax epochs and save the intermediate models Mβlow,t1, . . . , MβlowtT , Mβhigh,t1, . . . , MβhightT . 7: Pick the most promising models t∗ low, t∗ high ∈ [T] minimizing Les-cal(Mt) in (14). 8: Evaluate the scores ˆEi(Xn+1) = max{ˆqt∗ low(Xi) − Yi, Yi − ˆqt∗ high(Xi)} for all i ∈ Des-cal. 9: Compute ˆQ1−α(Xn+1) = ⌈(1 − α)(1 + |Des-cal|)⌉-th smallest value in ˆEi(Xn+1) for i ∈ Des-cal. 10: Output: ˆCnaive α (Xn+1) = [ˆqt∗ low(Xn+1) − ˆQ1−α(Xn+1), ˆqt∗ high(Xn+1) + ˆQ1−α(Xn+1)]. 36A8 Additional Results from Numerical Experiments A8.1 Outlier Detection Table A1: Performance of outlier detection based on classification models trained with different methods, on the CIFAR10 data set [52]. Other details are as in Figure 6. The numbers in paren- thesis indicate standard errors. The numbers in bold highlight TPR values within 1 standard error of the best TPR across all methods, for each sample size. Sample size Method TPR FPR 500 500 CES 0.296 (0.008) 0.098 (0.003) 500 Naive 0.295 (0.008) 0.097 (0.003) 500 Naive + theory 0.114 (0.006) 0.016 (0.001) 500 Data splitting 0.234 (0.008) 0.091 (0.003) 500 Full training 0.217 (0.011) 0.072 (0.004) 1000 1000 CES 0.401 (0.007) 0.100 (0.004) 1000 Naive 0.401 (0.007) 0.100 (0.004) 1000 Naive + theory 0.237 (0.006) 0.030 (0.002) 1000 Data splitting 0.337 (0.009) 0.094 (0.003) 1000 Full training 0.189 (0.013) 0.055 (0.004) 2000 2000 CES 0.450 (0.005) 0.100 (0.003) 2000 Naive 0.450 (0.005) 0.100 (0.003) 2000 Naive + theory 0.337 (0.006) 0.048 (0.002) 2000 Data splitting 0.404 (0.007) 0.095 (0.003) 2000 Full training 0.050 (0.009) 0.017 (0.003) A8.2 Multi-class Classification 37Table A2: Performance of multi-class classification based on classification models trained with different methods, on the CIFAR10 data set [52]. Other details are as in Figure 7. The numbers in parenthesis indicate standard errors. The numbers in bold highlight cardinality values within 1 standard error of the best cardinality across all methods, for each sample size. Sample size Method Cardinality Marignal coverage 500 500 CES 6.754 (0.074) 0.908 (0.003) 500 Naive 6.735 (0.072) 0.906 (0.003) 500 Naive + theory 9.193 (0.052) 0.988 (0.001) 500 Data splitting 7.022 (0.077) 0.900 (0.004) 500 Full training 6.759 (0.091) 0.909 (0.004) 1000 1000 CES 5.902 (0.060) 0.902 (0.003) 1000 Naive 5.908 (0.059) 0.901 (0.003) 1000 Naive + theory 7.767 (0.064) 0.972 (0.002) 1000 Data splitting 6.294 (0.063) 0.900 (0.004) 1000 Full training 6.270 (0.092) 0.897 (0.004) 2000 2000 CES 5.352 (0.045) 0.903 (0.003) 2000 Naive 5.347 (0.045) 0.902 (0.003) 2000 Naive + theory 6.609 (0.049) 0.955 (0.002) 2000 Data splitting 5.674 (0.040) 0.904 (0.003) 2000 Full training 7.776 (0.194) 0.934 (0.006) A8.3 Regression Coverage (marginal) Coverage (conditional) Width 200 500 1000 2000 200 500 1000 2000 200 500 1000 2000 400 600 800 0.7 0.8 0.9 1.0 0.7 0.8 0.9 1.0 Sample size Method CES Naive + theory Data splitting Full training Figure A12: Performance of conformal prediction intervals based on regression models trained with different methods, on the bike data set [54]. The results are shown as a function of the total sample size. The nominal marginal coverage level is 90%. See Table A4 for additional details and standard errors. 38Coverage (marginal) Coverage (conditional) Width 200 500 1000 200 500 1000 200 500 1000 20 40 60 80 0.7 0.8 0.9 1.0 0.7 0.8 0.9 1.0 Sample size Method CES Naive + theory Data splitting Full training Figure A13: Performance of conformal prediction intervals based on regression models trained with different methods, on the concrete data set [55]. The results are shown as a function of the total sample size. The nominal marginal coverage level is 90%. See Table A5 for additional details and standard errors. Table A3: Performance of conformal prediction intervals based on regression models trained with different methods, on the bio data set [44]. Other details are as in Figure 3. The numbers in parenthesis indicate standard errors. The numbers in bold highlight width values within 1 standard error of the best width across all methods, for each sample size. The numbers in red highlight coverage values below 0.85. Coverage Sample size Data Method Width Marginal Conditional 200 200 bio CES 18.740 (0.123) 0.924 (0.005) 0.898 (0.014) 200 bio Naive 18.544 (0.133) 0.917 (0.005) 0.899 (0.015) 200 bio Naive + theory 20.942 (0.006) 1.000 (0.000) 1.000 (0.000) 200 bio Data splitting 19.068 (0.113) 0.925 (0.005) 0.902 (0.015) 200 bio Full training 18.673 (0.125) 0.919 (0.004) 0.890 (0.018) 500 500 bio CES 17.435 (0.125) 0.914 (0.004) 0.909 (0.013) 500 bio Naive 17.363 (0.134) 0.910 (0.004) 0.890 (0.017) 500 bio Naive + theory 20.391 (0.061) 0.993 (0.001) 0.996 (0.001) 500 bio Data splitting 18.076 (0.123) 0.911 (0.004) 0.888 (0.015) 500 bio Full training 18.245 (0.129) 0.918 (0.003) 0.890 (0.019) 1000 1000 bio CES 16.251 (0.081) 0.901 (0.003) 0.885 (0.015) 1000 bio Naive 16.219 (0.084) 0.900 (0.003) 0.890 (0.012) 1000 bio Naive + theory 19.167 (0.073) 0.977 (0.002) 0.976 (0.006) 1000 bio Data splitting 16.728 (0.089) 0.903 (0.003) 0.887 (0.016) 1000 bio Full training 17.962 (0.141) 0.902 (0.004) 0.814 (0.022) 2000 2000 bio CES 15.812 (0.042) 0.899 (0.003) 0.893 (0.015) 2000 bio Naive 15.805 (0.042) 0.899 (0.003) 0.897 (0.014) 2000 bio Naive + theory 17.691 (0.047) 0.957 (0.002) 0.963 (0.006) 2000 bio Data splitting 16.043 (0.059) 0.900 (0.003) 0.903 (0.015) 2000 bio Full training 17.014 (0.113) 0.902 (0.003) 0.829 (0.019) 39Table A4: Performance of conformal prediction intervals based on regression models trained with different methods, on the bike data set [54]. Other details are as in Figure A12. The numbers in parenthesis indicate standard errors. The numbers in bold highlight width values within 1 standard error of the best width across all methods, for each sample size. The numbers in red highlight coverage values below 0.85. Coverage Sample size Data Method Width Marginal Conditional 200 200 bike CES 412.534 (6.439) 0.922 (0.004) 0.900 (0.018) 200 bike Naive 392.474 (6.019) 0.908 (0.005) 0.878 (0.016) 200 bike Naive + theory 913.440 (3.737) 1.000 (0.000) 1.000 (0.000) 200 bike Data splitting 427.964 (7.282) 0.920 (0.004) 0.910 (0.016) 200 bike Full training 444.656 (6.760) 0.920 (0.005) 0.905 (0.013) 500 500 bike CES 354.180 (4.183) 0.913 (0.004) 0.896 (0.017) 500 bike Naive 343.641 (4.220) 0.902 (0.004) 0.889 (0.018) 500 bike Naive + theory 622.837 (8.381) 0.990 (0.001) 0.989 (0.004) 500 bike Data splitting 371.079 (3.777) 0.912 (0.004) 0.908 (0.014) 500 bike Full training 381.951 (5.175) 0.913 (0.004) 0.881 (0.018) 1000 1000 bike CES 303.516 (3.047) 0.905 (0.004) 0.876 (0.017) 1000 bike Naive 300.091 (3.127) 0.903 (0.004) 0.894 (0.015) 1000 bike Naive + theory 484.565 (4.958) 0.977 (0.002) 0.982 (0.003) 1000 bike Data splitting 333.939 (2.891) 0.905 (0.003) 0.895 (0.018) 1000 bike Full training 308.981 (3.760) 0.901 (0.004) 0.891 (0.016) 2000 2000 bike CES 234.322 (1.935) 0.902 (0.003) 0.894 (0.018) 2000 bike Naive 231.571 (1.956) 0.897 (0.003) 0.893 (0.018) 2000 bike Naive + theory 334.724 (2.988) 0.957 (0.002) 0.954 (0.012) 2000 bike Data splitting 272.589 (2.532) 0.899 (0.003) 0.880 (0.019) 2000 bike Full training 240.714 (2.389) 0.901 (0.003) 0.901 (0.015) 40Table A5: Performance of conformal prediction intervals based on regression models trained with different methods, on the concrete data set [55]. Other details are as in Figure A13. The numbers in parenthesis indicate standard errors. The numbers in bold highlight width values within 1 standard error of the best width across all methods, for each sample size. The numbers in red highlight coverage values below 0.85. Coverage Sample size Data Method Width Marginal Conditional 200 200 concrete CES 26.948 (0.515) 0.928 (0.005) 0.904 (0.016) 200 concrete Naive 24.793 (0.520) 0.903 (0.006) 0.856 (0.021) 200 concrete Naive + theory 79.089 (0.130) 1.000 (0.000) 1.000 (0.000) 200 concrete Data splitting 29.021 (0.564) 0.924 (0.004) 0.888 (0.018) 200 concrete Full training 28.676 (0.568) 0.926 (0.005) 0.878 (0.021) 500 500 concrete CES 19.232 (0.263) 0.913 (0.004) 0.867 (0.018) 500 concrete Naive 18.340 (0.276) 0.896 (0.004) 0.829 (0.022) 500 concrete Naive + theory 39.492 (0.861) 0.989 (0.002) 0.990 (0.003) 500 concrete Data splitting 22.876 (0.323) 0.919 (0.003) 0.841 (0.023) 500 concrete Full training 19.857 (0.300) 0.903 (0.004) 0.824 (0.024) 930 930 concrete CES 14.399 (0.134) 0.908 (0.002) 0.863 (0.014) 930 concrete Naive 13.738 (0.127) 0.899 (0.002) 0.863 (0.011) 930 concrete Naive + theory 26.596 (0.334) 0.978 (0.001) 0.967 (0.004) 930 concrete Data splitting 16.659 (0.122) 0.906 (0.002) 0.864 (0.014) 930 concrete Full training 14.998 (0.143) 0.908 (0.002) 0.834 (0.016) 41A8.4 Quantile Regression Coverage (marginal) Coverage (conditional) Width 200 500 1000 200 500 1000 200 500 1000 2 3 4 0.7 0.8 1.0 0.7 0.8 1.0 Sample size Method CES Naive + theory Data splitting Full training Figure A14: Performance of conformal prediction intervals based on quantile regression models trained with different methods, on the community data set [60]. The results are shown as a function of the total sample size with error bars corresponding to standard error. The nominal marginal coverage level is 90%. See Table A7 for additional details and standard errors. Coverage (marginal) Coverage (conditional) Width 200 500 1000 2000 200 500 1000 2000 200 500 1000 2000 2 3 4 0.7 0.8 1.0 0.7 0.8 1.0 Sample size Method CES Naive + theory Data splitting Full training Figure A15: Performance of conformal prediction intervals based on quantile regression models trained with different methods, on the bio data set [44]. The results are shown as a function of the total sample size with error bars corresponding to standard error. The nominal marginal coverage level is 90%. See Table A8 for additional details and standard errors. Coverage (marginal) Coverage (conditional) Width 200 500 1000 2000 200 500 1000 2000 200 500 1000 2000 3 10 30 0.7 0.8 1.0 0.7 0.8 1.0 Sample size Method CES Naive + theory Data splitting Full training Figure A16: Performance of conformal prediction intervals based on quantile regression models trained with different methods, on the meps 21 data set [57].The results are shown as a function of the total sample size with error bars corresponding to standard error. The nominal marginal coverage level is 90%. See Table A9 for additional details and standard errors. 42Coverage (marginal) Coverage (conditional) Width 200 500 1000 2000 200 500 1000 2000 200 500 1000 2000 3 10 30 0.7 0.8 1.0 0.7 0.8 1.0 Sample size Method CES Naive + theory Data splitting Full training Figure A17: Performance of conformal prediction intervals based on quantile regression models trained with different methods, on the blog data data set [58]. The results are shown as a function of the total sample size with error bars corresponding to standard error. The nominal marginal coverage level is 90%. See Table A10 for additional details and standard errors. Coverage (marginal) Coverage (conditional) Width 200 500 1000 200 500 1000 200 500 1000 0.2 0.3 0.5 0.7 0.8 1.0 0.7 0.8 1.0 Sample size Method CES Naive + theory Data splitting Full training Figure A18: Performance of conformal prediction intervals based on quantile regression models trained with different methods, on the STAR data set [59]. The results are shown as a function of the total sample size with error bars corresponding to standard error. The nominal marginal coverage level is 90%. See Table A11 for additional details and standard errors. Coverage (marginal) Coverage (conditional) Width 200 500 1000 2000 200 500 1000 2000 200 500 1000 2000 1 2 3 0.7 0.8 1.0 0.7 0.8 1.0 Sample size Method CES Naive + theory Data splitting Full training Figure A19: Performance of conformal prediction intervals based on quantile regression models trained with different methods, on the bike data set [54]. The results are shown as a function of the total sample size with error bars corresponding to standard error. The nominal marginal coverage level is 90%. See Table A12 for additional details and standard errors. 43Table A6: Performance of conformal prediction intervals based on quantile regression models trained with different methods, on the homes data set [61]. Other details are as in Figure 8. The numbers in parenthesis indicate standard errors. The numbers in bold highlight width values within 1 standard error of the best width across all methods, for each sample size. The numbers in red highlight coverage values below 0.85. Coverage Sample size Data Method Width Marginal Conditional 200 200 homes CES 1.396 (0.047) 0.924 (0.007) 0.804 (0.016) 200 homes Naive 1.171 (0.048) 0.904 (0.009) 0.761 (0.029) 200 homes Naive + theory 2.607 (0.222) 0.983 (0.004) 0.931 (0.015) 200 homes Data splitting 1.329 (0.049) 0.907 (0.006) 0.730 (0.029) 200 homes Full training 1.195 (0.053) 0.919 (0.008) 0.674 (0.030) 500 500 homes CES 0.997 (0.022) 0.909 (0.007) 0.842 (0.016) 500 homes Naive 0.940 (0.026) 0.899 (0.007) 0.798 (0.018) 500 homes Naive + theory 2.698 (0.308) 0.989 (0.003) 0.950 (0.017) 500 homes Data splitting 0.985 (0.019) 0.896 (0.006) 0.810 (0.020) 500 homes Full training 0.940 (0.043) 0.913 (0.007) 0.710 (0.019) 1000 1000 homes CES 0.858 (0.015) 0.910 (0.005) 0.827 (0.017) 1000 homes Naive 0.824 (0.017) 0.894 (0.006) 0.784 (0.015) 1000 homes Naive + theory 1.229 (0.039) 0.969 (0.004) 0.917 (0.015) 1000 homes Data splitting 0.926 (0.020) 0.905 (0.004) 0.786 (0.021) 1000 homes Full training 0.755 (0.019) 0.902 (0.005) 0.698 (0.019) 2000 2000 homes CES 0.726 (0.016) 0.902 (0.005) 0.824 (0.013) 2000 homes Naive 0.667 (0.013) 0.891 (0.003) 0.799 (0.016) 2000 homes Naive + theory 0.865 (0.013) 0.949 (0.003) 0.880 (0.013) 2000 homes Data splitting 0.779 (0.011) 0.900 (0.003) 0.804 (0.013) 2000 homes Full training 0.653 (0.013) 0.889 (0.005) 0.686 (0.019) 44Table A7: Performance of conformal prediction intervals based on quantile regression models trained with different methods, on the community data set [60]. Other details are as in Figure A14. The numbers in parenthesis indicate standard errors. The numbers in bold highlight width values within 1 standard error of the best width across all methods, for each sample size. The numbers in red highlight coverage values below 0.85. Coverage Sample size Data Method Width Marginal Conditional 200 200 community CES 2.226 (0.057) 0.905 (0.006) 0.835 (0.018) 200 community Naive 2.100 (0.065) 0.891 (0.009) 0.814 (0.018) 200 community Naive + theory 3.876 (0.193) 0.980 (0.004) 0.963 (0.008) 200 community Data splitting 2.250 (0.081) 0.902 (0.007) 0.811 (0.017) 200 community Full training 2.345 (0.070) 0.903 (0.007) 0.752 (0.023) 500 500 community CES 1.957 (0.033) 0.902 (0.006) 0.849 (0.014) 500 community Naive 1.866 (0.043) 0.889 (0.007) 0.830 (0.012) 500 community Naive + theory 4.312 (0.254) 0.990 (0.002) 0.971 (0.008) 500 community Data splitting 2.041 (0.037) 0.904 (0.006) 0.843 (0.011) 500 community Full training 2.084 (0.067) 0.898 (0.007) 0.770 (0.018) 994 994 community CES 1.717 (0.016) 0.911 (0.003) 0.850 (0.007) 994 community Naive 1.601 (0.019) 0.902 (0.003) 0.853 (0.008) 994 community Naive + theory 2.486 (0.049) 0.971 (0.002) 0.943 (0.005) 994 community Data splitting 1.818 (0.013) 0.910 (0.002) 0.865 (0.008) 994 community Full training 1.784 (0.029) 0.901 (0.003) 0.797 (0.011) 45Table A8: Performance of conformal prediction intervals based on quantile regression models trained with different methods, on the bio data set [44]. Other details are as in Figure A15. The numbers in parenthesis indicate standard errors. The numbers in bold highlight width values within 1 standard error of the best width across all methods, for each sample size. The numbers in red highlight coverage values below 0.85. Coverage Sample size Data Method Width Marginal Conditional 200 200 bio CES 2.732 (0.074) 0.926 (0.006) 0.848 (0.022) 200 bio Naive 2.018 (0.064) 0.876 (0.011) 0.805 (0.025) 200 bio Naive + theory 3.392 (0.254) 0.971 (0.005) 0.954 (0.010) 200 bio Data splitting 2.334 (0.083) 0.902 (0.010) 0.803 (0.029) 200 bio Full training 2.848 (0.124) 0.899 (0.010) 0.844 (0.016) 500 500 bio CES 2.154 (0.034) 0.921 (0.004) 0.872 (0.010) 500 bio Naive 1.900 (0.032) 0.892 (0.007) 0.839 (0.014) 500 bio Naive + theory 3.893 (0.161) 0.995 (0.001) 0.996 (0.002) 500 bio Data splitting 2.015 (0.024) 0.903 (0.004) 0.840 (0.013) 500 bio Full training 2.579 (0.057) 0.890 (0.011) 0.819 (0.032) 1000 1000 bio CES 1.997 (0.019) 0.918 (0.004) 0.890 (0.013) 1000 bio Naive 1.871 (0.018) 0.894 (0.005) 0.838 (0.018) 1000 bio Naive + theory 2.318 (0.033) 0.969 (0.002) 0.943 (0.010) 1000 bio Data splitting 1.895 (0.018) 0.898 (0.006) 0.811 (0.018) 1000 bio Full training 2.381 (0.035) 0.877 (0.010) 0.786 (0.037) 2000 2000 bio CES 1.869 (0.018) 0.906 (0.004) 0.867 (0.016) 2000 bio Naive 1.763 (0.017) 0.890 (0.004) 0.834 (0.016) 2000 bio Naive + theory 1.998 (0.017) 0.948 (0.003) 0.922 (0.014) 2000 bio Data splitting 1.830 (0.013) 0.898 (0.004) 0.852 (0.015) 2000 bio Full training 2.219 (0.029) 0.874 (0.008) 0.781 (0.035) 46Table A9: Performance of conformal prediction intervals based on quantile regression models trained with different methods, on the MEPS 21 data set [57]. Other details are as in Figure A16. The numbers in parenthesis indicate standard errors. The numbers in bold highlight width values within 1 standard error of the best width across all methods, for each sample size. The numbers in red highlight coverage values below 0.85. Coverage Sample size Data Method Width Marginal Conditional 200 200 meps 21 CES 4.442 (0.468) 0.927 (0.007) 0.844 (0.020) 200 meps 21 Naive 3.602 (0.452) 0.901 (0.010) 0.786 (0.025) 200 meps 21 Naive + theory 17.470 (2.874) 0.979 (0.003) 0.943 (0.012) 200 meps 21 Data splitting 3.239 (0.212) 0.909 (0.008) 0.780 (0.021) 200 meps 21 Full training 6.070 (0.509) 0.906 (0.009) 0.654 (0.030) 500 500 meps 21 CES 3.453 (0.074) 0.912 (0.004) 0.845 (0.015) 500 meps 21 Naive 3.077 (0.068) 0.898 (0.005) 0.802 (0.012) 500 meps 21 Naive + theory 35.882 (4.705) 0.991 (0.002) 0.974 (0.008) 500 meps 21 Data splitting 3.147 (0.109) 0.906 (0.006) 0.832 (0.012) 500 meps 21 Full training 6.038 (0.305) 0.913 (0.005) 0.709 (0.021) 1000 1000 meps 21 CES 3.447 (0.068) 0.896 (0.005) 0.833 (0.012) 1000 meps 21 Naive 3.102 (0.073) 0.888 (0.006) 0.825 (0.014) 1000 meps 21 Naive + theory 6.054 (0.443) 0.970 (0.002) 0.930 (0.009) 1000 meps 21 Data splitting 3.231 (0.093) 0.901 (0.005) 0.817 (0.016) 1000 meps 21 Full training 5.183 (0.188) 0.902 (0.004) 0.646 (0.019) 2000 2000 meps 21 CES 3.666 (0.054) 0.907 (0.004) 0.867 (0.012) 2000 meps 21 Naive 3.454 (0.047) 0.902 (0.004) 0.875 (0.013) 2000 meps 21 Naive + theory 4.242 (0.069) 0.951 (0.003) 0.925 (0.010) 2000 meps 21 Data splitting 3.485 (0.060) 0.902 (0.003) 0.847 (0.009) 2000 meps 21 Full training 4.349 (0.128) 0.897 (0.003) 0.651 (0.018) 47Table A10: Performance of conformal prediction intervals based on quantile regression models trained with different methods, on the blog data data set [58]. Other details are as in Figure A17. The numbers in parenthesis indicate standard errors. The numbers in bold highlight width values within 1 standard error of the best width across all methods, for each sample size. The numbers in red highlight coverage values below 0.85. Coverage Sample size Data Method Width Marginal Conditional 200 200 blog data CES 4.658 (0.194) 0.911 (0.007) 0.779 (0.021) 200 blog data Naive 3.427 (0.210) 0.897 (0.008) 0.766 (0.021) 200 blog data Naive + theory 22.166 (5.645) 0.973 (0.004) 0.917 (0.017) 200 blog data Data splitting 4.049 (0.359) 0.892 (0.013) 0.720 (0.034) 200 blog data Full training 7.524 (0.544) 0.910 (0.007) 0.662 (0.026) 500 500 blog data CES 4.538 (0.291) 0.924 (0.005) 0.819 (0.017) 500 blog data Naive 3.986 (0.333) 0.917 (0.005) 0.796 (0.014) 500 blog data Naive + theory 58.965 (11.745) 0.992 (0.002) 0.987 (0.004) 500 blog data Data splitting 3.443 (0.139) 0.905 (0.005) 0.751 (0.020) 500 blog data Full training 9.364 (0.535) 0.927 (0.003) 0.719 (0.019) 1000 1000 blog data CES 4.313 (0.088) 0.903 (0.004) 0.812 (0.016) 1000 blog data Naive 3.559 (0.073) 0.896 (0.004) 0.816 (0.016) 1000 blog data Naive + theory 7.336 (0.620) 0.973 (0.002) 0.951 (0.007) 1000 blog data Data splitting 3.525 (0.119) 0.896 (0.003) 0.788 (0.015) 1000 blog data Full training 7.784 (0.427) 0.919 (0.003) 0.694 (0.018) 2000 2000 blog data CES 4.169 (0.066) 0.906 (0.003) 0.808 (0.013) 2000 blog data Naive 3.671 (0.047) 0.899 (0.003) 0.783 (0.018) 2000 blog data Naive + theory 4.486 (0.074) 0.954 (0.002) 0.911 (0.009) 2000 blog data Data splitting 3.625 (0.069) 0.899 (0.003) 0.805 (0.011) 2000 blog data Full training 5.323 (0.274) 0.909 (0.003) 0.718 (0.014) 48Table A11: Performance of conformal prediction intervals based on quantile regression models trained with different methods, on the STAR data set [59]. Other details are as in Figure A18. The numbers in parenthesis indicate standard errors. The numbers in bold highlight width values within 1 standard error of the best width across all methods, for each sample size. The numbers in red highlight coverage values below 0.85. Coverage Sample size Data Method Width Marginal Conditional 200 200 star CES 0.567 (0.023) 0.933 (0.006) 0.891 (0.012) 200 star Naive 0.421 (0.031) 0.904 (0.008) 0.847 (0.012) 200 star Naive + theory 0.656 (0.038) 0.984 (0.002) 0.960 (0.006) 200 star Data splitting 0.502 (0.022) 0.911 (0.009) 0.862 (0.013) 200 star Full training 0.323 (0.008) 0.914 (0.006) 0.872 (0.012) 500 500 star CES 0.318 (0.007) 0.929 (0.004) 0.893 (0.012) 500 star Naive 0.288 (0.008) 0.902 (0.007) 0.848 (0.016) 500 star Naive + theory 0.520 (0.025) 0.992 (0.001) 0.978 (0.005) 500 star Data splitting 0.328 (0.009) 0.910 (0.006) 0.860 (0.015) 500 star Full training 0.224 (0.005) 0.909 (0.006) 0.874 (0.013) 1000 1000 star CES 0.252 (0.005) 0.914 (0.003) 0.864 (0.012) 1000 star Naive 0.236 (0.007) 0.897 (0.003) 0.862 (0.011) 1000 star Naive + theory 0.323 (0.007) 0.967 (0.003) 0.929 (0.009) 1000 star Data splitting 0.269 (0.007) 0.903 (0.003) 0.846 (0.012) 1000 star Full training 0.182 (0.002) 0.898 (0.004) 0.848 (0.012) 1161 1161 star CES 0.237 (0.005) 0.920 (0.004) 0.882 (0.012) 1161 star Naive 0.220 (0.005) 0.900 (0.005) 0.860 (0.012) 1161 star Naive + theory 0.308 (0.007) 0.969 (0.002) 0.944 (0.007) 1161 star Data splitting 0.254 (0.007) 0.903 (0.004) 0.864 (0.014) 1161 star Full training 0.174 (0.003) 0.896 (0.004) 0.863 (0.013) 49Table A12: Performance of conformal prediction intervals based on quantile regression models trained with different methods, on the bike data set [54]. Other details are as in Figure A19. The numbers in parenthesis indicate standard errors. The numbers in bold highlight width values within 1 standard error of the best width across all methods, for each sample size. The numbers in red highlight coverage values below 0.85. Coverage Sample size Data Method Width Marginal Conditional 200 200 bike CES 2.690 (0.065) 0.923 (0.006) 0.895 (0.014) 200 bike Naive 2.535 (0.053) 0.905 (0.008) 0.868 (0.015) 200 bike Naive + theory 4.106 (0.192) 0.979 (0.003) 0.958 (0.008) 200 bike Data splitting 2.663 (0.056) 0.899 (0.008) 0.832 (0.018) 200 bike Full training 2.843 (0.095) 0.910 (0.006) 0.870 (0.012) 500 500 bike CES 2.244 (0.039) 0.913 (0.004) 0.886 (0.014) 500 bike Naive 2.137 (0.047) 0.901 (0.006) 0.880 (0.012) 500 bike Naive + theory 4.284 (0.152) 0.991 (0.002) 0.984 (0.005) 500 bike Data splitting 2.275 (0.032) 0.902 (0.004) 0.859 (0.014) 500 bike Full training 2.265 (0.062) 0.908 (0.004) 0.839 (0.014) 1000 1000 bike CES 1.978 (0.036) 0.911 (0.004) 0.872 (0.013) 1000 bike Naive 1.779 (0.057) 0.893 (0.004) 0.821 (0.016) 1000 bike Naive + theory 2.590 (0.061) 0.968 (0.002) 0.955 (0.007) 1000 bike Data splitting 1.996 (0.040) 0.903 (0.004) 0.852 (0.013) 1000 bike Full training 1.625 (0.045) 0.901 (0.005) 0.858 (0.013) 2000 2000 bike CES 1.262 (0.030) 0.902 (0.003) 0.854 (0.012) 2000 bike Naive 1.113 (0.029) 0.884 (0.004) 0.847 (0.012) 2000 bike Naive + theory 1.570 (0.033) 0.948 (0.003) 0.933 (0.009) 2000 bike Data splitting 1.517 (0.033) 0.893 (0.003) 0.853 (0.011) 2000 bike Full training 1.061 (0.019) 0.887 (0.004) 0.839 (0.013) 50A9 Mathematical Proofs Proof of Theorem 1. It suffices to show that the nonconformity scores ˆSi for i ∈ {n+1}∪D es-cal are exchangeable. In fact, if the nonconformity scores are almost-surely unique, this implies the rank of ˆSn+1 is uniformly distributed over {ˆSi}i∈{n+1}∪Des-cal, and in that case the conformal p-value is uniformly distributed over {1/(1+ |Des-cal|), 2/(1+ |Des-cal|), . . . ,1}. If the nonconformity scores are not almost-surely unique and ties are not broken at random, then the distribution of the conformal p-value becomes stochastically larger than uniform, in which case the result still holds. To prove the exchangeability of the nonconformity scores, let σ be any permutation of {n + 1} ∪ Des-cal, and imagine applying Algorithm 1, with the same random seed, to the shuffled data set indexed by σ({n + 1} ∪ Des-cal), which has the same distribution as the original data set. To clarify the notation, we will refer to quantities computed under this data shuffling scenario with their usual symbol followed by an apostrophe; i.e.,M′ t1 instead of Mt1. As the gradient updates only involve the unperturbed observations in Dtrain and the maximum number of epochs tmax is fixed, the sequence of saved models remains exactly the same under this scenario: ( M′ t1, . . . , M′ tT ) = ( Mt1, . . . , MtT ). Further, the loss function in (1) is also invariant to permutations of {n + 1} ∪ Des-cal, in the sense that L+1′ es-cal = L+1 es-cal, because L is additive. Therefore, the model selected according to (2) is also invariant, ˆM′ ces = ˆMces, which implies the nonconformity scores are simply re-ordered: ˆS′ σ(i) = ˆSi. Therefore, we have: σ({ˆSi}i∈{n+1}∪Dcal) = {ˆS′ i}i∈{n+1}∪Dcal d= {ˆSi}i∈{n+1}∪Dcal, where the last equality follows from the initial data exchangeability assumption. Proof of Theorem 2. Note that, conditional on Yn+1 = y, the miscoverage event Yn+1 ̸∈ ˆCα(Xn+1) occurs if and only if ˆ uy(Xn+1) ≤ α, where ˆuy(Xn+1) is defined as in (6). Therefore, it suffices to show P[ˆuy(Xn+1) ≤ α | Yn+1 = y] ≤ α for any α ∈ (0, 1). However, this is directly implied by Theorem 1, because the ˆuy(Xn+1) calculated by Algorithm 2 is equivalent to the conformal p-value ˆu0(Zn+1) given by Algorithm 1 applied to the subset of the data in Des-cal with Yi = y, with the understanding that Zi = (Xi, Yi) for all i ∈ {n + 1} ∪ Des-cal. Proof of Theorem A5. Note that Yn+1 ̸∈ ˆCm α (Xn+1) if and only if ˆ umarg(Xn+1; Yn+1) ≤ α, where ˆumarg(Xn+1; Yn+1) is defined as in (A23). Hence it suffices to show thatP[ˆumarg(Xn+1; Yn+1) ≤ α] ≤ α for any α ∈ (0, 1). This can be established using the same approach as in the proof of Theorem 1, setting Zi = (Xi, Yi) for all i ∈ {n + 1} ∪ Des-cal. In fact, the maximum number of epochs tmax is fixed, the sequence of saved models is invariant to permutations of {n + 1} ∪ Des-cal, and the model ˆMces selected according to (5) is also invariant. Thus, it follows that the nonconformity scores ˆSi are exchangeable with one another for all i ∈ {n + 1} ∪ Des-cal. Proof of Theorem 3. Consider an imaginary oracle algorithm producing an interval ˆCoracle α (Xn+1) defined as ˆCoracle α (Xn+1) = Bl∗(Yn+1) T ˆCα(Xn+1, Bl∗(Yn+1)), where l∗(Yn+1) is the exact index of the bin Bl to which the true Yn+1 belongs. Clearly, this oracle is just a theoretical tool, not a practical method because the outcome value for the test point is unknown. However, this oracle is useful because it is easier to analyze, and it suffices to establish that P[Yn+1 ∈ ˆCoracle α (Xn+1)] ≥ 1 − α, for any α ∈ (0, 1), since ˆCα(Xn+1) ⊇ ˆCoracle α (Xn+1) almost-surely. The coverage property for the oracle can be established using an approach similar to that of the proof of Theorem 1, setting 51Zi = ( Xi, Yi) for all i ∈ {n + 1} ∪ Des-cal. In fact, the maximum number of epochs tmax is fixed, the sequence of saved models is invariant to permutations of {n + 1} ∪ Des-cal, and the model ˆMces selected by the oracle according to (10) is also invariant. Thus, it follows that the oracle nonconformity scores ˆS∗ i = ˆSi(Xn+1, Bl∗(Yn+1)) are exchangeable with one another for all i ∈ {n + 1} ∪ Des-cal. Further, by construction of the prediction intervals (12), we know that the miscoverage event Yn+1 ̸∈ ˆCoracle α (Xn+1) occurs if and only if ˆS∗ i > ˆQ∗ 1−α, where ˆQ∗ 1−α is the ⌈(1 − α)(1 +|Des-cal|)⌉-th smallest value among all nonconformity scores ˆSi(Xn+1, Bl). However, it is a well-known exchangeability result that P[ ˆS∗ i ≤ ˆQ∗ 1−α] ≥ 1 − α; see for example Lemma 1 in Romano et al. [31]. Proof of Theorem 4. Same as the proof of Theorem 3. Proof of Corollary A3. This corollary follows immediately from Theorem 3 because the prediction interval given by Algorithm A11 is always contained in that output by Algorithm 3. Proof of Corollary A4. Same as the proof of Corollary A3. Proof of Proposition A1. Note that ˆunaive 0 (Zn+1) = ˆunaive 0 (Zn+1; t∗), hence P \u0002 ˆunaive 0 (Zn+1) > α \u0003 = E \u0002 P \u0002 ˆunaive 0 (Zn+1; t∗) > α| Des-cal \u0003\u0003 ≥ E \u0014 min t∈[T] Wt \u0015 ≥ sup a∈[0,1] a · P \u0014 min t∈[T] Wt ≥ a \u0015 = sup a∈[0,1] a \u0012 1 − P \u0014 min t∈[T] Wt ≤ a \u0015\u0013 ≥ sup a∈[0,1] a (1 − T · P[Wt ≤ a]) , where the last inequality follows from a union bound. To simplify the right-hand-side term above, let a = I−1 \u0000 1 bT ; nes-cal + 1 − l, l \u0001 , where b is any large constant. Hence we obtain P \u0002 ˆunaive 0 (Zn+1) > α \u0003 ≥ I−1 \u0012 1 bT ; nes-cal + 1 − l, l \u0013 · (1 − 1/b). Proof of Corollary A1. Note that Yn+1 ∈ ˆCnaive α (Xn+1) if and only if ˆunaive Yn+1 (Xn+1; t∗) > α. Let Wt denote the calibration conditional coverage P h ˆunaive Yn+1 (Xn+1; t) > α| Des-cal i . Then, we have P h Yn+1 ∈ ˆCnaive α (Xn+1) i = E h P h ˆunaive Yn+1 (Xn+1; t∗) > α| Des-cal ii = E[Wt∗] ≥ E \u0014 min t∈[T] Wt \u0015 . The rest of the proof follows the same argument as in the proof of Proposition A1. 52Proof of Corollary A2. Let ˆSi(Xn+1, t) = |Yi − ˆµt(Xi)| denote the residual score calculated with model t ∈ [T], for all i ∈ Des-cal. Note that Yn+1 ∈ ˆCnaive α (Xn+1) if and only if ˆSXn+1(Xn+1, t∗) ≤ ˆQ1−α. Then, we just need to bound Wt = P h ˆSXn+1(Xn+1, t∗) ≤ ˆQ1−α | Des-cal i , and the rest of the proof follows the same steps as the proof of Proposition A1. Proof of Lemma A1. Recall that l = ⌊α(nes-cal + 1)⌋, and define the following helpful notations: Beta (nes-cal + 1 − l, l) := Beta (nes-cal · c, nes-cal · d) , where c = nes-cal + 1 − l nes-cal , d = l nes-cal . Denote Gamma(k, θ) as the gamma distribution with shape parameter k and scale parameter θ. It is a well known fact that the beta distribution can be expressed as a ratio of gamma distributions as: Beta (nes-cal · c, nes-cal · d) = Gamma(nes-cal · c, 1) Gamma(nes-cal · c, 1) + Gamma(nes-cal · d, 1). Further, Gamma(nes-cal · c, 1) can be seen as the distribution of a sum of nes-cal · c independent exponentially distributed random variables with mean equal to 1; therefore, by the central limit theorem, Gamma( nes-cal · c, 1) has an asymptotic Gaussian distribution as nes-cal → ∞. Denote Φ(x, µ, σ2) as the cumulative distribution function of a Gaussian random variable with mean µ and variance σ2. Applying the delta method, it follows that, in the limit of large nes-cal, I (x; nes-cal · c, nes-cal · d) = Φ \u0012 x; c c + d, 1 nes-cal · cd (c + d)3 \u0013 + O \u0012 1 nes-cal \u0013 , for any x ∈ [0, 1]. Since I and Φ are continuous and strictly increasing over [0, 1], letting Φ−1 be the inverse Gaussian CDF, we have I−1 \u0012 1 bT ; nes-cal · c, nes-cal · d \u0013 = Φ−1 \u0012 1 bT ; c c + d, 1 nes-cal · cd (c + d)3 \u0013 + O \u0012 1 nes-cal \u0013 = Φ−1 \u0012 1 bT ; 1− α, α(1 − α) nes-cal + 1 \u0013 + O \u0012 1 nes-cal \u0013 = (1 − α) + s α(1 − α) nes-cal + 1 · Φ−1 \u0012 1 bT ; 0, 1 \u0013 + O \u0012 1 nes-cal \u0013 = (1 − α) − s α(1 − α) nes-cal + 1 · p 2 log(bT) + O   1p nes-cal log(T) ! , where the second equality is obtained by substituting c and d with their defined values and the last inequality follows from Equation 26.2.23 in Abramowitz and Stegun [65] for sufficiently large T. 53",
      "meta_data": {
        "arxiv_id": "2301.11556v2",
        "authors": [
          "Ziyi Liang",
          "Yanfei Zhou",
          "Matteo Sesia"
        ],
        "published_date": "2023-01-27T06:43:07Z",
        "pdf_url": "https://arxiv.org/pdf/2301.11556v2.pdf"
      }
    },
    {
      "title": "Scaling Laws for Hyperparameter Optimization",
      "abstract": "Hyperparameter optimization is an important subfield of machine learning that\nfocuses on tuning the hyperparameters of a chosen algorithm to achieve peak\nperformance. Recently, there has been a stream of methods that tackle the issue\nof hyperparameter optimization, however, most of the methods do not exploit the\ndominant power law nature of learning curves for Bayesian optimization. In this\nwork, we propose Deep Power Laws (DPL), an ensemble of neural network models\nconditioned to yield predictions that follow a power-law scaling pattern. Our\nmethod dynamically decides which configurations to pause and train\nincrementally by making use of gray-box evaluations. We compare our method\nagainst 7 state-of-the-art competitors on 3 benchmarks related to tabular,\nimage, and NLP datasets covering 59 diverse tasks. Our method achieves the best\nresults across all benchmarks by obtaining the best any-time results compared\nto all competitors.",
      "full_text": "Scaling Laws for Hyperparameter Optimization Arlind Kadra Representation Learning Lab University of Freiburg kadraa@cs.uni-freiburg.de Maciej Janowski Representation Learning Lab University of Freiburg janowski@cs.uni-freiburg.de Martin Wistuba Amazon Web Services Amazon Berlin marwistu@amazon.com Josif Grabocka Representation Learning Lab University of Freiburg grabocka@cs.uni-freiburg.de Abstract Hyperparameter optimization is an important subfield of machine learning that focuses on tuning the hyperparameters of a chosen algorithm to achieve peak performance. Recently, there has been a stream of methods that tackle the issue of hyperparameter optimization, however, most of the methods do not exploit the dominant power law nature of learning curves for Bayesian optimization. In this work, we propose Deep Power Laws (DPL), an ensemble of neural network models conditioned to yield predictions that follow a power-law scaling pattern. Our method dynamically decides which configurations to pause and train incre- mentally by making use of gray-box evaluations. We compare our method against 7 state-of-the-art competitors on 3 benchmarks related to tabular, image, and NLP datasets covering 59 diverse tasks. Our method achieves the best results across all benchmarks by obtaining the best any-time results compared to all competitors. 1 Introduction Hyperparameter Optimization (HPO) is a major challenge for the Machine Learning community. Unfortunately, HPO is not yet feasible for Deep Learning (DL) methods due to the high cost of evaluating multiple configurations. Recently multi-fidelity HPO (a sub-problem of gray-box HPO) has emerged as a promising paradigm for HPO in DL, by discarding poorly-performing hyperparameter configurations after observing the validation error on the low-level fidelities of the optimization procedure [28, 9, 1, 29]. The advantage of multi-fidelity HPO compared to online HPO [7, 38], or meta-gradient HPO [32, 10, 31] is the ability to tune all types of hyperparameters. In recent years, a stream of papers highlights the fact that the performance of DL methods is predictable [14], concretely, that the validation error rate is a power law function of the model size, or dataset size [41, 40]. Such a power law relationship has been subsequently validated in the domain of NLP, too [11]. In this paper, we demonstrate that the power-law principle has the potential to be a game-changer in HPO, because we can evaluate hyperparameter configurations in low-budget regimes (e.g. after a few epochs), then estimate the performance on the full budget using dataset-specific power law models. Concretely, we hypothesize and empirically demonstrate that optimization curves (training epochs versus accuracy, or loss) can be efficiently modeled as simple power law functions. As a result, we introduce Deep Power Law (DPL) ensembles, a probabilistic surrogate for Bayesian optimization (BO) that estimates the performance of a hyperparameter configuration at future budgets using ensembles of deep power law functions. Subsequently, our novel formulation of BO dynami- cally decides which configurations to pause and train incrementally by relying on the performance 37th Conference on Neural Information Processing Systems (NeurIPS 2023). arXiv:2302.00441v3  [cs.LG]  25 Oct 2023estimations of the surrogate. We demonstrate that our method achieves the new state-of-the-art in HPO for DL by comparing against 7 strong HPO baselines, and 59 datasets of three diverse modalities (tabular, image, and natural language processing). Our experimental protocol contains state-of-the-art deep learning architectures such as Transformers [44], XFormer [26], ResNeXt [49] and large language models like GPT-2 [39]. As a result, we believe the proposed method has the potential to finally make HPO for DL a feasible reality. Overall, our contributions can be summarized as follows: • We introduce a novel probabilistic surrogate for multi-fidelity HPO based on ensembles of deep power law functions. • We derive a simple mechanism to combine our surrogate with Bayesian optimization. • Finally, we demonstrate the empirical superiority of our method against the current state-of- the-art in HPO for Deep Learning, with a large-scale HPO experimental protocol. 2 Related Work Multi-fidelity HPO assumes a method has access to the learning curve of a hyperparameter con- figuration. Such a learning curve is the function that maps either training time or dataset size, to the validation performance. The early performance of configurations (i.e. first segment of the learning curve) is used to discard unpromising configurations, before waiting for full convergence. Successive halving [17] is a widely used multi-fidelity method that randomly samples hyperparameter configurations, starts evaluating them, and ends a fraction of them upon reaching a predefined budget. Afterwards, the budget is divided by the fraction of discarded hyperparameter configurations and the process continues until the maximum budget is reached. Although the method relies only on the last observed value of the learning curve, it is very efficient. In recent years, various flavors of successive halving have been suggested, including Hyperband [28], which effectively runs successive halving in parallel with different settings. A major improvement to Hyperband is replacing random search with a more efficient sampling strategy [1, 9]. A more efficient approach is to allocate the budget dynamically to the most promising configurations [47]. However, the only assumption these methods make about the learning curve is that it will improve over time, while recent work [ 4] exploits a power law assumption on the curves. Similarly, we fit surrogates that exploit a power law assumption, however, our method is able to estimate the performance of unobserved configurations through probabilistic surrogates learned jointly for all hyperparameter configurations. Learning curve prediction is a related topic, where the performance of a configuration is predicted based on a partially observed learning curve [36]. Typically, the assumptions about the learning curve are much stronger than those described above. The prediction is often based on the assumption that the performance increases at the beginning and then flattened towards the end. One way to model this behavior is to define a weighted set of parametric functions [8, 23]. Then, the parameters of all functions are determined so that the resulting prediction best matches the observed learning curve. Another approach is to use learning curves from already evaluated configurations and to find an affine transformation that leads to a well-matched learning curve [6]. A more data-driven approach is to learn the typical learning curve behaviour directly from learning curves across different datasets [48]. Learning curve prediction algorithms can be combined with successive halving [2]. In contrast to this research line, we fit ensembles of power law surrogates for conducting multi-fidelity HPO with Bayesian optimization. Scaling laws describe the relationship between the performance of deep learning models as a function of dataset size or model size as a power law [ 14, 16, 41, 40, 11, 20, 15, 33]. Another work tunes hyperparameters on a small-scale model and then transfers them to a large-scale version [ 50]. In contrast to these papers, we directly use the power law assumption for training surrogates in Bayesian optimization for HPO. 3 Preliminaries Hyperparameter Optimization (HPO) demands finding the configurations λ ∈ Λ of a Machine Learning method that achieve the lowest validation loss L(Val) of a model (e.g. a neural network), which is parameterized with θ ∈ Θ and learned to minimize the training loss L(Train) as: 2λ∗ := arg min λ∈Λ L(V al) (λ, θ∗ (λ)) , s.t. θ∗ (λ) := arg min θ∈Θ L(Train ) (λ, θ) (1) For simplicity, we denote the validation loss as our function of interestf(λ) := L(V al) (λ, θ∗ (λ)). The optimal hyperparameter configurations λ∗ of Equation 1 are found via an HPO policy A ∈ P(also called an HPO method) that given a history of N evaluated configurations H(N) := {λi, f(λi)}N i=1 suggests the (N + 1)-th configuration to evaluate as λN+1 := A \u0000 H(N)\u0001 where A : [Λ × R+]N → Λ. The search for an optimal HPO policy is a bi-objective problem in itself, aiming at (i) finding a configuration out of N evaluations that achieves the smallest validation loss f(λ), and (ii) ensuring that the costs of evaluating the N configurations do not exceed a total budget Ω, as shown in Equation 2. arg min A∈P min i∈{1,...,N} f \u0010 λi := A \u0010 H(i−1) \u0011\u0011 , (2) where: H(i) := ( {(λj, f(λj))}i j=1 i >0 ∅ i = 0 , subject to: Ω ≥ NX i=1 cost (f (λi)) Bayesian optimization (BO) is the most popular type of policy for HPO, due to its ability to balance the exploration and exploitation aspects of minimizing the validation loss f in terms of hyperparame- ters λ ∈ Λ. Technically speaking, BO fits a surrogate ˆf(λ; ϕ) parametrized with ϕ ∈ Φ to approximate the observed loss f(λ) using the history H(N), as ϕ∗ := arg maxϕ∈Φ E(λ,f(λ)) ∼pH(N) p(f(λ)|λ, ϕ), where, p represents the probability. Afterwards, BO uses an acquisition/utility function a : Λ → R+ to recommend the next configuration as λN+1 := A \u0000 H(N)\u0001 = arg maxλ∈Λ a \u0010 ˆf(λ; ϕ∗) \u0011 . Multi-fidelity HPO refers to the case where an approximation of the validation loss can be measured at a lower budget b ∈ B, where B := (0, bmax]. For instance, in Deep Learning we can measure the validation loss after a few epochs ( 0 < b < ϵ), rather than wait for a full convergence ( b = bmax). Throughout this paper, the term budget refers to a learning curve step. The evaluation of a configuration λ for a budget b is defined as f (λ, b) : Λ × B → R+. 4 Power Law Surrogates for Bayesian Optimization Prior work has demonstrated that the performance of Machine Learning methods as a function of budgets (i.e. dataset size, number of optimization epochs, model size, image resolution) follows a power law relationship [41, 40, 11, 20, 15]. In this work, we employ this power law dependence between the validation loss and the number of optimization epochs in Deep Learning. We propose a novel multi-fidelity Hyperparameter Optimization method which is based on power law surrogates. We assume that every learning curve f (λ, ·) can be described by a power law function defined by (α, β, γ). Concretely, we define a power law function for the validation loss of a configuration λ at a budget b (a.k.a. the number of epochs) as shown in Equation 3. ˆf (λ, b) := αλ + βλ b−γλ, αλ, βλ, γλ ∈ R (3) Instead of fitting one separate power law function to each learning curve, we fit a single shared power law function across all configurations by conditioning the power law coefficients α, β, γon λ using a parametric neural network g that maps a configuration to the power law coefficients of its learning curve as g : Λ → R3. The network g has three output nodes, corresponding to the power law coefficients, denoted as g(λ)α, g(λ)β, g(λ)γ, as defined in Equation 4. ˆf (λ, b) := g(λ)α + g(λ)β b−g(λ)γ , g : Λ → R3 (4) 3Using a history of learning curve evaluations H(N) := {(λi, bi, f(λi, bi))}N i=1 we can train the power law surrogate to minimize the following loss function using stochastic gradient descent: arg min ˆf E(λ,b,f(λ,b))∼pH(N) \f\f\ff (λi, bi) − ˆf (λi, bi) \f\f\f (5) BO surrogates need to be probabilistic regression models because the acquisition functions require the posterior variance of the predictions. As a result, we train an ensemble of K diverse surrogates ˆf(1)(λ, b), . . . ,ˆf(K)(λ, b) by initializing each surrogate with different weights and by training with a different sequence of mini-batches as in prior work [25, 22]. The posterior mean µ and the posterior variance σ2 of the power law ensemble are trivially computed as: µ ˆf (λ, b) := 1 K KX k=1 ˆf(k)(λ, b), σ 2 ˆf (λ, b) := 1 K KX k=1 \u0010 ˆf(k)(λ, b) − µ ˆf (λ, b) \u00112 (6) A commonly used acquisition function in the domain is Expected Improvement (EI) [ 35] which incorporates both the mean and uncertainty of predictions, applying a trade-off between exploration and exploitation. Consequently, in our work, we use the EI acquisition with the estimated full budget’s (bmax) posterior mean and variance. We briefly define the acquisition function in Equation 7: λN+1 := arg max λ∈Λ EI \u0010 λ, bmax|H(N) \u0011 , (7) EI(λ, bmax|H) := Eˆf(λ,bmax)∼N \u0010 µ ˆf (λ,bmax),σ2 ˆf (λ,bmax) \u0011 h max n ˆf(λ, bmax) − f (λbest, bmax) , 0 oi where, f (λbest, bmax) corresponds to the best observed loss for any budget b′ ≤ bmax from the history H(N). After selecting a configuration with our variant of the EI acquisition, we do not naively run it until convergence. Instead, we propose a novel multi-fidelity strategy that advances the selected λN+1 of Equation 7 by a small budget of bstep, e.g. 1 epoch of training. Therefore, the selected λN+1 will be evaluated at bN+1 as defined in Equation 8. Notice, our proposed strategy also covers new configurations with no learning curve evaluations in H(N). bN+1 := (bstep, ∄λN+1 : (λN+1, ·, ·) ∈ H(N) bstep + max (λN+1,b,·)∈H(N) b, otherwise (8) We provide the detailed pseudocode of our method at Algorithm 1. 5 Experimental Protocol 5.1 Benchmarks LCBench: A benchmark that features 2,000 hyperparameter configurations that parametrize the archi- tecture of simple feedforward neural networks, as well as, the training pipeline [51]. The benchmark features 7 numerical hyperparameters and 35 different datasets from the AutoML benchmark [12]. PD1: A deep learning benchmark [45] that consists of recent DL (including Transformers) architec- tures run on large vision datasets such as CIFAR-10, CIFAR-100, ImageNet, as well as statistical modeling corpora and protein sequence datasets from bioinformatics. Every search space includes varying learning curve lengths, ranging from 5 to 1414, and a different number of evaluated hy- perparameter configurations ranging from 807 to 2807. The search space includes hyperparameter configurations that parametrize the learning rate, the learning rate scheduler, and the momentum. TaskSet: A benchmark that features different optimization tasks evaluated in 5 different search spaces [34]. For our work, we focus on the Adam8p search space, which is among the largest search spaces 4Algorithm 1: Multi-Fidelity HPO with Deep Power Laws Input : Search space Λ, initial design H(init), budget increment bstep Output :Best hyperparameter configuration λ∗ 1 Iteration i ← 0; Evaluate initial configurations and budgets H(i) := H(init); 2 while still budget do 3 Fit a DPL ensemble ˆf(1)(λ, b), . . . ,ˆf(K)(λ, b) from history H(i) using Equation 5; 4 Recommend the next configuration λi+1 and its budget bi+1 using Equation 6, 7 and 8; 5 Train λi+1 until bi+1 and measure the validation loss f (λi+1, bi+1); 6 Append to history Hi+1 ← Hi ∪ {(λi+1, bi+1, f(λi+1, bi+1))}; 7 i ← i + 1 ; 8 end 9 return Best configuration λ∗ with the smallest validation loss min (λ∗,b,f(λ∗,b))∈Hi f (λ∗, b) ; in the benchmark with 1000 hyperparameter configurations. Every hyperparameter configuration features 8 continuous hyperparameters. The hyperparameters control the learning rate, the learning rate schedulers, and the optimizer. For variety among our benchmarks, we focus on 12 RNN text classification tasks that feature different RNN cells, embedding sizes, and batch sizes. For a more detailed explanation of the benchmarks, we refer the reader to Appendix F. 5.2 Baselines Random Search stochastically samples hyperparameter configurations for the largest possible budget. Hyperband uses multiple brackets with different trade-offs of the initial budget and number of epochs to initially train [28]. It then applies Successive Halving (SH) [17] on every bracket. ASHA is an asynchronous version of SH [ 27] that does not wait for all configurations to finish in an SH bracket before starting the next one. Furthermore, BOHB is a follow-up of Hyperband that uses TPE [3] to sample the initial hyperparameter configurations of a bracket [9]. DEHB, on the other hand, modifies Hyperband by using evolutionary strategies to sample the initial hyperparameter configurations [1]. Similarly, multi-fidelity SMAC extends Hyperband but uses random forests to sample the initial hyperparameter configurations for a bracket [ 30]. We also use the Dragonfly Library [19] to compare against BOCA [18], a multi-fidelity method that uses Gaussian Processes to predict the next hyperparameter to evaluate, as well as the fidelity for which it should be evaluated. For all the baselines, we use their official public implementations. We provide additional details in Appendix G. 5.3 Architecture & Training For our method, we use an ensemble of 5 models, where every model consists of a 2-layer feedforward neural network with 128 units per layer and Leaky ReLU for the non-linearity. The architecture of our method is motivated by prior work [25]. Our network has 3 output units, that are then combined with the budget b to yield the power law output. We apply the GLU non-linearity activation only on the β and γ output units, allowing α to take any value. We use the L1 loss to train our network, coupled with Adam featuring an initial learning rate of 10−3. For the first 10 iterations of our multi-fidelity HPO method in Algorithm 1 we train every network of our ensemble for 250 epochs with randomly sampled initial weights. This choice helps the convergence of the weights in the early stage of HPO. Next, we continuously refine the model for 20 epochs every HPO iteration. However, if the optimization stagnates (surrogate fitting loss does not improve) for more than the LC Length + a buffer of 0.2 × LC Length, the training procedure is restarted with random weights, where every model is trained again for 250 epochs and then only refined. During the refining phase, the new data point at an HPO iteration (Line 9 at Algorithm 1) is sampled with repeat on every batch, to learn new data points equally compared to older data points. Since we are working with discrete search spaces, we evaluate the acquisition function exhaustively 5on every hyperparameter configuration. When dealing with continuous search spaces, the acquisition function can either be evaluated exhaustively on a discretized version of the search space, or in a gradient-based way. Our implementation of DPL is publicly available.1 5.4 Protocol In our experiments, we standardize the hyperparameter values by performing min-max scaling for our method and every baseline. If a baseline has a specific preprocessing protocol, we do not apply min-max scaling but we apply the protocol as suggested by the authors. The benchmarks do not support a common evaluation metric for configurations (i.e. the function f). As a consequence, the evaluation metric for LCBench is the balanced accuracy, for TaskSet the log-likelihood loss, while for PD1 the accuracy. Moreover, the benchmarks do not offer learning curves with a common step size. For LCBench and PD1, one step size is equivalent to one epoch, while for TaskSet one step size is 200 batches. The HPO budget is defined as the maximum number of steps needed to fully evaluate 20 hyperparameter configurations. In that context, one unit step of the HPO budget signifies training a particular configuration for one more optimization step (e.g. 200 batches in TaskSet or 1 epoch in LCBench). In the following experiments, for all methods, we report the regret of the best-found configuration as shown in Equation 9: R = f (λbest, bmax) − f (λoracle, bmax) (9) where the oracle is given as f (λoracle, bmax) := min \b f (λ, b) | (λ, b, f(λ, b)) ∈ H(D) ∧ b ≤ bmax\t , and H(D) corresponds to the set of all the exhaustively-evaluated hyperparameter configurations’ performances on a dataset D. If the oracle configuration is not known in advance for the search space, H(D) can be replaced with the history H(N) at the end of the HPO procedure. The only difference between f (λbest, bmax) and f (λoracle, bmax) is that the former only considers the history at the HPO step for which we are reporting the results. In short, the regret is the difference in the evaluation metric performance from the best-found hyperparameter configuration during optimization to the best possible hyperparameter configuration (oracle) on the dataset (in a minimization setting). On a dataset level, we report the average regret across 10 repetitions with different seeds. To be able to aggregate results over datasets, we report the averaged normalized regret. As normalization, we divide the regret by the difference between the performances of the best and the worst hyperparameter configuration on a dataset. Then we compute the mean of the normalized regrets across all the datasets of a benchmark. Moreover, in the experiments that report the average normalized regret over time, we provide results over normalized wall clock time. The wall clock time includes both the method’s overhead (i.e. training the surrogate ˆf and selecting the next hyperparameter configuration to evaluate) and the time taken to evaluate the selected hyperparameter configuration (i.e. evaluating f). Since the methods have different run times, we normalize the individual times by the time it took Random Search (the fastest non-model-based method) to complete the HPO optimization process. To provide a fair any-time comparison, we report results until the time it took Random Search to evaluate 20 hyperparameter configurations. Furthermore, when reporting the learning curve (LC) length fraction, we imply the fraction of the total learning curve length. LCBench and TaskSet have LCs of a fixed length for all datasets, corresponding to 51 epochs for LCBench and 50 epochs for TaskSet. In contrast, PD1 has varying LC lengths for different datasets. In our experiments, all methods start with a history H(init) of 1 randomly sampled hyperparameter configuration evaluated for 1 step/epoch in the case of multi-fidelity techniques (Hyperband, BOHB, DEHB, SMAC, ASHA, Dragonfly; descriptions in Section 5.2), or for the full budget for the black- box technique (Random Search). We ran experiments on a CPU cluster, where every node contains two Intel Xeon E5-2630v4 CPUs with 20 CPU cores running at 2.2 GHz. The total memory of every node is 120GB, and every experiment is limited to 2 cores which offer 12GB. 1https://github.com/releaunifreiburg/DPL 66 Research Hypotheses and Experiments /uni00000013/uni00000011/uni00000014/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000016/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018 /uni0000002f/uni00000026/uni00000003/uni0000002f/uni00000048/uni00000051/uni0000004a/uni00000057/uni0000004b/uni00000003/uni00000029/uni00000055/uni00000044/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000019 /uni00000013/uni00000011/uni0000001b/uni00000026/uni00000052/uni00000055/uni00000055/uni00000048/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni0000001d/uni00000003/uni00000028/uni00000056/uni00000057/uni00000011/uni00000003/uni00000059/uni00000056/uni00000011/uni00000003/uni00000037/uni00000055/uni00000058/uni00000048 /uni00000027/uni00000033/uni0000002f/uni00000026/uni00000052/uni00000051/uni00000047/uni00000003/uni00000031/uni00000031/uni00000033/uni0000002f/uni00000031/uni00000031/uni0000002a/uni00000033 Figure 1: Rank correlations of learning curve fore- casting models, which are given a fraction of the learning curve and estimate the remaining curve segment. DPL: Deep Power Law, Cond NN: Con- ditioned neural network,PL: Power Law,NN: Neu- ral Network, GP: Gaussian processes. Hypothesis 1: The power law assumption im- proves the quality of learning curve forecasting. In this experiment, we evaluate the predictive performance of forecasting models that given a fraction of the observed learning curve, esti- mate the remaining unobserved segment of the curve, on the LCBench benchmark. The results of Figure 1 compare three different forecasting models, concretely, neural networks (NN), Gaus- sian Processes (GP), and Power Law functions (PL). For the three variants (PL, NN, GP) we fitted one model on every learning curve of each hyperparameter configuration (i.e. given b in the x-axis estimate one ˆf(b) separately for every λ). For the other two variants (DPL and Cond NN) we fit a single forecasting model (not an ensem- ble) for all configurations, by conditioning the surrogate on the configuration (i.e. given b and λ, estimate ˆf (λ, b)). The purpose of the experiment is to assess whether a power law function regressor leads to superior predictive accuracy, compared to generic forecasting models, such as neural networks, or Gaussian processes. The evaluation metric of the experiment highlighted in Figure 1 is the rank correlation between the estimated performances at the end of the learning curve and the true performances. We notice that although a Power Law regressor has significantly fewer parameters than a neural network (3 to 288 parameters), PL still achieves higher predictive performance than NN. Furthermore, our conditioning of the power law function to the hyperparameter configuration further improves the predictive quality, as the difference between DLP and PL demonstrates. Lastly, we refer the reader to Appendix H, where we provide an analysis of the distributions for the absolute error rate between the DPL predictions and the ground truth values over the different LC length fractions, showing that DPL does not only retain the ranks but, it also accurately predicts the final performance. Based on the results, we consider Hypothesis 1 to be validated and that DPL is accurate in terms of learning curve forecasting. What about learning curves that do not follow a power law pattern? Although the provided empirical evidence in this section strongly suggests that the presented power law model can accurately forecast learning curves, it is also true that some learning curves have divergent behaviour that does not follow the power law assumption. As a consequence, we investigate two different ways to handle curves that do not follow the power law assumption: i) recently-proposed power law functions that include breaking points [ 5], or shifts in the curve [ 8], and ii) min-smoothing the learning curves to transform them into monotonically decreasing time series. In Appendix A we provide ample empirical evidence showing that although the alternative formulations achieve comparable performances, still they do not outperform our simpler power law formulation. The findings indicate that even though not all learning curves are power laws, most of them are, therefore a power law surrogate is efficient in forecasting the final performance of a partially-observed learning curve. As a result, the forthcoming experiments will provide further empirical evidence that our power law surrogates lead to state-of-the-art HPO results when deployed in the proposed multi-fidelity Bayesian optimization setup. Hypothesis 2: Our method DPL achieves state-of-the-art results in HPO. In Figure 2, we show the performance of the considered methods over the HPO budget, where DPL manages to outperform all the rival baselines. In the case of LCBench, DPL quickly finds well-performing hyperparameter configurations compared to the competitor methods and continues to discover even better configurations until the HPO process ends. Furthermore, we observe the same trend with TaskSet and PD1, where after ca. 25% of the HPO budget, our method DPL converges to a better regret compared to the baselines and increases the lead until HPO ends. For a detailed overview of the performances of all methods on all individual datasets, we point the reader to Appendix H. 7/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000011/uni00000015/uni00000018/uni00000013/uni00000011/uni00000018/uni00000013/uni00000013/uni00000011/uni0000001a/uni00000018/uni00000014/uni00000011/uni00000013/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000031/uni00000052/uni00000055/uni00000050/uni00000044/uni0000004f/uni0000004c/uni0000005d/uni00000048/uni00000047/uni00000003/uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni0000002f/uni00000026/uni00000025/uni00000048/uni00000051/uni00000046/uni0000004b /uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000011/uni00000015/uni00000018/uni00000013/uni00000011/uni00000018/uni00000013/uni00000013/uni00000011/uni0000001a/uni00000018/uni00000014/uni00000011/uni00000013/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000037/uni00000044/uni00000056/uni0000004e/uni00000036/uni00000048/uni00000057 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000011/uni00000015/uni00000018/uni00000013/uni00000011/uni00000018/uni00000013/uni00000013/uni00000011/uni0000001a/uni00000018/uni00000014/uni00000011/uni00000013/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000033/uni00000027/uni00000014 /uni00000027/uni00000033/uni0000002f/uni00000024/uni00000036/uni0000002b/uni00000024/uni00000025/uni00000032/uni0000002b/uni00000025/uni00000027/uni00000028/uni0000002b/uni00000025/uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c/uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047/uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050/uni00000036/uni00000030/uni00000024/uni00000026 Figure 2: DPL discovers better hyperparameter configurations than all rival baselines in terms of regret (distance to oracle). Solid curves and shaded regions represent the mean and standard error of the averaged normalized regret. /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019/uni0000001a/uni0000001b /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000036/uni00000030/uni00000024/uni00000026/uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000033/uni0000002f /uni0000002f/uni00000026/uni00000025/uni00000048/uni00000051/uni00000046/uni0000004b/uni00000023/uni00000018/uni00000013/uni00000008 /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019/uni0000001a/uni0000001b /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c/uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000033/uni0000002f /uni00000037/uni00000044/uni00000056/uni0000004e/uni00000036/uni00000048/uni00000057/uni00000023/uni00000018/uni00000013/uni00000008 /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019/uni0000001a/uni0000001b /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni00000036/uni00000030/uni00000024/uni00000026/uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000033/uni0000002f /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000033/uni00000027/uni00000014/uni00000023/uni00000018/uni00000013/uni00000008 /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019/uni0000001a/uni0000001b /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni00000027/uni00000028/uni0000002b/uni00000025/uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000027/uni00000033/uni0000002f /uni0000002f/uni00000026/uni00000025/uni00000048/uni00000051/uni00000046/uni0000004b/uni00000023/uni00000014/uni00000013/uni00000013/uni00000008 /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019/uni0000001a/uni0000001b /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni00000027/uni00000028/uni0000002b/uni00000025/uni00000036/uni00000030/uni00000024/uni00000026 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000033/uni0000002f /uni00000037/uni00000044/uni00000056/uni0000004e/uni00000036/uni00000048/uni00000057/uni00000023/uni00000014/uni00000013/uni00000013/uni00000008 /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019/uni0000001a/uni0000001b /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000024/uni00000036/uni0000002b/uni00000024/uni00000036/uni00000030/uni00000024/uni00000026 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000027/uni00000033/uni0000002f /uni00000033/uni00000027/uni00000014/uni00000023/uni00000014/uni00000013/uni00000013/uni00000008 Figure 3: The critical difference diagrams at 50% and 100% of the HPO budget. The ranks indicate the sorted position in terms of regret, aggregated across datasets (the lower the better). Thick horizontal lines highlight differences that are not statistically significant. /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000031/uni00000052/uni00000055/uni00000050/uni00000044/uni0000004f/uni0000004c/uni0000005d/uni00000048/uni00000047/uni00000003/uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni0000002f/uni00000026/uni00000025/uni00000048/uni00000051/uni00000046/uni0000004b /uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000013/uni00000011/uni00000017/uni00000013/uni00000013/uni00000011/uni00000019/uni00000013/uni00000013/uni00000011/uni0000001b/uni00000013/uni00000014/uni00000011/uni00000013/uni00000013 /uni00000031/uni00000052/uni00000055/uni00000050/uni00000044/uni0000004f/uni0000004c/uni0000005d/uni00000048/uni00000047/uni00000003/uni0000003a/uni00000044/uni0000004f/uni0000004f/uni00000057/uni0000004c/uni00000050/uni00000048 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000031/uni00000052/uni00000055/uni00000050/uni00000044/uni0000004f/uni0000004c/uni0000005d/uni00000048/uni00000047/uni00000003/uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000033/uni00000027/uni00000014 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 Figure 4: The average normalized regret of DPL and rival methods over the normalized time for all the considered benchmarks. Solid curves and shaded regions represent the mean and standard average normalized regret. In addition, Figure 3 provides the critical differ- ence diagrams of the per-dataset regret ranks at 50% and 100% of the HPO budget. Our method DPL outperforms all baselines in 5 out of 6 cases (in 4 of which with a statistically significant mar- gin), while being second best only at the 50% of the HPO budget on the PD1 benchmark. We investigate the lack of statistical significance in PD1, by analyzing the individual dataset perfor- mances where DPL performs worse compared to other baselines. We notice that the datasets have a skewed distribution of hyperparameter configuration performances, where, a majority of the configurations achieve top performance. Based on the results, we conclude that a lack of statistical significance is the outcome of a search space that includes relatively simple optimiza- tion tasks and not a specific failure state of our method. We provide the detailed results of our analysis in Appendix D. Lastly, we analyse the performance of DPL over time in Figure 4. As it can be observed, DPL manages to outperform the competitors even when the method’s overhead time is included, showing that the overhead of DPL (i.e. fitting surrogate and running the acquisition) is negligible in terms of the quality of the HPO results. For a more detailed information, regarding the DPL overhead time, we point to Appendix E. TaskSet is not included in Figure 4 since the benchmark does not offer runtimes. Given the results, we conclude that Hypothesis 2 is validated and that DPL achieves state-of-the-art performance in HPO. 80.0 0.2 0.4 0.6 0.8 1.0 LC Length Fraction 0.0 0.2 0.4 0.6 Precision of Top Candidates LCBench 0.0 0.2 0.4 0.6 0.8 1.0 LC Length Fraction 0.05 0.10 0.15Average Regret LCBench 0.0 0.2 0.4 0.6 0.8 1.0 LC Length Fraction 0.0 0.2 0.4 0.6 0.8 Fraction of Poor Performer Promotions LCBench 0.0 0.2 0.4 0.6 0.8 1.0 LC Length Fraction 0.00 0.05 0.10 0.15 0.20 0.25 0.30 Precision of Top Candidates TaskSet 0.0 0.2 0.4 0.6 0.8 1.0 LC Length Fraction 0.15 0.20 0.25 0.30Average Regret TaskSet 0.0 0.2 0.4 0.6 0.8 1.0 LC Length Fraction 0.0 0.1 0.2 0.3 0.4 0.5 Fraction of Poor Performer Promotions TaskSet DPL ASHA BOHB DEHB Dragonfly Hyperband Random SMAC BaselineDPL ASHA BOHB DEHB Dragonfly Hyperband Random SMAC Baseline Figure 5: Post-hoc analysis to study DPL’s efficiency. Left: Share of the best candidates selected during training. Middle: Average regret of configurations chosen to be trained at each budget. Right: Share of top third configurations at a given budget which were bottom two third configurations at a previous budget. Hypothesis 3: DPL explores the search space more efficiently compared to the baselines. We conduct further analyses to understand the source of the efficiency of DPL versus the baselines. As a result, we analyze two important aspects, the quality of the evaluated configurations, as well as the exploration capability of our multi-fidelity HPO method. Initially, we measure what fraction of the top 1% configurations (ranked by accuracy) can our method discover. Figure 5 (left) shows that until convergence our method can discover significantly more top configurations compared to the baselines. The middle plots of Figure 5, show the average regret for each configuration promoted to the respective budget. According to the plot, DPL is more efficient and assigns the budget only to configurations with lower regret compared to the other methods. The precision and regret plots demonstrate that the quality of the evaluated configurations is largely better than all baselines, therefore, giving our method a significant lift in the performance rank. Last but not least, the right plot shows the percentage of configurations that were performing poorly in an earlier epoch (i.e. accuracy-wise in the bottom 2/3 of configurations up to the epoch indicated at the x-axis) but performed better at later epochs (i.e. at the top 1/3 of configurations). Furthermore, we added a line labeled with \"Baseline\", which represents the fraction of previously poor-performing configurations of all configurations. This behavior is observed often with learning curves, where for instance, strongly regularized networks converge slowly. For the same analysis regarding the PD1 benchmark, we point the reader to Appendix H. The results indicate that our method explores well the unpromising early configurations, by consider- ing them through the uncertainty estimation of our ensemble and the respective Bayesian optimization mechanism. The results validate Hypothesis 3 and confirm that DPL explores the search space more efficiently. Hypothesis 4: Our method DPL offers an effective tool for HPO in Large Language Models. In this experiment, we consider the case of tuning the hyperparameters of transformers in Large Language Models. To this end, we computed a tabular benchmark by training a smaller GPT-2 [39] model on the OpenWebText dataset [13] for a series of different hyperparameter configurations. We tune three learning rate hyperparameters: the fraction of warmup steps, the maximum learning rate at the end of warmup, and the minimum learning rate at the end of the decay. We repeat the experiments for seven model sizes ranging from 0.3M to 30M total parameters, ablating the embedding size of the multi-head attention layers (details in Appendix B). We follow the common practice of conducting HPO with small transformers and then deploying the discovered optimal configuration on the full-scale transformers [50]. As a result, we search for the optimal hyperparameters of small transformers (embedding size of {6, 12, . . . ,96, 192}) and then evaluate the discovered configurations at a full-scale transformer with an embedding size of 384. 90 3 6 9 12 6.20 6.40 6.60 6.80 7.00Validation Error Embedding size = 12 0 4 8 12 16 HPO Budget [GPU-hours] 6.00 7.00 Embedding size = 48 0 6 12 18 24 5.00 6.00 7.00 Embedding size = 192 12 48 192 3.90 3.92 3.95 3.98Error at Largest Scale Normalized HPO budget = 25% 12 48 192 Small Transformer Scale [embedding size] 3.90 3.92 3.95 3.98 Normalized HPO budget = 50% 12 48 192 3.90 3.92 3.94 3.96 Normalized HPO budget = 100% DPL BOHB Random oracle at small scale oracle at largest scale Figure 6: HPO for Transformer architectures. Top: HPO on small-scale transformers in terms of the embedding size. Bottom: Error on the full-scale transformer, using the hyperparameter configuration discovered by conducting HPO using the small transformers. We present three analyses, ablating the HPO time on the small-scale transformer up to the HPO budget of 2 full function evaluations. Figure 6 shows the HPO results of DPL against Random Search and BOHB (a rival multi-fidelity HPO baseline). In the top row of plots, we observe the performance of the discovered configurations at the small transformers for the indicated embedding size. We observe that our method finds better configurations than the baselines at any proxy space with small embedding sizes. On the other hand, the bottom row of plots presents the performance of the discovered configurations in the small embedding space, by applying such hyperparameter configurations to the full-scale transformers. We observe that the configurations discovered by DPL on the small search space achieve very competitive results on the full-scale transformers, finding the oracle configuration of the full-scale transformers in the majority of cases. It takes DPL 3.6 hours to find the oracle configuration for the largest model via HPO for the smallest model. In turn, it takes 21.52 hours to train the largest model only once. For more details, we refer the reader to Appendix B. The results validate Hypothesis 4 and confirm that DPL is an efficient HPO technique for tuning the hyperparameters of large language models when the HPO is conducted using smaller transformer model sizes. 7 Conclusions Summary. In this work, we introduce Deep Power Law (DPL), a probabilistic surrogate based on an ensemble of power law functions. The proposed surrogate is used within a novel multi-fidelity Hyperparameter Optimization (HPO) method based on Bayesian optimization. In contrast to the prior work, we exploit scaling laws for estimating the performance of Deep Learning (DL) models. Through extensive experiments comprising 7 baselines, 59 datasets, and search spaces of diverse deep learning architectures, we show that DPL outperforms strong HPO baselines for DL by a large margin. As an overarching contribution, we advance the state-of-the-art in the important field of HPO for DL. Limitations and future work. Contrary to the common perception, we experienced that the uncertainty estimation arising from the Deep Ensemble approach [ 25] is suboptimal compared to standard BO surrogates such as Gaussian Processes. In addition, having to train an ensemble has additional computational costs, due to the necessity of training multiple power law models. In the future, we plan to investigate the combination of power laws with Gaussian Processes, as well as investigate additional fidelity types. 10Acknowledgements JG, AK and MJ would like to acknowledge the grant awarded by the Eva-Mayr-Stihl Stiftung. In addition, this research was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under grant number 417962828 and grant INST 39/963-1 FUGG (bwForCluster NEMO). In addition, JG and AK acknowledge the support of the BrainLinks-BrainTools center of excellence. References [1] Noor H. Awad, Neeratyoy Mallik, and Frank Hutter. DEHB: evolutionary hyberband for scalable, robust and efficient hyperparameter optimization. In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI 2021, Virtual Event / Montreal, Canada, 19-27 August 2021, pages 2147–2153, 2021. [2] Bowen Baker, Otkrist Gupta, Ramesh Raskar, and Nikhil Naik. Accelerating neural archi- tecture search using performance prediction. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Workshop Track Proceedings, 2018. [3] James Bergstra, Rémi Bardenet, Yoshua Bengio, and Balázs Kégl. Algorithms for hyper- parameter optimization. Advances in neural information processing systems, 24, 2011. [4] Benedetto J. Buratti and Eli Upfal. Ordalia: Deep learning hyperparameter search via general- ization error bounds extrapolation. In 2019 IEEE International Conference on Big Data (Big Data), pages 180–187, 2019. [5] Ethan Caballero, Kshitij Gupta, Irina Rish, and David Krueger. Broken neural scaling laws, 2022. [6] Akshay Chandrashekaran and Ian R. Lane. Speeding up hyper-parameter optimization by extrapolation of learning curves using previous builds. In Machine Learning and Knowledge Discovery in Databases - European Conference, ECML PKDD 2017, Skopje, Macedonia, September 18-22, 2017, Proceedings, Part I , volume 10534 of Lecture Notes in Computer Science, pages 477–492. Springer, 2017. [7] Yutian Chen, Matthew W. Hoffman, Sergio Gomez Colmenarejo, Misha Denil, Timothy P. Lillicrap, Matthew Botvinick, and Nando de Freitas. Learning to learn without gradient descent by gradient descent. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pages 748–756, 2017. [8] Tobias Domhan, Jost Tobias Springenberg, and Frank Hutter. Speeding up automatic hy- perparameter optimization of deep neural networks by extrapolation of learning curves. In Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence, IJCAI 2015, Buenos Aires, Argentina, July 25-31, 2015, pages 3460–3468. AAAI Press, 2015. [9] Stefan Falkner, Aaron Klein, and Frank Hutter. BOHB: robust and efficient hyperparameter optimization at scale. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, pages 1436–1445, 2018. [10] Luca Franceschi, Michele Donini, Paolo Frasconi, and Massimiliano Pontil. Forward and reverse gradient-based hyperparameter optimization. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017 , pages 1165–1173, 2017. [11] Behrooz Ghorbani, Orhan Firat, Markus Freitag, Ankur Bapna, Maxim Krikun, Xavier Garcia, Ciprian Chelba, and Colin Cherry. Scaling laws for neural machine translation. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. [12] Pieter Gijsbers, Erin LeDell, Janek Thomas, Sébastien Poirier, Bernd Bischl, and Joaquin Vanschoren. An open source automl benchmark. arXiv preprint arXiv:1907.00909, 2019. 11[13] Aaron Gokaslan*, Vanya Cohen*, Ellie Pavlick, and Stefanie Tellex. Openwebtext corpus. http://Skylion007.github.io/OpenWebTextCorpus, 2019. [14] Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory F. Diamos, Heewoo Jun, Hassan Kianinejad, Md. Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling is predictable, empirically. CoRR, abs/1712.00409, 2017. [15] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katherine Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Oriol Vinyals, Jack William Rae, and Laurent Sifre. An empirical analysis of compute-optimal large language model training. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. [16] Achin Jain, Gurumurthy Swaminathan, Paolo Favaro, Hao Yang, Avinash Ravichandran, Hrayr Harutyunyan, Alessandro Achille, Onkar Dabeer, Bernt Schiele, Ashwin Swaminathan, and Stefano Soatto. A meta-learning approach to predicting performance and data requirements. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 3623–3632, June 2023. [17] Kevin G. Jamieson and Ameet Talwalkar. Non-stochastic best arm identification and hyper- parameter optimization. In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, AISTATS 2016, Cadiz, Spain, May 9-11, 2016, pages 240–248, 2016. [18] Kirthevasan Kandasamy, Gautam Dasarathy, Jeff G. Schneider, and Barnabás Póczos. Multi- fidelity bayesian optimisation with continuous approximations. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pages 1799–1808, 2017. [19] Kirthevasan Kandasamy, Karun Raju Vysyaraju, Willie Neiswanger, Biswajit Paria, Christo- pher R. Collins, Jeff Schneider, Barnabás Póczos, and Eric P. Xing. Tuning hyperparameters without grad students: Scalable and robust bayesian optimisation with dragonfly. J. Mach. Learn. Res., 21:81:1–81:27, 2020. [20] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. [21] Andrej Karpathy. nanoGPT. https://github.com/karpathy/nanoGPT, 2023. [22] Abdus Salam Khazi, Sebastian Pineda Arango, and Josif Grabocka. Deep ranking ensembles for hyperparameter optimization. In The Eleventh International Conference on Learning Representations, 2023. [23] Aaron Klein, Stefan Falkner, Jost Tobias Springenberg, and Frank Hutter. Learning curve prediction with bayesian neural networks. In 5th International Conference on Learning Rep- resentations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings, 2017. [24] Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009. [25] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable pre- dictive uncertainty estimation using deep ensembles. In I. Guyon, U. V on Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors,Advances in Neural Informa- tion Processing Systems, volume 30. Curran Associates, Inc., 2017. [26] Benjamin Lefaudeux, Francisco Massa, Diana Liskovich, Wenhan Xiong, Vittorio Caggiano, Sean Naren, Min Xu, Jieru Hu, Marta Tintore, Susan Zhang, Patrick Labatut, and Daniel Haziza. xformers: A modular and hackable transformer modelling library. https://github.com/ facebookresearch/xformers, 2022. 12[27] Liam Li, Kevin Jamieson, Afshin Rostamizadeh, Ekaterina Gonina, Moritz Hardt, Benjamin Recht, and Ameet Talwalkar. Massively parallel hyperparameter tuning. arXiv preprint arXiv:1810.05934, 5, 2018. [28] Lisha Li, Kevin G. Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. Hyperband: A novel bandit-based approach to hyperparameter optimization. J. Mach. Learn. Res., 18:185:1–185:52, 2017. [29] Shibo Li, Wei Xing, Robert M. Kirby, and Shandian Zhe. Multi-fidelity bayesian optimization via deep neural networks. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. [30] Marius Lindauer, Katharina Eggensperger, Matthias Feurer, André Biedenkapp, Difan Deng, Carolin Benjamins, Tim Ruhkopf, René Sass, and Frank Hutter. Smac3: A versatile bayesian optimization package for hyperparameter optimization. Journal of Machine Learning Research (JMLR) – MLOSS, 23(54):1–9, 2022. [31] Jonathan Lorraine, Paul Vicol, and David Duvenaud. Optimizing millions of hyperparameters by implicit differentiation. In The 23rd International Conference on Artificial Intelligence and Statistics, AISTATS 2020, 26-28 August 2020, Online [Palermo, Sicily, Italy], pages 1540–1552, 2020. [32] Dougal Maclaurin, David Duvenaud, and Ryan P. Adams. Gradient-based hyperparameter optimization through reversible learning. In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, pages 2113–2122, 2015. [33] Rafid Mahmood, James Lucas, Jose M. Alvarez, Sanja Fidler, and Marc Law. Optimizing data collection for machine learning. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems , volume 35, pages 29915–29928. Curran Associates, Inc., 2022. [34] Luke Metz, Niru Maheswaranathan, Ruoxi Sun, C. Daniel Freeman, Ben Poole, and Jascha Sohl-Dickstein. Using a thousand optimization tasks to learn hyperparameter search strategies. CoRR, abs/2002.11887, 2020. [35] Jonas Mockus, Vytautas Tiesis, and Antanas Zilinskas. The application of Bayesian methods for seeking the extremum. Towards Global Optimization, 2(117-129):2, 1978. [36] Felix Mohr and Jan N van Rijn. Learning curves for decision making in supervised machine learning–a survey. arXiv preprint arXiv:2201.12150, 2022. [37] OpenAI. Gpt-4 technical report, 2023. [38] Jack Parker-Holder, Vu Nguyen, and Stephen J. Roberts. Provably efficient online hyperparam- eter optimization with population-based bandits. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. [39] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019. [40] Jonathan S. Rosenfeld, Jonathan Frankle, Michael Carbin, and Nir Shavit. On the predictability of pruning across scales. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 9075–9083. PMLR, 2021. [41] Jonathan S. Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. A constructive prediction of the generalization error across scales. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. [42] David Salinas, Matthias Seeger, Aaron Klein, Valerio Perrone, Martin Wistuba, and Cedric Archambeau. Syne tune: A library for large scale hyperparameter tuning and reproducible research. In First Conference on Automated Machine Learning (Main Track), 2022. 13[43] Mingxing Tan and Quoc Le. EfficientNetV2: Smaller models and faster training. github.com/ google/automl/tree/master/efficientnetv2, 2021. [44] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V on Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors,Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. [45] Zi Wang, George E Dahl, Kevin Swersky, Chansoo Lee, Zelda Mariet, Zachary Nado, Justin Gilmer, Jasper Snoek, and Zoubin Ghahramani. Pre-training helps bayesian optimization too. arXiv preprint arXiv:2207.03084, 2022. [46] Ross Wightman. PyTorch image models. https://github.com/rwightman/ pytorch-image-models, 2019. [47] Martin Wistuba, Arlind Kadra, and Josif Grabocka. Supervising the multi-fidelity race of hyperparameter configurations. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. [48] Martin Wistuba and Tejaswini Pedapati. Learning to rank learning curves. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, pages 10303–10312, 2020. [49] Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1492–1500, 2017. [50] Greg Yang, Edward J. Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, and Jianfeng Gao. Tensor programs V: tuning large neural networks via zero-shot hyperparameter transfer. CoRR, abs/2203.03466, 2022. [51] Lucas Zimmer, Marius Lindauer, and Frank Hutter. Auto-pytorch tabular: Multi-fidelity metalearning for efficient and robust autodl. IEEE Transactions on Pattern Analysis and Machine Intelligence, 43(9):3079 – 3090, 2021. 14A Modeling /uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000011/uni00000015/uni00000018/uni00000013/uni00000011/uni00000018/uni00000013/uni00000013/uni00000011/uni0000001a/uni00000018/uni00000014/uni00000011/uni00000013/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000031/uni00000052/uni00000055/uni00000050/uni00000044/uni0000004f/uni0000004c/uni0000005d/uni00000048/uni00000047/uni00000003/uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni0000002f/uni00000026/uni00000025/uni00000048/uni00000051/uni00000046/uni0000004b /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000026/uni00000044/uni00000051/uni00000047/uni0000004c/uni00000047/uni00000044/uni00000057/uni00000048/uni00000003/uni00000014 /uni00000026/uni00000044/uni00000051/uni00000047/uni0000004c/uni00000047/uni00000044/uni00000057/uni00000048/uni00000003/uni00000015 /uni00000025/uni00000055/uni00000052/uni0000004e/uni00000048/uni00000051/uni00000003/uni0000002f/uni00000044/uni0000005a /uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000011/uni00000015/uni00000018/uni00000013/uni00000011/uni00000018/uni00000013/uni00000013/uni00000011/uni0000001a/uni00000018/uni00000014/uni00000011/uni00000013/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000031/uni00000052/uni00000055/uni00000050/uni00000044/uni0000004f/uni0000004c/uni0000005d/uni00000048/uni00000047/uni00000003/uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni0000002f/uni00000026/uni00000025/uni00000048/uni00000051/uni00000046/uni0000004b /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000036/uni00000033/uni0000002f Figure 7: Left: The performance of different power law formulations, as well as, the baselines on the LCBench benchmark over the HPO budget. Right: The performance of the power law formulation when min-smoothing is applied to the learning curve to ensure a monotonically decreasing learning curve. Label Formula DPL α+β · b−γ Candidate 1 α−β · (b +d)−γ Candidate 2 α−β · (e · b +d)−γ Broken Law α+β · b−γ · \u0010 1 + \u0010 b d 1 f \u0011\u0011−c·f Table 1: Alternative power law formulations for the DPL surrogate. To inspect the modeling choices of the power law functions used as our surrogate, we con- sider different formulations for the ensemble members of our surrogate as shown in Table 1. Initially, we consider Candidate 1 which can handle shifts in the learning curve by introduc- ing an additional parameter d. Furthermore, we consider a more complex version, Candidate 2, that allows us to additionally scale the budget, by introducing variable e. Lastly, we consider Broken Laws [5] (BPL), which can handle breaking points in the learning curve. We use a version that can handle one breaking point since the authors of the method suggest it as a sufficient formulation to approximate the majority of cases. We run the DPL surrogate with every individual formulation on the LCBench benchmark to investigate their empirical performance. Figure 7 presents the results, where, our chosen surrogate formulation (DPL) despite being the simplest, outperforms all other formulations. The Candidate 2 formulation does not manage to outperform all competitor methods, the Candidate 1 formulation manages to outperform all rival methods, however, only with a marginal difference. The Broken Law formulation manages to outperform all the rival baselines by a considerable margin, however, it still performs worse than the simple power law formulation used for DPL. We would like to point out that the alternative power law formulations are more difficult to opti- mize/run since they are prone to diverge and fall into a dead state given different combinations of parameter values. The most common is division by zero for e.g. d term in the Broken Law formulation, taking the root of a negative number b + d term in Candidate 1, e · b + d for Candidate 2 and the d term in Broken Law. We additionally consider min-smoothing the learning curve y, by taking at each step b of the learning curve the minimal value of the learning curve ysmooth b where, ysmooth b = min (y0, y1, ..., yb). The min-smoothing allows diverging curves to be formulated as power laws since the diverging phase will be substituted with stagnation. Figure 7 shows that incorporating learning curve min-smoothing for our surrogate (SPL) performs comparably to DPL without learning curve smoothing and manages to beat the other HPO baselines. 15B nanoGPT-Bench In recent years, deep learning research has increasingly focused on large-scale models, particularly Large Language Models (LLMs) like the Generative Pre-trained Transformers (GPTs). To evaluate the effectiveness of search algorithms, we propose a benchmark based on the nanoGPT model [21], reproducing the performance of the small GPT-2 [39] model trained on the OpenWebText dataset [13]. Our experimental setup is designed with the practicalities of real-world hardware constraints in mind. The common practice in the field is to perform Hyperparameter Optimization (HPO) on an informative proxy task that adheres to model size scaling laws [37], and then to apply these optimized parameters to larger models. This pragmatic approach is necessitated by the fact that training larger models would require significantly more expensive computational resources and time. Within these constraints, we focus our experiments on NVIDIA RTX 2080 GPUs. Warmup Max Steps Learning Rate Min Max Figure 8: nanoGPT-Bench search space parametrization. Baseline: We train a small nanoGPT model, a scaled-down variant of the small GPT-2, reducing the parameter count to approximately 30 M from the original 124 M parameters. The model architecture includes 6 transformer layers and 6 attention heads, and the embedding size is set to384. The AdamW optimizer is utilized for training, with the first and second moment estimates configured to 0.9 and 0.98, respectively. The weight decay is set to 10−1, and we apply gradient clipping at a value of 1.0 to prevent large gradients from causing instability in the model training. For the training process, we optimize the cross-entropy loss for next-token prediction. The process involves 350 steps, with each step encompassing 1000 random samples, with a batch size of 12. Each data point has a context size of 512 tokens, encoded using OpenAI’s token embeddings (sized of 50304). This procedure ensures that even the most resource-intensive experiments stay within the limits of a single GPU day. HP Values Max LR [10−5; 10−4; 10−3] Min LR [1%; 10%] of Max LR Warmup Steps [10%; 20%] of Budget Table 2: Search space of nanoGPT- Bench. Search Space: Our hyperparameter search space con- struction involves varying the number of warmup steps, along with the maximum and minimum learning rates for the cosine scheduler. The specific parametrization of the scheduler is illustrated in Figure 8. The discretized choices are presented in detail in Table 2. Fidelity Space: To construct the fidelity space, we focus on two key dimensions: the number of training steps and the transformer’s embedding size, serving as a proxy for model size. In this exploration, the natural fidelity of the number of training steps is visualized by the validation curves during model training as depicted in Figure 9. On the other hand, the end performance correlation between the different fidelities is reflected by the Pearson correlation in Table 3. We establish proxy tasks by sampling embedding size from a log scale,{6, 12, . . . ,96, 192}, with the maximum being 384. Consequently, each configuration has 6 proxy and one target task, leading to a total of 84 unique configurations in our full benchmark. Every configuration is trained for 350 steps. 1650 200 350 8 10 Embedding size = 6 50 200 350 Steps 6 8 10 Embedding size = 12 50 200 350 6 8 10 Embedding size = 24 50 200 350 6 8 10Validation Error Embedding size = 48 50 200 350 Steps 6 8 10 Embedding size = 96 50 200 350 4 6 8 10 Embedding size = 192 50 200 350 Steps 4 6 8 10 Embedding size = 384 Figure 9: Validation loss curves during model training for all nanoGPT-Bench configurations across model fidelity values. 6 48 96 192 384 0 1 2 3Size [bytes] 1e7 6 48 96 192 384 Small Transformer Scale [embedding size] 0.00 0.25 0.50 0.75 1.00FLOPS 1e11 6 48 96 192 384 10 15 20Runtime [GPU-hours] Figure 10: Scaling of model size in relation to bytes, FLOPS, and runtime based on average values across all nanoGPT-Bench configurations. Due to the GPU under-utilization of small model sizes, the runtime scales linearly as the model size scales exponentially. This relationship can be observed in Figure 10 and Figure 11. We expect the runtime to scale in a linear proportion to the model size when larger models are considered. Embedding Size Correlation 6 0 .951 12 0 .880 24 0 .971 48 0 .955 96 0 .987 192 0 .994 384 1 .000 Table 3: Pearson correlation across 7 fidelities. Figure 12 illustrates the effectiveness of DPL, particularly when the number of training steps is considered as a fidelity dimension. Vertical dotted lines denote the iteration at which an algo- rithm identifies the oracle value with an absolute tolerance of 0.01. Figure 13 depicts the results over the different values of the embedding fidelity. We utilized DPL, BOHB, and random search in proxy tasks, incrementing the budget allocation over each run, up to a horizon of 6 full-function evalua- tions. From these proxy tasks, we extracted the incumbent hyperparameters and evaluated their performance on the target task, that correponds to the maximum embedding size of 384. Despite operating within short regimes, DPL consistently outperforms baselines in terms of the mean incumbent value and the explored regime, as evidenced by error bars indicating the range between best and worst incumbents across 10 seeds. It should be noted, however, that the correlation between 176.7 6.8 6.9 7.0 7.10 1 2 3 Embedding size = 6 7.0 7.1 7.2 7.3 7.4 Runtime [GPU-hours] 0 2 4 Embedding size = 12 7.2 7.3 7.4 7.50 1 2 3 Embedding size = 24 7.95 8.00 8.05 8.10 8.150 1 2 3Count Embedding size = 48 8.90 8.95 9.00 9.05 9.10 9.15 Runtime [GPU-hours] 0 1 2 3 Embedding size = 96 13.6 13.7 13.8 13.9 14.00 1 2 3 Embedding size = 192 21.4 21.6 21.8 Runtime [GPU-hours] 0 1 2 3 Embedding size = 384 Figure 11: Distribution of GPU-hours required for training across different model fidelity values. 0 10 20 30 40 6.60 6.70 6.80 6.90 7.00 Embedding size = 6 0 10 20 30 40 HPO Budget [GPU-hours] 6.20 6.40 6.60 6.80 7.00 Embedding size = 12 0 10 20 30 40 6.00 7.00 Embedding size = 24 0 10 20 30 40 6.00 7.00Validation Error Embedding size = 48 0 15 30 45 HPO Budget [GPU-hours] 5.00 6.00 7.00 Embedding size = 96 0 20 40 60 80 5.00 6.00 7.00 Embedding size = 192 0 30 60 90 120 HPO Budget [GPU-hours] 4.00 5.00 6.00 7.00 Embedding size = 384 DPL BOHB Random oracle at small scale Figure 12: The incumbent performance of DPL and other baselines during the HPO budget of 6 full function evaluations for different values of the embedding size fidelity. Dashed lines indicate the point at which the oracle has been evaluated for every algorithm. Solid curves and shaded areas stand for mean value across runs and standard error. proxy and target tasks is not always perfect. This can result in a proxy task incumbent that does not translate to the oracle in the target task, which can be observed particularly at lower fidelity levels. 186 12 24 48 96 192 4.00 4.20 4.40Error at Largest Scale HPO budget = 1 full func evals 6 12 24 48 96 192 Small Transformer Scale [embedding size] 3.90 3.95 4.00 HPO budget = 2 full func evals 6 12 24 48 96 192 3.90 3.95 4.00 HPO budget = 3 full func evals 6 12 24 48 96 192 3.90 3.95 4.00Error at Largest Scale HPO budget = 4 full func evals 6 12 24 48 96 192 Small Transformer Scale [embedding size] 3.90 3.95 4.00 HPO budget = 5 full func evals 6 12 24 48 96 192 3.90 3.95 4.00 HPO budget = 6 full func evals DPL BOHB Random oracle at largest scale Figure 13: The target task performance distribution for DPL and other methods over different HPO budgets ranging from 1 − 6 full function evaluations. C Continuous Search Space 0.00 0.25 0.50 0.75 1.00 HPO Budget 0.1 0.3 0.6 0.9Incumbent Error DPL BOHB Random SMAC Figure 14: The incumbent error of DPL, as well as, the baselines on the CIFAR10 task over the HPO budget. The primary objective of this study is to investigate the efficacy of Deep Power Laws (DPL) in trading off exploration vs exploitation in a continuous HPO search space. In this study, we do not make use of pre-computed tabular tables, but instead we optimize the hyperparameters of an EfficientNetV2 model online, by iteratively pausing unpromising configurations and moving forward only promising hyperparameter configurations during the HPO optimization procedure. To benchmark our findings, we contrast the results against established baseline algorithms such as random search, BOHB, and SMAC. Baseline: We employ EfficientNetV2 [43] as a benchmarking model and train it on the CIFAR10 dataset [24]. Specifically, we train the lightweight variant of EfficientNetV2-b0 from scratch for 50 epochs, using the RMSprop optimizer. The learning rate is initiated at 10−6 and gradually increased over a span of five warmup epochs to reach the learning rate value of5 ·10−4. Following the warmup phase, we employ a cosine learning rate scheduler, with a decay factor of 0.97 applied every 10 epochs. The weight decay is set at 10−5, with no momentum used. Furthermore, the dropout rate is configured to be 10−6 and the model’s moving average exponential decay is set at0.9996. During the training phase, the batch size is set to 64, while for the validation phase, it is reduced to 8. All experiments are performed using the timm library [46]. 19HP Values LR [10−5, 10−2] weight decay [0, 10−1] Table 4: Search space of CIFAR10 task. Search Space: In our experiment, we concen- trate on optimizing the two most critical hy- perparameters, learning rate, and weight decay, while keeping the remaining hyperparameters fixed as per the baseline model. We construct a search space for these two hyperparameters in accordance with common practices (Table 4). To emulate a continuous search space for the acquisition function, we generate 100 equally-sized steps on a logarithmic scale from the lower bound to the upper bound of each dimension. This process yields a search space comprising 104 potential configurations. Our method demonstrates a substantial speedup in terms of anytime performance when compared to baseline algorithms (Figure 14). Acknowledging the practical constraints of evaluating large-scale models, we pragmatically allocated an HPO budget for a maximum of 3 full-function evaluations, equivalent to 150 epochs. The results of our exploration underline the compelling potential of the DPL algorithm to effectively manage HPO tasks within a continuous search space. Exhibiting significant speedup gains, DPL proves itself to be not only viable but an efficient method for identifying optimal hyperparameters. The findings further underscore the adaptability and efficacy of DPL in addressing complex HPO tasks, reinforcing its standing as a valuable tool in the machine learning toolbox. D PD1 Investigation /uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000016/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000011/uni0000001b/uni00000013/uni00000011/uni0000001c /uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c /uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000016/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013 /uni00000018/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000029/uni00000055/uni00000048/uni00000054/uni00000058/uni00000048/uni00000051/uni00000046/uni0000005c /uni00000049/uni00000044/uni00000056/uni0000004b/uni0000004c/uni00000052/uni00000051/uni00000042/uni00000050/uni00000051/uni0000004c/uni00000056/uni00000057/uni00000042/uni00000050/uni00000044/uni0000005b/uni00000042/uni00000053/uni00000052/uni00000052/uni0000004f/uni0000004c/uni00000051/uni0000004a/uni00000042/uni00000046/uni00000051/uni00000051/uni00000042/uni00000057/uni00000044/uni00000051/uni0000004b/uni00000042/uni00000045/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000042/uni00000056/uni0000004c/uni0000005d/uni00000048/uni00000042/uni00000015/uni00000018/uni00000019 /uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000016/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000011/uni0000001b/uni00000013/uni00000011/uni0000001c/uni00000014/uni00000011/uni00000013 /uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c /uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000016/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013 /uni00000018/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013 /uni0000001a/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000029/uni00000055/uni00000048/uni00000054/uni00000058/uni00000048/uni00000051/uni00000046/uni0000005c /uni00000056/uni00000059/uni0000004b/uni00000051/uni00000042/uni00000051/uni00000052/uni00000042/uni00000048/uni0000005b/uni00000057/uni00000055/uni00000044/uni00000042/uni0000005a/uni0000004c/uni00000047/uni00000048/uni00000042/uni00000055/uni00000048/uni00000056/uni00000051/uni00000048/uni00000057/uni00000042/uni00000045/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000042/uni00000056/uni0000004c/uni0000005d/uni00000048/uni00000042/uni00000014/uni00000013/uni00000015/uni00000017 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000014/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000016/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000011/uni0000001b /uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c /uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000016/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013 /uni00000018/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000029/uni00000055/uni00000048/uni00000054/uni00000058/uni00000048/uni00000051/uni00000046/uni0000005c /uni00000046/uni0000004c/uni00000049/uni00000044/uni00000055/uni00000014/uni00000013/uni00000013/uni00000042/uni0000005a/uni0000004c/uni00000047/uni00000048/uni00000042/uni00000055/uni00000048/uni00000056/uni00000051/uni00000048/uni00000057/uni00000042/uni00000045/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000042/uni00000056/uni0000004c/uni0000005d/uni00000048/uni00000042/uni00000015/uni00000018/uni00000019 Figure 15: Configuration test performance histograms for datasets where DPL does not outperform baselines. We investigate the datasets: fashion_mnist_max_pooling_cnn_tanh_batch_size_256, cifar100_wide_resnet_batch_size_256, svhn_no_extra_wide_resnet_batch_size_1024, where DPL does not outperform other methods for the PD1 benchmark as shown in Figure 23. We analyze the test performance of the individual hyperparameter configurations that belong to the aforementioned datasets. Figure 15 shows that the search spaces of the datasets have a skewed distribution of performances, where, there exist a large number of hyperparameter configurations that achieve top performance. In such datasets, even a non-model based technique will quickly find a well-performing configuration, since, there is a high chance for a randomly-sampled configuration to achieve the top performance. For this reason, there is little room for sophisticated HPO techniques in these datasets. To further validate our hypothesis, we investigate two additional datasetsdionis from the LCBench benchmark and uniref50_transformer _batch_size_128 from the PD1 benchmark, where DPL achieves a strong performance compared to baseline HPO methods. The results in Figure 16 show that DPL excels on tasks that are complex and that require optimizations to find hyperparameter configurations that achieve top performance. Based on the results, we conclude that the lack of statistical significance in PD1 is not a specific failure mode of DPL, but a consequence of multiple PD1 datasets where the majority of configurations achieve the top performance. 20/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b /uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c /uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000015/uni00000013/uni00000013/uni00000029/uni00000055/uni00000048/uni00000054/uni00000058/uni00000048/uni00000051/uni00000046/uni0000005c /uni00000047/uni0000004c/uni00000052/uni00000051/uni0000004c/uni00000056 /uni00000013/uni00000011/uni00000014/uni0000001a/uni00000013/uni00000011/uni00000014/uni0000001b/uni00000013/uni00000011/uni00000014/uni0000001c/uni00000013/uni00000011/uni00000015/uni00000013/uni00000013/uni00000011/uni00000015/uni00000014 /uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c /uni00000013 /uni00000018/uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000015/uni00000018/uni00000013 /uni00000016/uni00000013/uni00000013 /uni00000016/uni00000018/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000029/uni00000055/uni00000048/uni00000054/uni00000058/uni00000048/uni00000051/uni00000046/uni0000005c /uni00000058/uni00000051/uni0000004c/uni00000055/uni00000048/uni00000049/uni00000018/uni00000013/uni00000042/uni00000057/uni00000055/uni00000044/uni00000051/uni00000056/uni00000049/uni00000052/uni00000055/uni00000050/uni00000048/uni00000055/uni00000042/uni00000045/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000042/uni00000056/uni0000004c/uni0000005d/uni00000048/uni00000042/uni00000014/uni00000015/uni0000001b Figure 16: Histograms of the distribution of performances for datasets where DPL performs strongly. 0.0 0.5 1.0 0 20 Embedding size = 6 0.0 0.5 1.0 HPO Budget 0 20 Embedding size = 12 0.0 0.5 1.0 0 20 Embedding size = 24 0.0 0.5 1.0 0 20Percentage of Iteration Time Embedding size = 48 0.0 0.5 1.0 HPO Budget 0 10 Embedding size = 96 0.0 0.5 1.0 0 10 Embedding size = 192 0.0 0.5 1.0 HPO Budget 0 10 Embedding size = 384 Figure 17: The percentage taken by the DPL overhead in the total time per iteration for the different embedding sizes in nanoGPT-Bench. E DPL Overhead To investigate the efficiency of DPL in terms of method runtime, we investigate the percentage of time that the DPL overhead contributes in the total time taken to perform one HPO iteration in the nanoGPT-Bench. The total time taken constitutes of the DPL overhead (training the DPL surrogate, calling the acquisition function to suggest the next hyperparameter configuration) and the time taken to run the target algorithm for one more step. Figure 17 shows that the impact of the DPL overhead is negligible in the total time taken. For the smallest embedding of size 6, DPL takes only 10% of the total time taken to perform one HPO iteration after spending half of the optimization budget. At the end, after circa 40 hours of HPO optimization, DPL has an impact of 20% in the total time taken to perform one HPO iteration. The impact is even smaller for the largest embedding of size 384, where DPL has an impact of only 5% in the total time taken per iteration after spending half of the optimization budget and it has an impact of only 10% in the total time per iteration after more than 120 hours of HPO optimization. The findings validate our claim that DPL has a minor time overhead in performing hyperparameter optimization, which explains the strong any-time performance of our method. 21F Details of Considered Benchmarks LCBench: We use the official implementation as the interface for the LCBench benchmark2. As suggested by the authors, we use the benchmark information starting from the second step and we skip the last step of the curve since it is a repeat of the preceding step. TaskSet: The TaskSet benchmark features 1000 diverse tasks. We decide to focus on only 12 NLP tasks from the TaskSet benchmark to add variety to our entire collection of datasets. Our limitation on the number of included tasks is related to the limited compute power, as we are unable to run for the entire suite of tasks offered in TaskSet. TaskSet features a set of 8 hyperparameters, that consists of i) optimizer-specific hyperparameters, such as the learning rate, the exponential decay rate, β1 and β2, and Adam’s constant for numerical stability ε, ii) hyperparameters that control the linear and exponential decay schedulers for the learning rate decay, and lastly iii) hyperparameters that control the L1 and L2 regularization terms. Every hyperparameter in TaskSet except β1 and β2 is sampled logarithmically. PD1: We use the synetune library [42] for our interface to the PD1 benchmark. From the benchmark, we only include datasets that have a learning curve of length greater than 10. We furthermore only include datasets that have a learning curve lower or equal to 50 to have a fair comparison between all benchmarks by having approximately 20 full-function evaluations. PD1 features 4 numerical hyper- parameters, lr_initial_value, lr_power, lr_decay_steps_factor and one_minus_momentum, where lr_initial_value and one_minus_momentum are log sampled. The learning rate decay is applied based on a polynomial schedule, its hyperparameters taken from the search space. G Baselines Random Search: We implemented random search by randomly sampling hyperparameter configura- tions from the benchmarks with the maximal budget. Hyperband, BOHB, LCNet: We use version 0.7.4 of the HpBandSter library as a common codebase for all 3 baselines 3. For the last approach mentioned, despite heavy hyperparameter tuning of the method, we could not get stable results across all the benchmarks and hence dropped the method from our comparison. ASHA: For the implementation of ASHA we use the public implementation from the optuna library, version 2.10.0. DEHB: We use the public implementation offered by the authors 4. MF-DNN: In our experiments we used the official implementation from the authors 5. However, the method crashes which does not allow for full results on all benchmarks. SMAC: For our experiment with SMAC we used the official code base from the authors 6. Dragonfly: We use version 0.1.6 of the publicly available Dragonfly library. For all the multi-fidelity methods considered in the experiments, we use the same minimal and maximal fidelities. In more detail, for the LCBench, TaskSet and PD1 benchmarks we use a minimal fidelity lower bound of 1 and a maximal fidelity lower bound equal to the max budget. H Plots In Hypothesis 1, we prove that DPL achieves a better performance in comparison to other models in estimating the final performance for different hyperparameter configurations based on partial observations. Our analysis shows that DPL manages to retain the ranks of different hyperparameter 2https://github.com/automl/LCBench 3https://github.com/automl/HpBandSter 4https://github.com/automl/DEHB/ 5https://github.com/shib0li/DNN-MFBO 6https://github.com/automl/SMAC3 22/uni00000013/uni00000011/uni00000014/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000016/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018 /uni0000002f/uni00000026/uni00000003/uni0000002f/uni00000048/uni00000051/uni0000004a/uni00000057/uni0000004b/uni00000003/uni00000029/uni00000055/uni00000044/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000014 /uni00000013/uni00000011/uni00000015 /uni00000013/uni00000011/uni00000016 /uni00000013/uni00000011/uni00000017/uni00000024/uni00000045/uni00000056/uni00000052/uni0000004f/uni00000058/uni00000057/uni00000048/uni00000003/uni00000035/uni00000048/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000059/uni00000048/uni00000003/uni00000028/uni00000055/uni00000055/uni00000052/uni00000055 Figure 18: The absolute relative error distribution of DPL over the different learning curve fractions in the LCBench benchmark. The distribution is calculated from the ground truth and prediction values, averaged over all configurations of a dataset. Every distribution is over the datasets. configurations. We complement Hypothesis 1 by additionally investigating the absolute relative error. We measure the difference between the DPL estimation of the final learning curve value for different fractions of available partial observations vs the actual end performance. Figure 18 shows that DPL does not only retain the ranks of the final performances of different hyperparameter configurations, but it also correctly estimates the final performance by attaining a small relative error, where the error is reduced the more partial observations we have from the learning curve. In Hypothesis 3, we investigate the efficiency of DPL in exploring more promising configurations compared to other HPO methods. In Figure 19 we provide the same comparison with regards to the PD1 benchmark. Based on the results, we conclude that DPL explores more promising configurations compared to other HPO methods. 0.005 0.010 0.015 0.020 LC Length Fraction 0.00 0.05 0.10 0.15 0.20 Precision of T op Candidates PD1 0.005 0.010 0.015 0.020 LC Length Fraction 0.02 0.04 0.06 0.08Average Regret PD1 0.005 0.010 0.015 0.020 LC Length Fraction 0.2 0.4 0.6 0.8 Fraction of Poor Performer Promotions PD1 DPL ASHA BOHB DEHB Dragonfly Hyperband Random SMAC Baseline Figure 19: Post-hoc analysis to study DPL’s efficiency.Left: Share of the best candidates selected during training. Middle: Average regret of configurations chosen to be trained at each budget. Right: Share of top third configurations at a given budget which were bottom two third configurations at a previous budget. Lastly, we provide the per-dataset performances of all methods, where we present the mean regret of the incumbent trajectory and the standard error over 10 runs in LCBench (Figure 20 and 21), TaskSet (Figure 22), and PD1 (Figure 23). The results show that DPL consistently outperforms other methods in the majority of cases achieving strong any-time performance and not only a strong final performance. 23/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000024/uni00000033/uni00000036/uni00000029/uni00000044/uni0000004c/uni0000004f/uni00000058/uni00000055/uni00000048 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000024/uni00000050/uni00000044/uni0000005d/uni00000052/uni00000051/uni00000042/uni00000048/uni00000050/uni00000053/uni0000004f/uni00000052/uni0000005c/uni00000048/uni00000048/uni00000042/uni00000044/uni00000046/uni00000046/uni00000048/uni00000056/uni00000056 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000024/uni00000058/uni00000056/uni00000057/uni00000055/uni00000044/uni0000004f/uni0000004c/uni00000044/uni00000051 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000017 /uni00000014/uni00000013/uni00000016 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000029/uni00000044/uni00000056/uni0000004b/uni0000004c/uni00000052/uni00000051/uni00000010/uni00000030/uni00000031/uni0000002c/uni00000036/uni00000037 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni0000002e/uni00000027/uni00000027/uni00000026/uni00000058/uni00000053/uni00000013/uni0000001c/uni00000042/uni00000044/uni00000053/uni00000053/uni00000048/uni00000057/uni00000048/uni00000051/uni00000046/uni0000005c /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000030/uni0000004c/uni00000051/uni0000004c/uni00000025/uni00000052/uni00000052/uni00000031/uni00000028 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000016 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000044/uni00000047/uni00000058/uni0000004f/uni00000057 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000016 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000044/uni0000004c/uni00000055/uni0000004f/uni0000004c/uni00000051/uni00000048/uni00000056 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000044/uni0000004f/uni00000045/uni00000048/uni00000055/uni00000057 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000045/uni00000044/uni00000051/uni0000004e/uni00000010/uni00000050/uni00000044/uni00000055/uni0000004e/uni00000048/uni00000057/uni0000004c/uni00000051/uni0000004a /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000045/uni0000004f/uni00000052/uni00000052/uni00000047/uni00000010/uni00000057/uni00000055/uni00000044/uni00000051/uni00000056/uni00000049/uni00000058/uni00000056/uni0000004c/uni00000052/uni00000051/uni00000010/uni00000056/uni00000048/uni00000055/uni00000059/uni0000004c/uni00000046/uni00000048/uni00000010/uni00000046/uni00000048/uni00000051/uni00000057/uni00000048/uni00000055 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000046/uni00000044/uni00000055 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000046/uni0000004b/uni00000055/uni0000004c/uni00000056/uni00000057/uni0000004c/uni00000051/uni00000048 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000046/uni00000051/uni00000044/uni00000048/uni00000010/uni0000001c /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000016 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000046/uni00000052/uni00000051/uni00000051/uni00000048/uni00000046/uni00000057/uni00000010/uni00000017 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000046/uni00000052/uni00000059/uni00000048/uni00000055/uni00000057/uni0000005c/uni00000053/uni00000048 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000046/uni00000055/uni00000048/uni00000047/uni0000004c/uni00000057/uni00000010/uni0000004a /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000018 /uni00000014/uni00000013/uni00000017 /uni00000014/uni00000013/uni00000016 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000047/uni0000004c/uni00000052/uni00000051/uni0000004c/uni00000056 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 Figure 20: Performance comparison over the number of epochs on a dataset level for LCBench. We plot the mean value over 10 runs and the standard error. 24/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000049/uni00000044/uni00000045/uni00000048/uni00000055/uni00000057 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni0000004b/uni00000048/uni0000004f/uni00000048/uni00000051/uni00000044 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000017 /uni00000014/uni00000013/uni00000016 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni0000004b/uni0000004c/uni0000004a/uni0000004a/uni00000056 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000016 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni0000004d/uni00000044/uni00000051/uni00000051/uni0000004c/uni00000056 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni0000004d/uni00000044/uni00000056/uni00000050/uni0000004c/uni00000051/uni00000048 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000017 /uni00000014/uni00000013/uni00000016 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni0000004d/uni00000058/uni00000051/uni0000004a/uni0000004f/uni00000048/uni00000042/uni00000046/uni0000004b/uni00000048/uni00000056/uni00000056/uni00000042/uni00000015/uni00000053/uni00000046/uni00000056/uni00000042/uni00000055/uni00000044/uni0000005a/uni00000042/uni00000048/uni00000051/uni00000047/uni0000004a/uni00000044/uni00000050/uni00000048/uni00000042/uni00000046/uni00000052/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000057/uni00000048 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni0000004e/uni00000046/uni00000014 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni0000004e/uni00000055/uni00000010/uni00000059/uni00000056/uni00000010/uni0000004e/uni00000053 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000050/uni00000049/uni00000048/uni00000044/uni00000057/uni00000010/uni00000049/uni00000044/uni00000046/uni00000057/uni00000052/uni00000055/uni00000056 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000016 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000051/uni00000052/uni00000050/uni00000044/uni00000052 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000051/uni00000058/uni00000050/uni00000048/uni00000055/uni00000044/uni0000004c/uni00000015/uni0000001b/uni00000011/uni00000019 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000053/uni0000004b/uni00000052/uni00000051/uni00000048/uni00000050/uni00000048 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000056/uni00000048/uni0000004a/uni00000050/uni00000048/uni00000051/uni00000057 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000056/uni0000004b/uni00000058/uni00000057/uni00000057/uni0000004f/uni00000048 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000056/uni0000005c/uni0000004f/uni00000059/uni0000004c/uni00000051/uni00000048 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000016 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000059/uni00000048/uni0000004b/uni0000004c/uni00000046/uni0000004f/uni00000048 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000059/uni00000052/uni0000004f/uni0000004e/uni00000048/uni00000055/uni00000057 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 Figure 21: Performance comparison over the number of epochs on a dataset level for LCBench (cont.). We plot the mean value over 10 runs and the standard error. 25/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000029/uni0000004c/uni0000005b/uni00000048/uni00000047/uni00000037/uni00000048/uni0000005b/uni00000057/uni00000035/uni00000031/uni00000031/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051 /uni0000004c/uni00000050/uni00000047/uni00000045/uni00000042/uni00000053/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000014/uni00000015/uni0000001b/uni00000042/uni0000002f/uni00000036/uni00000037/uni00000030/uni00000014/uni00000015/uni0000001b/uni00000042/uni00000044/uni00000059/uni0000004a/uni00000042/uni00000045/uni00000056/uni00000019 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000014/uni00000017 /uni00000014/uni00000013/uni00000014/uni00000014 /uni00000014/uni00000013/uni0000001b /uni00000014/uni00000013/uni00000018 /uni00000014/uni00000013/uni00000015 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000029/uni0000004c/uni0000005b/uni00000048/uni00000047/uni00000037/uni00000048/uni0000005b/uni00000057/uni00000035/uni00000031/uni00000031/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051 /uni0000004c/uni00000050/uni00000047/uni00000045/uni00000042/uni00000053/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000014/uni00000015/uni0000001b/uni00000042/uni0000002f/uni00000036/uni00000037/uni00000030/uni00000014/uni00000015/uni0000001b/uni00000042/uni00000045/uni00000056/uni00000019 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000029/uni0000004c/uni0000005b/uni00000048/uni00000047/uni00000037/uni00000048/uni0000005b/uni00000057/uni00000035/uni00000031/uni00000031/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051 /uni0000004c/uni00000050/uni00000047/uni00000045/uni00000042/uni00000053/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000014/uni00000015/uni0000001b/uni00000042/uni0000002f/uni00000036/uni00000037/uni00000030/uni00000014/uni00000015/uni0000001b/uni00000042/uni00000048/uni00000050/uni00000045/uni00000048/uni00000047/uni00000014/uni00000015/uni0000001b/uni00000042/uni00000045/uni00000056/uni00000019 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000029/uni0000004c/uni0000005b/uni00000048/uni00000047/uni00000037/uni00000048/uni0000005b/uni00000057/uni00000035/uni00000031/uni00000031/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051 /uni0000004c/uni00000050/uni00000047/uni00000045/uni00000042/uni00000053/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000016/uni00000015/uni00000042/uni0000002a/uni00000035/uni00000038/uni00000014/uni00000015/uni0000001b/uni00000042/uni00000045/uni00000056/uni00000014/uni00000015 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000029/uni0000004c/uni0000005b/uni00000048/uni00000047/uni00000037/uni00000048/uni0000005b/uni00000057/uni00000035/uni00000031/uni00000031/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051 /uni0000004c/uni00000050/uni00000047/uni00000045/uni00000042/uni00000053/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000016/uni00000015/uni00000042/uni0000002a/uni00000035/uni00000038/uni00000019/uni00000017/uni00000042/uni00000044/uni00000059/uni0000004a/uni00000042/uni00000045/uni00000056/uni00000014/uni00000015 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000016 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000029/uni0000004c/uni0000005b/uni00000048/uni00000047/uni00000037/uni00000048/uni0000005b/uni00000057/uni00000035/uni00000031/uni00000031/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051 /uni0000004c/uni00000050/uni00000047/uni00000045/uni00000042/uni00000053/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000016/uni00000015/uni00000042/uni0000002c/uni00000035/uni00000031/uni00000031/uni00000019/uni00000017/uni00000042/uni00000055/uni00000048/uni0000004f/uni00000058/uni00000042/uni00000044/uni00000059/uni0000004a/uni00000042/uni00000045/uni00000056/uni00000014/uni00000015 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000017 /uni00000014/uni00000013/uni00000016 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000029/uni0000004c/uni0000005b/uni00000048/uni00000047/uni00000037/uni00000048/uni0000005b/uni00000057/uni00000035/uni00000031/uni00000031/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051 /uni0000004c/uni00000050/uni00000047/uni00000045/uni00000042/uni00000053/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000016/uni00000015/uni00000042/uni0000002c/uni00000035/uni00000031/uni00000031/uni00000019/uni00000017/uni00000042/uni00000055/uni00000048/uni0000004f/uni00000058/uni00000042/uni0000004f/uni00000044/uni00000056/uni00000057/uni00000042/uni00000045/uni00000056/uni00000014/uni00000015 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000029/uni0000004c/uni0000005b/uni00000048/uni00000047/uni00000037/uni00000048/uni0000005b/uni00000057/uni00000035/uni00000031/uni00000031/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051 /uni0000004c/uni00000050/uni00000047/uni00000045/uni00000042/uni00000053/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000016/uni00000015/uni00000042/uni0000002f/uni00000036/uni00000037/uni00000030/uni00000014/uni00000015/uni0000001b/uni00000042/uni00000028/uni00000014/uni00000015/uni0000001b/uni00000042/uni00000045/uni00000056/uni00000014/uni00000015 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000016 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000029/uni0000004c/uni0000005b/uni00000048/uni00000047/uni00000037/uni00000048/uni0000005b/uni00000057/uni00000035/uni00000031/uni00000031/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051 /uni0000004c/uni00000050/uni00000047/uni00000045/uni00000042/uni00000053/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000016/uni00000015/uni00000042/uni0000002f/uni00000036/uni00000037/uni00000030/uni00000014/uni00000015/uni0000001b/uni00000042/uni00000045/uni00000056/uni00000014/uni00000015 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000013 /uni00000015/uni000000ee/uni00000014/uni00000013/uni00000014 /uni00000016/uni000000ee/uni00000014/uni00000013/uni00000014 /uni00000017/uni000000ee/uni00000014/uni00000013/uni00000014 /uni00000019/uni000000ee/uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000029/uni0000004c/uni0000005b/uni00000048/uni00000047/uni00000037/uni00000048/uni0000005b/uni00000057/uni00000035/uni00000031/uni00000031/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051 /uni0000004c/uni00000050/uni00000047/uni00000045/uni00000042/uni00000053/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000016/uni00000015/uni00000042/uni00000039/uni00000035/uni00000031/uni00000031/uni00000014/uni00000015/uni0000001b/uni00000042/uni00000057/uni00000044/uni00000051/uni0000004b/uni00000042/uni00000045/uni00000056/uni00000014/uni00000015 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000029/uni0000004c/uni0000005b/uni00000048/uni00000047/uni00000037/uni00000048/uni0000005b/uni00000057/uni00000035/uni00000031/uni00000031/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051 /uni0000004c/uni00000050/uni00000047/uni00000045/uni00000042/uni00000053/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000016/uni00000015/uni00000042/uni00000039/uni00000035/uni00000031/uni00000031/uni00000019/uni00000017/uni00000042/uni00000055/uni00000048/uni0000004f/uni00000058/uni00000042/uni00000044/uni00000059/uni0000004a/uni00000042/uni00000045/uni00000056/uni00000014/uni00000015 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000016 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000029/uni0000004c/uni0000005b/uni00000048/uni00000047/uni00000037/uni00000048/uni0000005b/uni00000057/uni00000035/uni00000031/uni00000031/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051 /uni0000004c/uni00000050/uni00000047/uni00000045/uni00000042/uni00000053/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000016/uni00000015/uni00000042/uni00000039/uni00000035/uni00000031/uni00000031/uni00000019/uni00000017/uni00000042/uni00000057/uni00000044/uni00000051/uni0000004b/uni00000042/uni00000044/uni00000059/uni0000004a/uni00000042/uni00000045/uni00000056/uni00000014/uni00000015 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 Figure 22: Performance comparison over the number of steps on a dataset level for TaskSet. We plot the mean value over 10 runs and the standard error. 26/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000046/uni0000004c/uni00000049/uni00000044/uni00000055/uni00000014/uni00000013/uni00000013/uni00000042/uni0000005a/uni0000004c/uni00000047/uni00000048/uni00000042/uni00000055/uni00000048/uni00000056/uni00000051/uni00000048/uni00000057/uni00000042/uni00000045/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000042/uni00000056/uni0000004c/uni0000005d/uni00000048/uni00000042/uni00000015/uni00000018/uni00000019 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000049/uni00000044/uni00000056/uni0000004b/uni0000004c/uni00000052/uni00000051/uni00000042/uni00000050/uni00000051/uni0000004c/uni00000056/uni00000057/uni00000042/uni00000050/uni00000044/uni0000005b/uni00000042/uni00000053/uni00000052/uni00000052/uni0000004f/uni0000004c/uni00000051/uni0000004a/uni00000042/uni00000046/uni00000051/uni00000051/uni00000042/uni00000055/uni00000048/uni0000004f/uni00000058/uni00000042/uni00000045/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000042/uni00000056/uni0000004c/uni0000005d/uni00000048/uni00000042/uni00000015/uni00000018/uni00000019 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000049/uni00000044/uni00000056/uni0000004b/uni0000004c/uni00000052/uni00000051/uni00000042/uni00000050/uni00000051/uni0000004c/uni00000056/uni00000057/uni00000042/uni00000050/uni00000044/uni0000005b/uni00000042/uni00000053/uni00000052/uni00000052/uni0000004f/uni0000004c/uni00000051/uni0000004a/uni00000042/uni00000046/uni00000051/uni00000051/uni00000042/uni00000057/uni00000044/uni00000051/uni0000004b/uni00000042/uni00000045/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000042/uni00000056/uni0000004c/uni0000005d/uni00000048/uni00000042/uni00000015/uni00000018/uni00000019 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000049/uni00000044/uni00000056/uni0000004b/uni0000004c/uni00000052/uni00000051/uni00000042/uni00000050/uni00000051/uni0000004c/uni00000056/uni00000057/uni00000042/uni00000056/uni0000004c/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000042/uni00000046/uni00000051/uni00000051/uni00000042/uni00000045/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000042/uni00000056/uni0000004c/uni0000005d/uni00000048/uni00000042/uni00000015/uni00000018/uni00000019 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000016 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni0000004f/uni00000050/uni00000014/uni00000045/uni00000042/uni00000057/uni00000055/uni00000044/uni00000051/uni00000056/uni00000049/uni00000052/uni00000055/uni00000050/uni00000048/uni00000055/uni00000042/uni00000045/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000042/uni00000056/uni0000004c/uni0000005d/uni00000048/uni00000042/uni00000015/uni00000013/uni00000017/uni0000001b /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000016 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000050/uni00000051/uni0000004c/uni00000056/uni00000057/uni00000042/uni00000050/uni00000044/uni0000005b/uni00000042/uni00000053/uni00000052/uni00000052/uni0000004f/uni0000004c/uni00000051/uni0000004a/uni00000042/uni00000046/uni00000051/uni00000051/uni00000042/uni00000055/uni00000048/uni0000004f/uni00000058/uni00000042/uni00000045/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000042/uni00000056/uni0000004c/uni0000005d/uni00000048/uni00000042/uni00000015/uni00000018/uni00000019 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000016 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000050/uni00000051/uni0000004c/uni00000056/uni00000057/uni00000042/uni00000050/uni00000044/uni0000005b/uni00000042/uni00000053/uni00000052/uni00000052/uni0000004f/uni0000004c/uni00000051/uni0000004a/uni00000042/uni00000046/uni00000051/uni00000051/uni00000042/uni00000057/uni00000044/uni00000051/uni0000004b/uni00000042/uni00000045/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000042/uni00000056/uni0000004c/uni0000005d/uni00000048/uni00000042/uni00000015/uni00000018/uni00000019 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000016 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000050/uni00000051/uni0000004c/uni00000056/uni00000057/uni00000042/uni00000056/uni0000004c/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000042/uni00000046/uni00000051/uni00000051/uni00000042/uni00000045/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000042/uni00000056/uni0000004c/uni0000005d/uni00000048/uni00000042/uni00000015/uni00000018/uni00000019 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000056/uni00000059/uni0000004b/uni00000051/uni00000042/uni00000051/uni00000052/uni00000042/uni00000048/uni0000005b/uni00000057/uni00000055/uni00000044/uni00000042/uni0000005a/uni0000004c/uni00000047/uni00000048/uni00000042/uni00000055/uni00000048/uni00000056/uni00000051/uni00000048/uni00000057/uni00000042/uni00000045/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000042/uni00000056/uni0000004c/uni0000005d/uni00000048/uni00000042/uni00000014/uni00000013/uni00000015/uni00000017 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000058/uni00000051/uni0000004c/uni00000055/uni00000048/uni00000049/uni00000018/uni00000013/uni00000042/uni00000057/uni00000055/uni00000044/uni00000051/uni00000056/uni00000049/uni00000052/uni00000055/uni00000050/uni00000048/uni00000055/uni00000042/uni00000045/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000042/uni00000056/uni0000004c/uni0000005d/uni00000048/uni00000042/uni00000014/uni00000015/uni0000001b /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 Figure 23: Performance comparison over the fraction of the total optimization iterations on a dataset level for PD1. We plot the mean value over 10 runs and the standard error. 27",
      "meta_data": {
        "arxiv_id": "2302.00441v3",
        "authors": [
          "Arlind Kadra",
          "Maciej Janowski",
          "Martin Wistuba",
          "Josif Grabocka"
        ],
        "published_date": "2023-02-01T13:39:07Z",
        "pdf_url": "https://arxiv.org/pdf/2302.00441v3.pdf"
      }
    },
    {
      "title": "Efficient Hyperparameter Optimization with Adaptive Fidelity Identification"
    },
    {
      "title": "Bayesian Optimization for Iterative Learning",
      "abstract": "The performance of deep (reinforcement) learning systems crucially depends on\nthe choice of hyperparameters. Their tuning is notoriously expensive, typically\nrequiring an iterative training process to run for numerous steps to\nconvergence. Traditional tuning algorithms only consider the final performance\nof hyperparameters acquired after many expensive iterations and ignore\nintermediate information from earlier training steps. In this paper, we present\na Bayesian optimization (BO) approach which exploits the iterative structure of\nlearning algorithms for efficient hyperparameter tuning. We propose to learn an\nevaluation function compressing learning progress at any stage of the training\nprocess into a single numeric score according to both training success and\nstability. Our BO framework is then balancing the benefit of assessing a\nhyperparameter setting over additional training steps against their computation\ncost. We further increase model efficiency by selectively including scores from\ndifferent training steps for any evaluated hyperparameter set. We demonstrate\nthe efficiency of our algorithm by tuning hyperparameters for the training of\ndeep reinforcement learning agents and convolutional neural networks. Our\nalgorithm outperforms all existing baselines in identifying optimal\nhyperparameters in minimal time.",
      "full_text": "Bayesian Optimization for Iterative Learning Vu Nguyen ∗ University of Oxford vu@robots.ox.ac.uk Sebastian Schulze ∗ University of Oxford sebastian.schulze@eng.ox.ac.uk Michael A. Osborne University of Oxford mosb@robots.ox.ac.uk Abstract The performance of deep (reinforcement) learning systems crucially depends on the choice of hyperparameters. Their tuning is notoriously expensive, typically requiring an iterative training process to run for numerous steps to convergence. Traditional tuning algorithms only consider the ﬁnal performance of hyperparam- eters acquired after many expensive iterations and ignore intermediate information from earlier training steps. In this paper, we present a Bayesian optimization (BO) approach which exploits the iterative structure of learning algorithms for efﬁcient hyperparameter tuning. We propose to learn an evaluation function compress- ing learning progress at any stage of the training process into a single numeric score according to both training success and stability. Our BO framework is then balancing the beneﬁt of assessing a hyperparameter setting over additional train- ing steps against their computation cost. We further increase model efﬁciency by selectively including scores from different training steps for any evaluated hyper- parameter set. We demonstrate the efﬁciency of our algorithm by tuning hyperpa- rameters for the training of deep reinforcement learning agents and convolutional neural networks. Our algorithm outperforms all existing baselines in identifying optimal hyperparameters in minimal time. 1 Introduction Deep learning (DL) and deep reinforcement learning (DRL) have led to impressive breakthroughs in a broad range of applications such as game play [26, 36], motor control [43], and image recognition [20]. To maintain general applicability, these algorithms expose sets of hyperparameters to adapt their behavior to any particular task at hand. This ﬂexibility comes at the price of having to tune an additional set of parameters – poor settings lead to drastic performance losses [11, 30, 37]. On top of being notoriously sensitive to these choices, deep (reinforcement) learning systems often have high training costs, in computational resources and time. For example, a single training run on the Atari Breakout game took approximately 75 hours on a GPU cluster [26]. Tuning DRL parameters is further complicated as only noisy evaluations of an agent’s ﬁnal performance are obtainable. Bayesian optimization (BO) [12, 28, 35] has recently achieved considerable success in optimizing these hyperparameters. This approach casts the tuning process as a global optimization problem based on noisy evaluations of a black-box function f . BO constructs a surrogate model typically using a Gaussian process (GP) [31], over this unknown function. This GP surrogate is used to build an acquisition function [13, 44] which suggests the next hyperparameter to evaluate. In modern machine learning (ML) algorithms [15], the training process is often conducted in an iterative manner. A natural example is given by deep learning where training is usually based on stochastic gradient descent and other iterative procedures. Similarly, the training of reinforcement learning agents is mostly carried out using multiple episodes. The knowledge accumulated during these training iterations can be useful to inform BO. However, most existing BO approaches [35] ∗These authors contributed equally. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada. arXiv:1909.09593v5  [cs.LG]  16 Jan 2021deﬁne the objective function as the average performance over the ﬁnal training iterations. In doing so, they ignore the information contained in the preceding training steps. In this paper, we present a Bayesian optimization approach for tuning algorithms where iterative learning is available – the cases of deep learning and deep reinforcement learning. First, we consider the joint space of input hyperparameters and number of training iterations to capture the learning progress at different time steps in the training process. We then propose to transform the whole training curve into a numeric score according to user preference. To learn across the joint space efﬁciently, we introduce a data augmentation technique leveraging intermediate information from the iterative process. By exploiting the iterative structure of training procedures, we encourage our algorithm to consider running a larger number of cheap (but high-utility) experiments, when cost- ignorant algorithms would only be able to run a few expensive ones. We demonstrate the efﬁciency of our algorithm on training DRL agents on several well-known benchmarks as well as the training of convolutional neural networks. In particular, our algorithm outperforms existing baselines in ﬁnding the best hyperparameter in terms of wall-clock time. Our main contributions are: • an algorithm to optimize the learning curve of a ML algorithm by using training curve compression, instead of averaged ﬁnal performance; • an approach to learn the compression curve from the data and a data augmentation tech- nique for increased sample-efﬁciency; • demonstration on tuning DRL and convolutional neural networks. 2 Related Work in Iteration-Efﬁcient Bayesian Optimization The ﬁrst algorithm category employs stopping criteria to terminate some training runs early and allo- cate resources towards more promising settings. These criteria typically involve projecting towards a ﬁnal score from early training stages. Freeze-thaw BO [42] models the training loss over time us- ing a GP regressor under the assumption that the training loss roughly follows an exponential decay. Based on this projection, training resources are allocated to the most promising settings. Hyperband [8, 23] dynamically allocates computational resources (e.g. training epochs or dataset size) through random sampling and eliminates under-performing hyperparameter settings by successive halving. Attempts have also been made to improve the epoch efﬁciency of other hyperparameter optimization algorithms in [5, 7, 18] which predict the ﬁnal learning outcome based on partially trained learning curves to identify hyperparameter settings that are expected to under-perform and early-stop them. In the context of DRL, however, these stopping criteria, including the exponential decay assumed in Freeze-thaw BO [42], may not be applicable, due to the unpredictable ﬂuctuations of DRL reward curves. In the supplement, we illustrate the noisiness of DRL training. The second category [16, 17, 23, 41, 48] aims to reduce the resource consumption of BO by utilizing low-ﬁdelity functions which can be obtained by using a subset of the training data or by training the ML model for a small number of iterations. Multi-task BO [41] requires the user to deﬁne a division of the dataset into pre-deﬁned and discrete subtasks. Multi-ﬁdelity BO with continuous approximation (BOCA) [16] and hierarchical partition [34] extend this idea to continuous settings. Speciﬁcally, BOCA ﬁrst selects the hyperparameter input and then the corresponding ﬁdelity to be evaluated at. The ﬁdelity in this context refers to the use of different number of learning iterations. Analogous to BOCA’s consideration of continuous ﬁdelities, Fabolas [17] proposes to model the combined space of input hyperparameter and dataset size and then select the optimal input and dataset size jointly. The above approaches typically identify performance of hyperparameters via the average (either training or validation) loss of the last learning iterations. Thereby, they do not account for potential noise in the learning process (e.g., they might select unstable settings that jumped to high perfor- mance in the last couple of iterations). 3 Bayesian Optimization for Iterative Learning (BOIL) Problem setting. We consider training a machine learning algorithm given a d-dimensional hy- perparameter x ∈X ⊂Rd for t iterations. This process has a training time costc(x,t) and produces 20 100 200 300 400 500 #Episode t 0.80 0.85 0.90 0.95 1.00x Tmin Tmax Augmented Obs Observation 0 100 200 300 400 500 0 50 100 150 200Score 4 18 34 8 45 5 14 26Reward Curve Sigmoid Func 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l( . |m * 0 ,g* 0 ) for Cifar10 m* 0 =-4.0 g* 0 =1.476 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l( . |m * 0 ,g* 0 ) for Reacher m* 0 =2.779 g* 0 =1.973 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l( . |m * 0 ,g* 0 ) for CartPole m* 0 =-3.266 g* 0 =3.0 Figure 1: Left: the score in pink box is a convolution of the reward curve r(·| x = 0.9,t = 500) and a Sigmoid function l(u |g0,m0) = 1 1+exp(−g0[u−m0]) up to time step t. Bottom: observations are selected to augment the dataset (red dots). The heatmap indicates the GP predictive mean µ for f across the number of episodest used to train an agent. Tmin and Tmax are two user-deﬁned thresholds for the number of training episodes. x is a hyperparameter to be tuned. Right: we learn the optimal parameter g∗ 0 and m∗ 0 for each experiment separately. training evaluations r(·| x,t) for t iterations, t ∈[Tmin,Tmax]. These could be episode rewards in DRL or training accuracies in DL. An important property of iterative training is that we know the whole curve at preceding steps r(t′|x,t), ∀t′≤t. Given the raw training curve r(·| x,t), we assume an underlying smoothed black-box function f , deﬁned in Sec. 3.2. Formally, we aim to ﬁnd x∗= argmaxx∈X f (x,Tmax); at the same time, we want to keep the overall training time, ∑N i=1 c(xi,ti), of evaluated settings [xi,ti] as low as possible. We summarize our variables in Table 1 in the supplement for ease of reading. 3.1 Selecting a next point using iteration-efﬁcient modeling We follow popular designs in [17, 19, 39, 41] and model the cost-sensitive black-box function as f (x,t) ∼GP(0,k([x,t],[x′,t′])), where k is an appropriate covariance functions and [x,t] ∈Rd+1. For simplicity and robustness, the cost function c(x,t) is approximated by a linear regressor. De- pending on the setting, it may be more appropriate to employ a second GP or different parametric model if the cost has a more complex dependence on hyperparameters x and iterations t. We regu- larly (re-)optimize both kernel and cost function parameters in between point acquisitions. More speciﬁcally, we choose the covariance function as a productk ([x,t],[x′,t′]) =k(x,x′)×k(t,t′) to induce joint similarities over parameter and iteration space. We estimate the predictive mean and uncertainty for a GP [31] at any input z∗= [x∗,t∗] as µ (z∗) =k∗ [ K +σ2 y I ]−1 y (1) σ2 (z∗) =k∗∗−k∗ [ K +σ2 y I ]−1 kT ∗ (2) where y = [yi]∀i, k∗= [k (z∗,zi)]∀i, K = [k (zi,zj)]∀i, j, k∗∗= k (z∗,z∗), and σ2 y is the noise variance of f . Cost predictions at any particular parameter x and time t are given by µc([x∗,t∗]) =βT [x,t], where β is directly computed from data {Z = [xi,ti],c = [ci]}∀i as β = (ZT Z)−1Zc [1]. Our goal is to select a point with high function value (exploitation), high uncertainty (exploration) and low cost (cheap). At each iteration n, we query the input parameter xn and the number of iteration tn [38, 48]: zn = [xn,tn] = argmax x∈X ,t∈[Tmin,Tmax] α(x,t)/µc(x,t). (3) 3Although our framework is available for any acquisition choices [13, 22, 47], to cope with output noise, we follow [45] and slight modify the expected improvement criterion using the maximum mean GP prediction µmax n . Let λ = µn(z)−µmaxn σn(z) , we then have a closed-form for the new expected improvement (EI) as αEI n (z) =σn (z)φ (λ) + [µn (z)−µmax n ]Φ(λ) where φ is the standard normal p.d.f., Φ is the c.d.f, µn and σn are the GP predictive mean and variance deﬁned in Eq. (1) and Eq. (2), respectively. 3.2 Training curve compression and estimating the transformation function Existing BO approaches [4, 23] typically deﬁne the objective function as an average loss over the ﬁnal learning episodes. However, this does not take into consideration how stable performance is or the training stage at which it has been achieved. We argue that averaging learning losses is likely misleading due to the noise and ﬂuctuations of our observations (learning curves) – particularly during the early stages of training. We propose to compress the whole learning curve into a numeric score via a preference function representing the user’s desired training curve. In the following, we use the Sigmoid function (speciﬁcally the Logistic function) to compute the utility score as y = ˆy(r,m0,g0) =r(·|x,t)•l(·|m0,g0) = t ∑ u=1 r(u |x,t) 1 +exp(−g0 [u −m0]) (4) where •is a dot product, a Logistic function l(·| m0,g0) is parameterized by a growth parameter g0 deﬁning a slope and the middle point of the curve m0. The optimal parameters g0 and m0 are estimated directly from the data. We illustrate different shapes of l parameterized by g0 and m0 in the appendix. The Sigmoid preference has a number of desirable properties. As early weights are small, less credit is given to ﬂuctuations at the initial stages, making it less likely for our surrogate to be biased towards randomly well performing settings. However, as weights monotonically increase, hyperparameters with improving performance are preferred. As weights saturate over time, stable, high performing conﬁgurations are preferred over short “performance spikes” characteristic of un- stable training. Lastly, this utility score assigns higher values to the same performance if it is being maintained over more episodes. Learning the transformation function from data. Different compression curves l(), parameter- ized by different choices of g0 and m0 in Eq. (4), may lead to different utilities y and thus affect the performance. The optimal values of g∗ 0 and m∗ 0 are unknown in advance. Therefore, we propose to learn these values g∗ 0 and m∗ 0 directly from the data. Our intuition is that the ‘optimal’ compression curve l(m∗ 0,g∗ 0) will lead to a better ﬁt of the GP. This better GP surrogate model, thus, will result in better prediction as well as optimization performance. We parameterize the GP log marginal likelihood L [31] as the function of m0 and g0: L(m0,g0) =1 2 ˆyT ( K +σ2 y I )−1 ˆy −1 2 ln ⏐⏐K +σ2 y I ⏐⏐ +const (5) where σ2 y is the output noise variance, ˆy is the function of m0 and g0 deﬁned in Eq. (4). We optimize m0 and g0 (jointly with other GP hyperparameters) using multi-start gradient descent. We derive the derivative ∂L ∂m0 = ∂L ∂ ˆy ∂ ˆy ∂m0 and ∂L ∂g0 = ∂L ∂ ˆy ∂ ˆy ∂g0 which can be computed analytically as: ∂L ∂ ˆy = ( K +σ2 y IN )−1 ˆy; ∂ ˆy ∂m0 = −g0 ×exp(−g0 [u −m0]) [1 +exp(−g0 [u −m0])]2 ; ∂ ˆy ∂g0 = −m0 ×exp(−g0 [u −m0]) [1 +exp(−g0 [u −m0])]2 . The estimated compression curves are illustrated in Right Fig. 1 and in Sec. 4.1. 3.3 Augmenting the training data When evaluating a parameter x over t iterations, we obtain not only a ﬁnal score but also all reward sequences r(t′|x,t),∀t′= 1,..., t. The auxiliary information from the curve can be useful for BO. Therefore, we propose to augment the information from the curve into the sample set of our GP model. A naïve approach for augmentation is to add a full curve of points {[x, j],yj}t j=1 where yj is computed using Eq. (4). However, this approach can be redundant and may im- pose serious issues in the conditioning of the GP covariance matrix. As we cluster 40.80 0.85 0.90 0.95 1.00 200 300 400 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 x GP variance 2 0 2 4 6 8 10 12 0.000 0.025 0.050 0.075 0.100 0.125 0.150 0.175 0.80 0.85 0.90 0.95 1.00 200 300 400 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 GP variance 400 320 240 160 80 0 80 160 240 0.000 0.005 0.010 0.015 0.020 0.025 0.030 0.035 0.040 x Figure 2: GP with different settings. Left: our augmentation. Right: using a full curve. If we add too many observations, the GP covariance matrix becomes ill-conditioned. On the right, the GP ﬁt is poor with a large mean estimate range of [−400,240] even though the output is standardized N (0,1). All x-axis are over x, a hyperparameter to be tuned. more evaluations closely, the conditioning of the GP covariance degrades further, as dis- cussed in [24]. This conditioning issue is especially serious in our noisy DRL settings. 0 10 20 30 40 50 60 Iterations 0 5 10 15 20 25Log of Condition Number Condition Number of GP Covariance Augmentation No Augmentation Full Curve Reasonable Threshold Figure 3: The condition number of GP covari- ance matrix deteriorates if we add the whole curve of points into a GP. The large condition number indicates the nearness to singularity. We highlight this effect on GP estimation in Fig. 2 wherein the GP mean varies erratically when the natural log of the condition number of the GP co- variance matrix goes above 25 (see Fig. 3) as we include the whole curve. Selecting subset of points from the curve. Dif- ferent solutions, such as the addition of artiﬁcial noise or altering the kernel’s length-scales, have been proposed. We decide to use an active learn- ing approach [10, 29] as sampled data points are expected to contain a lot of redundant informa- tion. As a consequence, the loss of information from sub-sampling the data should be minimal and information-eroding modiﬁcation of the ker- nel matrix itself can be avoided. As a side beneﬁt, the reduced number of sampled points speeds up inference in our GP models. In particular, we select samples at the maximum of the GP predictive uncertainty. Formally, we sequentially select a set Z = [z1,...zM], zm = [x,tm], by varying tm while keeping x ﬁxed as zm =argmax ∀t′≤t σ([x,t′] |D′),∀m ≤M s.t. lnof cond(K) ≤δ (6) where D′= D∪{zj = [x,tj]}m−1 j=1 . This sub-optimisation problem is done in a one-dimensional space of t′∈{Tmin,..., t}, thus it is cheap to optimize using (multi-start) gradient descent (the derivative of GP predictive variance is available [31]). Alternatively, a ﬁxed-size grid could be considered, but this could cause conditioning issues when a point in the grid [ x,tgrid ] is placed near another existing point [ x′,tgrid ] , i.e., ||x −x′||2 ≤ε for some small ε. These generated points Z are used to calculate the output r(zm) and augmented into the observation set (X,Y ) for ﬁtting the GP. The number of samplesM is adaptively chosen such that the natural log of the condition number of the covariance matrix is less than a threshold. This is to ensure that the GP covariance matrix condition number behaves well by reducing the number of unnecessary points added to the GP at later stages. We compute the utility score ym given zm for each augmented point using Eq. (4). In addition, we can estimate the running time cm using the predictive mean µc(zm). We illustrate the augmented observations and estimated scores in Fig. 1. We summarize the overall algorithm in Alg. 1. To enforce non-negativity and numerical stability, we make use of the transformations α ←log[1 +exp(α)] and µc ←log[1 +exp(µc)]. 4 Experiments We assess our model by tuning hyperparameters for two DRL agents on three environments and a CNN on two datasets. We provide additional illustrations and experiments in the appendix. 5Algorithm 1 Bayesian Optimization with Iterative Learning (BOIL) Input: #iter N, initial data D0, z = [x,t]. Output: optimal x∗and y∗= max∀y∈DN y 1: for n = 1....N do 2: Fit a GP to estimate µf (),σf () from Eqs. (1,2) and a LR for cost µc() 3: Select zn = argmaxx,t α(x,t)/µc(x,t) and observe a curve r and a cost c from f (zn) 4: Compressing the learning curve r(zn) into numeric score using Eq. (4). 5: Sample augmented points zn,m,yn,m,cn,m,∀m ≤M given the curve and Dn in Eq. (6) 6: Augment the data into Dn and estimate Logistic curve hyperparameters m0 and g0. 7: end for Experimental setup. All experimental results are averaged over 20 independent runs with differ- ent random seeds. Final performance is estimated by evaluating the chosen hyperparameter over the maximum number of iterations. All experiments are executed on a NVIDIA 1080 GTX GPU using the tensorﬂow-gpu Python package. The DRL environments are available through the OpenAI gym [3] and Mujoco [43]. Our DRL implementations are based on the open source from Open AI Baselines [6]. We release our implementation at https://github.com/ntienvu/BOIL. We use square-exponential kernels for the GP in our model and estimate their parameters by maxi- mizing the marginal likelihood [31]. We set the maximum number of augmented points to beM = 15 and a threshold for a natural log of GP condition numberδ = 20. We note that the optimization over- head is much less than the black-box function evaluation time. Baselines. We compare with Hyperband [23] which demonstrated empirical successes in tuning deep learning applications in an iteration-efﬁcient manner. We extend the discrete multi-task BO [41] to the continuous case – which can also be seen as continuous multi-ﬁdelity BO [16, 39] as in our setting, they both consider cost-sensitivity and iteration-efﬁciency. We, therefore, label the two baselines as continuous multi-task/ﬁdelity BO (CM-T/F-BO). We have ignored the minor difference in these settings, such as multi-task approaches jointly optimizes the ﬁdelity and input while BOCA [16] ﬁrst selects the input and then the ﬁdelity. Our focus is to demonstrate the effectiveness of optimizing the learning curve using compression and augmentation techniques. We therefore omit the comparison of various acquisition functions and kernel choices which can easily be used in our model. We also do not compare with Fabolas [17] which is designed to vary dataset sizes, not iteration numbers. We would expect the performance of Fabolas to be close to CM-T/F-BO. We are unable to compare with FreezeThaw as the code is not available. However, the curves in our setting are not exponential decays and thus ill-suited to their model (see last ﬁgure in the appendix). We have considered an ablation study in the appendix using a time kernel following the exponential decay proposed in Freeze-thaw method [42]. Task descriptions. We consider three DRL settings including a Dueling DQN (DDQN) [46] agent in the CartPole-v0 environment and Advantage Actor Critic (A2C) [25] agents in the InvertedPendulum-v2 and Reacher-v2 environments. In addition to the DRL applications, we tune 6 hyperparameters for training a convolutional neural network [21] on the SVHN dataset and CI- FAR10. Due to space considerations, we refer to the appendix for further details. 4.1 Model illustration /uni00000013/uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni00000013 /uni00000018 /uni00000014/uni00000013 /uni00000014/uni00000018/uni00000006/uni00000024/uni00000058/uni0000004a/uni00000050/uni00000048/uni00000051/uni00000057/uni00000048/uni00000047/uni00000003/uni00000032/uni00000045/uni00000056 /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni00000057/uni00000048/uni00000047/uni00000003/uni00000024/uni00000058/uni0000004a/uni00000050/uni00000048/uni00000051/uni00000057/uni00000048/uni00000047/uni00000003/uni00000032/uni00000045/uni00000056 Figure 4: DDQN on CartPole. The number of augmented observations reduces over time. We ﬁrst illustrate the estimated compression func- tion l(m∗ 0,g∗ 0) in Right Fig. 1 from different experi- ments. These Logistic parameters g∗ 0 and m∗ 0 are es- timated by maximizing the GP marginal likelihood and used for compressing the curve. We show that the estimated curve from CartPole tends to reach the highest performance much earlier than Reacher because CartPole is somewhat easier to train than Reacher. We next examine the count of augmented observa- tions generated per iteration in Fig. 4. Although this number is ﬂuctuating, it tends to reduce over 6/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000018/uni00000013/uni00000013 /uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048 /uni0000001a/uni00000013 /uni00000019/uni00000013 /uni00000018/uni00000013 /uni00000017/uni00000013 /uni00000016/uni00000013 /uni00000015/uni00000013 /uni00000014/uni00000013 /uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni00000035/uni00000048/uni00000044/uni00000046/uni0000004b/uni00000048/uni00000055/uni00000040/uni00000003/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047/uni00000003/uni00000058/uni00000056/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000032/uni00000053/uni00000057/uni0000004c/uni00000050/uni0000004c/uni0000005d/uni00000048/uni00000047/uni00000003/uni00000033/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013/uni00000015/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013/uni0000001b/uni00000013/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000014/uni00000015/uni00000013/uni00000013/uni00000014/uni00000017/uni00000013/uni00000013 /uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048 /uni00000015/uni00000018 /uni00000018/uni00000013 /uni0000001a/uni00000018 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000015/uni00000018 /uni00000014/uni00000018/uni00000013 /uni00000014/uni0000001a/uni00000018 /uni00000015/uni00000013/uni00000013/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni0000002c/uni00000059/uni00000033/uni00000048/uni00000051/uni00000040/uni00000003/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047/uni00000003/uni00000058/uni00000056/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000032/uni00000053/uni00000057/uni0000004c/uni00000050/uni0000004c/uni0000005d/uni00000048/uni00000047/uni00000003/uni00000033/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000025/uni00000032/uni0000002c/uni0000002f Figure 5: The learning curves of the best found parameters by different approaches. The curves show that BO-L and BOIL reliably identify parameters leading to stable training. BOIL takes only half total time to ﬁnd this optimal curve. time. BOIL does not add more augmented observations at the later stage when we have gained sufﬁcient information and GP covariance conditioning falls below our threshold δ = 20. 4.2 Ablation study of curve compression To demonstrate the impact of our training curve compression, we compare BOIL to vanilla Bayesian optimization (BO) and with compression (BO-L) given the same number of iterations at Tmax. We show that using the curve compression leads to stable performance, as opposed to the existing tech- nique of averaging the last iterations. We plot the learning curves of the best hyperparameters identiﬁed by BO, BO-L and BOIL. Fig. 5 shows the learning progress over Tmax episodes for each of these. The curves are smoothed by averaging over 100 consecutive episodes for increased clarity. We ﬁrst note that all three algorithms eventually obtain similar performance at the end of learning. However, since BO-L and BOIL take into account the preceding learning steps, they achieve higher performance more quickly. Furthermore, they achieve this more reliably as evidenced by the smaller error bars (shaded regions). 4.3 Tuning deep reinforcement learning and CNN We now optimize hyperparameters for deep reinforcement learning algorithms; in fact, this applica- tion motivated the development of BOIL. The combinations of hyperparameters to be tuned, target DRL algorithm and environment can be found in the appendix. Comparisons by iterations and real-time. Fig. 6 illustrates the performance of different algo- rithms against the number of iterations as well as real-time (the plots for CIFAR10 are in the ap- pendix). The performance is the utility score of the best hyperparameters identiﬁed by the baselines. Across all three tasks, BOIL identiﬁes optimal hyperparameters using signiﬁcantly less computation time than other approaches. The plots show that other approaches such as BO and BO-L can identify well-performing hyperpa- rameters in fewer iterations than BOIL. However, they do so only considering costly, high-ﬁdelity evaluations resulting in signiﬁcantly higher evaluation times. In contrast to this behavior, BOIL ac- counts for the evaluation costs and chooses to initially evaluate low-ﬁdelity settings consuming less time. This allows fast assessments of a multitude of hyperparameters. The information gathered here is then used to inform later point acquisitions. Hereby, the inclusion of augmented observations is crucial in offering useful information readily available from the data. In addition, this augmenta- tion is essential to prevent from the GP kernel issue instead of adding the full curve of points into our GP model. Hyperband [23] exhibits similar behavior in that it uses low ﬁdelity (small t) evaluations to reduce a pool of randomly sampled conﬁgurations before evaluating at high ﬁdelity (large t). To deal with noisy evaluations and other effects, this process is repeated several times. This puts Hyperband at a disadvantage particularly in the noisy DRL tasks. Since early performance ﬂuctuates hugely, Hyperband can be misled in where to allocate evaluation effort. It is then incapable of revising 7/uni00000013 /uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni00000014/uni00000014/uni0000001a/uni00000011/uni00000018 /uni00000014/uni00000015/uni00000013/uni00000011/uni00000013 /uni00000014/uni00000015/uni00000015/uni00000011/uni00000018 /uni00000014/uni00000015/uni00000018/uni00000011/uni00000013 /uni00000014/uni00000015/uni0000001a/uni00000011/uni00000018 /uni00000014/uni00000016/uni00000013/uni00000011/uni00000013 /uni00000014/uni00000016/uni00000015/uni00000011/uni00000018 /uni00000014/uni00000016/uni00000018/uni00000011/uni00000013 /uni00000014/uni00000016/uni0000001a/uni00000011/uni00000018/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000027/uni00000027/uni00000034/uni00000031/uni00000010/uni00000026/uni00000044/uni00000055/uni00000057/uni00000033/uni00000052/uni0000004f/uni00000048/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000003/uni00000044/uni00000051/uni00000047/uni00000003 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013 /uni00000018/uni00000013/uni00000014/uni00000013/uni00000013/uni00000014/uni00000018/uni00000013/uni00000015/uni00000013/uni00000013/uni00000015/uni00000018/uni00000013/uni00000016/uni00000013/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni00000014/uni00000014/uni0000001a/uni00000011/uni00000018 /uni00000014/uni00000015/uni00000013/uni00000011/uni00000013 /uni00000014/uni00000015/uni00000015/uni00000011/uni00000018 /uni00000014/uni00000015/uni00000018/uni00000011/uni00000013 /uni00000014/uni00000015/uni0000001a/uni00000011/uni00000018 /uni00000014/uni00000016/uni00000013/uni00000011/uni00000013 /uni00000014/uni00000016/uni00000015/uni00000011/uni00000018 /uni00000014/uni00000016/uni00000018/uni00000011/uni00000013 /uni00000014/uni00000016/uni0000001a/uni00000011/uni00000018/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000027/uni00000027/uni00000034/uni00000031/uni00000010/uni00000026/uni00000044/uni00000055/uni00000057/uni00000033/uni00000052/uni0000004f/uni00000048/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000003/uni00000044/uni00000051/uni00000047/uni00000003 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013 /uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni00000014/uni00000015/uni0000001b /uni00000014/uni00000015/uni0000001c /uni00000014/uni00000016/uni00000013 /uni00000014/uni00000016/uni00000014 /uni00000014/uni00000016/uni00000015 /uni00000014/uni00000016/uni00000016 /uni00000014/uni00000016/uni00000017/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni00000035/uni00000048/uni00000044/uni00000046/uni0000004b/uni00000048/uni00000055/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013 /uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni00000014/uni00000015/uni0000001b /uni00000014/uni00000015/uni0000001c /uni00000014/uni00000016/uni00000013 /uni00000014/uni00000016/uni00000014 /uni00000014/uni00000016/uni00000015 /uni00000014/uni00000016/uni00000016 /uni00000014/uni00000016/uni00000017/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni00000035/uni00000048/uni00000044/uni00000046/uni0000004b/uni00000048/uni00000055/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013 /uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni00000015/uni00000018 /uni00000016/uni00000013 /uni00000016/uni00000018 /uni00000017/uni00000013 /uni00000017/uni00000018 /uni00000018/uni00000013 /uni00000018/uni00000018 /uni00000019/uni00000013 /uni00000019/uni00000018/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c /uni0000003e/uni00000026/uni00000031/uni00000031/uni00000010/uni00000036/uni00000039/uni0000002b/uni00000031/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000019/uni00000003/uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000053/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013/uni0000001a/uni00000013/uni0000001b/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni00000015/uni00000018 /uni00000016/uni00000013 /uni00000016/uni00000018 /uni00000017/uni00000013 /uni00000017/uni00000018 /uni00000018/uni00000013 /uni00000018/uni00000018 /uni00000019/uni00000013 /uni00000019/uni00000018/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c /uni0000003e/uni00000026/uni00000031/uni00000031/uni00000010/uni00000036/uni00000039/uni0000002b/uni00000031/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000019/uni00000003/uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000053/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f Figure 6: Comparison over BO evaluations (Left) and real-time (Right). Given the same time bud- get, CM-T/F-BO, Hyperband and BOIL can take more evaluations than vanilla BO, BO-L and Rand. BOIL outperforms other competitors in ﬁnding the optimal parameters in an iteration-efﬁcient man- ner. CM-T/F-BO does not augment the observations from the curve and requires more evaluations. The results of InvertedPendulum and CNN-CIFAR10 are in the appendix. these choices until an entirely new pool of hyperparameters is sampled and evaluated from scratch. In contrast to this, BOIL is more ﬂexible than Hyperband in that it can freely explore-exploit the whole joint space. The GP surrogate hereby allows BOIL to generalize across hyperparameters and propagate information through the joint space. 5 Conclusion and Future work Our framework complements the existing BO toolbox for hyperparameter tuning with iterative learn- ing. We present a way of leveraging our understanding that later stages of the training process are informed by progress made in earlier ones. This results in a more iteration-efﬁcient hyperparame- ter tuning algorithm that is applicable to a broad range of machine learning systems. We evaluate its performance on a set of diverse benchmarks. The results demonstrate that our model surpasses the performance of well-established alternatives while consuming signiﬁcantly fewer resources. Fi- nally, we note that our approach is not necessarily speciﬁc to machine learning algorithms, but more generally applies to any process exhibiting an iterative structure to be exploited. 86 Broader Impact Our work aims at making the optimization of processes operating in a step-wise fashion more efﬁ- cient. As demonstrated this makes BOIL particularly well-suited to supporting supervised learning models and RL systems. By increasing training efﬁcience of these models, we hope to contribute to their widespread deployment whilst reducing the computational and therefore environmental cost their implementation has. Deep (reinforcement) learning systems ﬁnd application in a wide range of settings that directly contribute to real world decisions, e.g., natural language processing, visual task, autonomous driving and many more. As machine learning models building on our contributions are being deployed in the real world, we encourage practicioners to put in place necessary supervision and override mechanisms as precautions against potential failure. In a more general context, our algorithm may be seen as a step towards the construction of an automated pipeline for the training and deployment of machine learning models. A potential danger is that humans become further and further removed from the modelling process, making it harder to spot (potentially critical) failures. We do not see this as an argument against the construction of such a pipeline in principle, but instead encourage practicioners to reﬂect on potential biases indirectly encoded in the choice of data sets and models, they are feeding into said automated processes. The growing opacity of machine learning models is a concern of its own and which automated training procedures will only contribute to. Opposing this is a rapidly growing corpus of work addressing the interpretability of trained machine learning models and their decision making. These can and should be used to rigorously analyse ﬁnal training outcomes. Only then can we ensure that machine learning algorithm do indeed become a beneﬁcial source of information guiding real world policy making as opposed to opaque, unquestioned entities. While our main interest lies in the hyperparameter optimization of machine learning models, it should be noted that any iterative process depending on a set of parameters can make use of our con- tributions. Possible settings could, for instance, include the optimization of manufacturing pipelines in which factory setting are adjusted to increase productivity. 7 Acknowledgements S. Schulze is supported by an I-CASE studentship funded by the EPSRC and Dyson. References [1] Christopher M Bishop. Pattern recognition and machine learning. springer New York, 2006. [2] Eric Brochu, Vlad M Cora, and Nando De Freitas. A tutorial on Bayesian optimization of ex- pensive cost functions, with application to active user modeling and hierarchical reinforcement learning. arXiv preprint arXiv:1012.2599, 2010. [3] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016. [4] Yutian Chen, Aja Huang, Ziyu Wang, Ioannis Antonoglou, Julian Schrittwieser, David Silver, and Nando de Freitas. Bayesian optimization in AlphaGo. arXiv preprint arXiv:1812.06855, 2018. [5] Zhongxiang Dai, Haibin Yu, Bryan Kian Hsiang Low, and Patrick Jaillet. Bayesian optimiza- tion meets Bayesian optimal stopping. In International Conference on Machine Learning , pages 1496–1506, 2019. [6] Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford, John Schulman, Szymon Sidor, Yuhuai Wu, and Peter Zhokhov. Openai baselines. GitHub, GitHub repository, 2017. [7] Tobias Domhan, Jost Tobias Springenberg, and Frank Hutter. Speeding up automatic hy- perparameter optimization of deep neural networks by extrapolation of learning curves. In Twenty-Fourth International Joint Conference on Artiﬁcial Intelligence, 2015. 9[8] Stefan Falkner, Aaron Klein, and Frank Hutter. Bohb: Robust and efﬁcient hyperparameter optimization at scale. In International Conference on Machine Learning , pages 1436–1445, 2018. [9] Peter I Frazier. A tutorial on Bayesian optimization. arXiv preprint arXiv:1807.02811, 2018. [10] Yarin Gal, Riashat Islam, and Zoubin Ghahramani. Deep Bayesian active learning with image data. In Proceedings of the 34th International Conference on Machine Learning, pages 1183– 1192, 2017. [11] Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. Deep reinforcement learning that matters. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence, 2018. [12] Philipp Hennig and Christian J Schuler. Entropy search for information-efﬁcient global opti- mization. Journal of Machine Learning Research, 13:1809–1837, 2012. [13] José Miguel Hernández-Lobato, Matthew W Hoffman, and Zoubin Ghahramani. Predictive entropy search for efﬁcient global optimization of black-box functions. In Advances in Neural Information Processing Systems, pages 918–926, 2014. [14] Donald R Jones, Matthias Schonlau, and William J Welch. Efﬁcient global optimization of expensive black-box functions. Journal of Global optimization, 13(4):455–492, 1998. [15] M. I. Jordan and T. M. Mitchell. Machine learning: Trends, perspectives, and prospects. Science, 349(6245):255–260, 2015. [16] Kirthevasan Kandasamy, Gautam Dasarathy, Jeff Schneider, and Barnabás Póczos. Multi- ﬁdelity Bayesian optimisation with continuous approximations. In Proceedings of the 34th International Conference on Machine Learning, pages 1799–1808, 2017. [17] Aaron Klein, Stefan Falkner, Simon Bartels, Philipp Hennig, and Frank Hutter. Fast Bayesian optimization of machine learning hyperparameters on large datasets. In Artiﬁcial Intelligence and Statistics, pages 528–536, 2017. [18] Aaron Klein, Stefan Falkner, Jost Tobias Springenberg, and Frank Hutter. Learning curve pre- diction with Bayesian neural networks. International Conference on Learning Representations (ICLR), 2017. [19] Andreas Krause and Cheng S Ong. Contextual Gaussian process bandit optimization. In Advances in Neural Information Processing Systems, pages 2447–2455, 2011. [20] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolutional neural networks. In Advances in Neural Information Processing Systems, pages 1097–1105, 2012. [21] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. [22] Benjamin Letham, Brian Karrer, Guilherme Ottoni, Eytan Bakshy, et al. Constrained Bayesian optimization with noisy experiments. Bayesian Analysis, 14(2):495–519, 2019. [23] Lisha Li and Kevin Jamieson. Hyperband: A novel bandit-based approach to hyperparameter optimization. Journal of Machine Learning Research, 18:1–52, 2018. [24] Mark McLeod, Stephen Roberts, and Michael A Osborne. Optimization, fast and slow: Op- timally switching between local and Bayesian optimization. In International Conference on Machine Learning, pages 3440–3449, 2018. [25] V olodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforce- ment learning. In International conference on machine learning, pages 1928–1937, 2016. [26] V olodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. NIPS Deep Learning Workshop, 2013. [27] Vu Nguyen, Sunil Gupta, Santu Rana, Cheng Li, and Svetha Venkatesh. Regret for expected improvement over the best-observed value and stopping condition. In Proceedings of The 9th Asian Conference on Machine Learning (ACML), pages 279–294, 2017. [28] Vu Nguyen and Michael A Osborne. Knowing the what but not the where in Bayesian opti- mization. In International Conference on Machine Learning, pages 7317–7326, 2020. 10[29] Michael Osborne, Roman Garnett, Zoubin Ghahramani, David K Duvenaud, Stephen J Roberts, and Carl E Rasmussen. Active learning of model evidence using Bayesian quadrature. In Advances in Neural Information Processing Systems, pages 46–54, 2012. [30] Jack Parker-Holder, Vu Nguyen, and Stephen Roberts. Provably efﬁcient online hyperparame- ter optimization with population-based bandits. In Advances in Neural Information Processing Systems, 2020. [31] Carl Edward Rasmussen. Gaussian processes for machine learning. 2006. [32] Binxin Ru, Mark McLeod, Diego Granziol, and Michael A Osborne. Fast information-theoretic Bayesian optimisation. In International Conference on Machine Learning, pages 4381–4389, 2018. [33] Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. International Conference on Learning Representations, 2016. [34] Rajat Sen, Kirthevasan Kandasamy, and Sanjay Shakkottai. Multi-ﬁdelity black-box opti- mization with hierarchical partitions. In International conference on machine learning, pages 4538–4547, 2018. [35] Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P Adams, and Nando de Freitas. Taking the human out of the loop: A review of Bayesian optimization. Proceedings of the IEEE , 104(1):148–175, 2016. [36] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanc- tot, et al. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484, 2016. [37] Leslie N Smith. A disciplined approach to neural network hyper-parameters: Part 1–learning rate, batch size, momentum, and weight decay. arXiv preprint arXiv:1803.09820, 2018. [38] Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical Bayesian optimization of ma- chine learning algorithms. In Advances in Neural Information Processing Systems , pages 2951–2959, 2012. [39] Jialin Song, Yuxin Chen, and Yisong Yue. A general framework for multi-ﬁdelity Bayesian optimization with Gaussian processes. In The 22nd International Conference on Artiﬁcial Intelligence and Statistics, pages 3158–3167, 2019. [40] Niranjan Srinivas, Andreas Krause, Sham Kakade, and Matthias Seeger. Gaussian process optimization in the bandit setting: No regret and experimental design. In Proceedings of the 27th International Conference on Machine Learning, pages 1015–1022, 2010. [41] Kevin Swersky, Jasper Snoek, and Ryan P Adams. Multi-task Bayesian optimization. In Advances in Neural Information Processing Systems, pages 2004–2012, 2013. [42] Kevin Swersky, Jasper Snoek, and Ryan Prescott Adams. Freeze-thaw Bayesian optimization. arXiv preprint arXiv:1406.3896, 2014. [43] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026–5033. IEEE, 2012. [44] Zi Wang and Stefanie Jegelka. Max-value entropy search for efﬁcient Bayesian optimization. In International Conference on Machine Learning, pages 3627–3635, 2017. [45] Ziyu Wang and Nando de Freitas. Theoretical analysis of Bayesian optimisation with unknown Gaussian process hyper-parameters. arXiv preprint arXiv:1406.7758, 2014. [46] Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and Nando Freitas. Dueling network architectures for deep reinforcement learning. In International Conference on Machine Learning, pages 1995–2003, 2016. [47] Jian Wu and Peter Frazier. The parallel knowledge gradient method for batch Bayesian opti- mization. In Advances In Neural Information Processing Systems, pages 3126–3134, 2016. [48] Jian Wu, Saul Toscano-Palmerin, Peter I Frazier, and Andrew Gordon Wilson. Practical multi- ﬁdelity Bayesian optimization for hyperparameter tuning. In 35th Conference on Uncertainty in Artiﬁcial Intelligence, 2019. 11The following sections are intended to give the reader further insights into our design choices and a deeper understanding of the algorithms properties. First, we give a brief overview of Bayesian optimization with Gaussian processes. We then illustrate our models behavior on a two dimensional problem. Last, we give further details of our experiments for reproducibility purposes. A Bayesian Optimization Preliminaries Bayesian optimization is a sequential approach to global optimization of black-box functions with- out making use of derivatives. It uses two components: a learned surrogate model of the objective function and an acquisition function derived from the surrogate for selecting new points to inform the surrogate with. In-depth discussions beyond our brief overview can be found in recent surveys [2, 9, 35]. Notation. We summarize all of the notations used in our model in Table 1 for ease of reading. A.1 Gaussian processes We present the GP surrogate model for the black-box function f [31]. A GP deﬁnes a probability distribution over functions f under the assumption that any subset of points {(xi, f (xi)}is normally distributed. Formally, this is denoted as: f (x) ∼GP ( m(x),k ( x,x′)) , where m(x) and k (x,x′) are the mean and covariance functions, given by m(x) =E[ f (x)] and k(x,x′) =E [ ( f (x)−m(x))( f (x′)−m(x′))T ] . Typically, the mean of the GP is assumed to be zero everywhere. The kernel k(x,x′) can be thought of as a similarity measure relating f (x) and f (x′). Numerous kernels encoding different prior be- liefs about f (x) have been proposed. A popular choice is given by the square exponential kernel k(x,x′) =σ2 f exp [ −(x −x′)2/2σ2 l ] . The length- and output-scales σ2 l and σ2 f regulate the maximal covariance between two points and can be estimated using maximum marginal likelihood. The SE kernel encodes the belief that nearby points are highly correlated as it is maximized at k(x,x) =σ2 f and decays the further x and x′are separated. For predicting f∗= f (x∗) at a new data point x∗, assuming a zero mean m(x) =0, we have: [ f f∗ ] ∼N ( 0, [ K kT ∗ k∗ k∗∗ ]) (7) where k∗∗= k (x∗,x∗), k∗= [k (x∗,xi)]∀i≤N and K = [k (xi,xj)]∀i, j≤N . The conditional probability of p( f∗|f ) follows a univariate Gaussian distribution as p( f∗|f ) ∼N ( µ (x∗),σ2 (x∗) ) . Its mean and variance are given by: µ (x∗) =k∗K−1y σ2 (x∗) =k∗∗−k∗K−1kT ∗. As GPs give full uncertainty information with any prediction, they provide a ﬂexible nonparametric prior for Bayesian optimization. We refer the interested readers to [31] for further details on GPs. A.2 Acquisition function Bayesian optimization is typically applied in settings in which the objective function is expensive to evaluate. To minimize interactions with that objective, an acquisition function is deﬁned to reason about the selection of the next evaluation point xt+1 = argmaxx∈X αt (x). The acquisition func- tion is constructed from the predictive mean and variance of the surrogate to be easy to evaluate and represents the trade-off between exploration (of points with high predictive uncertainty) and exploitation (of points with high predictive mean). Thus, by design the acquisition function can be maximized with standard global optimization toolboxes. Among the many acquisition functions [12, 13, 14, 32, 40, 44] available in the literature, the expected improvement [14, 27, 45] is one of the most popular. 12Table 1: Notation List Parameter Domain Meaning d integer, N dimension, no. of hyperparameters to be optimized x vector,Rd input hyperparameter N integer, N maximum number of BO iterations Tmin, Tmax integer, N the min/max no of iterations for training a ML algorithm t ∈[Tmin,...Tmax] index of training steps M integer, N the maximum number of augmentation. We set M = 15. δ scalar, R threshold for rejecting augmentation when ln of cond(K) > δ m ∈{1,...M} index of augmenting variables n ∈{1,..., N} index of BO iterations z = [x,t] vector, Rd+1 concatenation of the parameter x and iteration t cn,m scalar, R training cost (sec) yn scalar, R transformed score at the BO iteration n yn,m scalar, R transformed score at the BO iteration n, training step m α(x,t) function acquisition function for performance µc(x,t) function estimation of the cost by LR given x and t r(. |x,t) function a raw learning curve, r(x,t) = [r(1 |x,t),...r(t′|x,t),r(t |x,t)] f (x,t) function a black-box function which is compressed from the above f () l (. |m0,g0) function Logistic curve l(u |m0,g0) = 1 1+exp(−g0[u−m0]) g0, g∗ 0 scalar, R a growth parameter deﬁning a slope, g∗ 0 = argmaxg0 L m0, m∗ 0 scalar, R a middle point parameter, m∗ 0 = argmaxm0 L L scalar, R Gaussian process log marginal likelihood A.3 GP kernels and treatment of GP hyperparameters We present the GP kernels and treatment of GP hyperparameters for the black-box function f . Although the raw learning curve in DRL is noisy, the transformed version using our proposed curve compression is smooth. Therefore, we use two squared exponential kernels for input hyperparameter and training iteration, respectively. That iskx(x,x′) =exp ( −||x−x′||2 2σ2x ) and kt (t,t′) =exp ( −||t−t′||2 2σ2t ) where the observation x and t are normalized to [0,1]d and the outcome y is standardized y ∼ N (0,1) for robustness. As a result, our product kernel becomes k ( [x,t],[x′,t′] ) = k(x,x′)×k(t,t′) =exp ( −||x −x′||2 2σ2x −||t −t′||2 2σ2t ) . The length-scales σx and σt are learnable parameters indicating the variability of the function with regards to the hyperparameter input x and number of training iterations t. Estimating appropriate values for them is critical as this represents the GPs prior regarding the sensitivity of performance w.r.t. changes in the number of training iterations and hyperparameters. For extremely large σt we expect the objective function to change very little for different numbers of training iterations. For small σt by contrast we expect drastic changes even for small differences. We estimate these GP hyperparameters (including the length-scalesσx, σt and the output noise varianceσy) by maximizing their log marginal likelihood [31]. We optimize Eq. (5) with a gradient-based optimizer, providing the analytical gradient to the algo- rithm. We start the optimization from the previous hyperparameter values θprev. If the optimization fails due to numerical issues, we keep the previous value of the hyperparameters. We reﬁt the hy- perparameters every 3×d function evaluations where d is the dimension. B Algorithm Illustration and Further Experiments Fig. 7 and Fig. 8 illustrate the behavior of our proposed algorithm BOIL on the example of opti- mizing the discount factor γ of Dueling DQN [46] on the CartPole problem. The two settings differ in the inclusion augmented observations into BOIL in Fig. 7 and CM-T/F-BO (or BOIL without augmented observations) in Fig. 8. 130.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 1.2 0.8 0.4 0.0 0.4 0.8 1.2 1.6 2.0 2.4 0.00 0.15 0.30 0.45 0.60 0.75 0.90 1.05 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 0.0 0.4 0.8 1.2 1.6 2.0 2.4 2.8 3.2 3.6 0.3 0.6 0.9 1.2 1.5 1.8 2.1 2.4 2.7 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0 2.5 0.00 0.06 0.12 0.18 0.24 0.30 0.36 0.42 0.48 0.64 0.72 0.80 0.88 0.96 1.04 1.12 1.20 1.28 1.36 0.0 0.4 0.8 1.2 1.6 2.0 2.4 2.8 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 1.6 1.2 0.8 0.4 0.0 0.4 0.8 1.2 1.6 2.0 0.00 0.15 0.30 0.45 0.60 0.75 0.90 1.05 0.64 0.72 0.80 0.88 0.96 1.04 1.12 1.20 1.28 1.36 0.0 0.4 0.8 1.2 1.6 2.0 2.4 2.8 3.2 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 2.4 1.6 0.8 0.0 0.8 1.6 2.4 3.2 0.000 0.025 0.050 0.075 0.100 0.125 0.150 0.175 0.200 0.64 0.72 0.80 0.88 0.96 1.04 1.12 1.20 1.28 1.36 0.0 0.6 1.2 1.8 2.4 3.0 3.6 4.2 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 2.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0 0.00 0.15 0.30 0.45 0.60 0.75 0.90 1.05 0.64 0.72 0.80 0.88 0.96 1.04 1.12 1.20 1.28 0.0 0.4 0.8 1.2 1.6 2.0 2.4 2.8 3.2 3.6 0.0 0.4 0.8 1.2 1.6 2.0 2.4 2.8 3.2 3.6 Figure 7: Illustration of BOIL on a 2-dimensional optimization task of DDQN on CartPole. The augmented observations ﬁll the joint hyperparameter-iteration space quickly to inform our surrogate. Our decision balances utility α against cost τ for iteration-efﬁciency. Especially in situations of multiple locations sharing the same utility value, our algorithm prefers to select the cheapest option. Table 2: Dueling DQN algorithm on CartPole problem. Variables Min Max Best Found x∗ γ discount factor 0 .8 1 0 .95586 learning rate model 1 e−6 0.01 0 .00589 #Episodes 300 800 - 140.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 2.0 1.6 1.2 0.8 0.4 0.0 0.4 0.8 0.00 0.15 0.30 0.45 0.60 0.75 0.90 1.05 0.60 0.75 0.90 1.05 1.20 1.35 1.50 1.65 1.80 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 0.0 0.3 0.6 0.9 1.2 1.5 1.8 2.1 2.4 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 1.6 1.2 0.8 0.4 0.0 0.4 0.8 1.2 0.00 0.15 0.30 0.45 0.60 0.75 0.90 1.05 0.60 0.75 0.90 1.05 1.20 1.35 1.50 1.65 1.80 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 1.6 1.2 0.8 0.4 0.0 0.4 0.8 1.2 1.6 0.00 0.08 0.16 0.24 0.32 0.40 0.48 0.56 0.64 0.60 0.75 0.90 1.05 1.20 1.35 1.50 1.65 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 0.30 0.45 0.60 0.75 0.90 1.05 1.20 1.35 1.50 1.65 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 1.6 1.2 0.8 0.4 0.0 0.4 0.8 1.2 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 2.0 1.6 1.2 0.8 0.4 0.0 0.4 0.8 1.2 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 Figure 8: Illustration of the Continuous Multi task/ﬁdelity BO (CM-T/F-BO) -- this is the case of BOIL without using augmented observations (same setting as Fig. 7). This version leads to less efﬁcient optimization as the additional iteration dimension requires more evaluation than optimizing the hyperparameters on their own. 15Table 3: A2C algorithm on Reacher (left) and InvertedPendulum (right). Variables Min Max Best Found x∗ γ discount factor 0 .8 1 0 .8 learning rate actor 1 e−6 0.01 0 .00071 learning rate critic 1 e−6 0.01 0 .00042 #Episodes 200 500 - Min Max Best Found x∗ 0.8 1 0 .95586 1e−6 0.01 0 .00589 1e−6 0.01 0 .00037 700 1500 - Table 4: Convolutional Neural Network. Variables Min Max Best Found x∗ ﬁlter size 1 8 5 pool size 1 5 5 batch size 16 1000 8 learning rate 1 e−6 0.01 0 .000484 momentum 0 .8 0 .999 0 .82852 decay 0 .9 0 .999 0 .9746 number of epoch 30 150 - In both cases, we plot the GP predictive mean in Eq. (1), GP predictive variance in Eq. (2), the acquisition function in Eq. (3), the predicted function and the ﬁnal decision function in Eq. (8). These equations are deﬁned in the main manuscript. As shown in the respective ﬁgures the ﬁnal decision function balances between utility and cost of any pair (γ,t) to achieve iteration efﬁciency. Especially in situations where multiple locations share the same utility value, our decision will prefer to select the cheapest option. Using the augmented observations in Fig. 7, our joint space is ﬁlled quicker with points and the uncertainty (GP variance) across it reduces faster than in Fig. 8 – the case of vanilla CM-T/F-BO without augmenting obser- vations. A second advantage of having augmented observations is that the algorithm is discouraged to select the same hyperparameter setting at lower ﬁdelity than a previous evaluation. We do not add the full curve as it can be redundant while causing the conditioning problem of the GP covariance matrix. B.1 Experiment settings We summarize the hyperparameter search ranges for A2C on Reacher and InvertedPendulum in Table 3, CNN on SHVN in Table 4 and DDQN on CartPole in Table 2. Additionally, we present the best found parameter x∗for these problems. Further details of the DRL agents are listed in Table 5. B.2 Learning Logistic Function We ﬁrst present the Logistic curve l(u |x,t) = 1 1+exp(−g0[u−m0]) using different choices of g0 and m0 in Fig. 10. We then learn from the data to get the optimal choices g∗ 0 and m∗ 0 presented in Fig. 11. Table 5: Further speciﬁcation for DRL agents Hyperparameter Value A2C Critic-network architecture [32,32] Actor-network architecture [32,32] Entropy coefﬁcient 0 .01 Dueling DQN Q-network architecture [50,50] ε-greedy (start, ﬁnal, number of steps) (1.0,0.05,10000) Buffer size 10000 Batch size 64 PER-α [33] 1 .0 PER-β (start, ﬁnal, number of steps) (1.0,0.6,1000) 160 100 200 300 400 500 Episodes 70 60 50 40 30 20 Average Reward Preference Curve as Sigmoid Best Found Reward Curve Sigmoid Curve 0 100 200 300 400 500 Episodes 90 80 70 60 50 40 30 20 10 Reward Preference Curve as Sigmoid Best Found Reward Curve Sigmoid Curve 0 100 200 300 400 500 Episodes 60 50 40 30 20 10 Average Reward Preference Curve as Log Best Found Reward Curve Log Curve 0 100 200 300 400 500 Episodes 100 80 60 40 20 0 Reward Preference Curve as Log Best Found Reward Curve Log Curve 0 100 200 300 400 500 Episodes 60 50 40 30 20 Average Reward Preference Curve as Average Best Found Reward Curve Average Curve 0 100 200 300 400 500 Episodes 80 60 40 20 0 Reward Preference Curve as Average Best Found Reward Curve Average Curve Figure 9: To highlight the robustness, we examine the results using different preference functions such as Sigmoid curve, Log curve, and Average curve on Reacher experiments. The results include the best found reward curve with different preference choices that show the robustness of our model. Left column: the best found curve using averaged reward over 100 consecutive episodes. Right column: the best found curve using the original reward. 17/uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000019 /uni00000013/uni00000011/uni0000001a m0/uni00000020/uni00000010/uni00000016/uni00000003g0/uni00000020/uni00000013/uni00000011/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000016/uni00000003g0/uni00000020/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000016/uni00000003g0/uni00000020/uni00000015 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000016/uni00000003g0/uni00000020/uni00000016 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000019 /uni00000013/uni00000011/uni0000001a m0/uni00000020/uni00000010/uni00000015/uni00000003g0/uni00000020/uni00000013/uni00000011/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000015/uni00000003g0/uni00000020/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000015/uni00000003g0/uni00000020/uni00000015 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000015/uni00000003g0/uni00000020/uni00000016 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000019 m0/uni00000020/uni00000010/uni00000014/uni00000003g0/uni00000020/uni00000013/uni00000011/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000014/uni00000003g0/uni00000020/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000014/uni00000003g0/uni00000020/uni00000015 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000014/uni00000003g0/uni00000020/uni00000016 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000019 m0/uni00000020/uni00000013/uni00000003g0/uni00000020/uni00000013/uni00000011/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000013/uni00000003g0/uni00000020/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000013/uni00000003g0/uni00000020/uni00000015 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000013/uni00000003g0/uni00000020/uni00000016 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000019 m0/uni00000020/uni00000014/uni00000003g0/uni00000020/uni00000013/uni00000011/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000014/uni00000003g0/uni00000020/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000014/uni00000003g0/uni00000020/uni00000015 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000014/uni00000003g0/uni00000020/uni00000016 Figure 10: Examples of Logistic function l(u) = 1 1+exp(−g0[u−m0]) with different values of middle parameter m0 and growth parameter g0. B.3 Robustness over Different Preference Functions We next study the learning effects with respect to different choices of the preference functions. We pick three preference functions including the Sigmoid, Log and Average to compute the utility score for each learning curve. Then, we report the best found reward curve under such choices. The experiments are tested using A2C on Reacher-v2. The results presented in Fig. 9 demonstrate the robustness of our model with the preference functions. B.4 Applying Freeze-Thaw BO in the settings considered While both the exponential decay in Freeze-Thaw BO [42] and our compression function encode preferences regarding training development, there is an important distinction between the two ap- proaches. Freeze-thaw BO utilises the exponential decay property to terminate the training curve, while BOIL only uses the sigmoid curve to guide the search. We refer to Fig. 13 for further illustra- tion of why Freeze-thaw BO struggles in DRL settings. B.5 Ablation Study using Freeze-Thraw Kernel for Time In the joint modeling framework of hyperparameter and time (iteration), we can replace the kernel either k(x,x) or k(t,t) with different choices. We, therefore, set up a new baseline of using the time- kernel k(t,t′) in Freeze-Thaw approach [42] which encodes the monotonously exponential decay from the curve. Particularly, we use the kernel deﬁned as k(t,t′) = βα (t +t′+β)α for parameters α,β > 0 which are optimized in the GP models. 186  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l( . | m * 0 ,g * 0 ) for CartPole m * 0 =-3.266   g * 0 =3.0 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l(m * 0 ,g * 0 ) for CNN_SHVN m * 0 =2.245   g * 0 =2.092 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l(m * 0 ,g * 0 ) for InvPendulum m * 0 =1.649   g * 0 =1.833 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l( . | m * 0 ,g * 0 ) for Cifar10 m * 0 =-4.0   g * 0 =1.476 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l( . | m * 0 ,g * 0 ) for Reacher m * 0 =2.779   g * 0 =1.973 Figure 11: We learn the suitable transformation curve directly from the data. We parameterized the Logistic curve as l (m0,g0) = 1 1+exp(−g0[1−m0]) then estimate g0 and m0. The estimated function l(m∗ 0,g∗ 0) is then used to compress our curve. The above plots are the estimated l() at different environments and datasets. /uni00000013/uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013/uni0000001a/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni0000001b/uni00000013 /uni0000001c/uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000014/uni00000013 /uni00000014/uni00000015/uni00000013/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni0000002c/uni00000051/uni00000059/uni00000048/uni00000055/uni00000057/uni00000048/uni00000047/uni00000033/uni00000048/uni00000051/uni00000047/uni00000058/uni0000004f/uni00000058/uni00000050/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013 /uni00000018/uni00000013/uni00000014/uni00000013/uni00000013/uni00000014/uni00000018/uni00000013/uni00000015/uni00000013/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni0000001b/uni00000013 /uni0000001c/uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000014/uni00000013 /uni00000014/uni00000015/uni00000013/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni0000002c/uni00000051/uni00000059/uni00000048/uni00000055/uni00000057/uni00000048/uni00000047/uni00000033/uni00000048/uni00000051/uni00000047/uni00000058/uni0000004f/uni00000058/uni00000050/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013 /uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni00000014/uni0000001a/uni00000011/uni00000018 /uni00000015/uni00000013/uni00000011/uni00000013 /uni00000015/uni00000015/uni00000011/uni00000018 /uni00000015/uni00000018/uni00000011/uni00000013 /uni00000015/uni0000001a/uni00000011/uni00000018 /uni00000016/uni00000013/uni00000011/uni00000013 /uni00000016/uni00000015/uni00000011/uni00000018/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c /uni0000003e/uni00000026/uni00000031/uni00000031/uni00000010/uni00000026/uni0000004c/uni00000049/uni00000044/uni00000055/uni00000014/uni00000013/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000019/uni00000003/uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000053/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000014/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000018/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni00000014/uni0000001a/uni00000011/uni00000018 /uni00000015/uni00000013/uni00000011/uni00000013 /uni00000015/uni00000015/uni00000011/uni00000018 /uni00000015/uni00000018/uni00000011/uni00000013 /uni00000015/uni0000001a/uni00000011/uni00000018 /uni00000016/uni00000013/uni00000011/uni00000013 /uni00000016/uni00000015/uni00000011/uni00000018/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c /uni0000003e/uni00000026/uni00000031/uni00000031/uni00000010/uni00000026/uni0000004c/uni00000049/uni00000044/uni00000055/uni00000014/uni00000013/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000019/uni00000003/uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000053/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f Figure 12: Tuning hyperparameters of a DRL on InvertedPendulum and a CNN model on CIFAR10. 190 250 500 750 1000 1250 1500 Epoch 0 20 40 60 80 100 120Reward Reward Curve Freeze-thaw 0 250 500 750 1000 1250 1500 Epoch 0 50 100 150 0 250 500 750 1000 1250 1500 Epoch 0 50 100 150 0 250 500 750 1000 1250 1500 Epoch 0 50 100 150 Reward Curves Examples using A2C on Inverted Pendulum Figure 13: Illustration of Freeze-thaw BO in DRL. Freeze-thaw BO will terminate training processes when training performance (in blue) signiﬁcantly drops (i.e. at the red locations) as the exponential decay model will predict low ﬁnal performance. In most RL enviroments noisy training curves are unavoidable. Thus, Freeze-thaw BO will dismiss all curves including good setting, never completing a single training run before the ﬁnal epoch. /uni00000013 /uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni00000014/uni00000015/uni0000001b /uni00000014/uni00000015/uni0000001c /uni00000014/uni00000016/uni00000013 /uni00000014/uni00000016/uni00000014 /uni00000014/uni00000016/uni00000015 /uni00000014/uni00000016/uni00000016 /uni00000014/uni00000016/uni00000017/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni00000035/uni00000048/uni00000044/uni00000046/uni0000004b/uni00000048/uni00000055/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000025/uni00000032/uni0000002c/uni0000002f/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni00000014/uni00000015/uni0000001b /uni00000014/uni00000015/uni0000001c /uni00000014/uni00000016/uni00000013 /uni00000014/uni00000016/uni00000014 /uni00000014/uni00000016/uni00000015 /uni00000014/uni00000016/uni00000016 /uni00000014/uni00000016/uni00000017/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni00000035/uni00000048/uni00000044/uni00000046/uni0000004b/uni00000048/uni00000055/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000025/uni00000032/uni0000002c/uni0000002f/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000013 /uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni0000001c/uni00000013 /uni0000001c/uni00000018 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000018 /uni00000014/uni00000014/uni00000013 /uni00000014/uni00000014/uni00000018 /uni00000014/uni00000015/uni00000013 /uni00000014/uni00000015/uni00000018/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni0000002c/uni00000051/uni00000059/uni00000048/uni00000055/uni00000057/uni00000048/uni00000047/uni00000033/uni00000048/uni00000051/uni00000047/uni00000058/uni0000004f/uni00000058/uni00000050/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000026/uni00000010/uni00000030/uni00000012/uni00000037/uni00000010/uni00000025/uni00000032/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000025/uni00000032/uni0000002c/uni0000002f/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000013 /uni00000018/uni00000013/uni00000014/uni00000013/uni00000013/uni00000014/uni00000018/uni00000013/uni00000015/uni00000013/uni00000013/uni00000015/uni00000018/uni00000013/uni00000016/uni00000013/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni0000001c/uni00000013 /uni0000001c/uni00000018 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000018 /uni00000014/uni00000014/uni00000013 /uni00000014/uni00000014/uni00000018 /uni00000014/uni00000015/uni00000013 /uni00000014/uni00000015/uni00000018/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni0000002c/uni00000051/uni00000059/uni00000048/uni00000055/uni00000057/uni00000048/uni00000047/uni00000033/uni00000048/uni00000051/uni00000047/uni00000058/uni0000004f/uni00000058/uni00000050/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000025/uni00000032/uni0000002c/uni0000002f/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a Figure 14: Comparison using freezethaw kernel for time component. We present the result in Fig. 14 that CM-T/F-BO is still less competitive to BOIL using this speciﬁc time kernel. The results again validate the robustness our approach cross different choices of kernel. B.6 Additional Experiments for Tuning DRL and CNN We present the additional experiments for tuning a DRL model using InvertedPendulum environ- ment and a CNN model using a subset of CIFAR10 in Fig. 12. Again, we show that the proposed model clearly gain advantages against the baselines in tuning hyperparameters for model with itera- tive learning information available. B.7 Examples of Deep Reinforcement Learning Training Curves Finally, we present examples of training curves produced by the deep reinforcement learning al- gorithm A2C in Fig. 15. These ﬂuctuate widely and it may not be trivial to deﬁne good stopping criteria as done for other applications in previous work [42]. 200 200 400 80 70 60 50 40 0 200 400 110 100 90 80 70 60 50 0 200 400 110 100 90 80 70 60 50 0 200 400 70 60 50 40 30 0 200 400 70 60 50 40 30 20 10 0 200 400 50 40 30 20 0 200 400 90 80 70 60 50 0 200 400 70 60 50 40 30 20 10 0 200 400 85 80 75 70 0 200 400 60 50 40 30 20 0 200 400 100 90 80 70 60 0 200 400 100 90 80 70 0 200 400 80 60 40 20 0 200 400 100 90 80 70 60 0 200 400 100 90 80 70 0 200 400 80 70 60 50 40 30 20 0 200 400 70 60 50 40 30 20 0 200 400 70 60 50 40 30 20 0 200 400 100 90 80 70 60 50 0 200 400 70 60 50 40 30 20 0 200 400 100 90 80 70 60 50 0 200 400 100 90 80 70 60 0 200 400 60 50 40 30 20 0 200 400 80 60 40 20 0 200 400 85 80 75 70 65 60 0 200 400 100 95 90 85 80 0 200 400 80 60 40 20 0 200 400 100 90 80 70 60 50 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 Figure 15: Examples of reward curves using A2C on Reacher-v2 (rows 1 −3) and on InvertedPendulum-v2 (rows 4 −6). Y-axis is the reward averaged over 100 consecutive episodes. X-axis is the episode. The noisy performance illustrated is typical of DRL settings and complicates the design of early stopping criteria. Due to the property of DRL, it is not trivial to decide when to stop the training curve. In addition, it will be misleading if we only take average over the last 100 iterations. 21",
      "meta_data": {
        "arxiv_id": "1909.09593v5",
        "authors": [
          "Vu Nguyen",
          "Sebastian Schulze",
          "Michael A Osborne"
        ],
        "published_date": "2019-09-20T16:14:34Z",
        "pdf_url": "https://arxiv.org/pdf/1909.09593v5.pdf"
      }
    },
    {
      "title": "Provably Efficient Online Hyperparameter Optimization with Population-Based Bandits",
      "abstract": "Many of the recent triumphs in machine learning are dependent on well-tuned\nhyperparameters. This is particularly prominent in reinforcement learning (RL)\nwhere a small change in the configuration can lead to failure. Despite the\nimportance of tuning hyperparameters, it remains expensive and is often done in\na naive and laborious way. A recent solution to this problem is Population\nBased Training (PBT) which updates both weights and hyperparameters in a single\ntraining run of a population of agents. PBT has been shown to be particularly\neffective in RL, leading to widespread use in the field. However, PBT lacks\ntheoretical guarantees since it relies on random heuristics to explore the\nhyperparameter space. This inefficiency means it typically requires vast\ncomputational resources, which is prohibitive for many small and medium sized\nlabs. In this work, we introduce the first provably efficient PBT-style\nalgorithm, Population-Based Bandits (PB2). PB2 uses a probabilistic model to\nguide the search in an efficient way, making it possible to discover high\nperforming hyperparameter configurations with far fewer agents than typically\nrequired by PBT. We show in a series of RL experiments that PB2 is able to\nachieve high performance with a modest computational budget.",
      "full_text": "Provably Efﬁcient Online Hyperparameter Optimization with Population-Based Bandits Jack Parker-Holder University of Oxford jackph@robots.ox.ac.uk Vu Nguyen University of Oxford vu@robots.ox.ac.uk Stephen J. Roberts University of Oxford sjrob@robots.ox.ac.uk Abstract Many of the recent triumphs in machine learning are dependent on well-tuned hyperparameters. This is particularly prominent in reinforcement learning (RL) where a small change in the conﬁguration can lead to failure. Despite the importance of tuning hyperparameters, it remains expensive and is often done in a naive and laborious way. A recent solution to this problem is Population Based Training (PBT) which updates both weights and hyperparameters in a single training run of a population of agents. PBT has been shown to be particularly effective in RL, leading to widespread use in the ﬁeld. However, PBT lacks theoretical guarantees since it relies on random heuristics to explore the hyperparameter space. This inefﬁciency means it typically requires vast computational resources, which is prohibitive for many small and medium sized labs. In this work, we introduce the ﬁrst provably efﬁcient PBT-style algorithm, Population-Based Bandits (PB2). PB2 uses a probabilistic model to guide the search in an efﬁcient way, making it possible to discover high performing hyperparameter conﬁgurations with far fewer agents than typically required by PBT. We show in a series of RL experiments that PB2 is able to achieve high performance with a modest computational budget. 1 Introduction Deep neural networks [22, 26, 38] have achieved remarkable success in a variety of ﬁelds. Some of the most notable results have come in reinforcement learning (RL), where the last decade has seen a series of signiﬁcant achievements in games [ 60, 48, 8] and robotics [ 51, 33]. However, it is notoriously difﬁcult to reproduce RL results, often requiring excessive trial-and-error to ﬁnd the optimal hyperparameter conﬁgurations [6, 24, 50]. This has led to a surge in popularity for Automated Machine Learning (AutoML, [29]), which seeks to automate the training of machine learning models. A key component in AutoML is automatic hyperparameter selection [7, 47], where popular approaches include Bayesian Optimization (BO, [10, 25, 49]) and Evolutionary Algorithms (EAs, [ 12, 27]). Using automated methods for RL (AutoRL) is crucial for accessibility and for generalization, since different environments typically require totally different hyperparameters [ 24]. Furthermore, it may even be possible to improve performance of existing methods using learned parameters. In fact, BO was revealed to play a valuable role in AlphaGo, improving the win percentage before the ﬁnal match with Lee Sedol [13]. A particularly promising approach, Population Based Training (PBT, [32, 39]), showed it is possible to achieve impressive performance by updating both weights and hyperparameters during asingle training run of a population of agents. PBT works in a similar fashion to a human observing experiments, periodically replacing weaker performers with superior ones. PBT has shown to be particularly effective in reinforcement learning, and has been used in a series of recent works to improve performance [56, 44, 17, 31]. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada. arXiv:2002.02518v4  [cs.LG]  4 Jun 2021copy network  weights from  high performing agent high performance low perf BO A single training process Parallel Agent 1 Parallel Agent \u0000 \u0000 suggested by BO BO network w eights   ( \u0000 \u0000 ) hyperparameters  ( \u0000 \u0000 ) stop continue Figure 1: Population-Based Bandit Optimization: a population of agents is trained in parallel. Each agent has weights (grey) and hyperparameters (blue). The agents are evaluated periodically (orange bar), and if an agent is underperforming, it’s weights are replaced by randomly copying one of the better performing agents, and its hyperparameters are selected using Bayesian Optimization. However, PBT’s Achilles heel comes from its reliance on heuristics for exploring the hyperparameter space. This essentially creates new meta-parameters, which need to be tuned. In many cases, PBT underperforms a random baseline without vast computational resources, since small populations can collapse to a suboptimal mode. In addition, PBT lacks theoretical grounding, given that greedy exploration methods suffer from unbounded regret. Our key contribution is the ﬁrst provably efﬁcient PBT-style algorithm, Population-Based Bandit Optimization, or PB2 (Fig. 1). To do this, we draw the connection between maximizing the reward in a PBT-style training procedure to minimizing the regret in bandit optimization [61, 14]. Formally, we consider the online hyperparameter selection problem as batch Gaussian Process bandit optimization of a time-varying function. PB2 is more computationally efﬁcient than the existing batch BO approaches [15, 20] since it can (1) learn the optimal schedule of hyperparameters and (2) optimize them in a single training run. We derive a bound on the cumulative regret for PB2, the ﬁrst such result for a PBT-style algorithm. Furthermore, we show in a series of RL experiments that PB2 is able to achieve high rewards with a modest computational budget. 2 Problem Statement In this paper, we consider the problem of selecting optimal hyperparameters, x b t, from a compact, convex subset D∈ Rd where d is the number of hyperparameters. Here the index b refers to the bth agent in a population/batch, and the subscript t represents the number of timesteps/epochs/iterations elapsed during the training of a neural network. Particularly, we consider the schedule of optimal hyperparameters over time ( x b t ) t=1,...T. Population-Based Training (PBT, [ 32]) is a well-known algorithm for learning a schedule of hyperparameters by training a population (or batch) ofB agents in parallel. Each agentb ∈B has both hyperparameters x b t ∈Rdand weights θb t. At every tready step interval (i.e. if t mod tready = 0 ), the agents are ranked and the worst performing agents are replaced with members of the best performing agents (A ⊂B ) as follows: • Weights (θb t): copied from one of the best performing agents, i.e. θj t ∼Unif {θj t}j∈A. • Hyperparameters (x b t): with probability ϵ it uses random exploration, and re-samples from the original distribution, otherwise, it usesgreedy exploration, and perturbs one of the best performing agents, i.e {x j ×λ }j∈A,λ ∼[0 .8 ,1 .2] . This leads to the learning of hyperparameter schedules – a single agent can have different conﬁgurations at different time-steps during a single training process. This is important in the context of training deep reinforcement learning agents as dynamic hyperparameter schedules have been shown to be effective [ 52, 32, 17]. On the other hand, most of the existing hyperparameter optimization approaches aim to ﬁnd a ﬁxed set of hyperparameters over the course of optimization. To formalize this problem, let F t( x t) be an objective function under a given set of hyperparameters at timestep t. An example of F t( x t) could be the reward for a deep RL agent. When training for a 2total of T steps, our goal is to maximize the ﬁnal performance FT(xT). We formulate this problem as optimizing the time-varying black-box reward function ft, over D. Every tready steps, we observe and record noisy observations, yt = ft(xt) +ϵt, where ϵt ∼N (0,σ2I) for some ﬁxed σ2. The function ft represents the change in Ft after training for tready steps, i.e. Ft −Ft−tready . We deﬁne the best choice at each timestep as x∗ t = arg maxxt∈Dft(xt), and so the regret of each decision as rt = ft(x∗ t) −ft(xt). Lemma 1. Maximizing the ﬁnal performance FT of a model with respect to a given hyperparameter schedule {xt}T t=1 is equivalent to maximizing the time-varying black-box function ft(xt) and minimizing the corresponding cumulative regretrt(xt), max FT(xT) = max T∑ t=1 ft(xt) = min T∑ t=1 rt(xt). (1) In subsequent sections, we present a time-varying bandit approach which is used to minimize the cumulative regret RT = ∑T t=1 rt. Lemma 1 shows this is equivalent to maximizing the ﬁnal performance/reward of a neural network model (see: Section 9 in the appendix for the proof). 3 Population-Based Bandit Optimization We now introduce Population-Based Bandit Optimization (PB2) for optimizing the hyperparameter schedule ( xb t ) ,∀t= 1,...,T using parallel agents b= 1,...,B . After each agent bcompletes tready training steps, we store the data (yb t,t,x b t) in a dataset Dt which will be used to make an informed decision for the next set of hyperparameters. We below present the mechanism to select the next hyperparameters for parallel agents. Motivated by the equivalence of the maximized reward and minimized cumulative regret in Lemma 1, we propose the parallel time-varying bandit optimization. 3.1 Parallel Gaussian Process Bandits for a Time-Varying Function We ﬁrst describe the time-varying Gaussian process as the surrogate models, then we extend it to the parallel setting for our PB2 algorithm. Time-varying Gaussian process as the surrogate model. Following previous works in the GP- bandit literature [61], we model ft using a Gaussian Process (GP, [54]) which is speciﬁed by a mean function µt : X→ R and a kernel (covariance function) k: D×D→ R. If ft ∼GP(µt,k), then ft(xt) is distributed normally N(µt(xt),k(xt,xt)) for all xt ∈D. After we have observed T data points {(xt,f(xt))}T t=1, the GP posterior belief at new point x′ t ∈D, ft(x′ t) follows a Gaussian distribution with mean µt(x′) and variance σ2 t(x′) as: µt(x′) := kt(x′)T(Kt + σ2I)−1yt (2) σ2 t(x′) := k(x′,x′) −kt(x′)T(Kt + σ2I)−1kt(x′), (3) where Kt := {k(xi,xj)}t i,j=1 and kt := {k(xi,x′ t)}t i=1. The GP predictive mean and variance above will later be used to represent the exploration-exploitation trade-off in making decision under the presence of uncertainty. To represent the non-stationary nature of a neural network training process, we cast the problem of optimizing neural network hyperparameters as time-varying bandit optimization. We follow [9] to formulate this problem by modeling the reward function under the time-varying setting as follows: f1(x) =g1(x), ft+1(x) = √ 1 −ωft(x) +√ωfgt+1(x) ∀t≥2, (4) where g1,g2,... are independent random functions with g∼GP(0,k) and ω∈[0,1] models how the function varies with time, such that if ω= 0we return to GP-UCB and if ω= 1then each evaluation is completely independent. This model introduces a new hyperparameter (ω), however, we note it can be optimized by maximizing the marginal likelihood for a trivial additional cost compared to the expensive function evaluations (we include additional details on this in the Appendix). This leads to the extensions of Eqs. (2) and (3) using the new covariance matrix ˜Kt = Kt ◦Ktime t where Ktime t = [(1−ω)|i−j|/2]T i,j=1 and ˜kt(x) =kt ◦ktime t with ktime t = [(1−ω)(T+1−i)/2]T i=1. Here ◦ refers to the Hadamard product. 3Selecting hyperparameters for parallel agents. In the PBT setting, we consider an entire population of parallel agents. This changes the problem from a sequential to a batch blackbox optimization. This poses an additional challenge to select multiple points simultaneously xb t without full knowledge of all {(xj t,yj t)}j−1 j=1. A key observation in [14] is that since a GP’s variance (Eqn. 3) does not depend on yt, the acquisition function can account for incomplete trials by updating the uncertainty at the pending evaluation points. Concretely, we deﬁne xb t to be the b-th point selected in a batch, after t timesteps. This point may draw on information from t+ (b−1) previously selected points. In the single agent, sequential case, we set B = 1and recover t,b = t−1. Thus, at the iteration t, we ﬁnd a next batch of Bsamples [ x1 t,x2 t,...xB t ] by sequentially maximizing the following acquisition function: xb t = arg max x∈D µt,1(x) + √ βtσt,b(x),∀b= 1,...B (5) for βt > 0. In Eqn. (5) we have the mean from the previous batch ( µt,1(x)) which is ﬁxed, but can update the uncertainty using our knowledge of the agents currently training ( σt,b(x)). This signiﬁcantly reduces redundancy, as the model is able to explore distinct regions of the space. 3.2 PB2 Algorithm and Convergence Guarantee To estimate the GP predictive distribution in Eqs. (2, 3), we use the product form˜k= kSE ◦ktime [35, 9], which considers the time varying nature of training a neural network. Then, we use Eqn. (5) to select a batch of points by utilizing the reduction in uncertainty for models currently training. As far as we know, this is the ﬁrst use of a time-varying kernel in the PBT-style setting. Unlike PBT, this allows us to efﬁciently make use of data from previous trials when selecting new conﬁgurations, rather than reverting to a uniform prior, or perturbing existing conﬁgurations. The full algorithm is shown in Algorithm 1. Algorithm 1:Population-Based Bandit Optimization (PB2) Initialize:Network weights{θb 0}B b=1, hyperparameters{xb 0}B b=1, datasetD0 = ∅ (in parallel) fort= 1,...,T −1 do 1. Update Models:θb t ←step(θb t−1|xb t−1) 2. Evaluate Models:yb t = Ft(xb t) −Ft−1(xb t−1) +ϵt for allb 3. Record Data:Dt = Dt−1 ∪{(yb t,t,xb t)}B b=1 4. If t mod tready = 0: • Copy weights:Rank agents, ifθb is in the bottomλ% then copy weightsθj from the topλ%. • Select hyperparameters:Fit a GP model toDt and select hyper-parametersxb t,∀b≤Bby maximizing Eq. (5). Return the best trained modelθ Next we present our main theoretical result, showing that PB2 is able to achieve sublinear regret when the time-varying function is correlated. This is the ﬁrst such result for a PBT-style algorithm. Theorem 2. Let the domain D⊂ [0,r]d be compact and convex where d is the dimension and suppose that the kernel is such that ft ∼GP(0,k) is almost surely continuously differentiable and satisﬁes Lipschitz assumptions ∀Lt ≥0,t ≤T ,p(sup ⏐⏐⏐∂ft(x) ∂x(d) ⏐⏐⏐≥Lt) ≤ae−(Lt/b)2 for some a,b. Pick δ∈(0,1), set βT = 2 logπ2T2 2δ + 2dlog rdbT2 √ log daπ2T2 2δ and deﬁne C1 = 32/log(1 +σ2 f), the PB2 algorithm satisﬁes the following regret bound after T time steps over Bparallel agents with probability at least 1 −δ: RTB = T∑ t=1 ft(x∗ t) − max b=1,...,B ft(xt,b) ≤ √ C1TβT ( T ˜NB + 1 )( γ˜NB + [ ˜NB ]3 ω ) + 2 the bound holds for any block length ˜N ∈{1,...,T }and B ≪T. This result shows the regret for PB2 decreases as we increase the population size B. This should be expected, since adding computational resources should beneﬁt ﬁnal performance (which is equivalent to minimizing regret). We demonstrate this property in Table 1. When using a single agentB = 1, our bound becomes the time-varying GP-UCB setting in [9]. 4In our setting, if the time-varying function is highly correlated, i.e., the information between f1(.) and fT(.) does not change signiﬁcantly, we have ω→0 and ˜N →T. Then, the regret bound grows sublinearly with the number of iterations T, i.e., limT→∞ RTB T = 0. On the other hand (in the worst case), if the time-varying function is not correlated at all, such as ˜N →1 and ω →1, then PB2 achieves linear regret [9]. Remark. This theoretical result is novel and signiﬁcant in two folds. First, by showing the equivalent between maximizing reward and minimizing the bandit regret, this regret bound quantiﬁes the gap between the optimal reward and the achieved reward (using parameters selected by PB2). The regret bound extends the result established by [61, 9], to a more general case with parallelization. To the best of our knowledge, this is the ﬁrst kind of convergence guarantee in a PBT-style algorithm. Our approach is advantageous against all existing batch BO approaches [14, 20] in that PB2 considers optimization in a single training run of parallel agents while the existing works need to evaluate using multiple training runs of parallel agents which are more expensive. In addition, PB2 can learn a schedule of hyperparameters while the existing batch BO can only learn a static conﬁguration. 4 Related Work Hyperparameter Optimization. Hyperparameter optimization [34, 7, 30] is a crucial component of any high performing machine learning model. In the past, methods such as grid search and random search [5] have proved popular, however, with increased focus on Automated Machine Learning (AutoML, [29]), there has been a great deal of progress moving beyond these approaches. This success has led to a variety of tools becoming available [19, 45]. Population Based Approaches. Taking inspiration from biology, population-based methods have proved effective for blackbox optimization problems [46]. Evolutionary Algorithms (EAs, [3]) take many forms, one such method is Lamarckian EAs [ 65] in which parameters are inherited whilst hyperparameters are evolved. Meanwhile, other methods learn both hyperparameters and weights [12, 27]. These works motivate the recently introduced PBT algorithm [ 32], whereby network parameters are learned via gradient descent, while hyperparameters are evolved. Its key strengths lie in the ability to learn high performing hyperparameter schedules in a single training run, leading to strong performance in a variety of settings [ 44, 39, 17, 56]. PBT works by focusing on high performing conﬁgurations, however, this greedy property means it is unlikely to explore areas of the search space which are “late bloomers”. In addition, the core components of the algorithm rely on handcrafted meta-hyperparameters, such as the degree of mutation. These design choices mean the algorithm lacks theoretical guarantees. We address these issues in our work. Bayesian Optimization. Bayesian Optimization (BO [ 10, 59, 55]) is a sequential model-based blackbox optimization method [ 28]. BO works by building a surrogate model of the blackbox function, typically taken to be a Gaussian Process [ 54]. BO has been shown to produce state-of- the-art results in terms of sample efﬁciency, making dramatic gains in several prominent use cases [13]. Over the past few years there has been increasing focus on distributed implementations [21, 2] which seek to select a batch of conﬁgurations for concurrent evaluation. Despite this increased efﬁciency, these methods still require multiple training runs. Another recent method, Freeze-Thaw Bayesian Optimization [63] considers a ‘bag’ of current solutions, with their future loss assumed to follow an exponential decay. The next model to optimize is chosen via entropy maximisation. This approach bears similarities in principle with PBT, but from a Bayesian perspective. The method, however, makes assumptions about the shape of the loss curve, does not adapt hyperparameters during optimization (as in PBT) and is sequential rather than parallelized. Our approach takes appealing properties of both these methods, using the Bayesian principles but adapting in an online fashion. Hybrid Algorithms. Bayesian and Evolutionary approaches have been combined in the past. In [53], the authors demonstrate using an early version of BO improves a simple Genetic Algorithm, while [1] shows promising results through combining BO and random search. In addition, the recently popular Hyperband algorithm [40], was shown to exhibit stronger performance with a BO controller [18]. The main weakness of these methods is their inability to learn schedules. Hyperparameter Optimization for Reinforcement Learning (AutoRL). Finally, methods have been proposed speciﬁcally for RL, dating back to the early 1990s [62]. These methods have typically 5had a narrower focus, optimizing an individual parameter. More recent work [ 52] proposes to adapt hyperparameters online, by exploring different conﬁgurations in an off-policy manner in between iterations. Additionally [ 16] concurrently proposed a similar to PBT, using evolutionary hyperparameter updates speciﬁcally for RL. These methods show the beneﬁt of re-using samples, and we believe it would be interesting future work to consider augmenting our method with off-policy or synthetic samples (from a learned dynamics model) for the speciﬁc RL use case. 5 Experiments We focus our experiments on the RL setting, since it is notoriously sensitive to hyperparameters [24]. We evaluate population sizes ofB ∈{4,8}, which means the algorithm can be run locally on most modern computers. This is signiﬁcantly less than the B >20 used in the original PBT paper [32]. All experiments were conducted using the tune library [43, 42]1. Our GP implementation is extended from GPy [23], where we use the squared exponential kernel in combination with the time kernel as described in Section 3. We optimize all GP-kernel hyperparameters, as well as the block length ˜N, by maximizing the marginal likelihood [54]. 5.1 On Policy Reinforcement Learning We consider optimizing a policy for continuous control problems from the OpenAI Gym [11]. In particular, we seek to optimize the hyperparameters for Proximal Policy Optimization (PPO, [ 58]), for the following tasks: BipedalWalker, LunarLanderContinuous, Hopper and InvertedDoublePendulum. Our primary benchmark is PBT, where we use an identical conﬁguration to PB2 aside from the selection of xb t (the explore step). We also compare against a random search (RS) baseline [ 5]. Random search is a challenging baseline because it does not make assumptions about the underlying problem, and typically achieves close to optimal performance asymptotically [ 29]. We include a Bayesian Optimization (BO) baseline which uses the Expected Improvement acquisition function. Finally, we also compare our results against a recent state-of-the-art distributed algorithm (ASHA, [41]). ASHA is one of the most recent distributed methods, shown to outperform PBT for supervised learning. We believe we are the ﬁrst to test it for RL. Table 1: Median best performing agent across 10 seeds. The best performing algorithms are bolded. B RS BO ASHA PBT PB2 vs. PBT BipedalWalker 4 234 133 236 223 276 +24% LunarLanderContinuous 4 161 206 213 159 235 +48% Hopper 4 1638 1760 1819 1492 2346 +57% InvertedDoublePendulum4 8094 8607 7899 8893 8179 -8% BipedalWalker 8 240 237 255 277 291 +5% LunarLanderContinuous 8 175 240 231 247 275 +11% For all environments we use a neural network policy with two 32-unit hidden layers and tanh activations. During training, we optimize the following hyperparameters: batch size, learning rate, GAE parameter ( λ, [57]) and PPO clip parameter ( ϵ). We use the same ﬁxed ranges across all four environments (included in the Appendix Section 8). All experiments are conducted for 106 environment timesteps, with the tready command triggered every 5 ×104 timesteps. For BO, we train each agent sequentially for 500ksteps, and selects the best to train for the remaining budget. For ASHA, we initialize a population of 18 agents to compare against B = 4and 48 agents for B = 8. These were chosen to achieve the same total budget with the grace period equal to the tready criteria for PBT and PB2. Given the typically noisy evaluation of policy gradient algorithms [24] we repeat each experiment with ten seeds. We show the median best reward achieved from each run in Table 1 and plot the median best performing agent from each run, with the interquartile ranges (IQRs) in Fig. 2. 1See code here: https://github.com/jparkerholder/PB2. 6(a) BipedalWalker (4)  (b) LLC (4)  (c) Hopper (4) (d) IDP (4)  (e) BipedalWalker (8)  (f) LLC (8) Figure 2: Median best performing agent across ten seeds, with IQR shaded. Population size (B) is shown in brackets. In almost all cases we see performance gains from PB2 vs. PBT. In fact, 3/4 of cases PBT actually underperforms random search with the smaller population size (B = 4), demonstrating its reliance on large computational resources. This is conﬁrmed in the original PBT paper, where the smallest population size fails to outperform (see Table 1, [ 32]). One possible explanation for this is the greediness of PBT leads to prematurely abandoning promising regions of the search space. Another is that the small changes in parameters (multiple of 0.8 or 1.2) is mis-speciﬁed for discovering the optimal regions, thus requiring more initial samples to sufﬁciently span the space. This may also be the case if there is a shift later in the optimization process. Interestingly, PBT does perform well for InvertedDoublePendulum, this may be explained by the relative simplicity of the problem. We see that BO also performs well here, conﬁrming that it may not require the same degree of adaptation during training as the other tasks (such as BipedalWalker). For the larger population size we conﬁrm the effectiveness of PBT, which outperforms ASHA and Random Search. However PB2 outperforms PBT by +5% and +11% for the BipedalWalker and LunarLanderContinuous environments respectively. This shows that PB2 is able to scale to larger computational budgets. Interestingly, the state-of-the-art supervised learning performance of ASHA fails to translate to RL, where it clearly performs worse than both PBT and PB2 for the larger setting, and performs worse than PB2 for the smaller one. Figure 3: Median curves, IQR shaded. Robustness to Hyperparameter Ranges One key weakness of PBT is a reliance on a large population size to explore the hyperparameter space. This problem can be magniﬁed if the hyperparameter range is mis-speciﬁed or unknown (in a sense, the bounds placed on the hyperparameters may require tuning). PB2 avoids this issue, since it is able to select a point anywhere in the range, so does not rely on random sampling or gradual movements to get to optimal regions. We evaluate this by re- running the BipedalWalker task with a batch size drawn from {5000,200000}. This means many agents are initialized in a very inefﬁcient way, since when an agent has a batch size of 200,000 it is using 20% of total training samples for a single gradient step. As we see in Fig. 3 the performance for PBT is signiﬁcantly reduced, while PB2 is still able to learn good policies. While both methods perform worse than in Table 1, PB2 still achieves a median best of 203, vs. −12 for PBT. This is a critical issue, since for new problems we will not know the optimal hyperparameter ranges ex-ante, and thus require a method which is capable of learning without this knowledge. 7Figure 4: Median curves, IQR shaded. Scaling to Larger Populations. For PB2 to be broadly useful, it needs the ability to scale when more resources are available. To test this we repeated the BipedalWalker experiment with B = 16. As we see in Figure 4, both PBT and PB2 achieve optimal rewards (>300), but PB2 is still more efﬁcient. This is a promising initial result, although of course it will be interesting to test even larger settings in the future. 5.2 Off Policy Reinforcement Learning We now evaluate PB2 in a larger setting, optimizing hyperparameters for IMPALA [ 17]. in the breakout and SpaceInvaders environments from the Arcade Learning Environment [ 4]. In the original IMPALA paper, the best results come with the use of PBT with a population size of B = 24. Here we optimize the same three hyperparameters as in the original paper, but with a much smaller population (B = 4), making it essential to efﬁciently explore the hyperparameter space. We train for 10 million timesteps, equivalent to 40 million frames, and set tready to 5 ×105 timesteps. We repeat each experiment for 7 random seeds. PB2 achieves performance which is comparable with the hand-tuned performance reported in the rllib implementation,2 while PBT underperforms, particularly in the SpaceInvaders environment. (a) Breakout  (c) Space Invaders Figure 5: In both (a) and (b) we show training performance on the left, with median curves for seven seeds and inter-quartile range shaded. On the right, we show all agent conﬁgurations found by PB2. For each environment in Fig. 5 we include the hyperparameters used for all agents across all seeds of training. PB2 effectively explores the entire range of parameters, which enables it to ﬁnd optimal conﬁgurations even with a small number of trials. More details are in the Appendix, Section 8. 6 Conclusion We introduced Population-Based Bandits (PB2), the ﬁrst PBT-style algorithm with sublinear regret guarantees. PB2 replaces the heuristics from the original PBT algorithm with theoretically guided GP-bandit optimization. This allows it to balance exploration and exploitation in a principled manner, preventing mode collapse to suboptimal regions of the hyperparameter space and making it possible to ﬁnd high performing conﬁgurations with a small computational budget. Our algorithm complements the existing approaches for optimizing hyperparameters for deep learning frameworks. We believe the gains for reinforcement learning will be particularly useful, given the number of hyperparameters present, and the difﬁculty in optimizing them with existing techniques. Finally, we believe there are several future directions opened by taking our approach, such as updating population sizes based on the value of the acquisition function, and extending the search space to select the optimization algorithms or neural network architectures with BO. We also believe there may be further gains in reinforcement learning experiments through making use of off-policy data [52]. We leave these to exciting future work. 2See “RLlib IMPALA 32-workers” here:https://github.com/ray-project/rl-experiments 8Disclosure of Funding Nothing to declare. Broader Impact Population Based Training (PBT) has become a prominent algorithm in machine learning research, leading to gains in reinforcement learning (e.g. IMPALA [ 17]) and industrial applications (e.g. Waymo). As such, we believe the gains provided by PB2 will have a signiﬁcant impact. We believe this work will allow labs with small to medium sized computational resources to gain the beneﬁt of population-based training without the excessive computational cost required to ensure sufﬁcient exploration. This should be particularly helpful for achieving competitive performance in reinforcement learning experiments. To aid this, our implementation is integrated with the widely used ray library [43]. References [1] M. O. Ahmed. Do we need harmless Bayesian optimization and ﬁrst-order Bayesian optimization? In NIPS Workshop on Bayesian Optimization, 2017. [2] A. Alvi, B. Ru, J.-P. Calliess, S. Roberts, and M. A. Osborne. Asynchronous batch Bayesian optimisation with improved local penalisation. In Proceedings of the 36th International Conference on Machine Learning, pages 253–262, 2019. [3] P. J. Angeline. Evolutionary optimization versus particle swarm optimization: Philosophy and performance differences. In Evolutionary Programming VII, pages 601–610, Berlin, Heidelberg, 1998. Springer Berlin Heidelberg. [4] M. G. Bellemare, Y . Naddaf, J. Veness, and M. Bowling. The Arcade Learning Environment: An Evaluation Platform for General Agents. CoRR, abs/1207.4708, 2012. [5] J. Bergstra and Y . Bengio. Random search for hyper-parameter optimization. In Journal of Machine Learning Research. 2012. [6] J. Bergstra, D. Yamins, and D. Cox. Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures. In Proceedings of the 30th International Conference on Machine Learning, 2013. [7] J. S. Bergstra, R. Bardenet, Y . Bengio, and B. Kégl. Algorithms for hyper-parameter optimization. In Advances in Neural Information Processing Systems 24. 2011. [8] C. Berner, G. Brockman, B. Chan, V . Cheung, P. Debiak, C. Dennison, D. Farhi, Q. Fischer, S. Hashme, C. Hesse, R. Józefowicz, S. Gray, C. Olsson, J. Pachocki, M. Petrov, H. P. de Oliveira Pinto, J. Raiman, T. Salimans, J. Schlatter, J. Schneider, S. Sidor, I. Sutskever, J. Tang, F. Wolski, and S. Zhang. Dota 2 with large scale deep reinforcement learning. CoRR, abs/1912.06680, 2019. [9] I. Bogunovic, J. Scarlett, and V . Cevher. Time-Varying Gaussian Process Bandit Optimization. In Proceedings of the 19th International Conference on Artiﬁcial Intelligence and Statistics, 2016. [10] E. Brochu, V . M. Cora, and N. de Freitas. A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning. CoRR, abs/1012.2599, 2010. [11] G. Brockman, V . Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba. OpenAI Gym, 2016. [12] P. A. Castillo, V . Rivas, J. J. Merelo, J. Gonzalez, A. Prieto, and G. Romero. G-prop-ii: global optimization of multilayer perceptrons using gas. In Proceedings of the 1999 Congress on Evolutionary Computation-CEC99 (Cat. No. 99TH8406), 1999. 9[13] Y . Chen, A. Huang, Z. Wang, I. Antonoglou, J. Schrittwieser, D. Silver, and N. de Freitas. Bayesian optimization in AlphaGo. CoRR, abs/1812.06855, 2018. [14] T. Desautels, A. Krause, and J. W. Burdick. Parallelizing exploration-exploitation tradeoffs in Gaussian Process bandit optimization. Journal of Machine Learning Research, 15(119):4053– 4103, 2014. [15] T. Desautels, A. Krause, and J. W. Burdick. Parallelizing exploration-exploitation tradeoffs in Gaussian process bandit optimization. The Journal of Machine Learning Research, 2014. [16] S. Elfwing, E. Uchibe, and K. Doya. Online meta-learning by parallel algorithm competition. CoRR, abs/1702.07490, 2017. [17] L. Espeholt, H. Soyer, R. Munos, K. Simonyan, V . Mnih, T. Ward, Y . Doron, V . Firoiu, T. Harley, I. Dunning, S. Legg, and K. Kavukcuoglu. IMPALA: Scalable distributed deep-RL with importance weighted actor-learner architectures. In Proceedings of the 35th International Conference on Machine Learning, 2018. [18] S. Falkner, A. Klein, and F. Hutter. BOHB: Robust and efﬁcient hyperparameter optimization at scale. In Proceedings of the 35th International Conference on Machine Learning, 2018. [19] M. Feurer, A. Klein, K. Eggensperger, J. Springenberg, M. Blum, and F. Hutter. Efﬁcient and robust automated machine learning. In Advances in Neural Information Processing Systems 28. 2015. [20] J. González, Z. Dai, P. Hennig, and N. Lawrence. Batch bayesian optimization via local penalization. In Artiﬁcial intelligence and statistics, pages 648–657, 2016. [21] J. Gonzalez, Z. Dai, P. Hennig, and N. Lawrence. Batch Bayesian optimization via local penalization. In Proceedings of the 19th International Conference on Artiﬁcial Intelligence and Statistics, pages 648–657, 2016. [22] I. Goodfellow, Y . Bengio, and A. Courville. Deep Learning. The MIT Press, 2016. [23] GPy. GPy: A gaussian process framework in python. http://github.com/SheffieldML/ GPy, since 2012. [24] P. Henderson, R. Islam, P. Bachman, J. Pineau, D. Precup, and D. Meger. Deep reinforcement learning that matters. CoRR, abs/1709.06560, 2017. [25] P. Hennig and C. J. Schuler. Entropy search for information-efﬁcient global optimization. In Journal Machine Learning Research. 2012. [26] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 1997. [27] M. Husken, J. E. Gayko, and B. Sendhoff. Optimization for problem classes-neural networks that learn to learn. In 2000 IEEE Symposium on Combinations of Evolutionary Computation and Neural Networks. Proceedings of the First IEEE Symposium on Combinations of Evolutionary Computation and Neural Networks (Cat. No.00, 2000. [28] F. Hutter, H. H. Hoos, and K. Leyton-Brown. Sequential model-based optimization for general algorithm conﬁguration. In C. A. C. Coello, editor, Learning and Intelligent Optimization . Springer Berlin Heidelberg, 2011. [29] F. Hutter, L. Kotthoff, and J. Vanschoren, editors. Automated Machine Learning: Methods, Systems, Challenges. Springer, 2018. In press, available at http://automl.org/book. [30] F. Hutter, J. Lücke, and L. Schmidt-Thieme. Beyond manual tuning of hyperparameters. KI - Künstliche Intelligenz, 29(4):329–337, Nov 2015. [31] M. Jaderberg, W. M. Czarnecki, I. Dunning, L. Marris, G. Lever, A. G. Castañeda, C. Beattie, N. C. Rabinowitz, A. S. Morcos, A. Ruderman, N. Sonnerat, T. Green, L. Deason, J. Z. Leibo, D. Silver, D. Hassabis, K. Kavukcuoglu, and T. Graepel. Human-level performance in 3d multiplayer games with population-based reinforcement learning. Science, 364(6443):859–865, 2019. 10[32] M. Jaderberg, V . Dalibard, S. Osindero, W. M. Czarnecki, J. Donahue, A. Razavi, O. Vinyals, T. Green, I. Dunning, K. Simonyan, C. Fernando, and K. Kavukcuoglu. Population based training of neural networks. CoRR, abs/1711.09846, 2017. [33] D. Kalashnikov, A. Irpan, P. Pastor, J. Ibarz, A. Herzog, E. Jang, D. Quillen, E. Holly, M. Kalakrishnan, V . Vanhoucke, and S. Levine. Qt-opt: Scalable deep reinforcement learning for vision-based robotic manipulation. CoRR, abs/1806.10293, 2018. [34] R. Kohavi and G. H. John. Automatic parameter selection by minimizing estimated error. In In Proceedings of the Twelfth International Conference on Machine Learning. 1995. [35] A. Krause and C. S. Ong. Contextual gaussian process bandit optimization. In Advances in Neural Information Processing Systems, pages 2447–2455, 2011. [36] A. Krause, A. Singh, and C. Guestrin. Near-optimal sensor placements in Gaussian processes: Theory, efﬁcient algorithms and empirical studies. In Journal of Machine Learning Research. 2008. [37] A. Krizhevsky. Learning multiple layers of features from tiny images. University of Toronto, 2012. [38] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classiﬁcation with deep convolutional neural networks. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 25. 2012. [39] A. Li, O. Spyra, S. Perel, V . Dalibard, M. Jaderberg, C. Gu, D. Budden, T. Harley, and P. Gupta. A generalized framework for population based training. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. 2019. [40] L. Li, K. Jamieson, G. DeSalvo, A. Rostamizadeh, and A. Talwalkar. Hyperband: A novel bandit-based approach to hyperparameter optimization. In Journal Machine Learning Research. 2017. [41] L. Li, K. G. Jamieson, A. Rostamizadeh, E. Gonina, M. Hardt, B. Recht, and A. Talwalkar. Massively parallel hyperparameter tuning. CoRR, 2018. [42] E. Liang, R. Liaw, R. Nishihara, P. Moritz, R. Fox, K. Goldberg, J. E. Gonzalez, M. I. Jordan, and I. Stoica. RLlib: Abstractions for distributed reinforcement learning. In International Conference on Machine Learning (ICML), 2018. [43] R. Liaw, E. Liang, R. Nishihara, P. Moritz, J. E. Gonzalez, and I. Stoica. Tune: A research platform for distributed model selection and training. arXiv preprint, 2018. [44] S. Liu, G. Lever, N. Heess, J. Merel, S. Tunyasuvunakool, and T. Graepel. Emergent coordination through competition. In International Conference on Learning Representations, 2019. [45] J. R. Lloyd, D. Duvenaud, R. Grosse, J. B. Tenenbaum, and Z. Ghahramani. Automatic construction and natural-language description of nonparametric regression models. In Proceedings of the Twenty-Eighth AAAI Conference on Artiﬁcial, 2014. [46] I. Loshchilov and F. Hutter. Cma-es for hyperparameter optimization of deep neural networks. In In International Conference on Learning Representations Workshop track, 2016. [47] G. Melis, C. Dyer, and P. Blunsom. On the state of the art of evaluation in neural language models. In International Conference on Learning Representations, 2018. [48] V . Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. A. Riedmiller. Playing atari with deep reinforcement learning. CoRR, abs/1312.5602, 2013. [49] V . Nguyen and M. A. Osborne. Knowing the what but not the where in Bayesian optimization. In International Conference on Machine Learning (ICML), 2020. [50] V . Nguyen, S. Schulze, and M. A. Osborne. Bayesian optimization for iterative learning. In Advances in Neural Information Processing Systems (NeurIPS), 2020. 11[51] OpenAI, M. Andrychowicz, B. Baker, M. Chociej, R. Józefowicz, B. McGrew, J. W. Pachocki, J. Pachocki, A. Petron, M. Plappert, G. Powell, A. Ray, J. Schneider, S. Sidor, J. Tobin, P. Welinder, L. Weng, and W. Zaremba. Learning dexterous in-hand manipulation. CoRR, abs/1808.00177, 2018. [52] S. Paul, V . Kurin, and S. Whiteson. Fast efﬁcient hyperparameter tuning for policy gradients. Advances in Neural Information Processing Systems (NeurIPS), 2019. [53] M. Pelikan, D. E. Goldberg, and E. Cantú-Paz. Boa: The Bayesian optimization algorithm. In Proceedings of the 1st Annual Conference on Genetic and Evolutionary Computation - Volume 1, GECCO, 1999. [54] C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning). The MIT Press, 2005. [55] B. Ru, A. S. Alvi, V . Nguyen, M. A. Osborne, and S. J. Roberts. Bayesian optimisation over multiple continuous and categorical inputs. In International Conference on Machine Learning (ICML), 2020. [56] S. Schmitt, J. J. Hudson, A. Zídek, S. Osindero, C. Doersch, W. M. Czarnecki, J. Z. Leibo, H. Küttler, A. Zisserman, K. Simonyan, and S. M. A. Eslami. Kickstarting deep reinforcement learning. CoRR, abs/1803.03835, 2018. [57] J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel. High-dimensional continuous control using generalized advantage estimation. In Proceedings of the International Conference on Learning Representations (ICLR), 2016. [58] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms, 2017. [59] B. Shahriari, K. Swersky, Z. Wang, R. P. Adams, and N. De Freitas. Taking the human out of the loop: A review of bayesian optimization. Proceedings of the IEEE, 104(1):148–175, 2015. [60] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. van den Driessche, J. Schrittwieser, I. Antonoglou, V . Panneershelvam, M. Lanctot, S. Dieleman, D. Grewe, J. Nham, N. Kalchbrenner, I. Sutskever, T. P. Lillicrap, M. Leach, K. Kavukcuoglu, T. Graepel, and D. Hassabis. Mastering the game of Go with deep neural networks and tree search. Nature, 529:484–489, 2016. [61] N. Srinivas, A. Krause, S. Kakade, and M. Seeger. Gaussian process optimization in the bandit setting: No regret and experimental design. In Proceedings of the 27th International Conference on International Conference on Machine Learning, ICML’10, 2010. [62] R. S. Sutton. Adapting bias by gradient descent: An incremental version of delta-bar-delta. In AAAI, 1992. [63] K. Swersky, J. Snoek, and R. P. Adams. Freeze-thaw Bayesian optimization, 2014. [64] M. Wainwright. Basic tail and concentration bounds. URl: https://www. stat. berkeley. edu/.../Chap2_TailBounds_Jan22_2015. pdf (visited on 12/31/2017), 2015. [65] D. Whitley, V . S. Gordon, and K. Mathias. Lamarckian evolution, the baldwin effect and function optimization. In Y . Davidor, H.-P. Schwefel, and R. Männer, editors,Parallel Problem Solving from Nature — PPSN III , 1994. 12Appendix 7 Additional Experiments Supervised Learning While the primary motivation for our work is RL, we also evaluated PB2 the supervised learning case, to test the generality of the method. Concretely, we used PB2 to optimize six hyperparameters for a Convolutional Neural Network (CNN) on the CIFAR-10 dataset [37]. In each setting we randomly sample the initial hyperparameter conﬁgurations and train on half of the dataset for 50 epochs. We use B = 4agents for RS, PBT and PB2, with tready as 5 epochs. For ASHA we have the same maximum budget across all agents but begin with a population size of 16. Table 2: Median best performing agent across 5 seeds. The best performing methods are bolded. RS ASHA PBT PB2 Test Accuracy 84.43 88 .85 87 .20 89.10 In Table 7 we present the median best performing agent from ﬁve runs of each algorithm. We see that PB2 outperforms all other methods, including ASHA, which was speciﬁcally designed for SL problems [41]. This result indicates PB2 may be useful for a vast array of applications. 8 Experiment Details For all experiments we set βt = c1 + log(c2t) with c1 = 0.2 and c2 = 0.4, as in the trafﬁc speed data experiment from [9]. In Table 3, 5 & 7 and we show hyperparameters for the IMPALA, PPO and CIFAR experiments. In Table 8, 6 and 6 we show the bounds for the hyperparameters learned by PBT and PB2. All methods were initialized by randomly sampling from these bounds. Table 3: IMPALA: Fixed Parameter Value Num Workers 5 Num GPUs 0 Table 4: IMPALA: Learned Parameter Value Epsilon {0.01,0.5} Learning Rate {10−3,10−5} Entropy Coeff {0.001,0.1} Table 5: PPO: Fixed Parameter Value Filter MeanStdFilter SGD Iterations 10 Architecture 32-32 ready 5 ×104 Table 6: PPO: Learned Parameter Value Batch Size {1000,60000} GAEλ {0.9,0.99} PPO Clipϵ {0.1,0.5} Learning Rateη {10−3,10−5} The model used for the CIFAR dataset was from: https://zhenye-na.github.io/2018/09/ 28/pytorch-cnn-cifar10.html. All experiments were run using a 32 core machine. Table 7: CIFAR: Fixed Parameter Value Optimizer Adam Iterations 50 Architecture 3 Conv Layers ready 5 Table 8: CIFAR: Learned Parameter Value Train Batch Size {4,128} Dropout-1 {0.1,0.5} Dropout-2 {0.1,0.5} Learning Rate {10−3,10−4} Weight Decay {10−3,10−5} Momentum {0.8,0.99} 139 Theoretical Results We show the derivation for Lemma 1. Proof. We have a reward at the starting iteration F1(x1) as a constant that allows us to write the objective function as: FT(xT) −F1(x1) =FT(xT) −FT−1(xT−1) +··· + F3(x3) −F2(x2) +F2(x2) −F1(x1) (6) Therefore, maximizing the left of Eq. (6) is equivalent to minimizing the cummulative regret as follows: max [FT(xT) −F1(x1)] = max T∑ t=1 Ft(xt) −Ft−1(xt−1) = max T∑ t=1 ft(xt) = min T∑ t=1 rt(xt) where we deﬁne ft(xt) = Ft(xt) −Ft−1(xt−1), the regret rt = ft(x∗ t) −ft(xt) and ft(x∗ t) := max∀xft(x) is an unknown constant. 9.1 Convergence Analysis We minimize the cumulative regretRT by sequentially suggesting an xt to be used in each iteration t. We shall derive the upper bound in the cumulative regret and show that it asymptotically goes to zero as T increases, i.e., limT→∞ RT T = 0. We make the following smoothness assumption to derive the regret bound of the proposed algorithm. Assumptions. We will assume that the kernel kholds for some (a,b) and ∀Lt ≥0. The joint kernel satisﬁes for all dimensions j = 1,...,d, ∀Lt ≥0,t ≤T ,p(sup ⏐⏐⏐⏐ ∂ft(x) ∂x(j) ⏐⏐⏐⏐≥Lt) ≤ae−(Lt/b)2 . (7) These assumptions are achieved by using a time-varying kernel ktime(t,t′) = (1−ω) |t−t′| 2 [9] with the smooth functions [9]. For completeness, we restate Lemma 3, 4, 5, 6 from [61, 9], then present our new theoretical results in Lemma 7, 8, 9 and Theorem 10. Lemma 3 ([61]). Let Lt = b√log 3daπt δ ) where ∑T t=1 1 πt = 1, we have with probability 1 −δ 3 , |ft(x) −ft(x′)|≤ Lt||x −x′||1,∀t,x,x′∈D. (8) Lemma 4 ([61]). We deﬁne a discretization Dt ⊂ D ⊆ [0,r]d of size (τt)d satisfying ||x − [x]t||1 ≤ d τt ,∀x ∈D where [x]t denotes the closest point in Dt to x. By choosing τt = t2 Ltd = rdbt2√ log (3daπt/δ), we have |ft(x) −ft([x]t)|≤ 1 t2 . Lemma 5 ([61]). Let βt ≥2 log3πt δ + 2dlog ( rdbt2 √ log 3daπt 2δ ) where ∑T t=1 π−1 t = 1, then with probability at least 1 −δ 3 , we have |ft(xt) −µt(xt)|≤ √ βtσt(xt),∀t,∀x ∈D. Proof. We note that conditioned on the outputs (y1,...,y t−1), the sampled points (x1,..., xt) are deterministic, and ft(xt) ∼N ( µt(xt),σ2 t(xt) ) . Using Gaussian tail bounds [64], a random variable f ∼N(µ,σ2) is within √βσ of µwith probability at least 1 −exp ( −β 2 ) . Therefore, we ﬁrst claim that if βt ≥2 log3πt δ then the selected points {xt}T t=1 satisfy the conﬁdence bounds with probability at least 1 −δ 3 |ft(xt) −µt(xt)|≤ √ βtσt(xt),∀t. (9) 14This is true because the conﬁdence bound for individual xt will hold with probability at least 1 − δ 3πt and taking union bound over ∀twill lead to 1 −δ 3 . We show above that the bound is applicable for the selected points {xt}T t=1. To ensure that the bound is applicable for all points in the domain Dt and ∀t, we can set βt ≥2 log3|Dt|πt δ where∑T t=1 π−1 t = 1e.g., πt = π2t2 6 p(|ft(xt) −µt(xt)|≤ √ βtσt(xt) ≥1 −|Dt| T∑ t=1 exp (−βt/2) = 1−δ 3.. (10) By discretizing the domain Dt in Lem. 4, we have a connection to the cardinality of the domain that |Dt| = (τt)d = ( rdbt2√ log (3daπt/δ) )d . Therefore, we need to set βt such that both conditions in Eq. (9) and Eq. (10) are satisﬁed. We simply take a sum of them and get βt ≥ 2 log3πt δ + 2dlog ( rdbt2 √ log 3daπt 2δ ) . We use TB to denote the batch setting where we will run the algorithm over T iterations with a batch size B. The mutual information is deﬁned as ˜I(fTB; yTB) = 1 2 log det ( ITB + σ−2 f ˜KTB ) and the maximum information gain is as ˜γT := max˜I(fTB; yTB) where fTB := fTB(xTB) = (ft,b(xt,b),...,f T,B(xT,B)) ,∀b = 1....B,∀t = t...T for the time variant GP f. Using the result presented in [9], we can adapt the bound on the time-varying information gain into the parallel setting using a population size of Bbelow. Lemma 6. (adapted from [9] with a batch size B) Let ωbe the forgetting-remembering trade-off parameter and consider the kernel for time 1 −Ktime(t,t′) ≤ω|t−t′|, we bound the maximum information gain that ˜γTB ≤ ( T ˜N ×B + 1 )( γ˜N×B + σ−2 f [ ˜N ×B ]3 ω ) . Uncertainty Sampling (US). We next derive an upper bound over the maximum information gain obtained from a batch xt,b,∀b = 1,...,B . In other words, we want to show that the information gain by our chosen points xt,b will not go beyond the ones by maximizing the uncertainty. For this, we deﬁne an uncertainty sampling (US) scheme which ﬁlls in a batch xUS t,b by maximizing the GP predictive variance. Particularly, at iteration t, we select xUS t,b = arg maxx σt(x |Dt,b−1),∀b≤B and the data set is augmented over time to include the information of the new point, Dt,b = Dt,b−1 ∪xUS t,b. We note that we use xUS t,b to derive the upper bound, but this is not used by our PB2 algorithm. Lemma 7. Let xPB2 t,b be the point chosen by our algorithm andxUS t,b be the point chosen by uncertainty sampling (US) by maximizing the GP predictive variance xUS t,b = arg maxx∈Dσt(x |Dt,b−1),∀b= 1,...B and Dt,b = Dt,b−1 ∪xt,b. We have σt+1,1 ( xPB2 t+1,1 ) ≤σt+1,1 ( xUS t+1,1 ) ≤σt,b ( xUS t,b ) ,∀t∈{1,...,T },∀b∈{1,...B}. Proof. The ﬁrst inequality is straightforward that the point chosen by uncertainty sampling will have the highest uncertainty σt+1,1 ( xPB2 t+1,1 ) ≤σt+1,1 ( xUS t+1,1 ) = arg maxx σt(x |Dt,b−1). The second inequality is obtained by using the principle of “information never hurts” [36], we know that the GP uncertainty for all locations ∀x decreases with observing a new point. Therefore, the uncertainty at the future iteration σt+1 will be smaller than that of the current iteration σt, i.e., σt+1,b ( xUS t+1,b ) ≤σt,b ( xUS t,b ) ,∀b ≤B,∀t ≤T. We thus conclude the proof σt+1,1 ( xUS t+1,1 ) ≤ σt,b ( xUS t,b ) ,∀t∈{1,...,T },∀b∈{1,...B}. Lemma 8. The sum of variances of the points selected by the our PB2 algorithm σ() is bounded by the sum of variances by uncertainty sampling σUS(). Formally, w.h.p., ∑T t=2 σt,1 (xt,1) ≤ 1 B ∑T t=1 ∑B b=1 σt,b ( xUS t,b ) . 15Proof. By the deﬁnition of uncertainty sampling in Lem. 7, we have σt+1,1 (xt+1,1) ≤ σt,b ( xUS t,b ) ,∀t ∈{1,...,T },∀b ∈{2,...B}and σt,1 (xt,1) ≤σt,1 ( xUS t,1 ) where xt,1 is the point chosen by our PB2 and xUS t,1 is from uncertainty sampling. Summing all over B, we obtain σt,1 (xt,1) + (B−1) σt+1,1 (xt+1,1) ≤σt,1 ( xUS t,1 ) + B∑ b=2 σt,b ( xUS t,b ) T∑ t=1 σt,1 (xt,1) + (B−1) T∑ t=1 σt+1,1 (xt+1,1) ≤ T∑ t=1 B∑ b=1 σt,b ( xUS t,b ) by summing overT T∑ t=2 σt,1 (xt,1) ≤ 1 B T∑ t=1 B∑ b=1 σt,b ( xUS t,b ) . The last equation is obtained because of σ1,1 (x1,1) ≥0 and (B−1) σT+1,1 (xT+1,1) ≥0. Lemma 9. Let C1 = 32 log(1+σ−2 f ), σ2 f be the measurement noise variance and ˜γTB := max˜I be the maximum information gain of time-varying kernel, we have ∑T t=1 ∑B b=1 σ2 t,b(xUS t,b) ≤C1 16 ˜γTB where xUS t,b is the point selected by uncertainty sampling (US). Proof. We show that σ2 t,b(xUS t,b) = σ2 f ( σ−2 f σ2 t,b(xUS t,b) ) ≤σ2 fC2 log ( 1 +σ−2 f σ2 t,b ( xUS t,b )) ,∀b ≤ B,∀t ≤T where C2 = σ−2 f log(1+σ−2 f ) ≥1 and σ2 f is the measurement noise variance. We have the above inequality because s2 ≤ C2 log ( 1 +s2) for s ∈ [ 0,σ−2 f ] and σ−2 f σ2 t,b ( xUS t,b ) ≤ σ−2k ( xUS t,b,xUS t,b ) ≤ σ−2 f . We then use Lemma 5.3 of [ 14] to have the information gain over the points chosen by a time-varying kernel ˜I = 1 2 ∑T t=1 ∑B b=1 log ( 1 +σ−2 f σ2 t,b ( xUS t,b )) . Finally, we obtain T∑ t=1 B∑ b=1 σ2 t,b(xUS t,b) ≤σ2 fC2 T∑ t=1 B∑ b=1 log ( 1 +σ−2 f σ2 t,b ( xUS t,b )) = 2σ2 fC2˜I = C1 16 ˜γTB where C1 = 2 log(1+σ−2 f ) and ˜γTB := max˜I is the deﬁnition of maximum information gain given by T ×Bdata points from a GP for a speciﬁc time-varying kernel. Theorem 10. Let the domain D⊂ [0,r]d be compact and convex where dis the dimension and suppose that the kernel is such that f ∼GP(0,k) is almost surely continuously differentiable and satisﬁes Lipschitz assumptions for some a,b. Fix δ ∈ (0,1) and set βT = 2 logπ2T2 2δ + 2dlog rdbT2 √ log daπ2T2 2δ . Deﬁning C1 = 32/log(1+ σ2 f), the PB2 algorithm satisﬁes the following regret bound after T time steps: RTB = T∑ t=1 ft(x∗ t) − max b=1,...,B ft(xt,b) ≤ √ C1TβT ( T ˜NB + 1 )( γ˜NB + [ ˜NB ]3 ω ) + 2 with probability at least 1 −δ, the bound holds for any ˜N ∈{1,...,T }and B ≪T. Proof. Let x∗ t = arg max∀x ft(x) and xt,b be the point chosen by our algorithm at iteration tand batch element b, we deﬁne the (time-varying) instantaneous regret as rt,b = ft(x∗ t) −ft(xt,b) and the (time-varying) batch instantaneous regret over Bpoints is as follows rB t = min b≤B rt,b = min b≤B ft(x∗ t) −ft(xt,b),∀b≤B ≤ft(x∗ t) −ft(xt,1) ≤µt(x∗ t) +√κtσt(x∗ t) + 1 t2 −ft(xt,1) by Lem.4 ≤µt(xt,1) +√κtσt(xt,1) + 1 t2 −ft(xt,1) ≤2√κtσt(xt,1) + 1 t2 (11) 16where we have used the property thatµt(xt,1)+√βtσt(xt,1) ≥µt(x∗ t)+√βtσt(x∗ t) by the deﬁnition of selecting xt,1 = arg maxx µt(x) +√βtσt(x). Next, we bound the cumulative batch regret as RTB = T∑ t=1 rB t ≤ T∑ t=1 ( 2√κtσt(xt,1) + 1 t2 ) by Eq. (11) ≤2√κTσ1(x1,1) +2√κT B T∑ t=1 B∑ b=1 σt,b ( xUS t,b ) + T∑ t=1 1 t2 by Lem.7 and κT ≥κt,∀t≤T ≤4√κT B T∑ t=1 B∑ b=1 σt,b ( xUS t,b ) + T∑ t=1 1 t2 (12) ≤ 4 B √κT ×TB T∑ t=1 B∑ b=1 σ2 t,b(xUS t,b) + 2≤ √ C1 T BκT˜γTB + 2 (13) ≤ √C1 T BκT ( T ˜NB + 1 )( γ˜NB + 1 σ2 f [ ˜NB ]3 ω ) + 2 (14) where C1 = 32/log(1 +σ2 f), xUS t,b is the point chosen by uncertainty sampling – used to provide the upper bound in the uncertainty. In Eq. (12), we take the upper bound by considering two possible cases: either σ1(x1,1) ≥ 1 B ∑T t=1 ∑B b=1 σt,b ( xUS t,b ) or 1 B ∑T t=1 ∑B b=1 σt,b ( xUS t,b ) ≥σ1(x1,1). It results in 2 B ∑T t=1 ∑B b=1 σt,b ( xUS t,b ) ≥ 1 B ∑T t=1 ∑B b=1 σt,b ( xUS t,b ) + σ1(x1,1). In Eq. (13) we have used ∑∞ t=1 1 t2 ≤π2/6 ≤2 and ||z||1 ≤ √ T||z||2 for any vector z ∈RT. In Eq. (14), we utilize Lem. 6. Finally, given the squared exponential (SE) kernel deﬁned, γSE ˜NB = O( [ log ˜NB ]d+1 ), the bound is RTB ≤ √ C1 T BβT ( T ˜NB + 1 )( (d+ 1) log ( ˜NB ) + 1 σ2 f [ ˜NB ]3 ω ) + 2where ˜N ≤T and B ≪T. In our time-varying setting, if the time-varying function is highly correlated, i.e., the information between f1(.) and fT(.) does not change signiﬁcantly, we have ω →0 and ˜N →T. Then, the regret bound grows sublinearly with the number of iterations T, i.e., limT→∞ RTB T = 0. This bound suggests that the gap between ft(xt) and the optimal ft(x∗ t) vanishes asymptotically using PB2. In addition, our regret bound is tighter and better with increasing batch size B. On the other hand in the worst case, if the time-varying function is not correlated, such as ˜N →1 and ω→1, then PB2 achieves the linear regret [9]. 17",
      "meta_data": {
        "arxiv_id": "2002.02518v4",
        "authors": [
          "Jack Parker-Holder",
          "Vu Nguyen",
          "Stephen Roberts"
        ],
        "published_date": "2020-02-06T21:27:04Z",
        "pdf_url": "https://arxiv.org/pdf/2002.02518v4.pdf"
      }
    }
  ]
}