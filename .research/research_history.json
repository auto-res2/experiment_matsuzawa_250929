{
  "research_topic": "Improving efficiency of hyperparameter optimization",
  "queries": [
    "surrogate-assisted hyperparameter optimization",
    "multi-fidelity hyperparameter tuning",
    "early stopping hyperparameter search",
    "parallel hyperparameter optimization",
    "gradient-based hyperparameter optimization"
  ],
  "research_study_list": [
    {
      "title": "Deep Ranking Ensembles for Hyperparameter Optimization"
    },
    {
      "title": "Few-Shot Bayesian Optimization with Deep Kernel Surrogates"
    },
    {
      "title": "Efficient Hyperparameter Optimization with Adaptive Fidelity Identification"
    },
    {
      "title": "Meta-learning Hyperparameter Performance Prediction with Neural Processes"
    },
    {
      "title": "Bayesian Optimization for Iterative Learning"
    },
    {
      "title": "Multi-Fidelity Bayesian Optimization via Deep Neural Networks"
    },
    {
      "title": "Efficient Hyperparameter Optimization with Adaptive Fidelity Identification"
    },
    {
      "title": "Batch Multi-Fidelity Bayesian Optimization with  Deep Auto-Regressive Networks"
    },
    {
      "title": "Supervising the Multi-Fidelity Race of Hyperparameter Configurations"
    },
    {
      "title": "Supervising the Multi-Fidelity Race of Hyperparameter Configurations"
    },
    {
      "title": "Bayesian Optimization for Iterative Learning"
    },
    {
      "title": "Scaling Laws for Hyperparameter Optimization"
    },
    {
      "title": "Hyperparameter Optimization through Neural Network Partitioning"
    },
    {
      "title": "PASHA: Efficient HPO and NAS with Progressive Resource Allocation"
    },
    {
      "title": "Reshuffling Resampling Splits Can Improve Generalization of Hyperparameter Optimization"
    },
    {
      "title": "Differentiable Expected Hypervolume Improvement for Parallel Multi-Objective Bayesian Optimization"
    },
    {
      "title": "Hyperparameter Optimization through Neural Network Partitioning"
    },
    {
      "title": "Federated Hyperparameter Tuning: Challenges, Baselines, and Connections to Weight-Sharing"
    },
    {
      "title": "Reshuffling Resampling Splits Can Improve Generalization of Hyperparameter Optimization"
    },
    {
      "title": "DP-HyPO: An Adaptive Private Framework for Hyperparameter Optimization"
    },
    {
      "title": "Learning to Mutate with Hypergradient Guided Population"
    },
    {
      "title": "Gradient Descent: The Ultimate Optimizer"
    },
    {
      "title": "Implicit differentiation of Lasso-type models for hyperparameter optimization"
    },
    {
      "title": "Stochastic Marginal Likelihood Gradients using Neural Tangent Kernels"
    },
    {
      "title": "AUTOMATA: Gradient Based Data Subset Selection for Compute-Efficient Hyper-parameter Tuning"
    }
  ]
}