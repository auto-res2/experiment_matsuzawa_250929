{
  "research_topic": "Improving efficiency of hyperparameter optimization",
  "queries": [
    "surrogate-assisted hyperparameter optimization",
    "multi-fidelity hyperparameter tuning",
    "early stopping hyperparameter search",
    "parallel hyperparameter optimization",
    "gradient-based hyperparameter optimization"
  ],
  "research_study_list": [
    {
      "title": "Deep Ranking Ensembles for Hyperparameter Optimization",
      "abstract": "Automatically optimizing the hyperparameters of Machine Learning algorithms\nis one of the primary open questions in AI. Existing work in Hyperparameter\nOptimization (HPO) trains surrogate models for approximating the response\nsurface of hyperparameters as a regression task. In contrast, we hypothesize\nthat the optimal strategy for training surrogates is to preserve the ranks of\nthe performances of hyperparameter configurations as a Learning to Rank\nproblem. As a result, we present a novel method that meta-learns neural network\nsurrogates optimized for ranking the configurations' performances while\nmodeling their uncertainty via ensembling. In a large-scale experimental\nprotocol comprising 12 baselines, 16 HPO search spaces and 86 datasets/tasks,\nwe demonstrate that our method achieves new state-of-the-art results in HPO.",
      "full_text": "Published as a conference paper at ICLR 2023 DEEP RANKING ENSEMBLES FOR HYPERPARAMETER OPTIMIZATION Abdus Salam Khazi∗, Sebastian Pineda Arango∗, Josif Grabocka University of Freiburg Correspondence to Sebastian Pineda Arango: pineda@cs.uni-freiburg.edu ABSTRACT Automatically optimizing the hyperparameters of Machine Learning algorithms is one of the primary open questions in AI. Existing work in Hyperparameter Op- timization (HPO) trains surrogate models for approximating the response surface of hyperparameters as a regression task. In contrast, we hypothesize that the op- timal strategy for training surrogates is to preserve the ranks of the performances of hyperparameter conﬁgurations as a Learning to Rank problem. As a result, we present a novel method that meta-learns neural network surrogates optimized for ranking the conﬁgurations’ performances while modeling their uncertainty via en- sembling. In a large-scale experimental protocol comprising 12 baselines, 16 HPO search spaces and 86 datasets/tasks, we demonstrate that our method achieves new state-of-the-art results in HPO. 1 I NTRODUCTION Hyperparameter Optimization (HPO) is a crucial ingredient in training state-of-the-art Machine Learning (ML) algorithms. The three popular families of HPO techniques are Bayesian Optimiza- tion (Hutter et al., 2019), Evolutionary Algorithms (Awad et al., 2021a), and Reinforcement Learn- ing (Wu & Frazier, 2019; Jomaa et al., 2019). Among these paradigms, Bayesian Optimization (BO) stands out as the most popular approach to guide the HPO search. At its core, BO ﬁts a paramet- ric function (called a surrogate) to estimate the evaluated performances (e.g. validation error rates) of a set of hyperparameter conﬁgurations. The task of ﬁtting the surrogate to the observed data points is treated as a probabilistic regression, where the common choice for the surrogate is Gaus- sian Processes (GP) (Snoek et al., 2012). Consequently, BO uses the probabilistic predictions of the conﬁgurations’ performances for exploring the search space of hyperparameters. For an introduction to BO, we refer the interested reader to Hutter et al. (2019). In this paper, we highlight that the current BO approach of training surrogates through a regression task is sub-optimal. We furthermore hypothesize that ﬁtting a surrogate to evaluated conﬁgurations is instead a learning-to-rank (L2R) problem (Burges et al., 2005). The evaluation criterion for HPO is the performance of the top-ranked conﬁguration. In contrast, the regression loss measures the surrogate’s ability to estimate all observed performances and does not pay any special consideration to the top-performing conﬁguration(s). We propose that BO surrogates must be learned to estimate the ranks of the conﬁgurations with a special emphasis on correctly predicting the ranks of the top-performing conﬁgurations. Unfortunately, the current BO machinery cannot be naively extended for L2R, because Gaussian Processes (GP) are not directly applicable to ranking. In this paper, we propose a novel paradigm to train probabilistic surrogates for learning to rank in HPO with neural network ensembles 1. Our networks are learned to minimize L2R listwise losses (Cao et al., 2007), and the ensemble’s uncer- tainty estimation is modeled by training diverse networks via the Deep Ensemble paradigm (Laksh- minarayanan et al., 2017). While there have been a few HPO-related works using ﬂavors of basic ranking losses (Bardenet et al., 2013; Wistuba & Pedapati, 2020;¨Ozt¨urk et al., 2022), ours is the ﬁrst ∗Equal contribution 1Our code is available in the following repository: https://github.com/releaunifreiburg/ DeepRankingEnsembles 1 arXiv:2303.15212v2  [cs.LG]  21 May 2023Published as a conference paper at ICLR 2023 systematic treatment of HPO through a methodologically-principled L2R formulation. To achieve state-of-the-art HPO results, we follow the established practice of transfer-learning the ranking sur- rogates from evaluations on previous datasets (Wistuba & Grabocka, 2021). Furthermore, we boost the transfer quality by using dataset meta-features as an extra source of information (Jomaa et al., 2021a). We conducted large-scale experiments using HPO-B (Pineda Arango et al., 2021), the largest pub- lic HPO benchmark and compared them against 12 state-of-the-art HPO baselines. We ultimately demonstrate that our method Deep Ranking Ensembles (DRE) sets the new state-of-the-art in HPO by a statistically-signiﬁcant margin. This paper introduces three main technical contributions: • We introduce a novel neural network BO surrogate (named Deep Ranking Ensembles) optimized with Learning-to-Rank (L2R) losses; • We propose a new technique for meta-learning our ensemble surrogate from large-scale public meta-datasets; • Deep Ranking Ensembles achieve the new state-of-the-art in HPO, demonstrated through a very large-scale experimental protocol. 2 R ELATED WORK Hyperparameter Optimization (HPO)is a problem that has been well elaborated on during the last decade. The mainstream HPO strategies are Reinforcement Learning (RL) (Wu & Frazier, 2019), evolutionary search (Awad et al., 2021b), and Bayesian optimization (BO) (Hutter et al., 2019). The latter comprises two main components: a surrogate function that approximates the response function given some observations, and an acquisition function that leverages the probabilistic output of the surrogate to explore the search space, ultimately deciding which point to observe next. Previous work covers various choices for the surrogate model family, including Gaussian Processes (Snoek et al., 2012), and Bayesian Neural Networks (Springenberg et al., 2016a). Other authors report the advantages of using ensembles as a surrogate, such as Random Forests Hutter et al. (2011), or ensembles of neural networks White et al. (2021). In contrast, we train BO surrogates using a learning-to-rank problem deﬁnition (Cao et al., 2007). Transfer HPO refers to the problem deﬁnition of speeding up HPO by transferring knowledge from evaluations of hyperparameter conﬁgurations on other auxiliary datasets (Wistuba & Grabocka, 2021; Feurer et al., 2015; 2018). For example, the hyper-parameters of a Gaussian Process can be meta-learned on previous datasets and then transferred to new tasks (Wang et al., 2021). Similarly, a deep GP’s kernel parameters can also be meta-learned across auxiliary tasks (Wistuba & Grabocka, 2021). Another method trains ensembles of GPs weighted proportionally to the similarity between the new task and the auxiliary ones (Wistuba et al., 2016). When performing transfer HPO, it is use- ful to embed additional information about the dataset. Some approaches use dataset meta-features to warm-initialize the HPO (Feurer et al., 2015; Wistuba et al., 2015), or to condition the surrogate during pre-training (Bardenet et al., 2013). Recent works propose an attention mechanism to train dataset-aware surrogates (Wei et al., 2019), or utilize deep sets to extract meta-features (Jomaa et al., 2021b). In complement to the prior work, we meta-learn ranking surrogates with meta-features. Learning to Rank (L2R)is a problem deﬁnition that demands estimating the rank (a.k.a. relevance, or importance) of an instance in a set (Burges et al., 2005). The primary application domain for L2R is information retrieval (ranking websites in a search engine) (Ai et al., 2018), or e-commerce systems (ranking recommended products or advertisements) (Tang & Wang, 2018; Wu et al., 2018). However, L2R is applicable in diverse applications, from learning distance functions among images in computer vision (Cakir et al., 2019), up to ranking ﬁnancial events (Feng et al., 2021). In this paper, we emphasize the link between HPO and L2R and train neural surrogates for BO with L2R. Learning to Rank for HPO is a strategy for conducting HPO with an L2R optimization approach. There exist some literature on transfer-learning HPO methods that employ ranking objective within their transfer mechanisms. SCoT uses a surrogate-based ranking mechanism for transferring hyper- parameter conﬁgurations across datasets (Bardenet et al., 2013). On the other hand, Feurer et al. (2018) use a weighted ensemble of Gaussian Processes with one GP per auxiliary dataset, while the ensemble weights are learned with a pairwise ranking-based loss. Modeling the ranks of the learning 2Published as a conference paper at ICLR 2023 ... ... ... ... ...  Figure 1: The neural architecture of our Deep Ranking Ensembles (DRE) with inputs x (query points) and z (meta-features). curves also helps estimate the performance of conﬁgurations in a multi-ﬁdelity transfer setup (Wis- tuba & Pedapati, 2020). Recent work has demonstrated that pair-wise ranking losses can be used for transfer-learning surrogates in a zero-shot HPO protocol ( ¨Ozt¨urk et al., 2022). However, none of these approaches extensively study the core HPO problem with L2R, nor do they analyze which ranking loss types enable us to learn accurate BO surrogates. 3 D EEP RANKING ENSEMBLES (DRE) 3.1 P RELIMINARIES Hyperparameter Optimization is deﬁned as the problem of tuning the hyperparameters x ∈X of a ML algorithm to minimize the validation error achieved on a datasetD as arg minx∈XLVal (x,D ). The mainstream approach for tuning hyperparameters is Bayesian Optimization (BO), an introduc- tion of which is offered by Hutter et al. (2019). BO relies on ﬁtting a surrogate function for approx- imating the validation error on evaluated hyperparameter conﬁgurations. Consider having evaluated N conﬁgurations on a dataset and their respective validation errors as H = {(x i,y i)}N i=1, where y i = LVal (x,D ). We train a surrogate function ˆy (x i) =f (x i; θ ), typically a Gaussian Process, to estimate the observed y as arg maxθ E(xi,yi)∼pH log p (y i|x i,H/ {(x i,y i)}; θ ). Learning to Rank (L2R) differs from a standard supervised regression because instead of directly estimating the target variable it learns to estimate the rank of the target values. In the context of HPO, we deﬁne the rank of a conﬁguration as r (x i, {y 1,...,y N}) := ∑N j=1 1 yj≤yi. The core of a typical L2R method (Burges et al., 2005) includes training a parametric ranker ˆr (x i) :=f (x i; θ ) that correctly estimates the ranks of observed conﬁgurations’ validation errors. Instead of naively estimating the ranks as a direct regression task, i.e. arg maxθ E(xi,ri)∼pH log p (r i|x i; θ ), L2R techniques prioritize estimating the ranks of top-performing conﬁgurations more than bottom- performing ones (Cao et al., 2007). In general ranking losses can be deﬁned on the basis of sin- gle objects (point-wise approach), pairs of objects (pair-wise approach) or the whole list of objects (list-wise approach) (Chen et al., 2009). 3.2 D EEP RANKING ENSEMBLE (DRE) S URROGATE In this paper, we introduce a novel ranking model based on an ensemble of diverse neural net- works optimized for L2R. We aim to learn neural networks that output the ranking score of a hyperparameter conﬁguration s : X →R. The ranks of the estimated scores should match the true ranks ∑N j=1 1 yj≤yi ≈∑N j=1 1 s(xj;θ)≥s(xi;θ), however, with a higher priority in approximat- ing the ranks of the top-performing conﬁgurations using a weighted list-wise L2R loss (Cao et al., 2007). First of all, we deﬁne the indices of the ranked/ordered conﬁgurations as π : {1,...,N }→ {1,...,N }. Concretely, the ℓ -th observed conﬁguration is the k -th ranked conﬁguration π (ℓ ) =k if k = ∑N j=1 1 yj≤yℓ. Ultimately, we train the scoring network using the following loss: arg min θ N∑ i=1 L(x i,y i,y,θ ) , where L(x i,y i,y,θ ) =−w (π (i )) ·log exps(xπ(i);θ) ∑N j=iexps(xπ(j);θ) (1) 3Published as a conference paper at ICLR 2023 Algorithm 1: Meta-learning the Deep Ranking Ensembles Input : Set of datasets D, Number of iterations J, Number of ensemble scorers M Output: DRE parameters θ1,...,θ M, Meta-feature network parameters φ 1 Initialize scorer networks with parameters θ1,...,θ M ; 2 Initialize the parameters φof the meta-feature network zfrom Jomaa et al. (2021a) ; 3 for j = 1to J do 4 Sample dataset index i∈{1,...,D }, sample scorer network index m∈{1,...,M }; 5 Sample a query set H(s) := {( x(s) 1 ,y(s) 1 ) ,..., ( x(s) N(s) ,y(s) N(s) )} from Di ; 6 Sample a support set H(z) from Di \\H(s) ; 7 Compute meta-features z(H(z); φ) ; 8 Compute rank scores for the query set si = s ( x(s) i ,z ( H(z); φ ) ; θm ) ,i = 1,...,N (s) ; 9 Compute true ranks π(1) ,...,π ( N(s)) ; 10 Compute loss L(π,s; θm,φ) using Equation 1 ; 11 Update the meta-feature network φ←φ−ηφ ∂L(π,s; θm,φ) ∂φ ; 12 Update the ranker network θm ←θm −ηθm ∂L(π,s; θm,φ) ∂θm ; 13 end 14 return θ1,...,θ M,φ ; The weighting functions w : {1,...,N }→ R+ is deﬁned as w(π(i)) = 1 log(π(i)+1) and is used to assign a higher penalty to the top-performing hyper-parameter conﬁgurations, whose correct rank is more important in HPO (Chen et al., 2017). After having trained the scoring model of Equation 1 we estimate the rank of an unobserved conﬁguration as ˆr(x; θ) = ∑N j=1 1 s(xj;θ)≥s(x;θ). Further- more, Bayesian Optimization (BO) needs uncertainty estimates to be able to explore the search space (Hutter et al., 2019). As a result, we model uncertainty by training M diverse neural scorers s1(x,θ1),...,s M(x,θM) with stochastic gradient descent. The diversity of the ensemble scorers is ensured through the established mechanism of applying different per-scorer seeds for sampling mini-batches of hyperparameter conﬁgurations (Lakshminarayanan et al., 2017). Finally, the pos- terior mean and variance of the estimated ranks is computed trivially as µ(x) = 1 N ∑N i=1 ˆr(x; θi) and σ2(x) = 1 N ∑N i=1(ˆr(x; θi) −µ(x))2. The BO pseudo-code and the details for using our Deep Rankers in HPO are explained in Appendix A. 3.3 M ETA-LEARNING THE DEEP RANKING ENSEMBLES HPO is a very challenging problem due to the limited number of evaluated hyperparameter con- ﬁgurations. As a result, the current best practice in HPO relies on transfer-learning the knowledge of hyperparameters2 from evaluations on previous datasets (Wistuba & Grabocka, 2021; Wistuba et al., 2016; Salinas et al., 2020). In this paper, we meta-learn our ranker from Kdatasets assuming we have a set of observations H(k) := {( x(k) 1 ,y(k) 1 ) ,..., ( x(k) Nk,y(k) Nk )} ; k = 1,...,K with Nk evaluated hyperparameter conﬁgurations on the k-th dataset. We meta-learn our ensemble of M Deep Rankers with the meta-learning objective in Equation 2, where we learn to estimate the ranks of all observations on all evaluations for all the previous datasets using the loss of Equation 1. arg min θ1,...,θM K∑ k=1 Nk∑ n=1 M∑ m=1 L ( x(k) n ,y(k) n ,y(k); θm ) (2) 2Even in manually-designed ML systems, experts start their initial guess about hyper-parameters by transfer-learning the conﬁgurations that worked well on past projects (a.k.a. datasets). 4Published as a conference paper at ICLR 2023 −10 0 10 −1 0 1 BO Step 1 −10 0 10 BO Step 2 −10 0 10 BO Step 3 −10 0 10 BO Step 4 −10 0 10 BO Step 5 True Function Expected Improvement Observed Samples Next Sample Figure 2: BO Steps Example with a Random Initialized DRE. EI is scaled and shifted for clarity. −10 0 10 −4 −2 0 2 Scorers’ Output −10 0 10 0 50 100 150 200 Rankers’ Output 1 2 3 4 5 6 7 8 9 10 −4 −2 0 2 Distributions of Scorers’ Output 1 2 3 4 5 6 7 8 9 10 0 50 100 150 200 Distributions of Rankers’ Output −10 0 10 0 50 100 150 200 Uncertainty Estimations Figure 3: Understanding the outputs of DRE’s modules. Transfer-learning for HPO suffers from the negative-transfer phenomenon, where the distribution of the validation errors given hyperparameters changes across datasets. In such cases, using dataset meta-features helps condition the transfer only from evaluations on similar datasets (Rakotoarison et al., 2022; Jomaa et al., 2021a). We use the meta-features of Jomaa et al. (2021a) which are based on a deep set formulation (Zaheer et al., 2017) of the pairwise interactions between hyperparameters and their validation errors. The meta-feature network with parameterφtakes a history of evaluations H = {(xi,yi)}N i=1 as its input and outputs aL-dimensional representation of the history asz(H,φ) : (X× R)N →RL. Afterward, the scorer function becomes s(x,z(H; φ); θ) : X× RL →R. In other words, the dataset meta-features are additional features to the scorers. A graphical depiction of our architecture is shown in Figure 1. We update all the scorer networks of the ensemble independently using the loss of Equation 1. The pseudo-code of Algorithm 1 draws an evaluation set (called a query set) which is used as the training batch for updating the parameters of the sampled scorer network. We also meta-learn the meta- feature network (Jomaa et al., 2021a), however, by using a different batch of evaluations (called a support set). We do not meta-learn both the scorer and the meta-feature networks using the same batch of evaluations in order to promote generalization. 4 E XPERIMENTS AND RESULTS 4.1 M OTIVATING EXAMPLE We demonstrate our DRE with 10 base models on a simple sinusoid function y = sin(x+π 2 ) for x ∈[−10,10] sampled with a step size of 0.1 in equally spaced intervals. Further details on the architecture are explained in Section B. In Figure 2, we conduct BO with a variant of DRE without meta-learning, and we start the HPO with 3 initial random observations. We observe that a BO procedure with the Expected Improvement acquisition reaches an optimum after 8 observations. Furthermore, we plot the scorers’ and rankers’ outputs of the second BO step in Figure 3. The analysis illustrates that the distributions of the scorers’ outputs have different ranges because the loss function in Equation 1 models only the target rank, but not the scale of the target. However, the outputs of the rankers display similar distributions in the rank space, which is more adequate for 5Published as a conference paper at ICLR 2023 −10 −5 0 5 10 −1.0 −0.5 0.0 0.5 1.0 −10 −5 0 5 10 −1.5 −1.0 −0.5 0.0 0.5 1.0 Meta-Train Task 1 Meta-Train Task 2 Meta-Train Task 3 Meta-Train Task 4 Meta-Train Task 5 Meta-Test Task Expected Improvement Observed Samples Next Sample Figure 4: Meta-train and Meta-test Tasks (left) for optimizing the function. The meta-learned DRE ﬁnds the optimum in one step (right). computing the ranks’ uncertainties. Moreover, the rank distributions differ in certain regions of the search space, enabling BO to conduct exploration. To showcase the power of transfer-learning, we meta-learn DRE on 5 auxiliary tasks, corresponding to different sinusoidal functions y = sin(x+π 2 + β) with varying β ∈{11,.., 15}, as illustrated in Figure 4 (left). Subsequently, we deploy a meta-learned DRE surrogate on a test task (blue line with β = 8) which was not part of the meta-training set. Figure 4 reveals that DRE directly discovers a global optimum within one BO step (4 total observations). The success is attributed to the fact that the surrogate has been meta-learned to recognize sinusoidal shapes given the 3 initial observations in green, as is clearly shown by the acquisition in Figure 4 (right). 4.2 D ATASETS AND BASELINES We base our experiments on HPO-B (Pineda Arango et al., 2021), the largest public benchmark for HPO. It contains 16 search spaces, each of which comprises a meta-train, meta-test, and meta- validation split. Every split is a set of datasets, and for every dataset, the benchmark contains the validation errors of evaluated hyperparameter conﬁgurations. The benchmark also includes the re- sults of several HPO methods run in those datasets, including transfer and non-transfer algorithms3. Moreover, we generated new results for three additional state-of-the-art baselines (GCP, HEBO, and DKLM) that are not released by HPO-B. The benchmark provides 5 sets of 5 initial random seeds for every task in the meta-test split (86 in total). We use the meta-test datasets to compare the per- formance of the Deep Ranker Ensembles against the baselines. Speciﬁcally, our non-transfer HPO baselines are listed below: • Random Search (RS) (Bergstra & Bengio, 2012) is a simple yet strong baseline that selects a random conﬁguration at every step. • Gaussian Processes (GP) (Snoek et al., 2012) model the response function by computing the posterior distribution of functions induced by the observed data. • DNGO (Snoek et al., 2015) uses a neural network that models the uncertainty with a Bayesian linear regression on the last network layer. • BOHAMIANN (Springenberg et al., 2016b) is also a Bayesian neural network that per- forms Bayesian inference via Hamiltonian Monte Carlo. • Deep-Kernel Gaussian Processes (DKGP) (Wilson et al., 2016) learn a latent representa- tion of the features that are fed to a GP kernel function. • HEBO (Cowen-Rivers et al., 2020) is a state-of-the-art Bayesian optimization method. It combines input and output transformations and a multi-objective acquisition function. We use the implementation contained in the original repository.4 Transfer HPO methods use the evaluations of the tasks included in the meta-train split to meta-learn surrogates, that are subsequently applied for HPO on the meta-test tasks within the same search space. We consider the following baselines: 3Available in https://github.com/releaunifreiburg/HPO-B 4Available in https://github.com/huawei-noah/HEBO 6Published as a conference paper at ICLR 2023 • TST (Wistuba et al., 2016) constructs an ensemble of Gaussian Processes aggregated with a kernel-weighted average. Alternatively,TAFbuilds an ensemble of acquisition functions. • RGPE (Feurer et al., 2018) trains a Gaussian Process per each meta-train task and then combines for a new task through a weighting scheme, which accounts for the ranking per- formance of every base GP model. • FSBO (Wistuba & Grabocka, 2021) pre-trains a Deep Kernel Gaussian Process using meta- train tasks and then ﬁne-tunes the parameters when observations for new tasks are available. • GCP (Salinas et al., 2020) pre-trains a neural network to predict the residual performance on the auxiliary tasks and applies Gaussian Copulas to combine results for a new task. • DKLM (Jomaa et al., 2021b) adds a Deep Set as task contextualization on top of FSBO. We use the same hyperparameters as suggested in the original paper. 4.3 DRE-E XPERIMENTAL SETUP The meta-feature extractor zis based on the Deep Set architecture proposed by Jomaa et al. (2021a) with ﬁve hidden layers and 32 neurons per layer. The ensemble of scorers is composed of 10 MLPs with identical architectures: four layers and 32 neurons that we selected using the meta-validation split from HPO-B. We meta-learn DRE for 5000 epochs with Adam optimizer, learning rate 0.001 and batch size 100. Every element of the batch is a list of 100 elements. We select 20% of the samples in each list as input to the meta-feature extractor. During meta-test in every BO iteration, we update the pre-trained weights for 1000 epochs. For DRE-RI, we initialize randomly the scorers and train them for 1000 epochs using Adam Optimizer with a learning rate of 0.02. Every epoch, we use 20% of the observations to feed the meta-feature extractor. 4.4 R ESEARCH HYPOTHESIS AND EXPERIMENTAL RESULTS (a) Transfer Methods 5 20 35 50 65 80 95 Number of Evaluations 4 5 6 7Average Rank DRE FSBO RGPE TST TAF HEBO DKLM GCP Random (b) Non-Transfer Methods 5 20 35 50 65 80 95 Number of Evaluations 3.5 4.0 4.5 5.0 5.5 Average Rank DRE-RI HEBO GP DKGP BOHA. DNGO Random Figure 5: Results for Transfer and Non-transfer methods. Hypothesis 1. Deep Ranking Ensembles (DRE) achieve state-of-the-art results in transfer HPO. We compare against the transfer HPO baselines listed in Section 4.2 and report the average ranks across all the tasks in the meta-test split of all the HPO-B search spaces. Our protocol uses 5 initial conﬁgurations plus 100 BO iterations across 16 search spaces (the default HPO-B protocol). Our method uses meta-features (Jomaa et al., 2021a) and the scorer parameters are ﬁne-tuned after each BO observation. Figure 5 (left) shows that DRE clearly outperforms all baselines over 100 BO iterations based on the rank among the HPO methods averaged among 86 datasets and 5 runs. We compute the critical dif- ference diagram (Demˇsar, 2006) for 25, 50, and 100 iterations, and show the statistical signiﬁcance 7Published as a conference paper at ICLR 2023 (a) Ranking Loss 5 20 35 50 65 80 95 Number of Evaluations 2.5 3.0 3.5 4.0 Average Rank List W-List Pair Point Reg (b) Meta-features 5 20 35 50 65 80 95 Number of Evaluations 2.50 2.75 3.00 3.25 3.50 Average Rank F,M,T F,M F,T M,T T (c) Acquisition Function 5 20 35 50 65 80 95 Number of Evaluations 2.25 2.50 2.75 3.00 3.25 Average Rank Avg EI UCB Random Figure 6: Results after testing our hypothesis 3-5. of the results in Figure 9a (Appendix E). HEBO is not a transfer HPO method but is presented as a reference. These results demonstrate the advantage of training neural ensembles with L2R since our method outperforms other rivals which also meta-train neural networks (FSBO, DKLM), or en- sembles of neural networks (TST, TAF, RGPE). DRE also attains competitive results in individual search space, as shown in Figure 13, at Appendix E. Hypothesis 2. The randomly-initialized DRE performs competitively in non-transfer HPO. We test the hypothesis by comparing the performance of DRE against the non-transfer baselines mentioned in Section 4.2. Similar to Experiment 1, we compute the average rank over 100 BO iterations, aggregating across all the meta-test tasks of all the search spaces in HPO-B. The results of Figure 5 (right) show that a random initialized DRE (i.e. non meta-learned) is still a competitive surrogate for HPO. It exhibits good performance for up to 30 iterations compared to the other baselines and is second only to HEBO (notice our meta-learned DRE actually outperforms HEBO, Figure 5 (left)). This demonstrates the usefulness of deep ensembles with L2R as general- purpose HPO surrogates. Interestingly, DRE outperforms other surrogates using neural networks, such as BOHAMIANN, DNGO, and DKGP. We present the statistical signiﬁcance of the results after 25, 50, and 100 BO iterations in Figure 9b. Hypothesis 3. A weighted list-wise ranking loss is the best L2R strategy for DRE. We test DRE (meta-learned) with three different L2R losses: point-wise, pair-wise, and list-wise (weighted and non-weighted) ranking losses. Additionally, we compare to a surrogate predicting the performance in the original scale using Mean Squared Error as loss, i.e. a regression. Moreover, we compare the performance to a DRE trained with a regression loss. We omit the meta-features from all variants to avoid confounding factors from the analysis and use Expected Improvement as the acquisition function. The results in Figures 6a and 11a (Appendix E) show the advantage of the list-wise ranking losses over the other type of ranking losses. Moreover, the results highlight the advantage of list-weighted ranking losses, as it attained the best performance over the average rank among 100 BO iterations. Additionally, we observe that pairwise-losses also give a boost in performance compared to point- wise estimations. The message is: ”Any L2R loss is better than the regression one”. Hypothesis 4. Meta-features help the transfer HPO performance of DRE. We evaluate DRE with and without the meta-features extracted by the DeepSet module (Jomaa et al., 2021a), ablating the scenarios with and without meta-learning. Again we use all 16 search spaces from HPO-B for 100 BO iterations, starting with 5 random initial conﬁgurations. DRE uses the weighted list-wise loss, and Expected Improvement as the acquisition. 8Published as a conference paper at ICLR 2023 Figure 6b shows the performance obtained with meta-features (F) considering meta-learning (M) and ﬁne-tuning (T). A missing capital letter in the label stands for an experiment without that aspect (e.g. no M means no meta-learning, etc). The results indicate that the meta-features help DRE achieve better performance, both with and without meta-learning. The results also highlight that ﬁne-tuning (i.e. updating the scorer network’s parameters on the target tasks after each BO step) the meta-learned surrogate is important for achieving the best HPO performance. Further evidence of the signiﬁcance of these results is showcased in Figure 11b (Appendix E). Hypothesis 5. Expected Improvement is the best acquisition function for BO with DRE. We run experiments to address how DRE performs with different acquisition functions, which use DRE’s estimated rank uncertainty to explore the search space with Bayesian Optimization. Con- cretely, we ablate the Upper Conﬁdence Bound (UCB) and Expected Improvement (EI) acquisitions. Additionally, we added Average Rank (Avg) which simply recommends the conﬁguration with the highest estimated average rank, without using the posterior variance of the rank. We also add Ran- dom Search as a reference baseline. Further details on how we apply acquisitions in the BO loop are discussed in Appendix A. In this experiment, we use meta-features and weighted list-wise ranking losses. The results in Figures 6c and 10b (Appendix E) demonstrate that EI is the best choice for the ac- quisition function. As UCB and EI attained overall better performances than the simple average rank (no uncertainty), we conclude the uncertainties computed by DRE are effective in exploring the search space. 4.5 D ISCUSSION ON DRE HYPERPARAMETERS 5 20 35 50 65 80 95 Number of Evaluations 3.5 4.0 4.5 5.0 5.5 Average Rank Scorer Size 16x2 32x2 32x3 32x4 48x2 64x2 Random Figure 7: Average Rank on the meta-validation split from HPO- B. Given that DRE achieves state-of-the-art results across all the 16 search spaces (see Figure 12 in Appendix E) of HPO-B by using the same conﬁguration (e.g. number of layers for the scor- ers, number of layers for meta-feature extractor), we assume our settings (hyper-hyperparameters) are applicable straightfor- wardly to new search spaces. Such a generalization of the hyper- hyperparameters is desirable for any HPO method and liberates practitioners and researchers from having to tune DRE hyper- hyperparameters. In Figure 7 we show an ablation study compar- ing the performance of DRE for different numbers of layers (2, 3, 4), and different numbers of neurons per layer (16, 32, 64) on all the tasks of the meta-validation split from HPO-B. Given the critical difference diagram in Figure 10a, we observe the perfor- mance does not change signiﬁcantly when we vary any of these hyper-hyperparameters. However, we notice that the depth of the scorer is slightly more important to tune than the number of neu- rons per layer. We also notice that even an expressive ensemble of scorers (32x4) is able to generalize well on the meta-test split, as we have shown in our previous experiments. 5 C ONCLUSION The presented empirical results based on a very large-scale experimental protocol provide strong evidence of the state-of-the-art performance of deep ensembles optimized through learning to rank. We demonstrated that our method outperforms a large number of 11 baselines in both transfer and non-transfer HPO. In addition, we validated the design choices of our method through detailed ablations and analyses. Particularly, the results indicate the power of meta-learning surrogates from evaluations on other datasets. Overall, we believe that this paper will set a new trend in the HPO community for moving away from regression-learned surrogate functions in Bayesian Optimization. Finally, our surrogate DRE opens up an effective way to improve the HPO performance in different sub-problems, such as multi-ﬁdelity HPO, multi-objective HPO, or neural architecture search. 9Published as a conference paper at ICLR 2023 ACKNOWLEDGEMENTS This research was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foun- dation) under grant number 417962828 and grant INST 39/963-1 FUGG (bwForCluster NEMO). In addition, Josif Grabocka acknowledges the support of the BrainLinks- BrainTools Center of Excel- lence. REFERENCES Qingyao Ai, Keping Bi, Jiafeng Guo, and W. Bruce Croft. Learning a deep listwise context model for ranking reﬁnement. In The 41st International ACM SIGIR Conference on Research and De- velopment in Information Retrieval, SIGIR ’18, pp. 135–144, New York, NY , USA, 2018. Asso- ciation for Computing Machinery. ISBN 9781450356572. doi: 10.1145/3209978.3209985. URL https://doi.org/10.1145/3209978.3209985. N. Awad, N. Mallik, and F. Hutter. DEHB: Evolutionary hyberband for scalable, robust and efﬁcient hyperparameter optimization. In Z. Zhou (ed.), Proceedings of the Thirtieth International Joint Conference on Artiﬁcial Intelligence, IJCAI-21, pp. 2147–2153. ijcai.org, 2021a. Noor H. Awad, Neeratyoy Mallik, and Frank Hutter. DEHB: evolutionary hyberband for scalable, robust and efﬁcient hyperparameter optimization. CoRR, abs/2105.09821, 2021b. R´emi Bardenet, M ´aty´as Brendel, Bal ´azs K´egl, and Mich `ele Sebag. Collaborative hyperparameter tuning. In Proceedings of the 30th International Conference on Machine Learning, ICML 2013, Atlanta, GA, USA, 16-21 June 2013, pp. 199–207, 2013. James Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. J. Mach. Learn. Res., 13:281–305, 2012. Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt Deeds, Nicole Hamilton, and Greg Hul- lender. Learning to rank using gradient descent. In Proceedings of the 22nd international confer- ence on Machine learning, pp. 89–96, 2005. Fatih Cakir, Kun He, Xide Xia, Brian Kulis, and Stan Sclaroff. Deep metric learning to rank. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2019. Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and Hang Li. Learning to rank: from pairwise approach to listwise approach. In Zoubin Ghahramani (ed.), Machine Learning, Proceedings of the Twenty-Fourth International Conference (ICML 2007), Corvallis, Oregon, USA, June 20- 24, 2007, volume 227 of ACM International Conference Proceeding Series, pp. 129–136. ACM, 2007. URL https://doi.org/10.1145/1273496.1273513. Huadong Chen, Shujian Huang, David Chiang, Xinyu Dai, and Jiajun Chen. Top-rank enhanced listwise optimization for statistical machine translation. arXiv preprint arXiv:1707.05438, 2017. Wei Chen, Tie-yan Liu, Yanyan Lan, Zhi-ming Ma, and Hang Li. Ranking measures and loss functions in learning to rank. In Y . Bengio, D. Schuurmans, J. Lafferty, C. Williams, and A. Culotta (eds.), Advances in Neural Information Processing Systems , volume 22. Curran As- sociates, Inc., 2009. URL https://proceedings.neurips.cc/paper/2009/file/ 2f55707d4193dc27118a0f19a1985716-Paper.pdf. Alexander Imani Cowen-Rivers, Wenlong Lyu, Zhi Wang, Rasul Tutunov, Jianye Hao, Jun Wang, and Haitham Bou-Ammar. HEBO: heteroscedastic evolutionary bayesian optimisation. CoRR, abs/2012.03826, 2020. Janez Demˇsar. Statistical comparisons of classiﬁers over multiple data sets. J. Mach. Learn. Res., 7:1–30, dec 2006. ISSN 1532-4435. Fuli Feng, Moxin Li, Cheng Luo, Ritchie Ng, and Tat-Seng Chua. Hybrid learning to rank for ﬁnan- cial event ranking. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’21, pp. 233–243, New York, NY , USA, 2021. Association for Computing Machinery. ISBN 9781450380379. doi: 10.1145/3404835.3462969. URL https://doi.org/10.1145/3404835.3462969. 10Published as a conference paper at ICLR 2023 Matthias Feurer, Jost Tobias Springenberg, and Frank Hutter. Initializing bayesian hyperparameter optimization via meta-learning. In Blai Bonet and Sven Koenig (eds.),Proceedings of the Twenty- Ninth AAAI Conference on Artiﬁcial Intelligence, January 25-30, 2015, Austin, Texas, USA , pp. 1128–1135. AAAI Press, 2015. Matthias Feurer, Benjamin Letham, and Eytan Bakshy. Scalable meta-learning for bayesian opti- mization using ranking-weighted gaussian process ensembles. In AutoML Workshop at ICML , volume 7, 2018. Frank Hutter, Holger H. Hoos, and Kevin Leyton-Brown. Sequential model-based optimization for general algorithm conﬁguration. In Learning and Intelligent Optimization - 5th International Conference, LION 5, Rome, Italy, January 17-21, 2011. Selected Papers, pp. 507–523, 2011. doi: 10.1007/978-3-642-25566-3 \\40. Frank Hutter, Lars Kotthoff, and Joaquin Vanschoren (eds.). Automated Machine Learning - Meth- ods, Systems, Challenges . The Springer Series on Challenges in Machine Learning. Springer, 2019. ISBN 978-3-030-05317-8. doi: 10.1007/978-3-030-05318-5. Hadi S. Jomaa, Josif Grabocka, and Lars Schmidt-Thieme. Hyp-rl : Hyperparameter optimization by reinforcement learning. CoRR, abs/1906.11527, 2019. URL http://arxiv.org/abs/ 1906.11527. Hadi S Jomaa, Lars Schmidt-Thieme, and Josif Grabocka. Dataset2vec: Learning dataset meta- features. Data Mining and Knowledge Discovery, pp. 1–22, 2021a. Hadi Samer Jomaa, Sebastian Pineda Arango, Lars Schmidt-Thieme, and Josif Grabocka. Transfer learning for bayesian hpo with end-to-end landmark meta-features. In Fifth Workshop on Meta- Learning at the Conference on Neural Information Processing Systems, 2021b. Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predic- tive uncertainty estimation using deep ensembles. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V . N. Vishwanathan, and Roman Garnett (eds.), Ad- vances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 6402–6413, 2017. Ekrem ¨Ozt¨urk, Fabio Ferreira, Hadi S. Jomaa, Lars Schmidt-Thieme, Josif Grabocka, and Frank Hutter. Zero-shot automl with pretrained models. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesv ´ari, Gang Niu, and Sivan Sabato (eds.), International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA , volume 162 of Proceedings of Machine Learning Research , pp. 17138–17155. PMLR, 2022. URL https: //proceedings.mlr.press/v162/ozturk22a.html. Sebastian Pineda Arango, Hadi Jomaa, Martin Wistuba, and Josif Grabocka. Hpo- b: A large-scale reproducible benchmark for black-box hpo based on openml. In J. Vanschoren and S. Yeung (eds.), Proceedings of the Neural Information Process- ing Systems Track on Datasets and Benchmarks , volume 1, 2021. URL https: //datasets-benchmarks-proceedings.neurips.cc/paper/2021/file/ ec8956637a99787bd197eacd77acce5e-Paper-round2.pdf. Herilalaina Rakotoarison, Louisot Milijaona, Andry RASOANAIVO, Michele Sebag, and Marc Schoenauer. Learning meta-features for autoML. In International Conference on Learning Rep- resentations, 2022. URL https://openreview.net/forum?id=DTkEfj0Ygb8. David Salinas, Huibin Shen, and Valerio Perrone. A quantile-based approach for hyperparameter transfer learning. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, pp. 8438–8448, 2020. Jasper Snoek, Hugo Larochelle, and Ryan P. Adams. Practical bayesian optimization of machine learning algorithms. In Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held De- cember 3-6, 2012, Lake Tahoe, Nevada, United States, pp. 2960–2968, 2012. 11Published as a conference paper at ICLR 2023 Jasper Snoek, Oren Rippel, Kevin Swersky, Ryan Kiros, Nadathur Satish, Narayanan Sundaram, Md. Mostofa Ali Patwary, Prabhat, and Ryan P. Adams. Scalable bayesian optimization using deep neural networks. InProceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, pp. 2171–2180, 2015. Jost Tobias Springenberg, Aaron Klein, Stefan Falkner, and Frank Hutter. Bayesian optimization with robust bayesian neural networks. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 29. Curran Asso- ciates, Inc., 2016a. URL https://proceedings.neurips.cc/paper/2016/file/ a96d3afec184766bfeca7a9f989fc7e7-Paper.pdf. Jost Tobias Springenberg, Aaron Klein, Stefan Falkner, and Frank Hutter. Bayesian optimization with robust bayesian neural networks. In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pp. 4134–4142, 2016b. Jiaxi Tang and Ke Wang. Ranking distillation: Learning compact ranking models with high performance for recommender system. In Proceedings of the 24th ACM SIGKDD Interna- tional Conference on Knowledge Discovery and Data Mining , KDD ’18, pp. 2289–2298, New York, NY , USA, 2018. Association for Computing Machinery. ISBN 9781450355520. doi: 10.1145/3219819.3220021. URL https://doi.org/10.1145/3219819.3220021. Zi Wang, George E Dahl, Kevin Swersky, Chansoo Lee, Zelda Mariet, Zachary Nado, Justin Gilmer, Jasper Snoek, and Zoubin Ghahramani. Pre-trained gaussian processes for bayesian optimization. arXiv preprint arXiv:2109.08215, 2021. Ying Wei, Peilin Zhao, Huaxiu Yao, and Junzhou Huang. Transferable neural processes for hy- perparameter optimization. CoRR, abs/1909.03209, 2019. URL http://arxiv.org/abs/ 1909.03209. Colin White, Willie Neiswanger, and Yash Savani. BANANAS: bayesian optimization with neural architectures for neural architecture search. In Thirty-Fifth AAAI Conference on Artiﬁcial Intelli- gence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artiﬁcial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artiﬁcial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021, pp. 10293–10301, 2021. Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, and Eric P. Xing. Deep kernel learning. In Arthur Gretton and Christian C. Robert (eds.), Proceedings of the 19th International Con- ference on Artiﬁcial Intelligence and Statistics , volume 51 of Proceedings of Machine Learning Research, pp. 370–378, 2016. Martin Wistuba and Josif Grabocka. Few-shot bayesian optimization with deep kernel surrogates. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021, 2021. Martin Wistuba and Tejaswini Pedapati. Learning to rank learning curves. In Proceedings of the 37th International Conference on Machine Learning, ICML’20. JMLR.org, 2020. Martin Wistuba, Nicolas Schilling, and Lars Schmidt-Thieme. Learning hyperparameter optimiza- tion initializations. In 2015 IEEE International Conference on Data Science and Advanced Ana- lytics, DSAA 2015, Campus des Cordeliers, Paris, France, October 19-21, 2015, pp. 1–10, 2015. doi: 10.1109/DSAA.2015.7344817. Martin Wistuba, Nicolas Schilling, and Lars Schmidt-Thieme. Two-stage transfer surrogate model for automatic hyperparameter optimization. In Paolo Frasconi, Niels Landwehr, Giuseppe Manco, and Jilles Vreeken (eds.), Machine Learning and Knowledge Discovery in Databases - European Conference, ECML PKDD 2016, Riva del Garda, Italy, September 19-23, 2016, Proceedings, Part I, volume 9851 of Lecture Notes in Computer Science, pp. 199–214. Springer, 2016. Jian Wu and Peter I. Frazier. Practical Two-Step Look-Ahead Bayesian Optimization. Curran Asso- ciates Inc., Red Hook, NY , USA, 2019. 12Published as a conference paper at ICLR 2023 Liang Wu, Diane Hu, Liangjie Hong, and Huan Liu. Turning clicks into purchases: Revenue opti- mization for product search in e-commerce. In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, pp. 365–374, 2018. Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, and Alexander J Smola. Deep sets. In I. Guyon, U. V on Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems , volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/ paper/2017/file/f22e4747da1aa27e363d86d40ff442fe-Paper.pdf. 13Published as a conference paper at ICLR 2023 A B AYESIAN OPTIMIZATION WITH DEEP RANKING ENSEMBLES Once the Deep Ensembles are trained, we aggregate the predictions for an input x following the procedure explained in Section 3.2 to obtain µ(x),σ(x) and conditioning to a set of observations Ds. For the sake of simplicity, we omit this conditioning in our notation. These outputs can be fed in several types of acquisition functions and decide for the next point xto observe from the set of pending points to evaluate X. Notice that the lower rank, the better the conﬁguration, therefore we formulate the cast the acquisition function as a minimization problem. Speciﬁcally, we consider: • Average Rank: α(xj) =µ(xj) • Lower Conﬁdence Bound: α(xj) =µ(xj) −β·σ(xj) • Expected Improvement: α(xj) =− ∫ rmax (0,µ(xk) −r) N(r; µ(xj),σ(xj)) Where βis a factor that trades of exploitation and exploration andxk is the best-observed conﬁgura- tion, i.e. k = arg mini∈{1,...,|Ds|}yi and µ(xk) is the average rank predicted for that conﬁguration and yk is its validation error. The previous formulation assumes a minimization, thus to choose the next query point you apply: x= arg minxj∈Xα(xj). Algorithm 2: Bayesian Optimization with DRE Input : A prior distribution over datasets p(D), initial observations H = {(x1,y1),..., (xN,yN)}, pending points X, number of BO iterations K, black-box function to optimize f Output: Best observed conﬁguration x∗ 1 Train ensemble of MLP scorers following Algorithm 1 and prior p(D); 2 for j ←1 to Kdo 3 Fine-tune/Train MLP scorers ; 4 Suggest next candidate x= arg minxj∈Xα(xj,H) ; 5 Observe response y= f(x) ; 6 Update history H = H∪{(x,y)}; 7 end 8 Return top performing conﬁguration: arg min(xi,yi)∈H yi B E XPERIMENTAL SETUP FOR DEEP RANKING ENSEMBLES Meta-Feature Extractor The DRE model has two conﬁgurable components: the meta-feature net- work and the scorers. The meta-feature extractor is a DeepSet with an architecture similar to the one used by Jomaa et al. (2021a). However, we used 2 fully connected layers with 32 neurons each for both φand ρ(Deep Set parameters) instead of 3 fully connected layers. The output size is set to 16 by default. Ensemble of Scorers The ensemble of scorers is a group of 10 MLP (Multilayer Perceptrons) with identical architectures. Each neural network has 4 hidden layers and each hidden layer has 32 neurons. The neural networks are initialized independently and randomly (for DRE-RI) or warm- initialized with the meta-learned weights. The input size of each neural network is 16 (the dimesi- ionality of the meta-features), plus the HP search space dimensionality. their output size is 1. Setup for Motivating Example. For the creation of the Figure 2, we use as scorer network an MLP with 2 hidden layers and 10 neurons per layer. The meta-feature extractor has 4 layers and 10 neurons, and output dimensions equal to 10. The network is meta-trained for 1000 epochs, with batch size 10, learning rate 0.001, Adam Optimizer, and 10 models in the ensemble. For the meta-learning example, we do not ﬁne-tune the networks, while we ﬁne-tune the networks for the non-meta-learned example for 500 iterations. 14Published as a conference paper at ICLR 2023 Table 1: Average Cost per BO Step (in seconds) 4796 (3 Dims) 5636 (6 Dims) 5527 (8 Dims) 5965 (10 Dims) 5906 (16 Dims) HEBO 0.27 ±0.18 3.11 ±1.68 2.66 ±0.95 3.21 ±1.78 2.85 ±2.43 FSBO 10.49 ±2.92 10.13 ±1.51 10.61 ±4.47 11.45 ±4.35 12.13 ±6.41 DRE 22.29 ±3.81 18.8 ±3.57 22.61 ±3.85 19.39 ±3.81 22.29 ±3.79 C D ISCUSSION ON LIST SIZE AND LIST WEIGHTS (a) List Size Ablation 5 20 35 50 65 80 95 Number of Evaluations 3.0 3.5 4.0 Average Rank Random L=10 L=50 L=100 L=200 (b) List Weights 5 20 35 50 65 80 95 Position 0.0 0.5 1.0 1.5 Weight I. Log I. Linear PDA Figure 8: Effect of parameters in list-wise loss We present an additional ablation on the list size. We report the aver- age rank on the meta-validation split for different list sizes during meta- training on Figure 8a. Notice, that a small list size (10) leads to an un- derperforming setting. Therefore, it is important to consider relative large list sizes (100 ≤n). During meta-testing i.e. by perform- ing BO, there is no signiﬁcant over- head in terms of having a larger list size, because the true rank is derived from the observed validation accu- racy of conﬁgurations. During both meta-training, as well as the BO step, we ﬁt our surrogate to estimate the rank of previously observed conﬁgu- rations that have been already evalu- ated. Given nobservations, comput- ing the true rank is a simpleO(n·log(n)) sorting operation. Notice that in BO settingsnis typically small. There are several weighting schemes. Two alternatives to the weighting factor we use (inverse log weighting) are inverse linear weighting and position-dependent attention (PDA) (Chen et al., 2017). As you can see in Figure 8b, inverse linear gives very small weight to lower ranks, while the position- dependent gives too much importance. In this plot, PDA weights were scaled to make it comparable to the other schemes. We decided to use the inverse log weighting because it gives neither too low nor too high weight to lower ranks. For the j-th position in a list with kelements, these weights can be described as follows: • Inverse Log: w(j) = 1 log(j+1) • Inverse Linear: w(j) =1 j • Position-dependent attention: w(j) =k−j+1∑k t=1 t D D ISCUSSION ON COMPUTATIONAL COST We provide here a cost comparison between DRE, FSBO and HEBO. In the Table 1, we provide the average cost per BO step ( ±standard deviation) for different search spaces (with different di- mensions). DRE effectively incurs a cost higher than FSBO and HEBO, but <30 seconds, which is a very small overhead compared to the cost of actually evaluating hyperparameter conﬁgurations (evaluation means the expensive process of training classiﬁers given the hyperparameter conﬁgura- tions and computing the validation accuracy). 15Published as a conference paper at ICLR 2023 E A DDITIONAL PLOTS We present additional results on the critical difference diagrams fori) Transfer methods results (Fig- ure 9a), ii) Non-Transfer (Figure 9b, iii) Scorer size (Figure 10a, iv) Acquisition Function (Figure 10b, v Ranking Loss (Figure 11a) and vi Meta-features (Figure 11b). These CD plots show the com- parison of the performance at different number of trials (e.g. at 25 trials = Rank@25). The vertical lines connecting two methods indicate that their performances are not signiﬁcantly different. (a) Comparison vs. transfer methods  (b) Comparison vs. non-transfer methods Figure 9: Critical Difference Diagram for a) Transfer and b) Non-transfer. (a) Ablation of the DRE Scorer size  (b) Ablation of the Acquisition Function Figure 10: Critical Difference Diagram for the results of the ablation of DRE hyperparameters in (a) and the choice of the acquisition function from Hypothesis 5 in (b). 16Published as a conference paper at ICLR 2023 (a) Ablation of Ranking Loss  (b) Ablation of Meta-Features Figure 11: Critical Difference Diagrams for the results of Hypothesis 3 in a) and Hypothesis 4 in b). Figure 12: Average Rank per Search Space (Transfer Methods) 17Published as a conference paper at ICLR 2023 Figure 13: Average Rank per Search Space (Non-Transfer Methods) 18",
      "meta_data": {
        "arxiv_id": "2303.15212v2",
        "authors": [
          "Abdus Salam Khazi",
          "Sebastian Pineda Arango",
          "Josif Grabocka"
        ],
        "published_date": "2023-03-27T13:52:40Z",
        "pdf_url": "https://arxiv.org/pdf/2303.15212v2.pdf",
        "github_url": "https://github.com/huawei-noah/HEBO"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces Deep Ranking Ensembles (DRE), a novel neural network Bayesian Optimization (BO) surrogate optimized with Learning-to-Rank (L2R) losses. It addresses the sub-optimal performance of existing Hyperparameter Optimization (HPO) methods that train surrogates as regression tasks, hypothesizing that preserving the ranks of hyperparameter configuration performances is a superior strategy. DRE meta-learns an ensemble of neural networks for ranking and models their uncertainty. This method, along with a new technique for meta-learning from large-scale public meta-datasets, achieves new state-of-the-art results in HPO across extensive experimental protocols.",
        "methodology": "Deep Ranking Ensembles (DRE) utilize an ensemble of diverse neural networks, each trained to output a ranking score for hyperparameter configurations. These networks are optimized using a weighted list-wise L2R loss, which assigns higher penalties to top-performing configurations, emphasizing their correct rank prediction. Uncertainty estimation is achieved by training multiple diverse neural scorers with stochastic gradient descent and different random seeds. The method incorporates meta-learning by pre-training these rankers on evaluations from previous datasets and boosts transfer quality by integrating dataset meta-features, extracted using a Deep Set architecture, as additional input to the scorers. During Bayesian Optimization, the ensemble's posterior mean and variance of estimated ranks are used by acquisition functions like Expected Improvement to guide the search for the next optimal configuration.",
        "experimental_setup": "Experiments were conducted on HPO-B, the largest public HPO benchmark, featuring 16 search spaces and 86 meta-test datasets/tasks. The DRE method was rigorously compared against 12 state-of-the-art HPO baselines, including both transfer and non-transfer algorithms. The evaluation protocol involved starting with 5 initial random configurations and performing 100 Bayesian Optimization iterations, with results averaged across 5 runs per task. Ablation studies were performed to analyze the impact of different L2R losses (point-wise, pair-wise, list-wise, and weighted list-wise), the utility of meta-features, various acquisition functions (Expected Improvement, Upper Confidence Bound, Average Rank), and different scorer network architectures (number of layers and neurons). The meta-feature extractor used a Deep Set architecture with 5 hidden layers and 32 neurons, while the ensemble consisted of 10 MLPs, each with 4 hidden layers and 32 neurons.",
        "limitations": "Not mentioned",
        "future_research_directions": "The authors suggest that the DRE surrogate opens up effective avenues to improve HPO performance in various sub-problems, specifically mentioning multi-fidelity HPO, multi-objective HPO, and neural architecture search.",
        "experimental_code": "File Path: AIRBO/kernels/expected_rbf_kernel.py\nContent:\nfrom gpytorch.kernels.kernel import Kernel\nfrom typing import Optional, Tuple\nfrom gpytorch.priors import Prior\nfrom gpytorch.constraints import Interval\nimport torch\n\n\ndef postprocess_rbf(dist_mat):\n    return dist_mat.div_(-2).exp_()\n\n\ndef default_postprocess_script(x):\n    return x\n\n\nclass ExpectedRBFKernel(Kernel):\n    \"\"\"\n    Expected RBF kernel with close-form integral solution\n    \"\"\"\n    has_lengthscale = True\n\n    def __init__(self, ard_num_dims: Optional[int] = None,\n                 batch_shape: Optional[torch.Size] = torch.Size([]),\n                 active_dims: Optional[Tuple[int, ...]] = None,\n                 lengthscale_prior: Optional[Prior] = None,\n                 lengthscale_constraint: Optional[Interval] = None,\n                 eps: Optional[float] = 1e-6,\n                 **kwargs):\n        super(ExpectedRBFKernel, self).__init__(\n            ard_num_dims, batch_shape, active_dims,\n            lengthscale_prior, lengthscale_constraint, eps\n        )\n\n    def forward(self, x1, x2, diag=False, **params):\n        \"\"\"\n        We assume that x1 and x2 are Gaussian variables\n        :param x1: a tensor of M * 2D or B * M * 2D, the first D dimensions are the means and the rests are stds\n        :param x2: a tensor of N * 2D or B * M * 2D, the first D dimensions are the means and the rests are stds\n        :param diag: whether it only needs to compute the diagonal\n        :param params: configuration keywords\n        :return: a tensor of M * N\n        \"\"\"\n        B = x1.shape[:-2] if x1.ndim >= 3 else None\n        M = x1.shape[-2]\n        N = x2.shape[-2]\n        D = x1.shape[-1] // 2\n        dtype = x1.dtype\n        device = x1.device\n\n        x1_mean = x1[..., 0:D]\n        x1_var = x1[..., D:] ** 2.0\n        x2_mean = x2[..., 0:D]\n        x2_var = x2[..., D:] ** 2.0\n\n        Var1 = torch.diag_embed(x1_var)  # B * M * D * D\n        Var2 = torch.diag_embed(x2_var)  # B * N * D * D\n\n        if self.ard_num_dims is None:\n            ls_vec = self.lengthscale.repeat(1, D).view(-1)\n        else:\n            ls_vec = self.lengthscale.view(-1)\n        W = torch.diag(ls_vec ** 2.0)  # lengthscale in (D, D)\n\n        # numerator\n        AB = x1_mean.unsqueeze(-2) - x2_mean.unsqueeze(-3)  # AB=A-B: broadcast --> (B, M, N, D)\n        VAVB = Var1.unsqueeze(-3) + Var2.unsqueeze(-4)  # VAVB = VarA + VarB: (M, N, D, D)\n        Z = W.unsqueeze(0).unsqueeze(0) if B is None \\\n            else W.unsqueeze(0).unsqueeze(0).unsqueeze(\n            0) + VAVB  # Z = W + VarA + VarB: (M, N, D, D)\n        Z_inv = torch.inverse(Z)  # (M, N, D, D)\n\n        ABZ_eq = 'mnpd,mndd->mnpd' if B is None else 'bmnpd,bmndd->bmnpd'\n        ABZ = torch.einsum(ABZ_eq, [AB.unsqueeze(-2), Z_inv])  # (A-B)Z^-1, (M, N, 1, D)\n        nu_eq = 'mnpd,mndq->mnpq' if B is None else 'bmnpd,bmndq->bmnpq'\n        nu = torch.einsum(nu_eq, [ABZ, AB.unsqueeze(-1)]).squeeze(-1).squeeze(\n            -1)  # (A-B)Z^-1(A-B)^T:(M, N, 1, 1)\n\n        # denominator\n        x1_eq_x2 = torch.equal(x1, x2)\n        if diag:\n            # Special case the diagonal because we can return all zeros most of the time.\n            if x1_eq_x2:\n                res = torch.zeros(*x1.shape[:-1], x1.shape[-2], dtype=x1.dtype, device=x1.device)\n                res = postprocess_rbf(res)\n                return res\n            else:\n                res = torch.norm(x1 - x2, p=2, dim=-1)\n                res = res.pow(2)\n                res = postprocess_rbf(res)\n                return res\n        else:\n            W_inv = W.inverse().unsqueeze(0).unsqueeze(0)\n            if B is not None:\n                W_inv = W_inv.unsqueeze(0)\n            de_coeff = (W_inv.matmul(VAVB)).det().sqrt()\n            if x1_eq_x2:\n                # if the x1 == x2, then it needs separate computations for the diagonal\n                # and non-diagonal elements, see the paper appendix C for the details.\n                zero_diag_identity = (torch.ones((M, M), dtype=dtype).to(device) -\n                                      torch.eye(M, M, dtype=dtype).to(device))\n                one_diag = torch.eye(M, M, dtype=dtype).to(device)\n                if B is not None:\n                    zero_diag_identity = zero_diag_identity.unsqueeze(0)\n                    one_diag = one_diag.unsqueeze(0)\n                de = de_coeff * zero_diag_identity + one_diag\n            else:\n                de = de_coeff\n\n            covar = nu / de\n\n            return postprocess_rbf(covar)\n\nFile Path: AIRBO/kernels/kme_kernel.py\nContent:\n\"\"\"\nKernel-mean-embedding Kernel defined in uGP-UCB:\n    K(X, X`) = \\int_x \\int_x` k(x, x`) dP(x) dP(x`)\n\"\"\"\nfrom gpytorch.kernels.kernel import Kernel\nimport gc\nfrom utils import commons as cm\nfrom functools import partial\nimport torch\n\n\ndef postprocess_linear(dist_mat):\n    sym_dist_mat = ((dist_mat + dist_mat.transpose(-2, -1)) * 0.5)\n    return sym_dist_mat\n\n\ndef integral_KME_batch(X, Y, kernel: Kernel):\n    \"\"\"\n    Estimate the KME via sampling and integral\n    :param X: B * M * C * D tensor, B is the batch size, M is the sample size,\n              C is the sampling size, and D is the data dim.\n    :param Y: B * N * H * D tensor, B is the batch size, N is the sample size,\n              H is the sampling size, and D is the data dim.\n    :param kernel: A gpytorch.kernel.Kernel instance\n    :return: a tensor of M * N\n    \"\"\"\n    dist_mat = None\n    try:\n        B = X.shape[:-3]\n        M, C, D = X.shape[-3:]\n        N, H, D = Y.shape[-3:]\n        _original_shape = kernel.batch_shape\n        kernel.batch_shape = torch.Size([M, N])\n        dist_mat = kernel(\n            X.unsqueeze(-3),  # cast into B * M * 1 * m * D\n            Y.unsqueeze(-4)  # cast into B * 1 * N * m * D\n        ).evaluate()  # B * M * N * m * m\n\n        kme = dist_mat.mean(dim=[-1, -2])\n        kernel.batch_shape = _original_shape\n    finally:\n        cm.free_memory(\n            [\n                dist_mat,\n            ],\n            debug=False\n        )\n\n    return kme\n\n\ndef estimate_KME_in_chunks(X, Y, estimator, chunk_size=10):\n    \"\"\"\n    Estimate the KME in chunks, decrease the chunk size if GPU memory error happens\n    :param X: B * M * C * D or M * C * D tensor\n    :param Y: B * N * H * D or N * H * D tensor\n    :param estimator:\n    :param chunk_size:\n    :return:\n    \"\"\"\n    N = Y.shape[-3]\n\n    retry_i = 0\n    success = False\n    _practicable_chunk_size = chunk_size\n    kme_results = None\n    while not success:\n        kme_results = []\n        _Y = None\n        _kme = None\n        try:\n            _practicable_chunk_size = max(chunk_size // (2 ** retry_i), 1)\n            for bs in range(0, N, _practicable_chunk_size):\n                be = min(N, bs + _practicable_chunk_size)\n                _Y = Y[..., bs:be, :, :]\n                _kme = estimator(X, _Y)\n                kme_results.append(_kme)\n\n                gc.collect()\n                torch.cuda.empty_cache()\n            success = True\n        except RuntimeError as e:\n            if 'CUDA out of memory' in e.args[0] or 'not enough memory' in e.args[0]:\n                if _practicable_chunk_size > 1:  # we can still try to reduce the chunk size\n                    print(f'Chunk size {_practicable_chunk_size} is too large, '\n                          f'reduce it by a half:', e)\n                    retry_i += 1\n\n                    if len(kme_results) > 0:\n                        cm.free_memory(kme_results)\n                        for _m in kme_results:\n                            del _m\n                        del kme_results\n                    if '_kme' in locals():\n                        cm.free_memory([_kme])\n                    if '_Y' in locals():\n                        cm.free_memory([_Y])\n                else:\n                    raise ValueError('Chunk size has been reduced to 1 but still out of memory, '\n                                     'try cpu.')\n            else:\n                raise e\n        finally:\n            gc.collect()\n            torch.cuda.empty_cache()\n\n    kme = torch.concat(kme_results, dim=-1)  # concat to columns as we chuck the Y\n    return kme\n\n\nclass KMEKernel(Kernel):\n    \"\"\"\n    Decorating an existing kernel with KME\n    \"\"\"\n\n    has_lengthscale = True\n\n    def __init__(self, base_kernel, **kwargs):\n        super(KMEKernel, self).__init__(**kwargs)\n        self.base_kernel = base_kernel\n        self.chunk_size = kwargs.get('chunk_size', 100)\n        self.estimator = kwargs.get('estimator', 'integral')\n        self.estimation_trials = kwargs.get('estimation_trials', 1)\n        if self.estimator == 'integral':\n            self.estimation_func = partial(\n                integral_KME_batch, kernel=self.base_kernel\n            )\n        else:\n            raise ValueError('Unsupported estimator name', self.estimator)\n\n    @property\n    def is_stationary(self) -> bool:\n        \"\"\"\n        Kernel is stationary if base kernel is stationary.\n        \"\"\"\n        return self.base_kernel.is_stationary\n\n    def compute_distance_covariance_matrix(self, x1, x2, diag=False, **params):\n        chunk_size = params.get('chunk_size', self.chunk_size)\n        avg_dist_mat = None\n        for _ in range(self.estimation_trials):\n            dist_mat = estimate_KME_in_chunks(x1, x2, self.estimation_func, chunk_size)\n            avg_dist_mat = dist_mat if avg_dist_mat is None else avg_dist_mat + dist_mat\n        avg_dist_mat = avg_dist_mat / self.estimation_trials\n        cov_mat = postprocess_linear(avg_dist_mat.div(self.lengthscale ** 2.0))\n        return avg_dist_mat, cov_mat\n\n    def forward(self, x1, x2, diag=False, **params):\n        avg_dist_mat, cov_mat = self.compute_distance_covariance_matrix(x1, x2, diag, **params)\n        return cov_mat\n\nFile Path: AIRBO/kernels/mmd_kernel.py\nContent:\n\"\"\"\nMaximum Mean Discrepancy (MMD) Kernel\n\"\"\"\nimport gpytorch as gpyt\nfrom gpytorch.kernels.kernel import Kernel\nimport torch\nfrom tqdm.auto import trange\nimport gc\nfrom utils import commons as cm\nfrom functools import partial\n\n\ndef additive_RQ_kernel(alphas=(0.2, 0.5, 1, 2, 5), ls=1.0, learnable_ls=False):\n    assert len(alphas) > 0\n    _k_list = []\n    for a in alphas:\n        _k = gpyt.kernels.RQKernel()\n        _k.alpha = a\n        _k.lengthscale = ls\n        _k.raw_lengthscale.require_grad = learnable_ls\n        _k_list.append(_k)\n    k = gpyt.kernels.AdditiveKernel(*_k_list)\n    return k\n\n\ndef combo_kernel(alphas=(0.2, 0.5, 1, 2, 5), ls=1.0, learnable_ls=False):\n    assert len(alphas) > 0\n    _k_list = []\n    for a in alphas:\n        _k = gpyt.kernels.RQKernel()\n        _k.alpha = a\n        _k.lengthscale = ls\n        _k.raw_lengthscale.require_grad = learnable_ls\n        _k_list.append(_k)\n    _k_list.append(gpyt.kernels.LinearKernel())\n    k = gpyt.kernels.AdditiveKernel(*_k_list)\n    return k\n\n\ndef postprocess_mmd(dist_mat):\n    sym_dist_mat = (dist_mat + dist_mat.T) * 0.5\n    _dist_mat = sym_dist_mat.clamp_(min=0) ** 0.5\n    return torch.clamp(1.0 - _dist_mat, 0.0, 1.0)\n\n\ndef postprocess_mmd_rbf(dist_mat):\n    _dist_mat = ((dist_mat + dist_mat.transpose(-2, -1)) * 0.5)\n    return _dist_mat.div_(-2).exp_()\n\n\ndef nystrom_mmd(X, Y, kernel: Kernel, sub_samp_size: int = 100):\n    \"\"\"\n    nystrom estimator for MMD2\n    :param X: B * D tensor\n    :param Y: H * D tensor\n    :param kernel: gpytorch.kernel.Kernel instance\n    :param sub_samp_size: the subsample number\n    :return: a scalar\n    \"\"\"\n    # sub-sampling\n    B, D = X.shape\n    x_sub_inds = torch.randperm(B)[:sub_samp_size]\n    X_sub = X[x_sub_inds, :]\n\n    H, D = Y.shape\n    y_sub_inds = torch.randperm(H)[:sub_samp_size]\n    Y_sub = Y[y_sub_inds, :]\n\n    # compute alpha_x\n    k_m_x = kernel(X_sub, X_sub).evaluate()\n    k_m_x_inv = torch.linalg.pinv(k_m_x)\n    k_mn_x = kernel(X_sub, X).evaluate()\n    alpha_x = (k_m_x_inv @ k_mn_x @ torch.ones(X.shape[0], 1).type(X.dtype).to(X.device)) \\\n              / X.shape[0]\n\n    # compute alpha_y\n    k_m_y = kernel(Y_sub, Y_sub).evaluate()\n    k_m_y_inv = torch.linalg.pinv(k_m_y)\n    k_mn_y = kernel(Y_sub, Y).evaluate()\n    alpha_y = (k_m_y_inv @ k_mn_y @ torch.ones(Y.shape[0], 1).type(Y.dtype).to(Y.device)) \\\n              / Y.shape[0]\n\n    # nystrom estimator\n    part1 = alpha_x.T @ k_m_x @ alpha_x\n    part2 = alpha_y.T @ k_m_y @ alpha_y\n    part3 = alpha_x.T @ kernel(X_sub, Y_sub).evaluate() @ alpha_y * -2\n\n    mmd2 = part1 + part2 + part3\n\n    return mmd2\n\n\ndef nystrom_mmd_batch(X, Y, kernel: Kernel, sub_samp_size: int = 100):\n    \"\"\"\n    nystrom estimator for MMD2 in batches\n    :param X: B * M * C * D tensor, B is the batch size, M is the sample size,\n              C is the sampling size, and D is the data dim.\n    :param Y: B * N * H * D tensor, B is the batch size, N is the sample size,\n              H is the sampling size, and D is the data dim.\n    :param kernel: A gpytorch.kernel.Kernel instance\n    :param sub_samp_size: the subsampling size\n    :return: a tensor of M * N\n    \"\"\"\n    km_x_inv = None\n    kmn_x = None\n    ones_x = None\n    km_y_inv = None\n    kmn_y = None\n    ones_y = None\n    km_x, km_y = None, None\n    alpha_x, km_xy = None, None\n    alpha_y, X_sub, Y_sub = None, None, None\n    part1, part2, part3 = None, None, None\n    try:\n        B = X.shape[:-3]\n        M, C, D = X.shape[-3:]\n        N, H, D = Y.shape[-3:]\n        _original_shape = kernel.batch_shape\n\n        # sub-sampling\n        x_sub_inds = torch.randperm(C)[:sub_samp_size]\n        X_sub = X[..., x_sub_inds, :]  # M * m * D, m is the sub-sampling size\n\n        y_sub_inds = torch.randperm(H)[:sub_samp_size]\n        Y_sub = Y[..., y_sub_inds, :]  # N * m * D\n\n        # compute alpha for x variables\n        kernel.batch_shape = torch.Size([M])\n        km_x = kernel(X_sub, X_sub).evaluate()  # B * M * m * m\n        km_x_inv = torch.linalg.pinv(km_x)  # B * M * m * m\n        kmn_x = kernel(X_sub, X).evaluate()  # B * M * m * C\n        ones_x = torch.ones(M, C, 1).type(X.dtype).to(X.device)  # M * C * 1\n        alpha_x = (km_x_inv @ kmn_x @ ones_x) / C  # B * M * m * 1\n        # cm.free_memory([km_x_inv, kmn_x, ones_x], debug=False)\n\n        # compute alpha for y variables\n        kernel.batch_shape = torch.Size([N])\n        km_y = kernel(Y_sub, Y_sub).evaluate()  # B * N * m * m\n        km_y_inv = torch.linalg.pinv(km_y)  # B * N * m * m\n        kmn_y = kernel(Y_sub, Y).evaluate()  # B * N * m * H\n        ones_y = torch.ones(N, H, 1).type(Y.dtype).to(Y.device)  # M * H * 1\n        alpha_y = (km_y_inv @ kmn_y @ ones_y) / H  # B * N * m * 1\n        # cm.free_memory([km_y_inv, kmn_y, ones_y], debug=False)\n\n        # nystrom estimator\n        part1 = (alpha_x.transpose(-2, -1) @ km_x @ alpha_x).view(*B, M, 1)  # a_x^T * km_x * a_x\n        part2 = (alpha_y.transpose(-2, -1) @ km_y @ alpha_y).view(*B, 1, N)  # a_y^T * km_y * a_y\n        # cm.free_memory([km_x, km_y], debug=False)\n\n        kernel.batch_shape = torch.Size([M, N])\n        km_xy = kernel(X_sub.unsqueeze(-3),\n                       Y_sub.unsqueeze(-4)).evaluate()  # broadcast to M * N --> M * N * m * m\n        # a_x^T * km_xy * a_y\n        part3 = (alpha_x.unsqueeze(-3).transpose(-2, -1) @ km_xy @ alpha_y.unsqueeze(-4)).view(*B,\n                                                                                               M, N)\n\n        mmd2 = part1 + part2 - part3 * 2.0  # broadcast to M * n and add\n        # cm.free_memory([alpha_x, km_xy, alpha_y, X_sub, Y_sub, part1, part2, part3], debug=False)\n        kernel.batch_shape = _original_shape\n    finally:\n        cm.free_memory(\n            [\n                km_x_inv, kmn_x, ones_x,\n                km_y_inv, kmn_y, ones_y,\n                km_x, km_y,\n                alpha_x, km_xy, alpha_y, X_sub, Y_sub, part1, part2, part3,\n            ],\n            debug=False\n        )\n\n    return mmd2\n\n\ndef empirical_mmd(X, Y, kernel: Kernel):\n    \"\"\"\n    estimate the MMD\n    :param X: B*D tensor\n    :param Y: H*D tensor\n    :param kernel: gpytorch.kernels.Kernel\n    :return: a scalar\n    \"\"\"\n    # xx\n    cm_xx = kernel(X, X).evaluate()\n    avg_xx_mmd = (cm_xx.sum() - torch.diagonal(cm_xx).sum()) / (X.shape[0] * (X.shape[0] - 1))\n\n    # yy\n    cm_yy = kernel(Y, Y).evaluate()\n    avg_yy_mmd = (cm_yy.sum() - torch.diagonal(cm_yy).sum()) / (Y.shape[0] * (Y.shape[0] - 1))\n\n    # xy\n    cm_xy = kernel(X, Y).evaluate()\n    avg_xy_mmd = cm_xy.sum() / (X.shape[0] * Y.shape[0])\n\n    mmd = avg_xx_mmd + avg_yy_mmd - 2.0 * avg_xy_mmd\n\n    return mmd\n\n\ndef estimate_mmd_in_chunks(X, Y, estimator, chunk_size=10):\n    \"\"\"\n    estimate the MMD in chunks, decrease the chunk size if GPU memory error happens\n    :param X: B * M * C * D or M * C * D tensor\n    :param Y: B * N * H * D or N * H * D tensor\n    :param estimator:\n    :param chunk_size:\n    :return:\n    \"\"\"\n    N = Y.shape[-3]\n\n    retry_i = 0\n    success = False\n    _practicable_chunk_size = chunk_size\n    while not success:\n        try:\n            _practicable_chunk_size = max(chunk_size // (2 ** retry_i), 1)\n            mmd_results = []\n            for bs in range(0, N, _practicable_chunk_size):\n                be = min(N, bs + _practicable_chunk_size)\n                _Y = Y[..., bs:be, :, :]\n                _mmd = estimator(X, _Y)\n                mmd_results.append(_mmd)\n\n                gc.collect()\n                torch.cuda.empty_cache()\n            success = True\n        except RuntimeError as e:\n            if 'CUDA out of memory' in e.args[0] or 'not enough memory' in e.args[0]:\n                if _practicable_chunk_size > 1:  # we can still try to reduce the chunk size\n                    print(f'Chunk size {_practicable_chunk_size} is too large, '\n                          f'reduce it by a half:', e)\n                    retry_i += 1\n\n                    if len(mmd_results) > 0:\n                        cm.free_memory(mmd_results)\n                        for _m in mmd_results:\n                            del _m\n                        del mmd_results\n                    if '_mmd' in locals():\n                        cm.free_memory([_mmd])\n                    if '_Y' in locals():\n                        cm.free_memory([_Y])\n                else:\n                    raise ValueError('Chunk size has been reduced to 1 but still out of memory, '\n                                     'try cpu.')\n            else:\n                raise e\n        finally:\n            gc.collect()\n            torch.cuda.empty_cache()\n\n    mmd = torch.concat(mmd_results, dim=-1)  # concat to columns as we chuck the Y\n    return mmd\n\n\ndef empirical_mmd_batch(X, Y, kernel: gpyt.kernels.Kernel):\n    \"\"\"\n    Empirically estimate the MMD in batches\n    :param X: M * B * D tensor, M is the batch size, B is the sample size, D is the data dimension\n    :param Y: N * H * D tensor, N is the batch size, H is the sample size, D is the data dimension\n    :param kernel: the kernel to use\n    :return: a tensor of M*N\n    \"\"\"\n    B = X.shape[:-3]\n    M, C, D = X.shape[-3:]\n    N, H, D = Y.shape[-3:]\n    _original_shape = kernel.batch_shape\n\n    cm_xx = None\n    cm_yy = None\n    cm_xy = None\n    try:\n\n        # compute the covariance btw X and X\n        kernel.batch_shape = torch.Size([M])  # align along M, and compute kernel for each pair\n        cm_xx = kernel(X, X).evaluate()  # M * B * B\n        avg_xx_mmd = (cm_xx.sum((-2, -1)) - torch.diagonal(cm_xx, dim1=-2, dim2=-1).sum(-1)) \\\n                     / (C * (C - 1))  # M * 1\n\n        # compute the covariance btw Y and Y\n        kernel.batch_shape = torch.Size([N])  # align along N, and compute kernel for each pair\n        cm_yy = kernel(Y, Y).evaluate()  # N x H x H\n        avg_yy_mmd = (cm_yy.sum((-2, -1)) - torch.diagonal(cm_yy, dim1=-2, dim2=-1).sum(-1)) \\\n                     / (H * (H - 1))\n\n        # compute the covariance btw X and Y\n        kernel.batch_shape = torch.Size([M, N])  # make a grid of M * N, apply kernel on each pair\n        cm_xy = kernel(X.unsqueeze(-3), Y.unsqueeze(-4)).evaluate()  # broadcast to M * N, output M x N x B x H\n        avg_xy_mmd = cm_xy.sum((-2, -1)) / (C * H)\n\n        mmd2 = avg_xx_mmd.unsqueeze(-1) + avg_yy_mmd.unsqueeze(-2) - 2.0 * avg_xy_mmd  # broadcast and elementwise add\n\n    finally:\n        kernel.batch_shape = _original_shape\n        cm.free_memory(\n            [cm_xx, cm_yy, cm_xy, ],\n            debug=False\n        )\n    return mmd2\n\n\nclass MMDKernel(Kernel):\n    \"\"\"\n    Decorating an existing kernel with MMD distance\n    \"\"\"\n\n    has_lengthscale = True\n\n    def __init__(self, base_kernel, **kwargs):\n        super(MMDKernel, self).__init__(**kwargs)\n        self.base_kernel = base_kernel\n        self.chunk_size = kwargs.get('chunk_size', 100)\n        self.estimator = kwargs.get('estimator', 'nystrom')\n        self.sub_samp_size = kwargs.get('sub_samp_size', 100)\n        self.estimation_trials = kwargs.get('estimation_trials', 1)\n        if self.estimator == 'nystrom':\n            self.estimation_func = partial(\n                nystrom_mmd_batch, kernel=self.base_kernel, sub_samp_size=self.sub_samp_size\n            )\n        elif self.estimator == 'empirical':\n            self.estimation_func = partial(empirical_mmd_batch, kernel=self.base_kernel)\n        else:\n            raise ValueError('Unsupported estimator name', self.estimator)\n\n    # @property\n    # def lengthscale(self):\n    #     ls = None\n    #     if isinstance(self.base_kernel, gpyt.kernels.AdditiveKernel):\n    #         ls = torch.concat([_k.lengthscale for _k in self.base_kernel.kernels], dim=0)\n    #     else:\n    #         ls = self.base_kernel.lengthscale\n    #     return ls\n\n    @property\n    def is_stationary(self) -> bool:\n        \"\"\"\n        Kernel is stationary if base kernel is stationary.\n        \"\"\"\n        return self.base_kernel.is_stationary\n\n    def compute_distance_covariance_matrix(self, x1, x2, diag=False, **params):\n        chunk_size = params.get('chunk_size', self.chunk_size)\n        avg_dist_mat = None\n        for _ in range(self.estimation_trials):\n            dist_mat = estimate_mmd_in_chunks(x1, x2, self.estimation_func, chunk_size)\n            avg_dist_mat = dist_mat if avg_dist_mat is None else avg_dist_mat + dist_mat\n        avg_dist_mat = avg_dist_mat / self.estimation_trials\n        cov_mat = postprocess_mmd_rbf(avg_dist_mat.div(self.lengthscale ** 2.0))\n        return avg_dist_mat, cov_mat\n\n    def forward(self, x1, x2, diag=False, **params):\n        avg_dist_mat, cov_mat = self.compute_distance_covariance_matrix(x1, x2, diag, **params)\n        return cov_mat\n\nFile Path: AIRBO/model_utils/common_model_parts.py\nContent:\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom collections.abc import Iterable\nimport math\n\n\ndef compute_conv2d_output_shape(h_in, w_in, kernel_size, padding, stride, dilation=1):\n    \"\"\"\n    compute the output shape of the conv2D operation\n    \"\"\"\n    if isinstance(kernel_size, Iterable) is False:\n        kernel_size = (kernel_size, kernel_size)\n    if isinstance(padding, Iterable) is False:\n        padding = (padding, padding)\n    if isinstance(stride, Iterable) is False:\n        stride = (stride, stride)\n    if isinstance(dilation, Iterable) is False:\n        dilation = (dilation, dilation)\n\n    h_out = int(math.floor(\n        (h_in + 2 * padding[0] - dilation[0] * (kernel_size[0] - 1) - 1) / stride[0] + 1\n    ))\n\n    w_out = int(math.floor(\n        (w_in + 2 * padding[1] - dilation[1] * (kernel_size[1] - 1) - 1) / stride[1] + 1\n    ))\n\n    return (h_out, w_out)\n\n\ndef compute_conv2dTranspose_output_shape(h_in, w_in, kernel_size, padding, stride,\n                                         output_padding=0, dilation=1):\n    \"\"\"\n    compute the output shape of conv2DTranspose operation\n    \"\"\"\n    if isinstance(kernel_size, Iterable) is False:\n        kernel_size = (kernel_size, kernel_size)\n    if isinstance(padding, Iterable) is False:\n        padding = (padding, padding)\n    if isinstance(stride, Iterable) is False:\n        stride = (stride, stride)\n    if isinstance(dilation, Iterable) is False:\n        dilation = (dilation, dilation)\n    if isinstance(output_padding, Iterable) is False:\n        output_padding = (output_padding, output_padding)\n\n    h_out = (h_in - 1) * stride[0] - 2 * padding[0] + dilation[0] * (kernel_size[0] - 1) \\\n            + output_padding[0] + 1\n\n    w_out = (w_in - 1) * stride[1] - 2 * padding[1] + dilation[1] * (kernel_size[1] - 1) \\\n            + output_padding[1] + 1\n\n    return (h_out, w_out)\n\n\nclass MLP(nn.Module):\n    def __init__(self, input_dim, hidden_dims, output_dim, activation=\"leaky_relu\",\n                 act_in_last_layer=False, skip_connection=False, norm_method=None):\n        \"\"\"\n        A multi-layer perceptron\n        :param input_dim: input dimension\n        :param hidden_dims: a list of hidden dims\n        :param output_dim: output dimension\n        :param activation: which activation to use, can be \"relu\", \"tanh\", \"sigmoid\", \"leaky_relu\"\n        :param act_in_last_layer: whether to use activation in the last layer\n        \"\"\"\n        super(MLP, self).__init__()\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.hidden_dims = hidden_dims\n        self.layer_dims = [input_dim, *hidden_dims, output_dim]\n        self.layer_num = len(self.layer_dims) - 1\n        self.act_in_last_layer = act_in_last_layer\n        self.skip_connection = skip_connection\n        self.norm_method = norm_method\n\n        self.layers = []\n        for i in range(self.layer_num):\n            linear_layer = nn.Linear(self.layer_dims[i], self.layer_dims[i + 1])\n            if self.norm_method is not None and i != self.layer_num - 1:  # no need bn at the last layer:\n                if self.norm_method == 'spectral_norm':\n                    sn_l = nn.utils.spectral_norm(linear_layer)\n                    self.layers.append(sn_l)\n                elif self.norm_method == 'batch_norm':\n                    bn = nn.BatchNorm1d(self.layer_dims[i+1])\n                    self.layers.append(linear_layer)\n                    self.layers.append(bn)\n                else:\n                    raise ValueError(\"Invalid norm_method:\", self.norm_method)\n            else:\n                self.layers.append(linear_layer)\n\n            if i != self.layer_num - 1 or self.act_in_last_layer:\n                if activation == \"relu\":\n                    self.layers.append(torch.nn.ReLU())\n                elif activation == \"tanh\":\n                    self.layers.append(torch.nn.Tanh())\n                elif activation == \"sigmoid\":\n                    self.layers.append(torch.nn.Sigmoid())\n                elif activation == \"leaky_relu\":\n                    self.layers.append(torch.nn.LeakyReLU())\n                else:\n                    raise ValueError(\"Unsupported activation type: \", activation)\n        self.model = nn.Sequential(*self.layers)\n\n    def forward(self, x):\n        if self.skip_connection:\n            o = x + self.model(x)\n        else:\n            o = self.model(x)\n        return o\n\n\nclass CopyModule(nn.Module):\n    def __init__(self):\n        \"\"\"\n        A layer doing nothing but return the inputs\n        \"\"\"\n        super(CopyModule, self).__init__()\n\n    def forward(self, x):\n        return x\n\n\nclass MonoNet(torch.nn.Module):\n    def __init__(self, input_dim, hidden_dims, group_dims):\n        super(MonoNet, self).__init__()\n        self.input_dim = input_dim\n        self.hidden_dims = hidden_dims\n        self.group_dims = group_dims\n        self.n_group_dim = sum(group_dims)\n        self.model = MLP(self.input_dim, self.hidden_dims, self.n_group_dim)\n\n    def forward(self, x):\n        h = self.model(x)\n        group_outputs = []\n        d_ind = 0\n        for pgd in self.group_dims:\n            pgh = h[:, d_ind:d_ind + pgd]\n            pgo, _ = torch.max(pgh, dim=1, keepdim=True)\n            group_outputs.append(pgo)\n            d_ind += pgd\n        output, _ = torch.min(\n            torch.concat(group_outputs, dim=1),\n            dim=1, keepdim=True\n        )\n        return output\n\n    def __call__(self, *args, **kwargs):\n        for module in self.model.modules():\n            if hasattr(module, 'weight'):\n                w = module.weight.data\n                w = w.clamp(0.0, None)\n                module.weight.data = w\n        return self.forward(*args, **kwargs)\n\n\nclass ConvLayer(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=3,\n                 stride=1, padding=0, dilation=1,\n                 deconv=False, output_padding=0,\n                 use_batch_norm=True, activation=\"leaky_relu\"):\n        \"\"\"\n        A single conv2d/deconv2d layer + batch norm + activation\n        :param in_channels: input channel num\n        :param out_channels: output channel num\n        :param kernel_size: kernel size\n        :param stride: stride\n        :param padding: padding\n        :param dilation: dilation\n        :param deconv: whether to use DeconvTranspose2D\n        :param output_padding: only work if deconv is true\n        :param use_batch_norm: whether to use batch normalization\n        :param activation: which activation to use\n        \"\"\"\n        super(ConvLayer, self).__init__()\n\n        layer_list = []\n        conv_filter = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride,\n                                padding=padding, dilation=dilation) if deconv is False \\\n            else nn.ConvTranspose2d(in_channels, out_channels, kernel_size,\n                                    stride, padding, output_padding)\n        layer_list.append(conv_filter)\n\n        if use_batch_norm:\n            layer_list.append(nn.BatchNorm2d(num_features=out_channels))\n\n        act = None\n        if activation == \"leaky_relu\":\n            act = nn.LeakyReLU()\n        elif activation == \"relu\":\n            act = nn.ReLU()\n        elif activation == \"tanh\":\n            act = nn.Tanh()\n        elif activation == \"sigmoid\":\n            act = nn.Sigmoid()\n        else:\n            raise ValueError(\"Unsupported activation: {}\".format(activation))\n\n        if act is not None:\n            layer_list.append(act)\n\n        self.model = nn.Sequential(*layer_list)\n\n    def forward(self, x):\n        return self.model(x)\n\n\nclass MultiConvLayers(nn.Module):\n    def __init__(self, layer_configurations):\n        \"\"\"\n        Multiple conv2d/deconv2d layers\n        :param layer_configurations: a list of dict, each of which has key list= [in_channels,\n                                out_channels, kernel_size, stride, padding, dilation, is_deconv,\n                                output_padding, use_batch_norm, activation]\n        \"\"\"\n        super(MultiConvLayers, self).__init__()\n        self.layer_configurations = layer_configurations\n        layer_list = []\n        for i in range(len(layer_configurations)):\n            if i > 0 and layer_configurations[i][\"in_channels\"] \\\n                    != layer_configurations[i - 1][\"out_channels\"]:\n                raise ValueError(\"Unmatched in_channels with previous layer's out_channels\")\n\n            cfg = layer_configurations[i]\n            in_channels = cfg[\"in_channels\"]\n            out_channels = cfg[\"out_channels\"]\n            kernel_size = cfg[\"kernel_size\"]\n            stride = cfg.get(\"stride\", 1)\n            padding = cfg.get(\"padding\", 0)\n            dilation = cfg.get(\"dilation\", 1)\n            is_deconv = cfg.get(\"is_deconv\", False)\n            output_padding = cfg.get(\"output_padding\", None)\n            use_batch_norm = cfg.get(\"use_batch_norm\", True)\n            activation = cfg.get(\"activation\", \"relu\")\n\n            layer_list.append(ConvLayer(in_channels, out_channels, kernel_size,\n                                        stride, padding, dilation, is_deconv, output_padding,\n                                        use_batch_norm, activation))\n\n        self.model = nn.Sequential(*layer_list)\n\n    def forward(self, x):\n        return self.model(x)\n\n\nclass Encoder(nn.Module):\n    def __init__(self, sample_shape, latent_dim, conv_filter_cfgs,\n                 enc_fc_hidden_dims=(64, 32), enc_fc_out_dim=32,\n                 enc_mu_hdims=(16,), enc_logvar_hdims=(16,)):\n        super(Encoder, self).__init__()\n        self.sample_shape = sample_shape\n        self.latent_dim = latent_dim\n        self.conv_filter_cfgs = conv_filter_cfgs\n\n        self.shape_changes = [i for i in range(len(self.conv_filter_cfgs))\n                              if self.conv_filter_cfgs[i][\"stride\"] > 1]\n\n        # encoder\n        self.enc_conv_model = MultiConvLayers(conv_filter_cfgs) if len(\n            conv_filter_cfgs) > 0 else None\n        self.enc_flatten = nn.Flatten()\n        self.latent_shape = sample_shape // (2 ** len(self.shape_changes))\n        self.enc_fc_input_dim = self.latent_shape * self.latent_shape \\\n                                * (conv_filter_cfgs[-1][\"out_channels\"]\n                                   if len(conv_filter_cfgs) > 0 else 1)\n        self.enc_fc_model = MLP(self.enc_fc_input_dim, enc_fc_hidden_dims, enc_fc_out_dim)\n        self.enc_mu_model = MLP(enc_fc_out_dim, enc_mu_hdims, latent_dim)\n        self.enc_logvar_model = MLP(enc_fc_out_dim, enc_logvar_hdims, latent_dim)\n\n    def forward(self, x):\n        h = x\n        if self.enc_conv_model is not None:\n            h = self.enc_conv_model(h)\n        h = self.enc_flatten(h)\n        h = F.relu(self.enc_fc_model(h))\n        return self.enc_mu_model(h), self.enc_logvar_model(h)\n\n\nclass EncoderWithY(nn.Module):\n    def __init__(self, sample_shape, latent_dim, y_dim, conv_filter_cfgs,\n                 enc_fc_hidden_dims=(64, 32), enc_fc_out_dim=32,\n                 enc_mu_hdims=(16,), enc_logvar_hdims=(16,)):\n        super(EncoderWithY, self).__init__()\n        self.sample_shape = sample_shape\n        self.latent_dim = latent_dim\n        self.conv_filter_cfgs = conv_filter_cfgs\n        self.y_dim = y_dim\n\n        self.shape_changes = [i for i in range(len(self.conv_filter_cfgs))\n                              if self.conv_filter_cfgs[i][\"stride\"] > 1]\n\n        # encoder\n        self.enc_conv_model = MultiConvLayers(conv_filter_cfgs) \\\n            if len(conv_filter_cfgs) > 0 else None\n        self.enc_flatten = nn.Flatten()\n        self.latent_shape = (sample_shape[0] // (2 ** len(self.shape_changes)),\n                             sample_shape[1] // (2 ** len(self.shape_changes)))\n        self.enc_fc_input_dim = self.latent_shape[0] * self.latent_shape[1] \\\n                                * (conv_filter_cfgs[-1][\"out_channels\"]\n                                   if len(conv_filter_cfgs) > 0 else 1)\n        self.enc_fc_model = MLP(self.enc_fc_input_dim, enc_fc_hidden_dims, enc_fc_out_dim)\n        self.enc_mu_model = MLP(enc_fc_out_dim, enc_mu_hdims, latent_dim)\n        self.enc_logstd_model = MLP(enc_fc_out_dim, enc_logvar_hdims, latent_dim)\n        # self.enc_mu_model = MLP(enc_fc_out_dim+y_dim, enc_mu_hdims, latent_dim)\n        # self.enc_logvar_model = MLP(enc_fc_out_dim+y_dim, enc_logvar_hdims, latent_dim)\n\n    def forward(self, x, y):\n        h = x\n        if self.enc_conv_model is not None:\n            h = self.enc_conv_model(h)\n        h = self.enc_flatten(h)\n        h = F.leaky_relu(self.enc_fc_model(h))\n        # mu = self.enc_mu_model(torch.cat([h, y], dim=1))\n        mu = self.enc_mu_model(h)\n        # logvar = self.enc_logvar_model(torch.cat([h, y], dim=1))\n        logstd = self.enc_logstd_model(h)\n        return mu, logstd\n\n\nclass Decoder(nn.Module):\n    def __init__(self, latent_dim, latent_shape, latent_chns, deconv_filter_cfgs,\n                 dec_fc_hidden_dims=(64, 128), ):\n        super(Decoder, self).__init__()\n        self.latent_dim = latent_dim\n        self.latent_shape = latent_shape\n        self.latent_chns = latent_chns\n        self.deconv_filter_cfs = deconv_filter_cfgs\n\n        self.dec_fc_out_dim = self.latent_shape[0] * self.latent_shape[1] * self.latent_chns\n        self.dec_fc_model = MLP(latent_dim, dec_fc_hidden_dims, self.dec_fc_out_dim)\n        self.deconv_model = MultiConvLayers(deconv_filter_cfgs) \\\n            if len(deconv_filter_cfgs) > 0 else None\n\n    def forward(self, z):\n        h = F.leaky_relu(self.dec_fc_model(z))\n        if self.deconv_model is not None:\n            h = h.view(-1,\n                       self.latent_chns,\n                       self.latent_shape[0],\n                       self.latent_shape[1])\n            h = self.deconv_model(h)\n        return h\n\n\ndef reparameterize(mu, logvar):\n    std = torch.exp(0.5 * logvar)\n    eps = torch.randn_like(std)\n    return mu + eps * std\n\nFile Path: AIRBO/model_utils/input_transform.py\nContent:\nimport numpy as np\nimport torch\nimport botorch.models.transforms as trans\nfrom torch.nn import ModuleDict\nfrom collections import OrderedDict\nfrom typing import Tuple\nfrom torch import Tensor\nfrom botorch.exceptions.errors import BotorchTensorDimensionError\nfrom typing import Any, Callable, Dict, List, Optional, Union\n\n\ndef additional_std(X, std):\n    ret = torch.ones_like(X) * torch.tensor(std, dtype=X.dtype, device=X.device) \\\n        if isinstance(X, torch.Tensor) else np.ones_like(X) * std\n    return ret\n\n\ndef additional_xc_samples(X, n_sample, n_var, sampling_func, sampling_cfg={}, **kwargs):\n    # more samples around the raw\n    batch_shape = X.shape[:-1]\n    noises = sampling_func(\n        **{**sampling_cfg, 'x': X}, size=(*batch_shape, n_sample)\n    ).reshape(*batch_shape, n_sample, n_var)\n    if isinstance(X, torch.Tensor):\n        noises = torch.tensor(noises, dtype=X.dtype, device=X.device)\n    samples = X[..., None, :] + noises\n    return samples\n\n\ndef add_noise(X, sampling_func, sampling_cfg={}, **kwargs):\n    batch_shape = X.shape[:-1]\n    event_dim = X.shape[-1]\n    noise = sampling_func(**sampling_cfg, size=(*batch_shape, 1) ).reshape(*batch_shape, event_dim)\n    if isinstance(X, torch.Tensor):\n        noise = torch.tensor(noise, dtype=X.dtype, device=X.device)\n    return X + noise\n\n\nclass AdditionalFeatures(trans.input.AppendFeatures):\n    def transform(self, X):\n        expanded_features = self._f(X[..., self.indices], **self.fkwargs)\n        return X, expanded_features, torch.zeros(size=(*X.shape[:-1], 0), dtype=X.dtype,\n                                                 device=X.device)  # xc, xc_std/samples, xe\n\n\nclass TransformFeature(trans.input.AppendFeatures):\n    def transform(self, X):\n        transformed_features = self._f(X[..., self.indices], **self.fkwargs)\n        return transformed_features, torch.zeros(size=(*X.shape[:-1], 0), dtype=X.dtype,\n                                                 device=X.device)  # xc, xc_std/samples, xe\n\n\nclass SelectMultiInputs(trans.input.InputTransform, torch.nn.Module):\n    def __init__(\n            self,\n            sel_indices,\n            transform_on_train: bool = True,\n            transform_on_eval: bool = True,\n            transform_on_fantasize: bool = True,\n    ) -> None:\n        super().__init__()\n        self.transform_on_train = transform_on_train\n        self.transform_on_eval = transform_on_eval\n        self.transform_on_fantasize = transform_on_fantasize\n        self.register_buffer(\"sel_indices\", sel_indices)\n\n    def transform(self, X):\n        return tuple(X[i] for i in self.sel_indices)\n\n\nclass MultiInputTransform(trans.input.InputTransform, ModuleDict):\n    r\"\"\"An input transform representing the chaining of individual transforms.\"\"\"\n\n    def __init__(self, **transforms) -> None:\n        r\"\"\"Chaining of input transforms.\n\n        Args:\n            transforms: The transforms to chain. Internally, the names of the\n                kwargs are used as the keys for accessing the individual\n                transforms on the module.\n\n        \"\"\"\n        super().__init__(OrderedDict(transforms))\n        self.transform_on_train = False\n        self.transform_on_eval = False\n        self.transform_on_fantasize = False\n        for tf in transforms.values():\n            self.is_one_to_many |= tf.is_one_to_many\n            self.transform_on_train |= tf.transform_on_train\n            self.transform_on_eval |= tf.transform_on_eval\n            self.transform_on_fantasize |= tf.transform_on_fantasize\n\n    def transform(self, X):\n        ret = tuple([tf.transform(X[ind]) for ind, tf in enumerate(self.values())])\n        return ret\n\n    def untransform(self, X):\n        ret = tuple([tf.untransform(X[ind]) for ind, tf in enumerate(self.values())])\n        return ret\n\n    def equals(self, other: trans.input.InputTransform) -> bool:\n        return super().equals(other=other) and all(\n            t1.equals(t2) for t1, t2 in zip(self.values(), other.values())\n        )\n\n    def preprocess_transform(self, X):\n        ret = tuple([tf.preprocess_transform(X[ind]) for ind, tf in enumerate(self.values())])\n        return ret\n\n\nclass DummyTransform(trans.input.ReversibleInputTransform, torch.nn.Module):\n    def __init__(\n            self,\n            transform_on_train: bool = True,\n            transform_on_eval: bool = True,\n            transform_on_fantasize: bool = True,\n            reverse: bool = False,\n    ) -> None:\n        super().__init__()\n        self.transform_on_train = transform_on_train\n        self.transform_on_eval = transform_on_eval\n        self.transform_on_fantasize = transform_on_fantasize\n        self.reverse = reverse\n\n    def _transform(self, X: Tensor) -> Tensor:\n        return X\n\n    def _untransform(self, X: Tensor) -> Tensor:\n        return X\n\n\nclass ScaleTransform(trans.input.Normalize):\n    def _transform(self, X: Tensor) -> Tensor:\n        return X / self.coefficient\n\n    def _untransform(self, X: Tensor) -> Tensor:\n        return X * self.coefficient\n\nFile Path: AIRBO/model_utils/model_common_utils.py\nContent:\nimport model_utils.input_transform as tfx\n\nfrom sklearn.preprocessing import PowerTransformer, StandardScaler, MinMaxScaler\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom collections.abc import Iterable\nfrom typing import Callable\n\n\ndef safe_scaling(y, scaler=None, scaling_method=\"standardize\"):\n    \"\"\"\n    Apply scaling to the target\n    \"\"\"\n    try:\n        if scaler is None:  # fit a new scaler\n            if scaling_method == \"standardize\":\n                scaler = StandardScaler()\n                y_scaled = scaler.fit_transform(y)\n            elif scaling_method == \"power_transform\":\n                if y.min() <= 0:\n                    scaler = PowerTransformer(method='johnson', standardize=True)\n                    y_scaled = scaler.fit_transform(y)\n                else:\n                    scaler = PowerTransformer(method='box-cox', standardize=True)\n                    y_scaled = scaler.fit_transform(y)\n                    # try johnson\n                    if y_scaled.std() < 0.5:\n                        scaler = PowerTransformer(method='yeo-johnson', standardize=True)\n                        y_scaled = scaler.fit_transform(y)\n                    if y_scaled.std() < 0.5:\n                        raise RuntimeError('Power transformation failed')\n            elif scaling_method == \"min_max\":\n                scaler = MinMaxScaler()\n                y_scaled = scaler.fit_transform(y)\n            else:\n                raise ValueError(\"Unknown scaling method:\", scaling_method)\n        else:  # transform using the given scaler\n            y_scaled = scaler.transform(y)\n    except Exception as e:\n        print(f\"[Warn] scaling fails:\", e)\n        y_scaled = y.copy()\n        scaler = None\n    return y_scaled, scaler\n\n\ndef filter_nan(x, xe, y, keep_rule='any'):\n    assert x is None or np.isfinite(x).all()\n    assert xe is None or np.isfinite(xe).all()\n    assert torch.isfinite(y).any(), \"No valid data in the dataset\"\n\n    if keep_rule == 'any':\n        valid_id = torch.isfinite(y).any(dim=1)\n    else:\n        valid_id = torch.isfinite(y).all(dim=1)\n    x_filtered = x[valid_id] if x is not None else None\n    xe_filtered = xe[valid_id] if xe is not None else None\n    y_filtered = y[valid_id]\n    return x_filtered, xe_filtered, y_filtered\n\n\ndef get_gp_prediction(model, x, scaler, **kwargs):\n    pred = model.predict(x, **kwargs)\n    pred_lcb, pred_ucb = pred.confidence_region()\n    pred_mean = pred.mean\n    if scaler is not None:\n        mean = scaler.inverse_transform(pred_mean.detach().numpy().reshape(-1, 1)).flatten()\n        lcb = scaler.inverse_transform(pred_lcb.detach().numpy().reshape(-1, 1)).flatten()\n        ucb = scaler.inverse_transform(pred_ucb.detach().numpy().reshape(-1, 1)).flatten()\n    else:\n        mean = pred_mean.detach().numpy().flatten()\n        lcb = pred_lcb.detach().numpy().flatten()\n        ucb = pred_ucb.detach().numpy().flatten()\n    return pred, mean, lcb, ucb\n\n\nclass OneHotTransform(torch.nn.Module):\n    def __init__(self, num_uniqs):\n        super().__init__()\n        self.num_uniqs = num_uniqs\n\n    @property\n    def num_out(self) -> int:\n        return sum(self.num_uniqs)\n\n    def forward(self, xe):\n        return torch.cat(\n            [torch.nn.functional.one_hot(xe[:, i].long(), self.num_uniqs[i])\n             for i in range(xe.shape[1])], dim=1\n        ).float()\n\n\nclass EmbTransform(nn.Module):\n    def __init__(self, num_uniqs, **conf):\n        super().__init__()\n        self.emb_sizes = conf.get('emb_sizes')\n        if self.emb_sizes is None:\n            self.emb_sizes = [min(50, 1 + v // 2) for v in num_uniqs]\n\n        self.emb = nn.ModuleList([])\n        for num_uniq, emb_size in zip(num_uniqs, self.emb_sizes):\n            self.emb.append(nn.Embedding(num_uniq, emb_size))\n\n    @property\n    def num_out(self) -> int:\n        return sum(self.emb_sizes)\n\n    def forward(self, xe):\n        return torch.cat(\n            [self.emb[i](xe[:, i]).view(xe.shape[0], -1) for i in range(len(self.emb))], dim=1)\n\n\ndef get_model_prediction(model, Xc_te, support_decomposed_pred):\n    \"\"\"\n    Given a model and test data, return model predictions\n    :param model:\n    :param Xc_te:\n    :param support_decomposed_pred: whether to return a list of decomposed prediction\n    :return:\n    \"\"\"\n    preds = []\n    # full prediction\n    if support_decomposed_pred:\n        py_m0, ps2_m0 = model.predict(\n            torch.FloatTensor(Xc_te), torch.zeros(Xc_te.shape[0], 0), with_noise=True, mode=0\n        )\n    else:\n        py_m0, ps2_m0 = model.predict(torch.FloatTensor(Xc_te), torch.zeros(Xc_te.shape[0], 0))\n    ucb_m0 = py_m0 + (torch.sqrt(ps2_m0) * 2.0)\n    lcb_m0 = py_m0 - (torch.sqrt(ps2_m0) * 2.0)\n    preds.append(\n        (py_m0.detach().numpy(),\n         ps2_m0.detach().numpy(),\n         lcb_m0.detach().numpy(),\n         ucb_m0.detach().numpy(),\n         )\n    )\n\n    if support_decomposed_pred:\n        # mode 1 predict\n        py_m1, ps2_m1 = model.predict(\n            torch.FloatTensor(Xc_te), torch.zeros(Xc_te.shape[0], 0), with_noise=True, mode=1\n        )\n        ucb_m1 = py_m1 + (torch.sqrt(ps2_m1) * 2.0)\n        lcb_m1 = py_m1 - (torch.sqrt(ps2_m1) * 2.0)\n        preds.append(\n            (py_m1.detach().numpy(),\n             ps2_m1.detach().numpy(),\n             lcb_m1.detach().numpy(),\n             ucb_m1.detach().numpy(),\n             )\n        )\n\n        # mode 2 predict\n        py_m2, ps2_m2 = model.predict(\n            torch.FloatTensor(Xc_te), torch.zeros(Xc_te.shape[0], 0), with_noise=True, mode=2\n        )\n        ucb_m2 = py_m2 + (torch.sqrt(ps2_m2) * 2.0)\n        lcb_m2 = py_m2 - (torch.sqrt(ps2_m2) * 2.0)\n        preds.append(\n            (py_m2.detach().numpy(),\n             ps2_m2.detach().numpy(),\n             lcb_m2.detach().numpy(),\n             ucb_m2.detach().numpy(),\n             )\n        )\n    return preds\n\n\ndef get_kernel_lengthscale(kern_model):\n    # get lengthscale\n    ls = kern_model.lengthscale\n    km = kern_model\n    while ls is None and getattr(km, 'base_kernel', None) is not None:\n        km = km.base_kernel\n        ls = km.lengthscale\n    if ls is not None:\n        ls = ls.detach().cpu().numpy().flatten()\n\n    if isinstance(ls, Iterable) and len(ls) >= 1:\n        str_output = [f'{i:.3f}' for i in ls] if len(ls) > 1 else f'{ls[0]:.3f}'\n    else:\n        str_output = f'{ls}'\n\n    return ls, str_output\n\n\ndef get_kernel_output_scale(kernel):\n    oscale = kernel.outputscale.item() if hasattr(kernel, 'outputscale') else None\n    oscale_str = f'{oscale:.3f}' if oscale is not None else f'{oscale}'\n    return oscale, oscale_str\n\n\n\ndef prepare_data(input_type: str,\n                 n_var: int, raw_input_mean: [float, np.array], raw_input_std: [float, np.array],\n                 xc_sample_size: int, input_sampling_func: Callable,\n                 xc_raw: np.array, y: np.array,\n                 dtype: torch.dtype, device: torch.device,\n                 **data_cfg):\n    \"\"\"\n    Prepare the data acd. to the input type, transform them into tensor and put on right device\n    \"\"\"\n    if input_type == INPUT_TYPE_NOISED or input_type == INPUT_TYPE_MEAN:  # only the x_raw\n        x_ts = torch.tensor(xc_raw, dtype=dtype, device=device)\n    elif input_type == INPUT_TYPE_SAMPLES:  # observe some samples around x\n        tf_add_xsamp = tfx.AdditionalFeatures(\n            f=tfx.additional_xc_samples, transform_on_train=False,\n            fkwargs={'n_sample': xc_sample_size, 'n_var': n_var,\n                     'sampling_func': input_sampling_func}\n        )\n        x_ts = tf_add_xsamp.transform(torch.tensor(xc_raw, dtype=dtype, device=device))\n    elif input_type == INPUT_TYPE_DISTRIBUTION:\n        tf_add_std = tfx.AdditionalFeatures(f=tfx.additional_std, transform_on_train=False,\n                                            fkwargs={'std': raw_input_std})\n        x_ts = tf_add_std.transform(\n            torch.tensor(\n                xc_raw + raw_input_mean if raw_input_mean is not None else xc_raw,\n                dtype=dtype, device=device\n            )\n        )\n\n    else:\n        raise ValueError('Unknown input type:', input_type)\n\n    y_ts = torch.tensor(y.reshape(-1, 1), dtype=dtype, device=device)\n\n    return x_ts, y_ts\n\n\nNOISE_LB = 1e-4\nINPUT_TYPE_NOISED = 'exact_input'\nINPUT_TYPE_MEAN = 'mean_input'\nINPUT_TYPE_SAMPLES = 'sample_input'\nINPUT_TYPE_DISTRIBUTION = 'distribution_input'\n\nFile Path: AIRBO/model_utils/model_fit_utils.py\nContent:\nimport gpytorch as gpyt\nimport numpy as np\nimport torch\nfrom typing import Dict, List, Tuple\n\n\ndef fit_model_restarts(model_cls: torch.nn.Module, model_cfg: Dict,\n                       tr_Xc: torch.Tensor, tr_Xe: torch.Tensor, tr_y:torch.Tensor,\n                       fit_cfg: Dict) -> Tuple[torch.nn.Module, List]:\n    \"\"\"\n    Train a GP model with restarts (if NotPSD Error happens)\n    :param model_cls: model class\n    :param model_cfg: model configuration\n    :param tr_Xc: continuous inputs\n    :param tr_Xe: enumerate inputs\n    :param tr_y: training target\n    :param fit_cfg: fitting configurations\n    :return: a list of training history\n    \"\"\"\n    torch.autograd.set_detect_anomaly(True)\n    fit_restarts = fit_cfg.get('fit_restarts', 3)\n    results = []\n    while len(results) < fit_restarts:\n        try:\n            model_i = model_cls(**model_cfg)\n            tr_hist_i = model_i.fit(\n                tr_Xc,\n                tr_Xe,\n                tr_y.flatten(),\n                **fit_cfg\n            )\n        except gpyt.utils.errors.NotPSDError as e:\n            print('[WARN] model fit fails, try again:', e)\n            continue\n        results.append((model_i, tr_hist_i))\n\n    if len(results) > 0:\n        # find the optimal model\n        opt_ind = np.nanargmin([h[-1]['loss'] for (m, h) in results])\n        opt_model, opt_tr_hist = results[opt_ind]\n    else:\n        raise ValueError(f\"All the {fit_restarts} restarts fail!\")\n\n    return opt_model, opt_tr_hist\nFile Path: AIRBO/models/mmd_gp.py\nContent:\nfrom model_utils import input_transform as tfx\nfrom model_utils.common_model_parts import MLP, CopyModule\nfrom kernels.mmd_kernel import MMDKernel, additive_RQ_kernel\nfrom models.robust_gp import RobustGP\n\nimport gpytorch as gpyt\nfrom botorch.models import transforms as tf\nimport torch\nimport warnings\nfrom gpytorch import settings\nfrom gpytorch.utils.warnings import GPInputWarning\nfrom gpytorch.models.exact_prediction_strategies import prediction_strategy\nfrom gpytorch.models import ExactGP\nfrom gpytorch.distributions import MultivariateNormal\n\n\nclass MMDGP(RobustGP):\n    def __init__(self, train_inputs, train_targets, likelihood, num_inputs,\n                 input_transform=None, outcome_transform=None, additional_transform=None,\n                 hidden_dims=(4, 2), latent_dim=1, **kwargs):\n        super(MMDGP, self).__init__(train_inputs, train_targets, likelihood, num_inputs,\n                                    input_transform, outcome_transform,\n                                    additional_transform, **kwargs)\n\n        # latent mapping\n        self.norm_method = kwargs.get('latent_norm_method', None)\n        self.skip_conn = kwargs.get('skip_conn', False)\n        if hidden_dims is not None:\n            self.latent_dim = latent_dim\n            self.latent_mapping_module = MLP(\n                train_inputs[0].shape[-1], hidden_dims, latent_dim,\n                norm_method=self.norm_method\n            )\n        else:\n            self.latent_dim = train_inputs[0].shape[1] + train_inputs[0].shape[1]\n            self.latent_mapping_module = CopyModule()\n\n    def define_default_input_transform(self, **kwargs):\n        n_var = kwargs['n_var']\n        input_bounds = kwargs['input_bounds']\n        return tfx.MultiInputTransform(\n            tf1=tf.Normalize(d=n_var, bounds=input_bounds, transform_on_train=True),\n            tf2=tf.Normalize(d=n_var, bounds=input_bounds, transform_on_train=True),\n            tf3=tfx.DummyTransform(transform_on_train=True),\n        )\n\n    def define_covar_module(self, **kwargs):\n        xc_kern_params = {}\n        xc_lscale_constr = kwargs.get('xc_ls_constr', None)\n        if xc_lscale_constr is not None:\n            xc_kern_params['lengthscale_constraint'] = xc_lscale_constr\n\n        xc_mmd_inner_k = kwargs.get('base_kernel', None)\n        if xc_mmd_inner_k is None:\n            xc_mmd_inner_k = additive_RQ_kernel(\n                alphas=(0.2, 0.5, 1, 2, 5), ls=1.0, learnable_ls=False\n            )\n\n        estimator_name = kwargs.get('estimator_name', 'nystrom')\n        chunk_size = kwargs.get('chunk_size', 100)\n        sub_samp_size = kwargs.get('sub_samp_size', 100)\n        covar_module = gpyt.kernels.ScaleKernel(\n            MMDKernel(xc_mmd_inner_k, estimator=estimator_name, sub_samp_size=sub_samp_size,\n                      chunk_size=chunk_size)\n        )\n        return covar_module\n\n    def forward(self, xc_raw, xc_samp, xe):\n        # note that we assume X is already applied with additional transform\n        # input transform\n        X = (xc_raw, xc_samp, xe)\n        if self.training:\n            X = self.transform_inputs(X)\n        mean_x, covar = self.compute_mean_cov(X)\n        return gpyt.distributions.MultivariateNormal(mean_x, covar)\n\n    def compute_mean_cov(self, x, **kwargs):\n        \"\"\"\n        compute the mean and covariance matrix\n        :param x: a tuple of (Xc_raw, Xc_samples, Xe),\n        where Xc_raw is raw inputs, size= (M * D),\n        Xc_samples represents the nearby samples around the raw inputs, size= M * B * D tensor,\n        and Xe is the raw inputs of enumerate features, M * 0 tensor.\n        \"\"\"\n        Xc_raw, Xc_samples, Xe = x\n        Xe_trans = Xe\n        mean_x, covar = None, None\n        if Xc_raw.shape[-1] > 0 and Xc_samples.shape[-1] > 0:\n            _s = Xc_samples.shape[:-1]\n            D = Xc_samples.shape[-1]\n            proj_X_samples = self.latent_mapping_module(Xc_samples.view(-1, D)).view(*_s, -1)\n\n            # covar\n            with gpyt.settings.debug(True) and gpyt.settings.lazily_evaluate_kernels(False):\n                k_c = self.covar_module(proj_X_samples, **kwargs)\n            covar = k_c if covar is None else (k_c * covar)\n\n            # mean\n            Xc_samples_mean = Xc_samples.mean(dim=-2)\n            proj_X_raw = self.latent_mapping_module(Xc_samples_mean)\n            mean_x = self.mean_module(proj_X_raw)\n\n        if Xe.shape[-1] > 0:\n            Xe_trans = self.xe_transformer(Xe)\n            k_e = self.xe_covar_module(Xe_trans, **kwargs)\n            covar = k_e if covar is None else (k_e * covar)\n\n        return mean_x, covar\n\n    def define_additional_transform(self, **kwargs):\n        xc_sample_size = kwargs.get('xc_sample_size', 1000)\n        input_sampling_func = kwargs['input_sampling_func']\n        n_var = kwargs['n_var']\n        return tfx.AdditionalFeatures(f=tfx.additional_xc_samples, transform_on_train=False,\n                                      fkwargs={'n_sample': xc_sample_size, 'n_var': n_var,\n                                               'sampling_func': input_sampling_func})\n\n    def __call__(self, *args, **kwargs):\n        train_inputs = list(self.train_inputs) if self.train_inputs is not None else []\n        inputs = [i.unsqueeze(-1) if i.ndimension() == 1 else i for i in args]\n\n        # Training mode: optimizing\n        if self.training:\n            if self.train_inputs is None:\n                raise RuntimeError(\n                    \"train_inputs, train_targets cannot be None in training mode. \"\n                    \"Call .eval() for prior predictions, or call .set_train_data() to add training data.\"\n                )\n            if settings.debug.on():\n                if not all(torch.equal(train_input, input) for train_input, input in\n                           zip(train_inputs, inputs)):\n                    raise RuntimeError(\"You must train on the training inputs!\")\n            res = super().__call__(*inputs, **kwargs)\n            return res\n\n        # Prior mode\n        elif settings.prior_mode.on() or self.train_inputs is None or self.train_targets is None:\n            full_inputs = args\n            full_output = super(ExactGP, self).__call__(*full_inputs, **kwargs)\n            if settings.debug().on():\n                if not isinstance(full_output, MultivariateNormal):\n                    raise RuntimeError(\"ExactGP.forward must return a MultivariateNormal\")\n            return full_output\n\n        # Posterior mode\n        else:\n            if settings.debug.on():\n                if all(torch.equal(train_input, input) for train_input, input in\n                       zip(train_inputs, inputs)):\n                    warnings.warn(\n                        \"The input matches the stored training data. Did you forget to call model.train()?\",\n                        GPInputWarning,\n                    )\n\n            # Get the terms that only depend on training data\n            if self.prediction_strategy is None:\n                train_output = super(ExactGP, self).__call__(*train_inputs, **kwargs)\n\n                # Create the prediction strategy for\n                self.prediction_strategy = prediction_strategy(\n                    train_inputs=train_inputs,\n                    train_prior_dist=train_output,\n                    train_labels=self.train_targets,\n                    likelihood=self.likelihood,\n                )\n\n            # Concatenate the input to the training input\n            full_inputs = []\n            batch_shape = train_inputs[0].shape[:-2]\n            for i, (train_input, input) in enumerate(zip(train_inputs, inputs)):\n                # Make sure the batch shapes agree for training/test data\n                # special operations for MMD kernel\n                dim_2_concat = -3 if i == 1 else -2\n                batch_reserved_dim = -3 if i == 1 else -2\n                batch_shape = train_inputs[i].shape[:batch_reserved_dim]\n                if batch_shape != train_input.shape[:batch_reserved_dim]:\n                    batch_shape = torch.broadcast_shapes(batch_shape,\n                                                         train_input.shape[:batch_reserved_dim])\n                    train_input = train_input.expand(*batch_shape,\n                                                     *train_input.shape[batch_reserved_dim:])\n                if batch_shape != input.shape[:batch_reserved_dim]:\n                    batch_shape = torch.broadcast_shapes(batch_shape,\n                                                         input.shape[:batch_reserved_dim])\n                    train_input = train_input.expand(*batch_shape,\n                                                     *train_input.shape[batch_reserved_dim:])\n                    input = input.expand(*batch_shape, *input.shape[batch_reserved_dim:])\n                full_inputs.append(torch.cat([train_input, input], dim=dim_2_concat))\n\n            # Get the joint distribution for training/test data\n            full_output = super(ExactGP, self).__call__(*full_inputs, **kwargs)\n            if settings.debug().on():\n                if not isinstance(full_output, MultivariateNormal):\n                    raise RuntimeError(\"ExactGP.forward must return a MultivariateNormal\")\n            full_mean, full_covar = full_output.loc, full_output.lazy_covariance_matrix\n\n            # Determine the shape of the joint distribution\n            batch_shape = full_output.batch_shape\n            joint_shape = full_output.event_shape\n            tasks_shape = joint_shape[1:]  # For multitask learning\n            test_shape = torch.Size(\n                [joint_shape[0] - self.prediction_strategy.train_shape[0], *tasks_shape])\n\n            # Make the prediction\n            with settings.cg_tolerance(settings.eval_cg_tolerance.value()):\n                predictive_mean, predictive_covar = self.prediction_strategy.exact_prediction(\n                    full_mean, full_covar)\n\n            # Reshape predictive mean to match the appropriate event shape\n            predictive_mean = predictive_mean.view(*batch_shape, *test_shape).contiguous()\n            return full_output.__class__(predictive_mean, predictive_covar)\n\nFile Path: AIRBO/models/robust_gp.py\nContent:\n\"\"\"\nRobust model that is compatible with gpytorch and botorch.\n\"\"\"\nfrom model_utils import model_common_utils as mcu\n\nimport gpytorch as gpyt\nimport botorch as bot\nfrom botorch.models.gpytorch import GPyTorchModel\nfrom botorch.models import transforms as tf\nfrom typing import Any, Optional, Union\nimport torch\nfrom botorch.acquisition.objective import PosteriorTransform\nfrom botorch.models.utils import gpt_posterior_settings\nfrom botorch.posteriors.gpytorch import GPyTorchPosterior\nfrom torch import Tensor\nimport warnings\nfrom botorch.posteriors.transformed import TransformedPosterior  # pragma: no cover\n\n\n# %%\nclass RobustGP(gpyt.models.ExactGP, GPyTorchModel):\n    def __init__(self, train_inputs, train_targets, likelihood, num_inputs,\n                 input_transform=None, outcome_transform=None, additional_transform=None,\n                 **kwargs):\n        # Note:\n        # we first need to warm up the input and outcome transformers (say the params in\n        # the outcome standardize), then save the raw inputs and transformed outcome in the model\n        # via feeding them to the ExactGP.__init__().\n        # During the training, the input transform is applied before each forward() to\n        # normalize the train inputs.\n        # After that, if the model.eval() is called, the model will apply the input\n        # transformation on its saved train inputs and prepare to concatenate with the transformed\n        # test inputs.\n        # Moreover, the outcome is untransformed in the posterior() and the additional transform is\n        # only applied in the posterior\n        \"\"\"\n        A GP model that is compatible with BoTorch\n        :param train_inputs: a tensor or a tuple of tensors for training inputs\n        :param train_targets: a tensor of training targets\n        :param likelihood: likelihood to use\n        :param num_inputs: the required number of train inputs\n        :param input_transform: transformations to be applied on the inputs, e.g., Normalization\n        :param outcome_transform: transformation for the outcome, say standardization\n        :param additional_transform: additional transformation to apply on the inputs\n        :param kwargs: other model configurations.\n        \"\"\"\n        _in_tf = input_transform if input_transform is not None \\\n            else self.define_default_input_transform(**kwargs)\n        _out_tf = outcome_transform if outcome_transform is not None \\\n            else self.define_default_outcome_transform(m=1, **kwargs)\n\n        # Apply the transformers to warm up the params (say the params in standardization)\n        with torch.no_grad():\n            _in_tf.transform(train_inputs)\n        if _out_tf is not None:\n            # transform the outcome\n            train_targets, _ = _out_tf(train_targets)\n            train_targets = train_targets.squeeze(-1)\n\n        # save the raw inputs and transformed outcome\n        gpyt.models.ExactGP.__init__(self, train_inputs, train_targets, likelihood)\n\n        self.input_transform = _in_tf\n        self.outcome_transform = _out_tf\n        self.num_inputs = num_inputs\n        self.additional_transform = additional_transform if additional_transform is not None \\\n            else self.define_additional_transform(**kwargs)\n\n        self.mean_module = self.define_mean_module(**kwargs)\n        self.covar_module = self.define_covar_module(**kwargs)\n\n        self.kwargs = kwargs\n\n    def define_default_input_transform(self, **kwargs):\n        n_var = kwargs['n_var']\n        return tf.Normalize(d=n_var, transform_on_train=True)\n\n    def define_default_outcome_transform(self, m=1, **kwargs):\n        return tf.outcome.Standardize(m=m)\n\n    def define_mean_module(self, **kwargs):\n        return gpyt.means.ConstantMean()\n\n    def define_covar_module(self, **kwargs):\n        return gpyt.kernels.ScaleKernel(gpyt.kernels.MaternKernel())\n\n    def define_additional_transform(self, **kwargs):\n        return None\n\n    def transform_inputs_additional(self, X):\n        # apply the additional transform\n        n_inputs = 1 if isinstance(X, torch.Tensor) else len(X)\n        if n_inputs != self.num_inputs:\n            if self.additional_transform is not None:\n                X = self.additional_transform(X)\n            else:\n                raise ValueError(f\"Expect {self.num_inputs} inputs but found {len(X)} \"\n                                 f\"and no additional transformer.\")\n        return X\n\n    def forward(self, X):\n        # note that we assume X is already applied with additional transform\n        if self.training:\n            X = self.transform_inputs(X)\n        xc_raw = X\n        mean_x = self.mean_module(xc_raw)\n        covar_x = self.covar_module(xc_raw)\n        return gpyt.distributions.MultivariateNormal(mean_x, covar_x)\n\n    def posterior(\n            self, X,\n            observation_noise: Union[bool, Tensor] = False,\n            posterior_transform: Optional[PosteriorTransform] = None,\n            **kwargs: Any,\n    ) -> Union[GPyTorchPosterior, TransformedPosterior]:\n        r\"\"\"Computes the posterior over model outputs at the provided points.\n\n        Args:\n            X: A `(batch_shape) x q x d`-dim Tensor, where `d` is the dimension\n                of the feature space and `q` is the number of points considered\n                jointly.\n            observation_noise: If True, add the observation noise from the\n                likelihood to the posterior. If a Tensor, use it directly as the\n                observation noise (must be of shape `(batch_shape) x q`).\n            posterior_transform: An optional PosteriorTransform.\n\n        Returns:\n            A `GPyTorchPosterior` object, representing a batch of `b` joint\n            distributions over `q` points. Includes observation noise if\n            specified.\n        \"\"\"\n        self.eval()  # make sure model is in eval mode\n        # apply the additional transform\n        X = self.transform_inputs_additional(X)\n\n        # input transforms are applied at `posterior` in `eval` mode, and at\n        # `model.forward()` at the training time\n        X = self.transform_inputs(X)\n        with gpt_posterior_settings():\n            mvn = self(*X) if self.num_inputs > 1 else self(X)  # support multiple inputs\n            if observation_noise is not False:\n                if isinstance(observation_noise, torch.Tensor):\n                    # TODO: Make sure observation noise is transformed correctly\n                    self._validate_tensor_args(X=X, Y=observation_noise)\n                    if observation_noise.size(-1) == 1:\n                        observation_noise = observation_noise.squeeze(-1)\n                    mvn = self.likelihood(mvn, X, noise=observation_noise)\n                else:\n                    mvn = self.likelihood(mvn, X)\n        posterior = GPyTorchPosterior(distribution=mvn)\n        if hasattr(self, \"outcome_transform\"):\n            posterior = self.outcome_transform.untransform_posterior(posterior)\n        if posterior_transform is not None:\n            return posterior_transform(posterior)\n        return posterior\n\n    def _set_transformed_inputs(self) -> None:\n        r\"\"\"Update training inputs with transformed inputs.\"\"\"\n        if hasattr(self, \"input_transform\") and not self._has_transformed_inputs:\n            if hasattr(self, \"train_inputs\"):\n                self._original_train_inputs = self.train_inputs[0] if self.num_inputs == 1 \\\n                    else self.train_inputs  # support multiple inputs\n                with torch.no_grad():\n                    X_tf = self.input_transform.preprocess_transform(\n                        self.train_inputs[0] if self.num_inputs == 1 else self.train_inputs\n                    )\n                self.set_train_data(X_tf, strict=False)\n                self._has_transformed_inputs = True\n            else:\n                warnings.warn(\n                    \"Could not update `train_inputs` with transformed inputs \"\n                    f\"since {self.__class__.__name__} does not have a `train_inputs` \"\n                    \"attribute. Make sure that the `input_transform` is applied to \"\n                    \"both the train inputs and test inputs.\",\n                    RuntimeWarning,\n                )\n\n\nclass RobustGPModel():\n    def __init__(self, m_cls, num_inputs=1, **kwargs):\n        \"\"\"\n        A holistic model for easy use\n        :param m_cls: model class\n        :param num_inputs: number of inputs\n        :param kwargs: model configurations\n        \"\"\"\n        self.model = None\n        self.likelihood = None\n        self.mll = None\n        self.optimizer = None\n        self.m_cls = m_cls\n        self.num_inputs = num_inputs\n\n        # model config\n        self.noise_free = kwargs.get('noise_free', False)\n        self.dtype = kwargs.get('dtype', torch.float)\n        self.device = kwargs.get('device', torch.device('cpu'))\n        self.kwargs = kwargs\n\n    def define_optimizer(self, model: torch.nn.Module, **kwargs):\n        optimizer = None\n        if not kwargs.get('fit_with_scipy', False):\n            lr = kwargs.get('lr', 1e-2)\n            optimizer = torch.optim.Adam(\n                params=[{'params': model.parameters()}],\n                lr=lr\n            )\n\n        return optimizer\n\n    def define_likelihood(self, **kwargs):\n        noise_free = kwargs.get(\"noise_free\", False)\n        if noise_free:\n            lkh = gpyt.likelihoods.GaussianLikelihood()\n            lkh.noise = mcu.NOISE_LB\n            lkh.raw_noise.requires_grad = False\n        else:\n            noise_prior = kwargs.get(\"noise_prior\", None)\n            noise_constr = kwargs.get(\"noise_constr\", None)\n            lkh = gpyt.likelihoods.GaussianLikelihood(\n                noise_prior=noise_prior, noise_constraint=noise_constr\n            )\n\n        return lkh\n\n    def define_model(self, tr_x, tr_y, likelihood, **kwargs):\n        model = self.m_cls(tr_x, tr_y, likelihood, self.num_inputs, **kwargs)\n        return model\n\n    def define_mll(self, likelihood, model):\n        return gpyt.mlls.ExactMarginalLogLikelihood(likelihood, model)\n\n    def post_initialize(self, tr_x, tr_y):\n        self.likelihood = self.define_likelihood(**self.kwargs)\n        self.model = self.define_model(tr_x, tr_y, self.likelihood, **self.kwargs)\n        self.mll = self.define_mll(self.likelihood, self.model)\n\n        # dtype and device\n        self.likelihood = self.likelihood.to(self.dtype).to(self.device)\n        self.model = self.model.to(self.dtype).to(self.device)\n\n    def fit(self, **kwargs):\n        \"\"\"\n        Fit the model, retry if NotPSD Error happens\n        :param tr_x: training inputs\n        :param tr_y: training target\n        :param kwargs: fit configurations\n        :return:\n        \"\"\"\n        assert (self.model is not None and self.mll is not None and self.likelihood is not None)\n        tr_hist = None\n        success = False\n        max_retries = kwargs.get('max_retries', 5)\n        n_retry = 0\n        while not success:\n            try:\n                with bot.settings.debug(True):\n                    tr_hist = self.do_fit(**kwargs)\n                success = True\n            except Exception as e:\n                if n_retry < max_retries:\n                    n_retry += 1\n                    print(f\"[Warn] Model fit fails, retry cnt={n_retry}.\", e)\n                    success = False\n                else:\n                    raise e\n        return tr_hist\n\n    def do_fit(self, **kwargs):\n        self.model.train()\n        fit_with_scipy = kwargs.get('fit_with_scipy', True)\n        tr_history = None\n        if fit_with_scipy:\n            bot.fit_gpytorch_mll(self.mll)\n        else:\n            tr_history = []\n            epoch_num = kwargs.get('epoch_num', 100)\n            verbose = kwargs.get('verbose', True)\n            print_every = kwargs.get('print_every', 10)\n            if self.optimizer is None:\n                self.optimizer = self.define_optimizer(self.model, **kwargs)\n            for ep_i in range(epoch_num):\n                def closure():\n                    self.optimizer.zero_grad()\n                    output = self.model(self.model.train_inputs[0]) if self.num_inputs == 1 \\\n                        else self.model(*self.model.train_inputs)\n                    loss = -self.mll(output, self.model.train_targets)\n                    loss.backward()\n                    return loss\n\n                loss = self.optimizer.step(closure)\n                xc_ls, xc_ls_str = mcu.get_kernel_lengthscale(self.model.covar_module)\n                xc_os, xc_os_str = mcu.get_kernel_output_scale(self.model.covar_module)\n                y_noise = self.model.likelihood.noise.item()\n                tr_history.append((ep_i, loss.item(), xc_ls, xc_os, y_noise))\n                # print\n                if verbose and ((ep_i % print_every == 0) or (ep_i == epoch_num - 1)):\n                    print(f\"[epoch{ep_i}] loss={loss.item():.3f}, \"\n                          f\"xc_lscale={xc_ls_str}, \"\n                          f\"xc_oscale={xc_os_str}, \"\n                          f\"y_noise={y_noise:.3f}\")\n        return tr_history\n\n    def predict(self, X):\n        pred = self.model.posterior(X)\n        return pred.mean, pred.variance\n\n    def get_posterior(self, X):\n        return self.model.posterior(X)\n\nFile Path: AIRBO/models/ugp.py\nContent:\n\"\"\"\nuGP implementation\n\"\"\"\nfrom models.mmd_gp import MMDGP\nfrom kernels.kme_kernel import KMEKernel\nimport gpytorch as gpyt\n\n\nclass UGP(MMDGP):\n    def __init__(self, train_inputs, train_targets, likelihood, num_inputs,\n                 input_transform=None, outcome_transform=None, additional_transform=None,\n                 hidden_dims=(4, 2), latent_dim=1, **kwargs):\n        super(UGP, self).__init__(train_inputs, train_targets, likelihood, num_inputs,\n                                        input_transform, outcome_transform,\n                                        additional_transform, **kwargs)\n\n    def define_covar_module(self, **kwargs):\n        xc_kern_params = {}\n        xc_lscale_constr = kwargs.get('xc_ls_constr', None)\n        if xc_lscale_constr is not None:\n            xc_kern_params['lengthscale_constraint'] = xc_lscale_constr\n\n        xc_kme_inner_k = kwargs.get('base_kernel', None)\n        if xc_kme_inner_k is None:\n            xc_kme_inner_k = gpyt.kernels.RBFKernel(**xc_kern_params)\n\n        estimator_name = kwargs.get('estimator_name', 'integral')\n        chunk_size = kwargs.get('chunk_size', 100)\n        sub_samp_size = kwargs.get('sub_samp_size', 100)\n        covar_module = gpyt.kernels.ScaleKernel(\n            KMEKernel(xc_kme_inner_k, estimator=estimator_name, chunk_size=chunk_size)\n        )\n        return covar_module\nFile Path: AIRBO/models/uncertain_gp.py\nContent:\nfrom model_utils import input_transform as tfx\nfrom gpytorch.kernels import GaussianSymmetrizedKLKernel, RBFKernel\nfrom kernels.expected_rbf_kernel import ExpectedRBFKernel\nfrom models.robust_gp import RobustGP\n\nimport gpytorch as gpyt\nfrom botorch.models import transforms as tf\nimport torch\n\nKN_EXPECTED_RBF = 'ERBF'\nKN_SKL = \"SKL\"\nKN_RBF = 'rbf'\n\n\nclass UncertainGP(RobustGP):\n    def __init__(self, train_inputs, train_targets, likelihood, num_inputs,\n                 input_transform=None, outcome_transform=None, additional_transform=None,\n                 **kwargs):\n        super(UncertainGP, self).__init__(train_inputs, train_targets, likelihood, num_inputs,\n                                          input_transform, outcome_transform,\n                                          additional_transform, **kwargs)\n        self.kernel_name = kwargs.get('kernel_name', KN_SKL)\n\n    def define_covar_module(self, **kwargs):\n        n_var = kwargs['n_var']\n        self.kernel_name = kwargs['kernel_name']\n        xc_lscale_constr = kwargs.get('xc_lscale_constr', None)\n        xc_kern_params = {'ard_num_dims': n_var}\n        if xc_lscale_constr is not None:\n            xc_kern_params['lengthscale_constraint'] = xc_lscale_constr\n\n        if self.kernel_name == KN_EXPECTED_RBF:\n            xc_kern_params['ard_num_dims'] = None  # ERBF kernel cannot use ARD\n            xc_covar_module = gpyt.kernels.ScaleKernel(ExpectedRBFKernel(**xc_kern_params))\n        elif self.kernel_name == KN_SKL:\n            xc_kern_params['ard_num_dims'] = None  # SKL kernel cannot use ARD\n            xc_covar_module = gpyt.kernels.ScaleKernel(\n                GaussianSymmetrizedKLKernel(**xc_kern_params)\n            )\n        elif self.kernel_name == KN_RBF:\n            xc_covar_module = gpyt.kernels.ScaleKernel(RBFKernel(**xc_kern_params))\n        else:\n            raise ValueError(\"Unsupported kernel type:\", self.kernel_name)\n\n        return xc_covar_module\n\n    def define_default_input_transform(self, **kwargs):\n        n_var = kwargs['n_var']\n        input_bounds = kwargs['input_bounds']\n        return tfx.MultiInputTransform(\n            tf1=tf.Normalize(d=n_var, bounds=input_bounds, transform_on_train=True),\n            tf2=tfx.ScaleTransform(d=n_var, bounds=input_bounds, transform_on_train=True),\n            tf3=tfx.DummyTransform(transform_on_train=True),\n        )\n\n    def define_additional_transform(self, **kwargs):\n        raw_input_std = kwargs['raw_input_std']\n        return tfx.AdditionalFeatures(f=tfx.additional_std, transform_on_train=False,\n                                      fkwargs={'std': raw_input_std})\n\n    def compute_mean_cov(self, X, **kwargs):\n        xc_raw, xc_std, xe = X\n        mean_x, covar = None, None\n        if self.kernel_name == KN_SKL:\n            xc_input = torch.concat((xc_raw, xc_std.pow(2).log()), dim=-1)  # SKL takes log variance\n        elif self.kernel_name == KN_EXPECTED_RBF:\n            xc_input = torch.concat((xc_raw, xc_std.pow(2)), dim=-1)  # eRBF takes variance as input\n        else:\n            xc_input = xc_raw\n\n        # input K\n        mean_x = self.mean_module(xc_raw)\n        covar_x = self.covar_module(xc_input, **kwargs)\n        return mean_x, covar_x\n\n    def forward(self, xc_raw, xc_std, xe):\n        # note that we assume X is already applied with additional transform\n        # input transform\n        X = (xc_raw, xc_std, xe)\n        if self.training:\n            X = self.transform_inputs(X)\n        mean_x, covar = self.compute_mean_cov(X)\n        return gpyt.distributions.MultivariateNormal(mean_x, covar)\n\nFile Path: AIRBO/tests/compare_robust_optimization.py\nContent:\n\"\"\"\nTest the robust BO\n\"\"\"\nimport numpy as np\nimport torch\n\nRND_SEED = 42\nnp.random.seed(RND_SEED)\ntorch.manual_seed(RND_SEED)\n\nfrom model_utils.input_transform import additional_xc_samples, add_noise\nfrom problems.problem_factory import get_test_problem, TestFunctions\nfrom models.robust_gp import RobustGPModel, RobustGP\nfrom models.uncertain_gp import UncertainGP, KN_SKL, KN_EXPECTED_RBF\nfrom models.ugp import UGP\nfrom models.mmd_gp import MMDGP\nfrom utils.tb_logger import OptLogger\nimport utils.commons as cm\nfrom utils import visulaization as vis\nimport model_utils.model_common_utils as mcu\nfrom utils import input_uncertainty as iu\n\nimport matplotlib.pyplot as plt\nimport botorch as bot\nfrom tqdm.auto import tqdm\nfrom functools import partial\nfrom scipy import stats\nimport pandas as pd\nfrom typing import Callable, Dict\nimport os\nfrom torch import multiprocessing\nimport traceback\nimport copy\n\nmp = multiprocessing.get_context('spawn')\n\nOPTIMUM_BOV = 'best_observed_value'\nOPTIMUM_BE = 'best_expectation'\n\n\ndef run_BO(prob: TestFunctions,\n           minimization: bool,\n           num_expectation_eval,\n           raw_input_mean: [float, np.array],\n           raw_input_std: [float, np.array],\n           xc_sample_size: int, n_var: int,\n           input_sampling_func: Callable,\n           input_type: str,\n           init_xc_raw: np.array,\n           init_y,\n           init_expected_y: np.array,\n           model_name,\n           model_cls,\n           model_config: Dict,\n           fit_cfg: Dict,\n           pred_cfgs: Dict,\n           opt_cfg: Dict,\n           x_bounds: np.array,\n           n_iter: int,\n           batch_size: int,\n           n_restarts: int,\n           raw_samples: int,\n           converge_thr: float,\n           oracle_optimum,\n           oracle_opt_x,\n           plot_freq: int,\n           trial_name: str,\n           save_dir: str,\n           **kwargs):\n    \"\"\"\n    Run a BayesOpt\n    \"\"\"\n    print(f\"[pid{os.getpid()}] {trial_name} starts...\")\n    device = model_config['device']\n    cm.set_gmem_usage(device, reserved_gmem=6)\n    dtype = model_config['dtype']\n    max_acq_opt_retries = kwargs.get('max_acq_opt_retries', 3)\n    opt_hist = []\n    ret_que = kwargs.get(\"return_queue\", None)\n    # robust_end_pos = kwargs.get(\"robust_end_pos\", np.array([3, 3]).reshape(1, 2))\n    # env = prob.kwargs['env']\n    try:\n        # Initialize the opt logger\n        opt_logger = OptLogger(['E[y]', ], [1.0, ], constr_names=None, tag=trial_name,\n                               result_save_freq=1000, minimization=minimization)\n\n        # BO loop\n        tr_xc_raw = init_xc_raw.copy()\n        tr_y = init_y.copy()\n        optimum_method = opt_cfg.get('optimum_method', OPTIMUM_BOV)\n        opt_x, opt_py, opt_expected_y = find_optimum(\n            OPTIMUM_BOV, None, prob, n_var, minimization, tr_xc_raw, tr_y, None,\n            input_sampling_func, num_expectation_eval\n        )\n        opt_logger.tb_logger.add_scalar(\"opt_expected_y\", opt_expected_y, global_step=0)\n        opt_hist.append((0, init_xc_raw, init_y, init_expected_y, opt_x, opt_py, opt_expected_y))\n\n        obj2opt = bot.acquisition.objective.ScalarizedPosteriorTransform(\n            weights=torch.tensor([-1.0 if minimization else 1.0], dtype=dtype, device=device)\n        )\n        te_xc_raw, te_y = prob.mesh_coords, prob.mesh_vals\n        te_xc_raw_ts = torch.tensor(te_xc_raw, dtype=dtype, device=device) \\\n            if te_xc_raw is not None else None\n        for iter_i in tqdm(range(1, n_iter + 1), desc=f'{trial_name}',\n                           dynamic_ncols=True, leave=True):\n            # prepare train data\n            tr_x_ts, tr_y_ts = mcu.prepare_data(\n                input_type, n_var, raw_input_mean, raw_input_std, xc_sample_size,\n                input_sampling_func,\n                tr_xc_raw, tr_y, dtype=dtype, device=device\n            )\n\n            # build model\n            model = RobustGPModel(model_cls, **model_config)\n            model.post_initialize(tr_x_ts, tr_y_ts)\n\n            # fit\n            fit_cfg['fit_name'] = f\"{trial_name}/iter{iter_i}\"\n            model.fit(**fit_cfg)\n\n            # find current optimum\n            opt_x, opt_py, opt_expected_y = find_optimum(\n                optimum_method, model, prob, n_var, minimization,\n                tr_xc_raw, tr_y, tr_x_ts, input_sampling_func, num_expectation_eval\n            )\n            regret = oracle_optimum - opt_expected_y if oracle_optimum is not None \\\n                else opt_expected_y\n            dist_2_opt = np.linalg.norm(oracle_opt_x - opt_x) if oracle_opt_x is not None else None\n            opt_logger.tb_logger.add_scalar(\"opt_expected_y\", opt_expected_y, global_step=iter_i)\n            opt_logger.tb_logger.add_scalar(\"regret\", regret, global_step=iter_i)\n            if dist_2_opt is not None:\n                opt_logger.tb_logger.add_scalar(\"distance_2_opt\", dist_2_opt, global_step=iter_i)\n            tqdm.write(f'[{trial_name}] current opt E[y]: {opt_expected_y:.3f}')\n\n            # optimize\n            acq_opt_success = False\n            n_acq_opt_retry = 0\n            x_candidates, candidate_acq_vals, acq = None, None, None\n            while not acq_opt_success:\n                try:\n                    # acq = bot.acquisition.UpperConfidenceBound(\n                    #     model=model.model, beta=4.0, posterior_transform=obj2opt\n                    # )\n                    acq = bot.acquisition.analytic.ExpectedImprovement(\n                        model.model, best_f=min(tr_y) if minimization else max(tr_y),\n                        posterior_transform=obj2opt\n                    )\n                    x_candidates, candidate_acq_vals = bot.optim.optimize_acqf(\n                        acq_function=acq,\n                        bounds=x_bounds,\n                        q=batch_size,\n                        num_restarts=n_restarts,\n                        raw_samples=raw_samples,\n                        options={\"maxiter\": 500},\n                    )\n                    acq_opt_success = True\n                except Exception as e:\n                    acq_opt_success = False\n                    if n_acq_opt_retry >= max_acq_opt_retries:\n                        raise e\n                    else:\n                        print('[Warn] Acq. optimization fails, try again:', e)\n                        n_acq_opt_retry += 1\n\n            # observe new values\n            new_xc_raw = x_candidates.detach().cpu().numpy()\n            new_xc_n = add_noise(new_xc_raw, sampling_func=input_sampling_func)\n            new_y_n = prob.evaluate(new_xc_n)\n            new_expected_y = prob.evaluate(\n                additional_xc_samples(\n                    new_xc_raw, num_expectation_eval, n_var, input_sampling_func\n                ).reshape(-1, n_var),\n            ).reshape(new_xc_raw.shape[0], -1).mean(axis=-1)\n\n            # save to opt history\n            opt_hist.append((iter_i, new_xc_raw, new_y_n, new_expected_y,\n                             opt_x, opt_py, opt_expected_y))\n\n            # visualize model prediction & acq.\n            if n_var == 1 and (iter_i % plot_freq == 0 or iter_i == n_iter - 1):\n                # eval\n                with torch.no_grad():\n                    te_pred = model.get_posterior(te_xc_raw_ts)\n                    te_py = te_pred.mean.detach().cpu().numpy()\n                    te_lcb, te_ucb = te_pred.mvn.confidence_region()\n                    te_lcb, te_ucb = te_lcb.detach().cpu().numpy(), te_ucb.detach().cpu().numpy()\n                    te_acq_vals = acq(te_xc_raw_ts.unsqueeze(-2)).detach().cpu().numpy()\n                    xc_ls, xc_ls_str = mcu.get_kernel_lengthscale(model.model.covar_module)\n                    xc_os, xc_os_str = mcu.get_kernel_output_scale(model.model.covar_module)\n                    lkh_noise = model.likelihood.noise.item()\n\n                # plot\n                nrows, ncols, ax_scale = 2, 1, 1.5\n                fig = plt.figure(figsize=(4 * ncols * ax_scale, 3 * nrows * ax_scale))\n                axes = fig.subplots(nrows=nrows, ncols=ncols, squeeze=True, sharex=True)\n                # plot model pred\n                vis.plot_gp_predictions(\n                    [(model_name, te_py.flatten(), te_ucb.flatten(), te_lcb.flatten(),\n                      None, None), ],\n                    te_xc_raw, prob.mesh_coords, prob.mesh_vals,\n                    tr_xc_raw.flatten(), tr_y.flatten(),\n                    fig=fig, ax=axes[0]\n                )\n                vis.scatter(\n                    new_xc_raw.flatten(), new_y_n.flatten(), fig=fig, ax=axes[0],\n                    label='new_p', marker='^', color='lime'\n                )\n                vis.scatter(\n                    opt_x.flatten(), opt_py.flatten(), fig=fig, ax=axes[0],\n                    label='optimum', marker='*', color='magenta'\n                )\n                axes[0].axvline(opt_x.flatten()[0], color='magenta', ls='--')\n                axes[0].set_title(f\"{model_name}\\n\"\n                                  f\"xc_ls={xc_ls_str}, xc_os={xc_os_str}, noise={lkh_noise:.3f}\")\n                axes[0].legend()\n\n                # plot acq\n                axes[1].plot(te_xc_raw.flatten(), te_acq_vals.flatten(),\n                             label=f'{acq.__class__.__name__}')\n                vis.scatter(\n                    new_xc_raw.flatten(), candidate_acq_vals.detach().cpu().numpy().flatten(),\n                    fig=fig, ax=axes[1],\n                    label='new_p', marker='^', color='lime'\n                )\n                axes[1].legend()\n                fig.tight_layout()\n                opt_logger.tb_logger.add_figure('model_pred', fig, iter_i, close=True)\n\n            # concat to the training data\n            tr_xc_raw = np.concatenate([tr_xc_raw, new_xc_raw], axis=0)\n            tr_y = np.concatenate([tr_y, new_y_n], axis=0)\n\n    except Exception as e:\n        # except ImportError as e:\n        print(f\"[Error] Trial: {trial_name} fails, its opt_hist might be incomplete.\", e)\n        print(traceback.format_exc())\n    finally:\n        cm.serialize_obj(opt_hist, f\"{save_dir}{trial_name}.opt_hist\")\n\n    if ret_que is not None:\n        ret_que.put((trial_name, opt_hist))\n    else:\n        return trial_name, opt_hist\n\n\ndef find_optimum(optimum_method, model, prob, n_var, minimization,\n                 tr_xc_raw, tr_y, tr_x_ts, input_sampling_func, num_expectation_eval):\n    opt_x, opt_py, opt_expected_y = None, None, None\n    with torch.no_grad():\n        if optimum_method == OPTIMUM_BOV:\n            opt_ind = np.argmin(tr_y) if minimization else np.argmax(tr_y)\n            opt_x = tr_xc_raw[opt_ind:opt_ind + 1]\n            if model is not None and tr_x_ts is not None:\n                opt_py = model.get_posterior(tr_x_ts[opt_ind:opt_ind + 1]) \\\n                    .mean.detach().cpu().numpy()\n        elif optimum_method == OPTIMUM_BE:\n            if model is None or tr_x_ts is None:\n                raise ValueError(\"model and tr_x_ts should NOT be none.\")\n            observed_py = model.get_posterior(tr_x_ts).mean.detach().cpu().numpy()\n            opt_ind = np.argmin(observed_py) if minimization else np.argmax(observed_py)\n            opt_x = tr_xc_raw[opt_ind: opt_ind + 1]\n            opt_py = observed_py[opt_ind: opt_ind + 1]\n        else:\n            raise ValueError('Unsupported optimum method:', optimum_method)\n\n    opt_expected_y = prob.evaluate(\n        additional_xc_samples(\n            opt_x.reshape(-1, n_var), num_expectation_eval, n_var,\n            input_sampling_func\n        ).reshape(-1, n_var),\n    ).flatten().mean()\n    return opt_x, opt_py, opt_expected_y\n\nFile Path: AIRBO/tests/compare_optimization_in_push_world.py\nContent:\n\"\"\"\nTest the robust BO in the push world\n\"\"\"\nRND_SEED = 1910\nimport numpy as np\nimport torch\n\nnp.random.seed(RND_SEED)\ntorch.manual_seed(RND_SEED)\nfrom models.ugp import UGP\nfrom model_utils.input_transform import additional_xc_samples, add_noise, AdditionalFeatures\nfrom problems.problem_factory import get_test_problem, TestFunctions\nfrom problems.robot_pushing import push_env as pe\nfrom models.robust_gp import RobustGPModel, RobustGP\nfrom models.mmd_gp import MMDGP\nfrom models.uncertain_gp import UncertainGP, KN_SKL, KN_EXPECTED_RBF\nfrom utils.tb_logger import OptLogger\nimport utils.commons as cm\nfrom utils import visulaization as vis\nimport model_utils.model_common_utils as mcu\nfrom utils import input_uncertainty as iu\n\nimport matplotlib.pyplot as plt\nimport botorch as bot\nfrom tqdm.auto import tqdm\nimport pandas as pd\nfrom typing import Callable, Dict\nimport os\nfrom torch import multiprocessing\nimport traceback\nimport copy\n\nmp = multiprocessing.get_context('spawn')\n\nOPTIMUM_BOV = 'best_observed_value'\nOPTIMUM_BE = 'best_expectation'\n\n\ndef run_BO(prob: TestFunctions,\n           minimization: bool,\n           num_expectation_eval,\n           raw_input_mean: [float, np.array],\n           raw_input_std: [float, np.array],\n           xc_sample_size: int, n_var: int,\n           input_sampling_func: Callable,\n           input_type: str,\n           init_xc_raw: np.array,\n           init_y,\n           init_expected_y: np.array,\n           model_name,\n           model_cls,\n           model_config: Dict,\n           fit_cfg: Dict,\n           pred_cfgs: Dict,\n           opt_cfg: Dict,\n           x_bounds: np.array,\n           n_iter: int,\n           batch_size: int,\n           n_restarts: int,\n           raw_samples: int,\n           converge_thr: float,\n           oracle_optimum,\n           oracle_opt_x,\n           plot_freq: int,\n           trial_name: str,\n           save_dir: str,\n           **kwargs):\n    \"\"\"\n    Run a BayesOpt\n    \"\"\"\n    print(f\"[pid{os.getpid()}] {trial_name} starts...\")\n    device = model_config['device']\n    cm.set_gmem_usage(device, reserved_gmem=6)\n    dtype = model_config['dtype']\n    max_acq_opt_retries = kwargs.get('max_acq_opt_retries', 3)\n    opt_hist = []\n    ret_que = kwargs.get(\"return_queue\", None)\n    robust_end_pos = kwargs.get(\"robust_end_pos\", np.array([3, 3]).reshape(1, 2))\n    env = prob.kwargs['env']\n    try:\n        # Initialize the opt logger\n        opt_logger = OptLogger(['E[y]', ], [1.0, ], constr_names=None, tag=trial_name,\n                               result_save_freq=1000, minimization=minimization,\n                               log_root_dir='./tb_logs/')\n\n        # BO loop\n        tr_xc_raw = init_xc_raw.copy()\n        tr_y = init_y.copy()\n        optimum_method = opt_cfg.get('optimum_method', OPTIMUM_BOV)\n        opt_x, opt_py, opt_expected_y = find_optimum(\n            OPTIMUM_BOV, None, prob, n_var, minimization, tr_xc_raw, tr_y, None,\n            input_sampling_func, num_expectation_eval\n        )\n        opt_logger.tb_logger.add_scalar(\"opt_expected_y\", opt_expected_y, global_step=0)\n        opt_hist.append((0, init_xc_raw, init_y, init_expected_y, opt_x, opt_py, opt_expected_y))\n\n        obj2opt = bot.acquisition.objective.ScalarizedPosteriorTransform(\n            weights=torch.tensor([-1.0 if minimization else 1.0], dtype=dtype, device=device)\n        )\n        te_xc_raw, te_y = prob.mesh_coords, prob.mesh_vals\n        te_xc_raw_ts = torch.tensor(te_xc_raw, dtype=dtype, device=device) \\\n            if te_xc_raw is not None else None\n        for iter_i in tqdm(range(1, n_iter + 1), desc=f'{trial_name}',\n                           dynamic_ncols=True, leave=True):\n            # prepare train data\n            tr_x_ts, tr_y_ts = mcu.prepare_data(\n                input_type, n_var, raw_input_mean, raw_input_std, xc_sample_size,\n                input_sampling_func,\n                tr_xc_raw, tr_y, dtype=dtype, device=device\n            )\n\n            # build model\n            model = RobustGPModel(model_cls, **model_config)\n            model.post_initialize(tr_x_ts, tr_y_ts)\n\n            # fit\n            fit_cfg['fit_name'] = f\"{trial_name}/iter{iter_i}\"\n            model.fit(**fit_cfg)\n\n            # find current optimum\n            opt_x, opt_py, opt_expected_y = find_optimum(\n                optimum_method, model, prob, n_var, minimization,\n                tr_xc_raw, tr_y, tr_x_ts, input_sampling_func, num_expectation_eval\n            )\n\n            # log\n            goals, opt_end_pos, dist = env.evaluate_ex(*opt_x)\n            dist = float(dist)\n            regret = dist\n            dist_2_opt = np.linalg.norm(robust_end_pos - opt_end_pos)\n            opt_logger.tb_logger.add_scalar(\"opt_expected_y\", opt_expected_y, global_step=iter_i)\n            opt_logger.tb_logger.add_scalar(\"regret\", regret, global_step=iter_i)\n            if dist_2_opt is not None:\n                opt_logger.tb_logger.add_scalar(\"distance_2_opt\", dist_2_opt, global_step=iter_i)\n            tqdm.write(f'[{trial_name}] current opt E[y]: {opt_expected_y:.3f}')\n            rx, ry, rt = opt_x.flatten()[:3]\n            fig, ax = pe.plot_push_world(goals, opt_end_pos, dist, rx, ry, rt,\n                                         x_range=(-6, 6), y_range=(-6, 6))\n            opt_logger.tb_logger.add_figure(\"push_world\", fig, global_step=iter_i, close=True)\n\n            # optimize\n            acq_opt_success = False\n            n_acq_opt_retry = 0\n            x_candidates, candidate_acq_vals, acq = None, None, None\n            while not acq_opt_success:\n                try:\n                    # acq = bot.acquisition.UpperConfidenceBound(\n                    #     model=model.model, beta=4.0, posterior_transform=obj2opt\n                    # )\n                    acq = bot.acquisition.analytic.ExpectedImprovement(\n                        model.model, best_f=min(tr_y) if minimization else max(tr_y),\n                        posterior_transform=obj2opt\n                    )\n                    x_candidates, candidate_acq_vals = bot.optim.optimize_acqf(\n                        acq_function=acq,\n                        bounds=x_bounds,\n                        q=batch_size,\n                        num_restarts=n_restarts,\n                        raw_samples=raw_samples,\n                        options={\"maxiter\": 500},\n                    )\n                    acq_opt_success = True\n                except Exception as e:\n                    acq_opt_success = False\n                    if n_acq_opt_retry >= max_acq_opt_retries:\n                        raise e\n                    else:\n                        print('[Warn] Acq. optimization fails, try again:', e)\n                        n_acq_opt_retry += 1\n\n            # observe new values\n            new_xc_raw = x_candidates.detach().cpu().numpy()\n            new_xc_n = add_noise(new_xc_raw, sampling_func=input_sampling_func)\n            new_y_n = prob.evaluate(new_xc_n)\n            new_expected_y = prob.evaluate(\n                additional_xc_samples(\n                    new_xc_raw, num_expectation_eval, n_var, input_sampling_func\n                ).reshape(-1, n_var),\n            ).reshape(new_xc_raw.shape[0], -1).mean(axis=-1)\n\n            # save to opt history\n            opt_hist.append((iter_i, new_xc_raw, new_y_n, new_expected_y,\n                             opt_x, opt_py, opt_expected_y, opt_end_pos))\n\n            # visualize model prediction & acq.\n            if n_var == 1 and (iter_i % plot_freq == 0 or iter_i == n_iter - 1):\n                # eval\n                with torch.no_grad():\n                    te_pred = model.get_posterior(te_xc_raw_ts)\n                    te_py = te_pred.mean.detach().cpu().numpy()\n                    te_lcb, te_ucb = te_pred.mvn.confidence_region()\n                    te_lcb, te_ucb = te_lcb.detach().cpu().numpy(), te_ucb.detach().cpu().numpy()\n                    te_acq_vals = acq(te_xc_raw_ts.unsqueeze(-2)).detach().cpu().numpy()\n                    xc_ls, xc_ls_str = mcu.get_kernel_lengthscale(model.model.covar_module)\n                    xc_os, xc_os_str = mcu.get_kernel_output_scale(model.model.covar_module)\n                    lkh_noise = model.likelihood.noise.item()\n\n                # plot\n                nrows, ncols, ax_scale = 2, 1, 1.5\n                fig = plt.figure(figsize=(4 * ncols * ax_scale, 3 * nrows * ax_scale))\n                axes = fig.subplots(nrows=nrows, ncols=ncols, squeeze=True, sharex=True)\n                # plot model pred\n                vis.plot_gp_predictions(\n                    [(model_name, te_py.flatten(), te_ucb.flatten(), te_lcb.flatten(),\n                      None, None), ],\n                    te_xc_raw, prob.mesh_coords, prob.mesh_vals,\n                    tr_xc_raw.flatten(), tr_y.flatten(),\n                    fig=fig, ax=axes[0]\n                )\n                vis.scatter(\n                    new_xc_raw.flatten(), new_y_n.flatten(), fig=fig, ax=axes[0],\n                    label='new_p', marker='^', color='lime'\n                )\n                vis.scatter(\n                    opt_x.flatten(), opt_py.flatten(), fig=fig, ax=axes[0],\n                    label='optimum', marker='*', color='magenta'\n                )\n                axes[0].axvline(opt_x.flatten()[0], color='magenta', ls='--')\n                axes[0].set_title(f\"{model_name}\\n\"\n                                  f\"xc_ls={xc_ls_str}, xc_os={xc_os_str}, noise={lkh_noise:.3f}\")\n                axes[0].legend()\n\n                # plot acq\n                axes[1].plot(te_xc_raw.flatten(), te_acq_vals.flatten(),\n                             label=f'{acq.__class__.__name__}')\n                vis.scatter(\n                    new_xc_raw.flatten(), candidate_acq_vals.detach().cpu().numpy().flatten(),\n                    fig=fig, ax=axes[1],\n                    label='new_p', marker='^', color='lime'\n                )\n                axes[1].legend()\n                fig.tight_layout()\n                opt_logger.tb_logger.add_figure('model_pred', fig, iter_i, close=True)\n\n            # concat to the training data\n            tr_xc_raw = np.concatenate([tr_xc_raw, new_xc_raw], axis=0)\n            tr_y = np.concatenate([tr_y, new_y_n], axis=0)\n\n    except Exception as e:\n        # except ImportError as e:\n        print(f\"[Error] Trial: {trial_name} fails, its opt_hist might be incomplete.\", e)\n        print(traceback.format_exc())\n    finally:\n        cm.serialize_obj(opt_hist, f\"{save_dir}{trial_name}.opt_hist\")\n\n    if ret_que is not None:\n        ret_que.put((trial_name, opt_hist))\n    else:\n        return trial_name, opt_hist\n\n\ndef find_optimum(optimum_method, model, prob, n_var, minimization,\n                 tr_xc_raw, tr_y, tr_x_ts, input_sampling_func, num_expectation_eval):\n    opt_x, opt_py, opt_expected_y = None, None, None\n    with torch.no_grad():\n        if optimum_method == OPTIMUM_BOV:\n            opt_ind = np.argmin(tr_y) if minimization else np.argmax(tr_y)\n            opt_x = tr_xc_raw[opt_ind:opt_ind + 1]\n            if model is not None and tr_x_ts is not None:\n                opt_py = model.get_posterior(tr_x_ts[opt_ind:opt_ind + 1]) \\\n                    .mean.detach().cpu().numpy()\n        elif optimum_method == OPTIMUM_BE:\n            if model is None or tr_x_ts is None:\n                raise ValueError(\"model and tr_x_ts should NOT be none.\")\n            observed_py = model.get_posterior(tr_x_ts).mean.detach().cpu().numpy()\n            opt_ind = np.argmin(observed_py) if minimization else np.argmax(observed_py)\n            opt_x = tr_xc_raw[opt_ind: opt_ind + 1]\n            opt_py = observed_py[opt_ind: opt_ind + 1]\n        else:\n            raise ValueError('Unsupported optimum method:', optimum_method)\n\n    opt_expected_y = prob.evaluate(\n        additional_xc_samples(\n            opt_x.reshape(-1, n_var), num_expectation_eval, n_var,\n            input_sampling_func\n        ).reshape(-1, n_var),\n    ).flatten().mean()\n    return opt_x, opt_py, opt_expected_y",
        "experimental_info": "General Setup:\n- `debug_mode`: False\n- `use_gpu`: Varies based on available resources, `torch.cuda.is_available()` check.\n- `use_multiprocess`: True (parallel execution of trials).\n- `use_double_precision`: False (default `torch.float32`).\n- `n_trial`: 10 (number of trials for each optimization setting).\n- `n_iter`: 100 (number of optimization iterations).\n- `xc_sample_size`: 160 (sample size for each uncertain input sample, for MMD/KME).\n- `sub_samp_size`: 10 (sub-sampling size for Nystrom MMD estimator).\n- `naive_xc_sample_size`: derived as `int((xc_sample_size * sub_samp_size) ** 0.5)`.\n- `init_samp_num`: 10 (number of initial samples).\n- `save_root_dir`: \"./results/\".\n\nProblem Setup (from `compare_robust_optimization.py`):\n- `raw_fname`: 'RKHS-S' (default, can be 'BumpedBowlHD', 'RKHS-S', 'CustomK').\n- `n_var`: 1 (default, input dimension of test function).\n- `minimization`: True (problem is minimization).\n- `num_expectation_eval`: 500 (number of evaluations for true expectation).\n- `input_uncertainty_type`: 'step_chi2' (default, can be 'norm', 'gmm', 'beta', 'chi2', 'varying_beta', 'uniform', 'concated_circular').\n- `raw_input_std`: 0.01 (standard deviation of input uncertainty).\n- The true optimum (`oracle_robust_optimum`, `oracle_robust_opt_x`) and convergence thresholds (`converge_thr`) are computed based on the chosen function and input distribution.\n\nProblem Setup (from `compare_optimization_in_push_world.py`):\n- `raw_fname`: \"TripleGoalsP3\".\n- `robust_end_pos`: `np.array([5.3, 3.0]).reshape(1, 2)`. This is the target end position for the robot pushing task.\n- `unknown_optimum`: True.\n- `n_var`: 3.\n- `minimization`: True.\n- `num_expectation_eval`: 700.\n- `input_distrib`: `iu.GMMInputDistribution` with 2 components, specific means and tied covariance (including `min_cov = 1e-6`).\n- Initial samples (`init_xc_raw`, `init_y_n`, `init_expected_y`) are generated by random uniform sampling within input bounds, adding noise, and evaluating expected values.\n\nModel Configuration (`default_model_cfg`):\n- `noise_free`: False.\n- `n_var`: Problem's `n_var`.\n- `input_bounds`: Problem's `x_bounds`.\n- `num_inputs`: 1 (default for `RobustGP`), 3 for MMDGP/UGP/UncertainGP.\n- `dtype`: `torch.double` if `use_double_precision`, else `torch.float32`.\n- `device`: `torch.device('cuda:0')` if `use_gpu`, else `torch.device('cpu')`.\n- `input_type`: 'exact_input' (default for `RobustGP`), 'sample_input' for MMDGP/UGP, 'distribution_input' for UncertainGP.\n- `raw_input_std`: Problem's `raw_input_std`.\n- `raw_input_mean`: Problem's `raw_input_mean`.\n\nModel Training Configuration (`default_fit_cfg`):\n- `epoch_num`: 150 (5 if `debug_mode`).\n- `lr`: 5e-2.\n- `fit_with_scipy`: False.\n- `dtype`, `device`: Same as model config.\n- `print_every`: 10 (logging frequency).\n\nOptimization Settings:\n- `batch_size`: 1 (number of new points acquired in each iteration).\n- `n_restarts`: 10 (number of restarts for acquisition function optimization).\n- `raw_samples`: 512 (number of raw samples for acquisition function optimization).\n- `outcome_plot_freq` (or `pred_plot_freq`): 10 if `n_var == 1`, else 9e99 (visualization frequency).\n\nAcquisition Function:\n- `bot.acquisition.analytic.ExpectedImprovement` is used, optimized using `bot.optim.optimize_acqf`.\n\nOptimum Finding:\n- `find_optimum` function used to identify the current best configuration.\n- `optimum_method`: 'best_observed_value' or 'best_expectation' depending on the model.\n\nModel Candidates Compared:\n- **MMDGP-nystrom**: `MMDGP` with `nystrom` estimator, `input_type='sample_input'`, `num_inputs=3`, `xc_sample_size=160`, `sub_samp_size=10`.\n- **uGP**: `UGP` with `integral` estimator, `input_type='sample_input'`, `num_inputs=3`, `xc_sample_size` = `naive_xc_sample_size`.\n- **GP**: `RobustGP` (standard GP).\n- **skl**: `UncertainGP` with `kernel_name='SKL'`, `input_type='distribution_input'`, `num_inputs=3`.\n- **ERBF**: `UncertainGP` with `kernel_name='ERBF'`, `input_type='distribution_input'`, `num_inputs=3`."
      }
    },
    {
      "title": "Few-Shot Bayesian Optimization with Deep Kernel Surrogates",
      "abstract": "Hyperparameter optimization (HPO) is a central pillar in the automation of\nmachine learning solutions and is mainly performed via Bayesian optimization,\nwhere a parametric surrogate is learned to approximate the black box response\nfunction (e.g. validation error). Unfortunately, evaluating the response\nfunction is computationally intensive. As a remedy, earlier work emphasizes the\nneed for transfer learning surrogates which learn to optimize hyperparameters\nfor an algorithm from other tasks. In contrast to previous work, we propose to\nrethink HPO as a few-shot learning problem in which we train a shared deep\nsurrogate model to quickly adapt (with few response evaluations) to the\nresponse function of a new task. We propose the use of a deep kernel network\nfor a Gaussian process surrogate that is meta-learned in an end-to-end fashion\nin order to jointly approximate the response functions of a collection of\ntraining data sets. As a result, the novel few-shot optimization of our deep\nkernel surrogate leads to new state-of-the-art results at HPO compared to\nseveral recent methods on diverse metadata sets.",
      "full_text": "Published as a conference paper at ICLR 2021 FEW-SHOT BAYESIAN OPTIMIZATION WITH DEEP KERNEL SURROGATES Martin Wistuba IBM Research Dublin, Ireland martin.wistuba@ibm.com Josif Grabocka University of Freiburg Freiburg, Germany grabocka@cs.uni-freiburg.de ABSTRACT Hyperparameter optimization (HPO) is a central pillar in the automation of ma- chine learning solutions and is mainly performed via Bayesian optimization, where a parametric surrogate is learned to approximate the black box response function (e.g. validation error). Unfortunately, evaluating the response function is computationally intensive. As a remedy, earlier work emphasizes the need for transfer learning surrogates which learn to optimize hyperparameters for an algo- rithm from other tasks. In contrast to previous work, we propose to rethink HPO as a few-shot learning problem in which we train a shared deep surrogate model to quickly adapt (with few response evaluations) to the response function of a new task. We propose the use of a deep kernel network for a Gaussian process surro- gate that is meta-learned in an end-to-end fashion in order to jointly approximate the response functions of a collection of training data sets. As a result, the novel few-shot optimization of our deep kernel surrogate leads to new state-of-the-art results at HPO compared to several recent methods on diverse metadata sets. 1 I NTRODUCTION Many machine learning models have very sensitive hyperparameters that must be carefully tuned for efﬁcient use. Unfortunately, ﬁnding the right setting is a tedious trial and error process that requires expert knowledge. AutoML methods address this problem by providing tools to automate hyper- parameter optimization, where Bayesian optimization has become the standard for this task (Snoek et al., 2012). It treats the problem of hyperparameter optimization as a black box optimization problem. Here the black box function is the hyperparameter response function, which maps a hy- perparameter setting to the validation loss. Bayesian optimization consists of two parts. First, a surrogate model, often a Gaussian process, is used to approximate the response function. Second, an acquisition function is used that balances the trade-off between exploration and exploitation. In a sequential process, hyperparameter settings are selected and evaluated, followed by an update of the surrogate model. Recently, several attempts have been made to extend Bayesian optimization to account for a transfer learning setup. It is assumed here that historical information on machine learning algorithms is available with different hyperparameters. This can either be because this information is publicly available (e.g. OpenML) or because the algorithm is repeatedly optimized for different data sets. To this end, several transfer learning surrogates have been proposed that use this additional information to reduce the convergence time of Bayesian optimization. We propose a new paradigm for accomplishing the knowledge transfer by reconceptualizing the process as a few-shot learning task. Inspiration is drawn from the fact that there are a limited number of black box function evaluations for a new hyperparameter optimization task (i.e. few shots) but there are ample evaluations of related black box objectives (i.e. evaluated hyperparameters on other data sets). This approach has several advantages. First, a single model is learned that is trained to quickly adapt to a new task when few examples are available. This is exactly the challenge we face when optimizing hyperparameters. Second, this method can be scaled very well for any number of considered tasks. This not only enables the learning from large metadata sets but also enables the problem of label normalization to be dealt with in a new way. Finally, we present an 1 arXiv:2101.07667v1  [cs.LG]  19 Jan 2021Published as a conference paper at ICLR 2021 evolutionary algorithm that can use surrogate models to get a warm start initialization for Bayesian optimization. All of our contributions are empirically compared with several competitive methods in three different problems. Two ablation studies provide information about the inﬂuence of the individual components. Concluding, our contributions in this work are: • This is the ﬁrst work that, in the context of hyperparameter optimization (HPO), trains the initialization of the parameters of a probabilistic surrogate model from a collection of meta-tasks by few-shot learning and then transfers it by ﬁne-tuning the initialized kernel parameters to a target task. • We are the ﬁrst to consider transfer learning in HPO as a few-shot learning task. • We set the new state of the art in transfer learning for the HPO and provide ample evidence that we outperform strong baselines published at ICLR and NeurIPS with a statistically signiﬁcant margin. • We present an evolutionary algorithm that can use surrogate models to get a warm start initialization for Bayesian optimization. 2 R ELATED WORK The idea of using transfer learning to improve Bayesian optimization is being investigated in several papers. The early work suggests learning a single Gaussian process for the entire data (Bardenet et al., 2013; Swersky et al., 2013; Yogatama & Mann, 2014). Since the training of a Gaussian process is cubic in the number of training points, this idea does not scale well. This problem has recently been addressed by several proposals to use ensembles of Gaussian processes where a Gaussian process is learned for each task (Wistuba et al., 2018; Feurer et al., 2018). This idea scales linearly in the number of tasks but still cubically in the number of training points per task. Thus, the problem persists in scenarios where there is a lot of data available for each task. Bayesian neural networks are a possible more scalable way of learning with large amounts of data. For example, Schilling et al. (2015) propose to use a neural network with task embedding and vari- able interactions. To obtain mean and variance predictions, the authors propose using an ensemble of models. In contrast, Springenberg et al. (2016) use a Bayesian multi-task neural network. How- ever, since training Bayesian neural networks is computationally intensive, Perrone et al. (2018) propose a more scalable approach. They suggest using a neural network that is shared by all tasks and using Bayesian linear regression for each task. The parameters are trained jointly on the entire data. While our work shares some similarities with the previous work, our algorithm has unique properties. First of all, a meta-learning algorithm is used, which is motivated by recent work on model-agnostic meta-learning for few-shot learning (Finn et al., 2017). This will allow us to inte- grate all task-speciﬁc parameters out such that the model does not grow with the number of tasks. As a result, our algorithm scales very well with the number of tasks. Second, while we are also using a neural network, we combine it with a Gaussian process with a nonlinear kernel in order to obtain uncertainty predictions. A simpler solution for using the transfer learning idea in Bayesian optimization is initializa- tion (Feurer et al., 2015; Wistuba et al., 2015a). The standard Bayesian optimization routine with a simple Gaussian process is used in this case but it is warm-started by a number of hyperparameter settings that work well for related tasks. We also explore this idea in the context of this paper by proposing a simple evolutionary algorithm that can use a surrogate model to estimate a data-driven warm start initialization sequence. The use of an evolutionary algorithm is motivated by its ease of implementation and natural capability to deal with continuous and discrete hyperparameters. 3 P RELIMINARIES 3.1 B AYESIAN OPTIMIZATION Bayesian optimization is an optimization method for computationally intensive black box functions that consists of two main components, the surrogate model and the acquisition function. The sur- rogate model is a probabilistic model with mean µand variance σ2, which tries to approximate the unknown black box function f, in the following also response function. The acquisition function 2Published as a conference paper at ICLR 2021 can provide a score for each feasible solution based on the prediction of the surrogate model. This score balances between exploration and exploitation. The following steps are carried out sequen- tially. First, the feasible solution that maximizes the acquisition function is evaluated. In this way a new observation (x,f(x)) is obtained. Then, the surrogate model is updated based on the entirety of all observations D. This sequence of steps is carried out until a previously deﬁned convergence cri- terion occurs (e.g. exhaustion of the time budget). In Figure 1 we provide an example how Bayesian optimization is used to maximize a sine wave. Expected Improvement (Jones et al., 1998) is one of the most commonly used acquisition functions and will also be used in all our experiments. It is deﬁned as a(x|D) =E[max {f(x) −ymax,0}] , (1) where ymax is the largest observed value of f. 3.2 G AUSSIAN PROCESSES We have a training set Dof nobservations, D= {(xi,yi)|i= 1,...,n }, where yi are noisy obser- vations of the function values yi = f(xi) +ε. We assume that the noise εis additive independent identically distributed Gaussian with variance σ2 n. For Gaussian processes (Rasmussen & Williams, 2006) we consider yi to be a random variable and the joint distribution of all yi is assumed to be multivariate Gaussian distributed: y ∼N (m(X) ,k (X,X)) . (2) A Gaussian process is completely speciﬁed by its mean function m, its covariance function k, and may depend on parametersθ. A common choice is to setm(xi) = 0. At inference time for instances x∗, the assumption is that their ground truth f∗is jointly Gaussian with y [ y f∗ ] ∼N ( 0, ( Kn K∗ KT ∗ K∗∗ )) , (3) where Kn = k(X,X|θ) +σ2 nI, K∗= k(X,X∗|θ) , K∗∗= k(X∗,X∗|θ) (4) for brevity. Then, the posterior predictive distribution has mean and covariance E [f∗|X,y,X∗] =KT ∗K−1 n y, cov [f∗|X,X∗] =K∗∗−KT ∗K−1 n K∗ (5) Examples for covariance functions are the linear or squared exponential kernel. However, these kernel functions are designed by hand. The idea of deep kernel learning (Wilson et al., 2016) is to learn the kernel function. They propose to use a neural networkϕto transform the input x to a latent representation which serves as input for the kernel function. kdeep(x,x′|θ,w) =k(ϕ(x,w),ϕ(x′,w)|θ) (6) 4 F EW-SHOT BAYESIAN OPTIMIZATION Hyperparameter optimization is traditionally either tackled as an optimization problem without prior knowledge about the response function or alternatively as a multi-task or transfer learning problem. In the former, every search basically starts from scratch. In the latter, one or multiple models are trained that attempt to reuse knowledge of the source tasks for the target task. In this work we will address the problem as a few-shot problem. Given T related source tasks and very few examples of the target task, we want to make reliable predictions. For each of the source tasks we have observations D(t) = {(x(t) i ,y(t) i )}i=1...n(t) , where x(t) i is a hyperparameter setting and y(t) i is the noisy observations of f(t)(x(t) i ), i.e. a validation score of a machine learning model on data set t. In the following we will denote the set of all data points by D:= ⋃T t=1 D(t). Model- agnostic meta-learning (Finn et al., 2017) has become a popular choice for few-shot learning and we propose to use an adaptation for Gaussian processes (Patacchiola et al., 2020) as a surrogate model within the Bayesian optimization framework. The idea is simple. A deep kernel ϕis used to learn parameters across tasks such that all its parameters θ and w are task-independent. All 3Published as a conference paper at ICLR 2021 task-dependent parameters are kept separate which allows to marginalize its corresponding variable out when solely optimizing for the task-independent parameters. If we assume that the posteriors over θ and w are dominated by their respective maximum likelihood estimates ˆθ and ˆw, we can approximate the posterior predictive distribution by p(f∗|x∗,D) = ∫ p(f∗|x∗,θ,w) p(θ,w|D) dθ,w ≈p ( f∗|x∗,D,ˆθ, ˆw ) (7) We estimate ˆθ and ˆw by maximizing the log marginal likelihood for all tasks. log p ( y(1),..., y(T)|X(1),..., X(T),θ,w ) = T∑ t=1 log p ( y(t)|X(t),θ,w ) (8) ∝− T∑ t=1 ( y(t)T K(t)−1 n y(t) + log ⏐⏐⏐K(t) n ⏐⏐⏐ ) (9) We maximize the marginal likelihood with stochastic gradient ascent (SGA). Each batch contains data for one task. It is important to note that this batch data does not correspond to the data in D(t), but rather a subset of it. With small batch sizes, this is an efﬁcient way to train the Gaussian process, regardless of how many tasks knowledge is transferred from or how large D(t) is. It is important to note that there is a correlation between all data per task and not just one batch. Hence the stochastic gradient is a biased estimator of the full gradient. For a long time we lacked the theoretical understanding of how SGA would behave in this situation. Fortunately, recent work (Chen et al., 2020) proved that SGA still successfully converges and restores model parameters in these cases. The results by Chen et al. (2020) guarantee a O(1/K) optimization margin of error, where Kis the number of iterations. The training works as follows. In each iteration, a task tis sampled uniformly at random from all T tasks. Then we sample a batch of training instances uniformly at random from D(t). Finally, we calculate the log marginal likelihood for this batch (Equation 8) and update the kernel parameters with one step in the direction of the gradient. In hyperparameter optimization, we are only interested in the hyperparameter setting that works best according to a predeﬁned metric of interest. Therefore, only the ranking of the hyperparameter settings is important, not the actual value of the metric. Therefore, we are interested in a surrogate model whose prediction strongly correlate with the response function and the squared error is of little to no interest for us. In practice, however, the minimum / maximum score and the range of values differ signiﬁcantly between the tasks which makes it challenging to obtain a strongly correlating surrogate model. The most common way to address this problem is to normalize the labels, e.g. by z-normalizing or scaling the labels to [0,1] per data set. However, this does not fully solve the problem. The main problem is that this label normalization must also be applied to the target task. This is not possible with a satisfactory degree of accuracy, especially when only a few examples are available, since the approximate values for minimum / maximum or mean / standard deviation are insufﬁciently accurate. Since our proposed method scales easily to any number of tasks, we can afford to consider another option. We propose a task augmentation strategy that addresses the label normalization issue by randomly scaling the labels for each batch. Since ymin and ymax are the minimum and maximum values for all T tasks, we can generate augmented tasks for each batch B= {xi,yi}i=1,...,b ∼D(t), where bis the batch size, as follows. A lower and upper limit is sampled for each sample batch, l∼U(ymin,ymax),u ∼U(ymin,ymax) , (10) such that l<u holds. Then the labels for that batch are scaled to this range, y ←y −l u−l . (11) No further changes are required. We summarize this procedure in Algorithm 1. The idea here is to learn a representation that is invariant with respect to various offsets and ranges so that the target task does not require normalization. This strategy is not possible for other hyperparameter opti- mization methods, since this either increases the training data set size even further and thus becomes 4Published as a conference paper at ICLR 2021 Algorithm 1:Few-Shot GP Surrogate Input: Learning rates αand β, training data D, kernel k, and neural network ϕ. while not done do Sample task t∼U({1,...,T }); Estimate land u(Equation 10); for bn times do Sample batch B= {(xi,yi)}i=1,...,b ∼D(t) and scale labels y using land u; Compute marginal likelihood Lon B. (Equation 8); θ ←θ + α∇θL; w ←w + β∇wL; end end −2.5 0.0 2.5 −5 0 5 −2.5 0.0 2.5 −5 0 5 Response Function Posterior Mean Posterior Mean ± StDev Observations Expected Improvement −5 0 5 −5 0 5 Figure 1: Demonstration of ﬁve steps of Bayesian Optimization with FSBO for maximizing a sine wave (blue). One maximum is discovered within only three steps. Expected Improvement has been scaled to improve readability. In black the predictions of the surrogate model. Bottom right are examples of the source tasks. The deep kernel consists of a spectral kernel (Wilson & Adams, 2013) combined with a two-layer neural network (1 →64 →64). computationally impossible (Bardenet et al., 2013; Swersky et al., 2013; Yogatama & Mann, 2014) or thousands of new tasks would have to be generated, which is also is not practical (Springenberg et al., 2016; Perrone et al., 2018; Wistuba et al., 2018; Feurer et al., 2018). At test time, the posterior predictive distribution or the target task T + 1is computed according to Equation 5. In practice, the target data set can be very different to the learned prior. For this reason, the deep kernel parameters are ﬁne-tuned on the target task’s data for few epochs. Our task augmentation strategy described earlier has resulted in a model that has become invariant for different scales of y. For this reason we do not apply this strategy to the target task T + 1and only use it for all source tasks 1 to T. We discuss the usefulness of this step in more detail when we discuss the empirical results. 5 A M OTIVATING EXAMPLE We would like to ﬁrst motivate our method, FSBO, with an example, similar to the one described in Nichol et al. (2018). The aim is to ﬁnd the argument xthat maximizes a one-dimensional sine wave of the form f(t)(x) =a(t) sin(x+ b(t)). It is not known that the function is a sine wave and that one only has to determine the amplitude aand phase b. However, information about other sine waves 5Published as a conference paper at ICLR 2021 Mutation x1 x3 x5 x1 x12 x5 Replace one element at random Crossover x1 x3 x5 x1 x7 x9 x1 x3 x9 Merge two sets at random Figure 2: Examples for the mutation and crossover operation with I = 3. is available to the optimizer which are considered to be related. These source tasks t = 1,...,T are generated randomly with a ∼U(0.1,5) and b ∼U(0,2π). 50 examples (x(t) i ,f(t)(x(t) i )) are available for each task t, whereby the points x(t) i ∈[−5,5] are evenly spaced. It should be noted at this point that the expected value for each xi is 0. Thus, training on the data described above leads to a model predicting 0 for every xi. However, the used meta-learning procedure allows for accurately reconstructing the underlying sine wave after only a few examples (see Figure 1). Provided that the response function of the original task is similar to the target task, this is a very promising technique for accelerating the search for good hyperparameter settings. 6 A D ATA-DRIVEN INITIALIZATION The proposed few-shot surrogate model requires only a few examples before reliable predictions can be made for the target data set. How do you choose the initial hyper parameter settings? The simplest idea would be to pick them at random or use Latin Hypercube Sampling (LHS) (McKay et al., 1979). Since we already have data from various tasks and a surrogate model that can impute missing values, we propose a data-driven warm start approach. If we evaluate the performance of a hyperparameter optimization algorithm with a loss function L, we use an evolutionary algorithm to ﬁnd a set with I hyperparameter settings which minimizes this loss on the source tasks. X(init) = arg min X∈XI T∑ t=1 L ( f(t),X ) = arg min X∈XI T∑ t=1 min x∈X ˜f(t)(x) (12) The loss of a set of hyperparameters depends on the response function values for each of the ele- ments. Since these are not available for every x, we approximate f(t)(x) with the prediction of the surrogate model described in the last section whenever this is necessary. Arbitrary loss function can be considered. The speciﬁc loss function used in our experiments is the normalized regret and is deﬁned on the right side of Equation 12, where ˜f(t) is derived from f(t) by scaling it to [0,1] range: ˜f(t)(x) =f(t)(x) −f(t) min f(t) max −f(t) min , (13) where f(t) max and f(t) min are the maximum and minimum of f(t), respectively. The evolutionary algorithm works as follows. We initialize the population with sets containing I random settings, the settings being sampled in proportion to their performance according to the predictions. Precisely, a setting x is sampled proportional to exp ( − min t∈{1,...,T} ˜f(t)(x) ) . (14) The best sets are then selected to be reﬁned using an evolutionary algorithm. The algorithm ran- domly chooses either to mutate a set or to perform a crossover operation between two sets. When mutating a set, a setting is removed uniformly at random and a new setting is added proportional to its predicted performance (Figure 2, left). The crossover operation creates a new set and adds elements from the union of the parent sets until the new set hasIsettings (Figure 2, right). The new 6Published as a conference paper at ICLR 2021 set is added to the population. After 100,000 steps, the algorithm is stopped and the best set is used as the set of initial hyperparameter settings. In Figure 4 we compare this warm start initialization with the simple random or LHS initialization. 7 E XPERIMENTS We used three different optimization problems to compare the different hyperparameter optimiza- tion methods: AdaBoost, GLMNet, and SVM. We created the GLMNet and SVM metadata set by downloading the 30 data sets with the most reported hyperparameter settings from OpenML for each problem. The AdaBoost data set is publicly available (Wistuba et al., 2018). The number of settings per data set varies, the number of settings across all tasks for the GLMNet problem is approximately 800,000. See the appendix for more details about the metadata sets. We compare the following list of hyperparameter optimization methods. Random SearchThis is a simple but strong baseline for hyperparameter optimization (Bergstra & Bengio, 2012). Hyperparameters settings are selected uniformly at random from the search space. Gaussian Process (GP)Bayesian optimization with a Gaussian process as a surrogate model is a standard and strong baseline (Snoek et al., 2012). We use a Mat ´ern 5/2 kernel with ARD and rely on the scikit-optimize implementation. We compare with two variations. The ﬁrst is the vanilla method, which uses Latin Hypercube Sampling (LHS) with design size of 10 to initialize. The second method uses the warm start (WS) method described in Section 6 to ﬁnd an initial set of 5 settings to use the knowledge from other data sets. RGPE RGPE (Feurer et al., 2018) is one of the latest examples of methods that use GP ensembles to transfer knowledge across task (Wistuba et al., 2016; Schilling et al., 2016; Wistuba et al., 2018). In this case a GP is trained for each individual task and in a ﬁnal step the predictions of the GPs are aggregated. These ensembles scale linearly in the number of tasks, but the computation time is still cubic and the space requirement is still quadratic in the number of data points per task. For this reason we can only present results for AdaBoost and not for the other optimization problems, which have signiﬁcantly more data points. MetaBO Using the acquisition function instead of the surrogate model for transfer learning is an- other option. MetaBO is the state-of-the-art transfer acquisition function and was presented last year at ICLR (V olpp et al., 2020). We use the implementation provided by the authors. ABLR The state-of-the-art surrogate model for scalable hyperparameter transfer learning (Perrone et al., 2018). This method uses a multi-task GP which combines a linear kernel with a neural network and scales to very large data sets. Transfer learning is achieved by sharing the neural network parameters across different tasks. By ABLR (WS) we are referring to a version of ABLR that uses the same warm start as FSBO. Multi-Head GPs This closely resembles ABLR but uses the same deep kernel as our proposed method. The main difference from our proposed method is that it is using a GP for every task, only shares the neural network across tasks, and uses standard stochastic gradient ascent. Few-Shot Bayesian Optimization (FSBO)Our proposed method described in Section 4. The deep kernel is composed of a two-layer neural network ( 128 →128) with ReLU activations and a squared-exponential kernel. We use the Adam optimizer with learning rate 10−3 and a batch size of ﬁfty. The warm start length is ﬁve. The experiments are repeated ten times and evaluated in a leave-one-task-out cross-validation. This means that all transfer learning methods use one task as the target task and all other tasks as source tasks. For AdaBoost we use the same train/test split as used by V olpp et al. (2020) instead. We report the aggregated results for all tasks within one problem class with respect to the mean of normalized regrets. The normalized regret for a task is obtained by ﬁrst scaling the response function values between 0 and 1 before calculating the regret (Wistuba et al., 2018), i.e. ˜r(t) = ˜f(t)(xmin) −˜f(t) min, (15) 7Published as a conference paper at ICLR 2021 Method AdaBoost GLMNet SVM 15 33 50 33 67 100 33 67 100 RANDOM 4.87 3.02 2.16 0.85 0.40 0.29 2.01 1.12 0.83 GP (LHS) 3.87 2.49 2.23 1.32 1.01 0.73 1.94 1.40 1.14 GP (WS) 3.25 1.66 1.02 0.86 0.36 0.24 1.10 0.81 0.65 RGPE 5.29 3.26 2.83 -1 -1 -1 -1 -1 -1 METABO 5.27 3.52 1.96 11.02 10.97 10.96 12.39 12.39 11.93 ABLR 4.56 1.44 1.24 1.77 0.50 0.40 1.81 1.23 0.84 ABLR (WS) 3.17 1.72 1.17 0.54 0.36 0.29 1.47 1.08 0.87 MULTI -HEAD GPS (WS) 6.78 3.45 1.80 2.83 2.80 2.68 11.20 9.82 8.72 FSBO 3.10 1.13 0.80 0.42 0.22 0.16 0.79 0.51 0.36 Table 1: FSBO obtains better results for all hyperparameter optimization problems. The best results are in bold. Results that are not signiﬁcantly worse than the best are in italics. Used initialization in parentheses, (LHS) - Latin Hypercube Sampling, (WS) - Warm Start. where ˜f(t) is the normalized response function (Equation 13), ˜f(t) min is the global minimum for task t, and xmin is the best discovered hyperparameter setting by the optimizer. 7.1 H YPERPARAMETER OPTIMIZATION We conduct experiments on three different metadata sets and report the aggregated mean of normal- ized regrets in Table 1. The best results are in bold. Results that are not signiﬁcantly worse than the best are in italics. We determined the signiﬁcance using the Wilcoxon signed rank test with a conﬁ- dence level of 95%. Results are reported after every 33 trials of Bayesian optimization for GLMNet and SVM. Since AdaBoost has fewer test examples, we report its results in shorter intervals. Our proposed FSBO method outperforms all other transfer methods in all three tasks. Results are signiﬁcantly better but in the case of AdaBoost where GP (WS) achieves similar but on average worse results. The results obtained for MetaBO are the worst. We contacted V olpp et al. (2020) and they conﬁrmed that our results are obtained correctly. The problem is apparently that the Re- inforcement Learning method does not work well for larger number of trials. We provide a longer discussion in the appendix. Also ABLR and the very related Multi-Head GP are not performing very well. As we show in the next section, one possible reason for this might be because the neural network parameters are ﬁxed which will prohibit a possibly required adaptation to the target task. The vanilla GP and its transfer variant that uses a warm start turn out to be among the strongest baselines. This is in particular true for the warm start version which is the second best method. This simple baseline of knowledge transfer is often not taken into account when comparing transfer surrogates, although it is easy to implement. Due to the very competitive results, we recommend using it as a standard baseline to assess the usefulness of new transfer surrogates. 7.2 C OMPONENT CONTRIBUTIONS In this section we highlight the various components that make a signiﬁcant difference to ABLR. The results for each setup are reported in Figure 3. The Multi-Head GP is a surrogate model that shares the neural network between tasks but uses a separate Gaussian process for each task. It closely resembles the idea of ABLR but is closer to our proposed implementation of FSBO. Starting from this conﬁguration, we add various components that will eventually lead to our proposed model FSBO. We consider Multi-Head GP (WS), a version that additionally uses the warm start method described in Section 6 instead of a random initialization. Multi-Head GP (ﬁne-tune) not only updates the kernel parameters when a new observation is received but also ﬁne-tunes the parameters of the neural network. Finally, FSBO is our proposed method, which uses only one Gaussian process for all tasks. We see that all Multi-Head GP versions have a problem adapting to the target tasks efﬁciently. Fine-tuning the deep kernel is an important part of learning FSBO. Although FSBO outperforms all Multi-Head GP versions without ﬁne-tuning, the additional use of it makes for a signiﬁcant improvement. We analyzed the reason for this in more detail. We observed that the learned neural 1Out-of-memory exception due to too large data. 8Published as a conference paper at ICLR 2021 0 20 40 Number of Trials 100 101 Normalized Regret AdaBoost 0 20 40 Number of Trials 100 101 GLMNet Multi-Head GPs Multi-Head GPs (warm start) Multi-Head GPs (warm start+ﬁne-tune) FSBO (warm start) FSBO (warm start+ﬁne-tune) 0 20 40 Number of Trials 100 101 SVM Figure 3: Comparison of the contribution of the various FSBO components to the ﬁnal solution. Each component makes its own orthogonal contribution. 2 4 6 8 10 Initialization Length 10 20 30Normalized Regret AdaBoost 2 4 6 8 10 Initialization Length 20 40 60 GLMNet Random LHS Warm Start 2 4 6 8 10 Initialization Length 10 20 30 SVM Figure 4: The warm start initialization yields the best results on GLMNet and SVM for all initial- ization lengths. For AdaBoost it is comparable to LHS. network provides a strong prior that leads to strong exploitation behavior. Fine-tuning prevents this and thus ensures that the method does not get stuck in local optima. 7.3 D ATA-DRIVEN WARM START The use of a warm start is not the main contribution of this paper but it is interesting to explore fur- ther nonetheless. First of all, for some methods this is the only differentiating factor. We compare our suggested warm start initialization with a random and a Latin hypercube sampling (LHS) initial- ization in Figure 4. Considering that GP (WS) always performed better than GP (LHS) in Table 1, one would expect that the warm start itself also performs better than LHS. While this is the case for GLMNet and SVM, there are no signiﬁcant differences for the AdaBoost problem. Our assumption is that in this case the warm start might not have always found good settings as part of the initializa- tion, it still explored areas in the search space close to the optimum. This would facilitate ﬁnding a better solution in one of the subsequent steps of Bayesian optimization. 8 C ONCLUSIONS In this work, we propose to rethink hyperparameter optimization as a few-shot learning problem in which we train a shared deep surrogate model to quickly adapt (with few response evaluations) to the response function of a new task. We propose the use of a deep kernel network for a Gaussian process surrogate that is meta-learned in an end-to-end fashion in order to jointly approximate the response functions of a collection of training data sets. This few-shot surrogate model is used for two different purposes. First, we use it in combination with an evolutionary algorithm in order to estimate a data-driven warm start initialization for Bayesian optimization. Second, we use it directly for Bayesian optimization. In our empirical evaluation on three hyperparameter optimization problems, we observe signiﬁcantly better results than with state-of-the-art methods that use transfer learning. 9Published as a conference paper at ICLR 2021 ACKNOWLEDGMENTS This work has been supported by European Union’s Horizon 2020 research and innovation pro- gramme under grant number 951911 - AI4Media. Prof. Grabocka is thankful to the Eva Mayr-Stihl Foundation for their generous research grant. REFERENCES R´emi Bardenet, M ´aty´as Brendel, Bal ´azs K´egl, and Mich `ele Sebag. Collaborative hyperparameter tuning. In Proceedings of the 30th International Conference on Machine Learning, ICML 2013, Atlanta, GA, USA, 16-21 June 2013, pp. 199–207, 2013. URL http://proceedings.mlr. press/v28/bardenet13.html. James Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. J. Mach. Learn. Res., 13:281–305, 2012. URL http://dl.acm.org/citation.cfm?id= 2188395. Hao Chen, Lili Zheng, Raed AL Kontar, and Garvesh Raskutti. Stochastic gradient descent in correlated settings: A study on gaussian processes. In Advances in Neural Informa- tion Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020 , 2020. URL https://papers.nips.cc/paper/2020/hash/ 1cb524b5a3f3f82be4a7d954063c07e2-Abstract.html. Matthias Feurer, Jost Tobias Springenberg, and Frank Hutter. Initializing bayesian hyperparameter optimization via meta-learning. InProceedings of the Twenty-Ninth AAAI Conference on Artiﬁcial Intelligence, January 25-30, 2015, Austin, Texas, USA , pp. 1128–1135, 2015. URL http:// www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/10029. Matthias Feurer, Benjamin Letham, and Eytan Bakshy. Scalable meta-learning for bayesian opti- mization. CoRR, abs/1802.02219, 2018. URL http://arxiv.org/abs/1802.02219. Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017 , pp. 1126–1135, 2017. URL http: //proceedings.mlr.press/v70/finn17a.html. Donald R. Jones, Matthias Schonlau, and William J. Welch. Efﬁcient global optimization of ex- pensive black-box functions. J. Global Optimization , 13(4):455–492, 1998. doi: 10.1023/A: 1008306431147. URL https://doi.org/10.1023/A:1008306431147. Bal´azs K ´egl and R ´obert Busa-Fekete. Boosting products of base classiﬁers. In Proceedings of the 26th Annual International Conference on Machine Learning, ICML 2009, Montreal, Quebec, Canada, June 14-18, 2009, pp. 497–504, 2009. doi: 10.1145/1553374.1553439. URL https: //doi.org/10.1145/1553374.1553439. M. D. McKay, R. J. Beckman, and W. J. Conover. A comparison of three methods for selecting values of input variables in the analysis of output from a computer code. Technometrics, 21(2): 239–245, 1979. ISSN 00401706. URL http://www.jstor.org/stable/1268522. Alex Nichol, Joshua Achiam, and John Schulman. On ﬁrst-order meta-learning algorithms, 2018. Massimiliano Patacchiola, Jack Turner, Elliot J. Crowley, and Amos Storkey. Bayesian meta-learning for the few-shot setting via deep kernels. In Advances in Neural Informa- tion Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020 , 2020. URL https://papers.nips.cc/paper/2020/hash/ b9cfe8b6042cf759dc4c0cccb27a6737-Abstract.html. Valerio Perrone, Rodolphe Jenatton, Matthias W. Seeger, and C ´edric Archambeau. Scalable hy- perparameter transfer learning. In Advances in Neural Information Processing Systems 31: An- nual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, 3-8 December 2018, Montr´eal, Canada, pp. 6846–6856, 2018. URL http://papers.nips.cc/paper/ 7917-scalable-hyperparameter-transfer-learning . 10Published as a conference paper at ICLR 2021 Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian processes for machine learning. Adaptive computation and machine learning. MIT Press, 2006. ISBN 026218253X. Nicolas Schilling, Martin Wistuba, Lucas Drumond, and Lars Schmidt-Thieme. Hyperparameter optimization with factorized multilayer perceptrons. In Machine Learning and Knowledge Dis- covery in Databases - European Conference, ECML PKDD 2015, Porto, Portugal, September 7-11, 2015, Proceedings, Part II, pp. 87–103, 2015. doi: 10.1007/978-3-319-23525-7 \\6. URL https://doi.org/10.1007/978-3-319-23525-7_6 . Nicolas Schilling, Martin Wistuba, and Lars Schmidt-Thieme. Scalable hyperparameter optimiza- tion with products of gaussian process experts. In Machine Learning and Knowledge Discovery in Databases - European Conference, ECML PKDD 2016, Riva del Garda, Italy, September 19- 23, 2016, Proceedings, Part I , pp. 33–48, 2016. doi: 10.1007/978-3-319-46128-1 \\3. URL https://doi.org/10.1007/978-3-319-46128-1_3 . Jasper Snoek, Hugo Larochelle, and Ryan P. Adams. Practical bayesian optimiza- tion of machine learning algorithms. In Advances in Neural Information Process- ing Systems 25: 26th Annual Conference on Neural Information Processing Sys- tems 2012. Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United States , pp. 2960–2968, 2012. URL http://papers.nips.cc/paper/ 4522-practical-bayesian-optimization-of-machine-learning-algorithms . Jost Tobias Springenberg, Aaron Klein, Stefan Falkner, and Frank Hutter. Bayesian optimiza- tion with robust bayesian neural networks. In Advances in Neural Information Processing Sys- tems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain , pp. 4134–4142, 2016. URL http://papers.nips.cc/paper/ 6117-bayesian-optimization-with-robust-bayesian-neural-networks . Kevin Swersky, Jasper Snoek, and Ryan Prescott Adams. Multi-task bayesian optimization. In Ad- vances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Infor- mation Processing Systems 2013. Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States , pp. 2004–2012, 2013. URL http://papers.nips.cc/paper/ 5086-multi-task-bayesian-optimization . Michael V olpp, Lukas P. Fr¨ohlich, Kirsten Fischer, Andreas Doerr, Stefan Falkner, Frank Hutter, and Christian Daniel. Meta-learning acquisition functions for transfer learning in bayesian op- timization. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020, 2020. URL https://openreview.net/forum?id= ryeYpJSKwr. Andrew Gordon Wilson and Ryan Prescott Adams. Gaussian process kernels for pattern dis- covery and extrapolation. In Proceedings of the 30th International Conference on Machine Learning, ICML 2013, Atlanta, GA, USA, 16-21 June 2013 , pp. 1067–1075, 2013. URL http://proceedings.mlr.press/v28/wilson13.html. Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, and Eric P. Xing. Deep kernel learning. In Proceedings of the 19th International Conference on Artiﬁcial Intelligence and Statistics, AISTATS 2016, Cadiz, Spain, May 9-11, 2016 , pp. 370–378, 2016. URL http: //proceedings.mlr.press/v51/wilson16.html. Martin Wistuba, Nicolas Schilling, and Lars Schmidt-Thieme. Learning hyperparameter optimiza- tion initializations. In 2015 IEEE International Conference on Data Science and Advanced An- alytics, DSAA 2015, Campus des Cordeliers, Paris, France, October 19-21, 2015 , pp. 1–10, 2015a. doi: 10.1109/DSAA.2015.7344817. URL https://doi.org/10.1109/DSAA. 2015.7344817. Martin Wistuba, Nicolas Schilling, and Lars Schmidt-Thieme. Sequential model-free hyperparam- eter tuning. In 2015 IEEE International Conference on Data Mining, ICDM 2015, Atlantic City, NJ, USA, November 14-17, 2015 , pp. 1033–1038, 2015b. doi: 10.1109/ICDM.2015.20. URL https://doi.org/10.1109/ICDM.2015.20. 11Published as a conference paper at ICLR 2021 Martin Wistuba, Nicolas Schilling, and Lars Schmidt-Thieme. Two-stage transfer surrogate model for automatic hyperparameter optimization. In Machine Learning and Knowledge Discovery in Databases - European Conference, ECML PKDD 2016, Riva del Garda, Italy, September 19- 23, 2016, Proceedings, Part I, pp. 199–214, 2016. doi: 10.1007/978-3-319-46128-1 \\13. URL https://doi.org/10.1007/978-3-319-46128-1_13 . Martin Wistuba, Nicolas Schilling, and Lars Schmidt-Thieme. Scalable gaussian process-based transfer surrogates for hyperparameter optimization. Mach. Learn., 107(1):43–78, 2018. doi: 10. 1007/s10994-017-5684-y. URL https://doi.org/10.1007/s10994-017-5684-y . Dani Yogatama and Gideon Mann. Efﬁcient transfer learning method for automatic hyperparameter tuning. In Proceedings of the Seventeenth International Conference on Artiﬁcial Intelligence and Statistics, AISTATS 2014, Reykjavik, Iceland, April 22-25, 2014 , pp. 1077–1085, 2014. URL http://proceedings.mlr.press/v33/yogatama14.html. 12Published as a conference paper at ICLR 2021 0 20 40 Number of Trials 100 101 Normalized Regret AdaBoost 0 25 50 75 100 Number of Trials 100 101 102 GLMNet Random GP (LHS) GP (warm start) MetaBO ABLR Multi-Head GPs (warm start) FSBO 0 25 50 75 100 Number of Trials 100 101 SVM Figure 5: FSBO is the best of all considered methods. A H YPERPARAMETER OPTIMIZATION In the main paper we report the normalized regret after every 33 trials of Bayesian optimization in a table. This allows us to also report statistical signiﬁcance. In Figure 5 we show results for all 100 trials. The conclusions remain unchanged. FSBO provides consistently the best results. B C HALLENGES WITH METABO The results reported in the main paper for MetaBO seem not to align with the numbers reported by V olpp et al. (2020), in particular for AdaBoost. In order to understand this behavior, we conducted a deeper analysis for the AdaBoost metadata set which was used in the evaluation of V olpp et al. (2020) as well. Using the authors’ code2, we executed the same scripts used by the authors to report their results for T = 15trials. Furthermore, we changed two lines in the scripts to train MetaBO for T = 50, the number of trials considered for AdaBoost in our experiment. We report the outcome of these two experiments in Figure 6. In the two left plots we show how the learned policies for T = 15 and T = 50 perform on validation and test during training. The authors’ default setting for the total number of iterations is 2000. However, we noticed a sudden drop in regret for the setting T = 50around 2000 iterations. For that reason we assumed that further training might help to further improve MetaBO and increased it to 5000 iterations. As can be seen in the middle plot, that was not the case. We observe that the policy learned for T = 15 has a lower regret than the one for T = 50 even though it is only trying 15 compared to 50 trials. In the right plot we show the results compared to a simple random search baseline. The results for T = 15 are in line with those reported by V olpp et al. (2020). We contacted the authors and they conﬁrmed that we use their code correctly. Their explanation is that the increase of episode length makes this a harder problem for Reinforcement Learning. This results in worse results for T = 50and even worse for T = 100in case of the GLMNet and SVM problem. They proposed to randomly vary T between 5 and 50 during the training phase. We also considered this training protocol and report the results in Figure 6. This improves over training only with T = 50but does not improve over a simple random search. C M ETADATA We created the GLMNet and SVM metadata using OpenML. We chose the 30 OpenML ﬂows with the most observations. We also limited ourselves to the uploaded results from user with ID 2702 (OpenML Bot R), who uploaded the majority of all runs, to ensure that the metadata was generated under similar circumstances. In some cases the choice of a single hyperparameter is not indicated. In such a case, we assume that the default value was used. The GLMNet metadata has two conti- nous hyperparameters: the elastic-net mixing parameter α∈[0,1] and the regularization parameter λ ∈ [2−10,210]. The SVM metadata has two mandatory hyperparameters and two conditional 2https://github.com/boschresearch/MetaBO 13Published as a conference paper at ICLR 2021 0 500 1000 1500 2000 Iterations 0.00 0.01 0.02 0.03 0.04 0.05 Simple Regret T = 15 MetaBO (Valid) MetaBO (Test) Random (Test, T=50) 0 2000 4000 Iterations 0.00 0.01 0.02 0.03 0.04 0.05 T = 50 0 20 40 Number of Trials 101 Normalized Regret AdaBoost MetaBO (T=15) MetaBO (T=50) MetaBO (T=5..50) Random Figure 6: Left two plots: Training MetaBO for different number of trials T. Right plot: comparing the ﬁnal policy against a random search. Results for T = 15 match the results reported by V olpp et al. (2020). Method AdaBoost GLMNet SVM 15 33 50 33 67 100 33 67 100 FSBO 3.10 1.13 0.80 0.42 0.22 0.16 0.79 0.51 0.36 FSBO (R EPTILE ) 3.80 2.13 1.21 0.36 0.19 0.14 1.48 0.89 0.72 Table 2: Comparing different model-agnostic meta-learning approaches. Again, the best results are in bold and results that are not signiﬁcantly worse than the best are in italics. hyperparameters. The kernel (linear, polynomial or RBF) and the continuous trade-off parameter C ∈[2−10,210] must always be selected. The degree d ∈{2,3,4,5}is only considered for the polynomial kernel and the bandwidth γ ∈[2−10,210] is only considered for the RBF kernel. We further use the AdaBoost metadata which has been used to evaluate MetaBO (V olpp et al., 2020). According to Wistuba et al. (2015b), this metadata was created using AdaBoost (K ´egl & Busa-Fekete, 2009) with decision products as weak learners. The authors designed a grid over the two continuous hyperparameters: the number of iterations and the number of product terms. The number of iterations was chosen from {2,5,10,20,50,100,200,500,103,2 ·103,5 ·103,104}, the number of product term was chosen from {2,3,4,5,7,10,15,20,30}. The metadata was created by running a grid search on 50 different classiﬁcation data sets, using clas- siﬁcation accuracy as an objective. Compared to the other two metadata sets, this one is signiﬁcantly smaller. Its use is mainly motivated by comparing to MetaBO under the same circumstances as used to evaluate MetaBO. For that reason, we do not use the leave-one-data-set-out cross-validation pro- tocol but instead use the same ﬁxed split created by V olpp et al. (2020). Statistics about these metadata sets are provided in Table 3. D M ETA-LEARNING WITH REPTILE In principle our few-shot surrogate can be combined with any model-agnostic meta-learning ap- proach. In this section we compare our proposed meta-learning approach against Reptile (Nichol et al., 2018), a ﬁrst-order approximation of MAML. We use the same hyperparameters and data augmentation as described in Algorithm 1. Reptile introduces a new hyperparameter, an outer learn- ing rate. We set it to 0.1 and decay it linearly to 0. As summarized in Table 2, FSBO with the meta-learning approach described in Algorithm 1 is either better or not signiﬁcantly worse than its variation with Reptile. 14Published as a conference paper at ICLR 2021 AdaBoost GLMNet SVM DATA SET RUNS OPEN ML ID R UNS OPEN ML ID R UNS A9A 108 3 12796 37 2429 W8A 108 31 60721 3485 1000 ABALONE 108 37 17950 3492 3217 APPENDICITIS 108 219 13963 3493 1828 AUSTRALIAN 108 3492 16968 3494 1854 AUTOMOBILE 108 3493 19931 3889 1951 BANANA 108 3889 13942 3891 3083 BANDS 108 3891 13917 3899 2487 BREAST -CANCER 108 3899 15954 3902 1105 BUPA 108 3903 39155 3903 1018 CAR 108 3913 22949 3913 1448 CHESS 108 3917 16960 3918 1459 COD -RNA 108 3918 20984 3950 1998 COIL 2000 108 9914 37635 6566 2702 COLON -CANCER 108 9946 25532 9889 2485 CRX 108 9952 25854 9914 1498 DIABETES 108 9967 21994 9946 1291 ECOLI 108 9978 19956 9952 2483 GERMAN -NUMER 108 9980 31961 9967 1492 HABERMAN 108 9983 37134 9971 1381 HOUSEVOTES 108 10101 66277 9976 1473 IJCNN 1 108 125923 35555 9978 1503 KR-VS-K 108 145847 29927 9980 2943 LED 7DIGIT 108 145857 31180 9983 987 LETTER 108 145862 12954 10101 1871 LYMPHOGRAPHY 108 145872 17835 14951 1765 MAGIC 108 145953 47439 34536 1499 MONK -2 108 145972 19972 34537 1489 PENDIGITS 108 145979 12979 145878 1244 PHONEME 108 146064 43785 146064 1435 PIMA 108 RING 108 SAHEART 108 SEGMENT 108 SEISMIC 108 SHUTTLE 108 SONAR -SCALE 108 SPAMBASE 108 SPECTFHEART 108 SPLICE 108 TIC -TAC-TOE 108 TITANIC 108 TWONORM 108 USPS 108 VEHICLE 108 WDBC 108 WINE 108 WINEQUALITY -RED 108 WISCONSIN 108 YEAST 108 TOTAL 5400 TOTAL 804159 TOTAL 54418 Table 3: Metadata set statistics. OpenML ID refers to a task on openml.org. 15",
      "meta_data": {
        "arxiv_id": "2101.07667v1",
        "authors": [
          "Martin Wistuba",
          "Josif Grabocka"
        ],
        "published_date": "2021-01-19T15:00:39Z",
        "pdf_url": "https://arxiv.org/pdf/2101.07667v1.pdf",
        "github_url": "https://github.com/boschresearch/MetaBO"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper proposes rethinking hyperparameter optimization (HPO) as a few-shot learning problem, introducing Few-Shot Bayesian Optimization (FSBO). It uses a meta-learned deep kernel network for a Gaussian process surrogate to quickly adapt to new HPO tasks with minimal evaluations. This approach achieves new state-of-the-art results in HPO transfer learning, outperforming strong baselines on diverse metadata sets. Additionally, the work presents an evolutionary algorithm that leverages the surrogate model to provide a data-driven warm-start initialization for Bayesian optimization.",
        "methodology": "The core methodology involves a deep kernel network where a neural network transforms input features, which then feed into a Gaussian Process kernel for probabilistic predictions. The model is meta-learned using an adaptation of Model-Agnostic Meta-Learning (MAML) for GPs, where task-independent kernel parameters are optimized by maximizing the log marginal likelihood across a collection of source tasks via stochastic gradient ascent. To address label normalization challenges across diverse tasks, a task augmentation strategy is introduced, randomly scaling labels for each training batch to promote scale-invariant representations. For new target tasks, the deep kernel parameters are fine-tuned. A data-driven warm-start initialization is also proposed, utilizing an evolutionary algorithm to identify an optimal initial set of hyperparameters based on the surrogate model's predictions on source tasks, minimizing normalized regret.",
        "experimental_setup": "Experiments were conducted on three hyperparameter optimization problems: AdaBoost, GLMNet, and SVM. The GLMNet and SVM metadata sets were compiled from 30 OpenML datasets, specifically from 'OpenML Bot R' entries for consistency. The AdaBoost metadata was a publicly available set. Performance was evaluated using the aggregated mean of normalized regrets, with statistical significance determined by the Wilcoxon signed rank test. Validation employed a leave-one-task-out cross-validation for GLMNet and SVM, and a fixed train/test split for AdaBoost. Experiments were repeated ten times. The FSBO model utilized a deep kernel composed of a two-layer neural network (128 → 128 with ReLU) and a squared-exponential kernel, trained with the Adam optimizer (learning rate 10⁻³) and a batch size of 50. A warm start length of five was used. Baselines included Random Search, vanilla Gaussian Process (LHS and Warm Start), RGPE, MetaBO, ABLR (vanilla and Warm Start), Multi-Head GPs (various configurations), and Reptile.",
        "limitations": "Existing Gaussian Process-based transfer learning methods face scalability issues due to cubic computational complexity with respect to the number of training points. Accurate label normalization for target tasks with only a few observations is challenging, hindering the reliability of surrogate models. The MetaBO baseline, while a strong competitor, showed degraded performance with an increased number of trials, suggesting limitations in its reinforcement learning approach for extended HPO sequences. Furthermore, while the proposed warm-start initialization is generally beneficial, it did not consistently yield a statistically significant improvement over simpler initializations like Latin Hypercube Sampling for all problem types, such as AdaBoost.",
        "future_research_directions": "A direct future research direction implied by the paper is to explore the combination of the proposed few-shot surrogate model with other model-agnostic meta-learning approaches beyond the specific MAML adaptation and Reptile already tested.",
        "experimental_code": "# Copyright (c) 2019 Robert Bosch GmbH# This program is free software: you can redistribute it and/or modify# it under the terms of the GNU Affero General Public License as published# by the Free Software Foundation, either version 3 of the License, or# (at your option) any later version.)# This program is distributed in the hope that it will be useful,# but WITHOUT ANY WARRANTY; without even the implied warranty of# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the# GNU Affero General Public License for more details.# You should have received a copy of the GNU Affero General Public License# along with this program.  If not, see <https://www.gnu.org/licenses/>.)# ******************************************************************# policies.py# Implementation of the MetaBO neural AF as well as benchmark AFs.# ******************************************************************import torchimport torch.nn as nnimport torch.nn.functional as Ffrom torch.distributions import Categoricalimport numpy as npfrom metabo.policies.mlp import MLPclass NeuralAF(nn.Module):    \"\"\"    Base class for MetaBO-Policies. Subclasses have to implement init_structure() and forward().    SHAPES:    forward()     states: (N_batch, N_grid, N_features)     logits: (N_batch, N_grid)     values: (N_batch, )    act(): only one action/value at a time in self.act()     state: (N_grid, N_features)     action: ()     value: ()    predict_vals_logps_ents()     states: (N_batch, N_grid, N_features)     actions: (N_batch, )     values: (N_batch, )     logprobs: (N_batch, )     entropies: (N_batch, )    \"\"\"    def __init__(self, observation_space, action_space, deterministic, options):        super(NeuralAF, self).__init__()        self.N_features = None  # has to be set in init_structure()        self.deterministic = deterministic        # initialize the network structure        self.init_structure(observation_space=observation_space, action_space=action_space, options=options)        # initialize weights        self.apply(self.init_weights)    def init_structure(self, observation_space, action_space, options):        self.N_features = observation_space.shape[1]        # activation function        if options[\"activations\"] == \"relu\":            f_act = F.relu        elif options[\"activations\"] == \"tanh\":            f_act = torch.tanh        else:            raise NotImplementedError(\"Unknown activation function!\")        # policy network        self.N_features_policy = self.N_features        if \"exclude_t_from_policy\" in options:            self.exclude_t_from_policy = options[\"exclude_t_from_policy\"]            assert \"t_idx\" in options            self.t_idx = options[\"t_idx\"]            self.N_features_policy = self.N_features_policy - 1 if self.exclude_t_from_policy else self.N_features_policy        else:            self.exclude_t_from_policy = False        if \"exclude_T_from_policy\" in options:            self.exclude_T_from_policy = options[\"exclude_T_from_policy\"]            assert \"T_idx\" in options            self.T_idx = options[\"T_idx\"]            self.N_features_policy = self.N_features_policy - 1 if self.exclude_T_from_policy else self.N_features_policy        else:            self.exclude_T_from_policy = False        self.policy_net = MLP(d_in=self.N_features_policy, d_out=1, arch_spec=options[\"arch_spec\"], f_act=f_act)        # value network        if \"use_value_network\" in options and options[\"use_value_network\"]:            self.use_value_network = True            self.value_net = MLP(d_in=2, d_out=1, arch_spec=options[\"arch_spec_value\"], f_act=f_act)            self.t_idx = options[\"t_idx\"]            self.T_idx = options[\"T_idx\"]        else:            self.use_value_network = False    def forward(self, states):        assert states.dim() == 3        assert states.shape[-1] == self.N_features        # policy network        mask = [True] * self.N_features        if self.exclude_t_from_policy:            mask[self.t_idx] = False        if self.exclude_T_from_policy:            mask[self.T_idx] = False        logits = self.policy_net.forward(states[:, :, mask])        logits.squeeze_(2)        # value network        if self.use_value_network:            tT = states[:, [0], [self.t_idx, self.T_idx]]            values = self.value_net.forward(tT)            values.squeeze_(1)        else:            values = torch.zeros(states.shape[0]).to(logits.device)        return logits, values    def af(self, state):        state = torch.from_numpy(state[None, :].astype(np.float32))        with torch.no_grad():            out = self.forward(state)        af = out[0].to(\"cpu\").numpy().squeeze()        return af    def act(self, state):        # here, state is assumed to contain a single state, i.e. no batch dimension        state = state.unsqueeze(0)  # add batch dimension        out = self.forward(state)        logits = out[0]        value = out[1]        if self.deterministic:            action = torch.argmax(logits)        else:            distr = Categorical(logits=logits)            # to sample the action, the policy uses the current PROCESS-local random seed, don't re-seed in pi.act            action = distr.sample()        return action.squeeze(0), value.squeeze(0)    def predict_vals_logps_ents(self, states, actions):        assert actions.dim() == 1        assert states.shape[0] == actions.shape[0]        out = self.forward(states)        logits = out[0]        values = out[1]        distr = Categorical(logits=logits)        logprobs = distr.log_prob(actions)        entropies = distr.entropy()        return values, logprobs, entropies    def set_requires_grad(self, requires_grad):        for p in self.parameters():            p.requires_grad = requires_grad    def reset(self):        pass    @staticmethod    def num_flat_features(x):        return np.prod(x.size()[1:])    @staticmethod    def init_weights(m):        if type(m) == nn.Linear:            m.weight.data.normal_(mean=0.0, std=0.01)            m.bias.data.fill_(0.0)class MLP(nn.Module):    def __init__(self, d_in: int, d_out: int, arch_spec: list, f_act=None):        \"\"\"        A standard multi-layer perceptron.        :param d_in: number of input features.        :param d_out: number of output features.        :param arch_spec: list containing the number of units in each hidden layer. If arch_spec == [], this is a        linear model.        :param f_act: nonlinear activation function (if arch_spec != [])        \"\"\"        super(MLP, self).__init__()        self.arch_spec = arch_spec        self.f_act = f_act        self.is_linear = (arch_spec == [])  # no hidden layers --> linear model        if not self.is_linear:            assert f_act is not None        # define the network        if self.is_linear:            self.fc = nn.ModuleList([nn.Linear(in_features=d_in, out_features=d_out)])        else:            self.fc = nn.ModuleList([nn.Linear(in_features=d_in, out_features=arch_spec[0])])            for i in range(1, len(arch_spec)):                self.fc.append(nn.Linear(in_features=self.fc[-1].out_features, out_features=arch_spec[i]))            self.fc.append(nn.Linear(in_features=self.fc[-1].out_features, out_features=d_out))    def forward(self, X):        Y = X        if self.is_linear:            Y = self.fc[0](Y)        else:            for layer in self.fc[:-1]:                Y = self.f_act(layer(Y))            Y = self.fc[-1](Y)        return Y",
        "experimental_info": "The Neural Acquisition Function (NeuralAF) is implemented as an MLP using ReLU activations and an architecture of 4 hidden layers, each with 200 units. A separate value network is also used with the same architecture. The policy is trained using Proximal Policy Optimization (PPO) with a batch size of 1200, 2000 iterations (total 2.4 million steps), 4 epochs, and a learning rate of 1e-4. The PPO algorithm uses an epsilon of 0.15 for clipping, value function coefficient of 1.0, and entropy coefficient of 0.01. GAE-Lambda with gamma=0.98 and lambda=0.98 is used for advantage estimation, and advantages are normalized. Training is performed on 10 workers with seeds from 0 to 9. The environment is 'MetaBO-GP-v0', a 3-dimensional task sampling Gaussian Process objective functions (Matern52 kernel). Lengthscales are sampled between 0.05 and 0.5, and signal/noise variances are 1.0/0.1 respectively. The optimization horizon (T) is 30 steps with 0 initial samples. Rewards are transformed using 'neg_log10' and prior mean function is used. The domain is treated as continuous with local acquisition function optimization enabled, using 2000 multi-start points and 2000 local search points with k=5 top multi-start points for local search."
      }
    },
    {
      "title": "Efficient Hyperparameter Optimization with Adaptive Fidelity Identification",
      "abstract": "Hyperparameter optimization (HPO) and neural architecture search (NAS) are\npowerful in attaining state-of-the-art machine learning models, with Bayesian\noptimization (BO) standing out as a mainstream method. Extending BO into the\nmulti-fidelity setting has been an emerging research topic, but faces the\nchallenge of determining an appropriate fidelity for each hyperparameter\nconfiguration to fit the surrogate model. To tackle the challenge, we propose a\nmulti-fidelity BO method named FastBO, which adaptively decides the fidelity\nfor each configuration and efficiently offers strong performance. The\nadvantages are achieved based on the novel concepts of efficient point and\nsaturation point for each configuration.We also show that our adaptive fidelity\nidentification strategy provides a way to extend any single-fidelity method to\nthe multi-fidelity setting, highlighting its generality and applicability.",
      "full_text": "FastBO: Fast HPO and NAS with Adaptive Fidelity Identification Jiantong Jiang and Ajmal Mian The University of Western Australia, Perth WA 6009, Australia {jiantong.jiang@research.,ajmal.mian@}uwa.edu.au Abstract. Hyperparameter optimization (HPO) and neural architec- ture search (NAS) are powerful in attaining state-of-the-art machine learning models, with Bayesian optimization (BO) standing out as a mainstream method. Extending BO into the multi-fidelity setting has been an emerging research topic, but faces the challenge of determining an appropriate fidelity for each hyperparameter configuration to fit the surrogate model. To tackle the challenge, we propose a multi-fidelity BO method named FastBO, which adaptively decides the fidelity for each configuration and efficiently offers strong performance. The advantages are achieved based on the novel concepts ofefficient pointand saturation point for each configuration. We also show that our adaptive fidelity iden- tification strategy provides a way to extend any single-fidelity method to the multi-fidelity setting, highlighting its generality and applicability. Keywords: HPO · NAS · Multi-fidelity 1 Introduction HPO [9] and NAS [7] aim to find the hyperparameter configuration or archi- tecture λ∗ that minimizesf(λ), the performance obtained by configurationλ. BO [2,10,30] is an effective model-based method for HPO and NAS. It maintains asurrogate modelof the performancebasedon past evaluations ofconfigurations, which guides the choice of promising configurations to evaluate. Recent studies onBOhaveexploredexpertpriors[11,20,26,29],derivativeinformation[1,27,35], and enhancing the interpretability [5,36–39] of HPO and NAS [3,24,25]. However, standard BO requires full evaluations of configurations, which in- curssignificantcosts,especiallyconsideringtheescalatingmodelevaluationover- head. Despite efforts to accelerate model evaluation [13,15–17], smart strategies are required to widely adopt HPO and NAS. Thus, multi-fidelity methods have been proposed [4,12,21,22], where the fidelities mean the performance levels obtained under various resource levels. They follow the idea of successive halv- ing (SHA) [12]: initially, they evaluate many random configurations using few resources; then, based on the low-fidelity performances, only the well-performing ones successively continue to be evaluated with increasing resources. Follow-up studies [8,19,23,28,33] propose model-based multi-fidelity meth- ods, replacing random sampling with more informed models to improve samplearXiv:2409.00584v1  [cs.LG]  1 Sep 20242 J. Jiang and A. Mian efficiency. However, they are based on SHA, which assumes that learning curves of different configurations rarely intersect - a condition that often fails in prac- tice [32], i.e., early performance observations cannot always indicate the final fidelity performance. This raises a key challenge:What is the appropriate fidelity for each configuration to fit the surrogate model?In other words, which fidelity can offer observations that reliably indicate the final fidelity performance? Cur- rent methods struggle with this. Hyper-Tune [23] and BOHB [8] fit separate models for different fidelities, missing inter-fidelity correlations. FTBO [31] and A-BOHB [19] fit joint models but require strong assumptions. Salinas et al. [28] use the last observed fidelity performance, which may get inaccurate surrogate models as it widens the gap between poorly- and well-performing configurations. This paper is an extended abstract of our conference paper [14], highlighting key ideas and the main experimental results, while omitting finer details. 2 Key Idea of FastBO We propose a multi-fidelity extension of BO, namely FastBO, which tackles the challenge of deciding the appropriate fidelity for each configuration to fit the surrogate model. Here, we first propose the key concepts ofefficient point and saturation point, which are crucial in the optimization process. Then, we briefly describe the process of FastBO and highlight its generality. 2.1 Efficient Point and Saturation Point We first formally define the efficient point as follows. Definition 1 (Efficient point). For a given learning curveCi(r) of hyper- parameter configuration or architectureλi, wherer represents the resource level (also referred to as fidelity), the efficient pointei of λi is defined as:ei = min{r | Ci(r) − Ci(2r) < δ1}, whereδ1 is a predefined small threshold. The semantic of Definition 1 is that starting from the efficient point onwards, whentheresourcesaredoubled,theperformanceimprovementfallsbelowasmall threshold. Consequently, this point signifies a fidelity of performance achieved with comparably efficient resource usage. Thus, we make the following remark. Remark 1. The efficient points of the configurations can serve as their ap- propriate fidelities used for fitting the surrogate model. This is due to their (i) optimal resource-to-performance balance, (ii) ability to capture valuable learning curve trends, and (iii) customization for different hyperparameter configurations. We elaborate on the reasons as follows. Firstly, efficient points balance the trade- off between computational cost and result quality. Beyond the efficient point, allocating additional resources becomes less efficient. Secondly, efficient points capture valuable behaviors within the learning curves, enabling more informed decision-making. Thirdly, the ability to customize the fidelity for each specific configuration is an advantage. This adaptive approach is more reasonable than previous studies that use a fixed fidelity for all the configurations.FastBO: Fast HPO and NAS with Adaptive Fidelity Identification 3 1 FastBO Methodology•Process overviewWarm-up Learning curve modelingEfficient & saturation pointsConfig !! •Evaluate !! to the warm-up point to get early observation set.•Stop some bad configs. •Estimate !!’s learning curve from its observation set. Post-processing……•Adaptively extract the points from the learning curve. •Stop evaluating at efficient point.•Fit surrogate model. •Resume some best-performing configs to saturation point.Optimal config Fig. 1:Main process of FastBO. FastBO involves estimating efficient and saturation points, modeling learning curves, and auxiliary stages of warm-up and post-processing. Besides efficient points, we identify saturation points for all configurations as well. We provide the definition of the saturation point as follows. Definition 2 (Saturation point).For a given learning curveCi(r) of config- uration λi, wherer represents the resource level (also referred to as fidelity), the saturation pointsi of λi is defined as:si = min{r | ∀r′ > r,|Ci(r′)−Ci(r)| < δ2}, where δ2 is a predefined small threshold. The semantic of Definition 2 is that beyond the saturation point, the observed performance no longer exhibits notable variations with more resources. Thus, this point characterizes the fidelity at which the performance of a configuration stabilizes. Building on the above definition, we make the following remark. Remark 2. The saturation points of the configurations can serve as their ap- proximate final fidelities, as they provide performance results that meet predefined quality thresholds while reducing resource wastage. 2.2 FastBO Process and Generalization With the two crucial points, we show the main process of FastBO in Fig. 1. Each configurationλi first enters a warm-up stage to get its early observation set. Some configurations are terminated here if they are detected consecutive performance deterioration. Then, FastBO estimates the learning curve ofλi from its observation set. Thus, the efficient point and saturation points are adaptively extracted. After that, λi continues to be evaluated to its efficient point; the result is used to update the surrogate model. Finally, the post-processing stage let a small set of promising configurations resume evaluating to their saturation points, and the optimal configurations can be obtained. Generalizing FastBO to single-fidelity methods.The inefficiency of single- fidelity methods like BO stems from their reliance on expensive final fidelity evaluations. Notably, low-fidelity evaluations provide informative insights but are computationally cheaper. Therefore, we can extend single-fidelity methods to the multi-fidelity setting by acquiring the low-fidelity performance for each configuration to fit the surrogate model. To do this, we need to determine the fidelity used to fit the surrogate model. FastBO adaptively determines this fi- delity for each configuration by identifying its efficient point. While this adaptive identification strategy is described in the context of model-based methods, it can4 J. Jiang and A. Mian Fashion-MNIST Airlines Albert CovertypeChristine Fig. 2:Anytime performance on the LCBench benchmark. Slice CIFAR-10 CIFAR-100 ProteinImageNet16-120 (a) NAS-Bench-201 benchmark (b) FCNet benchmark Fig. 3:Anytime performance on(a) NAS-Bench-201 and(b) FCNet. be generalized to various single-fidelity methods. For example, when evaluating configurations within the population for an evolutionary algorithm-based HPO method, we can similarly evaluate the efficient point performances instead of the final performances and integrate them in the subsequent processes, such as selection and variation. To conclude, the proposed strategy provides a simple but effective way to bridge the gap between single- and multi-fidelity methods. 3 Experimental Evaluation We compare the performance of FastBO with random search (RS), standard BO [30], ASHA [21], Hyperband [22], PASHA [4], A-BOHB [19], A-CQR [28], BOHB [8], DyHPO [34], and Hyper-Tune [23]. The results on the LCBench [40], NAS-Bench-201 [6], and FCNet [18] benchmarks are shown in Figs. 2 and 3. Overall, FastBO can handle various performance metrics and shows strong any- time performance. We can observe that FastBO gains an advantage earlier than other methods, rapidly converging to the global optimum after the initial phase. 4 Conclusion and Discussion We propose FastBO, a model-based multi-fidelity HPO method, which excels in adaptively identifying the fidelity for each configuration to fit the surrogate modelandefficientlyprovidinghigh-qualityperformance.Theproposed adaptive fidelity identification strategy also provides a simple way to extend any single- fidelity method to the multi-fidelity setting. While this paper provides a strong foundation on HPO and NAS, we see challenges that demand future improve- ments. Future work could refine and expand Fast-BO to larger search spaces and distributed computing systems to improve its applicability and scalablity.FastBO: Fast HPO and NAS with Adaptive Fidelity Identification 5 References 1. Ament, S.E., Gomes, C.P.: Scalable first-order Bayesian Optimization via struc- tured automatic differentiation. In: International Conference on Machine Learning. pp. 500–516. PMLR (2022) 1 2. Bergstra, J., Bardenet, R., Bengio, Y., Kégl, B.: Algorithms for hyper-parameter optimization. Advances in Neural Information Processing Systems24 (2011) 1 3. Bischl, B., Binder, M., Lang, M., Pielok, T., Richter, J., Coors, S., Thomas, J., Ullmann, T., Becker, M., Boulesteix, A.L., et al.: Hyperparameter optimization: Foundations, algorithms, best practices, and open challenges. Wiley Interdisci- plinary Reviews: Data Mining and Knowledge Discovery13(2), e1484 (2023) 1 4. Bohdal, O., Balles, L., Wistuba, M., Ermis, B., Archambeau, C., Zappella, G.: PASHA: efficient HPO and NAS with progressive resource allocation. In: Interna- tional Conference on Learning Representations. OpenReview.net (2023) 1, 4 5. Chen, C., Li, O., Tao, D., Barnett, A., Rudin, C., Su, J.K.: This looks like that: deep learning for interpretable image recognition. Advances in Neural Information Processing Systems32 (2019) 1 6. Dong, X., Yang, Y.: NAS-Bench-201: Extending the scope of reproducible neu- ral architecture search. In: International Conference on Learning Representations (2020) 4 7. Elsken, T., Metzen, J.H., Hutter, F.: Neural architecture search: A survey. The Journal of Machine Learning Research20(1), 1997–2017 (2019) 1 8. Falkner, S., Klein, A., Hutter, F.: BOHB: Robust and efficient hyperparameter optimization at scale. In: International Conference on Machine Learning. pp. 1437– 1446. PMLR (2018) 1, 2, 4 9. Feurer, M., Hutter, F.: Hyperparameter optimization. Automated Machine Learn- ing: Methods, Systems, Challenges pp. 3–33 (2019) 1 10. Hutter, F., Hoos, H.H., Leyton-Brown, K.: Sequential model-based optimization for general algorithm configuration. In: Learning and Intelligent Optimization. pp. 507–523. Springer (2011) 1 11. Hvarfner,C.,Stoll,D.,Souza,A.L.F.,Lindauer,M.,Hutter,F.,Nardi,L.:$\\pi$BO: Augmenting acquisition functions with user beliefs for bayesian optimization. In: International Conference on Learning Representations. OpenReview.net (2022) 1 12. Jamieson, K., Talwalkar, A.: Non-stochastic best arm identification and hyperpa- rameter optimization. In: Artificial Intelligence and Statistics. pp. 240–248. PMLR (2016) 1 13. Jiang, J., Wen, Z., Mansoor, A., Mian, A.: Fast parallel exact inference on Bayesian networks. In: ACM SIGPLAN Annual Symposium on Principles and Practice of Parallel Programming. pp. 425–426 (2023) 1 14. Jiang, J., Wen, Z., Mansoor, A., Mian, A.: Efficient hyperparameter optimization with adaptive fidelity identification. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 26181–26190 (2024) 2 15. Jiang, J., Wen, Z., Mansoor, A., Mian, A.: Fast inference for probabilistic graphical models. In: 2024 USENIX Annual Technical Conference (USENIX ATC 24) (2024) 1 16. Jiang, J., Wen, Z., Mian, A.: Fast parallel bayesian network structure learning. In: IEEE International Parallel and Distributed Processing Symposium. pp. 617–627. IEEE (2022) 1 17. Jiang, J., Wen, Z., Yang, P., Mansoor, A., Mian, A.: Fast-pgm: Fast probabilistic graphical model learning and inference. arXiv preprint arXiv:2405.15605 (2024) 16 J. Jiang and A. Mian 18. Klein, A., Hutter, F.: Tabular benchmarks for joint architecture and hyperparam- eter optimization. arXiv preprint arXiv:1905.04970 (2019) 4 19. Klein, A., Tiao, L.C., Lienart, T., Archambeau, C., Seeger, M.: Model-based asynchronous hyperparameter and neural architecture search. arXiv preprint arXiv:2003.10865 (2020) 1, 2, 4 20. Li, C., Rana, S., Gupta, S., Nguyen, V., Venkatesh, S., Sutti, A., de Celis Leal, D.R.,Slezak,T.,Height,M.,Mohammed,M.,Gibson,I.:Acceleratingexperimental design by incorporating experimenter hunches. In: International Conference on Data Mining. pp. 257–266. IEEE Computer Society (2018) 1 21. Li,L.,Jamieson,K.,Rostamizadeh,A.,Gonina,E.,Ben-Tzur,J.,Hardt,M.,Recht, B., Talwalkar, A.: A system for massively parallel hyperparameter tuning. Proceed- ings of Machine Learning and Systems2, 230–246 (2020) 1, 4 22. Li, L., Jamieson, K., DeSalvo, G., Rostamizadeh, A., Talwalkar, A.: Hyperband: A novel bandit-based approach to hyperparameter optimization. Journal of Machine Learning Research18(1), 6765–6816 (2017) 1, 4 23. Li, Y., Shen, Y., Jiang, H., Zhang, W., Li, J., Liu, J., Zhang, C., Cui, B.: Hyper- tune: Towards efficient hyper-parameter tuning at scale. Proceedings of the VLDB Endowment 15(6), 1256–1265 (2022) 1, 2, 4 24. Moosbauer, J., Casalicchio, G., Lindauer, M., Bischl, B.: Improving accuracy of interpretability measures in hyperparameter optimization via Bayesian algorithm execution. arXiv preprint arXiv:2206.05447 (2022) 1 25. Moosbauer, J., Herbinger, J., Casalicchio, G., Lindauer, M., Bischl, B.: Explaining hyperparameter optimization via partial dependence plots. Advances in Neural Information Processing Systems34, 2280–2291 (2021) 1 26. Oh, C., Gavves, E., Welling, M.: BOCK: Bayesian optimization with cylindrical kernels. In: International Conference on Machine Learning. pp. 3868–3877. PMLR (2018) 1 27. Padidar, M., Zhu, X., Huang, L., Gardner, J., Bindel, D.: Scaling gaussian pro- cesses with derivative information using variational inference. Advances in Neural Information Processing Systems34, 6442–6453 (2021) 1 28. Salinas, D., Golebiowski, J., Klein, A., Seeger, M.W., Archambeau, C.: Optimizing hyperparameters with conformal quantile regression. In: International Conference on Machine Learning. vol. 202, pp. 29876–29893. PMLR (2023) 1, 2, 4 29. Shahriari, B., Bouchard-Côté, A., Freitas, N.: Unbounded Bayesian Optimization via regularization. In: Artificial intelligence and statistics. pp. 1168–1176. PMLR (2016) 1 30. Snoek, J., Larochelle, H., Adams, R.P.: Practical Bayesian optimization of machine learning algorithms. Advances in Neural Information Processing Systems25 (2012) 1, 4 31. Swersky, K., Snoek, J., Adams, R.P.: Freeze-thaw Bayesian optimization. arXiv preprint arXiv:1406.3896 (2014) 2 32. Viering, T., Loog, M.: The shape of learning curves: a review. IEEE Transactions on Pattern Analysis and Machine Intelligence (2022) 2 33. Wang, J., Xu, J., Wang, X.: Combination of hyperband and bayesian optimization for hyperparameter optimization in deep learning. arXiv preprint arXiv:1801.01596 (2018) 1 34. Wistuba, M., Kadra, A., Grabocka, J.: Supervising the multi-fidelity race of hy- perparameter configurations. Advances in Neural Information Processing Systems 35, 13470–13484 (2022) 4 35. Wu, J., Poloczek, M., Wilson, A.G., Frazier, P.: Bayesian optimization with gradi- ents. Advances in neural information processing systems30 (2017) 1FastBO: Fast HPO and NAS with Adaptive Fidelity Identification 7 36. Yang,P.,Akhtar,N.,Jiang,J.,Mian,A.:Backdoor-basedexplainableaibenchmark forhighfidelityevaluationofattributionmethods.arXivpreprintarXiv:2405.02344 (2024) 1 37. Yang, P., Akhtar, N., Shah, M., Mian, A.: Regulating model reliance on non-robust features by smoothing input marginal density. arXiv preprint arXiv:2407.04370 (2024) 1 38. Yang, P., Akhtar, N., Wen, Z., Mian, A.: Local path integration for attribution. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 37, pp. 3173–3180 (2023) 1 39. Yang, P., Akhtar, N., Wen, Z., Shah, M., Mian, A.: Re-calibrating feature attri- butions for model interpretation. In: International Conference on Learning Repre- sentations (2022) 1 40. Zimmer, L., Lindauer, M.T., Hutter, F.: Auto-Pytorch: Multi-fidelity metalearn- ing for efficient and robust AutoDL. IEEE Transactions on Pattern Analysis and Machine Intelligence43, 3079–3090 (2021) 4",
      "meta_data": {
        "arxiv_id": "2409.00584v1",
        "authors": [
          "Jiantong Jiang",
          "Ajmal Mian"
        ],
        "published_date": "2024-09-01T02:40:04Z",
        "pdf_url": "https://arxiv.org/pdf/2409.00584v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The main research problem is the challenge of adaptively determining an appropriate fidelity for each hyperparameter configuration in multi-fidelity Bayesian Optimization (BO) to fit the surrogate model. The paper proposes FastBO, a multi-fidelity BO method that addresses this challenge by adaptively deciding the fidelity for each configuration based on novel concepts of efficient point and saturation point. FastBO is shown to efficiently offer strong performance and its adaptive fidelity identification strategy provides a general way to extend any single-fidelity method to the multi-fidelity setting.",
        "methodology": "FastBO is a multi-fidelity Bayesian Optimization method built upon two key concepts: the efficient point and the saturation point. The efficient point (ei) for a configuration λi is defined as the minimum resource level 'r' where doubling resources beyond 'r' results in a performance improvement below a small threshold (δ1). This point represents an optimal resource-to-performance balance and is used as the appropriate fidelity for fitting the surrogate model. The saturation point (si) is defined as the minimum resource level 'r' where performance stabilizes (changes below δ2) beyond 'r', serving as an approximate final fidelity. The FastBO process involves an initial warm-up stage, learning curve estimation for configurations, adaptive extraction of efficient and saturation points, evaluation of configurations up to their efficient points to update the surrogate model, and a post-processing stage where a small set of promising configurations are further evaluated to their saturation points to find the optimal. This adaptive strategy can also be generalized to extend single-fidelity methods.",
        "experimental_setup": "FastBO's performance was evaluated by comparing it against ten other methods: random search (RS), standard BO, ASHA, Hyperband, PASHA, A-BOHB, A-CQR, BOHB, DyHPO, and Hyper-Tune. The experiments were conducted using three established benchmarks: LCBench, NAS-Bench-201, and FCNet. The primary validation metric focused on assessing the \"anytime performance\" of the algorithms.",
        "limitations": "Not mentioned",
        "future_research_directions": "Future research could focus on refining and expanding FastBO to effectively operate within larger search spaces and integrate with distributed computing systems. These extensions aim to improve its general applicability and scalability in more complex and resource-intensive scenarios."
      }
    },
    {
      "title": "Meta-learning Hyperparameter Performance Prediction with Neural Processes"
    },
    {
      "title": "Bayesian Optimization for Iterative Learning",
      "abstract": "The performance of deep (reinforcement) learning systems crucially depends on\nthe choice of hyperparameters. Their tuning is notoriously expensive, typically\nrequiring an iterative training process to run for numerous steps to\nconvergence. Traditional tuning algorithms only consider the final performance\nof hyperparameters acquired after many expensive iterations and ignore\nintermediate information from earlier training steps. In this paper, we present\na Bayesian optimization (BO) approach which exploits the iterative structure of\nlearning algorithms for efficient hyperparameter tuning. We propose to learn an\nevaluation function compressing learning progress at any stage of the training\nprocess into a single numeric score according to both training success and\nstability. Our BO framework is then balancing the benefit of assessing a\nhyperparameter setting over additional training steps against their computation\ncost. We further increase model efficiency by selectively including scores from\ndifferent training steps for any evaluated hyperparameter set. We demonstrate\nthe efficiency of our algorithm by tuning hyperparameters for the training of\ndeep reinforcement learning agents and convolutional neural networks. Our\nalgorithm outperforms all existing baselines in identifying optimal\nhyperparameters in minimal time.",
      "full_text": "Bayesian Optimization for Iterative Learning Vu Nguyen ∗ University of Oxford vu@robots.ox.ac.uk Sebastian Schulze ∗ University of Oxford sebastian.schulze@eng.ox.ac.uk Michael A. Osborne University of Oxford mosb@robots.ox.ac.uk Abstract The performance of deep (reinforcement) learning systems crucially depends on the choice of hyperparameters. Their tuning is notoriously expensive, typically requiring an iterative training process to run for numerous steps to convergence. Traditional tuning algorithms only consider the ﬁnal performance of hyperparam- eters acquired after many expensive iterations and ignore intermediate information from earlier training steps. In this paper, we present a Bayesian optimization (BO) approach which exploits the iterative structure of learning algorithms for efﬁcient hyperparameter tuning. We propose to learn an evaluation function compress- ing learning progress at any stage of the training process into a single numeric score according to both training success and stability. Our BO framework is then balancing the beneﬁt of assessing a hyperparameter setting over additional train- ing steps against their computation cost. We further increase model efﬁciency by selectively including scores from different training steps for any evaluated hyper- parameter set. We demonstrate the efﬁciency of our algorithm by tuning hyperpa- rameters for the training of deep reinforcement learning agents and convolutional neural networks. Our algorithm outperforms all existing baselines in identifying optimal hyperparameters in minimal time. 1 Introduction Deep learning (DL) and deep reinforcement learning (DRL) have led to impressive breakthroughs in a broad range of applications such as game play [26, 36], motor control [43], and image recognition [20]. To maintain general applicability, these algorithms expose sets of hyperparameters to adapt their behavior to any particular task at hand. This ﬂexibility comes at the price of having to tune an additional set of parameters – poor settings lead to drastic performance losses [11, 30, 37]. On top of being notoriously sensitive to these choices, deep (reinforcement) learning systems often have high training costs, in computational resources and time. For example, a single training run on the Atari Breakout game took approximately 75 hours on a GPU cluster [26]. Tuning DRL parameters is further complicated as only noisy evaluations of an agent’s ﬁnal performance are obtainable. Bayesian optimization (BO) [12, 28, 35] has recently achieved considerable success in optimizing these hyperparameters. This approach casts the tuning process as a global optimization problem based on noisy evaluations of a black-box function f . BO constructs a surrogate model typically using a Gaussian process (GP) [31], over this unknown function. This GP surrogate is used to build an acquisition function [13, 44] which suggests the next hyperparameter to evaluate. In modern machine learning (ML) algorithms [15], the training process is often conducted in an iterative manner. A natural example is given by deep learning where training is usually based on stochastic gradient descent and other iterative procedures. Similarly, the training of reinforcement learning agents is mostly carried out using multiple episodes. The knowledge accumulated during these training iterations can be useful to inform BO. However, most existing BO approaches [35] ∗These authors contributed equally. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada. arXiv:1909.09593v5  [cs.LG]  16 Jan 2021deﬁne the objective function as the average performance over the ﬁnal training iterations. In doing so, they ignore the information contained in the preceding training steps. In this paper, we present a Bayesian optimization approach for tuning algorithms where iterative learning is available – the cases of deep learning and deep reinforcement learning. First, we consider the joint space of input hyperparameters and number of training iterations to capture the learning progress at different time steps in the training process. We then propose to transform the whole training curve into a numeric score according to user preference. To learn across the joint space efﬁciently, we introduce a data augmentation technique leveraging intermediate information from the iterative process. By exploiting the iterative structure of training procedures, we encourage our algorithm to consider running a larger number of cheap (but high-utility) experiments, when cost- ignorant algorithms would only be able to run a few expensive ones. We demonstrate the efﬁciency of our algorithm on training DRL agents on several well-known benchmarks as well as the training of convolutional neural networks. In particular, our algorithm outperforms existing baselines in ﬁnding the best hyperparameter in terms of wall-clock time. Our main contributions are: • an algorithm to optimize the learning curve of a ML algorithm by using training curve compression, instead of averaged ﬁnal performance; • an approach to learn the compression curve from the data and a data augmentation tech- nique for increased sample-efﬁciency; • demonstration on tuning DRL and convolutional neural networks. 2 Related Work in Iteration-Efﬁcient Bayesian Optimization The ﬁrst algorithm category employs stopping criteria to terminate some training runs early and allo- cate resources towards more promising settings. These criteria typically involve projecting towards a ﬁnal score from early training stages. Freeze-thaw BO [42] models the training loss over time us- ing a GP regressor under the assumption that the training loss roughly follows an exponential decay. Based on this projection, training resources are allocated to the most promising settings. Hyperband [8, 23] dynamically allocates computational resources (e.g. training epochs or dataset size) through random sampling and eliminates under-performing hyperparameter settings by successive halving. Attempts have also been made to improve the epoch efﬁciency of other hyperparameter optimization algorithms in [5, 7, 18] which predict the ﬁnal learning outcome based on partially trained learning curves to identify hyperparameter settings that are expected to under-perform and early-stop them. In the context of DRL, however, these stopping criteria, including the exponential decay assumed in Freeze-thaw BO [42], may not be applicable, due to the unpredictable ﬂuctuations of DRL reward curves. In the supplement, we illustrate the noisiness of DRL training. The second category [16, 17, 23, 41, 48] aims to reduce the resource consumption of BO by utilizing low-ﬁdelity functions which can be obtained by using a subset of the training data or by training the ML model for a small number of iterations. Multi-task BO [41] requires the user to deﬁne a division of the dataset into pre-deﬁned and discrete subtasks. Multi-ﬁdelity BO with continuous approximation (BOCA) [16] and hierarchical partition [34] extend this idea to continuous settings. Speciﬁcally, BOCA ﬁrst selects the hyperparameter input and then the corresponding ﬁdelity to be evaluated at. The ﬁdelity in this context refers to the use of different number of learning iterations. Analogous to BOCA’s consideration of continuous ﬁdelities, Fabolas [17] proposes to model the combined space of input hyperparameter and dataset size and then select the optimal input and dataset size jointly. The above approaches typically identify performance of hyperparameters via the average (either training or validation) loss of the last learning iterations. Thereby, they do not account for potential noise in the learning process (e.g., they might select unstable settings that jumped to high perfor- mance in the last couple of iterations). 3 Bayesian Optimization for Iterative Learning (BOIL) Problem setting. We consider training a machine learning algorithm given a d-dimensional hy- perparameter x ∈X ⊂Rd for t iterations. This process has a training time costc(x,t) and produces 20 100 200 300 400 500 #Episode t 0.80 0.85 0.90 0.95 1.00x Tmin Tmax Augmented Obs Observation 0 100 200 300 400 500 0 50 100 150 200Score 4 18 34 8 45 5 14 26Reward Curve Sigmoid Func 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l( . |m * 0 ,g* 0 ) for Cifar10 m* 0 =-4.0 g* 0 =1.476 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l( . |m * 0 ,g* 0 ) for Reacher m* 0 =2.779 g* 0 =1.973 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l( . |m * 0 ,g* 0 ) for CartPole m* 0 =-3.266 g* 0 =3.0 Figure 1: Left: the score in pink box is a convolution of the reward curve r(·| x = 0.9,t = 500) and a Sigmoid function l(u |g0,m0) = 1 1+exp(−g0[u−m0]) up to time step t. Bottom: observations are selected to augment the dataset (red dots). The heatmap indicates the GP predictive mean µ for f across the number of episodest used to train an agent. Tmin and Tmax are two user-deﬁned thresholds for the number of training episodes. x is a hyperparameter to be tuned. Right: we learn the optimal parameter g∗ 0 and m∗ 0 for each experiment separately. training evaluations r(·| x,t) for t iterations, t ∈[Tmin,Tmax]. These could be episode rewards in DRL or training accuracies in DL. An important property of iterative training is that we know the whole curve at preceding steps r(t′|x,t), ∀t′≤t. Given the raw training curve r(·| x,t), we assume an underlying smoothed black-box function f , deﬁned in Sec. 3.2. Formally, we aim to ﬁnd x∗= argmaxx∈X f (x,Tmax); at the same time, we want to keep the overall training time, ∑N i=1 c(xi,ti), of evaluated settings [xi,ti] as low as possible. We summarize our variables in Table 1 in the supplement for ease of reading. 3.1 Selecting a next point using iteration-efﬁcient modeling We follow popular designs in [17, 19, 39, 41] and model the cost-sensitive black-box function as f (x,t) ∼GP(0,k([x,t],[x′,t′])), where k is an appropriate covariance functions and [x,t] ∈Rd+1. For simplicity and robustness, the cost function c(x,t) is approximated by a linear regressor. De- pending on the setting, it may be more appropriate to employ a second GP or different parametric model if the cost has a more complex dependence on hyperparameters x and iterations t. We regu- larly (re-)optimize both kernel and cost function parameters in between point acquisitions. More speciﬁcally, we choose the covariance function as a productk ([x,t],[x′,t′]) =k(x,x′)×k(t,t′) to induce joint similarities over parameter and iteration space. We estimate the predictive mean and uncertainty for a GP [31] at any input z∗= [x∗,t∗] as µ (z∗) =k∗ [ K +σ2 y I ]−1 y (1) σ2 (z∗) =k∗∗−k∗ [ K +σ2 y I ]−1 kT ∗ (2) where y = [yi]∀i, k∗= [k (z∗,zi)]∀i, K = [k (zi,zj)]∀i, j, k∗∗= k (z∗,z∗), and σ2 y is the noise variance of f . Cost predictions at any particular parameter x and time t are given by µc([x∗,t∗]) =βT [x,t], where β is directly computed from data {Z = [xi,ti],c = [ci]}∀i as β = (ZT Z)−1Zc [1]. Our goal is to select a point with high function value (exploitation), high uncertainty (exploration) and low cost (cheap). At each iteration n, we query the input parameter xn and the number of iteration tn [38, 48]: zn = [xn,tn] = argmax x∈X ,t∈[Tmin,Tmax] α(x,t)/µc(x,t). (3) 3Although our framework is available for any acquisition choices [13, 22, 47], to cope with output noise, we follow [45] and slight modify the expected improvement criterion using the maximum mean GP prediction µmax n . Let λ = µn(z)−µmaxn σn(z) , we then have a closed-form for the new expected improvement (EI) as αEI n (z) =σn (z)φ (λ) + [µn (z)−µmax n ]Φ(λ) where φ is the standard normal p.d.f., Φ is the c.d.f, µn and σn are the GP predictive mean and variance deﬁned in Eq. (1) and Eq. (2), respectively. 3.2 Training curve compression and estimating the transformation function Existing BO approaches [4, 23] typically deﬁne the objective function as an average loss over the ﬁnal learning episodes. However, this does not take into consideration how stable performance is or the training stage at which it has been achieved. We argue that averaging learning losses is likely misleading due to the noise and ﬂuctuations of our observations (learning curves) – particularly during the early stages of training. We propose to compress the whole learning curve into a numeric score via a preference function representing the user’s desired training curve. In the following, we use the Sigmoid function (speciﬁcally the Logistic function) to compute the utility score as y = ˆy(r,m0,g0) =r(·|x,t)•l(·|m0,g0) = t ∑ u=1 r(u |x,t) 1 +exp(−g0 [u −m0]) (4) where •is a dot product, a Logistic function l(·| m0,g0) is parameterized by a growth parameter g0 deﬁning a slope and the middle point of the curve m0. The optimal parameters g0 and m0 are estimated directly from the data. We illustrate different shapes of l parameterized by g0 and m0 in the appendix. The Sigmoid preference has a number of desirable properties. As early weights are small, less credit is given to ﬂuctuations at the initial stages, making it less likely for our surrogate to be biased towards randomly well performing settings. However, as weights monotonically increase, hyperparameters with improving performance are preferred. As weights saturate over time, stable, high performing conﬁgurations are preferred over short “performance spikes” characteristic of un- stable training. Lastly, this utility score assigns higher values to the same performance if it is being maintained over more episodes. Learning the transformation function from data. Different compression curves l(), parameter- ized by different choices of g0 and m0 in Eq. (4), may lead to different utilities y and thus affect the performance. The optimal values of g∗ 0 and m∗ 0 are unknown in advance. Therefore, we propose to learn these values g∗ 0 and m∗ 0 directly from the data. Our intuition is that the ‘optimal’ compression curve l(m∗ 0,g∗ 0) will lead to a better ﬁt of the GP. This better GP surrogate model, thus, will result in better prediction as well as optimization performance. We parameterize the GP log marginal likelihood L [31] as the function of m0 and g0: L(m0,g0) =1 2 ˆyT ( K +σ2 y I )−1 ˆy −1 2 ln ⏐⏐K +σ2 y I ⏐⏐ +const (5) where σ2 y is the output noise variance, ˆy is the function of m0 and g0 deﬁned in Eq. (4). We optimize m0 and g0 (jointly with other GP hyperparameters) using multi-start gradient descent. We derive the derivative ∂L ∂m0 = ∂L ∂ ˆy ∂ ˆy ∂m0 and ∂L ∂g0 = ∂L ∂ ˆy ∂ ˆy ∂g0 which can be computed analytically as: ∂L ∂ ˆy = ( K +σ2 y IN )−1 ˆy; ∂ ˆy ∂m0 = −g0 ×exp(−g0 [u −m0]) [1 +exp(−g0 [u −m0])]2 ; ∂ ˆy ∂g0 = −m0 ×exp(−g0 [u −m0]) [1 +exp(−g0 [u −m0])]2 . The estimated compression curves are illustrated in Right Fig. 1 and in Sec. 4.1. 3.3 Augmenting the training data When evaluating a parameter x over t iterations, we obtain not only a ﬁnal score but also all reward sequences r(t′|x,t),∀t′= 1,..., t. The auxiliary information from the curve can be useful for BO. Therefore, we propose to augment the information from the curve into the sample set of our GP model. A naïve approach for augmentation is to add a full curve of points {[x, j],yj}t j=1 where yj is computed using Eq. (4). However, this approach can be redundant and may im- pose serious issues in the conditioning of the GP covariance matrix. As we cluster 40.80 0.85 0.90 0.95 1.00 200 300 400 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 x GP variance 2 0 2 4 6 8 10 12 0.000 0.025 0.050 0.075 0.100 0.125 0.150 0.175 0.80 0.85 0.90 0.95 1.00 200 300 400 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 GP variance 400 320 240 160 80 0 80 160 240 0.000 0.005 0.010 0.015 0.020 0.025 0.030 0.035 0.040 x Figure 2: GP with different settings. Left: our augmentation. Right: using a full curve. If we add too many observations, the GP covariance matrix becomes ill-conditioned. On the right, the GP ﬁt is poor with a large mean estimate range of [−400,240] even though the output is standardized N (0,1). All x-axis are over x, a hyperparameter to be tuned. more evaluations closely, the conditioning of the GP covariance degrades further, as dis- cussed in [24]. This conditioning issue is especially serious in our noisy DRL settings. 0 10 20 30 40 50 60 Iterations 0 5 10 15 20 25Log of Condition Number Condition Number of GP Covariance Augmentation No Augmentation Full Curve Reasonable Threshold Figure 3: The condition number of GP covari- ance matrix deteriorates if we add the whole curve of points into a GP. The large condition number indicates the nearness to singularity. We highlight this effect on GP estimation in Fig. 2 wherein the GP mean varies erratically when the natural log of the condition number of the GP co- variance matrix goes above 25 (see Fig. 3) as we include the whole curve. Selecting subset of points from the curve. Dif- ferent solutions, such as the addition of artiﬁcial noise or altering the kernel’s length-scales, have been proposed. We decide to use an active learn- ing approach [10, 29] as sampled data points are expected to contain a lot of redundant informa- tion. As a consequence, the loss of information from sub-sampling the data should be minimal and information-eroding modiﬁcation of the ker- nel matrix itself can be avoided. As a side beneﬁt, the reduced number of sampled points speeds up inference in our GP models. In particular, we select samples at the maximum of the GP predictive uncertainty. Formally, we sequentially select a set Z = [z1,...zM], zm = [x,tm], by varying tm while keeping x ﬁxed as zm =argmax ∀t′≤t σ([x,t′] |D′),∀m ≤M s.t. lnof cond(K) ≤δ (6) where D′= D∪{zj = [x,tj]}m−1 j=1 . This sub-optimisation problem is done in a one-dimensional space of t′∈{Tmin,..., t}, thus it is cheap to optimize using (multi-start) gradient descent (the derivative of GP predictive variance is available [31]). Alternatively, a ﬁxed-size grid could be considered, but this could cause conditioning issues when a point in the grid [ x,tgrid ] is placed near another existing point [ x′,tgrid ] , i.e., ||x −x′||2 ≤ε for some small ε. These generated points Z are used to calculate the output r(zm) and augmented into the observation set (X,Y ) for ﬁtting the GP. The number of samplesM is adaptively chosen such that the natural log of the condition number of the covariance matrix is less than a threshold. This is to ensure that the GP covariance matrix condition number behaves well by reducing the number of unnecessary points added to the GP at later stages. We compute the utility score ym given zm for each augmented point using Eq. (4). In addition, we can estimate the running time cm using the predictive mean µc(zm). We illustrate the augmented observations and estimated scores in Fig. 1. We summarize the overall algorithm in Alg. 1. To enforce non-negativity and numerical stability, we make use of the transformations α ←log[1 +exp(α)] and µc ←log[1 +exp(µc)]. 4 Experiments We assess our model by tuning hyperparameters for two DRL agents on three environments and a CNN on two datasets. We provide additional illustrations and experiments in the appendix. 5Algorithm 1 Bayesian Optimization with Iterative Learning (BOIL) Input: #iter N, initial data D0, z = [x,t]. Output: optimal x∗and y∗= max∀y∈DN y 1: for n = 1....N do 2: Fit a GP to estimate µf (),σf () from Eqs. (1,2) and a LR for cost µc() 3: Select zn = argmaxx,t α(x,t)/µc(x,t) and observe a curve r and a cost c from f (zn) 4: Compressing the learning curve r(zn) into numeric score using Eq. (4). 5: Sample augmented points zn,m,yn,m,cn,m,∀m ≤M given the curve and Dn in Eq. (6) 6: Augment the data into Dn and estimate Logistic curve hyperparameters m0 and g0. 7: end for Experimental setup. All experimental results are averaged over 20 independent runs with differ- ent random seeds. Final performance is estimated by evaluating the chosen hyperparameter over the maximum number of iterations. All experiments are executed on a NVIDIA 1080 GTX GPU using the tensorﬂow-gpu Python package. The DRL environments are available through the OpenAI gym [3] and Mujoco [43]. Our DRL implementations are based on the open source from Open AI Baselines [6]. We release our implementation at https://github.com/ntienvu/BOIL. We use square-exponential kernels for the GP in our model and estimate their parameters by maxi- mizing the marginal likelihood [31]. We set the maximum number of augmented points to beM = 15 and a threshold for a natural log of GP condition numberδ = 20. We note that the optimization over- head is much less than the black-box function evaluation time. Baselines. We compare with Hyperband [23] which demonstrated empirical successes in tuning deep learning applications in an iteration-efﬁcient manner. We extend the discrete multi-task BO [41] to the continuous case – which can also be seen as continuous multi-ﬁdelity BO [16, 39] as in our setting, they both consider cost-sensitivity and iteration-efﬁciency. We, therefore, label the two baselines as continuous multi-task/ﬁdelity BO (CM-T/F-BO). We have ignored the minor difference in these settings, such as multi-task approaches jointly optimizes the ﬁdelity and input while BOCA [16] ﬁrst selects the input and then the ﬁdelity. Our focus is to demonstrate the effectiveness of optimizing the learning curve using compression and augmentation techniques. We therefore omit the comparison of various acquisition functions and kernel choices which can easily be used in our model. We also do not compare with Fabolas [17] which is designed to vary dataset sizes, not iteration numbers. We would expect the performance of Fabolas to be close to CM-T/F-BO. We are unable to compare with FreezeThaw as the code is not available. However, the curves in our setting are not exponential decays and thus ill-suited to their model (see last ﬁgure in the appendix). We have considered an ablation study in the appendix using a time kernel following the exponential decay proposed in Freeze-thaw method [42]. Task descriptions. We consider three DRL settings including a Dueling DQN (DDQN) [46] agent in the CartPole-v0 environment and Advantage Actor Critic (A2C) [25] agents in the InvertedPendulum-v2 and Reacher-v2 environments. In addition to the DRL applications, we tune 6 hyperparameters for training a convolutional neural network [21] on the SVHN dataset and CI- FAR10. Due to space considerations, we refer to the appendix for further details. 4.1 Model illustration /uni00000013/uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni00000013 /uni00000018 /uni00000014/uni00000013 /uni00000014/uni00000018/uni00000006/uni00000024/uni00000058/uni0000004a/uni00000050/uni00000048/uni00000051/uni00000057/uni00000048/uni00000047/uni00000003/uni00000032/uni00000045/uni00000056 /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni00000057/uni00000048/uni00000047/uni00000003/uni00000024/uni00000058/uni0000004a/uni00000050/uni00000048/uni00000051/uni00000057/uni00000048/uni00000047/uni00000003/uni00000032/uni00000045/uni00000056 Figure 4: DDQN on CartPole. The number of augmented observations reduces over time. We ﬁrst illustrate the estimated compression func- tion l(m∗ 0,g∗ 0) in Right Fig. 1 from different experi- ments. These Logistic parameters g∗ 0 and m∗ 0 are es- timated by maximizing the GP marginal likelihood and used for compressing the curve. We show that the estimated curve from CartPole tends to reach the highest performance much earlier than Reacher because CartPole is somewhat easier to train than Reacher. We next examine the count of augmented observa- tions generated per iteration in Fig. 4. Although this number is ﬂuctuating, it tends to reduce over 6/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000018/uni00000013/uni00000013 /uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048 /uni0000001a/uni00000013 /uni00000019/uni00000013 /uni00000018/uni00000013 /uni00000017/uni00000013 /uni00000016/uni00000013 /uni00000015/uni00000013 /uni00000014/uni00000013 /uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni00000035/uni00000048/uni00000044/uni00000046/uni0000004b/uni00000048/uni00000055/uni00000040/uni00000003/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047/uni00000003/uni00000058/uni00000056/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000032/uni00000053/uni00000057/uni0000004c/uni00000050/uni0000004c/uni0000005d/uni00000048/uni00000047/uni00000003/uni00000033/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013/uni00000015/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013/uni0000001b/uni00000013/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000014/uni00000015/uni00000013/uni00000013/uni00000014/uni00000017/uni00000013/uni00000013 /uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048 /uni00000015/uni00000018 /uni00000018/uni00000013 /uni0000001a/uni00000018 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000015/uni00000018 /uni00000014/uni00000018/uni00000013 /uni00000014/uni0000001a/uni00000018 /uni00000015/uni00000013/uni00000013/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni0000002c/uni00000059/uni00000033/uni00000048/uni00000051/uni00000040/uni00000003/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047/uni00000003/uni00000058/uni00000056/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000032/uni00000053/uni00000057/uni0000004c/uni00000050/uni0000004c/uni0000005d/uni00000048/uni00000047/uni00000003/uni00000033/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000025/uni00000032/uni0000002c/uni0000002f Figure 5: The learning curves of the best found parameters by different approaches. The curves show that BO-L and BOIL reliably identify parameters leading to stable training. BOIL takes only half total time to ﬁnd this optimal curve. time. BOIL does not add more augmented observations at the later stage when we have gained sufﬁcient information and GP covariance conditioning falls below our threshold δ = 20. 4.2 Ablation study of curve compression To demonstrate the impact of our training curve compression, we compare BOIL to vanilla Bayesian optimization (BO) and with compression (BO-L) given the same number of iterations at Tmax. We show that using the curve compression leads to stable performance, as opposed to the existing tech- nique of averaging the last iterations. We plot the learning curves of the best hyperparameters identiﬁed by BO, BO-L and BOIL. Fig. 5 shows the learning progress over Tmax episodes for each of these. The curves are smoothed by averaging over 100 consecutive episodes for increased clarity. We ﬁrst note that all three algorithms eventually obtain similar performance at the end of learning. However, since BO-L and BOIL take into account the preceding learning steps, they achieve higher performance more quickly. Furthermore, they achieve this more reliably as evidenced by the smaller error bars (shaded regions). 4.3 Tuning deep reinforcement learning and CNN We now optimize hyperparameters for deep reinforcement learning algorithms; in fact, this applica- tion motivated the development of BOIL. The combinations of hyperparameters to be tuned, target DRL algorithm and environment can be found in the appendix. Comparisons by iterations and real-time. Fig. 6 illustrates the performance of different algo- rithms against the number of iterations as well as real-time (the plots for CIFAR10 are in the ap- pendix). The performance is the utility score of the best hyperparameters identiﬁed by the baselines. Across all three tasks, BOIL identiﬁes optimal hyperparameters using signiﬁcantly less computation time than other approaches. The plots show that other approaches such as BO and BO-L can identify well-performing hyperpa- rameters in fewer iterations than BOIL. However, they do so only considering costly, high-ﬁdelity evaluations resulting in signiﬁcantly higher evaluation times. In contrast to this behavior, BOIL ac- counts for the evaluation costs and chooses to initially evaluate low-ﬁdelity settings consuming less time. This allows fast assessments of a multitude of hyperparameters. The information gathered here is then used to inform later point acquisitions. Hereby, the inclusion of augmented observations is crucial in offering useful information readily available from the data. In addition, this augmenta- tion is essential to prevent from the GP kernel issue instead of adding the full curve of points into our GP model. Hyperband [23] exhibits similar behavior in that it uses low ﬁdelity (small t) evaluations to reduce a pool of randomly sampled conﬁgurations before evaluating at high ﬁdelity (large t). To deal with noisy evaluations and other effects, this process is repeated several times. This puts Hyperband at a disadvantage particularly in the noisy DRL tasks. Since early performance ﬂuctuates hugely, Hyperband can be misled in where to allocate evaluation effort. It is then incapable of revising 7/uni00000013 /uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni00000014/uni00000014/uni0000001a/uni00000011/uni00000018 /uni00000014/uni00000015/uni00000013/uni00000011/uni00000013 /uni00000014/uni00000015/uni00000015/uni00000011/uni00000018 /uni00000014/uni00000015/uni00000018/uni00000011/uni00000013 /uni00000014/uni00000015/uni0000001a/uni00000011/uni00000018 /uni00000014/uni00000016/uni00000013/uni00000011/uni00000013 /uni00000014/uni00000016/uni00000015/uni00000011/uni00000018 /uni00000014/uni00000016/uni00000018/uni00000011/uni00000013 /uni00000014/uni00000016/uni0000001a/uni00000011/uni00000018/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000027/uni00000027/uni00000034/uni00000031/uni00000010/uni00000026/uni00000044/uni00000055/uni00000057/uni00000033/uni00000052/uni0000004f/uni00000048/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000003/uni00000044/uni00000051/uni00000047/uni00000003 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013 /uni00000018/uni00000013/uni00000014/uni00000013/uni00000013/uni00000014/uni00000018/uni00000013/uni00000015/uni00000013/uni00000013/uni00000015/uni00000018/uni00000013/uni00000016/uni00000013/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni00000014/uni00000014/uni0000001a/uni00000011/uni00000018 /uni00000014/uni00000015/uni00000013/uni00000011/uni00000013 /uni00000014/uni00000015/uni00000015/uni00000011/uni00000018 /uni00000014/uni00000015/uni00000018/uni00000011/uni00000013 /uni00000014/uni00000015/uni0000001a/uni00000011/uni00000018 /uni00000014/uni00000016/uni00000013/uni00000011/uni00000013 /uni00000014/uni00000016/uni00000015/uni00000011/uni00000018 /uni00000014/uni00000016/uni00000018/uni00000011/uni00000013 /uni00000014/uni00000016/uni0000001a/uni00000011/uni00000018/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000027/uni00000027/uni00000034/uni00000031/uni00000010/uni00000026/uni00000044/uni00000055/uni00000057/uni00000033/uni00000052/uni0000004f/uni00000048/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000003/uni00000044/uni00000051/uni00000047/uni00000003 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013 /uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni00000014/uni00000015/uni0000001b /uni00000014/uni00000015/uni0000001c /uni00000014/uni00000016/uni00000013 /uni00000014/uni00000016/uni00000014 /uni00000014/uni00000016/uni00000015 /uni00000014/uni00000016/uni00000016 /uni00000014/uni00000016/uni00000017/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni00000035/uni00000048/uni00000044/uni00000046/uni0000004b/uni00000048/uni00000055/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013 /uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni00000014/uni00000015/uni0000001b /uni00000014/uni00000015/uni0000001c /uni00000014/uni00000016/uni00000013 /uni00000014/uni00000016/uni00000014 /uni00000014/uni00000016/uni00000015 /uni00000014/uni00000016/uni00000016 /uni00000014/uni00000016/uni00000017/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni00000035/uni00000048/uni00000044/uni00000046/uni0000004b/uni00000048/uni00000055/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013 /uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni00000015/uni00000018 /uni00000016/uni00000013 /uni00000016/uni00000018 /uni00000017/uni00000013 /uni00000017/uni00000018 /uni00000018/uni00000013 /uni00000018/uni00000018 /uni00000019/uni00000013 /uni00000019/uni00000018/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c /uni0000003e/uni00000026/uni00000031/uni00000031/uni00000010/uni00000036/uni00000039/uni0000002b/uni00000031/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000019/uni00000003/uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000053/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013/uni0000001a/uni00000013/uni0000001b/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni00000015/uni00000018 /uni00000016/uni00000013 /uni00000016/uni00000018 /uni00000017/uni00000013 /uni00000017/uni00000018 /uni00000018/uni00000013 /uni00000018/uni00000018 /uni00000019/uni00000013 /uni00000019/uni00000018/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c /uni0000003e/uni00000026/uni00000031/uni00000031/uni00000010/uni00000036/uni00000039/uni0000002b/uni00000031/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000019/uni00000003/uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000053/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f Figure 6: Comparison over BO evaluations (Left) and real-time (Right). Given the same time bud- get, CM-T/F-BO, Hyperband and BOIL can take more evaluations than vanilla BO, BO-L and Rand. BOIL outperforms other competitors in ﬁnding the optimal parameters in an iteration-efﬁcient man- ner. CM-T/F-BO does not augment the observations from the curve and requires more evaluations. The results of InvertedPendulum and CNN-CIFAR10 are in the appendix. these choices until an entirely new pool of hyperparameters is sampled and evaluated from scratch. In contrast to this, BOIL is more ﬂexible than Hyperband in that it can freely explore-exploit the whole joint space. The GP surrogate hereby allows BOIL to generalize across hyperparameters and propagate information through the joint space. 5 Conclusion and Future work Our framework complements the existing BO toolbox for hyperparameter tuning with iterative learn- ing. We present a way of leveraging our understanding that later stages of the training process are informed by progress made in earlier ones. This results in a more iteration-efﬁcient hyperparame- ter tuning algorithm that is applicable to a broad range of machine learning systems. We evaluate its performance on a set of diverse benchmarks. The results demonstrate that our model surpasses the performance of well-established alternatives while consuming signiﬁcantly fewer resources. Fi- nally, we note that our approach is not necessarily speciﬁc to machine learning algorithms, but more generally applies to any process exhibiting an iterative structure to be exploited. 86 Broader Impact Our work aims at making the optimization of processes operating in a step-wise fashion more efﬁ- cient. As demonstrated this makes BOIL particularly well-suited to supporting supervised learning models and RL systems. By increasing training efﬁcience of these models, we hope to contribute to their widespread deployment whilst reducing the computational and therefore environmental cost their implementation has. Deep (reinforcement) learning systems ﬁnd application in a wide range of settings that directly contribute to real world decisions, e.g., natural language processing, visual task, autonomous driving and many more. As machine learning models building on our contributions are being deployed in the real world, we encourage practicioners to put in place necessary supervision and override mechanisms as precautions against potential failure. In a more general context, our algorithm may be seen as a step towards the construction of an automated pipeline for the training and deployment of machine learning models. A potential danger is that humans become further and further removed from the modelling process, making it harder to spot (potentially critical) failures. We do not see this as an argument against the construction of such a pipeline in principle, but instead encourage practicioners to reﬂect on potential biases indirectly encoded in the choice of data sets and models, they are feeding into said automated processes. The growing opacity of machine learning models is a concern of its own and which automated training procedures will only contribute to. Opposing this is a rapidly growing corpus of work addressing the interpretability of trained machine learning models and their decision making. These can and should be used to rigorously analyse ﬁnal training outcomes. Only then can we ensure that machine learning algorithm do indeed become a beneﬁcial source of information guiding real world policy making as opposed to opaque, unquestioned entities. While our main interest lies in the hyperparameter optimization of machine learning models, it should be noted that any iterative process depending on a set of parameters can make use of our con- tributions. Possible settings could, for instance, include the optimization of manufacturing pipelines in which factory setting are adjusted to increase productivity. 7 Acknowledgements S. Schulze is supported by an I-CASE studentship funded by the EPSRC and Dyson. References [1] Christopher M Bishop. Pattern recognition and machine learning. springer New York, 2006. [2] Eric Brochu, Vlad M Cora, and Nando De Freitas. A tutorial on Bayesian optimization of ex- pensive cost functions, with application to active user modeling and hierarchical reinforcement learning. arXiv preprint arXiv:1012.2599, 2010. [3] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016. [4] Yutian Chen, Aja Huang, Ziyu Wang, Ioannis Antonoglou, Julian Schrittwieser, David Silver, and Nando de Freitas. Bayesian optimization in AlphaGo. arXiv preprint arXiv:1812.06855, 2018. [5] Zhongxiang Dai, Haibin Yu, Bryan Kian Hsiang Low, and Patrick Jaillet. Bayesian optimiza- tion meets Bayesian optimal stopping. In International Conference on Machine Learning , pages 1496–1506, 2019. [6] Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford, John Schulman, Szymon Sidor, Yuhuai Wu, and Peter Zhokhov. Openai baselines. GitHub, GitHub repository, 2017. [7] Tobias Domhan, Jost Tobias Springenberg, and Frank Hutter. Speeding up automatic hy- perparameter optimization of deep neural networks by extrapolation of learning curves. In Twenty-Fourth International Joint Conference on Artiﬁcial Intelligence, 2015. 9[8] Stefan Falkner, Aaron Klein, and Frank Hutter. Bohb: Robust and efﬁcient hyperparameter optimization at scale. In International Conference on Machine Learning , pages 1436–1445, 2018. [9] Peter I Frazier. A tutorial on Bayesian optimization. arXiv preprint arXiv:1807.02811, 2018. [10] Yarin Gal, Riashat Islam, and Zoubin Ghahramani. Deep Bayesian active learning with image data. In Proceedings of the 34th International Conference on Machine Learning, pages 1183– 1192, 2017. [11] Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. Deep reinforcement learning that matters. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence, 2018. [12] Philipp Hennig and Christian J Schuler. Entropy search for information-efﬁcient global opti- mization. Journal of Machine Learning Research, 13:1809–1837, 2012. [13] José Miguel Hernández-Lobato, Matthew W Hoffman, and Zoubin Ghahramani. Predictive entropy search for efﬁcient global optimization of black-box functions. In Advances in Neural Information Processing Systems, pages 918–926, 2014. [14] Donald R Jones, Matthias Schonlau, and William J Welch. Efﬁcient global optimization of expensive black-box functions. Journal of Global optimization, 13(4):455–492, 1998. [15] M. I. Jordan and T. M. Mitchell. Machine learning: Trends, perspectives, and prospects. Science, 349(6245):255–260, 2015. [16] Kirthevasan Kandasamy, Gautam Dasarathy, Jeff Schneider, and Barnabás Póczos. Multi- ﬁdelity Bayesian optimisation with continuous approximations. In Proceedings of the 34th International Conference on Machine Learning, pages 1799–1808, 2017. [17] Aaron Klein, Stefan Falkner, Simon Bartels, Philipp Hennig, and Frank Hutter. Fast Bayesian optimization of machine learning hyperparameters on large datasets. In Artiﬁcial Intelligence and Statistics, pages 528–536, 2017. [18] Aaron Klein, Stefan Falkner, Jost Tobias Springenberg, and Frank Hutter. Learning curve pre- diction with Bayesian neural networks. International Conference on Learning Representations (ICLR), 2017. [19] Andreas Krause and Cheng S Ong. Contextual Gaussian process bandit optimization. In Advances in Neural Information Processing Systems, pages 2447–2455, 2011. [20] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolutional neural networks. In Advances in Neural Information Processing Systems, pages 1097–1105, 2012. [21] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. [22] Benjamin Letham, Brian Karrer, Guilherme Ottoni, Eytan Bakshy, et al. Constrained Bayesian optimization with noisy experiments. Bayesian Analysis, 14(2):495–519, 2019. [23] Lisha Li and Kevin Jamieson. Hyperband: A novel bandit-based approach to hyperparameter optimization. Journal of Machine Learning Research, 18:1–52, 2018. [24] Mark McLeod, Stephen Roberts, and Michael A Osborne. Optimization, fast and slow: Op- timally switching between local and Bayesian optimization. In International Conference on Machine Learning, pages 3440–3449, 2018. [25] V olodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforce- ment learning. In International conference on machine learning, pages 1928–1937, 2016. [26] V olodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. NIPS Deep Learning Workshop, 2013. [27] Vu Nguyen, Sunil Gupta, Santu Rana, Cheng Li, and Svetha Venkatesh. Regret for expected improvement over the best-observed value and stopping condition. In Proceedings of The 9th Asian Conference on Machine Learning (ACML), pages 279–294, 2017. [28] Vu Nguyen and Michael A Osborne. Knowing the what but not the where in Bayesian opti- mization. In International Conference on Machine Learning, pages 7317–7326, 2020. 10[29] Michael Osborne, Roman Garnett, Zoubin Ghahramani, David K Duvenaud, Stephen J Roberts, and Carl E Rasmussen. Active learning of model evidence using Bayesian quadrature. In Advances in Neural Information Processing Systems, pages 46–54, 2012. [30] Jack Parker-Holder, Vu Nguyen, and Stephen Roberts. Provably efﬁcient online hyperparame- ter optimization with population-based bandits. In Advances in Neural Information Processing Systems, 2020. [31] Carl Edward Rasmussen. Gaussian processes for machine learning. 2006. [32] Binxin Ru, Mark McLeod, Diego Granziol, and Michael A Osborne. Fast information-theoretic Bayesian optimisation. In International Conference on Machine Learning, pages 4381–4389, 2018. [33] Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. International Conference on Learning Representations, 2016. [34] Rajat Sen, Kirthevasan Kandasamy, and Sanjay Shakkottai. Multi-ﬁdelity black-box opti- mization with hierarchical partitions. In International conference on machine learning, pages 4538–4547, 2018. [35] Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P Adams, and Nando de Freitas. Taking the human out of the loop: A review of Bayesian optimization. Proceedings of the IEEE , 104(1):148–175, 2016. [36] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanc- tot, et al. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484, 2016. [37] Leslie N Smith. A disciplined approach to neural network hyper-parameters: Part 1–learning rate, batch size, momentum, and weight decay. arXiv preprint arXiv:1803.09820, 2018. [38] Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical Bayesian optimization of ma- chine learning algorithms. In Advances in Neural Information Processing Systems , pages 2951–2959, 2012. [39] Jialin Song, Yuxin Chen, and Yisong Yue. A general framework for multi-ﬁdelity Bayesian optimization with Gaussian processes. In The 22nd International Conference on Artiﬁcial Intelligence and Statistics, pages 3158–3167, 2019. [40] Niranjan Srinivas, Andreas Krause, Sham Kakade, and Matthias Seeger. Gaussian process optimization in the bandit setting: No regret and experimental design. In Proceedings of the 27th International Conference on Machine Learning, pages 1015–1022, 2010. [41] Kevin Swersky, Jasper Snoek, and Ryan P Adams. Multi-task Bayesian optimization. In Advances in Neural Information Processing Systems, pages 2004–2012, 2013. [42] Kevin Swersky, Jasper Snoek, and Ryan Prescott Adams. Freeze-thaw Bayesian optimization. arXiv preprint arXiv:1406.3896, 2014. [43] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026–5033. IEEE, 2012. [44] Zi Wang and Stefanie Jegelka. Max-value entropy search for efﬁcient Bayesian optimization. In International Conference on Machine Learning, pages 3627–3635, 2017. [45] Ziyu Wang and Nando de Freitas. Theoretical analysis of Bayesian optimisation with unknown Gaussian process hyper-parameters. arXiv preprint arXiv:1406.7758, 2014. [46] Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and Nando Freitas. Dueling network architectures for deep reinforcement learning. In International Conference on Machine Learning, pages 1995–2003, 2016. [47] Jian Wu and Peter Frazier. The parallel knowledge gradient method for batch Bayesian opti- mization. In Advances In Neural Information Processing Systems, pages 3126–3134, 2016. [48] Jian Wu, Saul Toscano-Palmerin, Peter I Frazier, and Andrew Gordon Wilson. Practical multi- ﬁdelity Bayesian optimization for hyperparameter tuning. In 35th Conference on Uncertainty in Artiﬁcial Intelligence, 2019. 11The following sections are intended to give the reader further insights into our design choices and a deeper understanding of the algorithms properties. First, we give a brief overview of Bayesian optimization with Gaussian processes. We then illustrate our models behavior on a two dimensional problem. Last, we give further details of our experiments for reproducibility purposes. A Bayesian Optimization Preliminaries Bayesian optimization is a sequential approach to global optimization of black-box functions with- out making use of derivatives. It uses two components: a learned surrogate model of the objective function and an acquisition function derived from the surrogate for selecting new points to inform the surrogate with. In-depth discussions beyond our brief overview can be found in recent surveys [2, 9, 35]. Notation. We summarize all of the notations used in our model in Table 1 for ease of reading. A.1 Gaussian processes We present the GP surrogate model for the black-box function f [31]. A GP deﬁnes a probability distribution over functions f under the assumption that any subset of points {(xi, f (xi)}is normally distributed. Formally, this is denoted as: f (x) ∼GP ( m(x),k ( x,x′)) , where m(x) and k (x,x′) are the mean and covariance functions, given by m(x) =E[ f (x)] and k(x,x′) =E [ ( f (x)−m(x))( f (x′)−m(x′))T ] . Typically, the mean of the GP is assumed to be zero everywhere. The kernel k(x,x′) can be thought of as a similarity measure relating f (x) and f (x′). Numerous kernels encoding different prior be- liefs about f (x) have been proposed. A popular choice is given by the square exponential kernel k(x,x′) =σ2 f exp [ −(x −x′)2/2σ2 l ] . The length- and output-scales σ2 l and σ2 f regulate the maximal covariance between two points and can be estimated using maximum marginal likelihood. The SE kernel encodes the belief that nearby points are highly correlated as it is maximized at k(x,x) =σ2 f and decays the further x and x′are separated. For predicting f∗= f (x∗) at a new data point x∗, assuming a zero mean m(x) =0, we have: [ f f∗ ] ∼N ( 0, [ K kT ∗ k∗ k∗∗ ]) (7) where k∗∗= k (x∗,x∗), k∗= [k (x∗,xi)]∀i≤N and K = [k (xi,xj)]∀i, j≤N . The conditional probability of p( f∗|f ) follows a univariate Gaussian distribution as p( f∗|f ) ∼N ( µ (x∗),σ2 (x∗) ) . Its mean and variance are given by: µ (x∗) =k∗K−1y σ2 (x∗) =k∗∗−k∗K−1kT ∗. As GPs give full uncertainty information with any prediction, they provide a ﬂexible nonparametric prior for Bayesian optimization. We refer the interested readers to [31] for further details on GPs. A.2 Acquisition function Bayesian optimization is typically applied in settings in which the objective function is expensive to evaluate. To minimize interactions with that objective, an acquisition function is deﬁned to reason about the selection of the next evaluation point xt+1 = argmaxx∈X αt (x). The acquisition func- tion is constructed from the predictive mean and variance of the surrogate to be easy to evaluate and represents the trade-off between exploration (of points with high predictive uncertainty) and exploitation (of points with high predictive mean). Thus, by design the acquisition function can be maximized with standard global optimization toolboxes. Among the many acquisition functions [12, 13, 14, 32, 40, 44] available in the literature, the expected improvement [14, 27, 45] is one of the most popular. 12Table 1: Notation List Parameter Domain Meaning d integer, N dimension, no. of hyperparameters to be optimized x vector,Rd input hyperparameter N integer, N maximum number of BO iterations Tmin, Tmax integer, N the min/max no of iterations for training a ML algorithm t ∈[Tmin,...Tmax] index of training steps M integer, N the maximum number of augmentation. We set M = 15. δ scalar, R threshold for rejecting augmentation when ln of cond(K) > δ m ∈{1,...M} index of augmenting variables n ∈{1,..., N} index of BO iterations z = [x,t] vector, Rd+1 concatenation of the parameter x and iteration t cn,m scalar, R training cost (sec) yn scalar, R transformed score at the BO iteration n yn,m scalar, R transformed score at the BO iteration n, training step m α(x,t) function acquisition function for performance µc(x,t) function estimation of the cost by LR given x and t r(. |x,t) function a raw learning curve, r(x,t) = [r(1 |x,t),...r(t′|x,t),r(t |x,t)] f (x,t) function a black-box function which is compressed from the above f () l (. |m0,g0) function Logistic curve l(u |m0,g0) = 1 1+exp(−g0[u−m0]) g0, g∗ 0 scalar, R a growth parameter deﬁning a slope, g∗ 0 = argmaxg0 L m0, m∗ 0 scalar, R a middle point parameter, m∗ 0 = argmaxm0 L L scalar, R Gaussian process log marginal likelihood A.3 GP kernels and treatment of GP hyperparameters We present the GP kernels and treatment of GP hyperparameters for the black-box function f . Although the raw learning curve in DRL is noisy, the transformed version using our proposed curve compression is smooth. Therefore, we use two squared exponential kernels for input hyperparameter and training iteration, respectively. That iskx(x,x′) =exp ( −||x−x′||2 2σ2x ) and kt (t,t′) =exp ( −||t−t′||2 2σ2t ) where the observation x and t are normalized to [0,1]d and the outcome y is standardized y ∼ N (0,1) for robustness. As a result, our product kernel becomes k ( [x,t],[x′,t′] ) = k(x,x′)×k(t,t′) =exp ( −||x −x′||2 2σ2x −||t −t′||2 2σ2t ) . The length-scales σx and σt are learnable parameters indicating the variability of the function with regards to the hyperparameter input x and number of training iterations t. Estimating appropriate values for them is critical as this represents the GPs prior regarding the sensitivity of performance w.r.t. changes in the number of training iterations and hyperparameters. For extremely large σt we expect the objective function to change very little for different numbers of training iterations. For small σt by contrast we expect drastic changes even for small differences. We estimate these GP hyperparameters (including the length-scalesσx, σt and the output noise varianceσy) by maximizing their log marginal likelihood [31]. We optimize Eq. (5) with a gradient-based optimizer, providing the analytical gradient to the algo- rithm. We start the optimization from the previous hyperparameter values θprev. If the optimization fails due to numerical issues, we keep the previous value of the hyperparameters. We reﬁt the hy- perparameters every 3×d function evaluations where d is the dimension. B Algorithm Illustration and Further Experiments Fig. 7 and Fig. 8 illustrate the behavior of our proposed algorithm BOIL on the example of opti- mizing the discount factor γ of Dueling DQN [46] on the CartPole problem. The two settings differ in the inclusion augmented observations into BOIL in Fig. 7 and CM-T/F-BO (or BOIL without augmented observations) in Fig. 8. 130.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 1.2 0.8 0.4 0.0 0.4 0.8 1.2 1.6 2.0 2.4 0.00 0.15 0.30 0.45 0.60 0.75 0.90 1.05 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 0.0 0.4 0.8 1.2 1.6 2.0 2.4 2.8 3.2 3.6 0.3 0.6 0.9 1.2 1.5 1.8 2.1 2.4 2.7 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0 2.5 0.00 0.06 0.12 0.18 0.24 0.30 0.36 0.42 0.48 0.64 0.72 0.80 0.88 0.96 1.04 1.12 1.20 1.28 1.36 0.0 0.4 0.8 1.2 1.6 2.0 2.4 2.8 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 1.6 1.2 0.8 0.4 0.0 0.4 0.8 1.2 1.6 2.0 0.00 0.15 0.30 0.45 0.60 0.75 0.90 1.05 0.64 0.72 0.80 0.88 0.96 1.04 1.12 1.20 1.28 1.36 0.0 0.4 0.8 1.2 1.6 2.0 2.4 2.8 3.2 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 2.4 1.6 0.8 0.0 0.8 1.6 2.4 3.2 0.000 0.025 0.050 0.075 0.100 0.125 0.150 0.175 0.200 0.64 0.72 0.80 0.88 0.96 1.04 1.12 1.20 1.28 1.36 0.0 0.6 1.2 1.8 2.4 3.0 3.6 4.2 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 2.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0 0.00 0.15 0.30 0.45 0.60 0.75 0.90 1.05 0.64 0.72 0.80 0.88 0.96 1.04 1.12 1.20 1.28 0.0 0.4 0.8 1.2 1.6 2.0 2.4 2.8 3.2 3.6 0.0 0.4 0.8 1.2 1.6 2.0 2.4 2.8 3.2 3.6 Figure 7: Illustration of BOIL on a 2-dimensional optimization task of DDQN on CartPole. The augmented observations ﬁll the joint hyperparameter-iteration space quickly to inform our surrogate. Our decision balances utility α against cost τ for iteration-efﬁciency. Especially in situations of multiple locations sharing the same utility value, our algorithm prefers to select the cheapest option. Table 2: Dueling DQN algorithm on CartPole problem. Variables Min Max Best Found x∗ γ discount factor 0 .8 1 0 .95586 learning rate model 1 e−6 0.01 0 .00589 #Episodes 300 800 - 140.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 2.0 1.6 1.2 0.8 0.4 0.0 0.4 0.8 0.00 0.15 0.30 0.45 0.60 0.75 0.90 1.05 0.60 0.75 0.90 1.05 1.20 1.35 1.50 1.65 1.80 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 0.0 0.3 0.6 0.9 1.2 1.5 1.8 2.1 2.4 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 1.6 1.2 0.8 0.4 0.0 0.4 0.8 1.2 0.00 0.15 0.30 0.45 0.60 0.75 0.90 1.05 0.60 0.75 0.90 1.05 1.20 1.35 1.50 1.65 1.80 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 1.6 1.2 0.8 0.4 0.0 0.4 0.8 1.2 1.6 0.00 0.08 0.16 0.24 0.32 0.40 0.48 0.56 0.64 0.60 0.75 0.90 1.05 1.20 1.35 1.50 1.65 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 0.30 0.45 0.60 0.75 0.90 1.05 1.20 1.35 1.50 1.65 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 1.6 1.2 0.8 0.4 0.0 0.4 0.8 1.2 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 2.0 1.6 1.2 0.8 0.4 0.0 0.4 0.8 1.2 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 Figure 8: Illustration of the Continuous Multi task/ﬁdelity BO (CM-T/F-BO) -- this is the case of BOIL without using augmented observations (same setting as Fig. 7). This version leads to less efﬁcient optimization as the additional iteration dimension requires more evaluation than optimizing the hyperparameters on their own. 15Table 3: A2C algorithm on Reacher (left) and InvertedPendulum (right). Variables Min Max Best Found x∗ γ discount factor 0 .8 1 0 .8 learning rate actor 1 e−6 0.01 0 .00071 learning rate critic 1 e−6 0.01 0 .00042 #Episodes 200 500 - Min Max Best Found x∗ 0.8 1 0 .95586 1e−6 0.01 0 .00589 1e−6 0.01 0 .00037 700 1500 - Table 4: Convolutional Neural Network. Variables Min Max Best Found x∗ ﬁlter size 1 8 5 pool size 1 5 5 batch size 16 1000 8 learning rate 1 e−6 0.01 0 .000484 momentum 0 .8 0 .999 0 .82852 decay 0 .9 0 .999 0 .9746 number of epoch 30 150 - In both cases, we plot the GP predictive mean in Eq. (1), GP predictive variance in Eq. (2), the acquisition function in Eq. (3), the predicted function and the ﬁnal decision function in Eq. (8). These equations are deﬁned in the main manuscript. As shown in the respective ﬁgures the ﬁnal decision function balances between utility and cost of any pair (γ,t) to achieve iteration efﬁciency. Especially in situations where multiple locations share the same utility value, our decision will prefer to select the cheapest option. Using the augmented observations in Fig. 7, our joint space is ﬁlled quicker with points and the uncertainty (GP variance) across it reduces faster than in Fig. 8 – the case of vanilla CM-T/F-BO without augmenting obser- vations. A second advantage of having augmented observations is that the algorithm is discouraged to select the same hyperparameter setting at lower ﬁdelity than a previous evaluation. We do not add the full curve as it can be redundant while causing the conditioning problem of the GP covariance matrix. B.1 Experiment settings We summarize the hyperparameter search ranges for A2C on Reacher and InvertedPendulum in Table 3, CNN on SHVN in Table 4 and DDQN on CartPole in Table 2. Additionally, we present the best found parameter x∗for these problems. Further details of the DRL agents are listed in Table 5. B.2 Learning Logistic Function We ﬁrst present the Logistic curve l(u |x,t) = 1 1+exp(−g0[u−m0]) using different choices of g0 and m0 in Fig. 10. We then learn from the data to get the optimal choices g∗ 0 and m∗ 0 presented in Fig. 11. Table 5: Further speciﬁcation for DRL agents Hyperparameter Value A2C Critic-network architecture [32,32] Actor-network architecture [32,32] Entropy coefﬁcient 0 .01 Dueling DQN Q-network architecture [50,50] ε-greedy (start, ﬁnal, number of steps) (1.0,0.05,10000) Buffer size 10000 Batch size 64 PER-α [33] 1 .0 PER-β (start, ﬁnal, number of steps) (1.0,0.6,1000) 160 100 200 300 400 500 Episodes 70 60 50 40 30 20 Average Reward Preference Curve as Sigmoid Best Found Reward Curve Sigmoid Curve 0 100 200 300 400 500 Episodes 90 80 70 60 50 40 30 20 10 Reward Preference Curve as Sigmoid Best Found Reward Curve Sigmoid Curve 0 100 200 300 400 500 Episodes 60 50 40 30 20 10 Average Reward Preference Curve as Log Best Found Reward Curve Log Curve 0 100 200 300 400 500 Episodes 100 80 60 40 20 0 Reward Preference Curve as Log Best Found Reward Curve Log Curve 0 100 200 300 400 500 Episodes 60 50 40 30 20 Average Reward Preference Curve as Average Best Found Reward Curve Average Curve 0 100 200 300 400 500 Episodes 80 60 40 20 0 Reward Preference Curve as Average Best Found Reward Curve Average Curve Figure 9: To highlight the robustness, we examine the results using different preference functions such as Sigmoid curve, Log curve, and Average curve on Reacher experiments. The results include the best found reward curve with different preference choices that show the robustness of our model. Left column: the best found curve using averaged reward over 100 consecutive episodes. Right column: the best found curve using the original reward. 17/uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000019 /uni00000013/uni00000011/uni0000001a m0/uni00000020/uni00000010/uni00000016/uni00000003g0/uni00000020/uni00000013/uni00000011/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000016/uni00000003g0/uni00000020/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000016/uni00000003g0/uni00000020/uni00000015 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000016/uni00000003g0/uni00000020/uni00000016 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000019 /uni00000013/uni00000011/uni0000001a m0/uni00000020/uni00000010/uni00000015/uni00000003g0/uni00000020/uni00000013/uni00000011/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000015/uni00000003g0/uni00000020/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000015/uni00000003g0/uni00000020/uni00000015 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000015/uni00000003g0/uni00000020/uni00000016 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000019 m0/uni00000020/uni00000010/uni00000014/uni00000003g0/uni00000020/uni00000013/uni00000011/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000014/uni00000003g0/uni00000020/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000014/uni00000003g0/uni00000020/uni00000015 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000014/uni00000003g0/uni00000020/uni00000016 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000019 m0/uni00000020/uni00000013/uni00000003g0/uni00000020/uni00000013/uni00000011/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000013/uni00000003g0/uni00000020/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000013/uni00000003g0/uni00000020/uni00000015 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000013/uni00000003g0/uni00000020/uni00000016 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000019 m0/uni00000020/uni00000014/uni00000003g0/uni00000020/uni00000013/uni00000011/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000014/uni00000003g0/uni00000020/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000014/uni00000003g0/uni00000020/uni00000015 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000014/uni00000003g0/uni00000020/uni00000016 Figure 10: Examples of Logistic function l(u) = 1 1+exp(−g0[u−m0]) with different values of middle parameter m0 and growth parameter g0. B.3 Robustness over Different Preference Functions We next study the learning effects with respect to different choices of the preference functions. We pick three preference functions including the Sigmoid, Log and Average to compute the utility score for each learning curve. Then, we report the best found reward curve under such choices. The experiments are tested using A2C on Reacher-v2. The results presented in Fig. 9 demonstrate the robustness of our model with the preference functions. B.4 Applying Freeze-Thaw BO in the settings considered While both the exponential decay in Freeze-Thaw BO [42] and our compression function encode preferences regarding training development, there is an important distinction between the two ap- proaches. Freeze-thaw BO utilises the exponential decay property to terminate the training curve, while BOIL only uses the sigmoid curve to guide the search. We refer to Fig. 13 for further illustra- tion of why Freeze-thaw BO struggles in DRL settings. B.5 Ablation Study using Freeze-Thraw Kernel for Time In the joint modeling framework of hyperparameter and time (iteration), we can replace the kernel either k(x,x) or k(t,t) with different choices. We, therefore, set up a new baseline of using the time- kernel k(t,t′) in Freeze-Thaw approach [42] which encodes the monotonously exponential decay from the curve. Particularly, we use the kernel deﬁned as k(t,t′) = βα (t +t′+β)α for parameters α,β > 0 which are optimized in the GP models. 186  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l( . | m * 0 ,g * 0 ) for CartPole m * 0 =-3.266   g * 0 =3.0 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l(m * 0 ,g * 0 ) for CNN_SHVN m * 0 =2.245   g * 0 =2.092 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l(m * 0 ,g * 0 ) for InvPendulum m * 0 =1.649   g * 0 =1.833 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l( . | m * 0 ,g * 0 ) for Cifar10 m * 0 =-4.0   g * 0 =1.476 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l( . | m * 0 ,g * 0 ) for Reacher m * 0 =2.779   g * 0 =1.973 Figure 11: We learn the suitable transformation curve directly from the data. We parameterized the Logistic curve as l (m0,g0) = 1 1+exp(−g0[1−m0]) then estimate g0 and m0. The estimated function l(m∗ 0,g∗ 0) is then used to compress our curve. The above plots are the estimated l() at different environments and datasets. /uni00000013/uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013/uni0000001a/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni0000001b/uni00000013 /uni0000001c/uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000014/uni00000013 /uni00000014/uni00000015/uni00000013/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni0000002c/uni00000051/uni00000059/uni00000048/uni00000055/uni00000057/uni00000048/uni00000047/uni00000033/uni00000048/uni00000051/uni00000047/uni00000058/uni0000004f/uni00000058/uni00000050/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013 /uni00000018/uni00000013/uni00000014/uni00000013/uni00000013/uni00000014/uni00000018/uni00000013/uni00000015/uni00000013/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni0000001b/uni00000013 /uni0000001c/uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000014/uni00000013 /uni00000014/uni00000015/uni00000013/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni0000002c/uni00000051/uni00000059/uni00000048/uni00000055/uni00000057/uni00000048/uni00000047/uni00000033/uni00000048/uni00000051/uni00000047/uni00000058/uni0000004f/uni00000058/uni00000050/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013 /uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni00000014/uni0000001a/uni00000011/uni00000018 /uni00000015/uni00000013/uni00000011/uni00000013 /uni00000015/uni00000015/uni00000011/uni00000018 /uni00000015/uni00000018/uni00000011/uni00000013 /uni00000015/uni0000001a/uni00000011/uni00000018 /uni00000016/uni00000013/uni00000011/uni00000013 /uni00000016/uni00000015/uni00000011/uni00000018/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c /uni0000003e/uni00000026/uni00000031/uni00000031/uni00000010/uni00000026/uni0000004c/uni00000049/uni00000044/uni00000055/uni00000014/uni00000013/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000019/uni00000003/uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000053/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000014/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000018/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni00000014/uni0000001a/uni00000011/uni00000018 /uni00000015/uni00000013/uni00000011/uni00000013 /uni00000015/uni00000015/uni00000011/uni00000018 /uni00000015/uni00000018/uni00000011/uni00000013 /uni00000015/uni0000001a/uni00000011/uni00000018 /uni00000016/uni00000013/uni00000011/uni00000013 /uni00000016/uni00000015/uni00000011/uni00000018/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c /uni0000003e/uni00000026/uni00000031/uni00000031/uni00000010/uni00000026/uni0000004c/uni00000049/uni00000044/uni00000055/uni00000014/uni00000013/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000019/uni00000003/uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000053/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f Figure 12: Tuning hyperparameters of a DRL on InvertedPendulum and a CNN model on CIFAR10. 190 250 500 750 1000 1250 1500 Epoch 0 20 40 60 80 100 120Reward Reward Curve Freeze-thaw 0 250 500 750 1000 1250 1500 Epoch 0 50 100 150 0 250 500 750 1000 1250 1500 Epoch 0 50 100 150 0 250 500 750 1000 1250 1500 Epoch 0 50 100 150 Reward Curves Examples using A2C on Inverted Pendulum Figure 13: Illustration of Freeze-thaw BO in DRL. Freeze-thaw BO will terminate training processes when training performance (in blue) signiﬁcantly drops (i.e. at the red locations) as the exponential decay model will predict low ﬁnal performance. In most RL enviroments noisy training curves are unavoidable. Thus, Freeze-thaw BO will dismiss all curves including good setting, never completing a single training run before the ﬁnal epoch. /uni00000013 /uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni00000014/uni00000015/uni0000001b /uni00000014/uni00000015/uni0000001c /uni00000014/uni00000016/uni00000013 /uni00000014/uni00000016/uni00000014 /uni00000014/uni00000016/uni00000015 /uni00000014/uni00000016/uni00000016 /uni00000014/uni00000016/uni00000017/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni00000035/uni00000048/uni00000044/uni00000046/uni0000004b/uni00000048/uni00000055/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000025/uni00000032/uni0000002c/uni0000002f/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni00000014/uni00000015/uni0000001b /uni00000014/uni00000015/uni0000001c /uni00000014/uni00000016/uni00000013 /uni00000014/uni00000016/uni00000014 /uni00000014/uni00000016/uni00000015 /uni00000014/uni00000016/uni00000016 /uni00000014/uni00000016/uni00000017/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni00000035/uni00000048/uni00000044/uni00000046/uni0000004b/uni00000048/uni00000055/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000025/uni00000032/uni0000002c/uni0000002f/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000013 /uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni0000001c/uni00000013 /uni0000001c/uni00000018 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000018 /uni00000014/uni00000014/uni00000013 /uni00000014/uni00000014/uni00000018 /uni00000014/uni00000015/uni00000013 /uni00000014/uni00000015/uni00000018/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni0000002c/uni00000051/uni00000059/uni00000048/uni00000055/uni00000057/uni00000048/uni00000047/uni00000033/uni00000048/uni00000051/uni00000047/uni00000058/uni0000004f/uni00000058/uni00000050/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000026/uni00000010/uni00000030/uni00000012/uni00000037/uni00000010/uni00000025/uni00000032/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000025/uni00000032/uni0000002c/uni0000002f/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000013 /uni00000018/uni00000013/uni00000014/uni00000013/uni00000013/uni00000014/uni00000018/uni00000013/uni00000015/uni00000013/uni00000013/uni00000015/uni00000018/uni00000013/uni00000016/uni00000013/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni0000001c/uni00000013 /uni0000001c/uni00000018 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000018 /uni00000014/uni00000014/uni00000013 /uni00000014/uni00000014/uni00000018 /uni00000014/uni00000015/uni00000013 /uni00000014/uni00000015/uni00000018/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni0000002c/uni00000051/uni00000059/uni00000048/uni00000055/uni00000057/uni00000048/uni00000047/uni00000033/uni00000048/uni00000051/uni00000047/uni00000058/uni0000004f/uni00000058/uni00000050/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000025/uni00000032/uni0000002c/uni0000002f/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a Figure 14: Comparison using freezethaw kernel for time component. We present the result in Fig. 14 that CM-T/F-BO is still less competitive to BOIL using this speciﬁc time kernel. The results again validate the robustness our approach cross different choices of kernel. B.6 Additional Experiments for Tuning DRL and CNN We present the additional experiments for tuning a DRL model using InvertedPendulum environ- ment and a CNN model using a subset of CIFAR10 in Fig. 12. Again, we show that the proposed model clearly gain advantages against the baselines in tuning hyperparameters for model with itera- tive learning information available. B.7 Examples of Deep Reinforcement Learning Training Curves Finally, we present examples of training curves produced by the deep reinforcement learning al- gorithm A2C in Fig. 15. These ﬂuctuate widely and it may not be trivial to deﬁne good stopping criteria as done for other applications in previous work [42]. 200 200 400 80 70 60 50 40 0 200 400 110 100 90 80 70 60 50 0 200 400 110 100 90 80 70 60 50 0 200 400 70 60 50 40 30 0 200 400 70 60 50 40 30 20 10 0 200 400 50 40 30 20 0 200 400 90 80 70 60 50 0 200 400 70 60 50 40 30 20 10 0 200 400 85 80 75 70 0 200 400 60 50 40 30 20 0 200 400 100 90 80 70 60 0 200 400 100 90 80 70 0 200 400 80 60 40 20 0 200 400 100 90 80 70 60 0 200 400 100 90 80 70 0 200 400 80 70 60 50 40 30 20 0 200 400 70 60 50 40 30 20 0 200 400 70 60 50 40 30 20 0 200 400 100 90 80 70 60 50 0 200 400 70 60 50 40 30 20 0 200 400 100 90 80 70 60 50 0 200 400 100 90 80 70 60 0 200 400 60 50 40 30 20 0 200 400 80 60 40 20 0 200 400 85 80 75 70 65 60 0 200 400 100 95 90 85 80 0 200 400 80 60 40 20 0 200 400 100 90 80 70 60 50 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 Figure 15: Examples of reward curves using A2C on Reacher-v2 (rows 1 −3) and on InvertedPendulum-v2 (rows 4 −6). Y-axis is the reward averaged over 100 consecutive episodes. X-axis is the episode. The noisy performance illustrated is typical of DRL settings and complicates the design of early stopping criteria. Due to the property of DRL, it is not trivial to decide when to stop the training curve. In addition, it will be misleading if we only take average over the last 100 iterations. 21",
      "meta_data": {
        "arxiv_id": "1909.09593v5",
        "authors": [
          "Vu Nguyen",
          "Sebastian Schulze",
          "Michael A Osborne"
        ],
        "published_date": "2019-09-20T16:14:34Z",
        "pdf_url": "https://arxiv.org/pdf/1909.09593v5.pdf",
        "github_url": "https://github.com/ntienvu/BOIL"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper presents a Bayesian optimization (BO) approach, termed BOIL, for efficient hyperparameter tuning of iterative learning algorithms, specifically deep learning (DL) and deep reinforcement learning (DRL). The main contributions include an algorithm that optimizes the learning curve by compressing the entire training progress into a single numeric score based on success and stability, rather than relying solely on final performance. It introduces a data augmentation technique that leverages intermediate information from the iterative process by selectively including scores from different training steps, enhancing sample efficiency and addressing potential Gaussian Process (GP) covariance matrix conditioning issues. The algorithm is demonstrated to outperform existing baselines in identifying optimal hyperparameters in minimal wall-clock time for DRL agents and convolutional neural networks.",
        "methodology": "The BOIL methodology models the cost-sensitive black-box function, representing hyperparameter performance over training iterations, using a Gaussian Process (GP) with a product kernel combining hyperparameter and iteration spaces. The training time cost is approximated by a linear regressor. The algorithm selects the next evaluation point by maximizing a cost-aware Expected Improvement acquisition function. A key component is training curve compression, which transforms the raw learning curve into a numeric score using a dynamically learned Sigmoid (Logistic) preference function. This preference function's parameters (growth and midpoint) are optimized by maximizing the GP's log marginal likelihood. To improve sample efficiency and prevent GP covariance matrix ill-conditioning, BOIL employs a selective data augmentation technique, sampling points from the observed curve that maximize GP predictive uncertainty while adhering to a condition number threshold for the covariance matrix.",
        "experimental_setup": "Experiments were conducted on two DRL agents (Dueling DQN on CartPole-v0, and Advantage Actor Critic (A2C) on InvertedPendulum-v2 and Reacher-v2) and a convolutional neural network on SVHN and CIFAR10 datasets. All results were averaged over 20 independent runs, with final performance evaluated at the maximum number of iterations. The setup utilized NVIDIA 1080 GTX GPUs with TensorFlow-GPU, OpenAI Gym, Mujoco, and OpenAI Baselines. The GP models used square-exponential kernels, with parameters estimated by maximizing marginal likelihood. A maximum of 15 augmented points were allowed, with a natural log of the GP condition number threshold set to 20. Baselines included Hyperband and Continuous Multi-Task/Fidelity Bayesian Optimization (CM-T/F-BO), with ablation studies also comparing against vanilla BO and BO with compression (BO-L). Detailed hyperparameter search ranges and agent configurations were provided for reproducibility.",
        "limitations": "The paper highlights that traditional early-stopping criteria, such as the exponential decay assumed in Freeze-thaw BO, are often unsuitable for DRL due to the unpredictable fluctuations and noisiness of DRL reward curves. It also notes that naive data augmentation by adding a full curve of intermediate training steps can lead to redundant information and severe ill-conditioning of the Gaussian Process covariance matrix, a problem BOIL addresses through selective augmentation. In a broader context, the paper discusses a potential societal limitation: as automated training pipelines, which BOIL contributes to, become more prevalent, humans might become further removed from the modeling process, making it harder to detect critical failures and increasing the opacity of machine learning models.",
        "future_research_directions": "Future research could involve extending the BOIL framework to optimize any iterative process beyond machine learning algorithms, such as adjusting factory settings to increase productivity in manufacturing pipelines. The authors also implicitly suggest further work in constructing fully automated pipelines for ML model training and deployment. Furthermore, while not explicitly stated as future work for BOIL itself, the broader impact discussion points to integrating techniques for analyzing the interpretability of trained machine learning models with automated training procedures to ensure rigorous analysis of final training outcomes and counter the growing opacity of ML models.",
        "experimental_code": "from bayes_opt.acquisition_functions import unique_rows\nfrom sklearn.preprocessing import MinMaxScaler\nfrom scipy.optimize import minimize\nfrom sklearn.metrics.pairwise import euclidean_distances\nimport scipy.linalg as spla\nfrom bayes_opt.curve_compression import apply_one_transform_logistic, transform_logistic\n\n\nclass ProductGaussianProcess(object):\n    # in this class of Gaussian process, we define k( {x,t}, {x',t'} )= k(x,x')*k(t,t')\n    \n    \n    #def __init__ (self,param):\n    def __init__ (self,SearchSpace,gp_hyper=None,logistic_hyper=None,verbose=0):\n        self.noise_delta=5e-4\n        self.noise_upperbound=1e-2\n        self.mycov=self.cov_RBF_time\n        self.SearchSpace=SearchSpace\n        scaler = MinMaxScaler()\n        scaler.fit(SearchSpace.T)\n        self.Xscaler=scaler\n        self.verbose=verbose\n        self.dim=SearchSpace.shape[0]\n        \n        if gp_hyper is None:\n            self.hyper={}\n            self.hyper['var']=1 # standardise the data\n            self.hyper['lengthscale_x']=0.02 #to be optimised\n            self.hyper['lengthscale_t']=0.2 #to be optimised\n        else:\n            self.hyper=gp_hyper\n\n        \n        if logistic_hyper is None:\n            self.logistic_hyper={}\n            self.logistic_hyper['midpoint']=0.0\n            self.logistic_hyper['growth']=1.0   \n        else:\n            self.logistic_hyper=logistic_hyper\n\n        self.X=[]\n        self.T=[]\n        self.Y=[]\n        self.Y_curves=None\n#        self.hyper['lengthscale_x']_old=self.hyper['lengthscale_x']\n#        self.hyper['lengthscale_x']_old_t=self.hyper['lengthscale_x']_t\n        \n        self.alpha=[] # for Cholesky update\n        self.L=[] # for Cholesky update LL'=A\n        \n        self.MaxEpisode=0\n        \n        return None\n       \n\n    def cov_RBF_time(self, x1,t1,x2,t2,lengthscale,lengthscale_t):\n        \n        Euc_dist=euclidean_distances(x1,x2)\n        exp_dist_x=np.exp(-np.square(Euc_dist)/lengthscale)\n        \n        Euc_dist=euclidean_distances(t1,t2)\n        exp_dist_t=np.exp(-np.square(Euc_dist)/lengthscale_t)\n        \n        return exp_dist_x*exp_dist_t\n                \n    def fit(self,X,T,Y,Y_curves):\n        \"\"\"\n        Fit Gaussian Process model\n\n        Input Parameters\n        ----------\n        x: the observed points \n        t: time or number of episode\n        y: the outcome y=f(x)\n        \n        \"\"\" \n        temp=np.hstack((X,T))\n        ur = unique_rows(temp)\n        \n        T=T[ur]\n        X=X[ur]\n        Y=Y[ur]\n        \n        self.X=X\n        self.Y=Y\n        self.T=T\n        self.Y_curves=[val for idx,val in enumerate(Y_curves) if ur[idx]==True]\n        \n        for curves in self.Y_curves:\n            self.MaxEpisode=max(len(curves),self.MaxEpisode)\n        #self.Y_curves=Y_curves[myidx]\n            \n        Euc_dist_x=euclidean_distances(X,X)\n        #exp_dist_x=np.exp(-np.square(Euc_dist)/self.hyper['lengthscale_x'])+np.eye(len(X))*self.noise_delta\n    \n        Euc_dist_t=euclidean_distances(T,T)\n        #exp_dist_t=np.exp(-np.square(Euc_dist)/self.hyper['lengthscale_x']_t)+np.eye(len(X))*self.noise_delta       \n    \n        self.KK_x_x=np.exp(-np.square(Euc_dist_x)/self.hyper['lengthscale_x']\\\n                           -np.square(Euc_dist_t)/self.hyper['lengthscale_t'])+np.eye(len(X))*self.noise_delta\n          \n        if np.isnan(self.KK_x_x).any(): #NaN\n            print(\"nan in KK_x_x\")\n        \n        #self.KK_x_x_inv=np.linalg.pinv(self.KK_x_x)\n        self.L=np.linalg.cholesky(self.KK_x_x)\n        temp=np.linalg.solve(self.L,self.Y)\n        self.alpha=np.linalg.solve(self.L.T,temp)\n        self.cond_num=self.compute_condition_number()\n        \n    def compute_condition_number(self):\n        cond_num=np.linalg.cond(self.KK_x_x)\n        return cond_num\n    \n\n    def log_marginal_lengthscale_logistic_hyper(self,hyper,noise_delta):\n        \"\"\"\n        Compute Log Marginal likelihood of the GP model w.r.t. the provided lengthscale, noise_delta and Logistic hyperparameter\n        \"\"\"\n\n        def compute_log_marginal_with_logistic_hyper(lengthscale, lengthscale_t,midpoint,growth,noise_delta):\n            # compute K\n            temp=np.hstack((self.X,self.T))\n            ur = unique_rows(temp)\n            myX=self.X[ur]\n            myT=self.T[ur]\n            \n            # transform Y_curve to Y_original, then to Y\n            Y_original=transform_logistic(self.Y_curves,midpoint,growth,self.MaxEpisode)\n            myY=(Y_original-np.mean(Y_original))/np.std(Y_original)\n            \n            myY=myY[ur]\n          \n            self.Euc_dist_x=euclidean_distances(myX,myX)\n            self.Euc_dist_t=euclidean_distances(myT,myT)\n        \n            KK=np.exp(-np.square(self.Euc_dist_x)/lengthscale-np.square(self.Euc_dist_t)/lengthscale_t)\n                +np.eye(len(myX))*noise_delta\n                    \n            \n            try:\n                temp_inv=np.linalg.solve(KK,myY)\n            except: # singular\n                return -np.inf\n            \n            try:\n                #logmarginal=-0.5*np.dot(self.Y.T,temp_inv)-0.5*np.log(np.linalg.det(KK+noise_delta))-0.5*len(X)*np.log(2*3.14)\n                first_term=-0.5*np.dot(myY.T,temp_inv)\n                \n                # if the matrix is too large, we randomly select a part of the data for fast computation\n                if KK.shape[0]>200:\n                    idx=np.random.permutation(KK.shape[0])\n                    idx=idx[:200]\n                    KK=KK[np.ix_(idx,idx)]\n                #Wi, LW, LWi, W_logdet = pdinv(KK)\n                #sign,W_logdet2=np.linalg.slogdet(KK)\n                chol  = spla.cholesky(KK, lower=True)\n                W_logdet=np.sum(np.log(np.diag(chol)))\n                # Uses the identity that log det A = log prod diag chol A = sum log diag chol A\n    \n                #second_term=-0.5*W_logdet2\n                second_term=-W_logdet\n            except: # singular\n                return -np.inf\n            \n\n            logmarginal=first_term+second_term-0.5*len(myY)*np.log(2*3.14)\n                \n            if np.isnan(np.asscalar(logmarginal))==True:\n                print(\"lengthscale_x={:f} lengthscale_t={:f} first term ={:.4f} second  term ={:.4f}\".format(\n                        lengthscale,lengthscale_t,np.asscalar(first_term),np.asscalar(second_term)))\n\n            #print(lengthscale, lengthscale_t,midpoint,growth,\"logmarginal:\",logmarginal)\n            return np.asscalar(logmarginal)\n        \n        logmarginal=0\n\n        if not isinstance(hyper,list) and len(hyper.shape)==2:\n            logmarginal=[0]*hyper.shape[0]\n            growth=hyper[:,3]\n            midpoint=hyper[:,2]\n            lengthscale_t=hyper[:,1]\n            lengthscale_x=hyper[:,0]\n            for idx in range(hyper.shape[0]):\n                logmarginal[idx]=compute_log_marginal_with_logistic_hyper(lengthscale_x[idx],\\\n                           lengthscale_t[idx],midpoint[idx],growth[idx],noise_delta)\n        else:\n            lengthscale_x,lengthscale_t,midpoint,growth=hyper\n            logmarginal=compute_log_marginal_with_logistic_hyper(lengthscale_x,lengthscale_t,\\\n                                                                 midpoint,growth,noise_delta)\n        return logmarginal\n\n#    def optimize_lengthscale_SE_maximizing(self,previous_theta,noise_delta):\n#        \"\"\"\n#        Optimize to select the optimal lengthscale parameter\n#        \"\"\"\n#                \n#        # define a bound on the lengthscale\n#        SearchSpace_lengthscale_min=0.01\n#        SearchSpace_lengthscale_max=0.5\n#        #mySearchSpace=[np.asarray([SearchSpace_lengthscale_min,SearchSpace_lengthscale_max]).T]\n#        \n#        mySearchSpace=np.asarray([[SearchSpace_lengthscale_min,SearchSpace_lengthscale_max],\\\n#                             [10*SearchSpace_lengthscale_min,2*SearchSpace_lengthscale_max]])\n#        \n#        # Concatenate new random points to possible existing\n#        # points from self.explore method.\n#        lengthscale_tries = np.random.uniform(mySearchSpace[:, 0], mySearchSpace[:, 1],size=(20, mySearchSpace.shape[0]))\n\n#        #print lengthscale_tries\n\n#        # evaluate\n#        self.flagOptimizeHyperFirst=0 # for efficiency\n\n#        logmarginal_tries=self.log_marginal_lengthscale(lengthscale_tries,noise_delta)\n#        #print logmarginal_tries\n\n#        #find x optimal for init\n#        idx_max=np.argmax(logmarginal_tries)\n#        lengthscale_init_max=lengthscale_tries[idx_max]\n#        #print lengthscale_init_max\n#        \n#        myopts ={'maxiter':20*self.dim,'maxfun':20*self.dim}\n\n#        x_max=[]\n#        max_log_marginal=None\n#        \n#        res = minimize(lambda x: -self.log_marginal_lengthscale(x,noise_delta),lengthscale_init_max,\n#                       SearchSpace=mySearchSpace,method=\"L-BFGS-B\",options=myopts)#L-BFGS-B\n#        if 'x' not in res:\n#            val=self.log_marginal_lengthscale(res,noise_delta)    \n#        else:\n#            val=self.log_marginal_lengthscale(res.x,noise_delta)  \n#        \n#        # Store it if better than previous minimum(maximum).\n#        if max_log_marginal is None or val >= max_log_marginal:\n#            if 'x' not in res:\n#                x_max = res\n#            else:\n#                x_max = res.x\n#            max_log_marginal = val\n#            #print res.x\n\n#        return x_max\n    \n    def optimize_lengthscale_SE_logistic_hyper(self,previous_hyper,noise_delta):\n        \"\"\"\n        Optimize to select the optimal lengthscale parameter\n        \"\"\"\n        \n        # define a bound on the lengthscale\n        SearchSpace_l_min=0.03\n        SearchSpace_l_max=0.3\n        \n        SearchSpace_midpoint_min=-2\n        SearchSpace_midpoint_max=3\n        \n        SearchSpace_growth_min=0.5\n        SearchSpace_growth_max=2\n        #mySearchSpace=[np.asarray([SearchSpace_lengthscale_min,SearchSpace_lengthscale_max]).T]\n        \n        mySearchSpace=np.asarray([[SearchSpace_l_min,SearchSpace_l_max],[10*SearchSpace_l_min,2*SearchSpace_l_max],\n                             [SearchSpace_midpoint_min,SearchSpace_midpoint_max],[SearchSpace_growth_min,SearchSpace_growth_max]])\n        \n        lengthscale_tries = np.random.uniform(mySearchSpace[:, 0], mySearchSpace[:, 1],size=(20, 4))\n\n        # evaluate\n        self.flagOptimizeHyperFirst=0 # for efficiency\n\n        logmarginal_tries=self.log_marginal_lengthscale_logistic_hyper(lengthscale_tries,noise_delta)\n\n        #find x optimal for init\n        idx_max=np.argmax(logmarginal_tries)\n        lengthscale_init_max=lengthscale_tries[idx_max]\n        #print lengthscale_init_max\n        \n        myopts ={'maxiter':30*self.dim,'maxfun':30*self.dim}\n\n        x_max=[]\n        max_log_marginal=None\n        \n        res = minimize(lambda x: -self.log_marginal_lengthscale_logistic_hyper(x,noise_delta),lengthscale_init_max,\n                       bounds=mySearchSpace,method=\"L-BFGS-B\",options=myopts)#L-BFGS-B\n        if 'x' not in res:\n            val=self.log_marginal_lengthscale_logistic_hyper(res,noise_delta)    \n        else:\n            val=self.log_marginal_lengthscale_logistic_hyper(res.x,noise_delta)  \n        \n        # Store it if better than previous minimum(maximum).\n        if max_log_marginal is None or val >= max_log_marginal:\n            if 'x' not in res:\n                x_max = res\n            else:\n                x_max = res.x\n            max_log_marginal = val\n            #print res.x\n\n        return x_max\n\n\n#    def optimize_lengthscale(self,previous_theta_x, previous_theta_t,noise_delta):\n#\n#        prev_theta=[previous_theta_x,previous_theta_t]\n#        newlengthscale,newlengthscale_t=self.optimize_lengthscale_SE_maximizing(prev_theta,noise_delta)\n#        self.hyper['lengthscale_x']=newlengthscale\n#        self.hyper['lengthscale_t']=newlengthscale_t\n#        \n#        # refit the model\n#        temp=np.hstack((self.X,self.T))\n#        ur = unique_rows(temp)\n#        \n#        self.fit(self.X[ur],self.T[ur],self.Y[ur],self.Y_curves)\n#        \n#        return newlengthscale,newlengthscale_t\n            \n    def optimize_lengthscale_logistic_hyper(self,prev_hyper,noise_delta):\n        # optimize both GP lengthscale and logistic hyperparameter\n\n            \n        #prev_theta=[prev_theta_x,prev_theta_t,prev_midpoint,prev_growth]\n        newlengthscale,newlengthscale_t,newmidpoint,newgrowth=self.optimize_lengthscale_SE_logistic_hyper(prev_hyper,noise_delta)\n        self.hyper['lengthscale_x']=newlengthscale\n        self.hyper['lengthscale_t']=newlengthscale_t\n        \n        # refit the model\n        temp=np.hstack((self.X,self.T))\n        ur = unique_rows(temp)\n\n        # update Y here\n        Y_original=transform_logistic(self.Y_curves,newmidpoint,newgrowth,self.SearchSpace[-1,1])\n        Y=(Y_original-np.mean(Y_original))/np.std(Y_original)\n        self.Y=Y\n        #\n        self.fit(self.X[ur],self.T[ur],self.Y[ur],self.Y_curves)\n        \n        return newlengthscale,newlengthscale_t,newmidpoint,newgrowth\n\n\n    def compute_var(self,X,T,xTest,tTest):\n        \"\"\"\n        compute variance given X and xTest\n        \n        Input Parameters\n        ----------\n        X: the observed points\n        xTest: the testing points \n        \n        Returns\n        -------\n        diag(var)\n        \"\"\" \n        \n        xTest=np.asarray(xTest)\n        xTest=np.atleast_2d(xTest)\n        \n        tTest=np.asarray(tTest)\n        tTest=np.atleast_2d(tTest)\n        tTest=np.reshape(tTest,(-1,1))\n        \n        if self.kernel_name=='SE':\n            #Euc_dist=euclidean_distances(xTest,xTest)\n            #KK_xTest_xTest=np.exp(-np.square(Euc_dist)/self.hyper['lengthscale_x'])+np.eye(xTest.shape[0])*self.noise_delta\n            #ur = unique_rows(X)\n            myX=X\n            myT=T\n            \n            Euc_dist_x=euclidean_distances(myX,myX)\n            #exp_dist_x=np.exp(-np.square(self.Euc_dist_x)/lengthscale)+np.eye(len(myX))*noise_delta\n        \n            Euc_dist_t=euclidean_distances(myT,myT)\n            #exp_dist_t=np.exp(-np.square(self.Euc_dist_t)/lengthscale_t)+np.eye(len(myX))*noise_delta      \n        \n            KK=np.exp(-np.square(Euc_dist_x)/self.hyper['lengthscale_x']-np.square(Euc_dist_t)/self.hyper['lengthscale_t'])\n                +np.eye(len(myX))*self.noise_delta\n                    \n                 \n            Euc_dist_test_train_x=euclidean_distances(xTest,X)\n            #Exp_dist_test_train_x=np.exp(-np.square(Euc_dist_test_train_x)/self.hyper['lengthscale_x'])\n            \n            Euc_dist_test_train_t=euclidean_distances(tTest,T)\n            #Exp_dist_test_train_t=np.exp(-np.square(Euc_dist_test_train_t)/self.hyper['lengthscale_t'])\n            \n            KK_xTest_xTrain=np.exp(-np.square(Euc_dist_test_train_x)/self.hyper['lengthscale_x']-np.square(Euc_dist_test_train_t)/self.hyper['lengthscale_t'])\n                \n        try:\n            temp=np.linalg.solve(KK,KK_xTest_xTrain.T)\n        except:\n            temp=np.linalg.lstsq(KK,KK_xTest_xTrain.T, rcond=-1)\n            temp=temp[0]\n            \n        #var=KK_xTest_xTest-np.dot(temp.T,KK_xTest_xTrain.T)\n        var=np.eye(xTest.shape[0])-np.dot(temp.T,KK_xTest_xTrain.T)\n        var=np.diag(var)\n        var.flags['WRITEABLE']=True\n        var[var<1e-100]=0\n        return var \n\n    \n        \n    def predict(self,xTest, eval_MSE=True):\n        \"\"\"\n        compute predictive mean and variance\n        Input Parameters\n        ----------\n        xTest: the testing points \n        \n        Returns\n        -------\n        mean, var\n        \"\"\"    \n\n        if len(xTest.shape)==1: # 1d\n            xTest=xTest.reshape((-1,self.X.shape[1]+1))\n            \n        tTest=xTest[:,-1]\n        tTest=np.atleast_2d(tTest)\n        tTest=np.reshape(tTest,(xTest.shape[0],-1))\n        \n        xTest=xTest[:,:-1]\n        \n        # prevent singular matrix\n        temp=np.hstack((self.X,self.T))\n        ur = unique_rows(temp)\n        \n        X=self.X[ur]\n        T=self.T[ur]\n                \n        Euc_dist_x=euclidean_distances(xTest,xTest)\n        Euc_dist_t=euclidean_distances(tTest,tTest)\n\n        KK_xTest_xTest=np.exp(-np.square(Euc_dist_x)/self.hyper['lengthscale_x']-np.square(Euc_dist_t)/self.hyper['lengthscale_t'])\n            +np.eye(xTest.shape[0])*self.noise_delta\n        \n        Euc_dist_test_train_x=euclidean_distances(xTest,X)\n        \n        Euc_dist_test_train_t=euclidean_distances(tTest,T)\n        \n        KK_xTest_xTrain=np.exp(-np.square(Euc_dist_test_train_x)/self.hyper['lengthscale_x']-np.square(Euc_dist_test_train_t)/self.hyper['lengthscale_t'])\n            \n        #Exp_dist_test_train_x*Exp_dist_test_train_t\n  \n        # using Cholesky update\n        mean=np.dot(KK_xTest_xTrain,self.alpha)\n        v=np.linalg.solve(self.L,KK_xTest_xTrain.T)\n        var=KK_xTest_xTest-np.dot(v.T,v)\n        \n\n        return mean.ravel(),np.diag(var)  \n\n    def posterior(self,x):\n        # compute mean function and covariance function\n        return self.predict(self,x)\n        \n    \ndef apply_one_transform_logistic(curve, midpoint=-2, growth=1,MaxEpisode=1000,IsReturnCurve=False):\n    # this is the Logistic transformation, used in the paper\n    if isinstance(curve, (list,)):\n        curve=curve[0]\n        \n    def logistic_func(x):\n        return 1.0/(1+np.exp(-growth*(x-midpoint)))\n\t\n    #print(MaxEpisode)\n    my_xrange_scaled=np.linspace(-6,6, int(MaxEpisode))\n\n    my_logistic_value_scaled=logistic_func(my_xrange_scaled)\n\n    my_logistic_value_scaled=my_logistic_value_scaled[:len(curve)]\n\n    # if curve is negative, add a constant to make it positive\n    if np.max(curve)<=0 and np.min(curve)<=0:\n        curve=curve+500\n    \n    threshold=(midpoint+6-2)*len(curve)/(12)\n    threshold=np.int(threshold)\n    \n    prod_func=curve*my_logistic_value_scaled\n    \n    average=[np.mean(prod_func[threshold:pos+1]) for pos in range(threshold,len(prod_func))]\n\n    if IsReturnCurve==True:\n        return average[-1],my_logistic_value_scaled\n    else:\n        return average[-1]\n\n\ndef transform_logistic(curves, midpoint=0, growth=1,MaxEpisode=1000):\n    # curve is a matrix [nParameter x MaxIter]\n    # or curve is a vector [1 x MaxIter]\n\n    if len(curves)==1:\n        output=apply_one_transform_logistic(curves[0], midpoint, growth,MaxEpisode)\n    else:\n        output=[0]*len(curves)\n        for idx, curve in enumerate(curves):\n            output[idx]=apply_one_transform_logistic(curve, midpoint, growth,MaxEpisode)\n    return output\n    \nimport numpy as np\nfrom scipy.stats import norm\n\n\ncounter = 0\n\n\nclass AcquisitionFunction(object):\n    \"\"\"\n    An object to compute the acquisition functions.\n    \"\"\"\n\n    def __init__(self, acq):\n\n        self.acq=acq\n        acq_name=acq['name']\n        \n        if 'mu_max' in acq:\n            self.mu_max=acq['mu_max'] # this is for ei_mu acquisition function\n        \n        ListAcq=['bucb','ucb', 'ei','poi','random','ucb_pe',\n                 'pure_exploration','mu','lcb','ei_mu_max'                          ]\n        \n        # check valid acquisition function\n        IsTrue=[val for idx,val in enumerate(ListAcq) if val in acq_name]\n        #if  not in acq_name:\n        if  IsTrue == []:\n            err = \"The utility function \" \\\n                  \"{} has not been implemented, \" \\\n                  \"please choose one of ucb, ei, or poi.\".format(acq_name)\n            raise NotImplementedError(err)\n        else:\n            self.acq_name = acq_name\n            \n        self.dim=acq['dim']\n        \n        if 'scalebounds' not in acq:\n            self.scalebounds=[0,1]*self.dim\n            \n        else:\n            self.scalebounds=acq['scalebounds']\n               \n\n    def acq_kind(self, x, gp):\n        \n        #if type(meta) is dict and 'y_max' in meta.keys():\n        #   y_max=meta['y_max']\n        y_max=np.max(gp.Y)\n        #print self.kind\n        if np.any(np.isnan(x)):\n            return 0\n       \n        if self.acq_name == 'ucb':\n            return self._ucb(x, gp)\n        if self.acq_name == 'lcb':\n            return self._lcb(x, gp)\n        if self.acq_name == 'ei':\n            return self._ei(x, gp, y_max)\n        if self.acq_name == 'ei_mu_max': # using max mu(x) as incumbent\n            return self._ei(x, gp, self.mu_max)\n        if self.acq_name == 'poi':\n            return self._poi(x, gp, y_max)\n        \n        if self.acq_name == 'pure_exploration':\n            return self._pure_exploration(x, gp) \n      \n        if self.acq_name == 'mu':\n            return self._mu(x, gp)\n        \n        if self.acq_name == 'ucb_pe':\n            return self._ucb_pe(x, gp,self.acq['kappa'],self.acq['maxlcb'])\n       \n            \n    def utility_plot(self, x, gp, y_max):\n        if np.any(np.isnan(x)):\n            return 0\n        if self.acq_name == 'ei':\n            return self._ei_plot(x, gp, y_max)\n  \n   \n    @staticmethod\n    def _mu(x, gp):\n        mean, var = gp.predict(x, eval_MSE=True)\n        mean=np.atleast_2d(mean).T\n        return mean\n                \n    @staticmethod\n    def _lcb(x, gp):\n        mean, var = gp.predict(x, eval_MSE=True)\n        var.flags['WRITEABLE']=True\n        #var=var.copy()\n        var[var<1e-10]=0\n        mean=np.atleast_2d(mean).T\n        var=np.atleast_2d(var).T\n        #beta_t = gp.X.shape[1] * np.log(len(gp.Y))\n        beta_t = 2 * np.log(len(gp.Y));\n\n        return mean - np.sqrt(beta_t) * np.sqrt(var) \n        \n    \n    @staticmethod\n    def _ucb(x, gp):\n        mean, var = gp.predict(x, eval_MSE=True)\n        var.flags['WRITEABLE']=True\n        #var=var.copy()\n        var[var<1e-10]=0\n        mean=np.atleast_2d(mean).T\n        var=np.atleast_2d(var).T                \n        \n        # Linear in D, log in t https://github.com/kirthevasank/add-gp-bandits/blob/master/BOLibkky/getUCBUtility.m\n        #beta_t = gp.X.shape[1] * np.log(len(gp.Y))\n        beta_t = 2 * np.log(len(gp.Y));\n  \n        #beta=300*0.1*np.log(5*len(gp.Y))# delta=0.2, gamma_t=0.1\n        return mean + np.sqrt(beta_t) * np.sqrt(var) \n    \n    \n    @staticmethod\n    def _ucb_pe(x, gp, kappa, maxlcb):\n        mean, var = gp.predict_bucb(x, eval_MSE=True)\n        var.flags['WRITEABLE']=True\n        var[var<1e-10]=0\n        mean=np.atleast_2d(mean).T\n        var=np.atleast_2d(var).T\n\n        value=mean + kappa * np.sqrt(var)        \n        myidx=[idx for idx,val in enumerate(value) if val<maxlcb]\n        var[myidx]=0        \n        return var\n    \n   \n    @staticmethod\n    def _pure_exploration(x, gp):\n        mean, var = gp.predict(x, eval_MSE=True)\n        var.flags['WRITEABLE']=True\n        #var=var.copy()\n        var[var<1e-10]=0\n        mean=np.atleast_2d(mean).T\n        var=np.atleast_2d(var).T\n        return np.sqrt(var)\n        \n   \n    @staticmethod\n    def _ei(x, gp, y_max):\n        y_max=np.asscalar(y_max)\n        mean, var = gp.predict(x, eval_MSE=True)\n        var2 = np.maximum(var, 1e-10 + 0 * var)\n        z = (mean - y_max)/np.sqrt(var2)        \n        out=(mean - y_max) * norm.cdf(z) + np.sqrt(var2) * norm.pdf(z)\n        \n        out[var2<1e-10]=0\n        return out\n \n \n    @staticmethod      \n    def _poi(x, gp,y_max): # run Predictive Entropy Search using Spearmint\n        mean, var = gp.predict(x, eval_MSE=True)    \n        # Avoid points with zero variance\n        var = np.maximum(var, 1e-9 + 0 * var)\n        z = (mean - y_max)/np.sqrt(var)        \n        return norm.cdf(z)\n\n   \ndef unique_rows(a):\n    \"\"\"\n    A functions to trim repeated rows that may appear when optimizing.\n    This is necessary to avoid the sklearn GP object from breaking\n\n    :param a: array to trim repeated rows from\n\n    :return: mask of unique rows\n    \"\"\"\n\n    # Sort array and kep track of where things should go back to\n    order = np.lexsort(a.T)\n    reorder = np.argsort(order)\n\n    a = a[order]\n    diff = np.diff(a, axis=0)\n    ui = np.ones(len(a), 'bool')\n    ui[1:] = (diff != 0).any(axis=1)\n\n    return ui[reorder]\n\n\n\nclass BColours(object):\n    BLUE = '\\033[94m'\n    CYAN = '\\033[36m'\n    GREEN = '\\033[32m'\n    MAGENTA = '\\033[35m'\n    RED = '\\033[31m'\n    ENDC = '\\033[0m'\n\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom bayes_opt.acquisition_functions import AcquisitionFunction\nimport sobol_seq\n\n\ndef acq_max_with_name(gp,scaleSearchSpace,acq_name=\"ei\",IsReturnY=False,IsMax=True,fstar_scaled=None):\n    acq={}\n    acq['name']=acq_name\n    acq['dim']=scaleSearchSpace.shape[0]\n    acq['scaleSearchSpace']=scaleSearchSpace   \n    if fstar_scaled:\n        acq['fstar_scaled']=fstar_scaled   \n\n    myacq=AcquisitionFunction(acq)\n    if IsMax:\n        x_max = acq_max(ac=myacq.acq_kind,gp=gp,bounds=scaleSearchSpace,opt_toolbox='scipy')\n    else:\n        x_max = acq_min_scipy(ac=myacq.acq_kind,gp=gp,bounds=scaleSearchSpace)\n    if IsReturnY==True:\n        y_max=myacq.acq_kind(x_max,gp=gp)\n        return x_max,y_max\n    return x_max\n\ndef generate_sobol_seq(dim,nSobol):\n    mysobol_seq = sobol_seq.i4_sobol_generate(dim, nSobol)\n    return mysobol_seq\n    \n\ndef acq_min_scipy_kwargs(myfunc, SearchSpace, **kwargs):\n    \"\"\"\n    A function to find the maximum of the acquisition function using\n    the scipy python\n\n    Input Parameters\n    ----------\n    ac: The acquisition function object that return its point-wise value.\n    gp: A gaussian process fitted to the relevant data.\n    y_max: The current maximum known value of the target function.\n    bounds: The variables bounds to limit the search of the acq max.\n    \n    Returns\n    -------\n    x_max, The arg max of the acquisition function.\n    \"\"\"\n    dim=SearchSpace.shape[0]\n    # Start with the lower bound as the argmax\n    x_max = SearchSpace[:, 0]\n    min_acq = None\n\n    #myopts ={'maxiter':2000,'fatol':0.01,'xatol':0.01}\n    myopts ={'maxiter':10*dim,'maxfun':20*dim}\n    #myopts ={'maxiter':5*dim}\n\n    #sobol_sequence=generate_sobol_seq(dim=dim,nSobol=500*dim)\n\n    # multi start\n    for i in range(3*dim):\n        # Find the minimum of minus the acquisition function        \n        x_tries = np.random.uniform(SearchSpace[:, 0], SearchSpace[:, 1],size=(100*dim, dim))\n        \n        #x_tries=sobol_sequence\n    \n        # evaluate\n        y_tries=myfunc(x_tries,**kwargs)\n        \n        #find x optimal for init\n        idx_min=np.argmin(y_tries)\n\n        x_init_min=x_tries[idx_min]\n    \n        res = minimize(lambda x: myfunc(x.reshape(1, -1), **kwargs),x_init_min.reshape(1, -1),bounds=SearchSpace,\n                       method=\"L-BFGS-B\",options=myopts)#L-BFGS-B\n\n        if 'x' not in res:\n            val=myfunc(res,**kwargs)        \n        else:\n            val=myfunc(res.x,**kwargs) \n        \n        # Store it if better than previous minimum(maximum).\n        if min_acq is None or val <= min_acq:\n            if 'x' not in res:\n                x_max = res\n            else:\n                x_max = res.x\n            min_acq = val\n            #print max_acq\n\n    return np.clip(x_max, SearchSpace[:, 0], SearchSpace[:, 1])\n\n    \ndef acq_max(ac, gp, bounds, opt_toolbox='scipy',seeds=[],IsMax=True):\n    \"\"\"\n    A function to find the maximum of the acquisition function using\n    the scipy python\n\n    Input Parameters\n    ----------\n    ac: The acquisition function object that return its point-wise value.\n    gp: A gaussian process fitted to the relevant data.\n    y_max: The current maximum known value of the target function.\n    bounds: The variables bounds to limit the search of the acq max.\n    \n    Returns\n    -------\n    x_max, The arg max of the acquisition function.\n    \"\"\"\n    y_max=np.max(gp.Y)\n  \n    x_max = acq_max_scipy(ac=ac,gp=gp,y_max=y_max,bounds=bounds)\n\n    return x_max\n\ndef acq_max_scipy(ac, gp, y_max, bounds):\n    \"\"\"\n    A function to find the maximum of the acquisition function using\n    the scipy python\n\n    Input Parameters\n    ----------\n    ac: The acquisition function object that return its point-wise value.\n    gp: A gaussian process fitted to the relevant data.\n    y_max: The current maximum known value of the target function.\n    bounds: The variables bounds to limit the search of the acq max.\n    \n    Returns\n    -------\n    x_max, The arg max of the acquisition function.\n    \"\"\"\n\n    dim=bounds.shape[0]\n    # Start with the lower bound as the argmax\n    x_max = bounds[:, 0]\n    max_acq = None\n\n    myopts ={'maxiter':10*dim,'maxfun':20*dim}\n    #myopts ={'maxiter':5*dim}\n\n\n    # multi start\n    for i in range(1*dim):\n        # Find the minimum of minus the acquisition function        \n        x_tries = np.random.uniform(bounds[:, 0], bounds[:, 1],size=(50*dim, dim))\n    \n        # evaluate\n        y_tries=ac(x_tries,gp=gp)\n        #print \"elapse evaluate={:.5f}\".format(end_eval-start_eval)\n        \n        #find x optimal for init\n        idx_max=np.argmax(y_tries)\n        #print \"max y_tries {:.5f} y_max={:.3f}\".format(np.max(y_tries),y_max)\n\n        x_init_max=x_tries[idx_max]\n        \n    \n        res = minimize(lambda x: -ac(x.reshape(1, -1), gp=gp),x_init_max.reshape(1, -1),bounds=bounds,\n                       method=\"L-BFGS-B\",options=myopts)#L-BFGS-B\n\n\n        \n        if 'x' not in res:\n            val=ac(res,gp)        \n        else:\n            val=ac(res.x,gp) \n\n        # Store it if better than previous minimum(maximum).\n        if max_acq is None or val >= max_acq:\n            if 'x' not in res:\n                x_max = res\n            else:\n                x_max = res.x\n            max_acq = val\n            #print max_acq\n\n    # Clip output to make sure it lies within the bounds. Due to floating\n    # point technicalities this is not always the case.\n    #return np.clip(x_max[0], bounds[:, 0], bounds[:, 1])\n        #print max_acq\n    return np.clip(x_max, bounds[:, 0], bounds[:, 1])\n    \nimport numpy as np\nfrom bayes_opt.acquisition_functions import AcquisitionFunction, unique_rows\nfrom bayes_opt import GaussianProcess\nfrom bayes_opt import ProductGaussianProcess\n\nfrom bayes_opt.acquisition_maximization import acq_max_with_name,acq_min_scipy_kwargs\nimport time\nfrom sklearn import linear_model\nimport copy\nfrom bayes_opt.curve_compression import transform_logistic\nfrom sklearn.preprocessing import MinMaxScaler\n\n\n#======================================================================================================\n#======================================================================================================\n#======================================================================================================\n#======================================================================================================\ncounter = 0\n\n\nclass BOIL(object):\n\n    #def __init__(self, gp_params, func_params, acq_params, verbose=True):\n    def __init__(self, func, SearchSpace,acq_name=\"ei_mu_max\",verbose=1):\n\n        \"\"\"      \n        Input parameters\n        ----------\n        \ngp_params:                  GP parameters\ngp_params.theta:            to compute the kernel\ngp_params.delta:            to compute the kernel\n\nfunc_params:                function to optimize\nfunc_params.init bound:     initial SearchSpace for parameters\nfunc_params.SearchSpace:        SearchSpace on parameters        \nfunc_params.func:           a function to be optimized\n\n\nacq_params:            acquisition function, \nacq_params.acq_func['name']=['ei','ucb','poi']\nacq_params.opt_toolbox:     optimization toolbox 'nlopt','direct','scipy'\n                            \n        Returns\n        -------\n        dim:            dimension\n        SearchSpace:         SearchSpace on original scale\n        scaleSearchSpace:    SearchSpace on normalized scale of 0-1\n        time_opt:       will record the time spent on optimization\n        gp:             Gaussian Process object\n        \"\"\"\n        \n        self.method='boil'\n        self.verbose=verbose\n        if isinstance(SearchSpace,dict):\n            # Get the name of the parameters\n            self.keys = list(SearchSpace.keys())\n            \n            self.SearchSpace = []\n            for key in list(SearchSpace.keys()):\n                self.SearchSpace.append(SearchSpace[key])\n            self.SearchSpace = np.asarray(self.SearchSpace)\n        else:\n            self.SearchSpace=np.asarray(SearchSpace)\n            \n            \n        self.dim = len(SearchSpace)\n\n        scaler = MinMaxScaler()\n        scaler.fit(self.SearchSpace[:-1,:].T)\n        \n        scalerT = MinMaxScaler()\n        SearchSpace_T=np.atleast_2d(self.SearchSpace[-1,:]).T\n        scalerT.fit(SearchSpace_T)\n\n        self.Xscaler=scaler\n        self.Tscaler=scalerT\n\n        # create a scaleSearchSpace 0-1\n        self.scaleSearchSpace=np.array([np.zeros(self.dim), np.ones(self.dim)]).T\n                \n        # function to be optimised\n        self.f = func\n    \n        # store X in original scale\n        self.X_ori= None\n\n        # store X in 0-1 scale\n        self.X = None\n        \n        # store y=f(x)\n        # (y - mean)/(max-min)\n        self.Y = None\n               \n        # y original scale\n        self.Y_ori = None\n        \n        # store the number of episode\n        self.T=None\n        self.T_original=None\n        \n        # store the cost original scale\n        self.Y_cost_original=None\n        \n        self.time_opt=0\n         \n        self.max_min_gap=self.SearchSpace[:,1]-self.SearchSpace[:,0]\n\n\n        # acquisition function\n        self.acq_name = acq_name\n        self.logmarginal=0\n\n        self.gp=ProductGaussianProcess(self.scaleSearchSpace,verbose=verbose)\n\n        # store the curves of performances\n        self.Y_curves=[]\n        \n        # store the cost original scale\n        self.Y_cost_original=None\n        \n        self.time_opt=0\n        \n        # acquisition function\n        self.acq_func = None\n   \n        self.logmarginal=0\n        \n        self.markVirtualObs=[]\n        \n        self.countVirtual=[]\n\n        self.linear_regression = linear_model.LinearRegression()\n\n        self.condition_number=[]\n        \n        # maximum number of augmentations\n        self.max_n_augmentation=10\n        self.threshold_cond=15\n        \n    def init(self, n_init_points=3, seed=1):\n        \"\"\"      \n        Input parameters\n        ----------\n        n_init_points:        # init points\n        \"\"\"\n        np.random.seed(seed)\n\n        # Generate random points\n        SearchSpace=np.copy(self.SearchSpace)\n        SearchSpace[-1,0]=SearchSpace[-1,1] # last dimension, set it to MaxIter\n\n        l = [np.random.uniform(x[0], x[1]) for _ in range(n_init_points) for x in SearchSpace] \n\n        # Concatenate new random points to possible existing\n        # points from self.explore method.\n        temp=np.asarray(l)\n        temp=temp.T\n        init_X=list(temp.reshape((n_init_points,-1)))\n        \n        self.X_original = np.asarray(init_X)\n        self.T_original=self.X_original[:,-1]\n        self.T_original=np.reshape(self.T_original,(n_init_points,-1))\n        \n        self.X_original=self.X_original[:,:-1] # remove the last dimension of MaxEpisode\n        self.X_original=np.reshape(self.X_original,(n_init_points,-1))\n\n        # Evaluate target function at all initialization           \n        y_init_curves, y_init_cost=self.f(init_X)\n\n        y_init_cost=np.atleast_2d(np.asarray(y_init_cost))#.astype('Float64')\n\n        self.Y_curves+=y_init_curves\n\n        # we transform the y_init_curves as the average of [ curves * logistic ]\n        y_init=transform_logistic(y_init_curves,self.gp.logistic_hyper['midpoint'],\\\n                                  self.gp.logistic_hyper['growth'], self.SearchSpace[-1,1])\n        #y_init=y_init_curves\n        y_init=np.reshape(y_init,(n_init_points,1))\n        \n        # record keeping ========================================================\n        self.Y_original = np.asarray(y_init)      \n        self.Y_cost_original=np.reshape(y_init_cost,(-1,1))\n\n        # convert it to scaleX\n        self.X = self.Xscaler.transform(np.asarray(init_X)[:,:-1])#remove the last dimension of MaxEpisode\n        #self.X=self.X[:,:-1]\n        self.X=np.reshape(self.X,(n_init_points,-1))\n\n        self.T = self.Tscaler.transform(self.T_original)\n\n        self.markVirtualObs+=[0]*n_init_points\n\n        # generating virtual observations for each initial point\n        for ii in range(n_init_points):\n            self.generating_virtual_observations(self.X[ii,:],\n                         self.T[ii],[y_init_curves[ii]],y_init_cost[0][ii],IsRandom=False)\n\n        self.Y_cost=(self.Y_cost_original-np.min(self.Y_cost_original))/(np.max(self.Y_cost_original)-np.min(self.Y_cost_original))\n\n        if np.std(self.Y_original)==0:\n            self.Y=(self.Y_original-np.mean(self.Y_original))\n        else:\n            self.Y=(self.Y_original-np.mean(self.Y_original))/np.std(self.Y_original)\n\n       \n    def utility_cost_evaluation(self,x,acq_func,isDebug=False):\n        # this is a wrapper function to evaluate at multiple x(s)\n        \n        \n        def utility_cost_evaluation_single(x,acq_func,isDebug=False):\n            # given a location x, we will evaluate the utility and cost\n            \n            utility=acq_func.acq_kind(x,gp=self.gp)\n            \n            try:\n                mean_cost=self.linear_regression.predict(np.reshape(x,(1,-1)))\n                \n            except:\n                print(x)\n                print(\"bug\")\n    \n            mean_cost=max(0,mean_cost)+0.1 # to avoid <=0 cost\n            \n            #acquisition_function_value= utility_normalized/cost_normalized\n            if 'ei' in acq_func.acq_name:\n                acquisition_function_value= np.log(utility)-np.log(mean_cost)\n            else:\n                acquisition_function_value= np.log(1+np.exp(utility))/np.log(1+np.exp(mean_cost))\n\n            if isDebug==True:\n                print(\"acq_func at the selected point \\t utility:\",np.round(utility,decimals=4),\"\\t cost:\",mean_cost)\n                if utility==0:\n                    print(\"utility =0===============================================================================\")\n       \n            return acquisition_function_value*(-1) # since we will minimize this acquisition function\n        \n        \n        if len(x)==self.dim: # one observation\n            temp=utility_cost_evaluation_single(x,acq_func,isDebug=isDebug)\n            if isDebug==True:\n                return temp\n            else:\n                utility=np.mean(temp)\n        \n        else: # multiple observations\n            utility=[0]*len(x)\n            for idx,val in enumerate(x):\n                temp=utility_cost_evaluation_single(x=val,acq_func=acq_func,isDebug=isDebug)\n                                                     \n                utility[idx]=np.mean(temp)\n                \n            utility=np.asarray(utility)    \t\t\t               \n        return utility   \n    \n        \n    def acq_utility_cost(self):\n        \n        # generate a set of x* at T=MaxIter\n        # instead of running optimization on the whole space, we will only operate on the region of interest\n        # the region of interest in DRL is where the MaxEpisode\n    \n        # we find maximum of EI\n\n        acq={}\n        acq['name']=self.acq_name\n        acq['dim']=self.scaleSearchSpace.shape[0]\n        acq['scaleSearchSpace']=self.scaleSearchSpace   \n    \n        if self.acq_name=='ei_mu_max':# using max of mean(x) as the incumbent\n            \n            # optimie the GP predictive mean function to find the max of mu\n            x_mu_max,mu_max_val=acq_max_with_name(gp=self.gp,scaleSearchSpace=self.scaleSearchSpace,acq_name='mu',IsReturnY=True)\n            acq['mu_max']=  mu_max_val\n\n        myacq=AcquisitionFunction(acq)\n        \n        x_min = acq_min_scipy_kwargs(myfunc=self.utility_cost_evaluation,SearchSpace=self.scaleSearchSpace,\n                        acq_func=myacq, isDebug=False)\n        \n        if self.verbose==True:\n            acq_val=self.utility_cost_evaluation(x_min,myacq,isDebug=False)\n            print(\"selected point from acq func:\",np.round(x_min,decimals=4),\"acq val=log(Utility/Cost)=\",(-1)*np.round(acq_val,decimals=4)) # since we minimize the acq func\n            if np.round(acq_val,decimals=4)==0:\n                print(\"acq value =0\")\n            \n        return x_min\n    \n    \n    def select_informative_location_by_uncertainty(self,n_virtual_obs,x_max,t_max):\n        # this function will select a list of informative locations to place a virtual obs\n        # x_max is the selected hyperparameter\n        # t_max is the selected number of epochs to train\n        \n        \n        SearchSpace=np.copy(self.scaleSearchSpace)\n        for dd in range(self.dim-1):\n            SearchSpace[dd,0],SearchSpace[dd,1]=x_max[dd],x_max[dd]\n            \n        SearchSpace[-1,1]=t_max\n        \n        temp_X,temp_T=self.X.copy(),self.T.copy()\n        temp_gp=copy.deepcopy(self.gp )\n        \n        temp_Y=np.random.random(size=(len(temp_T),1))\n        \n        temp_gp.fit(temp_X,temp_T,temp_Y,self.Y_curves)\n        \n        new_batch_T=None\n\n        pred_var_value=[0]*n_virtual_obs\n        for ii in range(n_virtual_obs):\n            x_max_pred_variance, pred_var_value[ii]=acq_max_with_name(gp=temp_gp,\n                              scaleSearchSpace=SearchSpace,acq_name='pure_exploration',IsReturnY=True)\n            \n            # stop augmenting if the uncertainty is smaller than a threshold\n            # or stop augmenting if the uncertainty is smaller than a threshold\n\n            log_cond=np.log( temp_gp.compute_condition_number() )\n            if log_cond>self.threshold_cond or pred_var_value[ii]<(self.gp.noise_delta+1e-3):\n                break\n          \n            if x_max_pred_variance[-1] in temp_T[-ii:]: # if repetition, stop augmenting\n                break\n            \n            temp_X = np.vstack((temp_X, x_max.reshape((1, -1)))) # append new x\n            temp_T = np.vstack((temp_T, x_max_pred_variance[-1].reshape((1, -1)))) # append new t\n            temp_gp.X,temp_gp.T=temp_X,temp_T\n            temp_Y=np.random.random(size=(len(temp_T),1))\n            \n            temp_gp.fit(temp_X,temp_T,temp_Y,self.Y_curves)\n\n            if new_batch_T is None:\n                new_batch_T=x_max_pred_variance[-1].reshape((1, -1))\n            else:\n                new_batch_T= np.vstack((new_batch_T, x_max_pred_variance[-1].reshape((1, -1))))\n        \n#        if self.verbose:\n#            print(\"pred_var_value at the augmented points:\",np.round( pred_var_value,decimals=4))\n\n        if new_batch_T is None:\n            return [],0\n\n        else:\n            output=np.sort(new_batch_T.ravel()).tolist()\n            return output, len(output)\n\n    \n    def generating_virtual_observations(self,x_max,t_max,y_original_curves,y_cost_original,IsRandom=False):\n        \n        #temp_X_new_original=x_max*self.max_min_gap[:-1]+self.SearchSpace[:-1,0]\n        temp_X_new_original=self.Xscaler.inverse_transform(np.reshape(x_max,(-1,self.dim-1)))\n\n        # selecting MAX number of virtual observations, e.g., we dont want to augment more than 10 points\n        max_n_virtual_obs=np.int(t_max*self.max_n_augmentation)\n        if max_n_virtual_obs==0:\n            self.countVirtual.append(0)\n            return\n        \n        if IsRandom==True:# select informative locations by random uniform   \n            l = [np.random.uniform(0, t_max) for _ in range(max_n_virtual_obs)]\n        else:\n            # select informative locations by uncertainty as in the paper\n            l,n_virtual_obs=self.select_informative_location_by_uncertainty(max_n_virtual_obs,x_max,t_max)        \n            \n        self.countVirtual.append(n_virtual_obs)\n        \n        if self.verbose:\n            np.set_printoptions(suppress=True)\n            print(\"Max #augmented points\",max_n_virtual_obs, \"\\t #augmented points \",len(l),\n                  \"\\t Augmented points: \",np.round(l,decimals=3))\n            \n        l_original=[self.SearchSpace[-1,0]+val*self.max_min_gap[-1] for val in l]\n        #l_original=[self.Tscaler.inverse_transform(val) for val in l]\n                           \n        virtual_obs_t_original=np.asarray(l_original).T\n        virtual_obs_t=np.asarray(l).T\n        \n        # compute y_original for the virtual observations\n        y_virtual_original=[0]*n_virtual_obs\n        for ii in range(n_virtual_obs):\n            \n            idx=np.int(virtual_obs_t_original[ii])\n            \n            temp_curve=y_original_curves[0][:idx+1]\n            self.markVirtualObs.append(1)\n\n            y_virtual_original[ii]=transform_logistic([temp_curve],\\\n                      self.gp.logistic_hyper['midpoint'],self.gp.logistic_hyper['growth'],self.SearchSpace[-1,1])\n           \n            self.X = np.vstack((self.X, x_max.reshape((1, -1))))\n            self.X_original=np.vstack((self.X_original, temp_X_new_original))\n        \n            self.T = np.vstack((self.T, virtual_obs_t[ii].reshape((1, -1))))\n            temp=np.asarray(virtual_obs_t_original[ii])\n            self.T_original=np.vstack((self.T_original, temp.reshape((1, -1))))\n\n\n            self.Y_original = np.append(self.Y_original,[y_virtual_original[ii]])\n            self.Y_curves.append(temp_curve)\n            \n            # interpolating the cost for augmented observation\n            y_cost_estimate=y_cost_original*virtual_obs_t[ii]\n            self.Y_cost_original = np.append(self.Y_cost_original,[y_cost_estimate])\n            \n        \n#        if self.verbose:\n#            temp_y_original_whole_curve=transform_logistic(y_original_curves,\\\n#                               self.gp.logistic_hyper['midpoint'],self.gp.logistic_hyper['growth'],self.SearchSpace[-1,1])\n#            print(np.round(temp_y_original_whole_curve,decimals=4), np.round(y_virtual_original,decimals=4))\n#            \n        \n    def suggest_nextpoint(self): # logistic, time-cost, virtual\n        \"\"\"\n        Main optimization method.\n\n\n        Returns\n        -------\n        x: recommented point for evaluation\n        \"\"\"\n \n        # init a new Gaussian Process============================================\n        self.gp=ProductGaussianProcess(self.scaleSearchSpace,self.gp.hyper,self.gp.logistic_hyper)\n        self.gp.fit(self.X, self.T,self.Y,self.Y_curves)\n            \n        # we store the condition number here=====================================\n        self.condition_number.append(self.gp.cond_num)\n        if self.verbose:\n            print(\"ln of conditioning number of GP covariance matrix\", np.round(np.log(self.gp.cond_num),decimals=1))\n\n        # count number of real observations\n        count=len(self.markVirtualObs)-np.sum(self.markVirtualObs)\n        count=np.int(count)\n\n        # optimize GP hyperparameters and Logistic hyper after 3*d iterations\n        if  len(self.Y)%(2*self.dim)==0:\n\n            hyper=[self.gp.hyper['lengthscale_x'],self.gp.hyper['lengthscale_t'], \\\n                   self.gp.logistic_hyper['midpoint'],self.gp.logistic_hyper['growth']]\n            newlengthscale_x,newlengthscale_t,new_midpoint, new_growth = self.gp.optimize_lengthscale_logistic_hyper(hyper,self.gp.noise_delta)\n            \n            self.gp.hyper['lengthscale_x']=newlengthscale_x\n            self.gp.hyper['lengthscale_t']=self.gp.hyper['lengthscale_t']\n            self.gp.logistic_hyper['midpoint']=new_midpoint\n            self.gp.logistic_hyper['growth']=new_growth\n          \n            if self.verbose:\n                print(\"==estimated lengthscale_x={:.4f}   lengthscale_t={:.3f}   Logistic_m0={:.1f}   Logistic_g0={:.1f}\".format(\n                    newlengthscale_x,newlengthscale_t,new_midpoint,new_growth))\n                \n        # Set acquisition function\n        start_opt=time.time()\n\n        # linear regression is used to fit the cost\n        # fit X and T\n        combine_input=np.hstack((self.X,self.T))\n        self.linear_regression.fit(combine_input,self.Y_cost)\n        \n        # maximize the acquisition function to select the next point =================================\n        x_max_temp=self.acq_utility_cost()\n        x_max=x_max_temp[:-1]\n        t_max=x_max_temp[-1]       \n            \n        # record keeping stuffs ====================================================\n        # record the optimization time\n        finished_opt=time.time()\n        elapse_opt=finished_opt-start_opt\n        self.time_opt=np.hstack((self.time_opt,elapse_opt))\n\n        # this is for house keeping stuff        \n        self.markVirtualObs.append(0)\n\n        self.X = np.vstack((self.X, x_max.reshape((1, -1))))\n        self.T = np.vstack((self.T, t_max.reshape((1, -1))))\n\n        # compute X in original scale\n        temp_X_new_original=self.Xscaler.inverse_transform(np.reshape(x_max,(-1,self.dim-1)))\n        #temp_X_new_original=x_max*self.max_min_gap[:-1]+self.SearchSpace[:-1,0]\n        self.X_original=np.vstack((self.X_original, temp_X_new_original))\n        \n        #temp_T_new_original=t_max*self.max_min_gap[-1]+self.SearchSpace[-1,0]\n        temp_T_new_original=self.Tscaler.inverse_transform(np.reshape(t_max,(-1,1)))\n        self.T_original=np.vstack((self.T_original, temp_T_new_original))\n\n        # evaluate Y using original X\n        x_original_to_test=x_max_temp*self.max_min_gap+self.SearchSpace[:,0]\n\n        # evaluate the black-box function=================================================\n        y_original_curves, y_cost_original= self.f(x_original_to_test)\n        \n        # compute the utility score by transformation\n        y_original=transform_logistic(y_original_curves,\\\n              self.gp.logistic_hyper['midpoint'],self.gp.logistic_hyper['growth'],self.SearchSpace[-1,1])\n        \n        if len(y_original_curves)==1: # list\n            self.Y_curves.append(y_original_curves[0])\n        else:\n            self.Y_curves.append(y_original_curves)\n\n        \n        self.Y_original = np.append(self.Y_original,y_original)\n        self.Y_cost_original = np.append(self.Y_cost_original,y_cost_original)\n\n        # augmenting virtual observations =====================================================\n        self.generating_virtual_observations(x_max,t_max,y_original_curves,y_cost_original[0])\n        \n        # update Y after change Y_original        \n        if np.std(self.Y_original)==0:\n            self.Y=(self.Y_original-np.mean(self.Y_original))\n        else:\n            self.Y=(self.Y_original-np.mean(self.Y_original))/np.std(self.Y_original)\n            \n        self.Y_cost=(self.Y_cost_original-np.min(self.Y_cost_original))/(np.max(self.Y_cost_original)-np.min(self.Y_cost_original))\n                    \n        #if self.verbose:\n        np.set_printoptions(suppress=True)\n\n        print(\"[original scale] x={} t={:.0f} current y={:.2f}, ybest={:.2f}\".format( np.round(self.X_original[-1],decimals=4),\\\n              np.asscalar(self.T_original[-1]),np.asscalar(self.Y_original[-1]), np.asscalar(self.Y_original.max())))",
        "experimental_info": "GP Noise Delta: 5e-4 (default in ProductGaussianProcess), optimized within bounds [1e-3, 1] for lengthscale_x, [1e-2, 1e-2] for noise_upperbound. GP Lengthscale (hyperparameters for the product kernel): Initial lengthscale_x=0.02, lengthscale_t=0.2. Optimized within bounds [0.03, 0.3] for lengthscale_x and [0.3, 0.6] (10*0.03, 2*0.3) for lengthscale_t. Logistic Preference Function Parameters: Initial midpoint=0.0, growth=1.0. Optimized within bounds [-2, 3] for midpoint and [0.5, 2] for growth. Acquisition Function: Cost-aware Expected Improvement ('ei_mu_max'), maximizing log(utility) - log(mean_cost). Cost Model: LinearRegression from sklearn.linear_model.Hyperparameter Optimization Frequency: GP and Logistic hyperparameters are optimized every `2*dim` iterations (where `dim` is the search space dimension, including time). Acquisition Function Maximization: scipy.optimize.minimize with 'L-BFGS-B' method. Multi-start optimization with 3*dim random initial points for cost-aware acquisition, and 1*dim for pure EI/UCB. Optimization Options: `maxiter=10*dim`, `maxfun=20*dim` for acquisition function optimization; `maxiter=30*dim`, `maxfun=30*dim` for hyperparameter optimization. Data Augmentation: Maximum of `10` virtual observations (`max_n_augmentation=10`) per real observation, selected based on maximizing GP predictive uncertainty ('pure_exploration'). Condition Number Threshold: Log of GP covariance matrix condition number `threshold_cond=15` to limit augmentation when the matrix becomes ill-conditioned. Normalization: MinMaxScaler used for input space and time dimensions. Target function values are standardized to N(0,1). Cost values are min-max scaled to [0,1]. Initial Points: `n_init_points=3` random points are used for initialization. Random Seed: `np.random.seed(seed)` is used for reproducibility of initial points and exploration."
      }
    },
    {
      "title": "Multi-Fidelity Bayesian Optimization via Deep Neural Networks",
      "abstract": "Bayesian optimization (BO) is a popular framework to optimize black-box\nfunctions. In many applications, the objective function can be evaluated at\nmultiple fidelities to enable a trade-off between the cost and accuracy. To\nreduce the optimization cost, many multi-fidelity BO methods have been\nproposed. Despite their success, these methods either ignore or over-simplify\nthe strong, complex correlations across the fidelities, and hence can be\ninefficient in estimating the objective function. To address this issue, we\npropose Deep Neural Network Multi-Fidelity Bayesian Optimization (DNN-MFBO)\nthat can flexibly capture all kinds of complicated relationships between the\nfidelities to improve the objective function estimation and hence the\noptimization performance. We use sequential, fidelity-wise Gauss-Hermite\nquadrature and moment-matching to fulfill a mutual information-based\nacquisition function, which is computationally tractable and efficient. We show\nthe advantages of our method in both synthetic benchmark datasets and\nreal-world applications in engineering design.",
      "full_text": "arXiv:2007.03117v4  [cs.LG]  10 Dec 2020 Multi-Fidelity Bayesian Optimization via Deep Neural Networks Shibo Li School of Computing University of Utah Salt Lake City, UT 84112 shibo@cs.utah.edu W ei Xing Scientiﬁc Computing and Imaging Institute University of Utah Salt Lake City, UT 84112 wxing@sci.utah.edu Robert M. Kirby School of Computing University of Utah Salt Lake City, UT 84112 kirby@cs.utah.edu Shandian Zhe School of Computing University of Utah Salt Lake City, UT 84112 zhe@cs.utah.edu Abstract Bayesian optimization (BO) is a popular framework for optim izing black-box functions. In many applications, the objective function ca n be evaluated at mul- tiple ﬁdelities to enable a trade-off between the cost and ac curacy. T o reduce the optimization cost, many multi-ﬁdelity BO methods have b een proposed. De- spite their success, these methods either ignore or over-si mplify the strong, com- plex correlations across the ﬁdelities. While the acquisit ion function is therefore easy and convenient to calculate, these methods can be inefﬁ cient in estimating the objective function. T o address this issue, we propose De ep Neural Network Multi-Fidelity Bayesian Optimization (DNN-MFBO) that can ﬂexibly capture all kinds of complicated relationships between the ﬁdelities t o improve the objective function estimation and hence the optimization performanc e. W e use sequential, ﬁdelity-wise Gauss-Hermite quadrature and moment-matchi ng to compute a mu- tual information based acquisition function in a tractable and highly efﬁcient way. W e show the advantages of our method in both synthetic benchm ark datasets and real-world applications in engineering design. 1 Introduction Bayesian optimization (BO) (Mockus et al., 1978; Snoek et al., 2012) is a general and powerful ap- proach for optimizing black-box functions. It uses a probab ilistic surrogate model (typically Gaus- sian process (GP) (Rasmussen and Williams, 2006)) to estima te the objective function. By repeat- edly maximizing an acquisition function computed with the i nformation of the surrogate model, BO ﬁnds and queries at new input locations that are closer and cl oser to the optimum; meanwhile the new training examples are incorporated into the surrogate m odel to improve the objective estimation. In practice, many applications allow us to query the objecti ve function at different ﬁdelities, where low ﬁdelity queries are cheap yet inaccurate, and high ﬁdeli ty queries more accurate but costly. For example, in physical simulation (Peherstorfer et al., 2018 ), the computation of an objective ( e.g., the elasticity of a part or energy of a system) often involves solving partial differential equations. Running a numerical solver with coarse meshes gives a quick y et rough result; using dense meshes substantially improves the accuracy but dramatically incr eases the computational cost. The multi- ﬁdelity queries enable us to choose a trade-off between the c ost and accuracy. 34th Conference on Neural Information Processing Systems ( NeurIPS 2020), V ancouver, Canada.Accordingly, to reduce the optimization cost, many multi-ﬁ delity BO methods (Huang et al., 2006; Lam et al., 2015; Kandasamy et al., 2016; Zhang et al., 2017; T akeno et al., 2019) have been pro- posed to jointly select the input locations and ﬁdelities to best balance the optimization progress and query cost, i.e., the beneﬁt-cost ratio. Despite their success, these method s often ignore the strong, complex correlations between the function outputs at diffe rent ﬁdelities, and learn an independent GP for each ﬁdelity (Lam et al., 2015; Kandasamy et al., 2016) . Recent works use multi-output GPs to capture the ﬁdelity correlations. However, to avoid intr actable computation of the acquisition function, they have to impose simpliﬁed correlation struct ures. For example, T akeno et al. (2019) assume a linear correlation between the ﬁdelities; Zhang et al. (2017) use kernel convolution to con- struct the cross-covariance function, and have to choose si mple, smooth kernels ( e.g., Gaussian) to ensure a tractable convolution. Therefore, the existing me thods can be inefﬁcient and inaccurate in estimating the objective function, which further lowers th e optimization efﬁciency and increases the cost. T o address these issues, we propose DNN-MFBO, a deep neural n etwork based multi-ﬁdelity Bayesian optimization that is ﬂexible enough to capture all kinds of complex (possibly highly nonlin- ear and nonstationary) relationships between the ﬁdelitie s, and exploit these relationships to jointly estimate the objective function in all the ﬁdelities to impr ove the optimization performance. Speciﬁ- cally, we stack a set of neural networks (NNs) where each NN mo dels one ﬁdelity. In each ﬁdelity, we feed both the original input (to the objective) and output from the previous ﬁdelity into the NN to propagate information throughout and to estimate the com plex relationships across the ﬁdelities. Then, the most challenging part is the calculation of the acq uisition function. For efﬁcient inference and tractable computation, we consider the NN weights in the output layer as random variables and all the other weights as hyper-parameters. W e develop a stoc hastic variational learning algorithm to jointly estimate the posterior of the random weights and h yper-parameters. Next, we sequen- tially perform Gauss-Hermite quadrature and moment matchi ng to approximate the posterior and conditional posterior of the output in each ﬁdelity, based o n which we calculate and optimize an information based acquisition function, which is not only c omputationally tractable and efﬁcient, but also conducts maximum entropy search (W ang and Jegelka, 2017), the state-of-the-art criterion in BO. For evaluation, we examined DNN-MFBO in three benchmark functions and two real-world applica- tions in engineering design that requires physical simulat ions. The results consistently demonstrate that DNN-MFBO can optimize the objective function (in the hi ghest ﬁdelity) more effectively, mean- while with smaller query cost, as compared with state-of-th e-art multi-ﬁdelity and single ﬁdelity BO algorithms. 2 Background Bayesian optimization. T o optimize a black-box objective function f : X → R, BO learns a probabilistic surrogate model to predict the function valu es across the input domain X and quantiﬁes the uncertainty of the predictions. This information is use d to calculate an acquisition function that measures the utility of querying at different input locatio ns, which usually encodes a exploration- exploitation trade-off. By maximizing the acquisition fun ction, BO ﬁnds new input locations at which to query, which are supposed to be closer to the optimum ; meanwhile the new examples are added into the training set to improve the accuracy of the surrogate model. The most commonly used surrogate model is Gaussian process (GP) (Rasmussen an d Williams, 2006). Given the training inputs X = [ x1, . . . , xN ]⊤ and (noisy) outputs y = [ y1, . . . , y N ]⊤, GP assumes the outputs follow a multivariate Gaussian distribution, p(y|X) = N (y|m, K + σ2I) where m are the values of the mean function at the inputs X, K is a kernel matrix on X, [K]ij = k(xi, xj ) (k(·, ·) is the kernel function), and σ2 is the noise variance. The mean function is usually set to the constant function 0 and so m = 0. Due to the multi-variate Gaussian form, given a new input x∗, the posterior distribution of the function output, p ( f(x∗)|x∗, X, y ) is a closed-form conditional Gaussian, and hence is convenient to quantify the uncertainty and calcula te the acquisition function. There are a variety of commonly used acquisition functions, such as expected improvement (EI) (Jones et al., 1998), upper conﬁdent bound (UCB) (Srini vas et al., 2010), entropy search (ES) (Hennig and Schuler, 2012), and predictive entropy sea rch (PES) (Hernández-Lobato et al., 2014). A particularly successful recent addition is the max -value entropy search (MES) (W ang and Jegelka, 2017), which not only enjoys a global util ity measure (like ES and PES), but also is computationally efﬁcient (because it calculates th e entropy of the function output rather than input like in ES/PES). Speciﬁcally, MES maximizes the mutua l information between the function 2value and its maximum f∗ to ﬁnd the next input at which to query, a(x) = I ( f(x), f ∗|D ) = H ( f(x)|D ) − Ep(f∗|D)[H ( f(x)|f∗, D ) ], (1) where I(·, ·) is the mutual information, H(·) the entropy, and D the training examples collected so far. Note that the function values and extremes are consider ed as generated from the posterior in the surrogate model, which includes all the knowledge we have fo r the black-box objective function. Multi-ﬁdelity Bayesian optimization . Many applications allow multi-ﬁdelity queries of the obje c- tive function, {f1(x), . . . , f M (x)}, where the higher (larger) the ﬁdelity m, the more accurate yet costly the query of fm(·). Many studies have extended BO for multi-ﬁdelity settings. For exam- ple, MF-GP-UCB (Kandasamy et al., 2016) starts from the lowe st ﬁdelity ( m = 1 ), and queries the objective at each ﬁdelity until the conﬁdence band excee ds a particular threshold. Despite its effectiveness and theoretical guarantees, MF-GP-UCB lear ns an independent GP surrogate for each ﬁdelity and ignores the strong correlations between the ﬁde lities. Recent works use a multi-output GP to model the ﬁdelity correlations. For example, MF-PES (Z hang et al., 2017) introduces a shared latent function, and uses kernel convolution to derive the c ross-covariance between the ﬁdelities. The most recent work, MF-MES (T akeno et al., 2019) introduces C kernel functions {κc(·, ·)} and, for each ﬁdelity m, C latent features {ωcm}. The covariance function is deﬁned as k ( fm(x), fm′ (x′) ) = ∑ C c=1 (ωcmωcm′ + τcmδmm′ )κc(x, x′), (2) where τcm > 0, δmm′ = 1 if and only if m = m′, and each kernel κc(·, ·) is usually assumed to be stationary, e.g., Gaussian kernel. 3 Multi-Fidelity Modeling with Deep Neural Networks Despite the success of existing multi-ﬁdelity BO methods, they either overlook the strong, complex correlations between different ﬁdelities ( e.g., MF-GP-UCB) or model these correlations with an over-simpliﬁed structure. For example, the convolved GP in MF-PES has to employ simple/smooth kernels (typically Gaussian) for both the latent function a nd convolution operation to obtain an ana- lytical cross-covariance function, which has limited expr essiveness. MF-MES essentially adopts a linear correlation assumption between the ﬁdelities. Acc ording to (2), if we choose each κc as a Gaussian kernel (with amplitude one), we have k ( fm(x), fm′ (x) ) = ω⊤ mωm′ + δmm′ τm where ωm = [ ω1m, . . . , ω Cm ]⊤ and τm = ∑ C c=1 τcm. These correlation structures might be over-simpliﬁed and insufﬁcient to estimate the complicate d relationships between the ﬁdelities ( e.g., highly nonlinear and nonstationary). Hence, they can limit the accuracy of the surrogate model and lower the optimization efﬁciency while increasing the quer y cost. T o address this issue, we use deep neural networks to build a m ulti-ﬁdelity model that is ﬂexible enough to capture all kinds of complicated relationships be tween the ﬁdelities, taking advantage of the relationships to promote the accuracy of the surrogat e model. Speciﬁcally, for each ﬁdelity m > 1, we introduce a neural network (NN) parameterized by {wm, θm}, where wm are the weights in the output layer and θm the weights in all the other layers. Denote the NN input by xm, the output by fm(x) and the noisy observation by ym(x). The model is deﬁned as xm = [ x; fm−1(x)], f m(x) = w⊤ mφθ m (xm), y m(x) = fm(x) + ǫm, (3) where x is the original input to the objective function, φθ m (xm) is the output vector of the second last layer (hence parameterized by θm) which can be viewed as a set of nonlinear basis functions, an d ǫm ∼ N (ǫm|0, σ2 m) is a Gaussian noise. The input xm is obtained by appending the output from the previous ﬁdelity to the original input. Through a series of l inear and nonlinear transformations inside the NN, we obtain the output fm(x). In this way, we digest the information from the lower ﬁdelit ies, and capture the complex relationships between the current a nd previous ﬁdelities by learning a nonlinear mapping fm(x) = h(x, fm−1(x)), where h(·) is fulﬁlled by the NN. When m = 1 , we set xm = x. A graphical representation of our model is given in Fig. 1 of the supplementary material. W e assign a standard normal prior over each wm. Following (Snoek et al., 2015), we con- sider all the remaining NN parameters as hyper-parameters. Given the training set D = {{(xnm, ynm)}Nm n=1}M m=1, the joint probability of our model is p(W, Y|X , Θ , s) = ∏ M m=1 N (wm|0, I) ∏ Nm n=1 N ( ynm|fm(xnm), σ2 m ) , (4) 3where W = {wm}, Θ = {θm}, s = [ σ2 1, . . . , σ 2 M ]⊤, and X , Y are the inputs and outputs in D. In order to obtain the posterior distribution of our model (w hich is in turn used to compute the ac- quisition function), we develop a stochastic variational l earning algorithm. Speciﬁcally, for each wm, we introduce a multivariate Gaussian posterior, q(wm) = N (wm|µm, Σ m). W e further pa- rameterize Σ m with its Cholesky decomposition to ensure the positive deﬁn iteness, Σ m = LmL⊤ m where Lm is a lower triangular matrix. W e assume q(W) = ∏ M m=1 q(wm), and construct a varia- tional model evidence lower bound (ELBO), L ( q(W), Θ , s ) = Eq[log(p(W, Y|X , Θ , s)/q(W))]. W e then maximize the ELBO to jointly estimate the variationa l posterior q(W) and all the other hyper-parameters. The ELBO is analytically intracta ble, and we use the reparameterization trick (Kingma and W elling, 2013) to conduct efﬁcient stocha stic optimization. The details are given in the supplementary material (Sec. 3). 4 Multi-Fidelity Optimization with Max-V alue Entropy Search W e now consider an acquisition function to select both the ﬁd elities and input locations at which we query during optimization. Following (T akeno et al., 2019) , we deﬁne the acquisition function as a(x, m) = 1 λm I (f∗, fm(x)|D) = 1 λm ( H ( fm(x)|D ) − Ep(f∗|D) [ H ( fm(x)|f∗, D )]) (5) where λm > 0 is the cost of querying with ﬁdelity m. In each step, we maximize the acquisition function to ﬁnd a pair of input location and ﬁdelity that prov ides the largest beneﬁt-cost ratio. However, given the model inference result, i.e., p(W|D) ≈ q(W), a critical challenge is to compute the posterior distribution of the output in each ﬁdelity, p(fm(x)|D), and use them to compute the acquisition function. Due to the nonlinear coupling of the o utputs in different ﬁdelities (see (3)), the computation is analytically intractable. T o address th is issue, we conduct ﬁdelity-wise moment matching and Gauss-Hermite quadrature to approximate each p(fm(x)|D) as a Gaussian distribu- tion. 4.1 Computing Output Posteriors Speciﬁcally, we ﬁrst assume that we have obtained the posterior of the output for ﬁdelity m − 1, p ( fm−1(x)|D ) ≈ N ( fm−1|αm−1(x), ηm−1(x) ) . For convenience, we slightly abuse the notation and use fm−1 and fm to denote fm−1(x) and fm(x), respectively. Now we consider calculat- ing p(fm|D). According to (3), we have fm = w⊤ mφθ m ([x; fm−1]). Based on our variational posterior q(wm) = N (wm|µm, LmL⊤ m), we can immediately derive the conditional posterior p(fm|fm−1, D) = N ( fm|u(fm−1, x), γ(fm−1, x) ) where u(fm−1, x) = µ⊤ mφθ m ([x; fm−1]) and γ(fm−1, x) = ∥L⊤ mφθ m ([x; fm−1])∥2. Here ∥ · ∥ 2 is the square norm. W e can thereby read out the ﬁrst and second conditional moments, E[fm|fm−1, D] = u(fm−1, x), E[f2 m|fm−1, D] = γ(fm−1, x) + u(fm−1, x)2. (6) T o obtain the moments, we need to take the expectation of the c onditional moments w .r.t p(fm−1|D) ≈ N ( fm−1|αm−1(x), ηm−1(x) ) . While the conditional moments are nonlinear to fm−1 and their expectation is not analytical, we can use Gauss-He rmite quadrature to give an accu- rate, closed-form approximation, E[fm|D] = Ep(fm−1|D)E[fm|fm−1, D] ≈ ∑ k gk · u(tk, x), E[f2 m|D] = Ep(fm−1|D)E[f2 m|fm−1, D] ≈ ∑ k gk · [γ(tk, x) + u(tk, x)2], (7) where {gk} and {tk} are quadrature weights and nodes, respectively. Note that e ach node tk is determined by αm−1(x) and ηm−1(x). W e then use these moments to construct a Gaus- sian posterior approximation, p(fm|D) ≈ N ( fm|αm(x), ηm(x) ) where αm(x) = E[fm|D] and ηm(x) = E[f2 m|D] − E[fm|D]2. This is called moment matching, which is widely used and ver y successful in approximate Bayesian inference, such as expe ctation-propagation (Minka, 2001). One may concern if the quadrature will give a positive variance. This is guaranteed by the follow lemma. Lemma 4.1. As long as the conditional posterior variance γ(fm−1, x) > 0, the posterior variance ηm(x), computed based on the quadrature in (7), is positive. 4The proof is given in the supplementary material. Following the same procedure, we can compute the posterior of the output in ﬁdelity m + 1. Note that when m = 1 , we do not need quadrature because the input of the NN is the same as the original input, not inclu ding other NN outputs. Hence, we can derive the Gaussian posterior outright from q(w1) — p(f1(x)|D) = N ( f1(x)|α1(x), η1(x) ) , where α1(x) = µ⊤ 1φθ 1 (x) and η1(x) = ∥L⊤ 1φθ 1 (x)∥2. 4.2 Computing Acquisition Function Given the posterior of the NN output in each ﬁdelity,p ( fm(x)|D) ≈ N (fm(x)|αm(x), ηm(x) ) (1 ≤ m ≤ M), we consider how to compute the acquisition function (5). Du e to the Gaussian posterior, the ﬁrst entropy term is straightforward, H ( fm(x)|D ) = 1 2 log ( 2πeηm(x) ) . The second term — a conditional entropy, however, is intractable. Hence, we f ollow (W ang and Jegelka, 2017) to use a Monte-Carlo approximation, Ep(f∗|D)[H ( fm(x)|f∗, D ) ] ≈ 1 |F| ∑ f∗∈F∗ H ( fm(x)|f∗, D ) , where F∗ are a collection of independent samples of the function maxi mums based on the posterior distribution of our model. T o obtain a sample of the function maximum, we ﬁrst generate a posterior sample for each wm, according to q(wm) = N (wm|µm, LmL⊤ m). W e replace each wm by their sample in calculating fM (x) so as to obtain a posterior sample of the objective function. W e then maximize this sample function to obtain one instance of f∗. W e use L-BFGS (Liu and Nocedal, 1989) for optimization. Given f∗, the computation of H ( fm(x)|f∗, D ) = H ( fm(x)| max fM (x) = f∗, D ) is still in- tractable. W e then follow (W ang and Jegelka, 2017) to calcul ate H ( fm(x)|fM (x) ≤ f∗, D ) instead as a reasonable approximation. For m = M, the entropy is based on a truncated Gaussian distribu- tion, p(fM (x)|fM (x) ≤ f∗, D) ∝ N ( fM (x)|αM (x), ηM (x) ) 1(fM (x) ≤ f∗) where 1(·) is the indicator function, and is given by H ( fm(x)|fM (x) ≤ f∗, D ) = log ( √ 2πeηM (x)Φ( β) ) − β · N (β|0, 1)/ ( 2Φ( β) ) , (8) where Φ( ·) is the cumulative density function (CDF) of the standard nor mal distribution, and β =( f∗ − αM (x) ) / √ ηM (x). When m < M , the entropy is based on the conditional distribution p(fm(x)|fM (x) ≤ f∗, D) = 1 Z · p ( fm(x)|D ) p(fM (x) ≤ f∗|fm(x), D) ≈ 1 Z · N ( fm(x)|αm(x), ηm(x) ) p(fM (x) ≤ f∗|fm(x), D). (9) where Z is the normalizer. T o obtain p(fM (x) ≤ f∗|fm(x), D), we ﬁrst consider how to compute p(fM (x)|fm(x), D). According to (3), it is trivial to derive that p(fm+1(x)|fm(x), D) = N ( fm+1|ˆαm+1(x, fm), ˆηm+1(x, fm) ) , where ˆαm+1(x, fm) = µ⊤ m+1φθ m+1 ([x; fm]) and ˆηm+1(x, fm) = ∥L⊤ m+1φθ m+1 ([x; fm])∥2. Note that we again use fm+1 and fm to denote fm+1(x) and fm(x) for convenience. Next, we follow the same method as in Section 4.1 to sequentially obtain the c onditional posterior for each higher ﬁdelity, p(fm+k|fm, D)(1 < k ≤ M − m). In more detail, we ﬁrst base on q(wm+k) to derive the conditional moments E(fm+k|fm+k−1, fm, D) and E(f2 m+k|fm+k−1, fm, D). They are calculated in the same way as in (6), because fm+k are independent to fm conditioned on fm+k−1. Then we take the expectation of the conditional moments w .r.t p(fm+k−1|fm, D) (that is Gaussian) to obtain E(fm+k|fm, D) and E(f2 m+k|fm, D). This again can be done by Gauss-Hermite quadrature. Finally, we use these moments to construct a Gaussian approx imation to the conditional posterior, p(fm+k|fm, D) ≈ N ( fm+k|ˆαm+k(x, fm), ˆηm+k(x, fm) ) , (10) where ˆαm+k(x, fm) = E(fm+k|fm, D) and ˆηm+k(x, fm) = E(f2 m+k|fm, D) − E(fm+k|fm, D)2. According to Lemma 4.1, we guarantee ˆηm+k(x, fm) > 0. Now we can obtain p(fm(x)|fM (x) ≤ f∗, D) ≈ 1 Z · N ( fm|αm(x), ηm(x) ) Φ ( f∗ − ˆαM (x, fm)√ ˆηM (x, fm) ) . (11) 5In order to compute the entropy analytically, we use moment m atching again to approximate this distribution as a Gaussian distribution. T o this end, w e use Gauss-Hermite quadrature to compute three integrals, Z = ∫ R(fm) · N ( fm|αm(x), ηm(x) ) dfm, Z1 = ∫ fmR(fm) · N ( fm|αm(x), ηm(x) ) dfm, and Z2 = ∫ f2 mR(fm) · N ( fm|αm(x), ηm(x) ) dfm, where R(fm) = Φ ( (f∗ − ˆαM (x, fm))/ √ ˆηM (x, fm) ) . Then we can obtain E[fm|fM ≤ f∗, D] = Z1/Z and E[f2 m|fM ≤ f∗, D] = Z2/Z, based on which we approximate p(fm(x)|fM (x) ≤ f∗, D) ≈ N ( fm|Z1/Z, Z2/Z − Z2 1 /Z2) . (12) Following the same idea to prove Lemma 4.1, we can show that th e variance is non-negative. See the details in the supplementary material (Sec. 5). With the Gaussian form, we can analytically compute the entropy, H(fm(x)|fM (x) ≤ f∗, D) = 1 2 log ( 2πe(Z2/Z − Z2 1 /Z2) ) . Although our calculation of the acquisition function is qui te complex, due to the analytical form, we can use automatic differentiation libraries (Baydin et al. , 2017), to compute the gradient efﬁciently and robustly for optimization. In our experiments, we used T ensorFlow (Abadi et al., 2016) and L-BFGS to maximize the acquisition function to ﬁnd the ﬁdeli ty and input location we query at in the next step. Our multi-ﬁdelity Bayesian optimization alg orithm is summarized in Algorithm 1. Algorithm 1 DNN-MFBO ( D, M, T , {λm}M m=1 ) 1: Learn the DNN-based multi-ﬁdelity model (4) on Dwith stochastic variational learning. 2: for t = 1, . . . , T do 3: Generate F∗ from the variational posterior q(W) and the NN output at ﬁdelity M, i.e., fM (x) 4: (xt, m t) = argmaxx∈X ,1≤ m≤ M MutualInfo(x, m, λ m, F∗ , D, M ) 5: D←D∪{ (xt, m t)} 6: Re-train the DNN-based multi-ﬁdelity model on D 7: end for Algorithm 2 MutualInfo(x, m, λm, F∗, D, M) 1: Compute each p(fm(x)|D) ≈N ( fm|α m(x), η m(x) ) (Sec. 4.1) 2: H0 ← 1 2 log(2πeη m(x)), H1 ← 0 3: for f∗ ∈F ∗ do 4: if m = M then 5: Use (8) to compute H(fm|fM ≤ f∗ , D) and add it to H1 6: else 7: Compute p(fm(x)|fM (x), D) following (10) and p(fm(x)|fM (x) ≤ f∗ , D) with (12) 8: H1 ← H1 + 1 2 log ( 2πe (Z2/Z − Z2 1 /Z 2) ) 9: end if 10: end for 11: return (H0 − H1/ |F∗ |)/λ m 5 Related W ork Most surrogate models used in Bayesian optimization (BO) (Mockus, 2012; Snoek et al., 2012) are based on Gaussian processes (GPs) (Rasmussen and Williams, 2006), partly because their closed- form posteriors (Gaussian) are convenient to quantify the u ncertainty and calculate the acquisition functions. However, GPs are known to be costly for training, and the exact inference takes O(N3) time complexity ( N is the number of samples). Recently, Snoek et al. (2015) show ed deep neural networks (NNs) can also be used in BO and performs very well. T he training of NNs are much more efﬁcient ( O(N)). T o conveniently quantify the uncertainty, Snoek et al. (2 015) consider the NN weights in the output layer as random variables and all the ot her weights as hyper-parameters (like the kernel parameters in GPs). They ﬁrst obtain a point estim ation of the hyper-parameters (typically through stochastic training). Then they ﬁx the hyper-param eters and compute the posterior distribu- tion of the random weights (in the last layer) and NN output — t his can be viewed as the inference for Bayesian linear regression. In our multi-ﬁdelity model , we also only consider the NN weights in the output layer of each ﬁdelity as random variables. Howe ver, we jointly estimate the hyper- parameters and posterior distribution of the random weight s. Since the NN outputs in successive ﬁdelities are coupled non-linearly, we use the variational estimation framework (W ainwright et al., 2008). 6Many multi-ﬁdelity BO algorithms have been proposed. For ex ample, Huang et al. (2006); Lam et al. (2015); Picheny et al. (2013) augmented the standa rd EI for the multi-ﬁdelity settings. Kandasamy et al. (2016, 2017) extended GP upper conﬁdence bo und (GP-UCB) (Srinivas et al., 2010). Poloczek et al. (2017); Wu and Frazier (2017) develop ed multi-ﬁdelity BO with knowledge gradients (Frazier et al., 2008). EI is a local measure of the utility and UCB requires us to explicitly tune the exploit-exploration trade-off. The recent works a lso extend the information-based acqui- sition functions to enjoy a global utility for multi-ﬁdelit y optimization, e.g., (Swersky et al., 2013; Klein et al., 2017) using entropy search (ES), (Zhang et al., 2017; McLeod et al., 2017) (PES) using predictive entropy search (PES), and (Song et al., 2019; T ak eno et al., 2019) using max-value en- tropy search (MES). Note that ES and PES are computationally more expensive than MES because the former calculate the entropy of the input (vector) and la tter the output scalar. Despite the great success of the existing methods, they either ignore or overs implify the complex correlations across the ﬁdelities, and hence might hurt the accuracy of the surro gate model and further the optimiza- tion performance. For example, Picheny et al. (2013); Lam et al. (2015); Kandasamy et al. (2016); Poloczek et al. (2017) train an independent GP for each ﬁdeli ty; Song et al. (2019) combined all the examples indiscriminately to train a single GP; Huang et al. (2006); T akeno et al. (2019) assume a linear correlation structure between ﬁdelities, and Zhang et al. (2017) used the convolution opera- tion to construct the covariance and so the involved kernels have to be simple and smooth enough (yet less expressive) to obtain an analytical form. T o overc ome these limitations, we propose an NN-based multi-ﬁdelity model, which is ﬂexible enough to ca pture arbitrarily complex relation- ships between the ﬁdelities and to promote the performance o f the surrogate model. Recently, a NN-based multi-task model (Perrone et al., 2018) was also de veloped for BO and hyper-parameter transfer learning. The model uses an NN to construct a shared feature map ( i.e., bases) across the tasks, and generates the output of each task by a linear combi nation of the latent features. While this model can also be used for multi-ﬁdelity BO (each task co rresponds to one ﬁdelity), it views each ﬁdelity as symmetric and does not reﬂect the monotonici ty of function accuracy/importance along with the ﬁdelities. More important, the model does not capture the correlation between ﬁdeli- ties — given the shared bases, different ﬁdelities are assum ed to be independent. Finally, while a few algorithms deal with continuous ﬁdelities, e.g., (Kandasamy et al., 2017; McLeod et al., 2017; Wu and Frazier, 2017), we focus on discrete ﬁdelities in this work. 6 Experiment 6.1 Synthetic Benchmarks W e ﬁrst evaluated DNN-MFBO in three popular synthetic bench mark tasks. (1) Branin func- tion (Forrester et al., 2008; Perdikaris et al., 2017) with t hree ﬁdelities. The input is two dimensional and ranges from [−5, 10] × [0, 15]. (2) P ark1function (Park, 1991) with two ﬁdelities. The input is four dimensional and each dimension is in [0, 1]. (3) Levy function (Laguna and Martí, 2005), having three ﬁdelities and two dimensional inputs. The doma in is [−10, 10] × [−10, 10]. For each objective function, between ﬁdelities can be nonlinear and /or nonstationary transformations. The detailed deﬁnitions are given in the supplementary materia l (Sec. 1). Competing Methods. W e compared with the following popular and state-of-the-ar t multi- ﬁdelity BO algorithms: (1) Multi-Fidelity Sequential Krig ing (MF-SKO) (Huang et al., 2006) that models the function of the current ﬁdelity as the functi on of the previous ﬁdelity plus a GP , (2) MF-GP-UCB (Kandasamy et al., 2016), (3) Multi- Fidelity Predictive Entropy Search (MF-PES) (Zhang et al., 2017) and (4) Multi-Fidelity Maximum Entropy Search (MF- MES) (T akeno et al., 2019). These algorithms extend the stan dard BO with EI, UCB, PES and MES principles respectively. W e also compared with (5) mult i-task NN based BO (MTNN-BO) by Perrone et al. (2018), where a set of latent bases (generat ed by an NN) are shared across the tasks, and the output of each task ( i.e., ﬁdelity) is predicted by a linear combination of the bases. W e tested the single ﬁdelity BO with MES, named as (6) SF-MES (W a ng and Jegelka, 2017). SF-MES only queries the objective at the highest ﬁdelity. Settings and Results.W e implemented our method and MTNN-BO with T ensorFlow . W e used the original Matlab implementatio n for MF-GP-UCB ( https://github.com/kirthevasank/mf-gp-ucb), MF-PES ( https://github.com/YehongZ/MixedTypeBO) and SF- MES ( https://github.com/zi-w/Max-value-Entropy-Search/ ), and 7500 1000 1500 2000 2500 Total Cos  10−1 100 101 102 103 Simple Regre  DNN -MFBO MF-MES MF-PES MF-SKO MF-GP-UCB SF-MES MTNN-BO (a) Branin 25 50 75 100 125 150 175 200 Total Cost 10−6 10−4 10−2 100 (b) P ark1 500 1000 1500 2000 2500 Total Cost 10 0 10 1 10 2 (c) Levy 100 200 300 400 500 Total Cost 170 180 190190 200 210 220Queried Maximum (d) V ibration Plate 500 1000 1500 2000 2500 Total Cost 10 −2 10 −1 10 0 10 1 10 2 10 3 Inference Regret (e) Branin 25 50 75 100 125 150 175 200 Total Cost 10−6 10−3 10−1 100 101 (f) P ark1 500 1000 1500 2000 2500 Total Cost 10−2 10−1 100 101 102 (g) Levy 100 200 300 400 500 Total cost 1.05 1.10 1.15 1.20 1.25 1.3 × 100 1.35 × 100 1.4 × 100 Queried Minimum (h) Thermal Conductor Figure 1: Simple and Inference regrets on three synthetic benchmark t asks (a-c, e-g) and the optimum queried function values (d, h) along with the query cost. Python/Numpy implementation for MF-MES. MF-SKO was implemented with Python as well. W e used the default settings in their implementations . SF-MES and MF-GP-UCB used the Squared Exponential (SE) kernel. MF-PES used the Automa tic Relevance Determination (ARD) kernel. MF-MES and MF-SKO used the Radial Basis (RBF) k ernel (within each ﬁdelity). For DNN-MFBO and MTNN-BO, we used ReLU activation. T o identi fy the architecture of the neural network in each ﬁdelity and learning rate, we ﬁrst ran the AutoML tool SMAC3 (https://github.com/automl/SMAC3) on the initial training dataset (we randomly split the data into half for training and the other half for test, an d repeated multiple times to obtain a cross-validation accuracy to guide the search) and then man ually tuned these hyper-parameters. The depth and width of each network were chosen from [2, 12] and [32, 512], and the learning rate [10−5, 10−1]. W e used ADAM (Kingma and Ba, 2014) for stochastic training. The number of epochs was set to 5, 000, which is enough for convergence. T o optimize the acquisiti on function, MF-MES and MF-PES ﬁrst run a global optimization algorithm D IRECT (Jones et al., 1993; Gablonsky et al., 2001) and then use the results as the initia lization to run L-BFGS. SF-MES uses a grid search ﬁrst and then runs L-BFGS. DNN-MFBO and MTNN-BO d irectly use L-BFGS with a random initialization. T o obtain the initial training poin ts, we randomly query in each ﬁdelity. For Branin and Levy, we generated 20, 20 and 2 training samples for the ﬁrst, second and third ﬁdelity, respectively. For P ark1, we generated 5 and 2 examples for the ﬁrst and second ﬁdelity. The query costs is (λ1, λ2, λ3) = (1 , 10, 100). W e examined the simple regret (SR) and inference regret (IR ). SR is deﬁned as the difference between the global optimum and the best queried function value so far: maxx∈X fM (x) − maxi∈{i|i∈[t],mi=M} fM (xi); IR is the difference between the global optimum and the optimum estimated by the surrogate model: maxx∈X fM (x) − maxx∈X ˆfM (x) where ˆfM (·) is the estimated objective. W e repeated the experiment for ﬁ ve times, and report on average how the simple and inference regrets vary along with the query cost in Fig. 1 (a-c, e-g). W e also show the standard error bars. As we can see, in all the t hree tasks, DNN-MFBO achieves the best regrets with much smaller or comparable querying co sts. The best regrets obtained by our method are much smaller (often orders of magnitude) than the baselines. In particular, DNN-MFBO almost achieved the global optimum after querying one point (IR < 10−6) (Fig. 1f). These results demonstrate our DNN based surrogate model is more ac curate in estimating the objective. Furthermore, our method spends less or comparable cost to ac hieve the best regrets, showing a much better beneﬁt/cost ratio. 6.2 Real-W orld Applications in Engineering Design Mechanical Plate Vibration Design.W e aim to optimize three material properties, Y oung’s modu- lus (in [1 × 1011, 5 × 1011]), Poisson’s ratio (in [0.2, 0.6]) and mass density (in [6 × 103, 9 × 103]), to maximize the fourth vibration mode frequency of a 3-D simp ly supported, square, elastic plate, of size 10 × 10 × 1. T o evaluate the frequency, we need to run a numerical solver on the discretized 8DNN-MFBO MF-MES MF-GP-UCB MF-PESMF-SKOMTNN-BO 0 50 100 150 200Time (seconds) (a) Branin DNN-MFBO MF-MES MF-GP-UCB MF-PESMF-SKOMTNN-BO 0 50 100 150 200 250Time (seconds) (b) P ark1 DNN-MFBO MF-MES MF-GP-UCB MF-PESMF-SKOMTNN-BO 0 50 100 150 200Time (seconds) (c) Levy DNN-MFBO MF-MES MF-GP-UCB MF-PESMF-SKOMTNN-BO 0 50 100 150 200Time (seconds) (d) V ibration Plate DNN-MFBO MF-MES MF-GP-UCB MF-PESMF-SKOMTNN-BO 0 50 100 150 200Time (seconds) (e) Heat Conductor Figure 2: The average query time on three synthetic tasks (a- c) and two real-world applications (d-e). plate. W e considered two ﬁdelities, one with a coarse mesh an d the other a dense mesh. The details about the settings of the solvers are provided the supplemen tary document. Thermal Conductor Design. Given the property of a particular thermal conductor, our go al is to optimize the shape of the central hole where we install/ﬁx th e conductor to make the heat conduction (from left to right) to be as as fast as possible. The shape of t he hole (an ellipse) is described by three parameters: x-radius, y-radius and angle. W e used the time to reach 70 degrees as the objective function value and we want to minimize the objective. W e need to run numerical solvers to calculate the objective. W e considered two ﬁdelities. The details are given in the supplementary material. For both problems, we randomly queried at 20 and 5 inputs in th e low and high ﬁdelities respectively, at the beginning. The query cost is (λ1, λ2) = (1 , 10). W e then ran each algorithm until convergence. W e repeated the experiments for ﬁve times. Since we do not kno w the ground-truth of the global optimum, we report how the average of the best function value s queried improves along with the cost. The results are shown in Fig. 1d and h. As we can see, in bo th applications, DNN-MFBO reaches the maximum/minimum function values with a smaller cost than all the competing methods, which is consistent with results in the synthetic benchmark tasks. Finally, we examined the average query time of each multi-ﬁd elity BO method, which is spent in calculating and optimizing the acquisition function to ﬁnd new inputs and ﬁdelities to query at in each step. For a fair comparison, we ran all the methods on a Li nux workstation with a 16-core Intel(R) Xeon(R) CPU E5-2670 and 16GB RAM. As shown in Fig. 2, DNN-MFBO spends much less time than MF-MES and MF-PES that are based on multi-outp ut GPs, and the speed of DNN- MFBO is close or comparable to MF-GP-UCB and MF-SKO, which us e independent and additive GPs for each ﬁdelity, respectively. On average, DNN-MFBO ac hieves 25x and 60x speedup over MF-MES and MF-PES. One reason might be that DNN-MFBO simply a dopts a random initializa- tion for L-BFGS rather than runs an expensive global optimiz ation (so does MTNN-BO). However, as we can see from Fig. 1, DNN-MFBO still obtains new input and ﬁdelities that achieve much better beneﬁt/cost ratio. On the other hand, the close speed to MF-GP-UCB and MF-SKO also demonstrate that our method is efﬁcient in acquisition func tion calculation, despite its seemingly complex approximations. 7 Conclusion W e have presented DNN-MFBO, a deep neural network based mult i-ﬁdelity Bayesian optimization algorithm. Our DNN surrogate model is ﬂexible enough to capt ure the strong and complicated relationships between ﬁdelities and promote objective est imation. Our information based acquisition function not only enjoys a global utility measure, but also i s computationally tractable and efﬁcient. Acknowledgments This work has been supported by DARP A TRADES A ward HR0011-17 -2-0016 and NSF IIS- 1910983. Broader Impact This work can be used in a variety of engineering design probl ems that involve intensive computa- tion, e.g., ﬁnite elements or differences. Hence, the work has potentia l positive impacts in the society if it is used to design passenger aircrafts, biomedical devi ces, automobiles, and all the other devices 9or machines that can beneﬁt human lives. At the same time, thi s work may have some negative consequences if it is used to design weapons or weapon parts. References Abadi, M., Barham, P ., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M., Ghemawat, S., Irving, G., Isard, M., et al. (2016). T ensorﬂow: A system for large-s cale machine learning. In 12th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 16) , pages 265–283. Baydin, A. G., Pearlmutter, B. A., Radul, A. A., and Siskind, J. M. (2017). Automatic differentiation in machine learning: a survey. The Journal of Machine Learning Research, 18(1):5595–5637. Forrester, A., Sobester, A., and Keane, A. (2008). Engineering design via surrogate modelling: a practical guide. John Wiley & Sons. Frazier, P . I., Powell, W . B., and Dayanik, S. (2008). A knowl edge-gradient policy for sequential information collection. SIAM Journal on Control and Optimization, 47(5):2410–2439. Gablonsky, J. M. et al. (2001). Modiﬁcations of the DIRECT Algorithm. PhD thesis. Hennig, P . and Schuler, C. J. (2012). Entropy search for info rmation-efﬁcient global optimization. Journal of Machine Learning Research, 13(Jun):1809–1837. Hernández-Lobato, J. M., Hoffman, M. W ., and Ghahramani, Z. (2014). Predictive entropy search for efﬁcient global optimization of black-box functions. I n Advances in neural information processing systems, pages 918–926. Huang, D., Allen, T . T ., Notz, W . I., and Miller, R. A. (2006). Sequential kriging optimization using multiple-ﬁdelity evaluations. Structural and Multidisciplinary Optimization, 32(5):369–382. Incropera, F . P ., Lavine, A. S., Bergman, T . L., and DeWitt, D . P . (2007). Fundamentals of heat and mass transfer. Wiley. Jones, D. R., Perttunen, C. D., and Stuckman, B. E. (1993). Li pschitzian optimization without the lipschitz constant. Journal of optimization Theory and Applications, 79(1):157–181. Jones, D. R., Schonlau, M., and W elch, W . J. (1998). Efﬁcient global optimization of expensive black-box functions. Journal of Global optimization, 13(4):455–492. Kandasamy, K., Dasarathy, G., Oliva, J. B., Schneider, J., a nd Póczos, B. (2016). Gaussian process bandit optimisation with multi-ﬁdelity evaluations. In Advances in Neural Information Processing Systems, pages 992–1000. Kandasamy, K., Dasarathy, G., Schneider, J., and Póczos, B. (2017). Multi-ﬁdelity bayesian optimi- sation with continuous approximations. In Proceedings of the 34th International Conference on Machine Learning-V olume70, pages 1799–1808. JMLR. org. Kingma, D. P . and Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980. Kingma, D. P . and W elling, M. (2013). Auto-encoding variati onal bayes. arXiv preprint arXiv:1312.6114. Klein, A., Falkner, S., Bartels, S., Hennig, P ., and Hutter, F . (2017). Fast bayesian optimization of machine learning hyperparameters on large datasets. In Artiﬁcial Intelligence and Statistics, pages 528–536. Laguna, M. and Martí, R. (2005). Experimental testing of adv anced scatter search designs for global optimization of multimodal functions. Journal of Global Optimization, 33(2):235–255. Lam, R., Allaire, D. L., and Willcox, K. E. (2015). Multiﬁdel ity optimization using statistical surrogate modeling for non-hierarchical information sour ces. In 56th AIAA/ASCE/AHS/ASC Structures, Structural Dynamics, and Materials Conference, page 0143. 10Liu, D. C. and Nocedal, J. (1989). On the limited memory bfgs m ethod for large scale optimization. Mathematical programming, 45(1-3):503–528. McLeod, M., Osborne, M. A., and Roberts, S. J. (2017). Practi cal bayesian optimization for variable cost objectives. arXiv preprint arXiv:1703.04335. Minka, T . P . (2001). Expectation propagation for approxima te bayesian inference. In Proceedings of the Seventeenth conference on Uncertainty in artiﬁcial intelligence, pages 362–369. Mockus, J. (2012). Bayesian approach to global optimization: theory and applications, volume 37. Springer Science & Business Media. Mockus, J., Tiesis, V ., and Zilinskas, A. (1978). The applic ation of Bayesian methods for seeking the extremum. T owards global optimization, 2(117-129):2. Park, J. S. (1991). Tuning complex computer codes to data and optimal designs. Peherstorfer, B., Willcox, K., and Gunzburger, M. (2018). S urvey of multiﬁdelity methods in uncer- tainty propagation, inference, and optimization. Siam Review, 60(3):550–591. Perdikaris, P ., Raissi, M., Damianou, A., Lawrence, N., and Karniadakis, G. E. (2017). Nonlinear in- formation fusion algorithms for data-efﬁcient multi-ﬁdel ity modelling. Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences, 473(2198):20160751. Perrone, V ., Jenatton, R., Seeger, M. W ., and Archambeau, C. (2018). Scalable hyperparameter transfer learning. In Advances in Neural Information Processing Systems, pages 6845–6855. Picheny, V ., Ginsbourger, D., Richet, Y ., and Caplin, G. (20 13). Quantile-based optimization of noisy computer experiments with tunable precision. T echnometrics, 55(1):2–13. Poloczek, M., W ang, J., and Frazier, P . (2017). Multi-infor mation source optimization. In Advances in Neural Information Processing Systems, pages 4288–4298. Rasmussen, C. E. and Williams, C. K. I. (2006). Gaussian Processes for Machine Learning. MIT Press. Snoek, J., Larochelle, H., and Adams, R. P . (2012). Practica l bayesian optimization of machine learning algorithms. In Advances in neural information processing systems, pages 2951–2959. Snoek, J., Rippel, O., Swersky, K., Kiros, R., Satish, N., Su ndaram, N., Patwary, M., Prabhat, M., and Adams, R. (2015). Scalable bayesian optimization us ing deep neural networks. In International conference on machine learning, pages 2171–2180. Song, J., Chen, Y ., and Y ue, Y . (2019). A general framework fo r multi-ﬁdelity bayesian optimization with gaussian processes. In The 22nd International Conference on Artiﬁcial Intelligence and Statistics, pages 3158–3167. Srinivas, N., Krause, A., Kakade, S., and Seeger, M. (2010). Gaussian process optimization in the bandit setting: no regret and experimental design. In Proceedings of the 27th International Conference on International Conference on Machine Learning, pages 1015–1022. Swersky, K., Snoek, J., and Adams, R. P . (2013). Multi-task b ayesian optimization. In Advances in neural information processing systems, pages 2004–2012. T akeno, S., Fukuoka, H., Tsukada, Y ., Koyama, T ., Shiga, M., T akeuchi, I., and Karasuyama, M. (2019). Multi-ﬁdelity bayesian optimization with max-val ue entropy search. arXiv preprint arXiv:1901.08275. W ainwright, M. J., Jordan, M. I., et al. (2008). Graphical mo dels, exponential families, and varia- tional inference. Foundations and Trends® in Machine Learning, 1(1–2):1–305. W ang, Z. and Jegelka, S. (2017). Max-value entropy search fo r efﬁcient bayesian optimization. In Proceedings of the 34th International Conference on Machine Learning-V olume 70 , pages 3627– 3635. JMLR. org. 11Wu, J. and Frazier, P . I. (2017). Continuous-ﬁdelity bayesi an optimization with knowledge gradient. In NIPS W orkshop on Bayesian Optimization. Zhang, Y ., Hoang, T . N., Low , B. K. H., and Kankanhalli, M. (20 17). Information-based multi- ﬁdelity bayesian optimization. In NIPS W orkshop on Bayesian Optimization. Zienkiewicz, O. C., T aylor, R. L., Zienkiewicz, O. C., and T a ylor, R. L. (1977). The ﬁnite element method, volume 36. McGraw-hill London. 12Supplementary Material . . .x x x f1(x) f2(x) fM (x) Figure 3: Graphical representation of the DNN based multi-ﬁdelity su rrogate model. The output in each ﬁdelity fm(x) (1 ≤ m ≤ M) is fulﬁlled by a (deep) neural network. 1 Deﬁnitions of Synthetic Benchmark Functions In the experiments, we used three synthetic benchmark tasksto evaluate our method. The deﬁnitions of the objective functions are given as follows. 1.1 Branin Function The input is two dimensional, x = [ x1, x2] ∈ [−5, 10] × [0, 15]. W e have three ﬁdelities to query the function, which, from high to low , are given by f3(x) = − ( −1.275x2 1 π2 + 5x1 π + x2 − 6 ) 2 − ( 10 − 5 4π ) cos(x1) − 10, f2(x) = −10 √ −f3(x − 2) − 2(x1 − 0.5) + 3(3 x2 − 1) + 1 , f1(x) = −f2 ( 1.2(x + 2) ) + 3x2 − 1. (13) W e can see that between ﬁdelities are nonlinear transformat ions and non-uniform scaling and shifts. The global maximum is -0.3979 at (−π, 12.275), (π, 2.275) and (9.425, 2.475). 1.2 Park1 Function The input is four dimensional, x = [ x1, x2, x3, x4] ∈ [0, 1]4. W e have two ﬁdelities, f2(x) = x1 2 [ √ 1 + ( x2 + x2 3)x4 x2 1 − 1 ] + (x1 + 3x4) exp[1 + sin( x3)], f1(x) = [ 1 + sin(x1) 10 ] f2(x) − 2x1 + x2 2+ x2 3+ 0.5. (14) The global maximum is at 25.5893 at (1.0, 1.0, 1.0, 1.0). 1.3 Levy Function The input is two dimensional, x = [ x1, x2] ∈ [−10, 10]2. The query has three ﬁdelities, f3(x) = − sin2(3πx1) − (x1 − 1)2[1 + sin 2(3πx2)] − (x2 − 1)2[1 + sin 2(2πx2)], f2(x) = − exp(0.1 · √ −f3(x)) − 0.1 · √ 1 + f2 3 (x), f1(x) = − √ 1 + f2 3 (x). (15) The global maximum is 0.0 at (1.0, 1.0). 2 Details of Real-W orld Applications 2.1 Mechanical Plate Vibration Design In this application, we want to make a 3-D simply supported, square, elastic plate, of size 10×10×1, as shown in Fig. 4. The goal is to ﬁnd materials that can maximi ze the fourth vibration mode 13frequency (so as to avoid resonance with other parts which ca uses damages). The materials are parameterized by three properties, Y oung’s modulus (in [1 × 1011, 5 × 1011]), Poisson’s ratio (in [0.2, 0.6]) and mass density (in [6 × 103, 9 × 103]). T o compute the frequency, we discretize the plate with quadr atic tetrahedral elements (see Fig. 4). W e consider two ﬁdelities. The low-ﬁdelity solution is o btained from setting a maximum mesh edge length to 1.2, while the high-ﬁdelity 0.6. W e then use the ﬁnite ﬁnite element method (Zienkiewicz et al., 1977) to solve for the ﬁrst 4th vibratio n mode and compute the frequency as our objective. Figure 4: The plate discretized with quadratic tetrahedral elements (the maximum mesh edge length is 1. 2). 2.2 Thermal Conductor Design In the second application, we consider the design of a thermal conductor, shown in Fig. 5a. The heat source is on the left, where the temperature is zero at th e beginning and ramps to 100 degrees in 0.5 seconds. The heat runs through the conductor to the right end . The size and properties of the conductor are ﬁxed: the thermal conductivity and mass densi ty are both 1. W e need to bore a hole in the centre to install the conductor. The edges on the top, bot tom and inside the hole are all insulated, i.e., no heat is transferred across these edges. Note that the size and the angle of the hole determine the speed of the heat transfusion. The hole in general is an el lipse, described by three parameters, x-radius, y-radius and angle. The goal is to make the heat con duction (from left to right) as fast as possible. Hence, we use the time to reach 70 degrees on the r ight end as the objective function value. T o compute the time, we discretize the conductor with quadratic tetrahedral elements, and apply the ﬁnite element methods to solve a transient heat tra nsfer problem (Incropera et al., 2007) to obtain a response heat curve on the right edge. An example is g iven in Fig. 5b. The response curve is a function of time, from which we can calculate when the tem perature reaches 70 degrees. W e consider queries of two ﬁdelities. The low ﬁdelity queries a re computed with the maximum mesh edge length being 0.8 in solving the heat transfer problem; t he high ﬁdelity queries are computed with the maximum mesh edge length being 0.2. 3 Details of Stochastic V ariational Learning W e develop a stochastic variational learning algorithm to j ointly estimate the posterior of W = {wm} — the NN weights in the output layer in each ﬁdelity, and the hy perparameters, including all the other NN weights Θ = {θm} and noise variance s = [ σ2 1 , . . . , σ 2 M ]⊤. T o this end, we assume q(W) = ∏ M m=1 q(wm) where each q(wm) = N (wm|µm, Σ m). W e parameterize Σ m with its Cholesky decomposition to ensure the positive deﬁniteness , Σ m = LmL⊤ mwhere Lm is a lower triangular matrix. W e then construct a variational model ev idence lower bound (ELBO) from the 14-0.5 0 0.5 -0.8 -0.6 -0.4 -0.2 0 0.2 0.4 0.6 88 90 92 94 96 98 100 (a) Conductor 0 1 2 3 4 5 Time (seconds) -20 0 20 40 60 80 100Temperature (degrees-Celsius) (b) Heat Response Curve Figure 5: The thermal conductor with one transient heat solution (a), and the heat responsive curve on the right edge (b). The white triangles in (a) are the ﬁnite eleme nts used to discretize the conductor to compute the solution. joint probability of our model (see (4) of the main paper), L ( q(W), Θ , s ) = Eq [ log(p(W, Y|X , Θ , s) q(W) ] = − M∑ m=1 KL ( q(wm)∥p(wm) ) + M∑ m=1 Nm∑ n=1 Eq [ log ( N (ynm|fm(xnm), σ2 m) )] , (16) where p(wm) = N (wm|0, I) and KL (·∥·) is the Kullback Leibler divergence. W e maximize L to estimate q(W), Θ and s jointly. However, since the NN outputs fm(·) in each ﬁdelity are coupled in a highly nonlinear way (see (3) of the main paper), the expect ation terms in L is analytical intractable. T o address this issue, we apply stochastic optimization. Sp eciﬁcally, we use the reparameterization trick (Kingma and W elling, 2013) and for each wm generate parameterized samples from their variational posterior, ˆwm = µm + Lmǫ where ǫ ∼ N (·|0, I). W e then substitute each sample ˆwm for wm in computing all log ( N (ynm|fm(xnm), σ2 m) ) in (16) and remove the expectation in front of them. W e therefore obtain ˆL, an unbiased estimate of ELBO, which is analytically tracta ble. Next, we compute ∇ ˆL, which is an unbiased estimate of the ∇L and hence can be used to maximize L. W e can use any stochastic optimization algorithm. 4 Proof of Lemma 4.1 Lemma 4.1.As long as the conditional posterior variance γ(fm−1, x) > 0, the posterior variance ηm(x), computed based on the quadrature in (7) of the main paper , ispositive. Proof. First, for brevity, we denote u(tk, x) and γ(tk, x) in (7) of the main paper by uk and γk, respectively. Then from the quadrature results, we compute the variance V ar(fm|D) = ∑ k gkγk + ∑ k gku2 k− ( ∑ k gkuk)2. 15Since γk > 0, the ﬁrst summation ∑ k gkγk > 0. Note that the quadrature weights have all gk > 0 and ∑ k gk = 1 . W e deﬁne ¯u = ∑ k gkuk. Next, we derive that ∑ k gku2 k− ( ∑ k gkuk)2 = ∑ k gku2 k− ¯u2 = ∑ k gku2 k+ ¯u2 − 2¯u2 = ∑ k gku2 k+ ∑ k gk ¯u2 − 2¯u2 = ∑ k gku2 k+ ∑ k gk ¯u2 − 2 ∑ k gkuk ¯u = ∑ k gk(u2 k+ ¯u2 − 2uk ¯u) = ∑ k gk(uk − ¯u)2 ≥ 0. (17) Therefore, V ar(fm|D) > 0. 5 Proof of Nonnegative V ariance in (12) of the Main Paper W e show the variance in (12) of the main paper, computed by qua drature, is non-negative. The proof is very similar to that of Lemma 4.1 (Section 4). W e denote the quadrature weights and nodes by {gk} and {tk}. Then we have Z = ∑ k gkR(tk), Z 1 = ∑ k gktkR(tk), Z 2 = ∑ k gkt2 kR(tk). (18) Therefore, Z1 Z = ∑ k tk gkR(tk)∑ j gjR(tj ) = ∑ k tkνk, Z2 Z = ∑ k t2 k gkR(tk) ∑ j gjR(tj ) = ∑ k t2 kνk (19) where νk = gk R(tk) ∑ j gj R(tj ) > 0 and ∑ k νk = 1 . Following the same derivation as in (17), we can immediately show that the variance Z2/Z − Z2 1 /Z2 = ∑ k νk(tk − ¯t)2 ≥ 0 where ¯t = Z1/Z = ∑ k tkνk. 16",
      "meta_data": {
        "arxiv_id": "2007.03117v4",
        "authors": [
          "Shibo Li",
          "Wei Xing",
          "Mike Kirby",
          "Shandian Zhe"
        ],
        "published_date": "2020-07-06T23:28:40Z",
        "pdf_url": "https://arxiv.org/pdf/2007.03117v4.pdf",
        "github_url": "https://github.com/kirthevasank/mf-gp-ucb"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper proposes Deep Neural Network Multi-Fidelity Bayesian Optimization (DNN-MFBO) to address the issue of existing multi-fidelity BO methods either ignoring or over-simplifying the strong, complex correlations across fidelities, which leads to inefficient objective function estimation. DNN-MFBO leverages deep neural networks to flexibly capture all kinds of complicated, potentially nonlinear and nonstationary, relationships between fidelities, thereby improving objective function estimation and overall optimization performance. A tractable and efficient mutual information-based acquisition function is computed using sequential, fidelity-wise Gauss-Hermite quadrature and moment-matching.",
        "methodology": "DNN-MFBO uses a stacked neural network architecture where each NN models one fidelity. For fidelity m > 1, the NN input is constructed by appending the output from the previous fidelity (f_m-1(x)) to the original input (x), allowing information propagation and capturing complex inter-fidelity relationships. The model defines f_m(x) = w_m^T * phi_theta_m(x_m) + epsilon_m, where w_m are output layer weights (random variables) and theta_m are other weights (hyper-parameters). Stochastic variational learning is developed to jointly estimate the posterior of random weights (q(w_m) = N(w_m|mu_m, Sigma_m)) and hyper-parameters by maximizing an ELBO using the reparameterization trick. The acquisition function is defined as a(x, m) = (1/lambda_m) * I(f*, f_m(x)|D), maximizing mutual information between the objective's maximum (f*) and the queried fidelity's output. To compute this, output posteriors p(f_m(x)|D) are approximated as Gaussian distributions using fidelity-wise moment matching and Gauss-Hermite quadrature, especially for non-linear coupling. A Monte-Carlo approximation and a truncated Gaussian approximation are used for the entropy terms in the acquisition function, with further moment matching to obtain Gaussian forms for analytical entropy calculation.",
        "experimental_setup": "DNN-MFBO was evaluated on three synthetic benchmark datasets: Branin function (3 fidelities, 2D input), Park1 function (2 fidelities, 4D input), and Levy function (3 fidelities, 2D input). It was also tested on two real-world engineering design applications: Mechanical Plate Vibration Design (optimizing material properties for maximum vibration frequency, 2 fidelities: coarse vs. dense mesh) and Thermal Conductor Design (optimizing central hole shape for fast heat conduction, 2 fidelities: coarse vs. dense mesh). Competing methods included Multi-Fidelity Sequential Kriging (MF-SKO), MF-GP-UCB, Multi-Fidelity Predictive Entropy Search (MF-PES), Multi-Fidelity Maximum Entropy Search (MF-MES), Multi-Task NN-based BO (MTNN-BO), and single-fidelity MES (SF-MES). Evaluation metrics included simple regret (SR) and inference regret (IR) for synthetic tasks, and queried maximum/minimum function values along with query cost for real-world tasks. The average query time was also measured. Hyper-parameters for NNs (depth, width, learning rate) were tuned using SMAC3 and manual adjustments. ADAM optimizer was used for training, and L-BFGS for acquisition function optimization (with random initialization for DNN-MFBO). Initial training points were randomly queried across fidelities.",
        "limitations": "The existing methods either ignore the strong, complex correlations between fidelities (e.g., MF-GP-UCB, independent GP for each fidelity) or model them with over-simplified structures (e.g., MF-PES requiring simple/smooth kernels for tractability, MF-MES assuming linear correlation), leading to inefficient and inaccurate objective function estimation. While DNN-MFBO addresses these, its computational complexity for the acquisition function, despite optimizations, is noted as 'quite complex'. The current work focuses on discrete fidelities, not continuous ones.",
        "future_research_directions": "Not mentioned"
      }
    },
    {
      "title": "Efficient Hyperparameter Optimization with Adaptive Fidelity Identification"
    },
    {
      "title": "Batch Multi-Fidelity Bayesian Optimization with  Deep Auto-Regressive Networks",
      "abstract": "Bayesian optimization (BO) is a powerful approach for optimizing black-box,\nexpensive-to-evaluate functions. To enable a flexible trade-off between the\ncost and accuracy, many applications allow the function to be evaluated at\ndifferent fidelities. In order to reduce the optimization cost while maximizing\nthe benefit-cost ratio, in this paper, we propose Batch Multi-fidelity Bayesian\nOptimization with Deep Auto-Regressive Networks (BMBO-DARN). We use a set of\nBayesian neural networks to construct a fully auto-regressive model, which is\nexpressive enough to capture strong yet complex relationships across all the\nfidelities, so as to improve the surrogate learning and optimization\nperformance. Furthermore, to enhance the quality and diversity of queries, we\ndevelop a simple yet efficient batch querying method, without any combinatorial\nsearch over the fidelities. We propose a batch acquisition function based on\nMax-value Entropy Search (MES) principle, which penalizes highly correlated\nqueries and encourages diversity. We use posterior samples and moment matching\nto fulfill efficient computation of the acquisition function and conduct\nalternating optimization over every fidelity-input pair, which guarantees an\nimprovement at each step. We demonstrate the advantage of our approach on four\nreal-world hyperparameter optimization applications.",
      "full_text": "Batch Multi-Fidelity Bayesian Optimization with Deep Auto-Regressive Networks Shibo Li, Robert M. Kirby, and Shandian Zhe School of Computing, University of Utah Salt Lake City, UT 84112 shibo@cs.utah.edu, kirby@cs.utah.edu, zhe@cs.utah.edu Abstract Bayesian optimization (BO) is a powerful approach for optimizing black-box, expensive-to-evaluate functions. To enable a ﬂexible trade-off between the cost and accuracy, many applications allow the function to be evaluated at different ﬁdelities. In order to reduce the optimization cost while maximizing the beneﬁt- cost ratio, in this paper we propose Batch Multi-ﬁdelity Bayesian Optimization with Deep Auto-Regressive Networks (BMBO-DARN). We use a set of Bayesian neural networks to construct a fully auto-regressive model, which is expressive enough to capture strong yet complex relationships across all the ﬁdelities, so as to improve the surrogate learning and optimization performance. Furthermore, to enhance the quality and diversity of queries, we develop a simple yet efﬁcient batch querying method, without any combinatorial search over the ﬁdelities. We propose a batch acquisition function based on Max-value Entropy Search (MES) principle, which penalizes highly correlated queries and encourages diversity. We use posterior samples and moment matching to fulﬁll efﬁcient computation of the acquisition function, and conduct alternating optimization over every ﬁdelity-input pair, which guarantees an improvement at each step. We demonstrate the advantage of our approach on four real-world hyperparameter optimization applications. 1 Introduction Many applications demand we optimize a complex function of an unknown form that is expensive to evaluate. Bayesian optimization (Mockus, 2012; Snoek et al., 2012) is a powerful approach to optimize such functions. The key idea is to use a probabilistic surrogate model, typically Gaussian processes (Rasmussen and Williams, 2006), to iteratively approximate the target function, integrate the posterior information to compute and maximize an acquisition function so as to generate new inputs at which to query, update the model with new examples, and meanwhile approach the optimum. In practice, to enable a ﬂexible trade-off between the computational cost and accuracy, many appli- cations allow us to evaluate the target function at different ﬁdelities. For example, to evaluate the performance of the hyperparameters for a machine learning model, we can train the model thoroughly, i.e., with sufﬁcient iterations/epochs, to obtain the accurate evaluation (high-ﬁdelity yet often costly) or just run a few iterations/epochs to obtain a rough estimate (low-ﬁdelity but much cheaper). Many multi-ﬁdelity BO algorithms (Lam et al., 2015; Kandasamy et al., 2016; Zhang et al., 2017; Song et al., 2019; Takeno et al., 2019) have therefore been proposed to identify both the ﬁdelities and inputs at which to query, so as to reduce the cost and achieve a good beneﬁt-cost balance. Notwithstanding their success, these methods often overlook the strong yet complex relationships between different ﬁdelities or adopt an over-simpliﬁed assumption, (partly) for the sake of convenience in calculating/maximizing the acquisition function considering ﬁdelities. This, however, can restrict the performance of the surrogate model, impair the optimization efﬁciency and increase the cost. For 35th Conference on Neural Information Processing Systems (NeurIPS 2021). arXiv:2106.09884v2  [cs.LG]  25 Oct 2021example, Lam et al. (2015); Kandasamy et al. (2016) learned an independent GP for each ﬁdelity, Zhang et al. (2017) used multitask GPs with a convolved kernel for multi-ﬁdelity modeling and have to use a simple smoothing kernel (e.g., Gaussian) for tractable convolutions. The recent work (Takeno et al., 2019) imposes a linear correlation across different ﬁdelities. In addition, the standard one-by- one querying strategy needs to sequentially run each query and cannot utilize parallel computing resources to accelerate, e.g., multi-core CPUs/GPUs and clusters. While incrementally absorbing more information, it does not explicitly account for the correlation between different queries, hence still has a risk to bring in highly correlated examples that includes redundant information. To address these issues, we propose BMBO-DARN, a novel batch multi-ﬁdelity Bayesian optimization method. First, we develop a deep auto-regressive model to integrate training examples at various ﬁdelities. Each ﬁdelity is modeled by a Bayesian neural network (NN), where the output predicts the objective function value at that ﬁdelity and the input consists of the original inputs and the outputs of all the previous ﬁdelities. In this way, our model is adequate to capture the complex, strong correlations ( e.g., nonstationary, highly nonlinear) across all the ﬁdelities to enhance the surrogate learning. We use Hamiltonian Monte-Carlo (HMC) sampling for posterior inference. Next, to improve the quality of the queries, we develop a simple yet efﬁcient method to jointly fetch a batch of inputs and ﬁdelities. Speciﬁcally, we propose a batch acquisition function based on the state-of-the-art Max-value Entropy Search (MES) principle (Wang and Jegelka, 2017). The batch acquisition function explicitly penalizes highly correlated queries and encourages diversity. To efﬁciently compute the acquisition function, we use the posterior samples of the NN weights and moment matching to construct a multi-variate Gaussian posterior for all the ﬁdelity outputs and the function optimum. To prevent a combinatorial search over multiple ﬁdelities in maximizing the acquisition function, we develop an alternating optimization algorithm to cyclically update each pair of input and ﬁdelity, which is much more efﬁcient and guarantees an improvement at each step. For evaluation, we examined BMBO-DARN in both synthetic benchmarks and real-world applications. The synthetic benchmark tasks show that given a small number of training examples, our deep auto- regressive model can learn a more accurate surrogate of the target function than other state-of-the-art multi-ﬁdelity BO models. We then evaluated BMBO-DARN on four popular machine learning models (CNN, online LDA, XGBoost and Physics informed NNs) for hyperparameter optimization. BMBO-DARN can ﬁnd more effective hyperparameters leading to superior predictive performance, and meanwhile spends smaller total evaluation costs, as compared with state-of-the-art multi-ﬁdelity BO algorithms and other popular hyperparameter tuning methods. 2 Background Bayesian Optimization (BO) (Mockus et al., 1978; Snoek et al., 2012) is a popular approach for optimizing black-box functions that are often costly to evaluate and cannot provide exact gradient information. BO learns a probabilistic surrogate model to predict the function value across the input space and quantiﬁes the predictive uncertainty. At each step, we use this information to compute an acquisition function to measure the utility of querying at different inputs. By maximizing the acquisition function, we ﬁnd the next input at which to query, which is supposed to be closer to the optimum. Then we add the new example into the training set to improve the accuracy of the surrogate model. The procedure is repeated until we ﬁnd the optimal input or the maximum number of queries have been ﬁnished. There are a variety of acquisition functions, such as Expected Improvement (EI) (Mockus et al., 1978) and Upper Conﬁdence Bound (UCB) (Srinivas et al., 2010). The recent state-of-the-art addition is Maximum-value Entropy Search (MES) (Wang and Jegelka, 2017), a(x) = I ( f(x),f∗|D ) , (1) where I(·,·) is the mutual information, f(x) is the objective function value at x, f∗the minimum, and Dthe training data collected so far for the surrogate model. Note that both f(x) and f∗are considered as generated by the posterior of the surrogate model given D; they are random variables. The most commonly used class of surrogate models is Gaussian process (GP) (Rasmussen and Williams, 2006). Given the training dataset X = [ x⊤ 1 ,..., x⊤ N]⊤and y = [ y1,...,y N]⊤, a GP assumes the outputs y follow a multivariate Gaussian distribution, p(y|X) = N(y|m,K + vI), where m is the mean function values at the inputs X, often set to 0, vis the noise variance, and K is a kernel matrix on X. Each [K]ij = κ(xi,xj), where κ(·,·) is a kernel function. For example, a popular one is the RBF kernel,κ(xi,xj) = exp ( −β−1∥xi −xj∥2) . An important advantage of GPs 2f1(θ) f2(θ) . . . f3(θ) fM (θ)θ Figure 1: Graphical representation of the deep auto-regressive model in BMBO-DARN. The output at each ﬁdelity fm(x) (1 ≤ m ≤ M) is calculated by a (deep) neural network. is their convenience in uncertainty quantiﬁcation. Since GPs assume any ﬁnite set of function values follow a multi-variate Gaussian distribution, given a test input ˆx, we can compute the predictive (or posterior) distribution p(f(ˆx)|ˆx,X,y) via a conditional Gaussian distribution, which is simple and analytical. Multi-Fidelity BO. Since evaluating the exact value of the object function is often expensive, many practical applications provide multi-ﬁdelity evaluations {f1(x),...,f M(x)}to allow us to choose a trade-off between the accuracy and cost. Accordingly, many multi-ﬁdelity BO algorithms have been developed to select both the inputs and ﬁdelities to reduce the cost and to achieve a good balance between the optimization progress and cost, i.e., the beneﬁt-cost ratio. For instance, MF- GP-UCB (Kandasamy et al., 2016) sequentially queries at each ﬁdelity (from the lowest one, i.e., m= 1) until the conﬁdence band is over a given threshold. In spite of its great success and guarantees in theory, MF-GP-UCB uses a set of independent GPs to estimate the objective at each ﬁdelity, and hence ignores the valuable correlations between different ﬁdelities. MF-PES (Zhang et al., 2017) uses a multi-task GP surrogate where each task corresponds to one ﬁdelity, and convolves a smoothing kernel with the kernel of a shared latent function to obtain the cross-covariance. The recent MF-MES (Takeno et al., 2019) also builds a multi-task GP surrogate, where the covariance function is κ(fm(x),fm′ (x′)) = ∑d j=1 (umjum′j + 1(m= m′) ·αmj) ρj(x1,x2), (2) where αmj >0, 1(·) is the indicator function, {umj}d j=1 is dlatent features for each ﬁdelity m, and {ρj(·,·)}are dbases kernels, usually chosen as a commonly used stationary kernel, e.g., RBF. 3 Deep Auto-Regressive Model for Multi-Fidelity Surrogate Learning Notwithstanding the elegance and success of the existing multi-ﬁdelity BO methods, they often ignore or oversimplify the complex, strong correlations between different ﬁdelities, and hence can be inefﬁcient for surrogate learning, which might further lower the optimization efﬁciency and incur more expenses. For example, the state-of-the-art methods MF-GP-UCB (Kandasamy et al., 2016) estimate a GP surrogate for each ﬁdelity independently; MF-PES (Zhang et al., 2017) has to adopt a simple form for both the smoothing and latent function kernel (e.g., Gaussian and delta) to achieve an analytically tractable convolution, which might limit the expressivity in estimating the cross-ﬁdelity covariance; MF-MES (Takeno et al., 2019) essentially imposes a linear correlation structure between different ﬁdelities — for any input x, κ(fm(x),fm′ (x)) = u⊤ m1 um2 + αm where um = [um1,...,u md] and ˜αm = ∑d j=1 αmj if we use a RBF basis kernel (see (2)). To overcome this limitation, we develop a deep auto-regressive model for multi-ﬁdelity surrogate learning. Our model is expressive enough to capture the strong, possibly very complex (e.g., highly nonlinear, nonstationary) relationships between all the ﬁdelities to improve the prediction (at the highest ﬁdelity). As such, our model can more effectively integrate multi-ﬁdelity training information to better estimate the objective function. Speciﬁcally, given M ﬁdelities, we introduce a chain of M neural networks, each of which models one ﬁdelity and predicts the target function at that ﬁdelity. Denote by xm, Wm, and ψWm(·) the NN input, parameters and output mapping at each ﬁdelity m. Our model is deﬁned as follows, xm = [x; f1(x); ... ; fm−1(x)], fm(x) = ψWm(xm), y m(x) = fm(x) + ϵm, (3) where x1 = x, fm(x) is the prediction (i.e., NN output) at the m-th ﬁdelity, ym(x) is the observed function value, and ϵm is a random noise, ϵm ∼N (ϵm|0,τ−1 m ). We can see that each input xm 3consists of not only the original input x of the objective function, but also the outputs from all the previous ﬁdelities. Via a series of linear projection and nonlinear activation from the NN, we obtain the output at ﬁdelity m. In this way, our model fully exploits the information from the lower ﬁdelities and can ﬂexibly capture arbitrarily complex relationships between the current and all the previous ﬁdelities by learning an NN mapping, fm(x) = ψWm ( xm,f1(x),...,f m−1(x) ) . We assign a standard Gaussian prior distribution over each element of the NN parameters W= {W1,..., WM}, and a Gamma prior over each noise precision, p(τm) = Gam(τm|a0,b0). Given the dataset D= {{(xnm,ynm)}Nm n=1}M m=1, the joint probability of our model is given by p(W,τ, Y,S|X) = N(vec(W)|0,I) M∏ m=1 Gam(τm|a0,b0) M∏ m=1 Nm∏ n=1 N ( ynm|fm(xnm),τ−1 m ) , (4) where τ = [ τ1,...,τ M], X = {xnm}, Y = {ynm}, and vec(·) is vectorization. The graphical representation of our model is given in Fig. 1. We use Hamiltonian Monte Carlo (HMC) (Neal et al., 2011) sampling to perform posterior inference due to its unbiased, high-quality uncertainty quantiﬁcation, which is critical to calculate the acquisition function. However, our method allows us to readily switch to other approximate inference approaches as needed (see Sec. 4), e.g., stochastic gradient HMC (Chen et al., 2014) used in the excellent work of Springenberg et al. (2016). 4 Batch Acquisition for Multi-Fidelity Optimization Given the posterior of our model, we aim to compute and optimize an acquisition function to identify the input and ﬁdelity at which to query next. Popular BO methods query at one input each time and then update the surrogate model. While successful, this one-by-one strategy has to run each query sequentially and cannot take advantage of parallel computing resources (that are often available in practice) to further accelerate, such as multi-core CPU and GPU workstations and computer clusters. In addition, the one-by-one strategy although gradually integrates more data information, it lacks an explicit mechanism to take into account the correlation across different queries, hence still has a risk to bring in highly correlated examples with redundant information, especially in the multi-ﬁdelity setting, e.g., querying at the same input with another ﬁdelity. To allow parallel query and to improve the query quality and diversity, we develop a batch acquiring approach to jointly identify a set of inputs and ﬁdelities at a time, presented as follows. 4.1 Batch Acquisition Function We ﬁrst propose a batch acquisition function based on the MES principle (Zhang et al., 2017) (see (1)). Denote by Bthe batch size and by {λ1,...,λ M}the cost of querying at M ﬁdelities. We want to jointly identify Bpairs of inputs and ﬁdelities (x1,m1),..., (xB,mB) at which to query. The batch acquisition function is given by abatch(X,m) = I({fm1 (x1),...,f mB (xB)},f∗|D)∑B k=1 λmk , (5) where X = {x1,..., xB}and m = [m1,...,m B]. As we can see, our batch acquisition function explicitly penalizes highly correlated queries, encouraging joint effectiveness and diversity — if between the outputs {fmk(xk)}B k=1 are high correlations, the mutual information in the numerator will decrease. Furthermore, by dividing the total querying cost in (5), the batch acquisition function expresses a balance between the beneﬁt of these queries (in probing the optimum) and the price, i.e., beneﬁt-cost ratio. When we set B = 1, our batch acquisition function is reduced to the single one used in (Takeno et al., 2019). 4.2 Efﬁcient Computation Given X and m, the computation of (5) is challenging, because it involves the mutual information between a set of NN outputs and the function optimum. To address this challenge, we use posterior samples and moment matching to approximate p(f,f∗|D) as a multi-variate Gaussian distribution, where f = [fm1 (x1),...,f mB (xB)]. Speciﬁcally, we ﬁrst draw a posterior sample of the NN weights Wfrom our model. We then calculate the output at each input and ﬁdelity to obtain a sample of f, and maximize (or minimize) fM(·) to obtain a sample of f∗. We use L-BFGS (Liu and Nocedal, 41989) for optimization. After we collect Lindependent samples {(ˆf1, ˆf∗ 1 ),..., (ˆfL, ˆf∗ L)}, we can estimate the ﬁrst and second moments of h = [f; f∗], namely, mean and covariance matrix, µ = 1 L L∑ j=1 ˆhj, Σ = 1 L−1 L∑ j=1 (ˆhj −µ)(ˆhj −µ)⊤, where each ˆhj = [ˆfj; ˆf∗ j]. We then use these moments to match a multivariate Gaussian posterior, p(h|D) ≈N(h|µ,Σ). Then the mutual information can be computed with a closed form, I(f,f∗|D) = H(f|D) + H(f∗|D) −H(f,f∗|D) ≈1 2 log |Σﬀ |+ 1 2 log σ∗∗−1 2 log |Σ|, (6) where Σﬀ = Σ[1 : B,1 : B], i.e., the ﬁrst B×B sub-matrix along the diagonal, which is the posterior covariance of f, and σ∗∗= Σ[B+ 1,B + 1], i.e., the posterior variance of f∗. The batch acquisition function is therefore calculated from abatch(X,m) ≈ 1 2 ∑B k=1 λmk (log |Σﬀ |+ logσ∗∗−log |Σ|) . (7) Note that Σ is a function of the inputs X and ﬁdelities m and hence so are its submatrix and elements, Σﬀ and σ∗∗. To obtain a reliable estimate of the moments, we set L= 100 in our experiments. Note that our method can be applied along with any posterior inference algorithm, such as variational inference and SGHMC (Chen et al., 2014), as long as we can generate posterior samples of the NN weights, not restricted to the HMC adopted in our paper. 4.3 Optimizing a Batch of Fidelities and Inputs Now, we consider maximizing (7) to identify Binputs X and their ﬁdelities m at which to query. However, since the optimization involves a mix of continuous inputs and discrete ﬁdelities, it is quite challenging. A straightforward approach would be to enumerate all possible conﬁgurations of m, for each particular conﬁguration, run a gradient based optimization algorithm to ﬁnd the optimal inputs, and then pick the conﬁguration and its optimal inputs that give the largest value of the acquisition function. However, doing so is essentially conducting a combinatorial search over Bﬁdelities, and the search space grows exponentially with B, i.e., O(MB) = O(eBlog M). Hence, it will be very costly, even infeasible for a moderate choice of B. To address this issue, we develop an alternating optimization algorithm. Speciﬁcally, we ﬁrst initialize all the Bqueries, Q= {(x1,m1),..., (xB,mB)}, say, randomly. Then each time, we only optimize one pair of the input and ﬁdelity (xk,mk)(1 ≤k≤B), while ﬁxing the others. We cyclically update each pair, where each update is much cheaper but guarantees to increase abatch. Speciﬁcally, each time, we maximize abatch,k(x,m) = I(F¬k ∪{fm(x)},f∗|D) λm + ∑ j̸=kλmj , (8) where F¬k = {fmj (xj)|j ̸= k}. Note that the computation of(8) still follows(7). We set(xk,mk) to the optimum (x∗,m∗), and then proceed to optimize the next input location and ﬁdelity(xk+1,mk+1) in Qwith the others ﬁxed. We continues this until we ﬁnish updating all the queries in Q, which corresponds to one iteration. We can keep running iterations until the increase of the batch acquisition function is less than a tolerance level or a maximum number of iterations has been done. Suppose we ran Giterations, the time complexity is O(GMB), which is linear in the number of ﬁdelities and batch size, and hence is much more efﬁcient than the naive combinatorial search. Our multi-ﬁdelity BO approach is summarized in Algorithm 1. 5 Related Work Most Bayesian optimization (BO) (Mockus, 2012; Snoek et al., 2012) methods are based on Gaussian processes (GPs) and a variety of acquisition functions, such as (Mockus et al., 1978; Auer, 2002; Srinivas et al., 2010; Hennig and Schuler, 2012; Hernández-Lobato et al., 2014; Wang and Jegelka, 2017; Kandasamy et al., 2017b; Garrido-Merchán and Hernández-Lobato, 2020). Snoek et al. (2015) showed Bayesian neural networks (NNs) can also be used as a general surrogate model, and has 5Algorithm 1 BMBO-DARN (D, B, M, T, {λm}M m=1 ) Learn the deep auto-regressive model (4) on Dwith HMC. for t= 1,...,T do Collect a batch of Bqueries, Q= {(xk,mk)}B k=1, with Algorithm 2. Query the objective function value at each input xk and ﬁdelity mk in Q D←D∪{ (xk,yk,mk)|1 ≤k≤B}. Re-train the deep auto-regressive model on Dwith HMC. end for Algorithm 2 BatchAcquisition({λm}, B, L, G, ξ) Initialize Q= {(x1,m1),..., (xB,mB)}randomly. Collect Lindependent posterior samples of the NN weights. repeat for k= 1,...,B do Use the posterior samples to calculate and optimize (8), (x∗,m∗) = argmax x∈Ω,1≤m≤M abatch,k(x,m), where Ω is the input domain. (xk,mk) ←(x∗,m∗). end for until Giterations are done or the increase of abatch in (7) is less than ξ Return Q. excellent performance. Moreover, the training of NNs is scalable, not suffering from O(N3) time complexity (N is the number of examples) of training exact GPs. Springenberg et al. (2016) further used scale adaption to develop a robust stochastic gradient HMC for the posterior inference in the NN based BO. Recent works that deal with discrete inputs (Baptista and Poloczek, 2018) or mixed discrete and continuous inputs (Daxberger et al., 2019) use an explicit nonlinear feature mapping and Bayesian linear regression, which can be viewed as one-layer Bayesian NNs. There have been many studies in multi-ﬁdelity (MF) BO, e.g., (Huang et al., 2006; Swersky et al., 2013; Lam et al., 2015; Picheny et al., 2013; Kandasamy et al., 2016, 2017a; Poloczek et al., 2017; McLeod et al., 2017; Wu and Frazier, 2017). While successful, these methods either ignore or oversimplify the strong, complex correlations between different ﬁdelities, and hence might be inefﬁcient in surrogate learning. For example, Picheny et al. (2013); Lam et al. (2015); Kandasamy et al. (2016); Poloczek et al. (2017) learned an independent GP for each ﬁdelity; Song et al. (2019) used all the examples without discrimination to train one single GP; Huang et al. (2006); Takeno et al. (2019) imposed a linear correlation across ﬁdelities, while Zhang et al. (2017) constructed a convolutional kernel as the cross-ﬁdelity covariance and so the involved kernels in the convolution must be simple and smooth enough (yet less expressive) to obtain a closed form. Recently, Perrone et al. (2018) developed an NN-based multi-task BO method for hyper-parameter transfer learning. Their model constructs an NN feature mapping shared by all the tasks, and uses an independent linear combination of the mapped features to predict each task output. While we can consider each task as evaluating the objective at a particular ﬁdelity, the model does not explicitly capture and exploit the correlations across different tasks — given the shared (latent) features, the predictions of these tasks (ﬁdelities) are independent. The most recent work (Li et al., 2020) also developed an NN-based multi-ﬁdelity BO method, which differs from our work in that (1) their model only estimates the relationship between successive ﬁdelities, and hence has less capacity, (2) their work uses a recursive one-dimensional quadrature to calculate the acquisition function, and is difﬁcult to extend to batch acquisitions. In a high level, the chain structure of Li et al. (2020)’s model also resembles deep GP based multi-ﬁdelity models (Perdikaris et al., 2017; Cutajar et al., 2019). Quite a few batch BO algorithms have been developed, such as (González et al., 2016; Wu and Frazier, 2016; Hernández-Lobato et al., 2017; Kandasamy et al., 2017b). However, they work with single-ﬁdelity queries and are not easily extended to multi-ﬁdelity optimization tasks. Takeno et al. (2019) proposed two batch querying strategies for their MF-BO framework. Both strategies 6leverage the property that the covariance of a conditional Gaussian does not rely on the values of the conditioned variables; so, there is no need to worry about conditioning on function values that are still in query. The asynchronous version generates new queries conditioned on different sets of function values in query (asynchronously). However, if the conditional parts are signiﬁcantly overlapping, which might not be uncommon in practice, there is a risk of generating redundant or even collapsed queries. Takeno et al. (2019) also talked about a synchronous version. While they discussed how to compute the information gain between the function maximum and a batch of function values, they did not provide an effective way to optimize it with the multi-ﬁdelity querying costs. Instead, they suggested a simple heuristics to sequentially ﬁnd each query by conditioning on the generated ones. However, there is no guarantee about this heuristics. While in our experiments, we mainly use hyperparameter optimization to evaluate our multi-ﬁdelity BO approach, there are many other excellent works speciﬁcally designed for hyperparameter tuning or selection, e.g., the non-Bayesian, random search based method Hyberband (Li et al., 2017) which also reﬂects the multi-ﬁdelity idea: it starts using few training iterations/epochs (low ﬁdelity) to evaluate many candidates, rank them, iteratively selects the top-ranked ones, and further evaluate them with more iterations/epochs (high ﬁdelity). BOHB (Falkner et al., 2018) is a hybrid of KDE based BO (Bergstra et al., 2011) and Hyperband. Li et al. (2018) further developed an asynchronous successive halving algorithm for parallel random search over hyperparameters. Domhan et al. (2015); Klein et al. (2017b) propose to estimate the learning curves, and early halt the evaluation of ominous hyperparameters according to the learning curve predictions. Swersky et al. (2014) introduced a kernel about the training steps, and developed Freeze-thaw BO (Swersky et al., 2014) that can temporarily pause the model training and explore several promising hyperparameter settings for a while and then continue on to the most promising one. The work in (Klein et al., 2017a) jointly estimates the cost as a function of the data size and training steps, which can be viewed as continuous ﬁdelities, like in (Kandasamy et al., 2017a; Wu and Frazier, 2017). 6 Experiment 6.1 Surrogate Learning Performance We ﬁrst examined if BMBO-DARN can learn a more accurate surrogate of the objective. We used two popular benchmark functions: (1) Levy (Laguna and Martí, 2005) with two-ﬁdelity evaluations, and (2) Branin (Forrester et al., 2008; Perdikaris et al., 2017) with three-ﬁdelity evaluations. Throughout different ﬁdelities are nonlinear/nonstationary transforms. We provide the details in the Appendix. Levy nRMSE MNLL MF-GP-UCB 0.831 ± 0.195 1 .824 ± 0.276 MF-MES 0.581 ± 0.032 1 .401 ± 0.031 SHTL 0.443 ± 0.009 1 .208 ± 0.026 DNN-MFBO 0.365 ± 0.035 1 .081 ± 0.011 BMBO-DARN 0.348 ± 0.021 1 .072 ± 0.016 Branin MF-GP-UCB 0.846 ± 0.147 1 .976 ± 0.208 MF-MES 0.719 ± 0.099 1 .796 ± 0.128 SHTL 0.835 ± 0.218 1 .958 ± 0.646 DNN-MFBO 0.182 ± 0.022 0 .973 ± 0.013 BMBO-DARN 0.158 ± 0.016 0 .965 ± 0.005 Table 1: Surrogate learning performance on Branin function with three-ﬁdelity training examples and Levy function with two-ﬁdelity examples: normalized root- mean-square-error (nRMSE) and mean-negative-log- likelihood (MNLL). The results were averaged over ﬁve runs. Methods. We compared with the following multi-ﬁdelity learning models used in the state- of-the-art BO methods: (1) MF-GP-UCB (Kan- dasamy et al., 2016) that learns an independent GP for each ﬁdelity. (2) MF-MES (Takeno et al., 2019) that uses a multi-output GP with a linear correlation structure across different outputs (ﬁ- delities), (3) Scalable Hyperparameter Transfer Learning (SHTL) (Perrone et al., 2018) that uses an NN to generate latent bases shared by all the tasks (ﬁdelities) and predicts the output of each task with a linear combination of the bases. (4) Deep Neural Network Multi-Fidelity BO (DNN- MFBO) (Li et al., 2020) that uses a chain of NNs to model each ﬁdelity, but only estimates the relationship between successive ﬁdelities. Settings. We implemented our model with PyTorch (Paszke et al., 2019) and HMC sam- pling based on the Hamiltorch library (Cobb and Jalaian, 2021) (https://github.com/ AdamCobb/hamiltorch). For each ﬁdelity, we used two hidden layers with 40 neurons and tanh activation. We ran HMC for 5K steps to reach burn in (by looking at the trace plots) and then produced 200 posterior samples with every 10 steps. To generate each sample proposal, we ran 10 leapfrog steps, and the step size was chosen as 0.012. 7We implemented DNN-MFBO and SHTL with PyTorch as well. For DNN-MFBO, we used the same NN architecture as in BMBO-DARN for each ﬁdelity, and ran HMC with the same setting for model estimation. For SHTL, we used two hidden layers with 40 neurons and an output layer with 32 neurons to generate the shared bases. We used ADAM (Kingma and Ba, 2014) to estimate the model parameters, and the learning rate was chosen from {10−4,5 ×10−4,10−3,5 ×10−3,10−2}. We ran 1K epochs, which are enough for convergence. Note that we also attempted to use L-BFGS to train SHTL, but it often runs into numerical issues. ADAM is far more stable. We used a Python implementation of MF-MES and MF-GP-UCB, both of which use the RBF kernel (consistent with the original papers). Results. We randomly generated{130,65}examples for Levy function at the two increasing ﬁdelities, and {320,130,65}examples for Branin function at its three increasing ﬁdelities. After training, we examined the prediction accuracy of all the models with 100 test samples uniformly sampled from the input space. We calculated the normalized root-mean-square-error (nRMSE) and mean- negative-log-likelihood (MNLL). We repeated the experiment for 5 times, and report their average and standard deviations in Table. 1. As we can see, for both benchmark functions, BMBO-DARN outperforms all the competing models, conﬁrming the advantage of our deep auto-regressive model in surrogate learning. Note that despite using a similar chain structure, DNN-MFBO is still inferior to BMBO-DARN, implying that our fully auto-regressive modeling (see (3)) can better estimate the relationships between the ﬁdelities to facilitate surrogate estimation. 6.2 Real-World Applications Next, we used BMBO-DARN to optimize the hyperparameters of four popular machine learning models: Convolutional Neural Networks (CNN) (Fukushima and Miyake, 1982; LeCun et al., 1990) for image classiﬁcation, Online Latent Dirichlet Allocation (LDA) (Hoffman et al., 2010) for text mining, XGBoost (Chen and Guestrin, 2016) for diabetes diagnosis, and Physics-Informed Neural Networks (PINN) (Raissi et al., 2019) for solving partial differential equations (PDE). Methods and Setting. We compared with the state-of-the-art multi-ﬁdelity BO algorithms men- tioned in Sec. 6.1, (1) MF-GP-UCB, (2) MF-MES, (3) SHTL, and (4) DNN-MFBO. In ad- dition, we compared with (5) MF-MES-Batch (Takeno et al., 2019), the (asynchronous) paral- lel version of MF-MES, (6) SF-Batch (Kandasamy et al., 2017b) ( https://github.com/ kirthevasank/gp-parallel-ts), a single-ﬁdelity GP-based BO that optimizes posterior samples of the objective function to obtain a batch of queries, (7) SMAC3 ( https://github. com/automl/SMAC3), BO based on random forests, (8) Hyperband (Li et al., 2017) ( https: //github.com/automl/HpBandSter) that conducts multi-ﬁdelity random search over the hy- perparameters, (9) BOHB (Falkner et al., 2018) that uses Tree Parzen Estimator (TPE) (Bergstra et al., 2011) to generate hyperparameter candidates in Hyperband iterations. We also tested our method that queries at one input and ﬁdelity each time (B = 1), which we denote by BMBO-DARN-1. We used the same setting as in Sec. 6.1 for all the multi-ﬁdelity methods, except that for SHTL, we ran 2K epochs in surrogate training to ensure the convergence. For our method, we set the maximum number of iterations in optimizing the batch acquisition function (see Algorithm 8) to 100 and tolerance level to 10−3. For the remaining methods, e.g., SMAC3 and Hyperband, we used their original implementations and default settings. For all the batch querying methods, we set the batch size to 5. All the single ﬁdelity methods queried at the highest ﬁdelity. Convolutional Neural Network (CNN). Our ﬁrst application is to train a CNN for image classiﬁca- tion. We used CIFAR-10 dataset (https://www.cs.toronto.edu/~kriz/cifar.html), from which we used 10K images for training and another 10K for evaluation. To optimize the hyperparameters, we considered three ﬁdelities, i.e., training with 1, 10, 50 epochs. We used the average negative log-loss (nLL) to evaluate the prediction accuracy of each method. We considered optimizing the following hyperparameters: # convolutional layers ranging from [1,4], # channels in the ﬁrst ﬁlter ([8, 136]), depth of the dense layers ([1, 8]), width of the dense layers ([32, 2080]), pooling type ([“max”, “average”]), and dropout rate ([10−3, 0.99]). We optimized the dropout rate in the log domain, and used a continuous relaxation of the discrete parameters. Initially, we queried at 10 random hyperparameter settings at each ﬁdelity. All the methods started with these evaluation results and repeatedly identiﬁed new hyperparameters. We used the average running time at each training ﬁdelity as the cost: λ1 : λ2 : λ3 = 1 : 10 : 50 . After each query, we evaluated the performance of the new hyperparameters at the highest 80 1000 2000 3000 4000 5000 6000 7000 8000 Accumulated Cost (Time in seconds) 1.0 1.5 2.0Negative Log Loss (a) CNN 0 200 400 600 800 1000 1200 Accumulated Cost (Time in seconds) 600 800 1000 1200 1400Perplexity (b) Online LDA 0 5 10 15 20 25 30 35 Accumulated Cost (Time in seconds) −0.4 −0.3 −0.2 −0.1Log nRMSE (c) XGBoost 0 1000 2000 3000 4000 5000 6000 7000 8000 Accumulated Cost (Time in seconds) −6 −4 −2 0 Log nRMSE (d) PINN Figure 2: Performance vs. accumulated cost (running time) in Hyperparameter optimization tasks. For fairness, all the batch methods queried new examples sequentially, i.e., no parallel querying was employed. The results were averaged over ﬁve runs. Note that MF-GP-UCB, MF-MES and MF-MES-Batch often obtained very close results and their curves overlap much. training level. We ran each method until 100 queries were issued. We repeated the exper- iment for 5 times and in Fig. 2a report the average accuracy (nLL) and its standard devi- ation for the hyperparameters found by each method throughout the optimization procedure. 0 250 500 750 1000 1250 1500 1750Time(seconds)-Per-Query BMBO-DARN BMBO-DARN-1 DNN-MFBO MF-GP-UCB MF-MES MF-MES-Batch    MF-GP-UCB MF-MES MF-MES-Batch    (a) CNN 0 250 500 750 1000 1250 1500 1750Time(seconds)-Per-Query (b) Online LDA Figure 3: Average time to generate queries (including surrogate training). Online Latent Dirichlet Allocation (LDA). Our second task is to train online LDA (Hoffman et al., 2010) to extract topics from 20NewsGroups corpus ( http://qwone.com/~jason/ 20Newsgroups/). We used 5K documents for training, and 2K for evaluation. We used the implement from the scikit-learn library (https: //scikit-learn.org/stable/). We considered optimizing the following hyperparam- eters: document topic prior α ∈ [10−3,1], topic word prior η ∈ [10−3,1], learn- ing decay κ ∈ [0.51,1], learning offset τ0 ∈[1,2,5,10,20,50,100,200], E-step stopping tolerance ϵ∈[10−5,10−1], document batch size in [2,4,8,16,32,64,128,256], and topic number K ∈[1,64]. We optimized α, η, κand ϵin the log domain, and used a continuous relaxation of the discrete parameters. We considered three ﬁdelities — training with 1, 10 and 50 epochs, and randomly queried 10 examples at each ﬁdelity to start each method. We evaluated the performance of the selected hyperparameters in terms of perplexity (the smaller, the better). In Fig. 2 b, we reported the average perplexity (and its standard deviation) of each method after ﬁve runs of the hyperparameter optimization. XGBoost. Third, we trained an XGBoost model (Chen and Guestrin, 2016) to predict a quantitative measure of the diabetes progression ( https://archive.ics.uci.edu/ml/datasets/ diabetes). The dataset includes 442 examples. We used two-thirds for training and the remaining one-third for evaluation. We used the implementation from the scikit-learn library. We optimized the following hyperparameters: Huber loss parameter α∈[0.01,0.1], the non-negative complexity pruning parameter ([0.01,100]), fraction of samples used to ﬁt individual base learners ( [0.1,1]), 9fraction of features considered to split the tree ( [0.01,1]), splitting criterion ([“MAE”, “MSE”]), minimum number of samples required to split an internal node ( [2,9]), and the maximum depth of individual trees ( [1,16]). The hyperparameter space is 12 dimensional. We considered three ﬁdelities — training XGBoost with 2, 10 and 100 weak learners (trees). The querying cost is therefore λ1 : λ2 : λ3 = 1 : 5 : 50. We started with 10 random queries at each ﬁdelity. We used the log of nRMSE to evaluate the performance. We ran 5 times and report the average log-nRMSE of the identiﬁed hyperparameters by each method in Fig. 2c. Physics-informed Neural Networks (PINN). Our fourth application is to learn a PINN to solve PDEs (Raissi et al., 2019). The key idea of PINN is to use boundary points to construct the training loss, and meanwhile use a set of collocation points in the domain to regularize the NN solver to respect the PDE. With appropriate choices of hyperparameters, PINNs can obtain very accurate solutions. We used PINNs to solve Burger’s equation (Morton and Mayers, 2005) with the viscosity 0.01/π. The solution becomes sharper with bigger time variables (see the Appendix) and hence the learning is quite challenging. We followed (Raissi et al., 2019) to use fully connected networks and L-BFGS for training. The hyperparameters include NN depth ([1,8]), width ([1,64]), and activations (8 choices: Relu, tanh, sigmoid, their variants, etc.). Following (Raissi et al., 2019), we used 100 boundary points as the training set and 10K collocation points for regularization. We used 10K points for evaluation. We chose 3 training ﬁdelities, running L-BFGS with 10, 100, 50K maximum iterations. The querying cost (average training time) is λ1 : λ2 : λ3 = 1 : 10 : 50 . Note that in ﬁdelity 3, L-BFGS usually converged before running 50K iterations. We initially issued 10 random queries at each ﬁdelity. We ran each method for 5 times and reported the average log nRMSE after each step in Fig. 2d. Results. As we can see, in all the applications, BMBO-DARN used the smallest cost (i.e., running time) to ﬁnd the hyperparameters that gives the best learning performance. In general, BMBO-DARN identiﬁed better hyperparameters with the same cost, or equally good hyperparameters with the smallest cost. BMBO-DARN-1 outperformed all the one-by-one querying methods, except that for online LDA (Fig. 2b) and PINN (Fig. 2d), it was worse than DNN-MFBO and Hyperband at the early stage, but ﬁnally obtained better learning performance. We observed that the GP based baselines (MF-MES, MF-GP-UCB, SF-Batch, etc.) are often easier to be stuck in suboptimal hyperparameters, this might because these models are not effective enough to integrate information of multiple ﬁdelities to obtain a good surrogate. Together these results have shown the advantage of our method, especially in our batch querying strategy. Finally, we show the average query generation time of BMBO-DARN for CNN and Online LDA in Fig. 3 (including surrogate training). It turns out BMBO-DARN spends much less time than MF-MES using the global optimization method DIRECT (Jones et al., 1998), and comparable to MF-GP-UCB and DNN-MFBO. Therefore, BMBO-DARN is efﬁcient to update the surrogate model and generate new queries. 7 Conclusion We have presented BMBO-DARN, a batch multi-ﬁdelity Bayesian optimization method. Our deep auto-regressive model can serve as a better surrogate of the black-box objective. Our batch query- ing method not only is efﬁcient, avoiding combinatorial search over discrete ﬁdelities, but also signiﬁcantly reduces the cost while improving the optimization performance. Acknowledgments This work has been supported by MURI AFOSR grant FA9550-20-1-0358. References Auer, P. (2002). Using conﬁdence bounds for exploitation-exploration trade-offs. Journal of Machine Learning Research, 3(Nov):397–422. Baptista, R. and Poloczek, M. (2018). Bayesian optimization of combinatorial structures. In International Conference on Machine Learning, pages 462–471. Bergstra, J., Bardenet, R., Bengio, Y ., and Kégl, B. (2011). Algorithms for hyper-parameter op- timization. In 25th annual conference on neural information processing systems (NIPS 2011), 10volume 24. Neural Information Processing Systems Foundation. Chen, T., Fox, E., and Guestrin, C. (2014). Stochastic gradient Hamiltonian Monte Carlo. In International conference on machine learning, pages 1683–1691. PMLR. Chen, T. and Guestrin, C. (2016). Xgboost: A scalable tree boosting system. In Proceedings of the 22nd acm SigKDD international conference on knowledge discovery and data mining, pages 785–794. Chung, T. (2010). Computational ﬂuid dynamics. Cambridge university press. Cobb, A. D. and Jalaian, B. (2021). Scaling Hamiltonian Monte Carlo inference for Bayesian neural networks with symmetric splitting. Uncertainty in Artiﬁcial Intelligence. Cutajar, K., Pullin, M., Damianou, A., Lawrence, N., and González, J. (2019). Deep gaussian processes for multi-ﬁdelity modeling. arXiv preprint arXiv:1903.07320. Daxberger, E., Makarova, A., Turchetta, M., and Krause, A. (2019). Mixed-variable Bayesian optimization. arXiv preprint arXiv:1907.01329. Domhan, T., Springenberg, J. T., and Hutter, F. (2015). Speeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves. In Twenty-fourth international joint conference on artiﬁcial intelligence. Falkner, S., Klein, A., and Hutter, F. (2018). BOHB: Robust and efﬁcient hyperparameter optimization at scale. In International Conference on Machine Learning, pages 1437–1446. PMLR. Forrester, A., Sobester, A., and Keane, A. (2008). Engineering design via surrogate modelling: a practical guide. John Wiley & Sons. Fukushima, K. and Miyake, S. (1982). Neocognitron: A self-organizing neural network model for a mechanism of visual pattern recognition. In Competition and cooperation in neural nets, pages 267–285. Springer. Garrido-Merchán, E. C. and Hernández-Lobato, D. (2020). Dealing with categorical and integer- valued variables in Bayesian optimization with gaussian processes. Neurocomputing, 380:20–35. González, J., Dai, Z., Hennig, P., and Lawrence, N. (2016). Batch Bayesian optimization via local penalization. In Artiﬁcial intelligence and statistics, pages 648–657. PMLR. Hennig, P. and Schuler, C. J. (2012). Entropy search for information-efﬁcient global optimization. Journal of Machine Learning Research, 13(Jun):1809–1837. Hernández-Lobato, J. M., Hoffman, M. W., and Ghahramani, Z. (2014). Predictive entropy search for efﬁcient global optimization of black-box functions. In Advances in neural information processing systems, pages 918–926. Hernández-Lobato, J. M., Requeima, J., Pyzer-Knapp, E. O., and Aspuru-Guzik, A. (2017). Parallel and distributed Thompson sampling for large-scale accelerated exploration of chemical space. In International conference on machine learning, pages 1470–1479. PMLR. Hoffman, M., Bach, F. R., and Blei, D. M. (2010). Online learning for latent dirichlet allocation. In advances in neural information processing systems, pages 856–864. Citeseer. Huang, D., Allen, T. T., Notz, W. I., and Miller, R. A. (2006). Sequential kriging optimization using multiple-ﬁdelity evaluations. Structural and Multidisciplinary Optimization, 32(5):369–382. Jones, D. R., Schonlau, M., and Welch, W. J. (1998). Efﬁcient global optimization of expensive black-box functions. Journal of Global optimization, 13(4):455–492. Kandasamy, K., Dasarathy, G., Oliva, J. B., Schneider, J., and Póczos, B. (2016). Gaussian process bandit optimisation with multi-ﬁdelity evaluations. In Advances in Neural Information Processing Systems, pages 992–1000. 11Kandasamy, K., Dasarathy, G., Schneider, J., and Póczos, B. (2017a). Multi-ﬁdelity Bayesian optimisation with continuous approximations. In Proceedings of the 34th International Conference on Machine Learning-V olume70, pages 1799–1808. JMLR. org. Kandasamy, K., Krishnamurthy, A., Schneider, J., and Poczos, B. (2017b). Asynchronous parallel Bayesian optimisation via Thompson sampling. arXiv preprint arXiv:1705.09236. Kingma, D. P. and Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980. Klein, A., Falkner, S., Bartels, S., Hennig, P., and Hutter, F. (2017a). Fast Bayesian optimization of machine learning hyperparameters on large datasets. In Artiﬁcial Intelligence and Statistics, pages 528–536. PMLR. Klein, A., Falkner, S., Springenberg, J. T., and Hutter, F. (2017b). Learning curve prediction with Bayesian neural networks. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net. Kutluay, S., Bahadir, A., and Özdecs, A. (1999). Numerical solution of one-dimensional burgers equation: explicit and exact-explicit ﬁnite difference methods. Journal of Computational and Applied Mathematics, 103(2):251–261. Laguna, M. and Martí, R. (2005). Experimental testing of advanced scatter search designs for global optimization of multimodal functions. Journal of Global Optimization, 33(2):235–255. Lam, R., Allaire, D. L., and Willcox, K. E. (2015). Multiﬁdelity optimization using statistical surrogate modeling for non-hierarchical information sources. In 56th AIAA/ASCE/AHS/ASC Structures, Structural Dynamics, and Materials Conference, page 0143. LeCun, Y ., Boser, B. E., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W. E., and Jackel, L. D. (1990). Handwritten digit recognition with a back-propagation network. In Advances in neural information processing systems, pages 396–404. Li, L., Jamieson, K., DeSalvo, G., Rostamizadeh, A., and Talwalkar, A. (2017). Hyperband: A novel bandit-based approach to hyperparameter optimization. The Journal of Machine Learning Research, 18(1):6765–6816. Li, L., Jamieson, K., Rostamizadeh, A., Gonina, E., Hardt, M., Recht, B., and Talwalkar, A. (2018). Massively parallel hyperparameter tuning. arXiv preprint arXiv:1810.05934. Li, S., Xing, W., Kirby, R., and Zhe, S. (2020). Multi-ﬁdelity Bayesian optimization via deep neural networks. In Advances in Neural Information Processing Systems. Liu, D. C. and Nocedal, J. (1989). On the limited memory BFGS method for large scale optimization. Mathematical programming, 45(1-3):503–528. McLeod, M., Osborne, M. A., and Roberts, S. J. (2017). Practical Bayesian optimization for variable cost objectives. arXiv preprint arXiv:1703.04335. Mockus, J. (2012). Bayesian approach to global optimization: theory and applications, volume 37. Springer Science & Business Media. Mockus, J., Tiesis, V ., and Zilinskas, A. (1978). The application of Bayesian methods for seeking the extremum. Towards global optimization, 2(117-129):2. Morton, K. W. and Mayers, D. F. (2005). Numerical solution of partial differential equations: an introduction. Cambridge university press. Nagel, K. (1996). Particle hopping models and trafﬁc ﬂow theory. Physical review E, 53(5):4655. Neal, R. M. et al. (2011). Mcmc using Hamiltonian dynamics. Handbook of markov chain monte carlo, 2(11):2. 12Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., and Chintala, S. (2019). Pytorch: An imperative style, high- performance deep learning library. In Wallach, H., Larochelle, H., Beygelzimer, A., d'Alché-Buc, F., Fox, E., and Garnett, R., editors, Advances in Neural Information Processing Systems 32, pages 8024–8035. Curran Associates, Inc. Perdikaris, P., Raissi, M., Damianou, A., Lawrence, N., and Karniadakis, G. E. (2017). Nonlinear information fusion algorithms for data-efﬁcient multi-ﬁdelity modelling. Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences, 473(2198):20160751. Perrone, V ., Jenatton, R., Seeger, M. W., and Archambeau, C. (2018). Scalable hyperparameter transfer learning. In Advances in Neural Information Processing Systems, pages 6845–6855. Picheny, V ., Ginsbourger, D., Richet, Y ., and Caplin, G. (2013). Quantile-based optimization of noisy computer experiments with tunable precision. Technometrics, 55(1):2–13. Poloczek, M., Wang, J., and Frazier, P. (2017). Multi-information source optimization. In Advances in Neural Information Processing Systems, pages 4288–4298. Raissi, M., Perdikaris, P., and Karniadakis, G. E. (2017). Physics informed deep learning (part i): Data-driven solutions of nonlinear partial differential equations. arXiv preprint arXiv:1711.10561. Raissi, M., Perdikaris, P., and Karniadakis, G. E. (2019). Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of Computational Physics, 378:686–707. Rasmussen, C. E. and Williams, C. K. I. (2006). Gaussian Processes for Machine Learning. MIT Press. Shah, A., Xing, W., and Triantafyllidis, V . (2017). Reduced-order modelling of parameter-dependent, linear and nonlinear dynamic partial differential equation models. Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences, 473(2200):20160809. Snoek, J., Larochelle, H., and Adams, R. P. (2012). Practical Bayesian optimization of machine learning algorithms. In Advances in neural information processing systems, pages 2951–2959. Snoek, J., Rippel, O., Swersky, K., Kiros, R., Satish, N., Sundaram, N., Patwary, M., Prabhat, M., and Adams, R. (2015). Scalable Bayesian optimization using deep neural networks. In International conference on machine learning, pages 2171–2180. Song, J., Chen, Y ., and Yue, Y . (2019). A general framework for multi-ﬁdelity Bayesian optimization with Gaussian processes. In The 22nd International Conference on Artiﬁcial Intelligence and Statistics, pages 3158–3167. Springenberg, J. T., Klein, A., Falkner, S., and Hutter, F. (2016). Bayesian optimization with robust Bayesian neural networks. In Advances in neural information processing systems, volume 29, pages 4134–4142. Srinivas, N., Krause, A., Kakade, S., and Seeger, M. (2010). Gaussian process optimization in the bandit setting: no regret and experimental design. In Proceedings of the 27th International Conference on International Conference on Machine Learning, pages 1015–1022. Sugimoto, N. (1991). Burgers equation with a fractional derivative; hereditary effects on nonlinear acoustic waves. Journal of ﬂuid mechanics, 225:631–653. Swersky, K., Snoek, J., and Adams, R. P. (2013). Multi-task Bayesian optimization. In Advances in neural information processing systems, pages 2004–2012. Swersky, K., Snoek, J., and Adams, R. P. (2014). Freeze-thaw Bayesian optimization. arXiv preprint arXiv:1406.3896. Takeno, S., Fukuoka, H., Tsukada, Y ., Koyama, T., Shiga, M., Takeuchi, I., and Karasuyama, M. (2019). Multi-ﬁdelity Bayesian optimization with max-value entropy search. arXiv preprint arXiv:1901.08275. 13Wang, Z. and Jegelka, S. (2017). Max-value entropy search for efﬁcient Bayesian optimization. In Proceedings of the 34th International Conference on Machine Learning-V olume70, pages 3627– 3635. JMLR. org. Wu, J. and Frazier, P. (2016). The parallel knowledge gradient method for batch Bayesian optimization. Advances in Neural Information Processing Systems, 29:3126–3134. Wu, J. and Frazier, P. I. (2017). Continuous-ﬁdelity Bayesian optimization with knowledge gradient. In NIPS Workshop on Bayesian Optimization. Zhang, Y ., Hoang, T. N., Low, B. K. H., and Kankanhalli, M. (2017). Information-based multi-ﬁdelity Bayesian optimization. In NIPS Workshop on Bayesian Optimization. Appendix 8 Synthetic Benchmark Functions 8.1 Branin Function The input is two dimensional, x = [x1,x2] ∈[−5,10] ×[0,15]. We have three ﬁdelities to evaluate the function, which, from high to low, are given by f3(x) = − (−1.275x2 1 π2 + 5x1 π + x2 −6 )2 − ( 10 − 5 4π ) cos(x1) −10, f2(x) = −10 √ −f3(x−2) −2(x1 −0.5) + 3(3x2 −1) + 1, f1(x) = −f2 ( 1.2(x + 2) ) + 3x2 −1. (9) We can see that between ﬁdelities are nonlinear transformations, nonuniform scaling, and shifts. 8.2 Levy Function The input is two dimensional, x = [x1,x2] ∈[−10,10]2. We have two ﬁdelities, f2(x) = −sin2(3πx1) −(x1 −1)2[1 + sin2(3πx2)] −(x2 −1)2[1 + sin2(2πx2)], f1(x) = − √ 1 + f2 2 (x). (10) 9 Details about Physics Informed Neural Networks Burgers’ equation is a canonical nonlinear hyperbolic PDE, and widely used to characterize a variety of physical phenomena, such as nonlinear acoustics (Sugimoto, 1991), ﬂuid dynamics (Chung, 2010), and trafﬁc ﬂows (Nagel, 1996). Since the solution can develop discontinuities (i.e., shock waves) based on a normal conservation equation, Burger’s equation is often used as a nontrivial benchmark test for numerical solvers and surrogate models (Kutluay et al., 1999; Shah et al., 2017; Raissi et al., 2017). We used physics informed neural networks (PINN) to solve the viscosity version of Burger’s equation, ∂u ∂t + u∂u ∂x = ν∂2u ∂x2 , (11) where uis the volume, xis the spatial location, tis the time, and ν is the viscosity. Note that the smaller ν, the sharper the solution of u. In our experiment, we set ν = 0.01 π , x ∈[−1,1], and t∈[0,1]. The boundary condition is given by u(0,x) = −sin(πx), u(t,−1) = u(t,1) = 0. We use an NNuWto represent the solution. To estimate the NN, we collectedN training points in the boundary, D= {(ti,xi,ui)}N i=1, and M collocation (input) points in the domain, C= {(ˆti,ˆxi)}M i=1. We then minimize the following loss function to estimate uW, L(W) = 1 N N∑ i=1 (uW(ti,xi) −ui)2 + 1 M M∑ i=1 (⏐⏐ψ(uW)(ˆti,ˆxi) ⏐⏐2) , 14where ψ(·) is a functional constructed from the PDE, ψ(u) = ∂u ∂t + u∂u ∂x −ν∂2u ∂x2 . Obviously, the loss consists of two terms, one is the training loss, and the other is a regularization term that enforces the NN solution to respect the PDE. 15",
      "meta_data": {
        "arxiv_id": "2106.09884v2",
        "authors": [
          "Shibo Li",
          "Robert M. Kirby",
          "Shandian Zhe"
        ],
        "published_date": "2021-06-18T02:55:48Z",
        "pdf_url": "https://arxiv.org/pdf/2106.09884v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "Proposes Batch Multi-Fidelity Bayesian Optimization with Deep Auto-Regressive Networks (BMBO-DARN) to optimize black-box, expensive-to-evaluate functions across multiple fidelities. Introduces a deep auto-regressive model using Bayesian neural networks to capture strong, complex (nonstationary, highly nonlinear) relationships across all fidelities, improving surrogate learning and optimization performance. Develops an efficient batch querying method that utilizes a novel batch acquisition function based on Max-value Entropy Search (MES), penalizing highly correlated queries and encouraging diversity. The method employs efficient computation of the acquisition function via posterior samples and moment matching, and an alternating optimization algorithm to select query points without combinatorial search. Demonstrates superior performance and reduced optimization cost in four real-world hyperparameter optimization applications.",
        "methodology": "The methodology centers on a deep auto-regressive model for multi-fidelity surrogate learning. This model consists of a chain of M Bayesian neural networks (BNNs), where each BNN `fm(x)` models a specific fidelity. The input `xm` for each `fm` includes the original input `x` and the outputs of all preceding fidelities (`f1(x), ..., fm-1(x)`), enabling the model to capture complex, possibly nonlinear and nonstationary, relationships across fidelities. Hamiltonian Monte Carlo (HMC) sampling is used for posterior inference of the BNN parameters and noise precisions. For batch querying, a batch acquisition function is proposed based on the Max-value Entropy Search (MES) principle, quantifying the mutual information between the outputs of a batch of queries and the function optimum, normalized by the total query cost to balance benefit and cost. To efficiently compute this acquisition function, posterior samples of NN weights are drawn, and moment matching is applied to approximate the joint posterior of the query outputs and the function optimum as a multivariate Gaussian distribution, allowing for a closed-form mutual information calculation. The maximization of this batch acquisition function, which involves a mix of continuous inputs and discrete fidelities, is performed using an alternating optimization algorithm that cyclically updates each fidelity-input pair to avoid computationally expensive combinatorial search, guaranteeing improvement at each step.",
        "experimental_setup": "The approach was evaluated on both synthetic benchmarks and real-world hyperparameter optimization tasks. Synthetic benchmarks included the Levy function (two-fidelity, 2D input) and Branin function (three-fidelity, 2D input), where nonlinear/nonstationary transformations existed between fidelities. Performance was measured by normalized root-mean-square-error (nRMSE) and mean-negative-log-likelihood (MNLL) on 100 test samples after training with specific numbers of examples at different fidelities. Real-world applications involved hyperparameter optimization for: (1) Convolutional Neural Networks (CNN) on CIFAR-10 for image classification (3 fidelities: 1, 10, 50 epochs; metric: negative log-loss), (2) Online Latent Dirichlet Allocation (LDA) on 20NewsGroups for topic extraction (3 fidelities: 1, 10, 50 epochs; metric: perplexity), (3) XGBoost for diabetes diagnosis (3 fidelities: 2, 10, 100 weak learners; metric: log nRMSE), and (4) Physics-Informed Neural Networks (PINN) for solving Burger's equation (3 fidelities: 10, 100, 50K max L-BFGS iterations; metric: log nRMSE). All experiments started with 10 random initial queries per fidelity. Batch size was set to 5 for batch methods. BMBO-DARN was implemented in PyTorch, utilizing Hamiltorch for HMC sampling (5K burn-in, 200 posterior samples). The alternating optimization for the batch acquisition function ran for a maximum of 100 iterations or until a tolerance of 10^-3 was reached. Results were averaged over five runs. Comparison methods included state-of-the-art multi-fidelity BO (MF-GP-UCB, MF-MES, SHTL, DNN-MFBO), batch BO (MF-MES-Batch, SF-Batch), and popular hyperparameter tuning methods (SMAC3, Hyperband, BOHB), as well as a single-query version of BMBO-DARN (BMBO-DARN-1).",
        "limitations": "While BMBO-DARN addresses several limitations of prior multi-fidelity Bayesian optimization methods, its own design involves certain inherent constraints or approximations. The computation of the batch acquisition function relies on approximating the joint posterior distribution of function values and the optimum as a multivariate Gaussian using moment matching, which might not perfectly capture the true, potentially complex, posterior of Bayesian Neural Networks. Additionally, the alternating optimization algorithm used to maximize the batch acquisition function is a heuristic employed to avoid computationally prohibitive combinatorial search, meaning it does not guarantee finding the global optimum for the batch of queries. Furthermore, although the paper mentions the possibility of switching to approximate inference, the primary method used, Hamiltonian Monte Carlo (HMC) sampling, can be computationally intensive, particularly for complex neural network architectures, despite offering high-quality uncertainty quantification.",
        "future_research_directions": "Not mentioned"
      }
    },
    {
      "title": "Supervising the Multi-Fidelity Race of Hyperparameter Configurations",
      "abstract": "Multi-fidelity (gray-box) hyperparameter optimization techniques (HPO) have\nrecently emerged as a promising direction for tuning Deep Learning methods.\nHowever, existing methods suffer from a sub-optimal allocation of the HPO\nbudget to the hyperparameter configurations. In this work, we introduce DyHPO,\na Bayesian Optimization method that learns to decide which hyperparameter\nconfiguration to train further in a dynamic race among all feasible\nconfigurations. We propose a new deep kernel for Gaussian Processes that embeds\nthe learning curve dynamics, and an acquisition function that incorporates\nmulti-budget information. We demonstrate the significant superiority of DyHPO\nagainst state-of-the-art hyperparameter optimization methods through\nlarge-scale experiments comprising 50 datasets (Tabular, Image, NLP) and\ndiverse architectures (MLP, CNN/NAS, RNN).",
      "full_text": "Supervising the Multi-Fidelity Race of Hyperparameter Configurations Martin Wistuba∗† Amazon Web Services, Berlin, Germany marwistu@amazon.com Arlind Kadra∗ University of Freiburg, Freiburg, Germany kadraa@cs.uni-freiburg.de Josif Grabocka University of Freiburg, Freiburg, Germany grabocka@cs.uni-freiburg.de Abstract Multi-fidelity (gray-box) hyperparameter optimization techniques (HPO) have recently emerged as a promising direction for tuning Deep Learning methods. However, existing methods suffer from a sub-optimal allocation of the HPO bud- get to the hyperparameter configurations. In this work, we introduce DyHPO, a Bayesian Optimization method that learns to decide which hyperparameter config- uration to train further in a dynamic race among all feasible configurations. We propose a new deep kernel for Gaussian Processes that embeds the learning curve dynamics, and an acquisition function that incorporates multi-budget information. We demonstrate the significant superiority of DyHPO against state-of-the-art hy- perparameter optimization methods through large-scale experiments comprising 50 datasets (Tabular, Image, NLP) and diverse architectures (MLP, CNN/NAS, RNN). 1 Introduction Hyperparameter Optimization (HPO) is arguably an acute open challenge for Deep Learning (DL), especially considering the crucial impact HPO has on achieving state-of-the-art empirical results. Unfortunately, HPO for DL is a relatively under-explored field and most DL researchers still optimize their hyperparameters via obscure trial-and-error practices. On the other hand, traditional Bayesian Optimization HPO methods [Snoek et al., 2012, Bergstra et al., 2011] are not directly applicable to deep networks, due to the infeasibility of evaluating a large number of hyperparameter configurations. In order to scale HPO for DL, three main directions of research have been recently explored. (i) Online HPO methods search for hyperparameters during the optimization process via meta-level controllers [Chen et al., 2017, Parker-Holder et al., 2020], however, this online adaptation can not accommodate all hyperparameters (e.g. related to architectural changes). (ii) Gradient-based HPO techniques, on the other hand, compute the derivative of the validation loss w.r.t. hyperparameters by reversing the training update steps [Maclaurin et al., 2015, Franceschi et al., 2017, Lorraine et al., 2020], however, the reversion is not directly applicable to all cases (e.g. dropout rate). The last direction, (iii) Gray-box HPO techniques discard sub-optimal configurations after evaluating them on lower budgets [Li et al., 2017, Falkner et al., 2018]. In contrast to the online and gradient-based alternatives, gray-box approaches can be deployed in an off-the-shelf manner to all types of hyperparameters and architectures. The gray-box concept is based on the intuition that a poorly-performing hyperparameter configuration can be identified and ∗equal contribution †work does not relate to position at Amazon 36th Conference on Neural Information Processing Systems (NeurIPS 2022). arXiv:2202.09774v2  [cs.LG]  1 Jun 2023terminated by inspecting the validation loss of the first few epochs, instead of waiting for the full convergence. The most prominent gray-box algorithm is Hyperband [Li et al., 2017], which is based on successive halving. It runs random configurations at different budgets (e.g. number of epochs) and successively halves these configurations by keeping only the top performers. Follow-up works, such as BOHB [Falkner et al., 2018] or DEHB [Awad et al., 2021], replace the random sampling of Hyperband with a sampling based on Bayesian optimization or differentiable evolution. Despite their great practical potential, gray-box methods suffer from a major issue. The low-budget (few epochs) performances are not always a good indicator for the full-budget (full convergence) performances. For example, a properly regularized network converges slower in the first few epochs, however, typically performs better than a non-regularized variant after the full convergence. In other words, there can be a poor rank correlation of the configurations’ performances at different budgets. Figure 1: Top: The learning curve for different hyperparameter configurations. The darker the learning curve, the later it was evaluated during the search. Bottom: The hyperparameter indices in a temporal order as evaluated during the opti- mization and their corresponding curves. We introduceDYHPO, a Bayesian Optimization (BO) approach based on Gaussian Processes (GP), that proposes a novel treatment to the multi-budget (a.k.a. multi-fidelity) setup. In this perspective, we propose a deep kernel GP that captures the learning dynamics. As a result, we train a kernel capable of capturing the similarity of a pair of hyperparameter configurations, even if the pair’s configurations are evaluated at differ- ent budgets. Furthermore, we extend Expected Improvement [Jones et al., 1998] to the multi- budget case, by introducing a new mechanism for the incumbent configuration of a budget. We illustrate the differences between our rac- ing strategy and successive halving with the experiment of Figure 1, where, we showcase the HPO progress of two different methods on the \"Helena\" dataset from the LCBench bench- mark [Zimmer et al., 2021]. Hyperband [Li et al., 2017] is a gray-box approach that stati- cally pre-allocates the budget for a set of candidates (Hyperband bracket) according to a predefined policy. However, DYHPO dynamically adapts the allocation of budgets for configurations after every HPO step (a.k.a. a dynamic race). As a result, DYHPO invests only a small budget on configurations that show little promise as indicated by the intermediate scores. The joint effect of modeling a GP kernel across budgets together with a dedicated acquisition function leads to DYHPO achieving a statistically significant empirical gain against state-of-the-art gray-box baselines [Falkner et al., 2018, Awad et al., 2021], including prior work on multi-budget GPs [Kandasamy et al., 2017, 2020] or neural networks [Li et al., 2020b]. We demonstrate the performance of DYHPO in three diverse deep learning architectures (MLP, CNN/NAS, RNN) and 50 datasets of three diverse modalities (tabular, image, natural language processing). We believe our method is a step forward toward making HPO for DL practical and feasible. Overall, our contributions can be summarized as follows: • We introduce a novel Bayesian surrogate for gray-box HPO optimization. Our novel surrogate model predicts the validation score of a machine learning model based on both the hyperparameter configuration, the budget information, and the learning curve. • We derive a simple yet robust way to combine this surrogate model with Bayesian opti- mization, reusing most of the existing components currently used in traditional Bayesian optimization methods. • Finally, we demonstrate the efficiency of our method for HPO and neural architecture search tasks compared to the current state-of-the-art methods in HPO, by outperforming seven strong HPO baselines with a statistically significant margin. As an overarching goal, we believe our method is an important step toward scaling HPO for DL. 22 Related Work on Gray-box HPO Multi-Fidelity Bayesian Optimization and Bandits. Bayesian optimization is a black-box func- tion optimization framework that has been successfully applied in optimizing hyperparameter and neural architectures alike [Snoek et al., 2012, Kandasamy et al., 2018, Bergstra et al., 2011]. To further improve Bayesian optimization, several works propose low-fidelity data approximations of hyperparameter configurations by training on a subset of the data [Swersky et al., 2013, Klein et al., 2017a], or by terminating training early [Swersky et al., 2014]. Additionally, several methods extend Bayesian optimization to multi-fidelity data by engineering new kernels suited for this problem [Swer- sky et al., 2013, 2014, Poloczek et al., 2017]. Kandasamy et al. [2016] extends GP-UCB [Srinivas et al., 2010] to the multi-fidelity setting by learning one Gaussian Process (GP) with a standard kernel for each fidelity. Their later work improves upon this method by learning one GP for all fidelities that enables the use of continuous fidelities [Kandasamy et al., 2017]. The work by Takeno et al. [2020] follows a similar idea but proposes to use an acquisition function based on information gain instead of UCB. While most of the works rely on GPs to model the surrogate function, Li et al. [2020b] use a Bayesian neural network that models the complex relationship between fidelities with stacked neural networks, one for each fidelity. Hyperband [Li et al., 2017] is a bandits-based multi-fidelity method for hyperparameter optimization that selects hyperparameter configurations at random and uses successive halving [Jamieson and Tal- walkar, 2016] with different settings to early-stop less promising training runs. Several improvements have been proposed to Hyperband with the aim to replace the random sampling of hyperparameter configurations with a more guided approach [Bertrand et al., 2017, Wang et al., 2018, Wistuba, 2017]. BOHB [Falkner et al., 2018] uses TPE [Bergstra et al., 2011] and builds a surrogate model for every fidelity adhering to a fixed-fidelity selection scheme. DEHB [Awad et al., 2021] samples candidates using differential evolution which handles large and discrete search spaces better than BOHB. Mendes et al. [2021] propose a variant of Hyperband which allows to skip stages. Learning Curve Prediction A variety of methods attempt to extrapolate a partially observed learning curve in order to estimate the probability that a configuration will improve over the current best solution. Domhan et al. [2015] propose to ensemble a set of parametric functions to extrapolate a partial learning curve. While this method is able to extrapolate with a single example, it requires a relatively long learning curve to do so. The work by Klein et al. [2017b] build upon the idea of using a set of parametric functions. The main difference is that they use a heteroscedastic Bayesian model to learn the ensemble weights. Baker et al. [2018] propose to use support vector machines (SVM) as an auto-regressive model. The SVM predicts the next value of a learning curve, the original learning curve is augmented by this value and we keep predicting further values. The work by Gargiani et al. [2019] use a similar idea but makes prediction based on the last K observations only and uses probabilistic models. Wistuba and Pedapati [2020] propose to learn a prediction model across learning curves from different tasks to avoid the costly learning curve collection. In contrast to DYHPO , none of these methods selects configuration but is limited to deciding when to stop a running configuration. Multi-Fidelity Acquisition Functions Klein et al. [2017a] propose an acquisition function which allows for selecting hyperparameter configurations and the dataset subset size. The idea is to reduce training time by considering only a smaller part of the training data. In contrast to EIMF, this acquisition function is designed to select arbitrary subset sizes whereas EIMF is intended to slowly increase the invested budget over time. Mendes et al. [2020] extend the work of Klein et al. [2017a] to take business constraints into account. Deep Kernel Learning with Bayesian Optimization. We are among the first to use deep kernel learning with Bayesian optimization and to the best of our knowledge the first to use it for multi- fidelity Bayesian optimization. Rai et al. [2016] consider the use of a deep kernel instead of a manually designed kernel in the context of standard Bayesian optimization, but, limit their experimentation to synthetic data and do not consider its use for hyperparameter optimization. Perrone et al. [2018], Wistuba and Grabocka [2021] use a pre-trained deep kernel to warm start Bayesian optimization with meta-data from previous optimizations. The aforementioned approaches are multi-task or transfer learning methods that require the availability of meta-data from related tasks. 3In contrast to prior work, we propose a method that introduces deep learning to multi-fidelity HPO with Bayesian Optimization, and captures the learning dynamics across fidelities/budgets, combined with an acquisition function that is tailored for the gray-box setup. 3 Dynamic Multi-Fidelity HPO 3.1 Preliminaries Gray-Box Optimization. The gray-box HPO setting allows querying configurations with a smaller budget compared to the total maximal budget B. Thus, we can query from the response function f : X ×N → R where fi,j = f(xi, j) is the response after spending a budget ofj on configuration xi. As before, these observations are noisy and we observe yi,j = f(xi, j) +εj where εj ∼ N(0, σ2 j,n). Please note, we assume that the budget required to query fi,j+b after having queried fi,j is only b. Furthermore, we use the learning curve Yi,j−1 = (yi,1, . . . , yi,j−1) when predicting fi,j. Gaussian Processes (GP). Given a training data set D = {(xi, yi)}n i=1, the Gaussian Process assumption is that yi is a random variable and the joint distribution of all yi is assumed to be multivariate Gaussian distributed as y ∼ N(m (X) , k(X, X)) . Furthermore, f∗ for test instances x∗ are jointly Gaussian with y as:\u0014 y f∗ \u0015 ∼ N \u0012 m (X, x∗) , \u0012 Kn K∗ KT ∗ K∗∗ \u0013\u0013 . (1) The mean function m is often set to 0 and its covariance function k depends on parameters θ. For notational convenience, we use Kn = k (X, X|θ) +σ2 nI, K∗ = k (X, X∗|θ) and K∗∗ = k (X∗, X∗|θ) to define the kernel matrices. We can derive the posterior predictive distribution with mean and covariance as follows: E[f∗|X, y, X∗] =KT ∗ K−1 n y, cov [f∗|X, X∗] =K∗∗ − KT ∗ K−1 n K∗ (2) Often, the kernel function is manually engineered, one popular example is the squared exponential kernel. However, in this work, we make use of the idea of deep kernel learning [Wilson et al., 2016]. The idea is to model the kernel as a neural network φ and learn the best kernel transformation K (θ, w) :=k(φ(x, w), φ(x′; w)|θ), which allows us to use convolutional operations in our kernel. 3.2 Deep Multi-Fidelity Surrogate We propose to use a Gaussian Process surrogate model that infers the value of fi,j based on the hyperparameter configuration xi, the budget j as well as the past learning curve Yi,j−1. For this purpose, we use a deep kernel as: K (θ, w) :=k(φ(xi, Yi,j−1, j; w), φ(xi′ , Yi′,j′−1, j′; w); θ) (3) xi j Yi,j−1 · 1B P P P Convolution max P P P Figure 2: The feature extractor φ of our kernel. We use a squared exponential kernel for k and the neural network φ is composed of linear and convolutional layers as shown in Figure 2. We normalize the budget j to a range between 0 and 1 by dividing it by the maximum budget B. Afterward, it is concatenated with the hyper- parameter configuration xi and fed to a linear layer. The learning curve Yi,j−1 is transformed by a one-dimensional convolution followed by a global max pooling layer. Finally, both repre- sentations are fed to another linear layer. Its output will be the input to the kernel function k. Both, the kernel k and the neural network φ consist of trainable parameters θ and w, respectively. We find their optimal values by computing the maximum likelihood estimates as: ˆθ, ˆw = arg max θ,w p(y|X, Y, θ, w) ∝ arg min θ,w yTK (θ, w)−1 y + log|K (θ, w)| (4) 4In order to solve this optimization problem, we use gradient descent and Adam [Kingma and Ba, 2015] with a learning rate of 0.1. Given the maximum likelihood estimates, we can approximate the predictive posterior through p \u0010 fi,j|xi, Yi,j−1, j,D, ˆθ, ˆw \u0011 , and ultimately compute the mean and covariance of this Gaussian using Equation 2. 3.3 Multi-Fidelity Expected Improvement Expected improvement [Jones et al., 1998] is a commonly used acquisition function and is defined as: EI(x|D) =E[max {f(x) − ymax, 0}] , (5) where ymax is the largest observed value of f. We propose a multi-fidelity version of it as: EIMF(x, j|D) =E \u0002 max \b f(x, j) − ymax j , 0 \t\u0003 , (6) where: ymax j = \u001amax {y | ((x, ·, j), y) ∈ D} if ((x, ·, j), y) ∈ D max {y | (·, y) ∈ D} otherwise (7) Simply put, ymax j is the largest observed value of f for a budget of j if it exists already, otherwise, it is the largest observed value for any budget. If there is only one possible budget, the multi-fidelity expected improvement is identical to expected improvement. 3.4 The D YHPO Algorithm The DYHPO algorithm looks very similar to many black-box Bayesian optimization algorithms as shown in Algorithm 1. The big difference is that at each step we dynamically decide which candidate configuration to train for a small additional budget. Algorithm 1 DYHPO Algorithm 1: b(x) = 0∀x ∈ X 2: while not converged do 3: xi ← arg maxx∈X EIMF (x, b(x) + 1)(Sec. 3.3) 4: Observe yi,b(xi)+1. 5: b(xi) ← b(xi) + 1 6: D ← D ∪ \b ((xi, Yi,b(xi)−1, b(xi)), yi,b(xi)) \t 7: Update the surrogate on D. (Sec. 3.2) return xi with largest yi,·. Possible candidates are previously un- considered configurations as well as configurations that did not reach the maximum budget. In Line 2, the most promising candidate is chosen using the acquisition function introduced in Section 3.3 and the surrogate model’s predictions. It is important to high- light that we do not maximize the ac- quisition function along the budget di- mensionality. Instead, we set the bud- get b such that it is by exactly one higher than the budget used to eval- uate xi before. This ensures that we explore configurations by slowly increasing the budget. After the candidate and the corresponding budget are selected, the function f is evaluated and we observe yi,j (Line 3). This additional data point is added to D in Line 4. Then in Line 5, the surrogate model is updated according to the training scheme described in Section 3.2. 4 Experimental Protocol 4.1 Experimental Setup We evaluate DYHPO in three different settings on hyperparameter optimization for tabular, text, and image classification against several competitor methods, the details of which are provided in the following subsections. We ran all of our experiments on an Amazon EC2 M5 Instance (m5.xlarge). In our experiments, we report the mean of ten repetitions and we report two common metrics, the regret and the average rank. The regret refers to the absolute difference between the score of the solution found by an optimizer compared to the best possible score. If we report the regret as an aggregate result over multiple datasets, we report the mean over all regrets. The average rank is the metric we use to aggregate rank results over different datasets. We provide further implementation and training details in Appendix A.4. Our implementation of DYHPO is publicly available.3 3https://github.com/releaunifreiburg/DyHPO 54.2 Benchmarks In our experiments, we use the following benchmarks. We provide more details in Appendix A.1. LCBench: A learning curve benchmark [Zimmer et al., 2021] that evaluates neural network architec- tures for tabular datasets. LCBench contains learning curves for 35 different datasets, where 2,000 neural networks per dataset are trained for 50 epochs with Auto-PyTorch. TaskSet: A benchmark that features diverse tasks Metz et al. [2020] from different domains and includes 5 search spaces with different degrees of freedom, where, every search space includes 1000 hyperparameter configurations. In this work, we focus on a subset of NLP tasks (12 tasks) and we use the Adam8p search space with 8 continuous hyperparameters. NAS-Bench-201: A benchmark consisting of 15625 hyperparameter configurations representing different architectures on the CIFAR-10, CIFAR-100 and ImageNet datasets Dong and Yang [2020]. NAS-Bench-201 features a search space of 6 categorical hyperparameters and each architecture is trained for 200 epochs. 4.3 Baselines Random Search: A random/stochastic black-box search method for HPO. HyperBand: A multi-arm bandit method that extends successive halving by multiple brackets with different combinations of the initial number of configurations, and their initial budget [Li et al., 2017]. BOHB: An extension of Hyperband that replaces the random sampling of the initial configurations for each bracket with recommended configurations from a model-based approach [Falkner et al., 2018]. BOHB builds a model for every fidelity that is considered. DEHB: A method that builds upon Hyperband by exploiting differential evolution to sample the initial candidates of a Hyperband bracket [Awad et al., 2021]. ASHA: An asynchronous version of successive halving (or an asynchronous version of Hyperband if multiple brackets are run). ASHA Li et al. [2020a] does not wait for all configurations to finish inside a successive halving bracket, but, instead promotes configurations to the next successive halving bracket in real-time. MF-DNN: A multi-fidelity Bayesian optimization method that uses deep neural networks to capture the relationships between different fidelities Li et al. [2020b]. Dragonfly: We compare against BOCA [Kandasamy et al., 2017] by using the Dragonfly library Kan- dasamy et al. [2020]. This method suggests the next hyperparameter configuration as well as the budget it should be evaluated for. 4.4 Research Hypotheses and Associated Experiments Hypothesis 1: DYHPO achieves state-of-the-art results in multi-fidelity HPO. Experiment 1: We compare DYHPO against the baselines of Section 4.3 on the benchmarks of Section 4.2 with the experimental setup of Section 4.1. For TaskSet we follow the authors’ recommendation and report the number of steps (every 200 iterations). Hypothesis 2: DYHPO’s runtime overhead has a negligible impact on the quality of results. Experiment 2: We compare DYHPO against the baselines of Section 4.3 over the wallclock time. The wallclock time includes both (i) the optimizer’s runtime overhead for recommending the next hyperparameter configuration, plus (ii) the time needed to evaluate the recommended configuration. In this experiment, we consider all datasets where the average training time per epoch is at least 10 seconds, because, for tasks where the training time is short, there is no practical justification for complex solutions and their overhead. In these cases, we recommend using a random search. We don’t report results for TaskSet because the benchmark lacks training times. Hypothesis 3: DYHPO uses the computational budget more efficiently than baselines. Experiment 3: To further verify that DYHPO is efficient compared to the baselines, we investigate whether competing methods spend their budgets on qualitative candidates. Concretely we: i) calculate 60 500 1000 Number of Epochs 10 2 10 1 Mean Regret LCBench 0 500 1000 Number of Steps 10 1 TaskSet 0 2000 4000 Number of Epochs 10 1 ImageNet16-120 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO Figure 3: The mean regret for the different benchmarks over the number of epochs or steps (every 200 iterations). The results are aggregated over 35 different datasets for LCBench and aggregated over 12 different NLP tasks for TaskSet. the precision of the top (w.r.t. ground truth) performing configurations that were selected by each method across different budgets, ii) compute the average regret of the selected configurations across budget, and iii) we compare the fraction of top-performing configurations at a given budget that were not top performers at lower budgets, i.e. measure the ability to handle the poor correlation of performances across budgets. 5 Results Experiment 1: DYHPO achieves state-of-the-art results. In our first experiment, we evaluate the various methods on the benchmarks listed in Section 4.2. We show the aggregated results in Figure 3, the results show that DYHPO manages to outperform competitor methods over the set of considered benchmarks by achieving a better mean regret across datasets. Not only does DYHPO achieve a better final performance, it also achieves strong anytime results by converging faster than the competitor methods. For the extended results, related to the performance of all methods on a dataset level, we refer the reader to Appendix B. 12345678 Random Dragonfly Hyperband MF-DNN ASHA BOHB DEHB DyHPO LCBench@50% 12345678 Random Hyperband MF-DNN Dragonfly DEHB BOHB ASHA DyHPO LCBench@100% 12345678 Random MF-DNN ASHA Dragonfly Hyperband DEHB BOHB DyHPO TaskSet@50% 12345678 Random MF-DNN ASHA Dragonfly DEHB Hyperband BOHB DyHPO TaskSet@100% Figure 4: Critical difference diagram for LCBench and TaskSet in terms of the number of HPO steps. The results correspond to results after 500 and 1,000 epochs. Connected ranks via a bold bar indicate that performances are not significantly different (p >0.05). In Figure 4, we provide further evidence thatDYHPO’s improvement over the baselines is statistically significant. The critical difference diagram presents the ranks of all methods and provides information on the pairwise statistical difference between all methods for two fractions of the number of HPO steps (50% and 100%). We included the LCBench and TaskSet benchmarks in our significance plots. NAS-Bench-201 was omitted because it has only 3 datasets and the statistical test cannot be applied. Horizontal lines indicate groupings of methods that are not significantly different. As suggested by the best published practices Demsar [2006], we use the Friedman test to reject the null hypothesis followed by a pairwise post-hoc analysis based on the Wilcoxon signed-rank test (α = 0.05). For LCBench, DYHPO already outperforms the baselines significantly after 50% of the search budget, with a statistically significant margin. As the optimization procedure continues, DYHPO manages to extend its gain in performance and is the only method that has a statistically significant improvement against all the other competitor methods. Similarly, for TaskSet, DYHPO manages to outperform all methods with a statistically significant margin only halfway through the optimization procedure and achieves the best rank over all methods. However, as the optimization procedure continues, BOHB manages to decrease the performance gap with DYHPO , although, it still achieves a worse rank across all datasets. Considering the empirical results, we conclude that Hypothesis 1 is validated and that DYHPO achieves state-of-the-art results on multi-fidelity HPO. Experiment 2: On the impact of DYHPO ’s overhead on the results. We present the results of our second experiment in Figure 5 (left), where, as it can be seen, DYHPO still outperforms the other methods when its overhead is considered. For LCBench, DYHPO manages to get an advantage 70.2 0.4 0.6 0.8 1.0 Normalized Wallclock Time 10 1 Mean Regret LCBench 103 105 Wallclock Time in Seconds 10 1 ImageNet16-120 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 12345678 Random Dragonfly ASHA Hyperband MF-DNN BOHB DEHB DyHPO LCBench@50% 12345678 Random Dragonfly ASHA Hyperband MF-DNN BOHB DEHB DyHPO LCBench@100% Figure 5: Left: The regret over time for all methods during the optimization procedure for the LCBench benchmark and the ImageNet dataset from the NAS-Bench-201 benchmark. The normalized wall clock time represents the actual run time divided by the total wall clock time ofDYHPO including the overhead of fitting the deep GP. Right: The critical difference diagram for LCBench halfway through the HPO wall-clock time, and in the end. Connected ranks via a bold bar indicate that differences are not significant (p >0.05). fairly quickly and it only increases the gap in performance with the other methods as the optimization process progresses. Similarly, in the case of ImageNet from NAS-Bench-201, DYHPO manages to gain an advantage earlier than other methods during the optimization procedure. Although in the end DYHPO still performs better than all the other methods, we believe most of the methods converge to a good solution and the differences in the final performance are negligible. For the extended results, related to the performance of all methods on a dataset level over time, we refer the reader to the plots in Appendix B. Additionally, in Figure 5 (right), we provide the critical difference diagrams for LCBench that present the ranks and the statistical difference of all methods halfway through the optimization procedure, and in the end. As it can be seen, DYHPO has a better rank with a significant margin with only half of the budget used and it retains the advantage until the end. Experiment 3: On the efficiency of DYHPO . In Figure 6 (left), we plot the precision of every method for different budgets during the optimization procedure, which demonstrates that DYHPO effectively explores the search space and identifies promising candidates. The precision at an epoch i is defined as the number of top 1% candidates that are trained, divided by the number of all candidates trained, both trained for at least i epochs. The higher the precision, the more relevant candidates were considered and the less computational resources were wasted. For small budgets, the precision is low since DYHPO spends budget to consider various candidates, but then, promising candidates are successfully identified and the precision quickly increases. This argument is further supported in Figure 6 (middle), where we visualize the average regret of all the candidates trained for at least the specified number of epochs on the x-axis. In contrast to the regret plots, here we do not show the regret of the best configuration, but the mean regret of all the selected configurations. The analysis deduces a similar finding, our method DYHPO selects more qualitative hyperparameter configurations than all the baselines. An interesting property of multi-fidelity HPO is the phenomenon of poor rank correlations among the validation performance of candidates at different budgets. In other words, a configuration that achieves a poor performance at a small budget can perform better at a larger budget. To analyze this phenomenon, we measure the percentage of \"good\" configurations at a particular budget, that were \"bad\" performers in at least one of the smaller budgets. We define a \"good\" performance at a budget B when a configuration achieves a validation accuracy ranked among the top 1/3 of the validation accuracies belonging to all the other configurations that were run until that budget B. In Figure 6 (right), we analyze the percentage of \"good\" configurations at each budget denoted by the x-axis, that were \"bad\" performers in at least one of the lower budgets. Such a metric is a proxy for the degree of the promotion of \"bad\" configurations towards higher budgets. We present the analysis for all the competing methods of our experimental protocol from Section 4. We have additionally included the ground-truth line annotated as \"Baseline\", which represents the fraction of past poor performers among all the feasible configurations in the search space. In contrast, the respective methods compute the fraction of promotions only among the configurations that those methods have considered (i.e. selected within their HPO trials) until the budget indicated by the x-axis. We see that there is a high degree of \"good\" configurations that were \"bad\" at a previous budget, with fractions of the ground-truth \"Baseline\" going up to 40% for the LCBench benchmark and up to 80% for the NAS-Bench-201 benchmark. 80 10 20 30 40 50 Number of Epochs 0.0 0.1 0.2 0.3 0.4Precision of Top Candidates LCBench Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 10 20 30 40 50 Number of Epochs 0.05 0.10 0.15Average Regret LCBench Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 10 20 30 40 50 Number of Epochs 0.0 0.2 0.4 0.6 0.8 Fraction of Poor Performer Promotions LCBench Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO Baseline 0 50 100 150 200 Number of Epochs 0.05 0.10 0.15Precision of Top Candidates NAS-Bench-201 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 50 100 150 200 Number of Epochs 0.04 0.06 0.08 0.10 0.12Average Regret NAS-Bench-201 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 50 100 150 200 Number of Epochs 0.0 0.2 0.4 0.6 0.8 1.0 Fraction of Poor Performer Promotions NAS-Bench-201 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO Baseline Figure 6: The efficiency of DYHPO as the optimization progresses. Left: The fraction of top- performing candidates from all candidates that were selected to be trained. Middle: The average regret for the configurations that were selected to be trained at a given budget. Right: The percentage of configurations that belong to the top 1/3 configurations at a given budget and that were in the top bottom 2/3 of the configurations at a previous budget. All of the results are from the LCBench and NAS-Bench-201 benchmark. On the other hand, the analysis demonstrates that our method DYHPO has promoted more \"good\" configurations that were \"bad\" in a lower budget, compared to all the rival methods. In particular, more than 80% of selected configurations from the datasets belonging to either benchmark were \"bad\" performers at a lower budget. The empirical evidence validates Hypothesis 3 and demonstrates that DYHPO efficiently explores qualitative candidates. We provide the results of our analysis for DYHPO’s efficiency on the additional benchmarks (Taskset) in Appendix B. Ablating the impact of the learning curve 102 103 104 105 Training Time in Seconds 10 1 Regret ImageNet16-120 DyHPO DyHPO w/o CNN Random Search Figure 7: Ablating the impact of the learning curve on DYHPO. One of the main differences between DYHPO and similar methods Kandasamy et al. [2017], is that the learning curve is an input to the kernel function. For this reason, we investigate the impact of this design choice. We consider a variation of DYHPO w/o CNN, which is simply DYHPO without the learning curve. It is worth emphasizing that both variants (with and without the learning curve) are multi-fidelity surrogates and both receive the budget informa- tion through the inputted index j in Equation 3. The only difference is that DYHPO additionally incorporates the pattern of the learning curve. We run the ablation on the NAS-Bench-201 benchmark and report the results for ImageNet, the largest dataset in our collection. The ablation results are shown in Figure 7, while the remaining results on the other datasets are shown in Figure 8 of the appendix. Based on the results from our learning curve ablation, we conclude that the use of an explicit learning curve representation leads to significantly better results. 6 Limitations of Our Method Although DYHPO shows a convincing and statistically significant reduction of the HPO time on diverse Deep Learning (DL) experiments, we cautiously characterized our method only as a \"step 9towards\" scaling HPO for DL. The reason for our restrain is the lack of tabular benchmarks for HPO on very large deep learning models, such as Transformers-based architectures [Devlin et al., 2019]. Additionally, the pause and resume part of our training procedure can only be applied when tuning the hyperparameters of parametric models, otherwise, the training of a hyperparameter configuration would have to be restarted. Lastly, for small datasets that can be trained fast, the overhead of model-based techniques would make an approach like random search more appealing. 7 Conclusions In this work, we present DYHPO , a new Bayesian optimization (BO) algorithm for the gray-box setting. We introduced a new surrogate model for BO that uses a learnable deep kernel and takes the learning curve as an explicit input. Furthermore, we motivated a variation of expected improvement for the multi-fidelity setting. Finally, we compared our approach on diverse benchmarks on a total of 50 different tasks against the current state-of-the-art methods on gray-box hyperparameter optimization (HPO). Our method shows significant gains and has the potential to become the de facto standard for HPO in Deep Learning. Acknowledgments Josif Grabocka and Arlind Kadra would like to acknowledge the grant awarded by the Eva-Mayr-Stihl Stiftung. In addition, this research was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under grant number 417962828 and grant INST 39/963-1 FUGG (bwForCluster NEMO). In addition, Josif Grabocka acknowledges the support of the BrainLinks- BrainTools center of excellence. References Noor H. Awad, Neeratyoy Mallik, and Frank Hutter. DEHB: evolutionary hyberband for scalable, robust and efficient hyperparameter optimization. In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI 2021, Virtual Event / Montreal, Canada, 19-27 August 2021, pages 2147–2153, 2021. doi: 10.24963/ijcai.2021/296. URL https://doi.org/ 10.24963/ijcai.2021/296. Bowen Baker, Otkrist Gupta, Ramesh Raskar, and Nikhil Naik. Accelerating neural architecture search using performance prediction. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Workshop Track Proceedings, 2018. URL https://openreview.net/forum?id=HJqk3N1vG. James Bergstra, Rémi Bardenet, Yoshua Bengio, and Balázs Kégl. Algorithms for hyper-parameter optimization. In Advances in Neural Information Processing Systems 24: 25th Annual Conference on Neural Information Processing Systems 2011. Proceedings of a meeting held 12-14 December 2011, Granada, Spain, pages 2546–2554, 2011. URL https://proceedings.neurips.cc/ paper/2011/hash/86e8f7ab32cfd12577bc2619bc635690-Abstract.html. Hadrien Bertrand, Roberto Ardon, Matthieu Perrot, and Isabelle Bloch. Hyperparameter optimization of deep neural networks: Combining hyperband with bayesian model selection. In Conférence sur l’Apprentissage Automatique, 2017. Yutian Chen, Matthew W. Hoffman, Sergio Gomez Colmenarejo, Misha Denil, Timothy P. Lillicrap, Matthew Botvinick, and Nando de Freitas. Learning to learn without gradient descent by gradient descent. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pages 748–756, 2017. URL http://proceedings. mlr.press/v70/chen17e.html. Janez Demsar. Statistical comparisons of classifiers over multiple data sets. J. Mach. Learn. Res., 7: 1–30, 2006. URL http://jmlr.org/papers/v7/demsar06a.html. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of 10the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 4171–4186, 2019. doi: 10.18653/v1/n19-1423. URL https://doi.org/ 10.18653/v1/n19-1423. Tobias Domhan, Jost Tobias Springenberg, and Frank Hutter. Speeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves. In Qiang Yang and Michael J. Wooldridge, editors, Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence, IJCAI 2015, Buenos Aires, Argentina, July 25-31, 2015, pages 3460–3468. AAAI Press, 2015. URL http://ijcai.org/Abstract/15/487. Xuanyi Dong and Yi Yang. Nas-bench-201: Extending the scope of reproducible neural architecture search. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020, 2020. URL https://openreview.net/forum?id=HJxyZkBKDr. Stefan Falkner, Aaron Klein, and Frank Hutter. BOHB: robust and efficient hyperparameter optimiza- tion at scale. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, pages 1436–1445, 2018. URL http://proceedings.mlr.press/v80/falkner18a.html. Luca Franceschi, Michele Donini, Paolo Frasconi, and Massimiliano Pontil. Forward and reverse gradient-based hyperparameter optimization. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pages 1165–1173, 2017. URL http://proceedings.mlr.press/v70/franceschi17a.html. Jacob R. Gardner, Geoff Pleiss, Kilian Q. Weinberger, David Bindel, and Andrew Gordon Wil- son. Gpytorch: Blackbox matrix-matrix gaussian process inference with GPU acceleration. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural In- formation Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada , pages 7587–7597, 2018. URL https://proceedings.neurips.cc/paper/2018/hash/ 27e8e17134dd7083b050476733207ea1-Abstract.html. Matilde Gargiani, Aaron Klein, Stefan Falkner, and Frank Hutter. Probabilistic rollouts for learning curve extrapolation across hyperparameter settings. CoRR, abs/1910.04522, 2019. URL http: //arxiv.org/abs/1910.04522. Kevin G. Jamieson and Ameet Talwalkar. Non-stochastic best arm identification and hyperparameter optimization. In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, AISTATS 2016, Cadiz, Spain, May 9-11, 2016 , pages 240–248, 2016. URL http://proceedings.mlr.press/v51/jamieson16.html. Donald R. Jones, Matthias Schonlau, and William J. Welch. Efficient global optimization of ex- pensive black-box functions. J. Global Optimization , 13(4):455–492, 1998. doi: 10.1023/A: 1008306431147. URL https://doi.org/10.1023/A:1008306431147. Kirthevasan Kandasamy, Gautam Dasarathy, Junier B. Oliva, Jeff G. Schneider, and Barn- abás Póczos. Gaussian process bandit optimisation with multi-fidelity evaluations. In Advances in Neural Information Processing Systems 29: Annual Conference on Neu- ral Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain , pages 992–1000, 2016. URL https://proceedings.neurips.cc/paper/2016/hash/ 605ff764c617d3cd28dbbdd72be8f9a2-Abstract.html. Kirthevasan Kandasamy, Gautam Dasarathy, Jeff G. Schneider, and Barnabás Póczos. Multi-fidelity bayesian optimisation with continuous approximations. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pages 1799–1808, 2017. URL http://proceedings.mlr.press/v70/kandasamy17a.html. Kirthevasan Kandasamy, Willie Neiswanger, Jeff Schneider, Barnabás Póczos, and Eric P. Xing. Neural architecture search with bayesian optimisation and optimal transport. In Ad- vances in Neural Information Processing Systems 31: Annual Conference on Neural Infor- mation Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada , pages 2020–2029, 2018. URL https://proceedings.neurips.cc/paper/2018/hash/ f33ba15effa5c10e873bf3842afb46a6-Abstract.html. 11Kirthevasan Kandasamy, Karun Raju Vysyaraju, Willie Neiswanger, Biswajit Paria, Christopher R. Collins, Jeff Schneider, Barnabás Póczos, and Eric P. Xing. Tuning hyperparameters without grad students: Scalable and robust bayesian optimisation with dragonfly. J. Mach. Learn. Res., 21: 81:1–81:27, 2020. URL http://jmlr.org/papers/v21/18-223.html. Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1412.6980. Aaron Klein, Stefan Falkner, Simon Bartels, Philipp Hennig, and Frank Hutter. Fast bayesian optimization of machine learning hyperparameters on large datasets. In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, AISTATS 2017, 20-22 April 2017, Fort Lauderdale, FL, USA, pages 528–536, 2017a. URL http://proceedings.mlr.press/ v54/klein17a.html. Aaron Klein, Stefan Falkner, Jost Tobias Springenberg, and Frank Hutter. Learning curve prediction with bayesian neural networks. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings , 2017b. URL https://openreview.net/forum?id=S11KBYclx. Liam Li, Kevin G. Jamieson, Afshin Rostamizadeh, Ekaterina Gonina, Jonathan Ben-tzur, Moritz Hardt, Benjamin Recht, and Ameet Talwalkar. A system for massively parallel hyperparameter tuning. In Inderjit S. Dhillon, Dimitris S. Papailiopoulos, and Vivienne Sze, editors,Proceedings of Machine Learning and Systems 2020, MLSys 2020, Austin, TX, USA, March 2-4, 2020. mlsys.org, 2020a. URL https://proceedings.mlsys.org/book/303.pdf. Lisha Li, Kevin G. Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. Hyper- band: A novel bandit-based approach to hyperparameter optimization. J. Mach. Learn. Res., 18: 185:1–185:52, 2017. URL http://jmlr.org/papers/v18/16-558.html. Shibo Li, Wei Xing, Robert M. Kirby, and Shandian Zhe. Multi-fidelity bayesian optimization via deep neural networks. In Advances in Neural Information Processing Systems 33: An- nual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual , 2020b. URL https://proceedings.neurips.cc/paper/2020/hash/ 60e1deb043af37db5ea4ce9ae8d2c9ea-Abstract.html. Jonathan Lorraine, Paul Vicol, and David Duvenaud. Optimizing millions of hyperparameters by implicit differentiation. In The 23rd International Conference on Artificial Intelligence and Statistics, AISTATS 2020, 26-28 August 2020, Online [Palermo, Sicily, Italy], pages 1540–1552, 2020. URL http://proceedings.mlr.press/v108/lorraine20a.html. Dougal Maclaurin, David Duvenaud, and Ryan P. Adams. Gradient-based hyperparameter opti- mization through reversible learning. In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015 , pages 2113–2122, 2015. URL http://proceedings.mlr.press/v37/maclaurin15.html. Pedro Mendes, Maria Casimiro, Paolo Romano, and David Garlan. Trimtuner: Efficient optimization of machine learning jobs in the cloud via sub-sampling. In 28th International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems, MASCOTS 2020, Nice, France, November 17-19, 2020, pages 1–8. IEEE, 2020. doi: 10.1109/MASCOTS50786. 2020.9285971. URL https://doi.org/10.1109/MASCOTS50786.2020.9285971. Pedro Mendes, Maria Casimiro, and Paolo Romano. Hyperjump: Accelerating hyperband via risk modelling. CoRR, abs/2108.02479, 2021. URL https://arxiv.org/abs/2108.02479. Luke Metz, Niru Maheswaranathan, Ruoxi Sun, C. Daniel Freeman, Ben Poole, and Jascha Sohl- Dickstein. Using a thousand optimization tasks to learn hyperparameter search strategies. CoRR, abs/2002.11887, 2020. URL https://arxiv.org/abs/2002.11887. Jack Parker-Holder, Vu Nguyen, and Stephen J. Roberts. Provably efficient online hyperparameter optimization with population-based bandits. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/ hash/c7af0926b294e47e52e46cfebe173f20-Abstract.html. 12Valerio Perrone, Rodolphe Jenatton, Matthias W. Seeger, and Cédric Archambeau. Scalable hyper- parameter transfer learning. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada, pages 6846–6856, 2018. URL https://proceedings.neurips.cc/ paper/2018/hash/14c879f3f5d8ed93a09f6090d77c2cc3-Abstract.html. Matthias Poloczek, Jialei Wang, and Peter I. Frazier. Multi-information source optimiza- tion. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA , pages 4288–4298, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/ df1f1d20ee86704251795841e6a9405a-Abstract.html. Akshara Rai, Ruta Desai, and Siddharth Goyal. Bayesian optimization with a neural network kernel, 2016. URL http://www.cs.cmu.edu/~rutad/files/BO_NN.pdf. Jasper Snoek, Hugo Larochelle, and Ryan P. Adams. Practical bayesian optimization of machine learn- ing algorithms. In Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United States, pages 2960–2968, 2012. URL https://proceedings. neurips.cc/paper/2012/hash/05311655a15b75fab86956663e1819cd-Abstract.html. Niranjan Srinivas, Andreas Krause, Sham M. Kakade, and Matthias W. Seeger. Gaussian process optimization in the bandit setting: No regret and experimental design. In Proceedings of the 27th International Conference on Machine Learning (ICML-10), June 21-24, 2010, Haifa, Israel, pages 1015–1022, 2010. URL https://icml.cc/Conferences/2010/papers/422.pdf. Kevin Swersky, Jasper Snoek, and Ryan Prescott Adams. Multi-task bayesian optimization. In Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States, pages 2004–2012, 2013. URL https://proceedings.neurips. cc/paper/2013/hash/f33ba15effa5c10e873bf3842afb46a6-Abstract.html. Kevin Swersky, Jasper Snoek, and Ryan Prescott Adams. Freeze-thaw bayesian optimization. CoRR, abs/1406.3896, 2014. URL http://arxiv.org/abs/1406.3896. Shion Takeno, Hitoshi Fukuoka, Yuhki Tsukada, Toshiyuki Koyama, Motoki Shiga, Ichiro Takeuchi, and Masayuki Karasuyama. Multi-fidelity bayesian optimization with max-value entropy search and its parallelization. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, pages 9334–9345, 2020. URLhttp://proceedings. mlr.press/v119/takeno20a.html. Jiazhuo Wang, Jason Xu, and Xuejun Wang. Combination of hyperband and bayesian optimization for hyperparameter optimization in deep learning. CoRR, abs/1801.01596, 2018. URL http: //arxiv.org/abs/1801.01596. Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, and Eric P. Xing. Deep kernel learning. In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, AISTATS 2016, Cadiz, Spain, May 9-11, 2016, pages 370–378, 2016. URL http://proceedings.mlr. press/v51/wilson16.html. Martin Wistuba. Bayesian optimization combined with incremental evaluation for neural network architecture optimization. In AutoML@PKDD/ECML, 2017. Martin Wistuba and Josif Grabocka. Few-shot bayesian optimization with deep kernel surrogates. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021, 2021. URL https://openreview.net/forum?id=bJxgv5C3sYc. Martin Wistuba and Tejaswini Pedapati. Learning to rank learning curves. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 10303–10312. PMLR, 2020. URL http://proceedings.mlr.press/v119/wistuba20a.html. Lucas Zimmer, Marius Lindauer, and Frank Hutter. Auto-pytorch: Multi-fidelity metalearning for efficient and robust autodl. IEEE Trans. Pattern Anal. Mach. Intell., 43(9):3079–3090, 2021. doi: 10.1109/TPAMI.2021.3067763. URL https://doi.org/10.1109/TPAMI.2021.3067763. 13Checklist 1. For all authors... (a) Do the main claims made in the abstract and introduction accurately reflect the paper’s contributions and scope? [Yes] (b) Did you describe the limitations of your work? [Yes] See Section 6. (c) Did you discuss any potential negative societal impacts of your work? [Yes] See Section “Societal Implications” in the Appendix. (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] 2. If you are including theoretical results... (a) Did you state the full set of assumptions of all theoretical results? [N/A] (b) Did you include complete proofs of all theoretical results? [N/A] 3. If you ran experiments... (a) Did you include the code, data, and instructions needed to reproduce the main experi- mental results (either in the supplemental material or as a URL)? [No] We provide our main algorithm in Section 3 and we additionally provide the detailed implementation details in Appendix A for all methods and benchmarks. We will release the code for the camera-ready version of our work. (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] Please see Appendix A. (c) Did you report error bars (e.g., with respect to the random seed after running experi- ments multiple times)? [Yes] We report the statistical significance of the performance difference between our method and the baselines in Section 5 (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See Section 4.1. 4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... (a) If your work uses existing assets, did you cite the creators? [Yes] See Section 4.2 and Section 4.3. (b) Did you mention the license of the assets? [Yes] See Appendix A.1 and A.5 where we provide references to the assets where the license is included. (c) Did you include any new assets either in the supplemental material or as a URL? [No] (d) Did you discuss whether and how consent was obtained from people whose data you’re using/curating? [N/A] The benchmarks and baselines are open-sourced. (e) Did you discuss whether the data you are using/curating contains personally identi- fiable information or offensive content? [N/A] The data does not contain personally identifiable information or offensive content. 5. If you used crowdsourcing or conducted research with human subjects... (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A] 14Societal Implications In our work, we use only publicly available data with no privacy concerns. Furthermore, our algorithm reduces the overall time for fitting deep networks, therefore, saving computational resources and yielding a positive impact on the environment. Moreover, our method can help smaller research organizations with limited access to resources to be competitive in the deep learning domain, which reduces the investment costs on hardware. Although our method significantly reduces the time taken for optimizing a machine learning algorithm that achieves peak performance, we warn against running our method for an extended time only to achieve marginal gains in performance, unless it is mission-critical. Last but not least, in order to save energy, we invite the community to create sparse benchmarks with surrogates, instead of dense tabular ones. A Experimental Setup A.1 Benchmarks LCBench. LCBench4 is a feedforward neural network benchmark on tabular data which consists of 2000 configuration settings for each of the 35 datasets. The configurations were evaluated during HPO runs with AutoPyTorch. LCBench features a search space of 7 numerical hyperparameters, where every hyperparameter configuration is trained for 50 epochs. The objective is to optimize seven different hyperparameters of funnel-shaped neural networks, i.e., batch size, learning rate, momentum, weight decay, dropout, number of layers, and maximum number of units per layer. TaskSet. TaskSet5 is a benchmark that features over 1162 diverse tasks from different domains and includes 5 search spaces. In this work, we focus on NLP tasks and we use the Adam8p search space with 8 continuous hyperparameters. We refer to Figure 11 for the exact task names considered in our experiments. The learning curves provided in TaskSet report scores after every 200 iterations. We refer to those as \"steps\". The objective is to optimize eight hyperparameters for a set of different recurrent neural networks (RNN) that differ in embedding size, RNN cell, and other architectural features. The set of hyperparameters consists of optimizer-specific hyperparameters, such as the learning rate, the exponential decay rate of the first and second momentum of Adam, β1 and β2, and Adam’s constant for numerical stability ε. Furthermore, there are two hyperparameters controlling linear and exponential learning rate decays, as well as L1 and L2 regularization terms. NAS-Bench-201. NAS-Bench-2016 is a benchmark that has precomputed about 15,600 archi- tectures trained for 200 epochs for the image classification datasets CIFAR-10, CIFAR-100, and ImageNet. The objective is to select for each of the six operations within the cell of the macro architecture one of five different operations. All other hyperparameters such as learning rate and batch size are kept fixed. NAS-Bench-201 features a search space of 6 categorical hyperparameters and each architecture is trained for 200 epochs. A.2 Preprocessing In the following, we describe the preprocessing applied to the hyperparameter representation. For LCBench, we apply a log-transform to batch size, learning rate, and weight decay. For TaskSet, we apply it on the learning rate, L1 and L2 regularization terms, epsilon, linear and exponential decay of the learning rate. All continuous hyperparameters are scaled to the range between 0 and 1 using sklearn’s MinMaxScaler. If not mentioned otherwise, we use one-hot encoding for the categorical hyperparameters. As detailed in subsection A.5, some baselines have a specific way of dealing with them. In that case, we use the method recommended by the authors. A.3 Framework The framework contains the evaluated hyperparameters and their corresponding validation curves. The list of candidate hyperparameters is passed to the baseline-specific interface, which in turn, 4https://github.com/automl/LCBench 5https://github.com/google-research/google-research/tree/master/task_set 6https://github.com/D-X-Y/NAS-Bench-201 15optimizes and queries the framework for the hyperparameter configuration that maximizes utility. Our framework in turn responds with the validation curve and the cost of the evaluation. In case a hyperparameter configuration has been evaluated previously up to a budget b and a baseline requires the response for budget b + 1, the cost is calculated accordingly only for the extra budget requested. A.4 Implementation Details We implement the Deep Kernel Gaussian Process using GPyTorch 1.5 [Gardner et al., 2018]. We use an RBF kernel and the dense layers of the transformation functionφ have 128 and 256 units. We used a convolutional layer with a kernel size of three and four filters. All parameters of the Deep Kernel are estimated by maximizing the marginal likelihood. We achieve this by using gradient ascent and Adam [Kingma and Ba, 2015] with a learning rate of 0.1 and batch size of 64. We stop training as soon as the training likelihood is not improving for 10 epochs in a row or we completed 1,000 epochs. For every new data point, we start training the GP with its old parameters to reduce the required effort for training. A.5 Baselines Random Search & Hyperband. Random search and Hyperband sample hyperparameter config- urations at random and therefore the preprocessing is irrelevant. We have implemented both from scratch and use the recommended hyperparameters for Hyperband, i.e. η = 3. BOHB. For our experiments with BOHB, we use version 0.7.4 of the officially-released code7. DEHB. For our experiments with DEHB, we use the official public implementation8. We devel- oped an interface that communicates between our framework and DEHB. In addition to the initial preprocessing common for all methods, we encode categorical hyperparameters with a numerical value in the interval [0, 1]. For a categorical hyperparameter xi, we take Ni equal-sized intervals, where Ni represents the number of unique categorical values for hyperparameter xi and we assign the value for a categorical value n ∈ Ni to the middle of the interval [n, n+ 1]as suggested by the authors. For configuring the DEHB algorithm we used the default values from the library. Dragonfly. We use the publicly available code of Dragonfly9. No special treatment of categorical hyperparameters is required since Dragonfly has its own way to deal with them. We use version 0.1.6 with default settings. MF-DNN. We use the official implementation of MF-DNN by the authors10. Initially, we tried to use multiple incremental fidelity levels like for DYHPO, however, the method runtime was too high and it could not achieve competitive results. For that reason, we use only a few fidelity levels like the authors do in their work Li et al. [2020b]. We use the same fidelity levels as for Hyperband, DEHB, and BOHB to have a fair comparison between the baselines. We also use the same number of initial points as for the other methods to have the same maximal resource allocated for every fidelity level. ASHA-HB. We use the public implementation from the well-known optuna library (version2.10.0). We used the same eta, minimum and maximal budget as for HB, DEHB, and BOHB in our experi- ments, to have a fair comparison. B Additional Plots In Figure 8, we ablate the learning curve input in our kernel, to see the effect it has on performance for the CIFAR-10 and CIFAR-100 datasets from the NAS-Bench-201 benchmark. The results indicate that the learning curve plays an important role in achieving better results by allowing faster convergence and a better anytime performance. 7https://github.com/automl/HpBandSter 8https://github.com/automl/DEHB/ 9https://github.com/dragonfly/dragonfly 10https://github.com/shib0li/DNN-MFBO 16102 103 104 105 Training Time in Seconds 10 2 10 1 Regret cifar10 DyHPO DyHPO w/o CNN Random Search 102 103 104 105 Training Time in Seconds 10 2 10 1 Regret cifar100 DyHPO DyHPO w/o CNN Random Search Figure 8: The learning curve ablation for the CIFAR-10 and CIFAR-100 tasks of NAS-Bench-201. 0 1000 2000 3000 4000 Number of Epochs 10 2 10 1 Regret cifar10 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 1000 2000 3000 4000 Number of Epochs 10 2 10 1 Regret cifar100 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO Figure 9: NAS-Bench-201 regret results over the number of epochs spent during the optimization. Additionally, in Figure 9, we show the performance comparison over the number of epochs of every method for the CIFAR-10 and CIFAR-100 datasets in the NAS-Bench-201 benchmark. While, in Figure 10, we present the performance comparison over time. As can be seen, DYHPO converges faster and has a better performance compared to the other methods over the majority of the time or steps, however, towards the end although it is the optimal method or close to the optimal method, the difference in regret is not significant anymore. Furthermore, Figure 11 shows the performance comparison for the datasets chosen from TaskSet over the number of steps. Looking at the results, DYHPO is outperforming all methods convincingly on the majority of datasets by converging faster and with significant differences in the regret evaluation metric. In Figure 12 and 13, we show the performance comparison for all the datasets from LCBench regarding regret over the number of epochs. Similarly, in Figure 14 and 15, we show the same performance comparison, however, over time. As can be seen, DYHPO manages to outperform the other competitors in the majority of the datasets, and in the datasets that it does not, it is always close to the top-performing method, and the difference between methods is marginal. In Figure 16 we provide the extended results of Experiment 3 for TaskSet. We show the precision, average regret, and promotion percentage for poor-performing configurations for DYHPO and the other competitor methods. Lastly, we explore the behavior of DYHPO after finding the configuration which is returned at the end of the optimization as the best configuration. In Figure 17, we show how the budget is distributed on the configurations considered during that part of the optimization. Clearly, DYHPO is spending very little budget on most configurations. Furthermore, we investigated how many new configurations are considered during this phase. For LCBench, 76.98% of considered configurations are new demonstrating that DYHPO is investigating most of the budget into exploration. These are even more extreme for TaskSet (93.16% and NAS-Bench-201 (97.51%). 17102 103 104 Wallclock Time in Seconds 10 2 10 1 Regret cifar10 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 102 103 104 Wallclock Time in Seconds 10 2 10 1 Regret cifar100 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO Figure 10: NAS-Bench-201 regret results over the total optimization time. The total time includes the method overhead time and the hyperparameter configuration evaluation time. 0 200 400 600 800 1000 Number of Steps 10 3 10 2 10 1 Regret FixedTextRNNClassification imdb_patch128_LSTM128_avg_bs64 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Steps 10 3 10 2 10 1 Regret FixedTextRNNClassification imdb_patch128_LSTM128_bs64 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Steps 10 1 Regret FixedTextRNNClassification imdb_patch128_LSTM128_embed128_bs64 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Steps 10 1 6 × 10 2 2 × 10 1 3 × 10 1 Regret FixedTextRNNClassification imdb_patch32_GRU128_bs128 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Steps 10 1 Regret FixedTextRNNClassification imdb_patch32_GRU64_avg_bs128 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Steps 10 1 Regret FixedTextRNNClassification imdb_patch32_IRNN64_relu_avg_bs128 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Steps 10 1 100 Regret FixedTextRNNClassification imdb_patch32_IRNN64_relu_last_bs128 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Steps 10 2 10 1 Regret FixedTextRNNClassification imdb_patch32_LSTM128_E128_bs128 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Steps 10 2 10 1 Regret FixedTextRNNClassification imdb_patch32_LSTM128_bs128 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Steps 10 1 2 × 10 1 Regret FixedTextRNNClassification imdb_patch32_VRNN128_tanh_bs128 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Steps 10 1 Regret FixedTextRNNClassification imdb_patch32_VRNN64_relu_avg_bs128 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Steps 10 3 10 2 10 1 Regret FixedTextRNNClassification imdb_patch32_VRNN64_tanh_avg_bs128 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO Figure 11: Performance comparison over the number of steps on a dataset level for TaskSet. 180 200 400 600 800 1000 Number of Epochs 10 2 Regret APSFailure Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 1 3 × 10 2 4 × 10 2 6 × 10 2 Regret Amazon_employee_access Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 3 10 2 10 1 Regret Australian Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 3 10 2 10 1 Regret Fashion-MNIST Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret KDDCup09_appetency Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 4 10 3 10 2 10 1 Regret MiniBooNE Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 3 10 2 10 1 Regret adult Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 3 10 2 Regret airlines Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 3 10 2 10 1 Regret albert Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret bank-marketing Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret blood-transfusion-service-center Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 1 Regret car Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret christine Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 100 Regret cnae-9 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret connect-4 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 4 10 3 10 2 10 1 Regret covertype Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 1 Regret credit-g Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 4 10 2 100 Regret dionis Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO Figure 12: Performance comparison over the number of steps on a dataset level for LCBench. 190 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret fabert Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 1 Regret helena Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret higgs Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret jannis Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret jasmine Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 4 10 3 10 2 10 1 Regret jungle_chess_2pcs_raw_endgame_complete Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 1 Regret kc1 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret kr-vs-kp Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret mfeat-factors Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 4 10 3 10 2 10 1 Regret nomao Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 Regret numerai28.6 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret phoneme Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 1 Regret segment Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 1 Regret shuttle Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 4 10 2 Regret sylvine Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 3 10 2 10 1 Regret vehicle Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret volkert Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO Figure 13: Performance comparison over the number of steps on a dataset level for LCBench (cont.). 200 250 500 750 1000 1250 Wallclock Time in Seconds 10 2 Regret APSFailure Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 500 1000 1500 Wallclock Time in Seconds 10 1 3 × 10 2 4 × 10 2 6 × 10 2 Regret Amazon_employee_access Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 100 200 300 Wallclock Time in Seconds 10 3 10 2 10 1 Regret Australian Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 1000 2000 3000 4000 5000 Wallclock Time in Seconds 10 2 10 1 Regret Fashion-MNIST Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 500 1000 1500 2000 2500 Wallclock Time in Seconds 10 2 10 1 Regret KDDCup09_appetency Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 500 1000 1500 Wallclock Time in Seconds 10 1 Regret MiniBooNE Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 500 1000 1500 2000 Wallclock Time in Seconds 10 3 10 2 10 1 Regret adult Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 2500 5000 7500 10000 12500 Wallclock Time in Seconds 10 3 10 2 Regret airlines Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 2000 4000 6000 8000 10000 Wallclock Time in Seconds 10 2 Regret albert Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 500 1000 1500 2000 Wallclock Time in Seconds 10 2 10 1 Regret bank-marketing Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Wallclock Time in Seconds 10 2 10 1 Regret blood-transfusion-service-center Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Wallclock Time in Seconds 10 1 Regret car Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 5000 10000 15000 Wallclock Time in Seconds 10 2 10 1 Regret christine Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 500 1000 1500 2000 2500 Wallclock Time in Seconds 10 2 10 1 100 Regret cnae-9 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 500 1000 1500 2000 2500 Wallclock Time in Seconds 10 2 10 1 Regret connect-4 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 1000 2000 3000 4000 5000 Wallclock Time in Seconds 10 1 Regret covertype Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Wallclock Time in Seconds 10 1 Regret credit-g Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 2000 4000 6000 8000 Wallclock Time in Seconds 10 1 Regret dionis Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO Figure 14: Performance comparison over time on a dataset level for LCBench with the overhead included. 210 1000 2000 3000 Wallclock Time in Seconds 10 2 10 1 Regret fabert Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Wallclock Time in Seconds 10 1 Regret helena Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Wallclock Time in Seconds 10 1 Regret higgs Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 1000 2000 3000 Wallclock Time in Seconds 10 2 10 1 Regret jannis Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 250 500 750 1000 1250 Wallclock Time in Seconds 10 2 10 1 Regret jasmine Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 500 1000 1500 2000 Wallclock Time in Seconds 10 2 10 1 Regret jungle_chess_2pcs_raw_endgame_complete Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Wallclock Time in Seconds 10 1 Regret kc1 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 250 500 750 1000 Wallclock Time in Seconds 10 2 10 1 Regret kr-vs-kp Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 250 500 750 1000 1250 Wallclock Time in Seconds 10 2 10 1 Regret mfeat-factors Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 500 1000 1500 2000 Wallclock Time in Seconds 10 2 10 1 Regret nomao Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Wallclock Time in Seconds 10 2 Regret numerai28.6 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 250 500 750 1000 1250 Wallclock Time in Seconds 10 2 10 1 Regret phoneme Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Wallclock Time in Seconds 10 1 Regret segment Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 500 1000 1500 2000 Wallclock Time in Seconds 10 1 Regret shuttle Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 250 500 750 1000 1250 Wallclock Time in Seconds 10 4 10 2 Regret sylvine Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Wallclock Time in Seconds 10 2 10 1 Regret vehicle Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 1000 2000 3000 Wallclock Time in Seconds 10 2 10 1 Regret volkert Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO Figure 15: Performance comparison over time on a dataset level for LCBench with the overhead included. (cont.). 220 10 20 30 40 50 Number of Epochs 0.0 0.1 0.2 0.3Precision of Top Candidates Taskset Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 10 20 30 40 50 Number of Epochs 0.10 0.15 0.20 0.25 0.30Average Regret Taskset Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 10 20 30 40 50 Number of Steps 0.0 0.2 0.4 0.6 0.8 Fraction of Poor Performer Promotions Taskset Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO Baseline Figure 16: The efficiency of DYHPO as the optimization progresses. Left: The fraction of top- performing candidates from all candidates that were selected to be trained. Middle: The average regret for the configurations that were selected to be trained at a given budget. Right: The percentage of configurations that belong to the top 1/3 configurations at a given budget and that were in the top bottom 2/3 of the configurations at a previous budget. All of the results are from the Taskset benchmark. 0 10 20 30 40 50 Epochs 0.0 0.2 0.4 0.6Relative Number of Candidates LCBench 0 50 100 150 200 Epochs 0.0 0.2 0.4 0.6 0.8Relative Number of Candidates NAS-Bench-201 0 10 20 30 40 50 Steps 0.0 0.2 0.4 0.6 0.8Relative Number of Candidates TaskSet Figure 17: These plots shed light on how DYHPO behaves after the configuration it finally returns as the best. The plots show how many epochs are spent per candidate. As we can see, for most candidates only a small budget was considered, indicating that DYHPO is mostly exploring at this point. 23",
      "meta_data": {
        "arxiv_id": "2202.09774v2",
        "authors": [
          "Martin Wistuba",
          "Arlind Kadra",
          "Josif Grabocka"
        ],
        "published_date": "2022-02-20T10:28:02Z",
        "pdf_url": "https://arxiv.org/pdf/2202.09774v2.pdf",
        "github_url": "https://github.com/releaunifreiburg/DyHPO"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces DyHPO, a Bayesian Optimization (BO) method designed to dynamically allocate hyperparameter optimization (HPO) budget by learning which configurations to train further in a multi-fidelity setting. It proposes a novel deep kernel for Gaussian Processes (GP) that embeds learning curve dynamics and an acquisition function incorporating multi-budget information. DyHPO significantly outperforms state-of-the-art HPO methods, especially gray-box baselines, across diverse deep learning architectures (MLP, CNN/NAS, RNN) and 50 datasets (tabular, image, NLP), making a step towards scaling HPO for Deep Learning. Key contributions include a novel Bayesian surrogate that predicts validation scores based on hyperparameter configuration, budget, and the learning curve, and a robust integration of this surrogate with Bayesian optimization.",
        "methodology": "DyHPO is a Bayesian Optimization approach based on Gaussian Processes (GP). Its core methodology involves a dynamic budget allocation strategy, contrasting with static methods. A central component is a deep kernel for the GP surrogate model, which captures the learning dynamics across different budgets. This kernel `K` takes as input the hyperparameter configuration `xi`, the past learning curve `Yi,j-1`, and the budget `j`. A neural network `φ` (composed of linear and 1D convolutional layers followed by global max pooling) extracts features from these inputs, which are then fed into a squared exponential kernel `k`. The parameters of both `k` and `φ` are learned by maximizing the marginal likelihood using gradient descent with Adam. The acquisition function is a multi-fidelity version of Expected Improvement (EIMF), which dynamically determines the next configuration and budget to evaluate. It selects the configuration `xi` to train for one additional budget step `b(xi)+1` by maximizing `EIMF (x, b(x) + 1)`, ensuring a slow, exploratory increase in budget investment.",
        "experimental_setup": "The experimental setup evaluates DyHPO across three diverse deep learning settings: hyperparameter optimization for tabular, text, and image classification. All experiments were conducted on an Amazon EC2 M5 Instance (m5.xlarge). Performance was measured using the mean of ten repetitions, focusing on two metrics: mean regret (absolute difference to the best possible score) and average rank across datasets. Statistical significance was assessed using the Friedman test followed by a Wilcoxon signed-rank test (α = 0.05). Three benchmarks were used: LCBench (35 tabular datasets, 2,000 neural networks per dataset, 50 epochs), TaskSet (12 NLP tasks with RNNs, Adam8p search space, scores every 200 iterations), and NAS-Bench-201 (15,625 architectures on CIFAR-10, CIFAR-100, ImageNet, 200 epochs). DyHPO was compared against seven baselines: Random Search, HyperBand, BOHB, DEHB, ASHA, MF-DNN, and Dragonfly (BOCA). The Deep Kernel Gaussian Process was implemented using GPyTorch 1.5 with an RBF kernel, dense layers of 128 and 256 units, and a convolutional layer with a kernel size of three and four filters. Training involved Adam optimizer with a learning rate of 0.1 and batch size of 64, with early stopping after 10 epochs without improvement or 1,000 total epochs.",
        "limitations": "The current work has several limitations. Firstly, there is a lack of suitable tabular benchmarks for evaluating HPO on very large deep learning models, such as Transformer-based architectures, which restricts the scope of current experiments. Secondly, the 'pause and resume' capability of the training procedure, crucial for multi-fidelity optimization, is only applicable to parametric models; for non-parametric models, training a configuration would require a complete restart. Lastly, for datasets where training is inherently fast, the computational overhead associated with model-based HPO techniques like DyHPO might outweigh their benefits, making simpler methods such as random search more practical and appealing.",
        "future_research_directions": "Future research directions include expanding the evaluation to HPO for very large deep learning models, such as Transformer-based architectures, which currently lack suitable tabular benchmarks. Further exploration is needed to adapt the 'pause and resume' training procedure to non-parametric models, where current multi-fidelity methods might require full restarts. For tasks with short training times, research could focus on reducing the overhead of model-based techniques to make them more competitive against simpler methods like random search. Additionally, the authors suggest that the community create sparse benchmarks with surrogates, rather than dense tabular ones, to save energy and computational resources in HPO research.",
        "experimental_code": "import copy\nimport json\nimport logging\nimport math\nimport os\nimport time\nfrom typing import Dict, List, Optional, Tuple\n\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nfrom scipy.stats import norm, t\nimport torch\n\nfrom surrogate_models.dyhpo import DyHPO\n\n\nclass DyHPOAlgorithm:\n\n    def __init__(\n        self,\n        hp_candidates: np.ndarray,\n        log_indicator: List,\n        seed: int = 11,\n        max_benchmark_epochs: int = 52,\n        fantasize_step: int = 1,\n        minimization: bool = True,\n        total_budget: int = 500,\n        device: str = None,\n        dataset_name: str = 'unknown',\n        output_path: str = '.',\n        surrogate_config: dict = None,\n        verbose: bool = True,\n    ):\n        \"\"\"\n        Args:\n            hp_candidates: np.ndarray\n                The full list of hyperparameter candidates for\n                a given dataset.\n            log_indicator: List\n                A list with boolean values indicating if a\n                hyperparameter has been log sampled or not.\n            seed: int\n                The seed that will be used for the surrogate.\n            max_benchmark_epochs: int\n                The maximal budget that a hyperparameter configuration\n                has been evaluated in the benchmark for.\n            fantasize_step: int\n                The number of steps for which we are looking ahead to\n                evaluate the performance of a hpc.\n            minimization: bool\n                If the objective should be maximized or minimized.\n            total_budget: int\n                The total budget given for hyperparameter optimization.\n            device: str\n                The device where the experiment will be run on.\n            dataset_name: str\n                The name of the dataset that the experiment will be run on.\n            output_path: str\n                The path where all the output will be stored.\n            surrogate_config: dict\n                The model configurations for the surrogate.\n            verbose: boolean\n                If detailed information is preferred in the log file.\n        \"\"\"\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        torch.manual_seed(seed)\n        np.random.seed(seed)\n\n        if device is None:\n            self.dev = torch.device(\n                'cuda') if torch.cuda.is_available() else torch.device('cpu')\n        else:\n            self.dev = torch.device(device)\n\n        self.hp_candidates = hp_candidates\n        self.log_indicator = log_indicator\n\n        self.scaler = MinMaxScaler()\n        self.hp_candidates = self.preprocess_hp_candidates()\n\n        self.minimization = minimization\n        self.seed = seed\n\n        if verbose:\n            logging_level = logging.DEBUG\n        else:\n            logging_level = logging.INFO\n        self.logger = logging.getLogger()\n\n        logging.basicConfig(\n            format='%(levelname)s:%(asctime)s:%(message)s',\n            filename=f'dyhpo_surrogate_{dataset_name}_{seed}.log',\n            level=logging_level,\n        )\n\n        # the keys will be hyperparameter indices while the value\n        # will be a list with all the budgets evaluated for examples\n        # and with all performances for the performances\n        self.examples = dict()\n        self.performances = dict()\n\n        # set a seed already, so that it is deterministic when\n        # generating the seeds of the ensemble\n        torch.manual_seed(seed)\n        np.random.seed(seed)\n\n        self.max_benchmark_epochs = max_benchmark_epochs\n        self.total_budget = total_budget\n        self.fantasize_step = fantasize_step\n        self.nr_features = self.hp_candidates.shape[1]\n\n        initial_configurations_nr = 1\n        conf_individual_budget = 1\n        self.init_conf_indices = np.random.choice(self.hp_candidates.shape[0], initial_configurations_nr, replace=False)\n        self.init_budgets = [conf_individual_budget] * initial_configurations_nr\n        # with what percentage configurations will be taken randomly instead of being sampled from the model\n        self.fraction_random_configs = 0.1\n\n        self.model = None\n        # An index keeping track of where we are in the init_conf_indices\n        # list of hyperparmeters that are not sampled from the model.\n        self.initial_random_index = 0\n\n        if surrogate_config is None:\n            self.surrogate_config = {\n                'nr_layers': 2,\n                'nr_initial_features': self.nr_features,\n                'layer1_units': 64,\n                'layer2_units': 128,\n                'cnn_nr_channels': 4,\n                'cnn_kernel_size': 3,\n                'batch_size': 64,\n                'nr_epochs': 1000,\n                'nr_patience_epochs': 10,\n                'learning_rate': 0.001,\n            }\n        else:\n            self.surrogate_config = surrogate_config\n\n        # the incumbent value observed during the hpo process.\n        self.best_value_observed = np.NINF\n        # a set which will keep track of the hyperparameter configurations that diverge.\n        self.diverged_configs = set()\n\n        # info dict to drop every surrogate iteration\n        self.info_dict = dict()\n\n        # the start time for the overhead of every surrogate optimization iteration\n        # will be recorded here\n        self.suggest_time_duration = 0\n        # the total budget consumed so far\n        self.budget_spent = 0\n\n        self.output_path = output_path\n        self.dataset_name = dataset_name\n\n        self.no_improvement_threshold = int(self.max_benchmark_epochs + 0.2 * self.max_benchmark_epochs)\n        self.no_improvement_patience = 0\n\n\n    def _prepare_dataset_and_budgets(self) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Prepare the data that will be the input to the surrogate.\n\n        Returns:\n            data: A Dictionary that contains inside the training examples,\n            the budgets, the curves and lastly the labels.\n        \"\"\"\n\n        train_examples, train_labels, train_budgets, train_curves = self.history_configurations()\n\n        train_examples = np.array(train_examples, dtype=np.single)\n        train_labels = np.array(train_labels, dtype=np.single)\n        train_budgets = np.array(train_budgets, dtype=np.single)\n        train_curves = self.patch_curves_to_same_length(train_curves)\n        train_curves = np.array(train_curves, dtype=np.single)\n\n        # scale budgets to [0, 1]\n        train_budgets = train_budgets / self.max_benchmark_epochs\n\n        train_examples = torch.tensor(train_examples)\n        train_labels = torch.tensor(train_labels)\n        train_budgets = torch.tensor(train_budgets)\n        train_curves = torch.tensor(train_curves)\n\n        train_examples = train_examples.to(device=self.dev)\n        train_labels = train_labels.to(device=self.dev)\n        train_budgets = train_budgets.to(device=self.dev)\n        train_curves = train_curves.to(device=self.dev)\n\n        data = {\n            'X_train': train_examples,\n            'train_budgets': train_budgets,\n            'train_curves': train_curves,\n            'y_train': train_labels,\n        }\n\n        return data\n\n    def _train_surrogate(self):\n        \"\"\"\n        Train the surrogate model.\n        \"\"\"\n        data = self._prepare_dataset_and_budgets()\n        self.logger.info(f'Started training the model')\n\n        self.model.train_pipeline(\n            data,\n            load_checkpoint=False,\n        )\n\n    def _predict(self) -> Tuple[np.ndarray, np.ndarray, List, List]:\n        \"\"\"\n        Predict the performances of the hyperparameter configurations\n        as well as the standard deviations based on the surrogate model.\n\n        Returns:\n            mean_predictions, std_predictions, hp_indices, non_scaled_budgets:\n                The mean predictions and the standard deviations over\n                all model predictions for the given hyperparameter\n                configurations with their associated indices, scaled and\n                non-scaled budgets.\n        \"\"\"\n        configurations, hp_indices, budgets, learning_curves = self.generate_candidate_configurations()\n        budgets = np.array(budgets, dtype=np.single)\n        non_scaled_budgets = copy.deepcopy(budgets)\n        # scale budgets to [0, 1]\n        budgets = budgets / self.max_benchmark_epochs\n\n        configurations = np.array(configurations, dtype=np.single)\n        configurations = torch.tensor(configurations)\n        configurations = configurations.to(device=self.dev)\n\n        budgets = torch.tensor(budgets)\n        budgets = budgets.to(device=self.dev)\n\n        learning_curves = self.patch_curves_to_same_length(learning_curves)\n        learning_curves = np.array(learning_curves, dtype=np.single)\n        learning_curves = torch.tensor(learning_curves)\n        learning_curves = learning_curves.to(device=self.dev)\n\n        train_data = self._prepare_dataset_and_budgets()\n        test_data = {\n            'X_test': configurations,\n            'test_budgets': budgets,\n            'test_curves': learning_curves,\n        }\n\n        mean_predictions, std_predictions = self.model.predict_pipeline(train_data, test_data)\n\n        return mean_predictions, std_predictions, hp_indices, non_scaled_budgets\n\n    def suggest(self) -> Tuple[int, int]:\n        \"\"\"\n        Suggest a hyperparameter configuration to be evaluated next.\n\n        Returns:\n            best_config_index, budget: The index of the hyperparamter\n                configuration to be evaluated and the budget for\n                what it is going to be evaluated for.\n        \"\"\"\n        suggest_time_start = time.time()\n        # check if we still have random hyperparameters to evaluate\n        if self.initial_random_index < len(self.init_conf_indices):\n            self.logger.info(\n                'Not enough configurations to build a model. '\n                'Returning randomly sampled configuration'\n            )\n\n            random_indice = self.init_conf_indices[self.initial_random_index]\n            budget = self.init_budgets[self.initial_random_index]\n            self.initial_random_index += 1\n\n            return random_indice, budget\n        else:\n            mean_predictions, std_predictions, hp_indices, non_scaled_budgets = self._predict()\n            best_prediction_index = self.find_suggested_config(\n                mean_predictions,\n                std_predictions,\n                non_scaled_budgets,\n            )\n            \"\"\"\n            the best prediction index is not always matching with the actual hp index.\n            Since when evaluating the acq function, we do not consider hyperparameter\n            candidates that diverged or that are evaluated fully.\n            \"\"\"\n            best_config_index = hp_indices[best_prediction_index]\n\n            # decide for what budget we will evaluate the most\n            # promising hyperparameter configuration next.\n            if best_config_index in self.examples:\n                evaluated_budgets = self.examples[best_config_index]\n                max_budget = max(evaluated_budgets)\n                budget = max_budget + self.fantasize_step\n                # this would only trigger if fantasize_step is bigger\n                # than 1\n                if budget > self.max_benchmark_epochs:\n                    budget = self.max_benchmark_epochs\n            else:\n                budget = self.fantasize_step\n\n        suggest_time_end = time.time()\n        self.suggest_time_duration = suggest_time_end - suggest_time_start\n\n        self.budget_spent += self.fantasize_step\n\n        # exhausted hpo budget, finish.\n        if self.budget_spent > self.total_budget:\n            exit(0)\n\n        return best_config_index, budget\n\n    def observe(\n        self,\n        hp_index: int,\n        b: int,\n        learning_curve: np.ndarray,\n        alg_time: Optional[float] = None,\n    ):\n        \"\"\"\n        Args:\n            hp_index: The index of the evaluated hyperparameter configuration.\n            b: The budget for which the hyperparameter configuration was evaluated.\n            learning_curve: The learning curve of the hyperparameter configuration.\n            alg_time: The time taken from the algorithm to evaluate the hp configuration.\n        \"\"\"\n        score = learning_curve[-1]\n        # if y is an undefined value, append 0 as the overhead since we finish here.\n        if np.isnan(learning_curve).any():\n            self.update_info_dict(hp_index, b, np.nan, 0)\n            self.diverged_configs.add(hp_index)\n            return\n\n        observe_time_start = time.time()\n\n        self.examples[hp_index] = np.arange(1, b + 1).tolist()\n        self.performances[hp_index] = learning_curve\n\n        if self.best_value_observed < score:\n            self.best_value_observed = score\n            self.no_improvement_patience = 0\n        else:\n            self.no_improvement_patience += 1\n\n        observe_time_end = time.time()\n        train_time_duration = 0\n\n        # initialization phase over. Now we can sample from the model.\n        if self.initial_random_index >= len(self.init_conf_indices):\n            train_time_start = time.time()\n            # create the model for the first time\n            if self.model is None:\n                # Starting a model from scratch\n                self.model = DyHPO(\n                    self.surrogate_config,\n                    self.dev,\n                    self.dataset_name,\n                    self.output_path,\n                    self.seed,\n                )\n\n            if self.no_improvement_patience == self.no_improvement_threshold:\n                self.model.restart = True\n\n            self._train_surrogate()\n\n            train_time_end = time.time()\n            train_time_duration = train_time_end - train_time_start\n\n        observe_time_duration = observe_time_end - observe_time_start\n        total_duration = observe_time_duration + self.suggest_time_duration + train_time_duration\n        if alg_time is not None:\n            total_duration = total_duration + alg_time\n\n        self.update_info_dict(hp_index, b, score, total_duration)\n\n    def prepare_examples(self, hp_indices: List) -> List[np.ndarray]:\n        \"\"\"\n        Prepare the examples to be given to the surrogate model.\n\n        Args:\n            hp_indices: The list of hp indices that are already evaluated.\n\n        Returns:\n            examples: A list of the hyperparameter configurations.\n        \"\"\"\n        examples = []\n        for hp_index in hp_indices:\n            examples.append(self.hp_candidates[hp_index])\n\n        return examples\n\n    def generate_candidate_configurations(\n        self,\n    ) -> Tuple[List, List, List, List]:\n        \"\"\"\n        Generate candidate configurations that will be\n        fantasized upon.\n\n        Returns:\n            (configurations, hp_indices, hp_budgets, learning_curves): Tuple\n                A tuple of configurations, their indices in the hp list\n                and the budgets that they should be fantasized upon.\n        \"\"\"\n        hp_indices = []\n        hp_budgets = []\n        learning_curves = []\n\n        for hp_index in range(0, self.hp_candidates.shape[0]):\n\n            if hp_index in self.examples:\n                budgets = self.examples[hp_index]\n                # Take the max budget evaluated for a certain hpc\n                max_budget = max(budgets)\n                next_budget = max_budget + self.fantasize_step\n                # take the learning curve until the point we have evaluated so far\n                curve = self.performances[hp_index][:max_budget]\n                # if the curve is shorter than the length of the kernel size,\n                # pad it with zeros\n                difference_curve_length = self.surrogate_config['cnn_kernel_size'] - len(curve)\n                if difference_curve_length > 0:\n                    curve.extend([0.0] * difference_curve_length)\n            else:\n                # The hpc was not evaluated before, so fantasize its\n                # performance\n                next_budget = self.fantasize_step\n                curve = [0, 0, 0]\n\n            # this hyperparameter configuration is not evaluated fully\n            if next_budget <= self.max_benchmark_epochs:\n                hp_indices.append(hp_index)\n                hp_budgets.append(next_budget)\n                learning_curves.append(curve)\n\n        configurations = self.prepare_examples(hp_indices)\n\n        return configurations, hp_indices, hp_budgets, learning_curves\n\n    def history_configurations(\n        self,\n    ) -> Tuple[List, List, List, List]:\n        \"\"\"\n        Generate the configurations, labels, budgets and curves based on\n        the history of evaluated configurations.\n\n        Returns:\n            (train_examples, train_labels, train_budgets, train_curves):\n                A tuple of examples, labels, budgets and curves for the\n                configurations evaluated so far.\n        \"\"\"\n        train_examples = []\n        train_labels = []\n        train_budgets = []\n        train_curves = []\n\n        for hp_index in self.examples:\n            budgets = self.examples[hp_index]\n            performances = self.performances[hp_index]\n            example = self.hp_candidates[hp_index]\n\n            for budget, performance in zip(budgets, performances):\n                train_examples.append(example)\n                train_budgets.append(budget)\n                train_labels.append(performance)\n                train_curve = performances[:budget - 1] if budget > 1 else [0.0]\n                difference_curve_length = self.surrogate_config['cnn_kernel_size'] - len(train_curve)\n                if difference_curve_length > 0:\n                    train_curve.extend([0.0] * difference_curve_length)\n\n                train_curves.append(train_curve)\n\n        return train_examples, train_labels, train_budgets, train_curves\n\n    def acq(\n        self,\n        best_value: float,\n        mean: float,\n        std: float,\n        explore_factor: Optional[float] = 0.25,\n        acq_fc: str = 'ei',\n    ) -> float:\n        \"\"\"\n        The acquisition function that will be called\n        to evaluate the score of a hyperparameter configuration.\n\n        Parameters\n        ----------\n        best_value: float\n            Best observed function evaluation. Individual per fidelity.\n        mean: float\n            Point mean of the posterior process.\n        std: float\n            Point std of the posterior process.\n        explore_factor: float\n            The exploration factor for when ucb is used as the\n            acquisition function.\n        ei_calibration_factor: float\n            The factor used to calibrate expected improvement.\n        acq_fc: str\n            The type of acquisition function to use.\n\n        Returns\n        -------\n        acq_value: float\n            The value of the acquisition function.\n        \"\"\"\n        if acq_fc == 'ei':\n            if std == 0:\n                return 0\n            z = (mean - best_value) / std\n            acq_value = (mean - best_value) * norm.cdf(z) + std * norm.pdf(z)\n        elif acq_fc == 'ucb':\n            acq_value = mean + explore_factor * std\n        elif acq_fc == 'thompson':\n            acq_value = np.random.normal(mean, std)\n        elif acq_fc == 'exploit':\n            acq_value = mean\n        else:\n            raise NotImplementedError(\n                f'Acquisition function {acq_fc} has not been'\n                f'implemented',\n            )\n\n        return acq_value\n\n    def find_suggested_config(\n        self,\n        mean_predictions: np.ndarray,\n        mean_stds: np.ndarray,\n        budgets: List,\n    ) -> int:\n        \"\"\"\n        Find the hyperparameter configuration that has the highest score\n        with the acquisition function.\n\n        Args:\n            mean_predictions: The mean predictions of the posterior.\n            mean_stds: The mean standard deviations of the posterior.\n            budgets: The next budgets that the hyperparameter configurations\n                will be evaluated for.\n\n        Returns:\n            best_index: The index of the hyperparameter configuration with the\n                highest score.\n        \"\"\"\n        highest_acq_value = np.NINF\n        best_index = -1\n\n        index = 0\n        for mean_value, std in zip(mean_predictions, mean_stds):\n            budget = int(budgets[index])\n            best_value = self.calculate_fidelity_ymax(budget)\n            acq_value = self.acq(best_value, mean_value, std, acq_fc='ei')\n            if acq_value > highest_acq_value:\n                highest_acq_value = acq_value\n                best_index = index\n\n            index += 1\n\n        return best_index\n\n    def calculate_fidelity_ymax(self, fidelity: int):\n        \"\"\"\n        Find ymax for a given fidelity level.\n\n        If there are hyperparameters evaluated for that fidelity\n        take the maximum from their values. Otherwise, take\n        the maximum from all previous fidelity levels for the\n        hyperparameters that we have evaluated.\n\n        Args:\n            fidelity: The fidelity of the hyperparameter\n                configuration.\n\n        Returns:\n            best_value: The best value seen so far for the\n                given fidelity.\n        \"\"\"\n        exact_fidelity_config_values = []\n        lower_fidelity_config_values = []\n\n        for example_index in self.examples.keys():\n            try:\n                performance = self.performances[example_index][fidelity - 1]\n                exact_fidelity_config_values.append(performance)\n            except IndexError:\n                learning_curve = self.performances[example_index]\n                # The hyperparameter was not evaluated until fidelity, or more.\n                # Take the maximum value from the curve.\n                lower_fidelity_config_values.append(max(learning_curve))\n\n        if len(exact_fidelity_config_values) > 0:\n            # lowest error corresponds to best value\n            best_value = max(exact_fidelity_config_values)\n        else:\n            best_value = max(lower_fidelity_config_values)\n\n        return best_value\n\n    def update_info_dict(\n        self,\n        hp_index: int,\n        budget: int,\n        performance: float,\n        overhead: float,\n    ):\n        \"\"\"\n        Update the info dict with the current HPO iteration info.\n\n        Dump a new json file that will update with additional information\n        given the current HPO iteration.\n\n        Args:\n            hp_index: The index of the hyperparameter configuration.\n            budget: The budget of the hyperparameter configuration.\n            performance:  The performance of the hyperparameter configuration.\n            overhead: The total overhead (in seconds) of the iteration.\n        \"\"\"\n        hp_index = int(hp_index)\n        if 'hp' in self.info_dict:\n            self.info_dict['hp'].append(hp_index)\n        else:\n            self.info_dict['hp'] = [hp_index]\n\n        if 'scores' in self.info_dict:\n            self.info_dict['scores'].append(performance)\n        else:\n            self.info_dict['scores'] = [performance]\n\n        if 'curve' in self.info_dict:\n            self.info_dict['curve'].append(self.best_value_observed)\n        else:\n            self.info_dict['curve'] = [self.best_value_observed]\n\n        if 'epochs' in self.info_dict:\n            self.info_dict['epochs'].append(budget)\n        else:\n            self.info_dict['epochs'] = [budget]\n\n        if 'overhead' in self.info_dict:\n            self.info_dict['overhead'].append(overhead)\n        else:\n            self.info_dict['overhead'] = [overhead]\n\n        with open(os.path.join(self.output_path, f'{self.dataset_name}_{self.seed}.json'), 'w') as fp:\n            json.dump(self.info_dict, fp)\n\n    def preprocess_hp_candidates(self) -> List:\n        \"\"\"\n        Preprocess the list of all hyperparameter candidates\n        by  performing a log transform for the hyperparameters that\n        were log sampled.\n\n        Returns:\n            log_hp_candidates: The list of all hyperparameter configurations\n                where hyperparameters that were log sampled are log transformed.\n        \"\"\"\n        log_hp_candidates = []\n\n        for hp_candidate in self.hp_candidates:\n            new_hp_candidate = []\n            for index, hp_value in enumerate(hp_candidate):\n                new_hp_candidate.append(math.log(hp_value) if self.log_indicator[index] else hp_value)\n\n            log_hp_candidates.append(new_hp_candidate)\n\n        log_hp_candidates = np.array(log_hp_candidates)\n        # scaler for the hp configurations\n\n        log_hp_candidates = self.scaler.fit_transform(log_hp_candidates)\n\n        return log_hp_candidates\n\n    @staticmethod\n    def patch_curves_to_same_length(curves):\n        \"\"\"\n        Patch the given curves to the same length.\n\n        Finds the maximum curve length and patches all\n        other curves that are shorter in length with zeroes.\n\n        Args:\n            curves: The given hyperparameter curves.\n\n        Returns:\n            curves: The updated array where the learning\n                curves are of the same length.\n        \"\"\"\n        max_curve_length = 0\n        for curve in curves:\n            if len(curve) > max_curve_length:\n                max_curve_length = len(curve)\n\n        for curve in curves:\n            difference = max_curve_length - len(curve)\n            if difference > 0:\n                curve.extend([0.0] * difference)\n\n        return curves\n\nfrom copy import deepcopy\nimport logging\nimport os\nfrom typing import Dict, Tuple\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch import cat\n\nimport gpytorch\n\n\nclass FeatureExtractor(nn.Module):\n    \"\"\"\n    The feature extractor that is part of the deep kernel.\n    \"\"\"\n    def __init__(self, configuration):\n        super(FeatureExtractor, self).__init__()\n\n        self.configuration = configuration\n\n        self.nr_layers = configuration['nr_layers']\n        self.act_func = nn.LeakyReLU()\n        # adding one to the dimensionality of the initial input features\n        # for the concatenation with the budget.\n        initial_features = configuration['nr_initial_features'] + 1\n        self.fc1 = nn.Linear(initial_features, configuration['layer1_units'])\n        self.bn1 = nn.BatchNorm1d(configuration['layer1_units'])\n        for i in range(2, self.nr_layers):\n            setattr(\n                self,f'fc{i + 1}',\n                nn.Linear(configuration[f'layer{i - 1}_units'], configuration[f'layer{i}_units']),\n            )\n            setattr(\n                self,f'bn{i + 1}',\n                nn.BatchNorm1d(configuration[f'layer{i}_units']),\n            )\n\n\n        setattr(\n            self,\n            f'fc{self.nr_layers}',\n            nn.Linear(\n                configuration[f'layer{self.nr_layers - 1}_units'] +\n                configuration['cnn_nr_channels'],  # accounting for the learning curve features\n                configuration[f'layer{self.nr_layers}_units']\n            ),\n        )\n        self.cnn = nn.Sequential(\n            nn.Conv1d(in_channels=1, kernel_size=(configuration['cnn_kernel_size'],), out_channels=4),\n            nn.AdaptiveMaxPool1d(1),\n        )\n\n    def forward(self, x, budgets, learning_curves):\n\n        # add an extra dimensionality for the budget\n        # making it nr_rows x 1.\n        budgets = torch.unsqueeze(budgets, dim=1)\n        # concatenate budgets with examples\n        x = cat((x, budgets), dim=1)\n        x = self.fc1(x)\n        x = self.act_func(self.bn1(x))\n\n        for i in range(2, self.nr_layers):\n            x = self.act_func(\n                getattr(self, f'bn{i}')(\n                    getattr(self, f'fc{i}')(\n                        x\n                    )\n                )\n            )\n\n        # add an extra dimensionality for the learning curve\n        # making it nr_rows x 1 x lc_values.\n        learning_curves = torch.unsqueeze(learning_curves, 1)\n        lc_features = self.cnn(learning_curves)\n        # revert the output from the cnn into nr_rows x nr_kernels.\n        lc_features = torch.squeeze(lc_features, 2)\n\n        # put learning curve features into the last layer along with the higher level features.\n        x = cat((x, lc_features), dim=1)\n        x = self.act_func(getattr(self, f'fc{self.nr_layers}')(x))\n\n        return x\n\n\nclass GPRegressionModel(gpytorch.models.ExactGP):\n    \"\"\"\n    A simple GP model.\n    \"\"\"\n    def __init__(\n        self,\n        train_x: torch.Tensor,\n        train_y: torch.Tensor,\n        likelihood: gpytorch.likelihoods.GaussianLikelihood,\n    ):\n        \"\"\"\n        Constructor of the GPRegressionModel.\n\n        Args:\n            train_x: The initial train examples for the GP.\n            train_y: The initial train labels for the GP.\n            likelihood: The likelihood to be used.\n        \"\"\"\n        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n\n        self.mean_module = gpytorch.means.ConstantMean()\n        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n\n    def forward(self, x):\n\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n\n        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n\n\nclass DyHPO:\n    \"\"\"\n    The DyHPO DeepGP model.\n    \"\"\"\n    def __init__(\n        self,\n        configuration: Dict,\n        device: torch.device,\n        dataset_name: str = 'unknown',\n        output_path: str = '.',\n        seed: int = 11,\n    ):\n        \"\"\"\n        The constructor for the DyHPO model.\n\n        Args:\n            configuration: The configuration to be used\n                for the different parts of the surrogate.\n            device: The device where the experiments will be run on.\n            dataset_name: The name of the dataset for the current run.\n            output_path: The path where the intermediate/final results\n                will be stored.\n            seed: The seed that will be used to store the checkpoint\n                properly.\n        \"\"\"\n        super(DyHPO, self).__init__()\n        self.feature_extractor = FeatureExtractor(configuration)\n        self.batch_size = configuration['batch_size']\n        self.nr_epochs = configuration['nr_epochs']\n        self.early_stopping_patience = configuration['nr_patience_epochs']\n        self.refine_epochs = 50\n        self.dev = device\n        self.seed = seed\n        self.model, self.likelihood, self.mll = \\\n            self.get_model_likelihood_mll(\n                configuration[f'layer{self.feature_extractor.nr_layers}_units']\n            )\n\n        self.model.to(self.dev)\n        self.likelihood.to(self.dev)\n        self.feature_extractor.to(self.dev)\n\n        self.optimizer = torch.optim.Adam([\n            {'params': self.model.parameters(), 'lr': configuration['learning_rate']},\n            {'params': self.feature_extractor.parameters(), 'lr': configuration['learning_rate']}],\n        )\n\n        self.configuration = configuration\n        # the number of initial points for which we will retrain fully from scratch\n        # This is basically equal to the dimensionality of the search space + 1.\n        self.initial_nr_points = 10\n        # keeping track of the total hpo iterations. It will be used during the optimization\n        # process to switch from fully training the model, to refining.\n        self.iterations = 0\n        # flag for when the optimization of the model should start from scratch.\n        self.restart = True\n\n        self.logger = logging.getLogger(__name__)\n\n        self.checkpoint_path = os.path.join(\n            output_path,\n            'checkpoints',\n            f'{dataset_name}',\n            f'{self.seed}',\n        )\n\n        os.makedirs(self.checkpoint_path, exist_ok=True)\n\n        self.checkpoint_file = os.path.join(\n            self.checkpoint_path,\n            'checkpoint.pth'\n        )\n\n    def restart_optimization(self):\n        \"\"\"\n        Restart the surrogate model from scratch.\n        \"\"\"\n        self.feature_extractor = FeatureExtractor(self.configuration).to(self.dev)\n        self.model, self.likelihood, self.mll = \\\n            self.get_model_likelihood_mll(\n                self.configuration[f'layer{self.feature_extractor.nr_layers}_units'],\n            )\n\n        self.optimizer = torch.optim.Adam([\n            {'params': self.model.parameters(), 'lr': self.configuration['learning_rate']},\n            {'params': self.feature_extractor.parameters(), 'lr': self.configuration['learning_rate']}],\n        )\n\n    def get_model_likelihood_mll(\n        self,\n        train_size: int,\n    ) -> Tuple[GPRegressionModel, gpytorch.likelihoods.GaussianLikelihood, gpytorch.mlls.ExactMarginalLogLikelihood]:\n        \"\"\"\n        Called when the surrogate is first initialized or restarted.\n\n        Args:\n            train_size: The size of the current training set.\n\n        Returns:\n            model, likelihood, mll - The GP model, the likelihood and\n                the marginal likelihood.\n        \"\"\"\n        train_x = torch.ones(train_size, train_size).to(self.dev)\n        train_y = torch.ones(train_size).to(self.dev)\n\n        likelihood = gpytorch.likelihoods.GaussianLikelihood().to(self.dev)\n        model = GPRegressionModel(train_x=train_x, train_y=train_y, likelihood=likelihood).to(self.dev)\n        mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model).to(self.dev)\n\n        return model, likelihood, mll\n\n    def train_pipeline(self, data: Dict[str, torch.Tensor], load_checkpoint: bool = False):\n        \"\"\"\n        Train the surrogate model.\n\n        Args:\n            data: A dictionary which has the training examples, training features,\n                training budgets and in the end the training curves.\n            load_checkpoint: A flag whether to load the state from a previous checkpoint,\n                or whether to start from scratch.\n        \"\"\"\n        self.iterations += 1\n        self.logger.debug(f'Starting iteration: {self.iterations}')\n        # whether the state has been changed. Basically, if a better loss was found during\n        # this optimization iteration then the state (weights) were changed.\n        weights_changed = False\n\n        if load_checkpoint:\n            try:\n                self.load_checkpoint()\n            except FileNotFoundError:\n                self.logger.error(f'No checkpoint file found at: {self.checkpoint_file}'\n                                  f'Training the GP from the beginning')\n\n        self.model.train()\n        self.likelihood.train()\n        self.feature_extractor.train()\n\n        self.optimizer = torch.optim.Adam([\n            {'params': self.model.parameters(), 'lr': self.configuration['learning_rate']},\n            {'params': self.feature_extractor.parameters(), 'lr': self.configuration['learning_rate']}],\n        )\n\n        X_train = data['X_train']\n        train_budgets = data['train_budgets']\n        train_curves = data['train_curves']\n        y_train = data['y_train']\n\n        initial_state = self.get_state()\n        training_errored = False\n\n        if self.restart:\n            self.restart_optimization()\n            nr_epochs = self.nr_epochs\n            # 2 cases where the statement below is hit.\n            # - We are switching from the full training phase in the beginning to refining.\n            # - We are restarting because our refining diverged\n            if self.initial_nr_points <= self.iterations:\n                self.restart = False\n        else:\n            nr_epochs = self.refine_epochs\n\n        # where the mean squared error will be stored\n        # when predicting on the train set\n        mse = 0.0\n\n        for epoch_nr in range(0, nr_epochs):\n\n            nr_examples_batch = X_train.size(dim=0)\n            # if only one example in the batch, skip the batch.\n            # Otherwise, the code will fail because of batchnorm\n            if nr_examples_batch == 1:\n                continue\n\n            # Zero backprop gradients\n            self.optimizer.zero_grad()\n\n            projected_x = self.feature_extractor(X_train, train_budgets, train_curves)\n            self.model.set_train_data(projected_x, y_train, strict=False)\n            output = self.model(projected_x)\n\n            try:\n                # Calc loss and backprop derivatives\n                loss = -self.mll(output, self.model.train_targets)\n                loss_value = loss.detach().to('cpu').item()\n                mse = gpytorch.metrics.mean_squared_error(output, self.model.train_targets)\n                self.logger.debug(\n                    f'Epoch {epoch_nr} - MSE {mse:.5f}, '\n                    f'Loss: {loss_value:.3f}, '\n                    f'lengthscale: {self.model.covar_module.base_kernel.lengthscale.item():.3f}, '\n                    f'noise: {self.model.likelihood.noise.item():.3f}, '\n                )\n                loss.backward()\n                self.optimizer.step()\n            except Exception as training_error:\n                self.logger.error(f'The following error happened while training: {training_error}')\n                # An error has happened, trigger the restart of the optimization and restart\n                # the model with default hyperparameters.\n                self.restart = True\n                training_errored = True\n                break\n\n        \"\"\"\n        # metric too high, time to restart, or we risk divergence\n        if mse > 0.15:\n            if not self.restart:\n                self.restart = True\n        \"\"\"\n        if training_errored:\n            self.save_checkpoint(initial_state)\n            self.load_checkpoint()\n\n    def predict_pipeline(\n        self,\n        train_data: Dict[str, torch.Tensor],\n        test_data: Dict[str, torch.Tensor],\n    ) -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n\n        Args:\n            train_data: A dictionary that has the training\n                examples, features, budgets and learning curves.\n            test_data: Same as for the training data, but it is\n                for the testing part and it does not feature labels.\n\n        Returns:\n            means, stds: The means of the predictions for the\n                testing points and the standard deviations.\n        \"\"\"\n        self.model.eval()\n        self.feature_extractor.eval()\n        self.likelihood.eval()\n\n        with torch.no_grad(): # gpytorch.settings.fast_pred_var():\n            projected_train_x = self.feature_extractor(\n                train_data['X_train'],\n                train_data['train_budgets'],\n                train_data['train_curves'],\n            )\n            self.model.set_train_data(inputs=projected_train_x, targets=train_data['y_train'], strict=False)\n            projected_test_x = self.feature_extractor(\n                test_data['X_test'],\n                test_data['test_budgets'],\n                test_data['test_curves'],\n            )\n            preds = self.likelihood(self.model(projected_test_x))\n\n        means = preds.mean.detach().to('cpu').numpy().reshape(-1, )\n        stds = preds.stddev.detach().to('cpu').numpy().reshape(-1, )\n\n        return means, stds\n\n    def load_checkpoint(self):\n        \"\"\"\n        Load the state from a previous checkpoint.\n        \"\"\"\n        checkpoint = torch.load(self.checkpoint_file)\n        self.model.load_state_dict(checkpoint['gp_state_dict'])\n        self.feature_extractor.load_state_dict(checkpoint['feature_extractor_state_dict'])\n        self.likelihood.load_state_dict(checkpoint['likelihood_state_dict'])\n\n    def save_checkpoint(self, state: Dict =None):\n        \"\"\"\n        Save the given state or the current state in a\n        checkpoint file.\n\n        Args:\n            state: The state to save, if none, it will\n            save the current state.\n        \"\"\"\n\n        if state is None:\n            torch.save(\n                self.get_state(),\n                self.checkpoint_file,\n            )\n        else:\n            torch.save(\n                state,\n                self.checkpoint_file,\n            )\n\n    def get_state(self) -> Dict[str, Dict]:\n        \"\"\"\n        Get the current state of the surrogate.\n\n        Returns:\n            current_state: A dictionary that represents\n                the current state of the surrogate model.\n        \"\"\"\n        current_state = {\n            'gp_state_dict': deepcopy(self.model.state_dict()),\n            'feature_extractor_state_dict': deepcopy(self.feature_extractor.state_dict()),\n            'likelihood_state_dict': deepcopy(self.likelihood.state_dict()),\n        }\n\n        return current_state\n",
        "experimental_info": "DyHPO is a Bayesian Optimization approach based on Gaussian Processes (GP) with a dynamic budget allocation strategy. The core method utilizes a deep kernel for the GP surrogate model. The deep kernel (implemented by `FeatureExtractor`) is a neural network composed of two linear layers (64 and 128 units respectively) and a 1D convolutional layer with 4 output channels and a kernel size of 3, followed by global max pooling (`AdaptiveMaxPool1d(1)`). It takes hyperparameter configurations, budgets, and past learning curves as inputs. The features extracted by this deep kernel are then fed into a squared exponential (RBF) kernel for the GP surrogate model. The parameters of both the deep kernel and the GP kernel are learned by maximizing the marginal likelihood using the Adam optimizer with a learning rate of 0.001. Training occurs for 1000 epochs during full retraining or 50 epochs during refinement, with a patience of 10 epochs for early stopping. The batch size for training the surrogate model is 64. The acquisition function used is a multi-fidelity version of Expected Improvement (EIMF). The budget allocation dynamically determines the next configuration and budget to evaluate, selecting a configuration to train for one additional budget step (controlled by `fantasize_step`, which defaults to 1). Initial exploration consists of 1 randomly chosen configuration evaluated for 1 budget step. Approximately 10% of configurations are taken randomly instead of being sampled from the model during the optimization process. The model restarts optimization if no improvement in the best observed value is seen for `max_benchmark_epochs + 0.2 * max_benchmark_epochs` steps. Hyperparameter candidates are preprocessed by applying a log transformation to specified hyperparameters, followed by `MinMaxScaler` for all. Learning curves are padded with zeros to ensure uniform length. Experiments are conducted on either the LCBench dataset (accuracy maximization) or the TaskSet dataset (loss minimization), both having a maximum budget (`max_benchmark_epochs`) of 51. The total budget for hyperparameter optimization is 1000 iterations (default). Random seeds for experiments are drawn from the range 0-9. The processing device used is CUDA if available, otherwise CPU."
      }
    },
    {
      "title": "Supervising the Multi-Fidelity Race of Hyperparameter Configurations",
      "abstract": "Multi-fidelity (gray-box) hyperparameter optimization techniques (HPO) have\nrecently emerged as a promising direction for tuning Deep Learning methods.\nHowever, existing methods suffer from a sub-optimal allocation of the HPO\nbudget to the hyperparameter configurations. In this work, we introduce DyHPO,\na Bayesian Optimization method that learns to decide which hyperparameter\nconfiguration to train further in a dynamic race among all feasible\nconfigurations. We propose a new deep kernel for Gaussian Processes that embeds\nthe learning curve dynamics, and an acquisition function that incorporates\nmulti-budget information. We demonstrate the significant superiority of DyHPO\nagainst state-of-the-art hyperparameter optimization methods through\nlarge-scale experiments comprising 50 datasets (Tabular, Image, NLP) and\ndiverse architectures (MLP, CNN/NAS, RNN).",
      "full_text": "Supervising the Multi-Fidelity Race of Hyperparameter Configurations Martin Wistuba∗† Amazon Web Services, Berlin, Germany marwistu@amazon.com Arlind Kadra∗ University of Freiburg, Freiburg, Germany kadraa@cs.uni-freiburg.de Josif Grabocka University of Freiburg, Freiburg, Germany grabocka@cs.uni-freiburg.de Abstract Multi-fidelity (gray-box) hyperparameter optimization techniques (HPO) have recently emerged as a promising direction for tuning Deep Learning methods. However, existing methods suffer from a sub-optimal allocation of the HPO bud- get to the hyperparameter configurations. In this work, we introduce DyHPO, a Bayesian Optimization method that learns to decide which hyperparameter config- uration to train further in a dynamic race among all feasible configurations. We propose a new deep kernel for Gaussian Processes that embeds the learning curve dynamics, and an acquisition function that incorporates multi-budget information. We demonstrate the significant superiority of DyHPO against state-of-the-art hy- perparameter optimization methods through large-scale experiments comprising 50 datasets (Tabular, Image, NLP) and diverse architectures (MLP, CNN/NAS, RNN). 1 Introduction Hyperparameter Optimization (HPO) is arguably an acute open challenge for Deep Learning (DL), especially considering the crucial impact HPO has on achieving state-of-the-art empirical results. Unfortunately, HPO for DL is a relatively under-explored field and most DL researchers still optimize their hyperparameters via obscure trial-and-error practices. On the other hand, traditional Bayesian Optimization HPO methods [Snoek et al., 2012, Bergstra et al., 2011] are not directly applicable to deep networks, due to the infeasibility of evaluating a large number of hyperparameter configurations. In order to scale HPO for DL, three main directions of research have been recently explored. (i) Online HPO methods search for hyperparameters during the optimization process via meta-level controllers [Chen et al., 2017, Parker-Holder et al., 2020], however, this online adaptation can not accommodate all hyperparameters (e.g. related to architectural changes). (ii) Gradient-based HPO techniques, on the other hand, compute the derivative of the validation loss w.r.t. hyperparameters by reversing the training update steps [Maclaurin et al., 2015, Franceschi et al., 2017, Lorraine et al., 2020], however, the reversion is not directly applicable to all cases (e.g. dropout rate). The last direction, (iii) Gray-box HPO techniques discard sub-optimal configurations after evaluating them on lower budgets [Li et al., 2017, Falkner et al., 2018]. In contrast to the online and gradient-based alternatives, gray-box approaches can be deployed in an off-the-shelf manner to all types of hyperparameters and architectures. The gray-box concept is based on the intuition that a poorly-performing hyperparameter configuration can be identified and ∗equal contribution †work does not relate to position at Amazon 36th Conference on Neural Information Processing Systems (NeurIPS 2022). arXiv:2202.09774v2  [cs.LG]  1 Jun 2023terminated by inspecting the validation loss of the first few epochs, instead of waiting for the full convergence. The most prominent gray-box algorithm is Hyperband [Li et al., 2017], which is based on successive halving. It runs random configurations at different budgets (e.g. number of epochs) and successively halves these configurations by keeping only the top performers. Follow-up works, such as BOHB [Falkner et al., 2018] or DEHB [Awad et al., 2021], replace the random sampling of Hyperband with a sampling based on Bayesian optimization or differentiable evolution. Despite their great practical potential, gray-box methods suffer from a major issue. The low-budget (few epochs) performances are not always a good indicator for the full-budget (full convergence) performances. For example, a properly regularized network converges slower in the first few epochs, however, typically performs better than a non-regularized variant after the full convergence. In other words, there can be a poor rank correlation of the configurations’ performances at different budgets. Figure 1: Top: The learning curve for different hyperparameter configurations. The darker the learning curve, the later it was evaluated during the search. Bottom: The hyperparameter indices in a temporal order as evaluated during the opti- mization and their corresponding curves. We introduceDYHPO, a Bayesian Optimization (BO) approach based on Gaussian Processes (GP), that proposes a novel treatment to the multi-budget (a.k.a. multi-fidelity) setup. In this perspective, we propose a deep kernel GP that captures the learning dynamics. As a result, we train a kernel capable of capturing the similarity of a pair of hyperparameter configurations, even if the pair’s configurations are evaluated at differ- ent budgets. Furthermore, we extend Expected Improvement [Jones et al., 1998] to the multi- budget case, by introducing a new mechanism for the incumbent configuration of a budget. We illustrate the differences between our rac- ing strategy and successive halving with the experiment of Figure 1, where, we showcase the HPO progress of two different methods on the \"Helena\" dataset from the LCBench bench- mark [Zimmer et al., 2021]. Hyperband [Li et al., 2017] is a gray-box approach that stati- cally pre-allocates the budget for a set of candidates (Hyperband bracket) according to a predefined policy. However, DYHPO dynamically adapts the allocation of budgets for configurations after every HPO step (a.k.a. a dynamic race). As a result, DYHPO invests only a small budget on configurations that show little promise as indicated by the intermediate scores. The joint effect of modeling a GP kernel across budgets together with a dedicated acquisition function leads to DYHPO achieving a statistically significant empirical gain against state-of-the-art gray-box baselines [Falkner et al., 2018, Awad et al., 2021], including prior work on multi-budget GPs [Kandasamy et al., 2017, 2020] or neural networks [Li et al., 2020b]. We demonstrate the performance of DYHPO in three diverse deep learning architectures (MLP, CNN/NAS, RNN) and 50 datasets of three diverse modalities (tabular, image, natural language processing). We believe our method is a step forward toward making HPO for DL practical and feasible. Overall, our contributions can be summarized as follows: • We introduce a novel Bayesian surrogate for gray-box HPO optimization. Our novel surrogate model predicts the validation score of a machine learning model based on both the hyperparameter configuration, the budget information, and the learning curve. • We derive a simple yet robust way to combine this surrogate model with Bayesian opti- mization, reusing most of the existing components currently used in traditional Bayesian optimization methods. • Finally, we demonstrate the efficiency of our method for HPO and neural architecture search tasks compared to the current state-of-the-art methods in HPO, by outperforming seven strong HPO baselines with a statistically significant margin. As an overarching goal, we believe our method is an important step toward scaling HPO for DL. 22 Related Work on Gray-box HPO Multi-Fidelity Bayesian Optimization and Bandits. Bayesian optimization is a black-box func- tion optimization framework that has been successfully applied in optimizing hyperparameter and neural architectures alike [Snoek et al., 2012, Kandasamy et al., 2018, Bergstra et al., 2011]. To further improve Bayesian optimization, several works propose low-fidelity data approximations of hyperparameter configurations by training on a subset of the data [Swersky et al., 2013, Klein et al., 2017a], or by terminating training early [Swersky et al., 2014]. Additionally, several methods extend Bayesian optimization to multi-fidelity data by engineering new kernels suited for this problem [Swer- sky et al., 2013, 2014, Poloczek et al., 2017]. Kandasamy et al. [2016] extends GP-UCB [Srinivas et al., 2010] to the multi-fidelity setting by learning one Gaussian Process (GP) with a standard kernel for each fidelity. Their later work improves upon this method by learning one GP for all fidelities that enables the use of continuous fidelities [Kandasamy et al., 2017]. The work by Takeno et al. [2020] follows a similar idea but proposes to use an acquisition function based on information gain instead of UCB. While most of the works rely on GPs to model the surrogate function, Li et al. [2020b] use a Bayesian neural network that models the complex relationship between fidelities with stacked neural networks, one for each fidelity. Hyperband [Li et al., 2017] is a bandits-based multi-fidelity method for hyperparameter optimization that selects hyperparameter configurations at random and uses successive halving [Jamieson and Tal- walkar, 2016] with different settings to early-stop less promising training runs. Several improvements have been proposed to Hyperband with the aim to replace the random sampling of hyperparameter configurations with a more guided approach [Bertrand et al., 2017, Wang et al., 2018, Wistuba, 2017]. BOHB [Falkner et al., 2018] uses TPE [Bergstra et al., 2011] and builds a surrogate model for every fidelity adhering to a fixed-fidelity selection scheme. DEHB [Awad et al., 2021] samples candidates using differential evolution which handles large and discrete search spaces better than BOHB. Mendes et al. [2021] propose a variant of Hyperband which allows to skip stages. Learning Curve Prediction A variety of methods attempt to extrapolate a partially observed learning curve in order to estimate the probability that a configuration will improve over the current best solution. Domhan et al. [2015] propose to ensemble a set of parametric functions to extrapolate a partial learning curve. While this method is able to extrapolate with a single example, it requires a relatively long learning curve to do so. The work by Klein et al. [2017b] build upon the idea of using a set of parametric functions. The main difference is that they use a heteroscedastic Bayesian model to learn the ensemble weights. Baker et al. [2018] propose to use support vector machines (SVM) as an auto-regressive model. The SVM predicts the next value of a learning curve, the original learning curve is augmented by this value and we keep predicting further values. The work by Gargiani et al. [2019] use a similar idea but makes prediction based on the last K observations only and uses probabilistic models. Wistuba and Pedapati [2020] propose to learn a prediction model across learning curves from different tasks to avoid the costly learning curve collection. In contrast to DYHPO , none of these methods selects configuration but is limited to deciding when to stop a running configuration. Multi-Fidelity Acquisition Functions Klein et al. [2017a] propose an acquisition function which allows for selecting hyperparameter configurations and the dataset subset size. The idea is to reduce training time by considering only a smaller part of the training data. In contrast to EIMF, this acquisition function is designed to select arbitrary subset sizes whereas EIMF is intended to slowly increase the invested budget over time. Mendes et al. [2020] extend the work of Klein et al. [2017a] to take business constraints into account. Deep Kernel Learning with Bayesian Optimization. We are among the first to use deep kernel learning with Bayesian optimization and to the best of our knowledge the first to use it for multi- fidelity Bayesian optimization. Rai et al. [2016] consider the use of a deep kernel instead of a manually designed kernel in the context of standard Bayesian optimization, but, limit their experimentation to synthetic data and do not consider its use for hyperparameter optimization. Perrone et al. [2018], Wistuba and Grabocka [2021] use a pre-trained deep kernel to warm start Bayesian optimization with meta-data from previous optimizations. The aforementioned approaches are multi-task or transfer learning methods that require the availability of meta-data from related tasks. 3In contrast to prior work, we propose a method that introduces deep learning to multi-fidelity HPO with Bayesian Optimization, and captures the learning dynamics across fidelities/budgets, combined with an acquisition function that is tailored for the gray-box setup. 3 Dynamic Multi-Fidelity HPO 3.1 Preliminaries Gray-Box Optimization. The gray-box HPO setting allows querying configurations with a smaller budget compared to the total maximal budget B. Thus, we can query from the response function f : X ×N → R where fi,j = f(xi, j) is the response after spending a budget ofj on configuration xi. As before, these observations are noisy and we observe yi,j = f(xi, j) +εj where εj ∼ N(0, σ2 j,n). Please note, we assume that the budget required to query fi,j+b after having queried fi,j is only b. Furthermore, we use the learning curve Yi,j−1 = (yi,1, . . . , yi,j−1) when predicting fi,j. Gaussian Processes (GP). Given a training data set D = {(xi, yi)}n i=1, the Gaussian Process assumption is that yi is a random variable and the joint distribution of all yi is assumed to be multivariate Gaussian distributed as y ∼ N(m (X) , k(X, X)) . Furthermore, f∗ for test instances x∗ are jointly Gaussian with y as:\u0014 y f∗ \u0015 ∼ N \u0012 m (X, x∗) , \u0012 Kn K∗ KT ∗ K∗∗ \u0013\u0013 . (1) The mean function m is often set to 0 and its covariance function k depends on parameters θ. For notational convenience, we use Kn = k (X, X|θ) +σ2 nI, K∗ = k (X, X∗|θ) and K∗∗ = k (X∗, X∗|θ) to define the kernel matrices. We can derive the posterior predictive distribution with mean and covariance as follows: E[f∗|X, y, X∗] =KT ∗ K−1 n y, cov [f∗|X, X∗] =K∗∗ − KT ∗ K−1 n K∗ (2) Often, the kernel function is manually engineered, one popular example is the squared exponential kernel. However, in this work, we make use of the idea of deep kernel learning [Wilson et al., 2016]. The idea is to model the kernel as a neural network φ and learn the best kernel transformation K (θ, w) :=k(φ(x, w), φ(x′; w)|θ), which allows us to use convolutional operations in our kernel. 3.2 Deep Multi-Fidelity Surrogate We propose to use a Gaussian Process surrogate model that infers the value of fi,j based on the hyperparameter configuration xi, the budget j as well as the past learning curve Yi,j−1. For this purpose, we use a deep kernel as: K (θ, w) :=k(φ(xi, Yi,j−1, j; w), φ(xi′ , Yi′,j′−1, j′; w); θ) (3) xi j Yi,j−1 · 1B P P P Convolution max P P P Figure 2: The feature extractor φ of our kernel. We use a squared exponential kernel for k and the neural network φ is composed of linear and convolutional layers as shown in Figure 2. We normalize the budget j to a range between 0 and 1 by dividing it by the maximum budget B. Afterward, it is concatenated with the hyper- parameter configuration xi and fed to a linear layer. The learning curve Yi,j−1 is transformed by a one-dimensional convolution followed by a global max pooling layer. Finally, both repre- sentations are fed to another linear layer. Its output will be the input to the kernel function k. Both, the kernel k and the neural network φ consist of trainable parameters θ and w, respectively. We find their optimal values by computing the maximum likelihood estimates as: ˆθ, ˆw = arg max θ,w p(y|X, Y, θ, w) ∝ arg min θ,w yTK (θ, w)−1 y + log|K (θ, w)| (4) 4In order to solve this optimization problem, we use gradient descent and Adam [Kingma and Ba, 2015] with a learning rate of 0.1. Given the maximum likelihood estimates, we can approximate the predictive posterior through p \u0010 fi,j|xi, Yi,j−1, j,D, ˆθ, ˆw \u0011 , and ultimately compute the mean and covariance of this Gaussian using Equation 2. 3.3 Multi-Fidelity Expected Improvement Expected improvement [Jones et al., 1998] is a commonly used acquisition function and is defined as: EI(x|D) =E[max {f(x) − ymax, 0}] , (5) where ymax is the largest observed value of f. We propose a multi-fidelity version of it as: EIMF(x, j|D) =E \u0002 max \b f(x, j) − ymax j , 0 \t\u0003 , (6) where: ymax j = \u001amax {y | ((x, ·, j), y) ∈ D} if ((x, ·, j), y) ∈ D max {y | (·, y) ∈ D} otherwise (7) Simply put, ymax j is the largest observed value of f for a budget of j if it exists already, otherwise, it is the largest observed value for any budget. If there is only one possible budget, the multi-fidelity expected improvement is identical to expected improvement. 3.4 The D YHPO Algorithm The DYHPO algorithm looks very similar to many black-box Bayesian optimization algorithms as shown in Algorithm 1. The big difference is that at each step we dynamically decide which candidate configuration to train for a small additional budget. Algorithm 1 DYHPO Algorithm 1: b(x) = 0∀x ∈ X 2: while not converged do 3: xi ← arg maxx∈X EIMF (x, b(x) + 1)(Sec. 3.3) 4: Observe yi,b(xi)+1. 5: b(xi) ← b(xi) + 1 6: D ← D ∪ \b ((xi, Yi,b(xi)−1, b(xi)), yi,b(xi)) \t 7: Update the surrogate on D. (Sec. 3.2) return xi with largest yi,·. Possible candidates are previously un- considered configurations as well as configurations that did not reach the maximum budget. In Line 2, the most promising candidate is chosen using the acquisition function introduced in Section 3.3 and the surrogate model’s predictions. It is important to high- light that we do not maximize the ac- quisition function along the budget di- mensionality. Instead, we set the bud- get b such that it is by exactly one higher than the budget used to eval- uate xi before. This ensures that we explore configurations by slowly increasing the budget. After the candidate and the corresponding budget are selected, the function f is evaluated and we observe yi,j (Line 3). This additional data point is added to D in Line 4. Then in Line 5, the surrogate model is updated according to the training scheme described in Section 3.2. 4 Experimental Protocol 4.1 Experimental Setup We evaluate DYHPO in three different settings on hyperparameter optimization for tabular, text, and image classification against several competitor methods, the details of which are provided in the following subsections. We ran all of our experiments on an Amazon EC2 M5 Instance (m5.xlarge). In our experiments, we report the mean of ten repetitions and we report two common metrics, the regret and the average rank. The regret refers to the absolute difference between the score of the solution found by an optimizer compared to the best possible score. If we report the regret as an aggregate result over multiple datasets, we report the mean over all regrets. The average rank is the metric we use to aggregate rank results over different datasets. We provide further implementation and training details in Appendix A.4. Our implementation of DYHPO is publicly available.3 3https://github.com/releaunifreiburg/DyHPO 54.2 Benchmarks In our experiments, we use the following benchmarks. We provide more details in Appendix A.1. LCBench: A learning curve benchmark [Zimmer et al., 2021] that evaluates neural network architec- tures for tabular datasets. LCBench contains learning curves for 35 different datasets, where 2,000 neural networks per dataset are trained for 50 epochs with Auto-PyTorch. TaskSet: A benchmark that features diverse tasks Metz et al. [2020] from different domains and includes 5 search spaces with different degrees of freedom, where, every search space includes 1000 hyperparameter configurations. In this work, we focus on a subset of NLP tasks (12 tasks) and we use the Adam8p search space with 8 continuous hyperparameters. NAS-Bench-201: A benchmark consisting of 15625 hyperparameter configurations representing different architectures on the CIFAR-10, CIFAR-100 and ImageNet datasets Dong and Yang [2020]. NAS-Bench-201 features a search space of 6 categorical hyperparameters and each architecture is trained for 200 epochs. 4.3 Baselines Random Search: A random/stochastic black-box search method for HPO. HyperBand: A multi-arm bandit method that extends successive halving by multiple brackets with different combinations of the initial number of configurations, and their initial budget [Li et al., 2017]. BOHB: An extension of Hyperband that replaces the random sampling of the initial configurations for each bracket with recommended configurations from a model-based approach [Falkner et al., 2018]. BOHB builds a model for every fidelity that is considered. DEHB: A method that builds upon Hyperband by exploiting differential evolution to sample the initial candidates of a Hyperband bracket [Awad et al., 2021]. ASHA: An asynchronous version of successive halving (or an asynchronous version of Hyperband if multiple brackets are run). ASHA Li et al. [2020a] does not wait for all configurations to finish inside a successive halving bracket, but, instead promotes configurations to the next successive halving bracket in real-time. MF-DNN: A multi-fidelity Bayesian optimization method that uses deep neural networks to capture the relationships between different fidelities Li et al. [2020b]. Dragonfly: We compare against BOCA [Kandasamy et al., 2017] by using the Dragonfly library Kan- dasamy et al. [2020]. This method suggests the next hyperparameter configuration as well as the budget it should be evaluated for. 4.4 Research Hypotheses and Associated Experiments Hypothesis 1: DYHPO achieves state-of-the-art results in multi-fidelity HPO. Experiment 1: We compare DYHPO against the baselines of Section 4.3 on the benchmarks of Section 4.2 with the experimental setup of Section 4.1. For TaskSet we follow the authors’ recommendation and report the number of steps (every 200 iterations). Hypothesis 2: DYHPO’s runtime overhead has a negligible impact on the quality of results. Experiment 2: We compare DYHPO against the baselines of Section 4.3 over the wallclock time. The wallclock time includes both (i) the optimizer’s runtime overhead for recommending the next hyperparameter configuration, plus (ii) the time needed to evaluate the recommended configuration. In this experiment, we consider all datasets where the average training time per epoch is at least 10 seconds, because, for tasks where the training time is short, there is no practical justification for complex solutions and their overhead. In these cases, we recommend using a random search. We don’t report results for TaskSet because the benchmark lacks training times. Hypothesis 3: DYHPO uses the computational budget more efficiently than baselines. Experiment 3: To further verify that DYHPO is efficient compared to the baselines, we investigate whether competing methods spend their budgets on qualitative candidates. Concretely we: i) calculate 60 500 1000 Number of Epochs 10 2 10 1 Mean Regret LCBench 0 500 1000 Number of Steps 10 1 TaskSet 0 2000 4000 Number of Epochs 10 1 ImageNet16-120 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO Figure 3: The mean regret for the different benchmarks over the number of epochs or steps (every 200 iterations). The results are aggregated over 35 different datasets for LCBench and aggregated over 12 different NLP tasks for TaskSet. the precision of the top (w.r.t. ground truth) performing configurations that were selected by each method across different budgets, ii) compute the average regret of the selected configurations across budget, and iii) we compare the fraction of top-performing configurations at a given budget that were not top performers at lower budgets, i.e. measure the ability to handle the poor correlation of performances across budgets. 5 Results Experiment 1: DYHPO achieves state-of-the-art results. In our first experiment, we evaluate the various methods on the benchmarks listed in Section 4.2. We show the aggregated results in Figure 3, the results show that DYHPO manages to outperform competitor methods over the set of considered benchmarks by achieving a better mean regret across datasets. Not only does DYHPO achieve a better final performance, it also achieves strong anytime results by converging faster than the competitor methods. For the extended results, related to the performance of all methods on a dataset level, we refer the reader to Appendix B. 12345678 Random Dragonfly Hyperband MF-DNN ASHA BOHB DEHB DyHPO LCBench@50% 12345678 Random Hyperband MF-DNN Dragonfly DEHB BOHB ASHA DyHPO LCBench@100% 12345678 Random MF-DNN ASHA Dragonfly Hyperband DEHB BOHB DyHPO TaskSet@50% 12345678 Random MF-DNN ASHA Dragonfly DEHB Hyperband BOHB DyHPO TaskSet@100% Figure 4: Critical difference diagram for LCBench and TaskSet in terms of the number of HPO steps. The results correspond to results after 500 and 1,000 epochs. Connected ranks via a bold bar indicate that performances are not significantly different (p >0.05). In Figure 4, we provide further evidence thatDYHPO’s improvement over the baselines is statistically significant. The critical difference diagram presents the ranks of all methods and provides information on the pairwise statistical difference between all methods for two fractions of the number of HPO steps (50% and 100%). We included the LCBench and TaskSet benchmarks in our significance plots. NAS-Bench-201 was omitted because it has only 3 datasets and the statistical test cannot be applied. Horizontal lines indicate groupings of methods that are not significantly different. As suggested by the best published practices Demsar [2006], we use the Friedman test to reject the null hypothesis followed by a pairwise post-hoc analysis based on the Wilcoxon signed-rank test (α = 0.05). For LCBench, DYHPO already outperforms the baselines significantly after 50% of the search budget, with a statistically significant margin. As the optimization procedure continues, DYHPO manages to extend its gain in performance and is the only method that has a statistically significant improvement against all the other competitor methods. Similarly, for TaskSet, DYHPO manages to outperform all methods with a statistically significant margin only halfway through the optimization procedure and achieves the best rank over all methods. However, as the optimization procedure continues, BOHB manages to decrease the performance gap with DYHPO , although, it still achieves a worse rank across all datasets. Considering the empirical results, we conclude that Hypothesis 1 is validated and that DYHPO achieves state-of-the-art results on multi-fidelity HPO. Experiment 2: On the impact of DYHPO ’s overhead on the results. We present the results of our second experiment in Figure 5 (left), where, as it can be seen, DYHPO still outperforms the other methods when its overhead is considered. For LCBench, DYHPO manages to get an advantage 70.2 0.4 0.6 0.8 1.0 Normalized Wallclock Time 10 1 Mean Regret LCBench 103 105 Wallclock Time in Seconds 10 1 ImageNet16-120 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 12345678 Random Dragonfly ASHA Hyperband MF-DNN BOHB DEHB DyHPO LCBench@50% 12345678 Random Dragonfly ASHA Hyperband MF-DNN BOHB DEHB DyHPO LCBench@100% Figure 5: Left: The regret over time for all methods during the optimization procedure for the LCBench benchmark and the ImageNet dataset from the NAS-Bench-201 benchmark. The normalized wall clock time represents the actual run time divided by the total wall clock time ofDYHPO including the overhead of fitting the deep GP. Right: The critical difference diagram for LCBench halfway through the HPO wall-clock time, and in the end. Connected ranks via a bold bar indicate that differences are not significant (p >0.05). fairly quickly and it only increases the gap in performance with the other methods as the optimization process progresses. Similarly, in the case of ImageNet from NAS-Bench-201, DYHPO manages to gain an advantage earlier than other methods during the optimization procedure. Although in the end DYHPO still performs better than all the other methods, we believe most of the methods converge to a good solution and the differences in the final performance are negligible. For the extended results, related to the performance of all methods on a dataset level over time, we refer the reader to the plots in Appendix B. Additionally, in Figure 5 (right), we provide the critical difference diagrams for LCBench that present the ranks and the statistical difference of all methods halfway through the optimization procedure, and in the end. As it can be seen, DYHPO has a better rank with a significant margin with only half of the budget used and it retains the advantage until the end. Experiment 3: On the efficiency of DYHPO . In Figure 6 (left), we plot the precision of every method for different budgets during the optimization procedure, which demonstrates that DYHPO effectively explores the search space and identifies promising candidates. The precision at an epoch i is defined as the number of top 1% candidates that are trained, divided by the number of all candidates trained, both trained for at least i epochs. The higher the precision, the more relevant candidates were considered and the less computational resources were wasted. For small budgets, the precision is low since DYHPO spends budget to consider various candidates, but then, promising candidates are successfully identified and the precision quickly increases. This argument is further supported in Figure 6 (middle), where we visualize the average regret of all the candidates trained for at least the specified number of epochs on the x-axis. In contrast to the regret plots, here we do not show the regret of the best configuration, but the mean regret of all the selected configurations. The analysis deduces a similar finding, our method DYHPO selects more qualitative hyperparameter configurations than all the baselines. An interesting property of multi-fidelity HPO is the phenomenon of poor rank correlations among the validation performance of candidates at different budgets. In other words, a configuration that achieves a poor performance at a small budget can perform better at a larger budget. To analyze this phenomenon, we measure the percentage of \"good\" configurations at a particular budget, that were \"bad\" performers in at least one of the smaller budgets. We define a \"good\" performance at a budget B when a configuration achieves a validation accuracy ranked among the top 1/3 of the validation accuracies belonging to all the other configurations that were run until that budget B. In Figure 6 (right), we analyze the percentage of \"good\" configurations at each budget denoted by the x-axis, that were \"bad\" performers in at least one of the lower budgets. Such a metric is a proxy for the degree of the promotion of \"bad\" configurations towards higher budgets. We present the analysis for all the competing methods of our experimental protocol from Section 4. We have additionally included the ground-truth line annotated as \"Baseline\", which represents the fraction of past poor performers among all the feasible configurations in the search space. In contrast, the respective methods compute the fraction of promotions only among the configurations that those methods have considered (i.e. selected within their HPO trials) until the budget indicated by the x-axis. We see that there is a high degree of \"good\" configurations that were \"bad\" at a previous budget, with fractions of the ground-truth \"Baseline\" going up to 40% for the LCBench benchmark and up to 80% for the NAS-Bench-201 benchmark. 80 10 20 30 40 50 Number of Epochs 0.0 0.1 0.2 0.3 0.4Precision of Top Candidates LCBench Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 10 20 30 40 50 Number of Epochs 0.05 0.10 0.15Average Regret LCBench Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 10 20 30 40 50 Number of Epochs 0.0 0.2 0.4 0.6 0.8 Fraction of Poor Performer Promotions LCBench Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO Baseline 0 50 100 150 200 Number of Epochs 0.05 0.10 0.15Precision of Top Candidates NAS-Bench-201 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 50 100 150 200 Number of Epochs 0.04 0.06 0.08 0.10 0.12Average Regret NAS-Bench-201 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 50 100 150 200 Number of Epochs 0.0 0.2 0.4 0.6 0.8 1.0 Fraction of Poor Performer Promotions NAS-Bench-201 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO Baseline Figure 6: The efficiency of DYHPO as the optimization progresses. Left: The fraction of top- performing candidates from all candidates that were selected to be trained. Middle: The average regret for the configurations that were selected to be trained at a given budget. Right: The percentage of configurations that belong to the top 1/3 configurations at a given budget and that were in the top bottom 2/3 of the configurations at a previous budget. All of the results are from the LCBench and NAS-Bench-201 benchmark. On the other hand, the analysis demonstrates that our method DYHPO has promoted more \"good\" configurations that were \"bad\" in a lower budget, compared to all the rival methods. In particular, more than 80% of selected configurations from the datasets belonging to either benchmark were \"bad\" performers at a lower budget. The empirical evidence validates Hypothesis 3 and demonstrates that DYHPO efficiently explores qualitative candidates. We provide the results of our analysis for DYHPO’s efficiency on the additional benchmarks (Taskset) in Appendix B. Ablating the impact of the learning curve 102 103 104 105 Training Time in Seconds 10 1 Regret ImageNet16-120 DyHPO DyHPO w/o CNN Random Search Figure 7: Ablating the impact of the learning curve on DYHPO. One of the main differences between DYHPO and similar methods Kandasamy et al. [2017], is that the learning curve is an input to the kernel function. For this reason, we investigate the impact of this design choice. We consider a variation of DYHPO w/o CNN, which is simply DYHPO without the learning curve. It is worth emphasizing that both variants (with and without the learning curve) are multi-fidelity surrogates and both receive the budget informa- tion through the inputted index j in Equation 3. The only difference is that DYHPO additionally incorporates the pattern of the learning curve. We run the ablation on the NAS-Bench-201 benchmark and report the results for ImageNet, the largest dataset in our collection. The ablation results are shown in Figure 7, while the remaining results on the other datasets are shown in Figure 8 of the appendix. Based on the results from our learning curve ablation, we conclude that the use of an explicit learning curve representation leads to significantly better results. 6 Limitations of Our Method Although DYHPO shows a convincing and statistically significant reduction of the HPO time on diverse Deep Learning (DL) experiments, we cautiously characterized our method only as a \"step 9towards\" scaling HPO for DL. The reason for our restrain is the lack of tabular benchmarks for HPO on very large deep learning models, such as Transformers-based architectures [Devlin et al., 2019]. Additionally, the pause and resume part of our training procedure can only be applied when tuning the hyperparameters of parametric models, otherwise, the training of a hyperparameter configuration would have to be restarted. Lastly, for small datasets that can be trained fast, the overhead of model-based techniques would make an approach like random search more appealing. 7 Conclusions In this work, we present DYHPO , a new Bayesian optimization (BO) algorithm for the gray-box setting. We introduced a new surrogate model for BO that uses a learnable deep kernel and takes the learning curve as an explicit input. Furthermore, we motivated a variation of expected improvement for the multi-fidelity setting. Finally, we compared our approach on diverse benchmarks on a total of 50 different tasks against the current state-of-the-art methods on gray-box hyperparameter optimization (HPO). Our method shows significant gains and has the potential to become the de facto standard for HPO in Deep Learning. Acknowledgments Josif Grabocka and Arlind Kadra would like to acknowledge the grant awarded by the Eva-Mayr-Stihl Stiftung. In addition, this research was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under grant number 417962828 and grant INST 39/963-1 FUGG (bwForCluster NEMO). In addition, Josif Grabocka acknowledges the support of the BrainLinks- BrainTools center of excellence. References Noor H. Awad, Neeratyoy Mallik, and Frank Hutter. DEHB: evolutionary hyberband for scalable, robust and efficient hyperparameter optimization. In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI 2021, Virtual Event / Montreal, Canada, 19-27 August 2021, pages 2147–2153, 2021. doi: 10.24963/ijcai.2021/296. URL https://doi.org/ 10.24963/ijcai.2021/296. Bowen Baker, Otkrist Gupta, Ramesh Raskar, and Nikhil Naik. Accelerating neural architecture search using performance prediction. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Workshop Track Proceedings, 2018. URL https://openreview.net/forum?id=HJqk3N1vG. James Bergstra, Rémi Bardenet, Yoshua Bengio, and Balázs Kégl. Algorithms for hyper-parameter optimization. In Advances in Neural Information Processing Systems 24: 25th Annual Conference on Neural Information Processing Systems 2011. Proceedings of a meeting held 12-14 December 2011, Granada, Spain, pages 2546–2554, 2011. URL https://proceedings.neurips.cc/ paper/2011/hash/86e8f7ab32cfd12577bc2619bc635690-Abstract.html. Hadrien Bertrand, Roberto Ardon, Matthieu Perrot, and Isabelle Bloch. Hyperparameter optimization of deep neural networks: Combining hyperband with bayesian model selection. In Conférence sur l’Apprentissage Automatique, 2017. Yutian Chen, Matthew W. Hoffman, Sergio Gomez Colmenarejo, Misha Denil, Timothy P. Lillicrap, Matthew Botvinick, and Nando de Freitas. Learning to learn without gradient descent by gradient descent. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pages 748–756, 2017. URL http://proceedings. mlr.press/v70/chen17e.html. Janez Demsar. Statistical comparisons of classifiers over multiple data sets. J. Mach. Learn. Res., 7: 1–30, 2006. URL http://jmlr.org/papers/v7/demsar06a.html. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of 10the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 4171–4186, 2019. doi: 10.18653/v1/n19-1423. URL https://doi.org/ 10.18653/v1/n19-1423. Tobias Domhan, Jost Tobias Springenberg, and Frank Hutter. Speeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves. In Qiang Yang and Michael J. Wooldridge, editors, Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence, IJCAI 2015, Buenos Aires, Argentina, July 25-31, 2015, pages 3460–3468. AAAI Press, 2015. URL http://ijcai.org/Abstract/15/487. Xuanyi Dong and Yi Yang. Nas-bench-201: Extending the scope of reproducible neural architecture search. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020, 2020. URL https://openreview.net/forum?id=HJxyZkBKDr. Stefan Falkner, Aaron Klein, and Frank Hutter. BOHB: robust and efficient hyperparameter optimiza- tion at scale. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, pages 1436–1445, 2018. URL http://proceedings.mlr.press/v80/falkner18a.html. Luca Franceschi, Michele Donini, Paolo Frasconi, and Massimiliano Pontil. Forward and reverse gradient-based hyperparameter optimization. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pages 1165–1173, 2017. URL http://proceedings.mlr.press/v70/franceschi17a.html. Jacob R. Gardner, Geoff Pleiss, Kilian Q. Weinberger, David Bindel, and Andrew Gordon Wil- son. Gpytorch: Blackbox matrix-matrix gaussian process inference with GPU acceleration. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural In- formation Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada , pages 7587–7597, 2018. URL https://proceedings.neurips.cc/paper/2018/hash/ 27e8e17134dd7083b050476733207ea1-Abstract.html. Matilde Gargiani, Aaron Klein, Stefan Falkner, and Frank Hutter. Probabilistic rollouts for learning curve extrapolation across hyperparameter settings. CoRR, abs/1910.04522, 2019. URL http: //arxiv.org/abs/1910.04522. Kevin G. Jamieson and Ameet Talwalkar. Non-stochastic best arm identification and hyperparameter optimization. In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, AISTATS 2016, Cadiz, Spain, May 9-11, 2016 , pages 240–248, 2016. URL http://proceedings.mlr.press/v51/jamieson16.html. Donald R. Jones, Matthias Schonlau, and William J. Welch. Efficient global optimization of ex- pensive black-box functions. J. Global Optimization , 13(4):455–492, 1998. doi: 10.1023/A: 1008306431147. URL https://doi.org/10.1023/A:1008306431147. Kirthevasan Kandasamy, Gautam Dasarathy, Junier B. Oliva, Jeff G. Schneider, and Barn- abás Póczos. Gaussian process bandit optimisation with multi-fidelity evaluations. In Advances in Neural Information Processing Systems 29: Annual Conference on Neu- ral Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain , pages 992–1000, 2016. URL https://proceedings.neurips.cc/paper/2016/hash/ 605ff764c617d3cd28dbbdd72be8f9a2-Abstract.html. Kirthevasan Kandasamy, Gautam Dasarathy, Jeff G. Schneider, and Barnabás Póczos. Multi-fidelity bayesian optimisation with continuous approximations. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pages 1799–1808, 2017. URL http://proceedings.mlr.press/v70/kandasamy17a.html. Kirthevasan Kandasamy, Willie Neiswanger, Jeff Schneider, Barnabás Póczos, and Eric P. Xing. Neural architecture search with bayesian optimisation and optimal transport. In Ad- vances in Neural Information Processing Systems 31: Annual Conference on Neural Infor- mation Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada , pages 2020–2029, 2018. URL https://proceedings.neurips.cc/paper/2018/hash/ f33ba15effa5c10e873bf3842afb46a6-Abstract.html. 11Kirthevasan Kandasamy, Karun Raju Vysyaraju, Willie Neiswanger, Biswajit Paria, Christopher R. Collins, Jeff Schneider, Barnabás Póczos, and Eric P. Xing. Tuning hyperparameters without grad students: Scalable and robust bayesian optimisation with dragonfly. J. Mach. Learn. Res., 21: 81:1–81:27, 2020. URL http://jmlr.org/papers/v21/18-223.html. Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1412.6980. Aaron Klein, Stefan Falkner, Simon Bartels, Philipp Hennig, and Frank Hutter. Fast bayesian optimization of machine learning hyperparameters on large datasets. In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, AISTATS 2017, 20-22 April 2017, Fort Lauderdale, FL, USA, pages 528–536, 2017a. URL http://proceedings.mlr.press/ v54/klein17a.html. Aaron Klein, Stefan Falkner, Jost Tobias Springenberg, and Frank Hutter. Learning curve prediction with bayesian neural networks. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings , 2017b. URL https://openreview.net/forum?id=S11KBYclx. Liam Li, Kevin G. Jamieson, Afshin Rostamizadeh, Ekaterina Gonina, Jonathan Ben-tzur, Moritz Hardt, Benjamin Recht, and Ameet Talwalkar. A system for massively parallel hyperparameter tuning. In Inderjit S. Dhillon, Dimitris S. Papailiopoulos, and Vivienne Sze, editors,Proceedings of Machine Learning and Systems 2020, MLSys 2020, Austin, TX, USA, March 2-4, 2020. mlsys.org, 2020a. URL https://proceedings.mlsys.org/book/303.pdf. Lisha Li, Kevin G. Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. Hyper- band: A novel bandit-based approach to hyperparameter optimization. J. Mach. Learn. Res., 18: 185:1–185:52, 2017. URL http://jmlr.org/papers/v18/16-558.html. Shibo Li, Wei Xing, Robert M. Kirby, and Shandian Zhe. Multi-fidelity bayesian optimization via deep neural networks. In Advances in Neural Information Processing Systems 33: An- nual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual , 2020b. URL https://proceedings.neurips.cc/paper/2020/hash/ 60e1deb043af37db5ea4ce9ae8d2c9ea-Abstract.html. Jonathan Lorraine, Paul Vicol, and David Duvenaud. Optimizing millions of hyperparameters by implicit differentiation. In The 23rd International Conference on Artificial Intelligence and Statistics, AISTATS 2020, 26-28 August 2020, Online [Palermo, Sicily, Italy], pages 1540–1552, 2020. URL http://proceedings.mlr.press/v108/lorraine20a.html. Dougal Maclaurin, David Duvenaud, and Ryan P. Adams. Gradient-based hyperparameter opti- mization through reversible learning. In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015 , pages 2113–2122, 2015. URL http://proceedings.mlr.press/v37/maclaurin15.html. Pedro Mendes, Maria Casimiro, Paolo Romano, and David Garlan. Trimtuner: Efficient optimization of machine learning jobs in the cloud via sub-sampling. In 28th International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems, MASCOTS 2020, Nice, France, November 17-19, 2020, pages 1–8. IEEE, 2020. doi: 10.1109/MASCOTS50786. 2020.9285971. URL https://doi.org/10.1109/MASCOTS50786.2020.9285971. Pedro Mendes, Maria Casimiro, and Paolo Romano. Hyperjump: Accelerating hyperband via risk modelling. CoRR, abs/2108.02479, 2021. URL https://arxiv.org/abs/2108.02479. Luke Metz, Niru Maheswaranathan, Ruoxi Sun, C. Daniel Freeman, Ben Poole, and Jascha Sohl- Dickstein. Using a thousand optimization tasks to learn hyperparameter search strategies. CoRR, abs/2002.11887, 2020. URL https://arxiv.org/abs/2002.11887. Jack Parker-Holder, Vu Nguyen, and Stephen J. Roberts. Provably efficient online hyperparameter optimization with population-based bandits. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/ hash/c7af0926b294e47e52e46cfebe173f20-Abstract.html. 12Valerio Perrone, Rodolphe Jenatton, Matthias W. Seeger, and Cédric Archambeau. Scalable hyper- parameter transfer learning. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada, pages 6846–6856, 2018. URL https://proceedings.neurips.cc/ paper/2018/hash/14c879f3f5d8ed93a09f6090d77c2cc3-Abstract.html. Matthias Poloczek, Jialei Wang, and Peter I. Frazier. Multi-information source optimiza- tion. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA , pages 4288–4298, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/ df1f1d20ee86704251795841e6a9405a-Abstract.html. Akshara Rai, Ruta Desai, and Siddharth Goyal. Bayesian optimization with a neural network kernel, 2016. URL http://www.cs.cmu.edu/~rutad/files/BO_NN.pdf. Jasper Snoek, Hugo Larochelle, and Ryan P. Adams. Practical bayesian optimization of machine learn- ing algorithms. In Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United States, pages 2960–2968, 2012. URL https://proceedings. neurips.cc/paper/2012/hash/05311655a15b75fab86956663e1819cd-Abstract.html. Niranjan Srinivas, Andreas Krause, Sham M. Kakade, and Matthias W. Seeger. Gaussian process optimization in the bandit setting: No regret and experimental design. In Proceedings of the 27th International Conference on Machine Learning (ICML-10), June 21-24, 2010, Haifa, Israel, pages 1015–1022, 2010. URL https://icml.cc/Conferences/2010/papers/422.pdf. Kevin Swersky, Jasper Snoek, and Ryan Prescott Adams. Multi-task bayesian optimization. In Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States, pages 2004–2012, 2013. URL https://proceedings.neurips. cc/paper/2013/hash/f33ba15effa5c10e873bf3842afb46a6-Abstract.html. Kevin Swersky, Jasper Snoek, and Ryan Prescott Adams. Freeze-thaw bayesian optimization. CoRR, abs/1406.3896, 2014. URL http://arxiv.org/abs/1406.3896. Shion Takeno, Hitoshi Fukuoka, Yuhki Tsukada, Toshiyuki Koyama, Motoki Shiga, Ichiro Takeuchi, and Masayuki Karasuyama. Multi-fidelity bayesian optimization with max-value entropy search and its parallelization. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, pages 9334–9345, 2020. URLhttp://proceedings. mlr.press/v119/takeno20a.html. Jiazhuo Wang, Jason Xu, and Xuejun Wang. Combination of hyperband and bayesian optimization for hyperparameter optimization in deep learning. CoRR, abs/1801.01596, 2018. URL http: //arxiv.org/abs/1801.01596. Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, and Eric P. Xing. Deep kernel learning. In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, AISTATS 2016, Cadiz, Spain, May 9-11, 2016, pages 370–378, 2016. URL http://proceedings.mlr. press/v51/wilson16.html. Martin Wistuba. Bayesian optimization combined with incremental evaluation for neural network architecture optimization. In AutoML@PKDD/ECML, 2017. Martin Wistuba and Josif Grabocka. Few-shot bayesian optimization with deep kernel surrogates. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021, 2021. URL https://openreview.net/forum?id=bJxgv5C3sYc. Martin Wistuba and Tejaswini Pedapati. Learning to rank learning curves. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 10303–10312. PMLR, 2020. URL http://proceedings.mlr.press/v119/wistuba20a.html. Lucas Zimmer, Marius Lindauer, and Frank Hutter. Auto-pytorch: Multi-fidelity metalearning for efficient and robust autodl. IEEE Trans. Pattern Anal. Mach. Intell., 43(9):3079–3090, 2021. doi: 10.1109/TPAMI.2021.3067763. URL https://doi.org/10.1109/TPAMI.2021.3067763. 13Checklist 1. For all authors... (a) Do the main claims made in the abstract and introduction accurately reflect the paper’s contributions and scope? [Yes] (b) Did you describe the limitations of your work? [Yes] See Section 6. (c) Did you discuss any potential negative societal impacts of your work? [Yes] See Section “Societal Implications” in the Appendix. (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] 2. If you are including theoretical results... (a) Did you state the full set of assumptions of all theoretical results? [N/A] (b) Did you include complete proofs of all theoretical results? [N/A] 3. If you ran experiments... (a) Did you include the code, data, and instructions needed to reproduce the main experi- mental results (either in the supplemental material or as a URL)? [No] We provide our main algorithm in Section 3 and we additionally provide the detailed implementation details in Appendix A for all methods and benchmarks. We will release the code for the camera-ready version of our work. (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] Please see Appendix A. (c) Did you report error bars (e.g., with respect to the random seed after running experi- ments multiple times)? [Yes] We report the statistical significance of the performance difference between our method and the baselines in Section 5 (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See Section 4.1. 4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... (a) If your work uses existing assets, did you cite the creators? [Yes] See Section 4.2 and Section 4.3. (b) Did you mention the license of the assets? [Yes] See Appendix A.1 and A.5 where we provide references to the assets where the license is included. (c) Did you include any new assets either in the supplemental material or as a URL? [No] (d) Did you discuss whether and how consent was obtained from people whose data you’re using/curating? [N/A] The benchmarks and baselines are open-sourced. (e) Did you discuss whether the data you are using/curating contains personally identi- fiable information or offensive content? [N/A] The data does not contain personally identifiable information or offensive content. 5. If you used crowdsourcing or conducted research with human subjects... (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A] 14Societal Implications In our work, we use only publicly available data with no privacy concerns. Furthermore, our algorithm reduces the overall time for fitting deep networks, therefore, saving computational resources and yielding a positive impact on the environment. Moreover, our method can help smaller research organizations with limited access to resources to be competitive in the deep learning domain, which reduces the investment costs on hardware. Although our method significantly reduces the time taken for optimizing a machine learning algorithm that achieves peak performance, we warn against running our method for an extended time only to achieve marginal gains in performance, unless it is mission-critical. Last but not least, in order to save energy, we invite the community to create sparse benchmarks with surrogates, instead of dense tabular ones. A Experimental Setup A.1 Benchmarks LCBench. LCBench4 is a feedforward neural network benchmark on tabular data which consists of 2000 configuration settings for each of the 35 datasets. The configurations were evaluated during HPO runs with AutoPyTorch. LCBench features a search space of 7 numerical hyperparameters, where every hyperparameter configuration is trained for 50 epochs. The objective is to optimize seven different hyperparameters of funnel-shaped neural networks, i.e., batch size, learning rate, momentum, weight decay, dropout, number of layers, and maximum number of units per layer. TaskSet. TaskSet5 is a benchmark that features over 1162 diverse tasks from different domains and includes 5 search spaces. In this work, we focus on NLP tasks and we use the Adam8p search space with 8 continuous hyperparameters. We refer to Figure 11 for the exact task names considered in our experiments. The learning curves provided in TaskSet report scores after every 200 iterations. We refer to those as \"steps\". The objective is to optimize eight hyperparameters for a set of different recurrent neural networks (RNN) that differ in embedding size, RNN cell, and other architectural features. The set of hyperparameters consists of optimizer-specific hyperparameters, such as the learning rate, the exponential decay rate of the first and second momentum of Adam, β1 and β2, and Adam’s constant for numerical stability ε. Furthermore, there are two hyperparameters controlling linear and exponential learning rate decays, as well as L1 and L2 regularization terms. NAS-Bench-201. NAS-Bench-2016 is a benchmark that has precomputed about 15,600 archi- tectures trained for 200 epochs for the image classification datasets CIFAR-10, CIFAR-100, and ImageNet. The objective is to select for each of the six operations within the cell of the macro architecture one of five different operations. All other hyperparameters such as learning rate and batch size are kept fixed. NAS-Bench-201 features a search space of 6 categorical hyperparameters and each architecture is trained for 200 epochs. A.2 Preprocessing In the following, we describe the preprocessing applied to the hyperparameter representation. For LCBench, we apply a log-transform to batch size, learning rate, and weight decay. For TaskSet, we apply it on the learning rate, L1 and L2 regularization terms, epsilon, linear and exponential decay of the learning rate. All continuous hyperparameters are scaled to the range between 0 and 1 using sklearn’s MinMaxScaler. If not mentioned otherwise, we use one-hot encoding for the categorical hyperparameters. As detailed in subsection A.5, some baselines have a specific way of dealing with them. In that case, we use the method recommended by the authors. A.3 Framework The framework contains the evaluated hyperparameters and their corresponding validation curves. The list of candidate hyperparameters is passed to the baseline-specific interface, which in turn, 4https://github.com/automl/LCBench 5https://github.com/google-research/google-research/tree/master/task_set 6https://github.com/D-X-Y/NAS-Bench-201 15optimizes and queries the framework for the hyperparameter configuration that maximizes utility. Our framework in turn responds with the validation curve and the cost of the evaluation. In case a hyperparameter configuration has been evaluated previously up to a budget b and a baseline requires the response for budget b + 1, the cost is calculated accordingly only for the extra budget requested. A.4 Implementation Details We implement the Deep Kernel Gaussian Process using GPyTorch 1.5 [Gardner et al., 2018]. We use an RBF kernel and the dense layers of the transformation functionφ have 128 and 256 units. We used a convolutional layer with a kernel size of three and four filters. All parameters of the Deep Kernel are estimated by maximizing the marginal likelihood. We achieve this by using gradient ascent and Adam [Kingma and Ba, 2015] with a learning rate of 0.1 and batch size of 64. We stop training as soon as the training likelihood is not improving for 10 epochs in a row or we completed 1,000 epochs. For every new data point, we start training the GP with its old parameters to reduce the required effort for training. A.5 Baselines Random Search & Hyperband. Random search and Hyperband sample hyperparameter config- urations at random and therefore the preprocessing is irrelevant. We have implemented both from scratch and use the recommended hyperparameters for Hyperband, i.e. η = 3. BOHB. For our experiments with BOHB, we use version 0.7.4 of the officially-released code7. DEHB. For our experiments with DEHB, we use the official public implementation8. We devel- oped an interface that communicates between our framework and DEHB. In addition to the initial preprocessing common for all methods, we encode categorical hyperparameters with a numerical value in the interval [0, 1]. For a categorical hyperparameter xi, we take Ni equal-sized intervals, where Ni represents the number of unique categorical values for hyperparameter xi and we assign the value for a categorical value n ∈ Ni to the middle of the interval [n, n+ 1]as suggested by the authors. For configuring the DEHB algorithm we used the default values from the library. Dragonfly. We use the publicly available code of Dragonfly9. No special treatment of categorical hyperparameters is required since Dragonfly has its own way to deal with them. We use version 0.1.6 with default settings. MF-DNN. We use the official implementation of MF-DNN by the authors10. Initially, we tried to use multiple incremental fidelity levels like for DYHPO, however, the method runtime was too high and it could not achieve competitive results. For that reason, we use only a few fidelity levels like the authors do in their work Li et al. [2020b]. We use the same fidelity levels as for Hyperband, DEHB, and BOHB to have a fair comparison between the baselines. We also use the same number of initial points as for the other methods to have the same maximal resource allocated for every fidelity level. ASHA-HB. We use the public implementation from the well-known optuna library (version2.10.0). We used the same eta, minimum and maximal budget as for HB, DEHB, and BOHB in our experi- ments, to have a fair comparison. B Additional Plots In Figure 8, we ablate the learning curve input in our kernel, to see the effect it has on performance for the CIFAR-10 and CIFAR-100 datasets from the NAS-Bench-201 benchmark. The results indicate that the learning curve plays an important role in achieving better results by allowing faster convergence and a better anytime performance. 7https://github.com/automl/HpBandSter 8https://github.com/automl/DEHB/ 9https://github.com/dragonfly/dragonfly 10https://github.com/shib0li/DNN-MFBO 16102 103 104 105 Training Time in Seconds 10 2 10 1 Regret cifar10 DyHPO DyHPO w/o CNN Random Search 102 103 104 105 Training Time in Seconds 10 2 10 1 Regret cifar100 DyHPO DyHPO w/o CNN Random Search Figure 8: The learning curve ablation for the CIFAR-10 and CIFAR-100 tasks of NAS-Bench-201. 0 1000 2000 3000 4000 Number of Epochs 10 2 10 1 Regret cifar10 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 1000 2000 3000 4000 Number of Epochs 10 2 10 1 Regret cifar100 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO Figure 9: NAS-Bench-201 regret results over the number of epochs spent during the optimization. Additionally, in Figure 9, we show the performance comparison over the number of epochs of every method for the CIFAR-10 and CIFAR-100 datasets in the NAS-Bench-201 benchmark. While, in Figure 10, we present the performance comparison over time. As can be seen, DYHPO converges faster and has a better performance compared to the other methods over the majority of the time or steps, however, towards the end although it is the optimal method or close to the optimal method, the difference in regret is not significant anymore. Furthermore, Figure 11 shows the performance comparison for the datasets chosen from TaskSet over the number of steps. Looking at the results, DYHPO is outperforming all methods convincingly on the majority of datasets by converging faster and with significant differences in the regret evaluation metric. In Figure 12 and 13, we show the performance comparison for all the datasets from LCBench regarding regret over the number of epochs. Similarly, in Figure 14 and 15, we show the same performance comparison, however, over time. As can be seen, DYHPO manages to outperform the other competitors in the majority of the datasets, and in the datasets that it does not, it is always close to the top-performing method, and the difference between methods is marginal. In Figure 16 we provide the extended results of Experiment 3 for TaskSet. We show the precision, average regret, and promotion percentage for poor-performing configurations for DYHPO and the other competitor methods. Lastly, we explore the behavior of DYHPO after finding the configuration which is returned at the end of the optimization as the best configuration. In Figure 17, we show how the budget is distributed on the configurations considered during that part of the optimization. Clearly, DYHPO is spending very little budget on most configurations. Furthermore, we investigated how many new configurations are considered during this phase. For LCBench, 76.98% of considered configurations are new demonstrating that DYHPO is investigating most of the budget into exploration. These are even more extreme for TaskSet (93.16% and NAS-Bench-201 (97.51%). 17102 103 104 Wallclock Time in Seconds 10 2 10 1 Regret cifar10 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 102 103 104 Wallclock Time in Seconds 10 2 10 1 Regret cifar100 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO Figure 10: NAS-Bench-201 regret results over the total optimization time. The total time includes the method overhead time and the hyperparameter configuration evaluation time. 0 200 400 600 800 1000 Number of Steps 10 3 10 2 10 1 Regret FixedTextRNNClassification imdb_patch128_LSTM128_avg_bs64 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Steps 10 3 10 2 10 1 Regret FixedTextRNNClassification imdb_patch128_LSTM128_bs64 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Steps 10 1 Regret FixedTextRNNClassification imdb_patch128_LSTM128_embed128_bs64 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Steps 10 1 6 × 10 2 2 × 10 1 3 × 10 1 Regret FixedTextRNNClassification imdb_patch32_GRU128_bs128 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Steps 10 1 Regret FixedTextRNNClassification imdb_patch32_GRU64_avg_bs128 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Steps 10 1 Regret FixedTextRNNClassification imdb_patch32_IRNN64_relu_avg_bs128 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Steps 10 1 100 Regret FixedTextRNNClassification imdb_patch32_IRNN64_relu_last_bs128 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Steps 10 2 10 1 Regret FixedTextRNNClassification imdb_patch32_LSTM128_E128_bs128 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Steps 10 2 10 1 Regret FixedTextRNNClassification imdb_patch32_LSTM128_bs128 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Steps 10 1 2 × 10 1 Regret FixedTextRNNClassification imdb_patch32_VRNN128_tanh_bs128 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Steps 10 1 Regret FixedTextRNNClassification imdb_patch32_VRNN64_relu_avg_bs128 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Steps 10 3 10 2 10 1 Regret FixedTextRNNClassification imdb_patch32_VRNN64_tanh_avg_bs128 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO Figure 11: Performance comparison over the number of steps on a dataset level for TaskSet. 180 200 400 600 800 1000 Number of Epochs 10 2 Regret APSFailure Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 1 3 × 10 2 4 × 10 2 6 × 10 2 Regret Amazon_employee_access Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 3 10 2 10 1 Regret Australian Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 3 10 2 10 1 Regret Fashion-MNIST Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret KDDCup09_appetency Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 4 10 3 10 2 10 1 Regret MiniBooNE Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 3 10 2 10 1 Regret adult Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 3 10 2 Regret airlines Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 3 10 2 10 1 Regret albert Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret bank-marketing Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret blood-transfusion-service-center Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 1 Regret car Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret christine Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 100 Regret cnae-9 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret connect-4 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 4 10 3 10 2 10 1 Regret covertype Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 1 Regret credit-g Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 4 10 2 100 Regret dionis Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO Figure 12: Performance comparison over the number of steps on a dataset level for LCBench. 190 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret fabert Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 1 Regret helena Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret higgs Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret jannis Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret jasmine Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 4 10 3 10 2 10 1 Regret jungle_chess_2pcs_raw_endgame_complete Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 1 Regret kc1 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret kr-vs-kp Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret mfeat-factors Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 4 10 3 10 2 10 1 Regret nomao Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 Regret numerai28.6 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret phoneme Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 1 Regret segment Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 1 Regret shuttle Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 4 10 2 Regret sylvine Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 3 10 2 10 1 Regret vehicle Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Number of Epochs 10 2 10 1 Regret volkert Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO Figure 13: Performance comparison over the number of steps on a dataset level for LCBench (cont.). 200 250 500 750 1000 1250 Wallclock Time in Seconds 10 2 Regret APSFailure Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 500 1000 1500 Wallclock Time in Seconds 10 1 3 × 10 2 4 × 10 2 6 × 10 2 Regret Amazon_employee_access Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 100 200 300 Wallclock Time in Seconds 10 3 10 2 10 1 Regret Australian Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 1000 2000 3000 4000 5000 Wallclock Time in Seconds 10 2 10 1 Regret Fashion-MNIST Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 500 1000 1500 2000 2500 Wallclock Time in Seconds 10 2 10 1 Regret KDDCup09_appetency Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 500 1000 1500 Wallclock Time in Seconds 10 1 Regret MiniBooNE Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 500 1000 1500 2000 Wallclock Time in Seconds 10 3 10 2 10 1 Regret adult Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 2500 5000 7500 10000 12500 Wallclock Time in Seconds 10 3 10 2 Regret airlines Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 2000 4000 6000 8000 10000 Wallclock Time in Seconds 10 2 Regret albert Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 500 1000 1500 2000 Wallclock Time in Seconds 10 2 10 1 Regret bank-marketing Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Wallclock Time in Seconds 10 2 10 1 Regret blood-transfusion-service-center Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Wallclock Time in Seconds 10 1 Regret car Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 5000 10000 15000 Wallclock Time in Seconds 10 2 10 1 Regret christine Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 500 1000 1500 2000 2500 Wallclock Time in Seconds 10 2 10 1 100 Regret cnae-9 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 500 1000 1500 2000 2500 Wallclock Time in Seconds 10 2 10 1 Regret connect-4 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 1000 2000 3000 4000 5000 Wallclock Time in Seconds 10 1 Regret covertype Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Wallclock Time in Seconds 10 1 Regret credit-g Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 2000 4000 6000 8000 Wallclock Time in Seconds 10 1 Regret dionis Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO Figure 14: Performance comparison over time on a dataset level for LCBench with the overhead included. 210 1000 2000 3000 Wallclock Time in Seconds 10 2 10 1 Regret fabert Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Wallclock Time in Seconds 10 1 Regret helena Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Wallclock Time in Seconds 10 1 Regret higgs Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 1000 2000 3000 Wallclock Time in Seconds 10 2 10 1 Regret jannis Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 250 500 750 1000 1250 Wallclock Time in Seconds 10 2 10 1 Regret jasmine Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 500 1000 1500 2000 Wallclock Time in Seconds 10 2 10 1 Regret jungle_chess_2pcs_raw_endgame_complete Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Wallclock Time in Seconds 10 1 Regret kc1 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 250 500 750 1000 Wallclock Time in Seconds 10 2 10 1 Regret kr-vs-kp Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 250 500 750 1000 1250 Wallclock Time in Seconds 10 2 10 1 Regret mfeat-factors Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 500 1000 1500 2000 Wallclock Time in Seconds 10 2 10 1 Regret nomao Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Wallclock Time in Seconds 10 2 Regret numerai28.6 Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 250 500 750 1000 1250 Wallclock Time in Seconds 10 2 10 1 Regret phoneme Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Wallclock Time in Seconds 10 1 Regret segment Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 500 1000 1500 2000 Wallclock Time in Seconds 10 1 Regret shuttle Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 250 500 750 1000 1250 Wallclock Time in Seconds 10 4 10 2 Regret sylvine Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 200 400 600 800 1000 Wallclock Time in Seconds 10 2 10 1 Regret vehicle Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 1000 2000 3000 Wallclock Time in Seconds 10 2 10 1 Regret volkert Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO Figure 15: Performance comparison over time on a dataset level for LCBench with the overhead included. (cont.). 220 10 20 30 40 50 Number of Epochs 0.0 0.1 0.2 0.3Precision of Top Candidates Taskset Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 10 20 30 40 50 Number of Epochs 0.10 0.15 0.20 0.25 0.30Average Regret Taskset Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO 0 10 20 30 40 50 Number of Steps 0.0 0.2 0.4 0.6 0.8 Fraction of Poor Performer Promotions Taskset Random Hyperband BOHB DEHB Dragonfly ASHA MF-DNN DyHPO Baseline Figure 16: The efficiency of DYHPO as the optimization progresses. Left: The fraction of top- performing candidates from all candidates that were selected to be trained. Middle: The average regret for the configurations that were selected to be trained at a given budget. Right: The percentage of configurations that belong to the top 1/3 configurations at a given budget and that were in the top bottom 2/3 of the configurations at a previous budget. All of the results are from the Taskset benchmark. 0 10 20 30 40 50 Epochs 0.0 0.2 0.4 0.6Relative Number of Candidates LCBench 0 50 100 150 200 Epochs 0.0 0.2 0.4 0.6 0.8Relative Number of Candidates NAS-Bench-201 0 10 20 30 40 50 Steps 0.0 0.2 0.4 0.6 0.8Relative Number of Candidates TaskSet Figure 17: These plots shed light on how DYHPO behaves after the configuration it finally returns as the best. The plots show how many epochs are spent per candidate. As we can see, for most candidates only a small budget was considered, indicating that DYHPO is mostly exploring at this point. 23",
      "meta_data": {
        "arxiv_id": "2202.09774v2",
        "authors": [
          "Martin Wistuba",
          "Arlind Kadra",
          "Josif Grabocka"
        ],
        "published_date": "2022-02-20T10:28:02Z",
        "pdf_url": "https://arxiv.org/pdf/2202.09774v2.pdf",
        "github_url": "https://github.com/releaunifreiburg/DyHPO"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces DyHPO, a Bayesian Optimization (BO) method for multi-fidelity hyperparameter optimization (HPO) that dynamically decides which hyperparameter configuration to train further. Its main contributions include a novel Bayesian surrogate model for gray-box HPO that predicts validation scores based on hyperparameter configuration, budget information, and the learning curve; a robust integration of this surrogate with Bayesian optimization; and demonstrating its superior efficiency and state-of-the-art results against seven strong HPO baselines across diverse Deep Learning architectures and 50 datasets.",
        "methodology": "DyHPO is a Bayesian Optimization method based on Gaussian Processes (GP). It proposes a deep kernel for GPs that embeds the learning curve dynamics, capturing the similarity of hyperparameter configurations evaluated at different budgets. The feature extractor (φ) for the kernel uses a neural network composed of linear and convolutional layers, taking hyperparameter configuration, normalized budget, and the past learning curve as input. An acquisition function, Multi-Fidelity Expected Improvement (EIMF), is introduced, which extends Expected Improvement to the multi-budget case by dynamically adjusting the incumbent configuration based on the budget. The algorithm dynamically selects the most promising candidate to train for a small additional budget (incrementally increasing budget by one epoch/step) at each HPO step, updating the surrogate model after observing the new data point.",
        "experimental_setup": "DyHPO was evaluated on three diverse deep learning architectures (MLP, CNN/NAS, RNN) and 50 datasets across three modalities (tabular, image, NLP). Benchmarks used include LCBench (35 tabular datasets, 2,000 neural networks, 50 epochs), TaskSet (12 NLP tasks, Adam8p search space, RNNs, scores every 200 iterations), and NAS-Bench-201 (15,625 architectures for CIFAR-10, CIFAR-100, ImageNet, 200 epochs). Experiments were run on Amazon EC2 M5 Instances (m5.xlarge), with results reported as the mean of ten repetitions using regret and average rank as metrics. Baselines included Random Search, HyperBand, BOHB, DEHB, ASHA, MF-DNN, and Dragonfly (BOCA). Preprocessing involved log-transformations for certain hyperparameters and Min-Max scaling for continuous ones, with one-hot encoding or specific numerical encoding for categorical hyperparameters depending on the baseline.",
        "limitations": "The method's characterization as a 'step towards' scaling HPO for DL is cautious due to the lack of tabular benchmarks for very large deep learning models (e.g., Transformer-based architectures). The 'pause and resume' training procedure is only applicable to parametric models, requiring restarts for other types. For small datasets that train quickly, the overhead of model-based techniques like DyHPO makes simpler approaches like random search more appealing.",
        "future_research_directions": "Implicitly, the paper suggests future work on adapting DyHPO for very large deep learning models (like Transformers), addressing the limitations of 'pause and resume' for non-parametric models, and potentially exploring ways to reduce overhead for fast-training tasks. The authors also invite the community to create sparse benchmarks with surrogates to save energy, implying research into more efficient benchmark design.",
        "experimental_code": "import copy\nimport json\nimport logging\nimport math\nimport os\nimport time\nfrom typing import Dict, List, Optional, Tuple\n\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nfrom scipy.stats import norm, t\nimport torch\n\nfrom surrogate_models.dyhpo import DyHPO\n\n\nclass DyHPOAlgorithm:\n\n    def __init__(\n        self,\n        hp_candidates: np.ndarray,\n        log_indicator: List,\n        seed: int = 11,\n        max_benchmark_epochs: int = 52,\n        fantasize_step: int = 1,\n        minimization: bool = True,\n        total_budget: int = 500,\n        device: str = None,\n        dataset_name: str = 'unknown',\n        output_path: str = '.',\n        surrogate_config: dict = None,\n        verbose: bool = True,\n    ):\n        \"\"\"\n        Args:\n            hp_candidates: np.ndarray\n                The full list of hyperparameter candidates for\n                a given dataset.\n            log_indicator: List\n                A list with boolean values indicating if a\n                hyperparameter has been log sampled or not.\n            seed: int\n                The seed that will be used for the surrogate.\n            max_benchmark_epochs: int\n                The maximal budget that a hyperparameter configuration\n                has been evaluated in the benchmark for.\n            fantasize_step: int\n                The number of steps for which we are looking ahead to\n                evaluate the performance of a hpc.\n            minimization: bool\n                If the objective should be maximized or minimized.\n            total_budget: int\n                The total budget given for hyperparameter optimization.\n            device: str\n                The device where the experiment will be run on.\n            dataset_name: str\n                The name of the dataset that the experiment will be run on.\n            output_path: str\n                The path where all the output will be stored.\n            surrogate_config: dict\n                The model configurations for the surrogate.\n            verbose: boolean\n                If detailed information is preferred in the log file.\n        \"\"\"\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        torch.manual_seed(seed)\n        np.random.seed(seed)\n\n        if device is None:\n            self.dev = torch.device(\n                'cuda') if torch.cuda.is_available() else torch.device('cpu')\n        else:\n            self.dev = torch.device(device)\n\n        self.hp_candidates = hp_candidates\n        self.log_indicator = log_indicator\n\n        self.scaler = MinMaxScaler()\n        self.hp_candidates = self.preprocess_hp_candidates()\n\n        self.minimization = minimization\n        self.seed = seed\n\n        if verbose:\n            logging_level = logging.DEBUG\n        else:\n            logging_level = logging.INFO\n        self.logger = logging.getLogger()\n\n        logging.basicConfig(\n            format='%(levelname)s:%(asctime)s:%(message)s',\n            filename=f'dyhpo_surrogate_{dataset_name}_{seed}.log',\n            level=logging_level,\n        )\n\n        # the keys will be hyperparameter indices while the value\n        # will be a list with all the budgets evaluated for examples\n        # and with all performances for the performances\n        self.examples = dict()\n        self.performances = dict()\n\n        # set a seed already, so that it is deterministic when\n        # generating the seeds of the ensemble\n        torch.manual_seed(seed)\n        np.random.seed(seed)\n\n        self.max_benchmark_epochs = max_benchmark_epochs\n        self.total_budget = total_budget\n        self.fantasize_step = fantasize_step\n        self.nr_features = self.hp_candidates.shape[1]\n\n        initial_configurations_nr = 1\n        conf_individual_budget = 1\n        self.init_conf_indices = np.random.choice(self.hp_candidates.shape[0], initial_configurations_nr, replace=False)\n        self.init_budgets = [conf_individual_budget] * initial_configurations_nr\n        # with what percentage configurations will be taken randomly instead of being sampled from the model\n        self.fraction_random_configs = 0.1\n\n        self.model = None\n        # An index keeping track of where we are in the init_conf_indices\n        # list of hyperparmeters that are not sampled from the model.\n        self.initial_random_index = 0\n\n        if surrogate_config is None:\n            self.surrogate_config = {\n                'nr_layers': 2,\n                'nr_initial_features': self.nr_features,\n                'layer1_units': 64,\n                'layer2_units': 128,\n                'cnn_nr_channels': 4,\n                'cnn_kernel_size': 3,\n                'batch_size': 64,\n                'nr_epochs': 1000,\n                'nr_patience_epochs': 10,\n                'learning_rate': 0.001,\n            }\n        else:\n            self.surrogate_config = surrogate_config\n\n        # the incumbent value observed during the hpo process.\n        self.best_value_observed = np.NINF\n        # a set which will keep track of the hyperparameter configurations that diverge.\n        self.diverged_configs = set()\n\n        # info dict to drop every surrogate iteration\n        self.info_dict = dict()\n\n        # the start time for the overhead of every surrogate optimization iteration\n        # will be recorded here\n        self.suggest_time_duration = 0\n        # the total budget consumed so far\n        self.budget_spent = 0\n\n        self.output_path = output_path\n        self.dataset_name = dataset_name\n\n        self.no_improvement_threshold = int(self.max_benchmark_epochs + 0.2 * self.max_benchmark_epochs)\n        self.no_improvement_patience = 0\n\n\n    def _prepare_dataset_and_budgets(self) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Prepare the data that will be the input to the surrogate.\n\n        Returns:\n            data: A Dictionary that contains inside the training examples,\n            the budgets, the curves and lastly the labels.\n        \"\"\"\n\n        train_examples, train_labels, train_budgets, train_curves = self.history_configurations()\n\n        train_examples = np.array(train_examples, dtype=np.single)\n        train_labels = np.array(train_labels, dtype=np.single)\n        train_budgets = np.array(train_budgets, dtype=np.single)\n        train_curves = self.patch_curves_to_same_length(train_curves)\n        train_curves = np.array(train_curves, dtype=np.single)\n\n        # scale budgets to [0, 1]\n        train_budgets = train_budgets / self.max_benchmark_epochs\n\n        train_examples = torch.tensor(train_examples)\n        train_labels = torch.tensor(train_labels)\n        train_budgets = torch.tensor(train_budgets)\n        train_curves = torch.tensor(train_curves)\n\n        train_examples = train_examples.to(device=self.dev)\n        train_labels = train_labels.to(device=self.dev)\n        train_budgets = train_budgets.to(device=self.dev)\n        train_curves = train_curves.to(device=self.dev)\n\n        data = {\n            'X_train': train_examples,\n            'train_budgets': train_budgets,\n            'train_curves': train_curves,\n            'y_train': train_labels,\n        }\n\n        return data\n\n    def _train_surrogate(self):\n        \"\"\"\n        Train the surrogate model.\n        \"\"\"\n        data = self._prepare_dataset_and_budgets()\n        self.logger.info(f'Started training the model')\n\n        self.model.train_pipeline(\n            data,\n            load_checkpoint=False,\n        )\n\n    def _predict(self) -> Tuple[np.ndarray, np.ndarray, List, List]:\n        \"\"\"\n        Predict the performances of the hyperparameter configurations\n        as well as the standard deviations based on the surrogate model.\n\n        Returns:\n            mean_predictions, std_predictions, hp_indices, non_scaled_budgets:\n                The mean predictions and the standard deviations over\n                all model predictions for the given hyperparameter\n                configurations with their associated indices, scaled and\n                non-scaled budgets.\n        \"\"\"\n        configurations, hp_indices, budgets, learning_curves = self.generate_candidate_configurations()\n        budgets = np.array(budgets, dtype=np.single)\n        non_scaled_budgets = copy.deepcopy(budgets)\n        # scale budgets to [0, 1]\n        budgets = budgets / self.max_benchmark_epochs\n\n        configurations = np.array(configurations, dtype=np.single)\n        configurations = torch.tensor(configurations)\n        configurations = configurations.to(device=self.dev)\n\n        budgets = torch.tensor(budgets)\n        budgets = budgets.to(device=self.dev)\n\n        learning_curves = self.patch_curves_to_same_length(learning_curves)\n        learning_curves = np.array(learning_curves, dtype=np.single)\n        learning_curves = torch.tensor(learning_curves)\n        learning_curves = learning_curves.to(device=self.dev)\n\n        train_data = self._prepare_dataset_and_budgets()\n        test_data = {\n            'X_test': configurations,\n            'test_budgets': budgets,\n            'test_curves': learning_curves,\n        }\n\n        mean_predictions, std_predictions = self.model.predict_pipeline(train_data, test_data)\n\n        return mean_predictions, std_predictions, hp_indices, non_scaled_budgets\n\n    def suggest(self) -> Tuple[int, int]:\n        \"\"\"\n        Suggest a hyperparameter configuration to be evaluated next.\n\n        Returns:\n            best_config_index, budget: The index of the hyperparamter\n                configuration to be evaluated and the budget for\n                what it is going to be evaluated for.\n        \"\"\"\n        suggest_time_start = time.time()\n        # check if we still have random hyperparameters to evaluate\n        if self.initial_random_index < len(self.init_conf_indices):\n            self.logger.info(\n                'Not enough configurations to build a model. '\n                'Returning randomly sampled configuration'\n            )\n\n            random_indice = self.init_conf_indices[self.initial_random_index]\n            budget = self.init_budgets[self.initial_random_index]\n            self.initial_random_index += 1\n\n            return random_indice, budget\n        else:\n            mean_predictions, std_predictions, hp_indices, non_scaled_budgets = self._predict()\n            best_prediction_index = self.find_suggested_config(\n                mean_predictions,\n                std_predictions,\n                non_scaled_budgets,\n            )\n            \"\"\"\n            the best prediction index is not always matching with the actual hp index.\n            Since when evaluating the acq function, we do not consider hyperparameter\n            candidates that diverged or that are evaluated fully.\n            \"\"\"\n            best_config_index = hp_indices[best_prediction_index]\n\n            # decide for what budget we will evaluate the most\n            # promising hyperparameter configuration next.\n            if best_config_index in self.examples:\n                evaluated_budgets = self.examples[best_config_index]\n                max_budget = max(evaluated_budgets)\n                budget = max_budget + self.fantasize_step\n                # this would only trigger if fantasize_step is bigger\n                # than 1\n                if budget > self.max_benchmark_epochs:\n                    budget = self.max_benchmark_epochs\n            else:\n                budget = self.fantasize_step\n\n        suggest_time_end = time.time()\n        self.suggest_time_duration = suggest_time_end - suggest_time_start\n\n        self.budget_spent += self.fantasize_step\n\n        # exhausted hpo budget, finish.\n        if self.budget_spent > self.total_budget:\n            exit(0)\n\n        return best_config_index, budget\n\n    def observe(\n        self,\n        hp_index: int,\n        b: int,\n        learning_curve: np.ndarray,\n        alg_time: Optional[float] = None,\n    ):\n        \"\"\"\n        Args:\n            hp_index: The index of the evaluated hyperparameter configuration.\n            b: The budget for which the hyperparameter configuration was evaluated.\n            learning_curve: The learning curve of the hyperparameter configuration.\n            alg_time: The time taken from the algorithm to evaluate the hp configuration.\n        \"\"\"\n        score = learning_curve[-1]\n        # if y is an undefined value, append 0 as the overhead since we finish here.\n        if np.isnan(learning_curve).any():\n            self.update_info_dict(hp_index, b, np.nan, 0)\n            self.diverged_configs.add(hp_index)\n            return\n\n        observe_time_start = time.time()\n\n        self.examples[hp_index] = np.arange(1, b + 1).tolist()\n        self.performances[hp_index] = learning_curve\n\n        if self.best_value_observed < score:\n            self.best_value_observed = score\n            self.no_improvement_patience = 0\n        else:\n            self.no_improvement_patience += 1\n\n        observe_time_end = time.time()\n        train_time_duration = 0\n\n        # initialization phase over. Now we can sample from the model.\n        if self.initial_random_index >= len(self.init_conf_indices):\n            train_time_start = time.time()\n            # create the model for the first time\n            if self.model is None:\n                # Starting a model from scratch\n                self.model = DyHPO(\n                    self.surrogate_config,\n                    self.dev,\n                    self.dataset_name,\n                    self.output_path,\n                    self.seed,\n                )\n\n            if self.no_improvement_patience == self.no_improvement_threshold:\n                self.model.restart = True\n\n            self._train_surrogate()\n\n            train_time_end = time.time()\n            train_time_duration = train_time_end - train_time_start\n\n        observe_time_duration = observe_time_end - observe_time_start\n        total_duration = observe_time_duration + self.suggest_time_duration + train_time_duration\n        if alg_time is not None:\n            total_duration = total_duration + alg_time\n\n        self.update_info_dict(hp_index, b, score, total_duration)\n\n    def prepare_examples(self, hp_indices: List) -> List[np.ndarray]:\n        \"\"\"\n        Prepare the examples to be given to the surrogate model.\n\n        Args:\n            hp_indices: The list of hp indices that are already evaluated.\n\n        Returns:\n            examples: A list of the hyperparameter configurations.\n        \"\"\"\n        examples = []\n        for hp_index in hp_indices:\n            examples.append(self.hp_candidates[hp_index])\n\n        return examples\n\n    def generate_candidate_configurations(\n        self,\n    ) -> Tuple[List, List, List, List]:\n        \"\"\"\n        Generate candidate configurations that will be\n        fantasized upon.\n\n        Returns:\n            (configurations, hp_indices, hp_budgets, learning_curves): Tuple\n                A tuple of configurations, their indices in the hp list\n                and the budgets that they should be fantasized upon.\n        \"\"\"\n        hp_indices = []\n        hp_budgets = []\n        learning_curves = []\n\n        for hp_index in range(0, self.hp_candidates.shape[0]):\n\n            if hp_index in self.examples:\n                budgets = self.examples[hp_index]\n                # Take the max budget evaluated for a certain hpc\n                max_budget = max(budgets)\n                next_budget = max_budget + self.fantasize_step\n                # take the learning curve until the point we have evaluated so far\n                curve = self.performances[hp_index][:max_budget]\n                # if the curve is shorter than the length of the kernel size,\n                # pad it with zeros\n                difference_curve_length = self.surrogate_config['cnn_kernel_size'] - len(curve)\n                if difference_curve_length > 0:\n                    curve.extend([0.0] * difference_curve_length)\n            else:\n                # The hpc was not evaluated before, so fantasize its\n                # performance\n                next_budget = self.fantasize_step\n                curve = [0, 0, 0]\n\n            # this hyperparameter configuration is not evaluated fully\n            if next_budget <= self.max_benchmark_epochs:\n                hp_indices.append(hp_index)\n                hp_budgets.append(next_budget)\n                learning_curves.append(curve)\n\n        configurations = self.prepare_examples(hp_indices)\n\n        return configurations, hp_indices, hp_budgets, learning_curves\n\n    def history_configurations(\n        self,\n    ) -> Tuple[List, List, List, List]:\n        \"\"\"\n        Generate the configurations, labels, budgets and curves based on\n        the history of evaluated configurations.\n\n        Returns:\n            (train_examples, train_labels, train_budgets, train_curves):\n                A tuple of examples, labels, budgets and curves for the\n                configurations evaluated so far.\n        \"\"\"\n        train_examples = []\n        train_labels = []\n        train_budgets = []\n        train_curves = []\n\n        for hp_index in self.examples:\n            budgets = self.examples[hp_index]\n            performances = self.performances[hp_index]\n            example = self.hp_candidates[hp_index]\n\n            for budget, performance in zip(budgets, performances):\n                train_examples.append(example)\n                train_budgets.append(budget)\n                train_labels.append(performance)\n                train_curve = performances[:budget - 1] if budget > 1 else [0.0]\n                difference_curve_length = self.surrogate_config['cnn_kernel_size'] - len(train_curve)\n                if difference_curve_length > 0:\n                    train_curve.extend([0.0] * difference_curve_length)\n\n                train_curves.append(train_curve)\n\n        return train_examples, train_labels, train_budgets, train_curves\n\n    def acq(\n        self,\n        best_value: float,\n        mean: float,\n        std: float,\n        explore_factor: Optional[float] = 0.25,\n        acq_fc: str = 'ei',\n    ) -> float:\n        \"\"\"\n        The acquisition function that will be called\n        to evaluate the score of a hyperparameter configuration.\n\n        Parameters\n        ----------\n        best_value: float\n            Best observed function evaluation. Individual per fidelity.\n        mean: float\n            Point mean of the posterior process.\n        std: float\n            Point std of the posterior process.\n        explore_factor: float\n            The exploration factor for when ucb is used as the\n            acquisition function.\n        ei_calibration_factor: float\n            The factor used to calibrate expected improvement.\n        acq_fc: str\n            The type of acquisition function to use.\n\n        Returns\n        -------\n        acq_value: float\n            The value of the acquisition function.\n        \"\"\"\n        if acq_fc == 'ei':\n            if std == 0:\n                return 0\n            z = (mean - best_value) / std\n            acq_value = (mean - best_value) * norm.cdf(z) + std * norm.pdf(z)\n        elif acq_fc == 'ucb':\n            acq_value = mean + explore_factor * std\n        elif acq_fc == 'thompson':\n            acq_value = np.random.normal(mean, std)\n        elif acq_fc == 'exploit':\n            acq_value = mean\n        else:\n            raise NotImplementedError(\n                f'Acquisition function {acq_fc} has not been'\n                f'implemented',\n            )\n\n        return acq_value\n\n    def find_suggested_config(\n        self,\n        mean_predictions: np.ndarray,\n        mean_stds: np.ndarray,\n        budgets: List,\n    ) -> int:\n        \"\"\"\n        Find the hyperparameter configuration that has the highest score\n        with the acquisition function.\n\n        Args:\n            mean_predictions: The mean predictions of the posterior.\n            mean_stds: The mean standard deviations of the posterior.\n            budgets: The next budgets that the hyperparameter configurations\n                will be evaluated for.\n\n        Returns:\n            best_index: The index of the hyperparameter configuration with the\n                highest score.\n        \"\"\"\n        highest_acq_value = np.NINF\n        best_index = -1\n\n        index = 0\n        for mean_value, std in zip(mean_predictions, mean_stds):\n            budget = int(budgets[index])\n            best_value = self.calculate_fidelity_ymax(budget)\n            acq_value = self.acq(best_value, mean_value, std, acq_fc='ei')\n            if acq_value > highest_acq_value:\n                highest_acq_value = acq_value\n                best_index = index\n\n            index += 1\n\n        return best_index\n\n    def calculate_fidelity_ymax(self, fidelity: int):\n        \"\"\"\n        Find ymax for a given fidelity level.\n\n        If there are hyperparameters evaluated for that fidelity\n        take the maximum from their values. Otherwise, take\n        the maximum from all previous fidelity levels for the\n        hyperparameters that we have evaluated.\n\n        Args:\n            fidelity: The fidelity of the hyperparameter\n                configuration.\n\n        Returns:\n            best_value: The best value seen so far for the\n                given fidelity.\n        \"\"\"\n        exact_fidelity_config_values = []\n        lower_fidelity_config_values = []\n\n        for example_index in self.examples.keys():\n            try:\n                performance = self.performances[example_index][fidelity - 1]\n                exact_fidelity_config_values.append(performance)\n            except IndexError:\n                learning_curve = self.performances[example_index]\n                # The hyperparameter was not evaluated until fidelity, or more.\n                # Take the maximum value from the curve.\n                lower_fidelity_config_values.append(max(learning_curve))\n\n        if len(exact_fidelity_config_values) > 0:\n            # lowest error corresponds to best value\n            best_value = max(exact_fidelity_config_values)\n        else:\n            best_value = max(lower_fidelity_config_values)\n\n        return best_value\n\n    def update_info_dict(\n        self,\n        hp_index: int,\n        budget: int,\n        performance: float,\n        overhead: float,\n    ):\n        \"\"\"\n        Update the info dict with the current HPO iteration info.\n\n        Dump a new json file that will update with additional information\n        given the current HPO iteration.\n\n        Args:\n            hp_index: The index of the hyperparameter configuration.\n            budget: The budget of the hyperparameter configuration.\n            performance:  The performance of the hyperparameter configuration.\n            overhead: The total overhead (in seconds) of the iteration.\n        \"\"\"\n        hp_index = int(hp_index)\n        if 'hp' in self.info_dict:\n            self.info_dict['hp'].append(hp_index)\n        else:\n            self.info_dict['hp'] = [hp_index]\n\n        if 'scores' in self.info_dict:\n            self.info_dict['scores'].append(performance)\n        else:\n            self.info_dict['scores'] = [performance]\n\n        if 'curve' in self.info_dict:\n            self.info_dict['curve'].append(self.best_value_observed)\n        else:\n            self.info_dict['curve'] = [self.best_value_observed]\n\n        if 'epochs' in self.info_dict:\n            self.info_dict['epochs'].append(budget)\n        else:\n            self.info_dict['epochs'] = [budget]\n\n        if 'overhead' in self.info_dict:\n            self.info_dict['overhead'].append(overhead)\n        else:\n            self.info_dict['overhead'] = [overhead]\n\n        with open(os.path.join(self.output_path, f'{self.dataset_name}_{self.seed}.json'), 'w') as fp:\n            json.dump(self.info_dict, fp)\n\n    def preprocess_hp_candidates(self) -> List:\n        \"\"\"\n        Preprocess the list of all hyperparameter candidates\n        by  performing a log transform for the hyperparameters that\n        were log sampled.\n\n        Returns:\n            log_hp_candidates: The list of all hyperparameter configurations\n                where hyperparameters that were log sampled are log transformed.\n        \"\"\"\n        log_hp_candidates = []\n\n        for hp_candidate in self.hp_candidates:\n            new_hp_candidate = []\n            for index, hp_value in enumerate(hp_candidate):\n                new_hp_candidate.append(math.log(hp_value) if self.log_indicator[index] else hp_value)\n\n            log_hp_candidates = np.array(log_hp_candidates)\n            # scaler for the hp configurations\n\n            log_hp_candidates = self.scaler.fit_transform(log_hp_candidates)\n\n        return log_hp_candidates\n\n    @staticmethod\n    def patch_curves_to_same_length(curves):\n        \"\"\"\n        Patch the given curves to the same length.\n\n        Finds the maximum curve length and patches all\n        other curves that are shorter in length with zeroes.\n\n        Args:\n            curves: The given hyperparameter curves.\n\n        Returns:\n            curves: The updated array where the learning\n                curves are of the same length.\n        \"\"\"\n        max_curve_length = 0\n        for curve in curves:\n            if len(curve) > max_curve_length:\n                max_curve_length = len(curve)\n\n        for curve in curves:\n            difference = max_curve_length - len(curve)\n            if difference > 0:\n                curve.extend([0.0] * difference)\n\n        return curves\n\nfrom copy import deepcopy\nimport logging\nimport os\nfrom typing import Dict, Tuple\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch import cat\n\nimport gpytorch\n\n\nclass FeatureExtractor(nn.Module):\n    \"\"\"\n    The feature extractor that is part of the deep kernel.\n    \"\"\"\n    def __init__(self, configuration):\n        super(FeatureExtractor, self).__init__()\n\n        self.configuration = configuration\n\n        self.nr_layers = configuration['nr_layers']\n        self.act_func = nn.LeakyReLU()\n        # adding one to the dimensionality of the initial input features\n        # for the concatenation with the budget.\n        initial_features = configuration['nr_initial_features'] + 1\n        self.fc1 = nn.Linear(initial_features, configuration['layer1_units'])\n        self.bn1 = nn.BatchNorm1d(configuration['layer1_units'])\n        for i in range(2, self.nr_layers):\n            setattr(\n                self,\n                f'fc{i + 1}',\n                nn.Linear(configuration[f'layer{i - 1}_units'], configuration[f'layer{i}_units']),\n            )\n            setattr(\n                self,\n                f'bn{i + 1}',\n                nn.BatchNorm1d(configuration[f'layer{i}_units']),\n            )\n\n\n        setattr(\n            self,\n            f'fc{self.nr_layers}',\n            nn.Linear(\n                configuration[f'layer{self.nr_layers - 1}_units'] +\n                configuration['cnn_nr_channels'],  # accounting for the learning curve features\n                configuration[f'layer{self.nr_layers}_units']\n            ),\n        )\n        self.cnn = nn.Sequential(\n            nn.Conv1d(in_channels=1, kernel_size=(configuration['cnn_kernel_size'],), out_channels=4),\n            nn.AdaptiveMaxPool1d(1),\n        )\n\n    def forward(self, x, budgets, learning_curves):\n\n        # add an extra dimensionality for the budget\n        # making it nr_rows x 1.\n        budgets = torch.unsqueeze(budgets, dim=1)\n        # concatenate budgets with examples\n        x = cat((x, budgets), dim=1)\n        x = self.fc1(x)\n        x = self.act_func(self.bn1(x))\n\n        for i in range(2, self.nr_layers):\n            x = self.act_func(\n                getattr(self, f'bn{i}')(\n                    getattr(self, f'fc{i}')(\n                        x\n                    )\n                )\n            )\n\n        # add an extra dimensionality for the learning curve\n        # making it nr_rows x 1 x lc_values.\n        learning_curves = torch.unsqueeze(learning_curves, 1)\n        lc_features = self.cnn(learning_curves)\n        # revert the output from the cnn into nr_rows x nr_kernels.\n        lc_features = torch.squeeze(lc_features, 2)\n\n        # put learning curve features into the last layer along with the higher level features.\n        x = cat((x, lc_features), dim=1)\n        x = self.act_func(getattr(self, f'fc{self.nr_layers}')(x))\n\n        return x\n\n\nclass GPRegressionModel(gpytorch.models.ExactGP):\n    \"\"\"\n    A simple GP model.\n    \"\"\"\n    def __init__(\n        self,\n        train_x: torch.Tensor,\n        train_y: torch.Tensor,\n        likelihood: gpytorch.likelihoods.GaussianLikelihood,\n    ):\n        \"\"\"\n        Constructor of the GPRegressionModel.\n\n        Args:\n            train_x: The initial train examples for the GP.\n            train_y: The initial train labels for the GP.\n            likelihood: The likelihood to be used.\n        \"\"\"\n        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n\n        self.mean_module = gpytorch.means.ConstantMean()\n        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n\n    def forward(self, x):\n\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n\n        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n\n\nclass DyHPO:\n    \"\"\"\n    The DyHPO DeepGP model.\n    \"\"\"\n    def __init__(\n        self,\n        configuration: Dict,\n        device: torch.device,\n        dataset_name: str = 'unknown',\n        output_path: str = '.',\n        seed: int = 11,\n    ):\n        \"\"\"\n        The constructor for the DyHPO model.\n\n        Args:\n            configuration: The configuration to be used\n                for the different parts of the surrogate.\n            device: The device where the experiments will be run on.\n            dataset_name: The name of the dataset for the current run.\n            output_path: The path where the intermediate/final results\n                will be stored.\n            seed: The seed that will be used to store the checkpoint\n                properly.\n        \"\"\"\n        super(DyHPO, self).__init__()\n        self.feature_extractor = FeatureExtractor(configuration)\n        self.batch_size = configuration['batch_size']\n        self.nr_epochs = configuration['nr_epochs']\n        self.early_stopping_patience = configuration['nr_patience_epochs']\n        self.refine_epochs = 50\n        self.dev = device\n        self.seed = seed\n        self.model, self.likelihood, self.mll = \\\n            self.get_model_likelihood_mll(\n                configuration[f'layer{self.feature_extractor.nr_layers}_units']\n            )\n\n        self.model.to(self.dev)\n        self.likelihood.to(self.dev)\n        self.feature_extractor.to(self.dev)\n\n        self.optimizer = torch.optim.Adam([\n            {'params': self.model.parameters(), 'lr': configuration['learning_rate']},\n            {'params': self.feature_extractor.parameters(), 'lr': configuration['learning_rate']}],\n        )\n\n        self.configuration = configuration\n        # the number of initial points for which we will retrain fully from scratch\n        # This is basically equal to the dimensionality of the search space + 1.\n        self.initial_nr_points = 10\n        # keeping track of the total hpo iterations. It will be used during the optimization\n        # process to switch from fully training the model, to refining.\n        self.iterations = 0\n        # flag for when the optimization of the model should start from scratch.\n        self.restart = True\n\n        self.logger = logging.getLogger(__name__)\n\n        self.checkpoint_path = os.path.join(\n            output_path,\n            'checkpoints',\n            f'{dataset_name}',\n            f'{self.seed}',\n        )\n\n        os.makedirs(self.checkpoint_path, exist_ok=True)\n\n        self.checkpoint_file = os.path.join(\n            self.checkpoint_path,\n            'checkpoint.pth'\n        )\n\n    def restart_optimization(self):\n        \"\"\"\n        Restart the surrogate model from scratch.\n        \"\"\"\n        self.feature_extractor = FeatureExtractor(self.configuration).to(self.dev)\n        self.model, self.likelihood, self.mll = \\\n            self.get_model_likelihood_mll(\n                self.configuration[f'layer{self.feature_extractor.nr_layers}_units'],\n            )\n\n        self.optimizer = torch.optim.Adam([\n            {'params': self.model.parameters(), 'lr': self.configuration['learning_rate']},\n            {'params': self.feature_extractor.parameters(), 'lr': self.configuration['learning_rate']}],\n        )\n\n    def get_model_likelihood_mll(\n        self,\n        train_size: int,\n    ) -> Tuple[GPRegressionModel, gpytorch.likelihoods.GaussianLikelihood, gpytorch.mlls.ExactMarginalLogLikelihood]:\n        \"\"\"\n        Called when the surrogate is first initialized or restarted.\n\n        Args:\n            train_size: The size of the current training set.\n\n        Returns:\n            model, likelihood, mll - The GP model, the likelihood and\n                the marginal likelihood.\n        \"\"\"\n        train_x = torch.ones(train_size, train_size).to(self.dev)\n        train_y = torch.ones(train_size).to(self.dev)\n\n        likelihood = gpytorch.likelihoods.GaussianLikelihood().to(self.dev)\n        model = GPRegressionModel(train_x=train_x, train_y=train_y, likelihood=likelihood).to(self.dev)\n        mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model).to(self.dev)\n\n        return model, likelihood, mll\n\n    def train_pipeline(self, data: Dict[str, torch.Tensor], load_checkpoint: bool = False):\n        \"\"\"\n        Train the surrogate model.\n\n        Args:\n            data: A dictionary which has the training examples, training features,\n                training budgets and in the end the training curves.\n            load_checkpoint: A flag whether to load the state from a previous checkpoint,\n                or whether to start from scratch.\n        \"\"\"\n        self.iterations += 1\n        self.logger.debug(f'Starting iteration: {self.iterations}')\n        # whether the state has been changed. Basically, if a better loss was found during\n        # this optimization iteration then the state (weights) were changed.\n        weights_changed = False\n\n        if load_checkpoint:\n            try:\n                self.load_checkpoint()\n            except FileNotFoundError:\n                self.logger.error(f'No checkpoint file found at: {self.checkpoint_file}'\n                                  f'Training the GP from the beginning')\n\n        self.model.train()\n        self.likelihood.train()\n        self.feature_extractor.train()\n\n        self.optimizer = torch.optim.Adam([\n            {'params': self.model.parameters(), 'lr': self.configuration['learning_rate']},\n            {'params': self.feature_extractor.parameters(), 'lr': self.configuration['learning_rate']}],\n        )\n\n        X_train = data['X_train']\n        train_budgets = data['train_budgets']\n        train_curves = data['train_curves']\n        y_train = data['y_train']\n\n        initial_state = self.get_state()\n        training_errored = False\n\n        if self.restart:\n            self.restart_optimization()\n            nr_epochs = self.nr_epochs\n            # 2 cases where the statement below is hit.\n            # - We are switching from the full training phase in the beginning to refining.\n            # - We are restarting because our refining diverged\n            if self.initial_nr_points <= self.iterations:\n                self.restart = False\n        else:\n            nr_epochs = self.refine_epochs\n\n        # where the mean squared error will be stored\n        # when predicting on the train set\n        mse = 0.0\n\n        for epoch_nr in range(0, nr_epochs):\n\n            nr_examples_batch = X_train.size(dim=0)\n            # if only one example in the batch, skip the batch.\n            # Otherwise, the code will fail because of batchnorm\n            if nr_examples_batch == 1:\n                continue\n\n            # Zero backprop gradients\n            self.optimizer.zero_grad()\n\n            projected_x = self.feature_extractor(X_train, train_budgets, train_curves)\n            self.model.set_train_data(projected_x, y_train, strict=False)\n            output = self.model(projected_x)\n\n            try:\n                # Calc loss and backprop derivatives\n                loss = -self.mll(output, self.model.train_targets)\n                loss_value = loss.detach().to('cpu').item()\n                mse = gpytorch.metrics.mean_squared_error(output, self.model.train_targets)\n                self.logger.debug(\n                    f'Epoch {epoch_nr} - MSE {mse:.5f}, '\n                    f'Loss: {loss_value:.3f}, '\n                    f'lengthscale: {self.model.covar_module.base_kernel.lengthscale.item():.3f}, '\n                    f'noise: {self.model.likelihood.noise.item():.3f}, '\n                )\n                loss.backward()\n                self.optimizer.step()\n            except Exception as training_error:\n                self.logger.error(f'The following error happened while training: {training_error}')\n                # An error has happened, trigger the restart of the optimization and restart\n                # the model with default hyperparameters.\n                self.restart = True\n                training_errored = True\n                break\n\n        \"\"\"\n        # metric too high, time to restart, or we risk divergence\n        if mse > 0.15:\n            if not self.restart:\n                self.restart = True\n        \"\"\"\n        if training_errored:\n            self.save_checkpoint(initial_state)\n            self.load_checkpoint()\n\n    def predict_pipeline(\n        self,\n        train_data: Dict[str, torch.Tensor],\n        test_data: Dict[str, torch.Tensor],\n    ) -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n\n        Args:\n            train_data: A dictionary that has the training\n                examples, features, budgets and learning curves.\n            test_data: Same as for the training data, but it is\n                for the testing part and it does not feature labels.\n\n        Returns:\n            means, stds: The means of the predictions for the\n                testing points and the standard deviations.\n        \"\"\"\n        self.model.eval()\n        self.feature_extractor.eval()\n        self.likelihood.eval()\n\n        with torch.no_grad(): # gpytorch.settings.fast_pred_var():\n            projected_train_x = self.feature_extractor(\n                train_data['X_train'],\n                train_data['train_budgets'],\n                train_data['train_curves'],\n            )\n            self.model.set_train_data(inputs=projected_train_x, targets=train_data['y_train'], strict=False)\n            projected_test_x = self.feature_extractor(\n                test_data['X_test'],\n                test_data['test_budgets'],\n                test_data['test_curves'],\n            )\n            preds = self.likelihood(self.model(projected_test_x))\n\n        means = preds.mean.detach().to('cpu').numpy().reshape(-1, )\n        stds = preds.stddev.detach().to('cpu').numpy().reshape(-1, )\n\n        return means, stds\n\n    def load_checkpoint(self):\n        \"\"\"\n        Load the state from a previous checkpoint.\n        \"\"\"\n        checkpoint = torch.load(self.checkpoint_file)\n        self.model.load_state_dict(checkpoint['gp_state_dict'])\n        self.feature_extractor.load_state_dict(checkpoint['feature_extractor_state_dict'])\n        self.likelihood.load_state_dict(checkpoint['likelihood_state_dict'])\n\n    def save_checkpoint(self, state: Dict =None):\n        \"\"\"\n        Save the given state or the current state in a\n        checkpoint file.\n\n        Args:\n            state: The state to save, if none, it will\n            save the current state.\n        \"\"\"\n\n        if state is None:\n            torch.save(\n                self.get_state(),\n                self.checkpoint_file,\n            )\n        else:\n            torch.save(\n                state,\n                self.checkpoint_file,\n            )\n\n    def get_state(self) -> Dict[str, Dict]:\n        \"\"\"\n        Get the current state of the surrogate.\n\n        Returns:\n            current_state: A dictionary that represents\n                the current state of the surrogate model.\n        \"\"\"\n        current_state = {\n            'gp_state_dict': deepcopy(self.model.state_dict()),\n            'feature_extractor_state_dict': deepcopy(self.feature_extractor.state_dict()),\n            'likelihood_state_dict': deepcopy(self.likelihood.state_dict()),\n        }\n\n        return current_state",
        "experimental_info": "HPO Algorithm: DyHPO\nBenchmarks: LCBench, TaskSet\nOptimization goal: Minimization=False (Maximization) for LCBench (accuracy), Minimization=True for TaskSet (loss).\nTotal HPO budget (budget_limit): 1000.\nMaximum budget (epochs/steps) for a single configuration in benchmark (max_benchmark_epochs): 51 for LCBench and TaskSet.\nFantasize step (incremental budget increase): 1 epoch/step.\nRandom seeds: 10 seeds (0-9) are used for experimental runs.\n\nDyHPOAlgorithm settings:\n- Initial configurations (initial_configurations_nr): 1.\n- Budget for initial configurations (conf_individual_budget): 1 epoch/step.\n- Fraction of random configurations: 0.1.\n- No improvement patience threshold (for model restart): `int(max_benchmark_epochs + 0.2 * max_benchmark_epochs)` (e.g., for LCBench, 51 + 0.2 * 51 = 61).\n- Hyperparameter preprocessing: Log transform for log-sampled hyperparameters (indicated by `log_indicator`), followed by `MinMaxScaler`.\n- Learning curve preprocessing: Curves are padded with zeros to the `cnn_kernel_size` length for training input, and to the max observed length for `patch_curves_to_same_length`.\n- Acquisition function: Expected Improvement (`ei`).\n\nDyHPO (Deep Gaussian Process Surrogate) settings:\n- Feature Extractor (`FeatureExtractor`) neural network architecture:\n    - Number of dense layers (`nr_layers`): 2.\n    - `layer1_units`: 64.\n    - `layer2_units`: 128.\n    - CNN layers: `cnn_nr_channels=4`, `cnn_kernel_size=3`.\n    - Input: Hyperparameter configuration + normalized budget (concatenated, initial_features + 1), then concatenated with CNN features from the past learning curve.\n- GP model: `GPRegressionModel` using `gpytorch.means.ConstantMean` and `gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())`.\n- Training settings:\n    - `batch_size`: 64.\n    - `nr_epochs`: 1000 (for full training/from scratch).\n    - `nr_patience_epochs`: 10 (early stopping patience, though not explicitly used for stopping in `train_pipeline` but defined).\n    - `learning_rate`: 0.001.\n    - `refine_epochs`: 50 (for refinement training after initial phase or restart).\n    - `initial_nr_points`: 10 (number of HPO iterations before switching from full training to refining, or for full restart).\n    - Optimizer: Adam."
      }
    },
    {
      "title": "Bayesian Optimization for Iterative Learning",
      "abstract": "The performance of deep (reinforcement) learning systems crucially depends on\nthe choice of hyperparameters. Their tuning is notoriously expensive, typically\nrequiring an iterative training process to run for numerous steps to\nconvergence. Traditional tuning algorithms only consider the final performance\nof hyperparameters acquired after many expensive iterations and ignore\nintermediate information from earlier training steps. In this paper, we present\na Bayesian optimization (BO) approach which exploits the iterative structure of\nlearning algorithms for efficient hyperparameter tuning. We propose to learn an\nevaluation function compressing learning progress at any stage of the training\nprocess into a single numeric score according to both training success and\nstability. Our BO framework is then balancing the benefit of assessing a\nhyperparameter setting over additional training steps against their computation\ncost. We further increase model efficiency by selectively including scores from\ndifferent training steps for any evaluated hyperparameter set. We demonstrate\nthe efficiency of our algorithm by tuning hyperparameters for the training of\ndeep reinforcement learning agents and convolutional neural networks. Our\nalgorithm outperforms all existing baselines in identifying optimal\nhyperparameters in minimal time.",
      "full_text": "Bayesian Optimization for Iterative Learning Vu Nguyen ∗ University of Oxford vu@robots.ox.ac.uk Sebastian Schulze ∗ University of Oxford sebastian.schulze@eng.ox.ac.uk Michael A. Osborne University of Oxford mosb@robots.ox.ac.uk Abstract The performance of deep (reinforcement) learning systems crucially depends on the choice of hyperparameters. Their tuning is notoriously expensive, typically requiring an iterative training process to run for numerous steps to convergence. Traditional tuning algorithms only consider the ﬁnal performance of hyperparam- eters acquired after many expensive iterations and ignore intermediate information from earlier training steps. In this paper, we present a Bayesian optimization (BO) approach which exploits the iterative structure of learning algorithms for efﬁcient hyperparameter tuning. We propose to learn an evaluation function compress- ing learning progress at any stage of the training process into a single numeric score according to both training success and stability. Our BO framework is then balancing the beneﬁt of assessing a hyperparameter setting over additional train- ing steps against their computation cost. We further increase model efﬁciency by selectively including scores from different training steps for any evaluated hyper- parameter set. We demonstrate the efﬁciency of our algorithm by tuning hyperpa- rameters for the training of deep reinforcement learning agents and convolutional neural networks. Our algorithm outperforms all existing baselines in identifying optimal hyperparameters in minimal time. 1 Introduction Deep learning (DL) and deep reinforcement learning (DRL) have led to impressive breakthroughs in a broad range of applications such as game play [26, 36], motor control [43], and image recognition [20]. To maintain general applicability, these algorithms expose sets of hyperparameters to adapt their behavior to any particular task at hand. This ﬂexibility comes at the price of having to tune an additional set of parameters – poor settings lead to drastic performance losses [11, 30, 37]. On top of being notoriously sensitive to these choices, deep (reinforcement) learning systems often have high training costs, in computational resources and time. For example, a single training run on the Atari Breakout game took approximately 75 hours on a GPU cluster [26]. Tuning DRL parameters is further complicated as only noisy evaluations of an agent’s ﬁnal performance are obtainable. Bayesian optimization (BO) [12, 28, 35] has recently achieved considerable success in optimizing these hyperparameters. This approach casts the tuning process as a global optimization problem based on noisy evaluations of a black-box function f . BO constructs a surrogate model typically using a Gaussian process (GP) [31], over this unknown function. This GP surrogate is used to build an acquisition function [13, 44] which suggests the next hyperparameter to evaluate. In modern machine learning (ML) algorithms [15], the training process is often conducted in an iterative manner. A natural example is given by deep learning where training is usually based on stochastic gradient descent and other iterative procedures. Similarly, the training of reinforcement learning agents is mostly carried out using multiple episodes. The knowledge accumulated during these training iterations can be useful to inform BO. However, most existing BO approaches [35] ∗These authors contributed equally. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada. arXiv:1909.09593v5  [cs.LG]  16 Jan 2021deﬁne the objective function as the average performance over the ﬁnal training iterations. In doing so, they ignore the information contained in the preceding training steps. In this paper, we present a Bayesian optimization approach for tuning algorithms where iterative learning is available – the cases of deep learning and deep reinforcement learning. First, we consider the joint space of input hyperparameters and number of training iterations to capture the learning progress at different time steps in the training process. We then propose to transform the whole training curve into a numeric score according to user preference. To learn across the joint space efﬁciently, we introduce a data augmentation technique leveraging intermediate information from the iterative process. By exploiting the iterative structure of training procedures, we encourage our algorithm to consider running a larger number of cheap (but high-utility) experiments, when cost- ignorant algorithms would only be able to run a few expensive ones. We demonstrate the efﬁciency of our algorithm on training DRL agents on several well-known benchmarks as well as the training of convolutional neural networks. In particular, our algorithm outperforms existing baselines in ﬁnding the best hyperparameter in terms of wall-clock time. Our main contributions are: • an algorithm to optimize the learning curve of a ML algorithm by using training curve compression, instead of averaged ﬁnal performance; • an approach to learn the compression curve from the data and a data augmentation tech- nique for increased sample-efﬁciency; • demonstration on tuning DRL and convolutional neural networks. 2 Related Work in Iteration-Efﬁcient Bayesian Optimization The ﬁrst algorithm category employs stopping criteria to terminate some training runs early and allo- cate resources towards more promising settings. These criteria typically involve projecting towards a ﬁnal score from early training stages. Freeze-thaw BO [42] models the training loss over time us- ing a GP regressor under the assumption that the training loss roughly follows an exponential decay. Based on this projection, training resources are allocated to the most promising settings. Hyperband [8, 23] dynamically allocates computational resources (e.g. training epochs or dataset size) through random sampling and eliminates under-performing hyperparameter settings by successive halving. Attempts have also been made to improve the epoch efﬁciency of other hyperparameter optimization algorithms in [5, 7, 18] which predict the ﬁnal learning outcome based on partially trained learning curves to identify hyperparameter settings that are expected to under-perform and early-stop them. In the context of DRL, however, these stopping criteria, including the exponential decay assumed in Freeze-thaw BO [42], may not be applicable, due to the unpredictable ﬂuctuations of DRL reward curves. In the supplement, we illustrate the noisiness of DRL training. The second category [16, 17, 23, 41, 48] aims to reduce the resource consumption of BO by utilizing low-ﬁdelity functions which can be obtained by using a subset of the training data or by training the ML model for a small number of iterations. Multi-task BO [41] requires the user to deﬁne a division of the dataset into pre-deﬁned and discrete subtasks. Multi-ﬁdelity BO with continuous approximation (BOCA) [16] and hierarchical partition [34] extend this idea to continuous settings. Speciﬁcally, BOCA ﬁrst selects the hyperparameter input and then the corresponding ﬁdelity to be evaluated at. The ﬁdelity in this context refers to the use of different number of learning iterations. Analogous to BOCA’s consideration of continuous ﬁdelities, Fabolas [17] proposes to model the combined space of input hyperparameter and dataset size and then select the optimal input and dataset size jointly. The above approaches typically identify performance of hyperparameters via the average (either training or validation) loss of the last learning iterations. Thereby, they do not account for potential noise in the learning process (e.g., they might select unstable settings that jumped to high perfor- mance in the last couple of iterations). 3 Bayesian Optimization for Iterative Learning (BOIL) Problem setting. We consider training a machine learning algorithm given a d-dimensional hy- perparameter x ∈X ⊂Rd for t iterations. This process has a training time costc(x,t) and produces 20 100 200 300 400 500 #Episode t 0.80 0.85 0.90 0.95 1.00x Tmin Tmax Augmented Obs Observation 0 100 200 300 400 500 0 50 100 150 200Score 4 18 34 8 45 5 14 26Reward Curve Sigmoid Func 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l( . |m * 0 ,g* 0 ) for Cifar10 m* 0 =-4.0 g* 0 =1.476 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l( . |m * 0 ,g* 0 ) for Reacher m* 0 =2.779 g* 0 =1.973 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l( . |m * 0 ,g* 0 ) for CartPole m* 0 =-3.266 g* 0 =3.0 Figure 1: Left: the score in pink box is a convolution of the reward curve r(·| x = 0.9,t = 500) and a Sigmoid function l(u |g0,m0) = 1 1+exp(−g0[u−m0]) up to time step t. Bottom: observations are selected to augment the dataset (red dots). The heatmap indicates the GP predictive mean µ for f across the number of episodest used to train an agent. Tmin and Tmax are two user-deﬁned thresholds for the number of training episodes. x is a hyperparameter to be tuned. Right: we learn the optimal parameter g∗ 0 and m∗ 0 for each experiment separately. training evaluations r(·| x,t) for t iterations, t ∈[Tmin,Tmax]. These could be episode rewards in DRL or training accuracies in DL. An important property of iterative training is that we know the whole curve at preceding steps r(t′|x,t), ∀t′≤t. Given the raw training curve r(·| x,t), we assume an underlying smoothed black-box function f , deﬁned in Sec. 3.2. Formally, we aim to ﬁnd x∗= argmaxx∈X f (x,Tmax); at the same time, we want to keep the overall training time, ∑N i=1 c(xi,ti), of evaluated settings [xi,ti] as low as possible. We summarize our variables in Table 1 in the supplement for ease of reading. 3.1 Selecting a next point using iteration-efﬁcient modeling We follow popular designs in [17, 19, 39, 41] and model the cost-sensitive black-box function as f (x,t) ∼GP(0,k([x,t],[x′,t′])), where k is an appropriate covariance functions and [x,t] ∈Rd+1. For simplicity and robustness, the cost function c(x,t) is approximated by a linear regressor. De- pending on the setting, it may be more appropriate to employ a second GP or different parametric model if the cost has a more complex dependence on hyperparameters x and iterations t. We regu- larly (re-)optimize both kernel and cost function parameters in between point acquisitions. More speciﬁcally, we choose the covariance function as a productk ([x,t],[x′,t′]) =k(x,x′)×k(t,t′) to induce joint similarities over parameter and iteration space. We estimate the predictive mean and uncertainty for a GP [31] at any input z∗= [x∗,t∗] as µ (z∗) =k∗ [ K +σ2 y I ]−1 y (1) σ2 (z∗) =k∗∗−k∗ [ K +σ2 y I ]−1 kT ∗ (2) where y = [yi]∀i, k∗= [k (z∗,zi)]∀i, K = [k (zi,zj)]∀i, j, k∗∗= k (z∗,z∗), and σ2 y is the noise variance of f . Cost predictions at any particular parameter x and time t are given by µc([x∗,t∗]) =βT [x,t], where β is directly computed from data {Z = [xi,ti],c = [ci]}∀i as β = (ZT Z)−1Zc [1]. Our goal is to select a point with high function value (exploitation), high uncertainty (exploration) and low cost (cheap). At each iteration n, we query the input parameter xn and the number of iteration tn [38, 48]: zn = [xn,tn] = argmax x∈X ,t∈[Tmin,Tmax] α(x,t)/µc(x,t). (3) 3Although our framework is available for any acquisition choices [13, 22, 47], to cope with output noise, we follow [45] and slight modify the expected improvement criterion using the maximum mean GP prediction µmax n . Let λ = µn(z)−µmaxn σn(z) , we then have a closed-form for the new expected improvement (EI) as αEI n (z) =σn (z)φ (λ) + [µn (z)−µmax n ]Φ(λ) where φ is the standard normal p.d.f., Φ is the c.d.f, µn and σn are the GP predictive mean and variance deﬁned in Eq. (1) and Eq. (2), respectively. 3.2 Training curve compression and estimating the transformation function Existing BO approaches [4, 23] typically deﬁne the objective function as an average loss over the ﬁnal learning episodes. However, this does not take into consideration how stable performance is or the training stage at which it has been achieved. We argue that averaging learning losses is likely misleading due to the noise and ﬂuctuations of our observations (learning curves) – particularly during the early stages of training. We propose to compress the whole learning curve into a numeric score via a preference function representing the user’s desired training curve. In the following, we use the Sigmoid function (speciﬁcally the Logistic function) to compute the utility score as y = ˆy(r,m0,g0) =r(·|x,t)•l(·|m0,g0) = t ∑ u=1 r(u |x,t) 1 +exp(−g0 [u −m0]) (4) where •is a dot product, a Logistic function l(·| m0,g0) is parameterized by a growth parameter g0 deﬁning a slope and the middle point of the curve m0. The optimal parameters g0 and m0 are estimated directly from the data. We illustrate different shapes of l parameterized by g0 and m0 in the appendix. The Sigmoid preference has a number of desirable properties. As early weights are small, less credit is given to ﬂuctuations at the initial stages, making it less likely for our surrogate to be biased towards randomly well performing settings. However, as weights monotonically increase, hyperparameters with improving performance are preferred. As weights saturate over time, stable, high performing conﬁgurations are preferred over short “performance spikes” characteristic of un- stable training. Lastly, this utility score assigns higher values to the same performance if it is being maintained over more episodes. Learning the transformation function from data. Different compression curves l(), parameter- ized by different choices of g0 and m0 in Eq. (4), may lead to different utilities y and thus affect the performance. The optimal values of g∗ 0 and m∗ 0 are unknown in advance. Therefore, we propose to learn these values g∗ 0 and m∗ 0 directly from the data. Our intuition is that the ‘optimal’ compression curve l(m∗ 0,g∗ 0) will lead to a better ﬁt of the GP. This better GP surrogate model, thus, will result in better prediction as well as optimization performance. We parameterize the GP log marginal likelihood L [31] as the function of m0 and g0: L(m0,g0) =1 2 ˆyT ( K +σ2 y I )−1 ˆy −1 2 ln ⏐⏐K +σ2 y I ⏐⏐ +const (5) where σ2 y is the output noise variance, ˆy is the function of m0 and g0 deﬁned in Eq. (4). We optimize m0 and g0 (jointly with other GP hyperparameters) using multi-start gradient descent. We derive the derivative ∂L ∂m0 = ∂L ∂ ˆy ∂ ˆy ∂m0 and ∂L ∂g0 = ∂L ∂ ˆy ∂ ˆy ∂g0 which can be computed analytically as: ∂L ∂ ˆy = ( K +σ2 y IN )−1 ˆy; ∂ ˆy ∂m0 = −g0 ×exp(−g0 [u −m0]) [1 +exp(−g0 [u −m0])]2 ; ∂ ˆy ∂g0 = −m0 ×exp(−g0 [u −m0]) [1 +exp(−g0 [u −m0])]2 . The estimated compression curves are illustrated in Right Fig. 1 and in Sec. 4.1. 3.3 Augmenting the training data When evaluating a parameter x over t iterations, we obtain not only a ﬁnal score but also all reward sequences r(t′|x,t),∀t′= 1,..., t. The auxiliary information from the curve can be useful for BO. Therefore, we propose to augment the information from the curve into the sample set of our GP model. A naïve approach for augmentation is to add a full curve of points {[x, j],yj}t j=1 where yj is computed using Eq. (4). However, this approach can be redundant and may im- pose serious issues in the conditioning of the GP covariance matrix. As we cluster 40.80 0.85 0.90 0.95 1.00 200 300 400 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 x GP variance 2 0 2 4 6 8 10 12 0.000 0.025 0.050 0.075 0.100 0.125 0.150 0.175 0.80 0.85 0.90 0.95 1.00 200 300 400 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 GP variance 400 320 240 160 80 0 80 160 240 0.000 0.005 0.010 0.015 0.020 0.025 0.030 0.035 0.040 x Figure 2: GP with different settings. Left: our augmentation. Right: using a full curve. If we add too many observations, the GP covariance matrix becomes ill-conditioned. On the right, the GP ﬁt is poor with a large mean estimate range of [−400,240] even though the output is standardized N (0,1). All x-axis are over x, a hyperparameter to be tuned. more evaluations closely, the conditioning of the GP covariance degrades further, as dis- cussed in [24]. This conditioning issue is especially serious in our noisy DRL settings. 0 10 20 30 40 50 60 Iterations 0 5 10 15 20 25Log of Condition Number Condition Number of GP Covariance Augmentation No Augmentation Full Curve Reasonable Threshold Figure 3: The condition number of GP covari- ance matrix deteriorates if we add the whole curve of points into a GP. The large condition number indicates the nearness to singularity. We highlight this effect on GP estimation in Fig. 2 wherein the GP mean varies erratically when the natural log of the condition number of the GP co- variance matrix goes above 25 (see Fig. 3) as we include the whole curve. Selecting subset of points from the curve. Dif- ferent solutions, such as the addition of artiﬁcial noise or altering the kernel’s length-scales, have been proposed. We decide to use an active learn- ing approach [10, 29] as sampled data points are expected to contain a lot of redundant informa- tion. As a consequence, the loss of information from sub-sampling the data should be minimal and information-eroding modiﬁcation of the ker- nel matrix itself can be avoided. As a side beneﬁt, the reduced number of sampled points speeds up inference in our GP models. In particular, we select samples at the maximum of the GP predictive uncertainty. Formally, we sequentially select a set Z = [z1,...zM], zm = [x,tm], by varying tm while keeping x ﬁxed as zm =argmax ∀t′≤t σ([x,t′] |D′),∀m ≤M s.t. lnof cond(K) ≤δ (6) where D′= D∪{zj = [x,tj]}m−1 j=1 . This sub-optimisation problem is done in a one-dimensional space of t′∈{Tmin,..., t}, thus it is cheap to optimize using (multi-start) gradient descent (the derivative of GP predictive variance is available [31]). Alternatively, a ﬁxed-size grid could be considered, but this could cause conditioning issues when a point in the grid [ x,tgrid ] is placed near another existing point [ x′,tgrid ] , i.e., ||x −x′||2 ≤ε for some small ε. These generated points Z are used to calculate the output r(zm) and augmented into the observation set (X,Y ) for ﬁtting the GP. The number of samplesM is adaptively chosen such that the natural log of the condition number of the covariance matrix is less than a threshold. This is to ensure that the GP covariance matrix condition number behaves well by reducing the number of unnecessary points added to the GP at later stages. We compute the utility score ym given zm for each augmented point using Eq. (4). In addition, we can estimate the running time cm using the predictive mean µc(zm). We illustrate the augmented observations and estimated scores in Fig. 1. We summarize the overall algorithm in Alg. 1. To enforce non-negativity and numerical stability, we make use of the transformations α ←log[1 +exp(α)] and µc ←log[1 +exp(µc)]. 4 Experiments We assess our model by tuning hyperparameters for two DRL agents on three environments and a CNN on two datasets. We provide additional illustrations and experiments in the appendix. 5Algorithm 1 Bayesian Optimization with Iterative Learning (BOIL) Input: #iter N, initial data D0, z = [x,t]. Output: optimal x∗and y∗= max∀y∈DN y 1: for n = 1....N do 2: Fit a GP to estimate µf (),σf () from Eqs. (1,2) and a LR for cost µc() 3: Select zn = argmaxx,t α(x,t)/µc(x,t) and observe a curve r and a cost c from f (zn) 4: Compressing the learning curve r(zn) into numeric score using Eq. (4). 5: Sample augmented points zn,m,yn,m,cn,m,∀m ≤M given the curve and Dn in Eq. (6) 6: Augment the data into Dn and estimate Logistic curve hyperparameters m0 and g0. 7: end for Experimental setup. All experimental results are averaged over 20 independent runs with differ- ent random seeds. Final performance is estimated by evaluating the chosen hyperparameter over the maximum number of iterations. All experiments are executed on a NVIDIA 1080 GTX GPU using the tensorﬂow-gpu Python package. The DRL environments are available through the OpenAI gym [3] and Mujoco [43]. Our DRL implementations are based on the open source from Open AI Baselines [6]. We release our implementation at https://github.com/ntienvu/BOIL. We use square-exponential kernels for the GP in our model and estimate their parameters by maxi- mizing the marginal likelihood [31]. We set the maximum number of augmented points to beM = 15 and a threshold for a natural log of GP condition numberδ = 20. We note that the optimization over- head is much less than the black-box function evaluation time. Baselines. We compare with Hyperband [23] which demonstrated empirical successes in tuning deep learning applications in an iteration-efﬁcient manner. We extend the discrete multi-task BO [41] to the continuous case – which can also be seen as continuous multi-ﬁdelity BO [16, 39] as in our setting, they both consider cost-sensitivity and iteration-efﬁciency. We, therefore, label the two baselines as continuous multi-task/ﬁdelity BO (CM-T/F-BO). We have ignored the minor difference in these settings, such as multi-task approaches jointly optimizes the ﬁdelity and input while BOCA [16] ﬁrst selects the input and then the ﬁdelity. Our focus is to demonstrate the effectiveness of optimizing the learning curve using compression and augmentation techniques. We therefore omit the comparison of various acquisition functions and kernel choices which can easily be used in our model. We also do not compare with Fabolas [17] which is designed to vary dataset sizes, not iteration numbers. We would expect the performance of Fabolas to be close to CM-T/F-BO. We are unable to compare with FreezeThaw as the code is not available. However, the curves in our setting are not exponential decays and thus ill-suited to their model (see last ﬁgure in the appendix). We have considered an ablation study in the appendix using a time kernel following the exponential decay proposed in Freeze-thaw method [42]. Task descriptions. We consider three DRL settings including a Dueling DQN (DDQN) [46] agent in the CartPole-v0 environment and Advantage Actor Critic (A2C) [25] agents in the InvertedPendulum-v2 and Reacher-v2 environments. In addition to the DRL applications, we tune 6 hyperparameters for training a convolutional neural network [21] on the SVHN dataset and CI- FAR10. Due to space considerations, we refer to the appendix for further details. 4.1 Model illustration /uni00000013/uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni00000013 /uni00000018 /uni00000014/uni00000013 /uni00000014/uni00000018/uni00000006/uni00000024/uni00000058/uni0000004a/uni00000050/uni00000048/uni00000051/uni00000057/uni00000048/uni00000047/uni00000003/uni00000032/uni00000045/uni00000056 /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni00000057/uni00000048/uni00000047/uni00000003/uni00000024/uni00000058/uni0000004a/uni00000050/uni00000048/uni00000051/uni00000057/uni00000048/uni00000047/uni00000003/uni00000032/uni00000045/uni00000056 Figure 4: DDQN on CartPole. The number of augmented observations reduces over time. We ﬁrst illustrate the estimated compression func- tion l(m∗ 0,g∗ 0) in Right Fig. 1 from different experi- ments. These Logistic parameters g∗ 0 and m∗ 0 are es- timated by maximizing the GP marginal likelihood and used for compressing the curve. We show that the estimated curve from CartPole tends to reach the highest performance much earlier than Reacher because CartPole is somewhat easier to train than Reacher. We next examine the count of augmented observa- tions generated per iteration in Fig. 4. Although this number is ﬂuctuating, it tends to reduce over 6/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000018/uni00000013/uni00000013 /uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048 /uni0000001a/uni00000013 /uni00000019/uni00000013 /uni00000018/uni00000013 /uni00000017/uni00000013 /uni00000016/uni00000013 /uni00000015/uni00000013 /uni00000014/uni00000013 /uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni00000035/uni00000048/uni00000044/uni00000046/uni0000004b/uni00000048/uni00000055/uni00000040/uni00000003/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047/uni00000003/uni00000058/uni00000056/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000032/uni00000053/uni00000057/uni0000004c/uni00000050/uni0000004c/uni0000005d/uni00000048/uni00000047/uni00000003/uni00000033/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013/uni00000015/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013/uni0000001b/uni00000013/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000014/uni00000015/uni00000013/uni00000013/uni00000014/uni00000017/uni00000013/uni00000013 /uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048 /uni00000015/uni00000018 /uni00000018/uni00000013 /uni0000001a/uni00000018 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000015/uni00000018 /uni00000014/uni00000018/uni00000013 /uni00000014/uni0000001a/uni00000018 /uni00000015/uni00000013/uni00000013/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni0000002c/uni00000059/uni00000033/uni00000048/uni00000051/uni00000040/uni00000003/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047/uni00000003/uni00000058/uni00000056/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000032/uni00000053/uni00000057/uni0000004c/uni00000050/uni0000004c/uni0000005d/uni00000048/uni00000047/uni00000003/uni00000033/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000025/uni00000032/uni0000002c/uni0000002f Figure 5: The learning curves of the best found parameters by different approaches. The curves show that BO-L and BOIL reliably identify parameters leading to stable training. BOIL takes only half total time to ﬁnd this optimal curve. time. BOIL does not add more augmented observations at the later stage when we have gained sufﬁcient information and GP covariance conditioning falls below our threshold δ = 20. 4.2 Ablation study of curve compression To demonstrate the impact of our training curve compression, we compare BOIL to vanilla Bayesian optimization (BO) and with compression (BO-L) given the same number of iterations at Tmax. We show that using the curve compression leads to stable performance, as opposed to the existing tech- nique of averaging the last iterations. We plot the learning curves of the best hyperparameters identiﬁed by BO, BO-L and BOIL. Fig. 5 shows the learning progress over Tmax episodes for each of these. The curves are smoothed by averaging over 100 consecutive episodes for increased clarity. We ﬁrst note that all three algorithms eventually obtain similar performance at the end of learning. However, since BO-L and BOIL take into account the preceding learning steps, they achieve higher performance more quickly. Furthermore, they achieve this more reliably as evidenced by the smaller error bars (shaded regions). 4.3 Tuning deep reinforcement learning and CNN We now optimize hyperparameters for deep reinforcement learning algorithms; in fact, this applica- tion motivated the development of BOIL. The combinations of hyperparameters to be tuned, target DRL algorithm and environment can be found in the appendix. Comparisons by iterations and real-time. Fig. 6 illustrates the performance of different algo- rithms against the number of iterations as well as real-time (the plots for CIFAR10 are in the ap- pendix). The performance is the utility score of the best hyperparameters identiﬁed by the baselines. Across all three tasks, BOIL identiﬁes optimal hyperparameters using signiﬁcantly less computation time than other approaches. The plots show that other approaches such as BO and BO-L can identify well-performing hyperpa- rameters in fewer iterations than BOIL. However, they do so only considering costly, high-ﬁdelity evaluations resulting in signiﬁcantly higher evaluation times. In contrast to this behavior, BOIL ac- counts for the evaluation costs and chooses to initially evaluate low-ﬁdelity settings consuming less time. This allows fast assessments of a multitude of hyperparameters. The information gathered here is then used to inform later point acquisitions. Hereby, the inclusion of augmented observations is crucial in offering useful information readily available from the data. In addition, this augmenta- tion is essential to prevent from the GP kernel issue instead of adding the full curve of points into our GP model. Hyperband [23] exhibits similar behavior in that it uses low ﬁdelity (small t) evaluations to reduce a pool of randomly sampled conﬁgurations before evaluating at high ﬁdelity (large t). To deal with noisy evaluations and other effects, this process is repeated several times. This puts Hyperband at a disadvantage particularly in the noisy DRL tasks. Since early performance ﬂuctuates hugely, Hyperband can be misled in where to allocate evaluation effort. It is then incapable of revising 7/uni00000013 /uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni00000014/uni00000014/uni0000001a/uni00000011/uni00000018 /uni00000014/uni00000015/uni00000013/uni00000011/uni00000013 /uni00000014/uni00000015/uni00000015/uni00000011/uni00000018 /uni00000014/uni00000015/uni00000018/uni00000011/uni00000013 /uni00000014/uni00000015/uni0000001a/uni00000011/uni00000018 /uni00000014/uni00000016/uni00000013/uni00000011/uni00000013 /uni00000014/uni00000016/uni00000015/uni00000011/uni00000018 /uni00000014/uni00000016/uni00000018/uni00000011/uni00000013 /uni00000014/uni00000016/uni0000001a/uni00000011/uni00000018/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000027/uni00000027/uni00000034/uni00000031/uni00000010/uni00000026/uni00000044/uni00000055/uni00000057/uni00000033/uni00000052/uni0000004f/uni00000048/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000003/uni00000044/uni00000051/uni00000047/uni00000003 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013 /uni00000018/uni00000013/uni00000014/uni00000013/uni00000013/uni00000014/uni00000018/uni00000013/uni00000015/uni00000013/uni00000013/uni00000015/uni00000018/uni00000013/uni00000016/uni00000013/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni00000014/uni00000014/uni0000001a/uni00000011/uni00000018 /uni00000014/uni00000015/uni00000013/uni00000011/uni00000013 /uni00000014/uni00000015/uni00000015/uni00000011/uni00000018 /uni00000014/uni00000015/uni00000018/uni00000011/uni00000013 /uni00000014/uni00000015/uni0000001a/uni00000011/uni00000018 /uni00000014/uni00000016/uni00000013/uni00000011/uni00000013 /uni00000014/uni00000016/uni00000015/uni00000011/uni00000018 /uni00000014/uni00000016/uni00000018/uni00000011/uni00000013 /uni00000014/uni00000016/uni0000001a/uni00000011/uni00000018/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000027/uni00000027/uni00000034/uni00000031/uni00000010/uni00000026/uni00000044/uni00000055/uni00000057/uni00000033/uni00000052/uni0000004f/uni00000048/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000003/uni00000044/uni00000051/uni00000047/uni00000003 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013 /uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni00000014/uni00000015/uni0000001b /uni00000014/uni00000015/uni0000001c /uni00000014/uni00000016/uni00000013 /uni00000014/uni00000016/uni00000014 /uni00000014/uni00000016/uni00000015 /uni00000014/uni00000016/uni00000016 /uni00000014/uni00000016/uni00000017/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni00000035/uni00000048/uni00000044/uni00000046/uni0000004b/uni00000048/uni00000055/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013 /uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni00000014/uni00000015/uni0000001b /uni00000014/uni00000015/uni0000001c /uni00000014/uni00000016/uni00000013 /uni00000014/uni00000016/uni00000014 /uni00000014/uni00000016/uni00000015 /uni00000014/uni00000016/uni00000016 /uni00000014/uni00000016/uni00000017/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni00000035/uni00000048/uni00000044/uni00000046/uni0000004b/uni00000048/uni00000055/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013 /uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni00000015/uni00000018 /uni00000016/uni00000013 /uni00000016/uni00000018 /uni00000017/uni00000013 /uni00000017/uni00000018 /uni00000018/uni00000013 /uni00000018/uni00000018 /uni00000019/uni00000013 /uni00000019/uni00000018/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c /uni0000003e/uni00000026/uni00000031/uni00000031/uni00000010/uni00000036/uni00000039/uni0000002b/uni00000031/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000019/uni00000003/uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000053/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013/uni0000001a/uni00000013/uni0000001b/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni00000015/uni00000018 /uni00000016/uni00000013 /uni00000016/uni00000018 /uni00000017/uni00000013 /uni00000017/uni00000018 /uni00000018/uni00000013 /uni00000018/uni00000018 /uni00000019/uni00000013 /uni00000019/uni00000018/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c /uni0000003e/uni00000026/uni00000031/uni00000031/uni00000010/uni00000036/uni00000039/uni0000002b/uni00000031/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000019/uni00000003/uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000053/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f Figure 6: Comparison over BO evaluations (Left) and real-time (Right). Given the same time bud- get, CM-T/F-BO, Hyperband and BOIL can take more evaluations than vanilla BO, BO-L and Rand. BOIL outperforms other competitors in ﬁnding the optimal parameters in an iteration-efﬁcient man- ner. CM-T/F-BO does not augment the observations from the curve and requires more evaluations. The results of InvertedPendulum and CNN-CIFAR10 are in the appendix. these choices until an entirely new pool of hyperparameters is sampled and evaluated from scratch. In contrast to this, BOIL is more ﬂexible than Hyperband in that it can freely explore-exploit the whole joint space. The GP surrogate hereby allows BOIL to generalize across hyperparameters and propagate information through the joint space. 5 Conclusion and Future work Our framework complements the existing BO toolbox for hyperparameter tuning with iterative learn- ing. We present a way of leveraging our understanding that later stages of the training process are informed by progress made in earlier ones. This results in a more iteration-efﬁcient hyperparame- ter tuning algorithm that is applicable to a broad range of machine learning systems. We evaluate its performance on a set of diverse benchmarks. The results demonstrate that our model surpasses the performance of well-established alternatives while consuming signiﬁcantly fewer resources. Fi- nally, we note that our approach is not necessarily speciﬁc to machine learning algorithms, but more generally applies to any process exhibiting an iterative structure to be exploited. 86 Broader Impact Our work aims at making the optimization of processes operating in a step-wise fashion more efﬁ- cient. As demonstrated this makes BOIL particularly well-suited to supporting supervised learning models and RL systems. By increasing training efﬁcience of these models, we hope to contribute to their widespread deployment whilst reducing the computational and therefore environmental cost their implementation has. Deep (reinforcement) learning systems ﬁnd application in a wide range of settings that directly contribute to real world decisions, e.g., natural language processing, visual task, autonomous driving and many more. As machine learning models building on our contributions are being deployed in the real world, we encourage practicioners to put in place necessary supervision and override mechanisms as precautions against potential failure. In a more general context, our algorithm may be seen as a step towards the construction of an automated pipeline for the training and deployment of machine learning models. A potential danger is that humans become further and further removed from the modelling process, making it harder to spot (potentially critical) failures. We do not see this as an argument against the construction of such a pipeline in principle, but instead encourage practicioners to reﬂect on potential biases indirectly encoded in the choice of data sets and models, they are feeding into said automated processes. The growing opacity of machine learning models is a concern of its own and which automated training procedures will only contribute to. Opposing this is a rapidly growing corpus of work addressing the interpretability of trained machine learning models and their decision making. These can and should be used to rigorously analyse ﬁnal training outcomes. Only then can we ensure that machine learning algorithm do indeed become a beneﬁcial source of information guiding real world policy making as opposed to opaque, unquestioned entities. While our main interest lies in the hyperparameter optimization of machine learning models, it should be noted that any iterative process depending on a set of parameters can make use of our con- tributions. Possible settings could, for instance, include the optimization of manufacturing pipelines in which factory setting are adjusted to increase productivity. 7 Acknowledgements S. Schulze is supported by an I-CASE studentship funded by the EPSRC and Dyson. References [1] Christopher M Bishop. Pattern recognition and machine learning. springer New York, 2006. [2] Eric Brochu, Vlad M Cora, and Nando De Freitas. A tutorial on Bayesian optimization of ex- pensive cost functions, with application to active user modeling and hierarchical reinforcement learning. arXiv preprint arXiv:1012.2599, 2010. [3] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016. [4] Yutian Chen, Aja Huang, Ziyu Wang, Ioannis Antonoglou, Julian Schrittwieser, David Silver, and Nando de Freitas. Bayesian optimization in AlphaGo. arXiv preprint arXiv:1812.06855, 2018. [5] Zhongxiang Dai, Haibin Yu, Bryan Kian Hsiang Low, and Patrick Jaillet. Bayesian optimiza- tion meets Bayesian optimal stopping. In International Conference on Machine Learning , pages 1496–1506, 2019. [6] Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford, John Schulman, Szymon Sidor, Yuhuai Wu, and Peter Zhokhov. Openai baselines. GitHub, GitHub repository, 2017. [7] Tobias Domhan, Jost Tobias Springenberg, and Frank Hutter. Speeding up automatic hy- perparameter optimization of deep neural networks by extrapolation of learning curves. In Twenty-Fourth International Joint Conference on Artiﬁcial Intelligence, 2015. 9[8] Stefan Falkner, Aaron Klein, and Frank Hutter. Bohb: Robust and efﬁcient hyperparameter optimization at scale. In International Conference on Machine Learning , pages 1436–1445, 2018. [9] Peter I Frazier. A tutorial on Bayesian optimization. arXiv preprint arXiv:1807.02811, 2018. [10] Yarin Gal, Riashat Islam, and Zoubin Ghahramani. Deep Bayesian active learning with image data. In Proceedings of the 34th International Conference on Machine Learning, pages 1183– 1192, 2017. [11] Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. Deep reinforcement learning that matters. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence, 2018. [12] Philipp Hennig and Christian J Schuler. Entropy search for information-efﬁcient global opti- mization. Journal of Machine Learning Research, 13:1809–1837, 2012. [13] José Miguel Hernández-Lobato, Matthew W Hoffman, and Zoubin Ghahramani. Predictive entropy search for efﬁcient global optimization of black-box functions. In Advances in Neural Information Processing Systems, pages 918–926, 2014. [14] Donald R Jones, Matthias Schonlau, and William J Welch. Efﬁcient global optimization of expensive black-box functions. Journal of Global optimization, 13(4):455–492, 1998. [15] M. I. Jordan and T. M. Mitchell. Machine learning: Trends, perspectives, and prospects. Science, 349(6245):255–260, 2015. [16] Kirthevasan Kandasamy, Gautam Dasarathy, Jeff Schneider, and Barnabás Póczos. Multi- ﬁdelity Bayesian optimisation with continuous approximations. In Proceedings of the 34th International Conference on Machine Learning, pages 1799–1808, 2017. [17] Aaron Klein, Stefan Falkner, Simon Bartels, Philipp Hennig, and Frank Hutter. Fast Bayesian optimization of machine learning hyperparameters on large datasets. In Artiﬁcial Intelligence and Statistics, pages 528–536, 2017. [18] Aaron Klein, Stefan Falkner, Jost Tobias Springenberg, and Frank Hutter. Learning curve pre- diction with Bayesian neural networks. International Conference on Learning Representations (ICLR), 2017. [19] Andreas Krause and Cheng S Ong. Contextual Gaussian process bandit optimization. In Advances in Neural Information Processing Systems, pages 2447–2455, 2011. [20] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolutional neural networks. In Advances in Neural Information Processing Systems, pages 1097–1105, 2012. [21] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. [22] Benjamin Letham, Brian Karrer, Guilherme Ottoni, Eytan Bakshy, et al. Constrained Bayesian optimization with noisy experiments. Bayesian Analysis, 14(2):495–519, 2019. [23] Lisha Li and Kevin Jamieson. Hyperband: A novel bandit-based approach to hyperparameter optimization. Journal of Machine Learning Research, 18:1–52, 2018. [24] Mark McLeod, Stephen Roberts, and Michael A Osborne. Optimization, fast and slow: Op- timally switching between local and Bayesian optimization. In International Conference on Machine Learning, pages 3440–3449, 2018. [25] V olodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforce- ment learning. In International conference on machine learning, pages 1928–1937, 2016. [26] V olodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. NIPS Deep Learning Workshop, 2013. [27] Vu Nguyen, Sunil Gupta, Santu Rana, Cheng Li, and Svetha Venkatesh. Regret for expected improvement over the best-observed value and stopping condition. In Proceedings of The 9th Asian Conference on Machine Learning (ACML), pages 279–294, 2017. [28] Vu Nguyen and Michael A Osborne. Knowing the what but not the where in Bayesian opti- mization. In International Conference on Machine Learning, pages 7317–7326, 2020. 10[29] Michael Osborne, Roman Garnett, Zoubin Ghahramani, David K Duvenaud, Stephen J Roberts, and Carl E Rasmussen. Active learning of model evidence using Bayesian quadrature. In Advances in Neural Information Processing Systems, pages 46–54, 2012. [30] Jack Parker-Holder, Vu Nguyen, and Stephen Roberts. Provably efﬁcient online hyperparame- ter optimization with population-based bandits. In Advances in Neural Information Processing Systems, 2020. [31] Carl Edward Rasmussen. Gaussian processes for machine learning. 2006. [32] Binxin Ru, Mark McLeod, Diego Granziol, and Michael A Osborne. Fast information-theoretic Bayesian optimisation. In International Conference on Machine Learning, pages 4381–4389, 2018. [33] Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. International Conference on Learning Representations, 2016. [34] Rajat Sen, Kirthevasan Kandasamy, and Sanjay Shakkottai. Multi-ﬁdelity black-box opti- mization with hierarchical partitions. In International conference on machine learning, pages 4538–4547, 2018. [35] Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P Adams, and Nando de Freitas. Taking the human out of the loop: A review of Bayesian optimization. Proceedings of the IEEE , 104(1):148–175, 2016. [36] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanc- tot, et al. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484, 2016. [37] Leslie N Smith. A disciplined approach to neural network hyper-parameters: Part 1–learning rate, batch size, momentum, and weight decay. arXiv preprint arXiv:1803.09820, 2018. [38] Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical Bayesian optimization of ma- chine learning algorithms. In Advances in Neural Information Processing Systems , pages 2951–2959, 2012. [39] Jialin Song, Yuxin Chen, and Yisong Yue. A general framework for multi-ﬁdelity Bayesian optimization with Gaussian processes. In The 22nd International Conference on Artiﬁcial Intelligence and Statistics, pages 3158–3167, 2019. [40] Niranjan Srinivas, Andreas Krause, Sham Kakade, and Matthias Seeger. Gaussian process optimization in the bandit setting: No regret and experimental design. In Proceedings of the 27th International Conference on Machine Learning, pages 1015–1022, 2010. [41] Kevin Swersky, Jasper Snoek, and Ryan P Adams. Multi-task Bayesian optimization. In Advances in Neural Information Processing Systems, pages 2004–2012, 2013. [42] Kevin Swersky, Jasper Snoek, and Ryan Prescott Adams. Freeze-thaw Bayesian optimization. arXiv preprint arXiv:1406.3896, 2014. [43] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026–5033. IEEE, 2012. [44] Zi Wang and Stefanie Jegelka. Max-value entropy search for efﬁcient Bayesian optimization. In International Conference on Machine Learning, pages 3627–3635, 2017. [45] Ziyu Wang and Nando de Freitas. Theoretical analysis of Bayesian optimisation with unknown Gaussian process hyper-parameters. arXiv preprint arXiv:1406.7758, 2014. [46] Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and Nando Freitas. Dueling network architectures for deep reinforcement learning. In International Conference on Machine Learning, pages 1995–2003, 2016. [47] Jian Wu and Peter Frazier. The parallel knowledge gradient method for batch Bayesian opti- mization. In Advances In Neural Information Processing Systems, pages 3126–3134, 2016. [48] Jian Wu, Saul Toscano-Palmerin, Peter I Frazier, and Andrew Gordon Wilson. Practical multi- ﬁdelity Bayesian optimization for hyperparameter tuning. In 35th Conference on Uncertainty in Artiﬁcial Intelligence, 2019. 11The following sections are intended to give the reader further insights into our design choices and a deeper understanding of the algorithms properties. First, we give a brief overview of Bayesian optimization with Gaussian processes. We then illustrate our models behavior on a two dimensional problem. Last, we give further details of our experiments for reproducibility purposes. A Bayesian Optimization Preliminaries Bayesian optimization is a sequential approach to global optimization of black-box functions with- out making use of derivatives. It uses two components: a learned surrogate model of the objective function and an acquisition function derived from the surrogate for selecting new points to inform the surrogate with. In-depth discussions beyond our brief overview can be found in recent surveys [2, 9, 35]. Notation. We summarize all of the notations used in our model in Table 1 for ease of reading. A.1 Gaussian processes We present the GP surrogate model for the black-box function f [31]. A GP deﬁnes a probability distribution over functions f under the assumption that any subset of points {(xi, f (xi)}is normally distributed. Formally, this is denoted as: f (x) ∼GP ( m(x),k ( x,x′)) , where m(x) and k (x,x′) are the mean and covariance functions, given by m(x) =E[ f (x)] and k(x,x′) =E [ ( f (x)−m(x))( f (x′)−m(x′))T ] . Typically, the mean of the GP is assumed to be zero everywhere. The kernel k(x,x′) can be thought of as a similarity measure relating f (x) and f (x′). Numerous kernels encoding different prior be- liefs about f (x) have been proposed. A popular choice is given by the square exponential kernel k(x,x′) =σ2 f exp [ −(x −x′)2/2σ2 l ] . The length- and output-scales σ2 l and σ2 f regulate the maximal covariance between two points and can be estimated using maximum marginal likelihood. The SE kernel encodes the belief that nearby points are highly correlated as it is maximized at k(x,x) =σ2 f and decays the further x and x′are separated. For predicting f∗= f (x∗) at a new data point x∗, assuming a zero mean m(x) =0, we have: [ f f∗ ] ∼N ( 0, [ K kT ∗ k∗ k∗∗ ]) (7) where k∗∗= k (x∗,x∗), k∗= [k (x∗,xi)]∀i≤N and K = [k (xi,xj)]∀i, j≤N . The conditional probability of p( f∗|f ) follows a univariate Gaussian distribution as p( f∗|f ) ∼N ( µ (x∗),σ2 (x∗) ) . Its mean and variance are given by: µ (x∗) =k∗K−1y σ2 (x∗) =k∗∗−k∗K−1kT ∗. As GPs give full uncertainty information with any prediction, they provide a ﬂexible nonparametric prior for Bayesian optimization. We refer the interested readers to [31] for further details on GPs. A.2 Acquisition function Bayesian optimization is typically applied in settings in which the objective function is expensive to evaluate. To minimize interactions with that objective, an acquisition function is deﬁned to reason about the selection of the next evaluation point xt+1 = argmaxx∈X αt (x). The acquisition func- tion is constructed from the predictive mean and variance of the surrogate to be easy to evaluate and represents the trade-off between exploration (of points with high predictive uncertainty) and exploitation (of points with high predictive mean). Thus, by design the acquisition function can be maximized with standard global optimization toolboxes. Among the many acquisition functions [12, 13, 14, 32, 40, 44] available in the literature, the expected improvement [14, 27, 45] is one of the most popular. 12Table 1: Notation List Parameter Domain Meaning d integer, N dimension, no. of hyperparameters to be optimized x vector,Rd input hyperparameter N integer, N maximum number of BO iterations Tmin, Tmax integer, N the min/max no of iterations for training a ML algorithm t ∈[Tmin,...Tmax] index of training steps M integer, N the maximum number of augmentation. We set M = 15. δ scalar, R threshold for rejecting augmentation when ln of cond(K) > δ m ∈{1,...M} index of augmenting variables n ∈{1,..., N} index of BO iterations z = [x,t] vector, Rd+1 concatenation of the parameter x and iteration t cn,m scalar, R training cost (sec) yn scalar, R transformed score at the BO iteration n yn,m scalar, R transformed score at the BO iteration n, training step m α(x,t) function acquisition function for performance µc(x,t) function estimation of the cost by LR given x and t r(. |x,t) function a raw learning curve, r(x,t) = [r(1 |x,t),...r(t′|x,t),r(t |x,t)] f (x,t) function a black-box function which is compressed from the above f () l (. |m0,g0) function Logistic curve l(u |m0,g0) = 1 1+exp(−g0[u−m0]) g0, g∗ 0 scalar, R a growth parameter deﬁning a slope, g∗ 0 = argmaxg0 L m0, m∗ 0 scalar, R a middle point parameter, m∗ 0 = argmaxm0 L L scalar, R Gaussian process log marginal likelihood A.3 GP kernels and treatment of GP hyperparameters We present the GP kernels and treatment of GP hyperparameters for the black-box function f . Although the raw learning curve in DRL is noisy, the transformed version using our proposed curve compression is smooth. Therefore, we use two squared exponential kernels for input hyperparameter and training iteration, respectively. That iskx(x,x′) =exp ( −||x−x′||2 2σ2x ) and kt (t,t′) =exp ( −||t−t′||2 2σ2t ) where the observation x and t are normalized to [0,1]d and the outcome y is standardized y ∼ N (0,1) for robustness. As a result, our product kernel becomes k ( [x,t],[x′,t′] ) = k(x,x′)×k(t,t′) =exp ( −||x −x′||2 2σ2x −||t −t′||2 2σ2t ) . The length-scales σx and σt are learnable parameters indicating the variability of the function with regards to the hyperparameter input x and number of training iterations t. Estimating appropriate values for them is critical as this represents the GPs prior regarding the sensitivity of performance w.r.t. changes in the number of training iterations and hyperparameters. For extremely large σt we expect the objective function to change very little for different numbers of training iterations. For small σt by contrast we expect drastic changes even for small differences. We estimate these GP hyperparameters (including the length-scalesσx, σt and the output noise varianceσy) by maximizing their log marginal likelihood [31]. We optimize Eq. (5) with a gradient-based optimizer, providing the analytical gradient to the algo- rithm. We start the optimization from the previous hyperparameter values θprev. If the optimization fails due to numerical issues, we keep the previous value of the hyperparameters. We reﬁt the hy- perparameters every 3×d function evaluations where d is the dimension. B Algorithm Illustration and Further Experiments Fig. 7 and Fig. 8 illustrate the behavior of our proposed algorithm BOIL on the example of opti- mizing the discount factor γ of Dueling DQN [46] on the CartPole problem. The two settings differ in the inclusion augmented observations into BOIL in Fig. 7 and CM-T/F-BO (or BOIL without augmented observations) in Fig. 8. 130.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 1.2 0.8 0.4 0.0 0.4 0.8 1.2 1.6 2.0 2.4 0.00 0.15 0.30 0.45 0.60 0.75 0.90 1.05 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 0.0 0.4 0.8 1.2 1.6 2.0 2.4 2.8 3.2 3.6 0.3 0.6 0.9 1.2 1.5 1.8 2.1 2.4 2.7 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0 2.5 0.00 0.06 0.12 0.18 0.24 0.30 0.36 0.42 0.48 0.64 0.72 0.80 0.88 0.96 1.04 1.12 1.20 1.28 1.36 0.0 0.4 0.8 1.2 1.6 2.0 2.4 2.8 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 1.6 1.2 0.8 0.4 0.0 0.4 0.8 1.2 1.6 2.0 0.00 0.15 0.30 0.45 0.60 0.75 0.90 1.05 0.64 0.72 0.80 0.88 0.96 1.04 1.12 1.20 1.28 1.36 0.0 0.4 0.8 1.2 1.6 2.0 2.4 2.8 3.2 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 2.4 1.6 0.8 0.0 0.8 1.6 2.4 3.2 0.000 0.025 0.050 0.075 0.100 0.125 0.150 0.175 0.200 0.64 0.72 0.80 0.88 0.96 1.04 1.12 1.20 1.28 1.36 0.0 0.6 1.2 1.8 2.4 3.0 3.6 4.2 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 2.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0 0.00 0.15 0.30 0.45 0.60 0.75 0.90 1.05 0.64 0.72 0.80 0.88 0.96 1.04 1.12 1.20 1.28 0.0 0.4 0.8 1.2 1.6 2.0 2.4 2.8 3.2 3.6 0.0 0.4 0.8 1.2 1.6 2.0 2.4 2.8 3.2 3.6 Figure 7: Illustration of BOIL on a 2-dimensional optimization task of DDQN on CartPole. The augmented observations ﬁll the joint hyperparameter-iteration space quickly to inform our surrogate. Our decision balances utility α against cost τ for iteration-efﬁciency. Especially in situations of multiple locations sharing the same utility value, our algorithm prefers to select the cheapest option. Table 2: Dueling DQN algorithm on CartPole problem. Variables Min Max Best Found x∗ γ discount factor 0 .8 1 0 .95586 learning rate model 1 e−6 0.01 0 .00589 #Episodes 300 800 - 140.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 2.0 1.6 1.2 0.8 0.4 0.0 0.4 0.8 0.00 0.15 0.30 0.45 0.60 0.75 0.90 1.05 0.60 0.75 0.90 1.05 1.20 1.35 1.50 1.65 1.80 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 0.0 0.3 0.6 0.9 1.2 1.5 1.8 2.1 2.4 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 1.6 1.2 0.8 0.4 0.0 0.4 0.8 1.2 0.00 0.15 0.30 0.45 0.60 0.75 0.90 1.05 0.60 0.75 0.90 1.05 1.20 1.35 1.50 1.65 1.80 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 1.6 1.2 0.8 0.4 0.0 0.4 0.8 1.2 1.6 0.00 0.08 0.16 0.24 0.32 0.40 0.48 0.56 0.64 0.60 0.75 0.90 1.05 1.20 1.35 1.50 1.65 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 0.30 0.45 0.60 0.75 0.90 1.05 1.20 1.35 1.50 1.65 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 1.6 1.2 0.8 0.4 0.0 0.4 0.8 1.2 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 2.0 1.6 1.2 0.8 0.4 0.0 0.4 0.8 1.2 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 Figure 8: Illustration of the Continuous Multi task/ﬁdelity BO (CM-T/F-BO) -- this is the case of BOIL without using augmented observations (same setting as Fig. 7). This version leads to less efﬁcient optimization as the additional iteration dimension requires more evaluation than optimizing the hyperparameters on their own. 15Table 3: A2C algorithm on Reacher (left) and InvertedPendulum (right). Variables Min Max Best Found x∗ γ discount factor 0 .8 1 0 .8 learning rate actor 1 e−6 0.01 0 .00071 learning rate critic 1 e−6 0.01 0 .00042 #Episodes 200 500 - Min Max Best Found x∗ 0.8 1 0 .95586 1e−6 0.01 0 .00589 1e−6 0.01 0 .00037 700 1500 - Table 4: Convolutional Neural Network. Variables Min Max Best Found x∗ ﬁlter size 1 8 5 pool size 1 5 5 batch size 16 1000 8 learning rate 1 e−6 0.01 0 .000484 momentum 0 .8 0 .999 0 .82852 decay 0 .9 0 .999 0 .9746 number of epoch 30 150 - In both cases, we plot the GP predictive mean in Eq. (1), GP predictive variance in Eq. (2), the acquisition function in Eq. (3), the predicted function and the ﬁnal decision function in Eq. (8). These equations are deﬁned in the main manuscript. As shown in the respective ﬁgures the ﬁnal decision function balances between utility and cost of any pair (γ,t) to achieve iteration efﬁciency. Especially in situations where multiple locations share the same utility value, our decision will prefer to select the cheapest option. Using the augmented observations in Fig. 7, our joint space is ﬁlled quicker with points and the uncertainty (GP variance) across it reduces faster than in Fig. 8 – the case of vanilla CM-T/F-BO without augmenting obser- vations. A second advantage of having augmented observations is that the algorithm is discouraged to select the same hyperparameter setting at lower ﬁdelity than a previous evaluation. We do not add the full curve as it can be redundant while causing the conditioning problem of the GP covariance matrix. B.1 Experiment settings We summarize the hyperparameter search ranges for A2C on Reacher and InvertedPendulum in Table 3, CNN on SHVN in Table 4 and DDQN on CartPole in Table 2. Additionally, we present the best found parameter x∗for these problems. Further details of the DRL agents are listed in Table 5. B.2 Learning Logistic Function We ﬁrst present the Logistic curve l(u |x,t) = 1 1+exp(−g0[u−m0]) using different choices of g0 and m0 in Fig. 10. We then learn from the data to get the optimal choices g∗ 0 and m∗ 0 presented in Fig. 11. Table 5: Further speciﬁcation for DRL agents Hyperparameter Value A2C Critic-network architecture [32,32] Actor-network architecture [32,32] Entropy coefﬁcient 0 .01 Dueling DQN Q-network architecture [50,50] ε-greedy (start, ﬁnal, number of steps) (1.0,0.05,10000) Buffer size 10000 Batch size 64 PER-α [33] 1 .0 PER-β (start, ﬁnal, number of steps) (1.0,0.6,1000) 160 100 200 300 400 500 Episodes 70 60 50 40 30 20 Average Reward Preference Curve as Sigmoid Best Found Reward Curve Sigmoid Curve 0 100 200 300 400 500 Episodes 90 80 70 60 50 40 30 20 10 Reward Preference Curve as Sigmoid Best Found Reward Curve Sigmoid Curve 0 100 200 300 400 500 Episodes 60 50 40 30 20 10 Average Reward Preference Curve as Log Best Found Reward Curve Log Curve 0 100 200 300 400 500 Episodes 100 80 60 40 20 0 Reward Preference Curve as Log Best Found Reward Curve Log Curve 0 100 200 300 400 500 Episodes 60 50 40 30 20 Average Reward Preference Curve as Average Best Found Reward Curve Average Curve 0 100 200 300 400 500 Episodes 80 60 40 20 0 Reward Preference Curve as Average Best Found Reward Curve Average Curve Figure 9: To highlight the robustness, we examine the results using different preference functions such as Sigmoid curve, Log curve, and Average curve on Reacher experiments. The results include the best found reward curve with different preference choices that show the robustness of our model. Left column: the best found curve using averaged reward over 100 consecutive episodes. Right column: the best found curve using the original reward. 17/uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000019 /uni00000013/uni00000011/uni0000001a m0/uni00000020/uni00000010/uni00000016/uni00000003g0/uni00000020/uni00000013/uni00000011/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000016/uni00000003g0/uni00000020/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000016/uni00000003g0/uni00000020/uni00000015 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000016/uni00000003g0/uni00000020/uni00000016 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000019 /uni00000013/uni00000011/uni0000001a m0/uni00000020/uni00000010/uni00000015/uni00000003g0/uni00000020/uni00000013/uni00000011/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000015/uni00000003g0/uni00000020/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000015/uni00000003g0/uni00000020/uni00000015 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000015/uni00000003g0/uni00000020/uni00000016 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000019 m0/uni00000020/uni00000010/uni00000014/uni00000003g0/uni00000020/uni00000013/uni00000011/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000014/uni00000003g0/uni00000020/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000014/uni00000003g0/uni00000020/uni00000015 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000014/uni00000003g0/uni00000020/uni00000016 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000019 m0/uni00000020/uni00000013/uni00000003g0/uni00000020/uni00000013/uni00000011/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000013/uni00000003g0/uni00000020/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000013/uni00000003g0/uni00000020/uni00000015 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000013/uni00000003g0/uni00000020/uni00000016 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000019 m0/uni00000020/uni00000014/uni00000003g0/uni00000020/uni00000013/uni00000011/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000014/uni00000003g0/uni00000020/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000014/uni00000003g0/uni00000020/uni00000015 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000014/uni00000003g0/uni00000020/uni00000016 Figure 10: Examples of Logistic function l(u) = 1 1+exp(−g0[u−m0]) with different values of middle parameter m0 and growth parameter g0. B.3 Robustness over Different Preference Functions We next study the learning effects with respect to different choices of the preference functions. We pick three preference functions including the Sigmoid, Log and Average to compute the utility score for each learning curve. Then, we report the best found reward curve under such choices. The experiments are tested using A2C on Reacher-v2. The results presented in Fig. 9 demonstrate the robustness of our model with the preference functions. B.4 Applying Freeze-Thaw BO in the settings considered While both the exponential decay in Freeze-Thaw BO [42] and our compression function encode preferences regarding training development, there is an important distinction between the two ap- proaches. Freeze-thaw BO utilises the exponential decay property to terminate the training curve, while BOIL only uses the sigmoid curve to guide the search. We refer to Fig. 13 for further illustra- tion of why Freeze-thaw BO struggles in DRL settings. B.5 Ablation Study using Freeze-Thraw Kernel for Time In the joint modeling framework of hyperparameter and time (iteration), we can replace the kernel either k(x,x) or k(t,t) with different choices. We, therefore, set up a new baseline of using the time- kernel k(t,t′) in Freeze-Thaw approach [42] which encodes the monotonously exponential decay from the curve. Particularly, we use the kernel deﬁned as k(t,t′) = βα (t +t′+β)α for parameters α,β > 0 which are optimized in the GP models. 186  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l( . | m * 0 ,g * 0 ) for CartPole m * 0 =-3.266   g * 0 =3.0 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l(m * 0 ,g * 0 ) for CNN_SHVN m * 0 =2.245   g * 0 =2.092 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l(m * 0 ,g * 0 ) for InvPendulum m * 0 =1.649   g * 0 =1.833 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l( . | m * 0 ,g * 0 ) for Cifar10 m * 0 =-4.0   g * 0 =1.476 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l( . | m * 0 ,g * 0 ) for Reacher m * 0 =2.779   g * 0 =1.973 Figure 11: We learn the suitable transformation curve directly from the data. We parameterized the Logistic curve as l (m0,g0) = 1 1+exp(−g0[1−m0]) then estimate g0 and m0. The estimated function l(m∗ 0,g∗ 0) is then used to compress our curve. The above plots are the estimated l() at different environments and datasets. /uni00000013/uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013/uni0000001a/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni0000001b/uni00000013 /uni0000001c/uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000014/uni00000013 /uni00000014/uni00000015/uni00000013/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni0000002c/uni00000051/uni00000059/uni00000048/uni00000055/uni00000057/uni00000048/uni00000047/uni00000033/uni00000048/uni00000051/uni00000047/uni00000058/uni0000004f/uni00000058/uni00000050/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013 /uni00000018/uni00000013/uni00000014/uni00000013/uni00000013/uni00000014/uni00000018/uni00000013/uni00000015/uni00000013/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni0000001b/uni00000013 /uni0000001c/uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000014/uni00000013 /uni00000014/uni00000015/uni00000013/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni0000002c/uni00000051/uni00000059/uni00000048/uni00000055/uni00000057/uni00000048/uni00000047/uni00000033/uni00000048/uni00000051/uni00000047/uni00000058/uni0000004f/uni00000058/uni00000050/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013 /uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni00000014/uni0000001a/uni00000011/uni00000018 /uni00000015/uni00000013/uni00000011/uni00000013 /uni00000015/uni00000015/uni00000011/uni00000018 /uni00000015/uni00000018/uni00000011/uni00000013 /uni00000015/uni0000001a/uni00000011/uni00000018 /uni00000016/uni00000013/uni00000011/uni00000013 /uni00000016/uni00000015/uni00000011/uni00000018/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c /uni0000003e/uni00000026/uni00000031/uni00000031/uni00000010/uni00000026/uni0000004c/uni00000049/uni00000044/uni00000055/uni00000014/uni00000013/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000019/uni00000003/uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000053/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000014/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000018/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni00000014/uni0000001a/uni00000011/uni00000018 /uni00000015/uni00000013/uni00000011/uni00000013 /uni00000015/uni00000015/uni00000011/uni00000018 /uni00000015/uni00000018/uni00000011/uni00000013 /uni00000015/uni0000001a/uni00000011/uni00000018 /uni00000016/uni00000013/uni00000011/uni00000013 /uni00000016/uni00000015/uni00000011/uni00000018/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c /uni0000003e/uni00000026/uni00000031/uni00000031/uni00000010/uni00000026/uni0000004c/uni00000049/uni00000044/uni00000055/uni00000014/uni00000013/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000019/uni00000003/uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000053/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f Figure 12: Tuning hyperparameters of a DRL on InvertedPendulum and a CNN model on CIFAR10. 190 250 500 750 1000 1250 1500 Epoch 0 20 40 60 80 100 120Reward Reward Curve Freeze-thaw 0 250 500 750 1000 1250 1500 Epoch 0 50 100 150 0 250 500 750 1000 1250 1500 Epoch 0 50 100 150 0 250 500 750 1000 1250 1500 Epoch 0 50 100 150 Reward Curves Examples using A2C on Inverted Pendulum Figure 13: Illustration of Freeze-thaw BO in DRL. Freeze-thaw BO will terminate training processes when training performance (in blue) signiﬁcantly drops (i.e. at the red locations) as the exponential decay model will predict low ﬁnal performance. In most RL enviroments noisy training curves are unavoidable. Thus, Freeze-thaw BO will dismiss all curves including good setting, never completing a single training run before the ﬁnal epoch. /uni00000013 /uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni00000014/uni00000015/uni0000001b /uni00000014/uni00000015/uni0000001c /uni00000014/uni00000016/uni00000013 /uni00000014/uni00000016/uni00000014 /uni00000014/uni00000016/uni00000015 /uni00000014/uni00000016/uni00000016 /uni00000014/uni00000016/uni00000017/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni00000035/uni00000048/uni00000044/uni00000046/uni0000004b/uni00000048/uni00000055/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000025/uni00000032/uni0000002c/uni0000002f/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni00000014/uni00000015/uni0000001b /uni00000014/uni00000015/uni0000001c /uni00000014/uni00000016/uni00000013 /uni00000014/uni00000016/uni00000014 /uni00000014/uni00000016/uni00000015 /uni00000014/uni00000016/uni00000016 /uni00000014/uni00000016/uni00000017/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni00000035/uni00000048/uni00000044/uni00000046/uni0000004b/uni00000048/uni00000055/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000025/uni00000032/uni0000002c/uni0000002f/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000013 /uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni0000001c/uni00000013 /uni0000001c/uni00000018 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000018 /uni00000014/uni00000014/uni00000013 /uni00000014/uni00000014/uni00000018 /uni00000014/uni00000015/uni00000013 /uni00000014/uni00000015/uni00000018/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni0000002c/uni00000051/uni00000059/uni00000048/uni00000055/uni00000057/uni00000048/uni00000047/uni00000033/uni00000048/uni00000051/uni00000047/uni00000058/uni0000004f/uni00000058/uni00000050/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000026/uni00000010/uni00000030/uni00000012/uni00000037/uni00000010/uni00000025/uni00000032/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000025/uni00000032/uni0000002c/uni0000002f/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000013 /uni00000018/uni00000013/uni00000014/uni00000013/uni00000013/uni00000014/uni00000018/uni00000013/uni00000015/uni00000013/uni00000013/uni00000015/uni00000018/uni00000013/uni00000016/uni00000013/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni0000001c/uni00000013 /uni0000001c/uni00000018 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000018 /uni00000014/uni00000014/uni00000013 /uni00000014/uni00000014/uni00000018 /uni00000014/uni00000015/uni00000013 /uni00000014/uni00000015/uni00000018/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni0000002c/uni00000051/uni00000059/uni00000048/uni00000055/uni00000057/uni00000048/uni00000047/uni00000033/uni00000048/uni00000051/uni00000047/uni00000058/uni0000004f/uni00000058/uni00000050/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000025/uni00000032/uni0000002c/uni0000002f/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a Figure 14: Comparison using freezethaw kernel for time component. We present the result in Fig. 14 that CM-T/F-BO is still less competitive to BOIL using this speciﬁc time kernel. The results again validate the robustness our approach cross different choices of kernel. B.6 Additional Experiments for Tuning DRL and CNN We present the additional experiments for tuning a DRL model using InvertedPendulum environ- ment and a CNN model using a subset of CIFAR10 in Fig. 12. Again, we show that the proposed model clearly gain advantages against the baselines in tuning hyperparameters for model with itera- tive learning information available. B.7 Examples of Deep Reinforcement Learning Training Curves Finally, we present examples of training curves produced by the deep reinforcement learning al- gorithm A2C in Fig. 15. These ﬂuctuate widely and it may not be trivial to deﬁne good stopping criteria as done for other applications in previous work [42]. 200 200 400 80 70 60 50 40 0 200 400 110 100 90 80 70 60 50 0 200 400 110 100 90 80 70 60 50 0 200 400 70 60 50 40 30 0 200 400 70 60 50 40 30 20 10 0 200 400 50 40 30 20 0 200 400 90 80 70 60 50 0 200 400 70 60 50 40 30 20 10 0 200 400 85 80 75 70 0 200 400 60 50 40 30 20 0 200 400 100 90 80 70 60 0 200 400 100 90 80 70 0 200 400 80 60 40 20 0 200 400 100 90 80 70 60 0 200 400 100 90 80 70 0 200 400 80 70 60 50 40 30 20 0 200 400 70 60 50 40 30 20 0 200 400 70 60 50 40 30 20 0 200 400 100 90 80 70 60 50 0 200 400 70 60 50 40 30 20 0 200 400 100 90 80 70 60 50 0 200 400 100 90 80 70 60 0 200 400 60 50 40 30 20 0 200 400 80 60 40 20 0 200 400 85 80 75 70 65 60 0 200 400 100 95 90 85 80 0 200 400 80 60 40 20 0 200 400 100 90 80 70 60 50 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 Figure 15: Examples of reward curves using A2C on Reacher-v2 (rows 1 −3) and on InvertedPendulum-v2 (rows 4 −6). Y-axis is the reward averaged over 100 consecutive episodes. X-axis is the episode. The noisy performance illustrated is typical of DRL settings and complicates the design of early stopping criteria. Due to the property of DRL, it is not trivial to decide when to stop the training curve. In addition, it will be misleading if we only take average over the last 100 iterations. 21",
      "meta_data": {
        "arxiv_id": "1909.09593v5",
        "authors": [
          "Vu Nguyen",
          "Sebastian Schulze",
          "Michael A Osborne"
        ],
        "published_date": "2019-09-20T16:14:34Z",
        "pdf_url": "https://arxiv.org/pdf/1909.09593v5.pdf",
        "github_url": "https://github.com/ntienvu/BOIL"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces a Bayesian Optimization (BO) approach, named BOIL, for efficient hyperparameter tuning in iterative learning systems like Deep Learning (DL) and Deep Reinforcement Learning (DRL). It addresses the problem of expensive hyperparameter tuning that traditionally ignores intermediate training information. Key contributions include an algorithm to optimize the learning curve by compressing training progress into a single numeric score (considering stability and success), an approach to learn this compression curve from data, and a data augmentation technique for improved sample-efficiency. BOIL balances assessment benefit against computation cost and selectively includes scores from different training steps. It is demonstrated to outperform existing baselines in identifying optimal hyperparameters in minimal wall-clock time across DRL agents and convolutional neural networks.",
        "methodology": "The BOIL framework models the cost-sensitive black-box function `f(x,t)` (hyperparameter `x`, iterations `t`) as a Gaussian Process (GP) using a product kernel `k(x,x') × k(t,t')`. The cost function `c(x,t)` is approximated by a linear regressor. The next hyperparameter-iteration point is selected by maximizing a cost-aware acquisition function `α(x,t)/µc(x,t)`. A novel 'training curve compression' method transforms the entire learning curve into a single numeric score `y` using a parameterized Sigmoid preference function `l(u|m0,g0)`, where parameters `m0` and `g0` are learned by maximizing the GP log marginal likelihood. To enhance sample efficiency and prevent GP covariance matrix ill-conditioning, a data augmentation technique is used, selectively sampling intermediate points at maximum GP predictive uncertainty, ensuring the natural log of the covariance matrix condition number remains below a threshold (δ = 20).",
        "experimental_setup": "Experiments were conducted on two DRL agents (Dueling DQN, Advantage Actor Critic) across three OpenAI gym/Mujoco environments (CartPole-v0, InvertedPendulum-v2, Reacher-v2) and a Convolutional Neural Network on two datasets (SVHN, CIFAR10). All results were averaged over 20 independent runs using NVIDIA 1080 GTX GPUs with TensorFlow-GPU. The GP models utilized square-exponential kernels whose parameters were optimized via marginal likelihood maximization. The data augmentation strategy set a maximum of 15 augmented points and a condition number threshold of 20. Baselines included Hyperband and Continuous Multi-task/Multi-fidelity BO (CM-T/F-BO), which extends discrete multi-task BO to continuous settings. Ablation studies with vanilla BO and BO-L (BO with curve compression) were also performed, and a comparison using a Freeze-Thaw-like exponential decay kernel was included.",
        "limitations": "The unpredictability and significant fluctuations of DRL reward curves make traditional stopping criteria, such as the exponential decay assumed in methods like Freeze-thaw BO, unsuitable. A naive approach to data augmentation by adding an entire curve of points can lead to redundancy and serious ill-conditioning issues for the GP covariance matrix, particularly in noisy DRL environments. The approximation of the cost function with a linear regressor might be insufficient if the cost dependence on hyperparameters and iterations is more complex, requiring alternative models like a second GP.",
        "future_research_directions": "The proposed framework's applicability extends beyond machine learning, suggesting its use in any iterative process dependent on a set of parameters, such as optimizing manufacturing pipelines to increase productivity. Broader implications include increasing the training efficiency of ML models to reduce computational and environmental costs, facilitating their widespread deployment. However, it also highlights the need for practitioners to implement robust supervision and override mechanisms for deployed models, and to critically reflect on potential biases in datasets and models used in automated pipelines. Future work should continue to address the growing opacity of ML models through rigorous analysis of training outcomes, potentially using rapidly developing interpretability techniques.",
        "experimental_code": "from bayes_opt.sequentialBO.boil import BOIL\nfrom bayes_opt.product_gaussian_process import ProductGaussianProcess\nfrom bayes_opt.curve_compression import apply_one_transform_logistic, transform_logistic\nfrom bayes_opt.acquisition_functions import AcquisitionFunction\nfrom bayes_opt.acquisition_maximization import acq_max_with_name, acq_min_scipy_kwargs\nfrom sklearn import linear_model\n\n# --- bayes_opt/sequentialBO/boil.py ---\n\nclass BOIL(object):\n\n    def __init__(self, func, SearchSpace,acq_name=\"ei_mu_max\",verbose=1):\n        self.method='boil'\n        self.verbose=verbose\n        if isinstance(SearchSpace,dict):\n            self.keys = list(SearchSpace.keys())\n            self.SearchSpace = []\n            for key in list(SearchSpace.keys()):\n                self.SearchSpace.append(SearchSpace[key])\n            self.SearchSpace = np.asarray(self.SearchSpace)\n        else:\n            self.SearchSpace=np.asarray(SearchSpace)\n            \n        self.dim = len(SearchSpace)\n\n        scaler = MinMaxScaler()\n        scaler.fit(self.SearchSpace[:-1,:].T)\n        \n        scalerT = MinMaxScaler()\n        SearchSpace_T=np.atleast_2d(self.SearchSpace[-1,:]).T\n        scalerT.fit(SearchSpace_T)\n\n        self.Xscaler=scaler\n        self.Tscaler=scalerT\n\n        self.scaleSearchSpace=np.array([np.zeros(self.dim), np.ones(self.dim)]).T\n                \n        self.f = func\n    \n        self.X_ori= None\n        self.X = None\n        self.Y = None\n        self.Y_ori = None\n        self.T=None\n        self.T_original=None\n        self.Y_cost_original=None\n        self.time_opt=0\n         \n        self.max_min_gap=self.SearchSpace[:,1]-self.SearchSpace[:,0]\n\n        self.acq_name = acq_name\n        self.logmarginal=0\n\n        self.gp=ProductGaussianProcess(self.scaleSearchSpace,verbose=verbose)\n\n        self.Y_curves=[]\n        self.Y_cost_original=None\n        self.time_opt=0\n        self.acq_func = None\n        self.logmarginal=0\n        self.markVirtualObs=[]\n        self.countVirtual=[]\n        self.linear_regression = linear_model.LinearRegression()\n        self.condition_number=[]\n        self.max_n_augmentation=10\n        self.threshold_cond=15\n        \n    def init(self, n_init_points=3, seed=1):\n        np.random.seed(seed)\n\n        SearchSpace=np.copy(self.SearchSpace)\n        SearchSpace[-1,0]=SearchSpace[-1,1]\n\n        l = [np.random.uniform(x[0], x[1]) for _ in range(n_init_points) for x in SearchSpace] \n\n        temp=np.asarray(l)\n        temp=temp.T\n        init_X=list(temp.reshape((n_init_points,-1)))\n        \n        self.X_original = np.asarray(init_X)\n        self.T_original=self.X_original[:,-1]\n        self.T_original=np.reshape(self.T_original,(n_init_points,-1))\n        \n        self.X_original=self.X_original[:,:-1]\n        self.X_original=np.reshape(self.X_original,(n_init_points,-1))\n\n        y_init_curves, y_init_cost=self.f(init_X)\n\n        y_init_cost=np.atleast_2d(np.asarray(y_init_cost))\n\n        self.Y_curves+=y_init_curves\n\n        y_init=transform_logistic(y_init_curves,self.gp.logistic_hyper['midpoint'],\\\n                                  self.gp.logistic_hyper['growth'], self.SearchSpace[-1,1])\n        y_init=np.reshape(y_init,(n_init_points,1))\n        \n        self.Y_original = np.asarray(y_init)      \n        self.Y_cost_original=np.reshape(y_init_cost,(-1,1))\n\n        self.X = self.Xscaler.transform(np.asarray(init_X)[:,:-1])\n        self.X=np.reshape(self.X,(n_init_points,-1))\n\n        self.T = self.Tscaler.transform(self.T_original)\n\n        self.markVirtualObs+=[0]*n_init_points\n\n        for ii in range(n_init_points):\n            self.generating_virtual_observations(self.X[ii,:],\n                         self.T[ii],[y_init_curves[ii]],y_init_cost[0][ii],IsRandom=False)\n\n        self.Y_cost=(self.Y_cost_original-np.min(self.Y_cost_original))/(np.max(self.Y_cost_original)-np.min(self.Y_cost_original))\n\n        if np.std(self.Y_original)==0:\n            self.Y=(self.Y_original-np.mean(self.Y_original))\n        else:\n            self.Y=(self.Y_original-np.mean(self.Y_original))/np.std(self.Y_original)\n\n       \n    def utility_cost_evaluation(self,x,acq_func,isDebug=False):\n        def utility_cost_evaluation_single(x,acq_func,isDebug=False):\n            utility=acq_func.acq_kind(x,gp=self.gp)\n            \n            try:\n                mean_cost=self.linear_regression.predict(np.reshape(x,(1,-1)))\n                \n            except:\n                print(x)\n                print(\"bug\")\n    \n            mean_cost=max(0,mean_cost)+0.1\n            \n            if 'ei' in acq_func.acq_name:\n                acquisition_function_value= np.log(utility)-np.log(mean_cost)\n            else:\n                acquisition_function_value= np.log(1+np.exp(utility))/np.log(1+np.exp(mean_cost))\n    \n            if isDebug==True:\n                print(\"acq_func at the selected point \\t utility:\",np.round(utility,decimals=4),\"\\t cost:\",mean_cost)\n                if utility==0:\n                    print(\"utility =0===============================================================================\")\n       \n            return acquisition_function_value*(-1)\n        \n        \n        if len(x)==self.dim:\n            temp=utility_cost_evaluation_single(x,acq_func,isDebug=isDebug)\n            if isDebug==True:\n                return temp\n            else:\n                utility=np.mean(temp)\n        \n        else:\n            utility=[0]*len(x)\n            for idx,val in enumerate(x):\n                temp=utility_cost_evaluation_single(x=val,acq_func=acq_func,isDebug=isDebug)\n                                                     \n                utility[idx]=np.mean(temp)\n                \n            utility=np.asarray(utility)    \n        return utility   \n    \n        \n    def acq_utility_cost(self):\n        acq={}\n        acq['name']=self.acq_name\n        acq['dim']=self.scaleSearchSpace.shape[0]\n        acq['scaleSearchSpace']=self.scaleSearchSpace   \n    \n        if self.acq_name=='ei_mu_max':\n            x_mu_max,mu_max_val=acq_max_with_name(gp=self.gp,scaleSearchSpace=self.scaleSearchSpace,acq_name='mu',IsReturnY=True)\n            acq['mu_max']=  mu_max_val\n\n        myacq=AcquisitionFunction(acq)\n        \n        x_min = acq_min_scipy_kwargs(myfunc=self.utility_cost_evaluation,SearchSpace=self.scaleSearchSpace,\n                        acq_func=myacq, isDebug=False)\n        \n        if self.verbose==True:\n            acq_val=self.utility_cost_evaluation(x_min,myacq,isDebug=False)\n            print(\"selected point from acq func:\",np.round(x_min,decimals=4),\"acq val=log(Utility/Cost)=\",(-1)*np.round(acq_val,decimals=4))\n            if np.round(acq_val,decimals=4)==0:\n                print(\"acq value =0\")\n            \n        return x_min\n    \n    \n    def select_informative_location_by_uncertainty(self,n_virtual_obs,x_max,t_max):\n        SearchSpace=np.copy(self.scaleSearchSpace)\n        for dd in range(self.dim-1):\n            SearchSpace[dd,0],SearchSpace[dd,1]=x_max[dd],x_max[dd]\n            \n        SearchSpace[-1,1]=t_max\n        \n        temp_X,temp_T=self.X.copy(),self.T.copy()\n        temp_gp=copy.deepcopy(self.gp )\n        \n        temp_Y=np.random.random(size=(len(temp_T),1))\n        \n        temp_gp.fit(temp_X,temp_T,temp_Y,self.Y_curves)\n        \n        new_batch_T=None\n\n        pred_var_value=[0]*n_virtual_obs\n        for ii in range(n_virtual_obs):\n            x_max_pred_variance, pred_var_value[ii]=acq_max_with_name(gp=temp_gp,\n                              scaleSearchSpace=SearchSpace,acq_name='pure_exploration',IsReturnY=True)\n            \n            log_cond=np.log( temp_gp.compute_condition_number() )\n            if log_cond>self.threshold_cond or pred_var_value[ii]<(self.gp.noise_delta+1e-3):\n                break\n          \n            if x_max_pred_variance[-1] in temp_T[-ii:]: # if repetition, stop augmenting\n                break\n            \n            temp_X = np.vstack((temp_X, x_max.reshape((1, -1))))\n            temp_T = np.vstack((temp_T, x_max_pred_variance[-1].reshape((1, -1))))\n            temp_gp.X,temp_gp.T=temp_X,temp_T\n            temp_Y=np.random.random(size=(len(temp_T),1))\n            \n            temp_gp.fit(temp_X,temp_T,temp_Y,self.Y_curves)\n\n            if new_batch_T is None:\n                new_batch_T=x_max_pred_variance[-1].reshape((1, -1))\n            else:\n                new_batch_T= np.vstack((new_batch_T, x_max_pred_variance[-1].reshape((1, -1))))\n        \n        if new_batch_T is None:\n            return [],0\n\n        else:\n            output=np.sort(new_batch_T.ravel()).tolist()\n            return output, len(output)\n\n    \n    def generating_virtual_observations(self,x_max,t_max,y_original_curves,y_cost_original,IsRandom=False):\n        \n        temp_X_new_original=self.Xscaler.inverse_transform(np.reshape(x_max,(-1,self.dim-1)))\n\n        max_n_virtual_obs=np.int(t_max*self.max_n_augmentation)\n        if max_n_virtual_obs==0:\n            self.countVirtual.append(0)\n            return\n        \n        if IsRandom==True:\n            l = [np.random.uniform(0, t_max) for _ in range(max_n_virtual_obs)]\n        else:\n            l,n_virtual_obs=self.select_informative_location_by_uncertainty(max_n_virtual_obs,x_max,t_max)\n            \n        self.countVirtual.append(n_virtual_obs)\n        \n        if self.verbose:\n            np.set_printoptions(suppress=True)\n            print(\"Max #augmented points\",max_n_virtual_obs, \"\\t #augmented points \",len(l),\n                  \"\\t Augmented points: \",np.round(l,decimals=3))\n            \n        l_original=[self.SearchSpace[-1,0]+val*self.max_min_gap[-1] for val in l]\n                           \n        virtual_obs_t_original=np.asarray(l_original).T\n        virtual_obs_t=np.asarray(l).T\n        \n        y_virtual_original=[0]*n_virtual_obs\n        for ii in range(n_virtual_obs):\n            \n            idx=np.int(virtual_obs_t_original[ii])\n            \n            temp_curve=y_original_curves[0][:idx+1]\n            self.markVirtualObs.append(1)\n\n            y_virtual_original[ii]=transform_logistic([temp_curve],\\\n                      self.gp.logistic_hyper['midpoint'],self.gp.logistic_hyper['growth'],self.SearchSpace[-1,1])\n           \n            self.X = np.vstack((self.X, x_max.reshape((1, -1))))\n            self.X_original=np.vstack((self.X_original, temp_X_new_original))\n        \n            self.T = np.vstack((self.T, virtual_obs_t[ii].reshape((1, -1))))\n            temp=np.asarray(virtual_obs_t_original[ii])\n            self.T_original=np.vstack((self.T_original, temp.reshape((1, -1))))\n\n\n            self.Y_original = np.append(self.Y_original,[y_virtual_original[ii]])\n            self.Y_curves.append(temp_curve)\n            \n            y_cost_estimate=y_cost_original*virtual_obs_t[ii]\n            self.Y_cost_original = np.append(self.Y_cost_original,[y_cost_estimate])\n            \n        \n    def suggest_nextpoint(self):\n \n        self.gp=ProductGaussianProcess(self.scaleSearchSpace,self.gp.hyper,self.gp.logistic_hyper)\n        self.gp.fit(self.X, self.T,self.Y,self.Y_curves)\n            \n        self.condition_number.append(self.gp.cond_num)\n        if self.verbose:\n            print(\"ln of conditioning number of GP covariance matrix\", np.round(np.log(self.gp.cond_num),decimals=1))\n\n        count=len(self.markVirtualObs)-np.sum(self.markVirtualObs)\n        count=np.int(count)\n\n        if  len(self.Y)%(2*self.dim)==0:\n\n            hyper=[self.gp.hyper['lengthscale_x'],self.gp.hyper['lengthscale_t'], \\\n                   self.gp.logistic_hyper['midpoint'],self.gp.logistic_hyper['growth']]\n            newlengthscale_x,newlengthscale_t,new_midpoint, new_growth = self.gp.optimize_lengthscale_logistic_hyper(hyper,self.gp.noise_delta)\n            \n            self.gp.hyper['lengthscale_x']=newlengthscale_x\n            self.gp.hyper['lengthscale_t']=self.gp.hyper['lengthscale_t']\n            self.gp.logistic_hyper['midpoint']=new_midpoint\n            self.gp.logistic_hyper['growth']=new_growth\n          \n            if self.verbose:\n                print(\"==estimated lengthscale_x={:.4f}   lengthscale_t={:.3f}   Logistic_m0={:.1f}   Logistic_g0={:.1f}\".format(\n                    newlengthscale_x,newlengthscale_t,new_midpoint,new_growth))\n                \n        start_opt=time.time()\n\n        combine_input=np.hstack((self.X,self.T))\n        self.linear_regression.fit(combine_input,self.Y_cost)\n        \n        x_max_temp=self.acq_utility_cost()\n        x_max=x_max_temp[:-1]\n        t_max=x_max_temp[-1]\n            \n        finished_opt=time.time()\n        elapse_opt=finished_opt-start_opt\n        self.time_opt=np.hstack((self.time_opt,elapse_opt))\n\n        self.markVirtualObs.append(0)\n\n        self.X = np.vstack((self.X, x_max.reshape((1, -1))))\n        self.T = np.vstack((self.T, t_max.reshape((1, -1))))\n\n        temp_X_new_original=self.Xscaler.inverse_transform(np.reshape(x_max,(-1,self.dim-1)))\n        self.X_original=np.vstack((self.X_original, temp_X_new_original))\n        \n        temp_T_new_original=self.Tscaler.inverse_transform(np.reshape(t_max,(-1,1)))\n        self.T_original=np.vstack((self.T_original, temp_T_new_original))\n\n        x_original_to_test=x_max_temp*self.max_min_gap+self.SearchSpace[:,0]\n\n        y_original_curves, y_cost_original= self.f(x_original_to_test)\n        \n        y_original=transform_logistic(y_original_curves,\\\n              self.gp.logistic_hyper['midpoint'],self.gp.logistic_hyper['growth'],self.SearchSpace[-1,1])\n        \n        if len(y_original_curves)==1:\n            self.Y_curves.append(y_original_curves[0])\n        else:\n            self.Y_curves.append(y_original_curves)\n\n        \n        self.Y_original = np.append(self.Y_original,y_original)\n        self.Y_cost_original = np.append(self.Y_cost_original,y_cost_original)\n\n        self.generating_virtual_observations(x_max,t_max,y_original_curves,y_cost_original[0])\n        \n        if np.std(self.Y_original)==0:\n            self.Y=(self.Y_original-np.mean(self.Y_original))\n        else:\n            self.Y=(self.Y_original-np.mean(self.Y_original))/np.std(self.Y_original)\n            \n        self.Y_cost=(self.Y_cost_original-np.min(self.Y_cost_original))/(np.max(self.Y_cost_original)-np.min(self.Y_cost_original))\n                    \n        np.set_printoptions(suppress=True)\n\n        print(\"[original scale] x={} t={:.0f} current y={:.2f}, ybest={:.2f}\".format( np.round(self.X_original[-1],decimals=4),\\n              np.asscalar(self.T_original[-1]),np.asscalar(self.Y_original[-1]), np.asscalar(self.Y_original.max())))\n\n\n# --- bayes_opt/product_gaussian_process.py ---\n\nclass ProductGaussianProcess(object):\n    \n    def __init__ (self,SearchSpace,gp_hyper=None,logistic_hyper=None,verbose=0):\n        self.noise_delta=5e-4\n        self.noise_upperbound=1e-2\n        self.mycov=self.cov_RBF_time\n        self.SearchSpace=SearchSpace\n        scaler = MinMaxScaler()\n        scaler.fit(SearchSpace.T)\n        self.Xscaler=scaler\n        self.verbose=verbose\n        self.dim=SearchSpace.shape[0]\n        \n        if gp_hyper is None:\n            self.hyper={}\n            self.hyper['var']=1\n            self.hyper['lengthscale_x']=0.02\n            self.hyper['lengthscale_t']=0.2\n        else:\n            self.hyper=gp_hyper\n\n        \n        if logistic_hyper is None:\n            self.logistic_hyper={}\n            self.logistic_hyper['midpoint']=0.0\n            self.logistic_hyper['growth']=1.0   \n        else:\n            self.logistic_hyper=logistic_hyper\n\n        self.X=[]\n        self.T=[]\n        self.Y=[]\n        self.Y_curves=None\n        \n        self.alpha=[]\n        self.L=[]\n        \n        self.MaxEpisode=0\n        \n        return None\n       \n\n    def cov_RBF_time(self, x1,t1,x2,t2,lengthscale,lengthscale_t):\n        \n        Euc_dist=euclidean_distances(x1,x2)\n        exp_dist_x=np.exp(-np.square(Euc_dist)/lengthscale)\n        \n        Euc_dist=euclidean_distances(t1,t2)\n        exp_dist_t=np.exp(-np.square(Euc_dist)/lengthscale_t)\n        \n        return exp_dist_x*exp_dist_t\n                \n    def fit(self,X,T,Y,Y_curves):\n        temp=np.hstack((X,T))\n        ur = unique_rows(temp)\n        \n        T=T[ur]\n        X=X[ur]\n        Y=Y[ur]\n        \n        self.X=X\n        self.Y=Y\n        self.T=T\n        self.Y_curves=[val for idx,val in enumerate(Y_curves) if ur[idx]==True]\n        \n        for curves in self.Y_curves:\n            self.MaxEpisode=max(len(curves),self.MaxEpisode)\n            \n        Euc_dist_x=euclidean_distances(X,X)\n    \n        Euc_dist_t=euclidean_distances(T,T)\n       \n        self.KK_x_x=np.exp(-np.square(Euc_dist_x)/self.hyper['lengthscale_x']\\\n                           -np.square(Euc_dist_t)/self.hyper['lengthscale_t'])+np.eye(len(X))*self.noise_delta\n          \n        if np.isnan(self.KK_x_x).any():\n            print(\"nan in KK_x_x\")\n        \n        self.L=np.linalg.cholesky(self.KK_x_x)\n        temp=np.linalg.solve(self.L,self.Y)\n        self.alpha=np.linalg.solve(self.L.T,temp)\n        self.cond_num=self.compute_condition_number()\n        \n    def compute_condition_number(self):\n        cond_num=np.linalg.cond(self.KK_x_x)\n        return cond_num\n    \n\n    def log_marginal_lengthscale_logistic_hyper(self,hyper,noise_delta):\n        def compute_log_marginal_with_logistic_hyper(lengthscale, lengthscale_t,midpoint,growth,noise_delta):\n            temp=np.hstack((self.X,self.T))\n            ur = unique_rows(temp)\n            myX=self.X[ur]\n            myT=self.T[ur]\n            \n            Y_original=transform_logistic(self.Y_curves,midpoint,growth,self.MaxEpisode)\n            myY=(Y_original-np.mean(Y_original))/np.std(Y_original)\n            \n            myY=myY[ur]\n          \n            self.Euc_dist_x=euclidean_distances(myX,myX)\n            self.Euc_dist_t=euclidean_distances(myT,myT)\n        \n            KK=np.exp(-np.square(self.Euc_dist_x)/lengthscale-np.square(self.Euc_dist_t)/lengthscale_t)\n                +np.eye(len(myX))*noise_delta\n                    \n            \n            try:\n                temp_inv=np.linalg.solve(KK,myY)\n            except: # singular\n                return -np.inf\n            \n            try:\n                first_term=-0.5*np.dot(myY.T,temp_inv)\n                \n                if KK.shape[0]>200:\n                    idx=np.random.permutation(KK.shape[0])\n                    idx=idx[:200]\n                    KK=KK[np.ix_(idx,idx)]\n                chol  = spla.cholesky(KK, lower=True)\n                W_logdet=np.sum(np.log(np.diag(chol)))\n    \n                second_term=-W_logdet\n            except: # singular\n                return -np.inf\n            \n\n            logmarginal=first_term+second_term-0.5*len(myY)*np.log(2*3.14)\n                \n            if np.isnan(np.asscalar(logmarginal))==True:\n                print(\"lengthscale_x={:f} lengthscale_t={:f} first term ={:.4f} second  term ={:.4f}\".format(\n                        lengthscale,lengthscale_t,np.asscalar(first_term),np.asscalar(second_term)))\n\n            return np.asscalar(logmarginal)\n        \n        logmarginal=0\n\n        if not isinstance(hyper,list) and len(hyper.shape)==2:\n            logmarginal=[0]*hyper.shape[0]\n            growth=hyper[:,3]\n            midpoint=hyper[:,2]\n            lengthscale_t=hyper[:,1]\n            lengthscale_x=hyper[:,0]\n            for idx in range(hyper.shape[0]):\n                logmarginal[idx]=compute_log_marginal_with_logistic_hyper(lengthscale_x[idx],\\\n                           lengthscale_t[idx],midpoint[idx],growth[idx],noise_delta)\n        else:\n            lengthscale_x,lengthscale_t,midpoint,growth=hyper\n            logmarginal=compute_log_marginal_with_logistic_hyper(lengthscale_x,lengthscale_t,\\\n                                                                 midpoint,growth,noise_delta)\n        return logmarginal\n    \n    def optimize_lengthscale_SE_logistic_hyper(self,previous_hyper,noise_delta):\n        SearchSpace_l_min=0.03\n        SearchSpace_l_max=0.3\n        \n        SearchSpace_midpoint_min=-2\n        SearchSpace_midpoint_max=3\n        \n        SearchSpace_growth_min=0.5\n        SearchSpace_growth_max=2\n        \n        mySearchSpace=np.asarray([[SearchSpace_l_min,SearchSpace_l_max],[10*SearchSpace_l_min,2*SearchSpace_l_max],\n                             [SearchSpace_midpoint_min,SearchSpace_midpoint_max],[SearchSpace_growth_min,SearchSpace_growth_max]])\n        \n        lengthscale_tries = np.random.uniform(mySearchSpace[:, 0], mySearchSpace[:, 1],size=(20, 4))\n\n        self.flagOptimizeHyperFirst=0\n\n        logmarginal_tries=self.log_marginal_lengthscale_logistic_hyper(lengthscale_tries,noise_delta)\n\n        idx_max=np.argmax(logmarginal_tries)\n        lengthscale_init_max=lengthscale_tries[idx_max]\n        \n        myopts ={'maxiter':30*self.dim,'maxfun':30*self.dim}\n\n        x_max=[]\n        max_log_marginal=None\n        \n        res = minimize(lambda x: -self.log_marginal_lengthscale_logistic_hyper(x,noise_delta),lengthscale_init_max,\n                       bounds=mySearchSpace,method=\"L-BFGS-B\",options=myopts)\n        if 'x' not in res:\n            val=self.log_marginal_lengthscale_logistic_hyper(res,noise_delta)    \n        else:\n            val=self.log_marginal_lengthscale_logistic_hyper(res.x,noise_delta)  \n        \n        if max_log_marginal is None or val >= max_log_marginal:\n            if 'x' not in res:\n                x_max = res\n            else:\n                x_max = res.x\n            max_log_marginal = val\n\n        return x_max\n\n    def optimize_lengthscale_logistic_hyper(self,prev_hyper,noise_delta):\n        newlengthscale,newlengthscale_t,newmidpoint,newgrowth=self.optimize_lengthscale_SE_logistic_hyper(prev_hyper,noise_delta)\n        self.hyper['lengthscale_x']=newlengthscale\n        self.hyper['lengthscale_t']=newlengthscale_t\n        \n        temp=np.hstack((self.X,self.T))\n        ur = unique_rows(temp)\n\n        Y_original=transform_logistic(self.Y_curves,newmidpoint,newgrowth,self.SearchSpace[-1,1])\n        Y=(Y_original-np.mean(Y_original))/np.std(Y_original)\n        self.Y=Y\n        \n        self.fit(self.X[ur],self.T[ur],self.Y[ur],self.Y_curves)\n        \n        return newlengthscale,newlengthscale_t,newmidpoint,newgrowth\n        \n    def predict(self,xTest, eval_MSE=True):\n        if len(xTest.shape)==1:\n            xTest=xTest.reshape((-1,self.X.shape[1]+1))\n            \ntTest=xTest[:,-1]\ntTest=np.atleast_2d(tTest)\ntTest=np.reshape(tTest,(xTest.shape[0],-1))\n\nxTest=xTest[:,:-1]\n\ntemp=np.hstack((self.X,self.T))\nur = unique_rows(temp)\n\nX=self.X[ur]\nT=self.T[ur]\n        \nEuc_dist_x=euclidean_distances(xTest,xTest)\nEuc_dist_t=euclidean_distances(tTest,tTest)\n\nKK_xTest_xTest=np.exp(-np.square(Euc_dist_x)/self.hyper['lengthscale_x']-np.square(Euc_dist_t)/self.hyper['lengthscale_t'])\n    +np.eye(xTest.shape[0])*self.noise_delta\n\nEuc_dist_test_train_x=euclidean_distances(xTest,X)\n\nEuc_dist_test_train_t=euclidean_distances(tTest,T)\n\nKK_xTest_xTrain=np.exp(-np.square(Euc_dist_test_train_x)/self.hyper['lengthscale_x']-np.square(Euc_dist_test_train_t)/self.hyper['lengthscale_t'])\n    \nmean=np.dot(KK_xTest_xTrain,self.alpha)\nv=np.linalg.solve(self.L,KK_xTest_xTrain.T)\nvar=KK_xTest_xTest-np.dot(v.T,v)\n\nreturn mean.ravel(),np.diag(var)\n\n\n# --- bayes_opt/curve_compression.py ---\n\ndef apply_one_transform_logistic(curve, midpoint=-2, growth=1,MaxEpisode=1000,IsReturnCurve=False):\n    if isinstance(curve, (list,)):\n        curve=curve[0]\n        \n    def logistic_func(x):\n        return 1.0/(1+np.exp(-growth*(x-midpoint)))\n\t\n    my_xrange_scaled=np.linspace(-6,6, int(MaxEpisode))\n\n    my_logistic_value_scaled=logistic_func(my_xrange_scaled)\n\n    my_logistic_value_scaled=my_logistic_value_scaled[:len(curve)]\n\n    if np.max(curve)<=0 and np.min(curve)<=0:\n        curve=curve+500\n    \n    threshold=(midpoint+6-2)*len(curve)/(12)\n    threshold=np.int(threshold)\n    \n    prod_func=curve*my_logistic_value_scaled\n    \n    average=[np.mean(prod_func[threshold:pos+1]) for pos in range(threshold,len(prod_func))]\n\n    if IsReturnCurve==True:\n        return average[-1],my_logistic_value_scaled\n    else:\n        return average[-1]\n\ndef transform_logistic(curves, midpoint=0, growth=1,MaxEpisode=1000):\n    if len(curves)==1:\n        output=apply_one_transform_logistic(curves[0], midpoint, growth,MaxEpisode)\n    else:\n        output=[0]*len(curves)\n        for idx, curve in enumerate(curves):\n            output[idx]=apply_one_transform_logistic(curve, midpoint, growth,MaxEpisode)\n    return output\n\n# --- bayes_opt/acquisition_functions.py ---\n\nclass AcquisitionFunction(object):\n    def __init__(self, acq):\n\n        self.acq=acq\n        acq_name=acq['name']\n        \n        if 'mu_max' in acq:\n            self.mu_max=acq['mu_max']\n        \n        ListAcq=['bucb','ucb', 'ei','poi','random','ucb_pe',\n                 'pure_exploration','mu','lcb','ei_mu_max']\n        \n        IsTrue=[val for idx,val in enumerate(ListAcq) if val in acq_name]\n        if  IsTrue == []:\n            err = \"The utility function \" \\\n                  \"{} has not been implemented, \" \\\n                  \"please choose one of ucb, ei, or poi.\".format(acq_name)\n            raise NotImplementedError(err)\n        else:\n            self.acq_name = acq_name\n            \n        self.dim=acq['dim']\n        \n        if 'scalebounds' not in acq:\n            self.scalebounds=[0,1]*self.dim\n            \n        else:\n            self.scalebounds=acq['scalebounds']\n               \n\n    def acq_kind(self, x, gp):\n        y_max=np.max(gp.Y)\n        if np.any(np.isnan(x)):\n            return 0\n       \n        if self.acq_name == 'ucb':\n            return self._ucb(x, gp)\n        if self.acq_name == 'lcb':\n            return self._lcb(x, gp)\n        if self.acq_name == 'ei':\n            return self._ei(x, gp, y_max)\n        if self.acq_name == 'ei_mu_max':\n            return self._ei(x, gp, self.mu_max)\n        if self.acq_name == 'poi':\n            return self._poi(x, gp, y_max)\n        \n        if self.acq_name == 'pure_exploration':\n            return self._pure_exploration(x, gp) \n      \n        if self.acq_name == 'mu':\n            return self._mu(x, gp)\n        \n        if self.acq_name == 'ucb_pe':\n            return self._ucb_pe(x, gp,self.acq['kappa'],self.acq['maxlcb'])\n       \n            \n    @staticmethod\n    def _mu(x, gp):\n        mean, var = gp.predict(x, eval_MSE=True)\n        mean=np.atleast_2d(mean).T\n        return mean\n                \n    @staticmethod\n    def _ei(x, gp, y_max):\n        y_max=np.asscalar(y_max)\n        mean, var = gp.predict(x, eval_MSE=True)\n        var2 = np.maximum(var, 1e-10 + 0 * var)\n        z = (mean - y_max)/np.sqrt(var2)\n        out=(mean - y_max) * norm.cdf(z) + np.sqrt(var2) * norm.pdf(z)\n        \nout[var2<1e-10]=0\n        return out\n\n    @staticmethod\n    def _pure_exploration(x, gp):\n        mean, var = gp.predict(x, eval_MSE=True)\n        var.flags['WRITEABLE']=True\n        var[var<1e-10]=0\n        mean=np.atleast_2d(mean).T\n        var=np.atleast_2d(var).T\n        return np.sqrt(var)\n\n\n# --- bayes_opt/acquisition_maximization.py ---\n\ndef acq_max_with_name(gp,scaleSearchSpace,acq_name=\"ei\",IsReturnY=False,IsMax=True,fstar_scaled=None):\n    acq={}\n    acq['name']=acq_name\n    acq['dim']=scaleSearchSpace.shape[0]\n    acq['scaleSearchSpace']=scaleSearchSpace   \n    if fstar_scaled:\n        acq['fstar_scaled']=fstar_scaled   \n\n    myacq=AcquisitionFunction(acq)\n    if IsMax:\n        x_max = acq_max(ac=myacq.acq_kind,gp=gp,bounds=scaleSearchSpace,opt_toolbox='scipy')\n    else:\n        x_max = acq_min_scipy(ac=myacq.acq_kind,gp=gp,bounds=scaleSearchSpace)\n    if IsReturnY==True:\n        y_max=myacq.acq_kind(x_max,gp=gp)\n        return x_max,y_max\n    return x_max\n\ndef acq_min_scipy_kwargs(myfunc, SearchSpace, **kwargs):\n    dim=SearchSpace.shape[0]\n    x_max = SearchSpace[:, 0]\n    min_acq = None\n\n    myopts ={'maxiter':10*dim,'maxfun':20*dim}\n\n    for i in range(3*dim):\n        x_tries = np.random.uniform(SearchSpace[:, 0], SearchSpace[:, 1],size=(100*dim, dim))\n        \n        y_tries=myfunc(x_tries,**kwargs)\n        \n        idx_min=np.argmin(y_tries)\n\n        x_init_min=x_tries[idx_min]\n    \n        res = minimize(lambda x: myfunc(x.reshape(1, -1), **kwargs),x_init_min.reshape(1, -1),bounds=SearchSpace,\n                       method=\"L-BFGS-B\",options=myopts)\n\n        if 'x' not in res:\n            val=myfunc(res,**kwargs)        \n        else:\n            val=myfunc(res.x,**kwargs) \n        \n        if min_acq is None or val <= min_acq:\n            if 'x' not in res:\n                x_max = res\n            else:\n                x_max = res.x\n            min_acq = val\n\n    return np.clip(x_max, SearchSpace[:, 0], SearchSpace[:, 1])\n\n\ndef acq_max(ac, gp, bounds, opt_toolbox='scipy',seeds=[],IsMax=True):\n    y_max=np.max(gp.Y)\n  \n    x_max = acq_max_scipy(ac=ac,gp=gp,y_max=y_max,bounds=bounds)\n\n    return x_max\n\ndef acq_max_scipy(ac, gp, y_max, bounds):\n    dim=bounds.shape[0]\n    x_max = bounds[:, 0]\n    max_acq = None\n\n    myopts ={'maxiter':10*dim,'maxfun':20*dim}\n\n    for i in range(1*dim):\n        x_tries = np.random.uniform(bounds[:, 0], bounds[:, 1],size=(50*dim, dim))\n    \n        y_tries=ac(x_tries,gp=gp)\n        \n        idx_max=np.argmax(y_tries)\n\n        x_init_max=x_tries[idx_max]\n        \n    \n        res = minimize(lambda x: -ac(x.reshape(1, -1), gp=gp),x_init_max.reshape(1, -1),bounds=bounds,\n                       method=\"L-BFGS-B\",options=myopts)\n\n        if 'x' not in res:\n            val=ac(res,gp)        \n        else:\n            val=ac(res.x,gp) \n\n        if max_acq is None or val >= max_acq:\n            if 'x' not in res:\n                x_max = res\n            else:\n                x_max = res.x\n            max_acq = val\n\n    return np.clip(x_max, bounds[:, 0], bounds[:, 1])",
        "experimental_info": "The BOIL framework uses a Product Gaussian Process (GP) with a product kernel `k(x,x') × k(t,t')`. The specific kernel used for both `x` and `t` dimensions is a Radial Basis Function (RBF) kernel. The GP model `ProductGaussianProcess` is initialized with a `noise_delta` of `5e-4`.\n\nThe initial hyperparameters for the GP are:\n- `lengthscale_x`: 0.02\n- `lengthscale_t`: 0.2\n\nThe training curve compression uses a parameterized Sigmoid preference function `l(u|m0,g0)`.\nInitial hyperparameters for the Sigmoid function are:\n- `midpoint` (`m0`): 0.0\n- `growth` (`g0`): 1.0\n\nThese hyperparameters (GP lengthscales and Sigmoid parameters) are optimized by maximizing the GP log marginal likelihood every `2 * dim` iterations (where `dim` is the total dimensionality of the search space including `x` and `t`). The optimization is performed using `L-BFGS-B` with options `{'maxiter': 30 * dim, 'maxfun': 30 * dim}`. The search bounds for these hyperparameters are:\n- `lengthscale_x`: [0.03, 0.3]\n- `lengthscale_t`: [0.3, 0.6] (calculated as [10 * 0.03, 2 * 0.3])\n- `midpoint` (`m0`): [-2, 3]\n- `growth` (`g0`): [0.5, 2]\n\nThe cost function `c(x,t)` is approximated by a `linear_model.LinearRegression` from scikit-learn.\n\nThe next hyperparameter-iteration point is selected by maximizing a cost-aware acquisition function `α(x,t)/µc(x,t)`. The default acquisition function `acq_name` is `'ei_mu_max'`, which is a variant of Expected Improvement (EI) using the maximum of the GP mean as the incumbent.\n\nA data augmentation technique is employed to enhance sample efficiency and prevent GP covariance matrix ill-conditioning. This involves selectively sampling intermediate points at maximum GP predictive uncertainty.\n- Maximum number of augmented points (`max_n_augmentation`): 10 (per real observation).\n- Threshold for the natural logarithm of the covariance matrix condition number (`threshold_cond`): 15. Augmentation stops if this threshold is exceeded or if the predictive uncertainty falls below `noise_delta + 1e-3`."
      }
    },
    {
      "title": "Scaling Laws for Hyperparameter Optimization",
      "abstract": "Hyperparameter optimization is an important subfield of machine learning that\nfocuses on tuning the hyperparameters of a chosen algorithm to achieve peak\nperformance. Recently, there has been a stream of methods that tackle the issue\nof hyperparameter optimization, however, most of the methods do not exploit the\ndominant power law nature of learning curves for Bayesian optimization. In this\nwork, we propose Deep Power Laws (DPL), an ensemble of neural network models\nconditioned to yield predictions that follow a power-law scaling pattern. Our\nmethod dynamically decides which configurations to pause and train\nincrementally by making use of gray-box evaluations. We compare our method\nagainst 7 state-of-the-art competitors on 3 benchmarks related to tabular,\nimage, and NLP datasets covering 59 diverse tasks. Our method achieves the best\nresults across all benchmarks by obtaining the best any-time results compared\nto all competitors.",
      "full_text": "Scaling Laws for Hyperparameter Optimization Arlind Kadra Representation Learning Lab University of Freiburg kadraa@cs.uni-freiburg.de Maciej Janowski Representation Learning Lab University of Freiburg janowski@cs.uni-freiburg.de Martin Wistuba Amazon Web Services Amazon Berlin marwistu@amazon.com Josif Grabocka Representation Learning Lab University of Freiburg grabocka@cs.uni-freiburg.de Abstract Hyperparameter optimization is an important subfield of machine learning that focuses on tuning the hyperparameters of a chosen algorithm to achieve peak performance. Recently, there has been a stream of methods that tackle the issue of hyperparameter optimization, however, most of the methods do not exploit the dominant power law nature of learning curves for Bayesian optimization. In this work, we propose Deep Power Laws (DPL), an ensemble of neural network models conditioned to yield predictions that follow a power-law scaling pattern. Our method dynamically decides which configurations to pause and train incre- mentally by making use of gray-box evaluations. We compare our method against 7 state-of-the-art competitors on 3 benchmarks related to tabular, image, and NLP datasets covering 59 diverse tasks. Our method achieves the best results across all benchmarks by obtaining the best any-time results compared to all competitors. 1 Introduction Hyperparameter Optimization (HPO) is a major challenge for the Machine Learning community. Unfortunately, HPO is not yet feasible for Deep Learning (DL) methods due to the high cost of evaluating multiple configurations. Recently multi-fidelity HPO (a sub-problem of gray-box HPO) has emerged as a promising paradigm for HPO in DL, by discarding poorly-performing hyperparameter configurations after observing the validation error on the low-level fidelities of the optimization procedure [28, 9, 1, 29]. The advantage of multi-fidelity HPO compared to online HPO [7, 38], or meta-gradient HPO [32, 10, 31] is the ability to tune all types of hyperparameters. In recent years, a stream of papers highlights the fact that the performance of DL methods is predictable [14], concretely, that the validation error rate is a power law function of the model size, or dataset size [41, 40]. Such a power law relationship has been subsequently validated in the domain of NLP, too [11]. In this paper, we demonstrate that the power-law principle has the potential to be a game-changer in HPO, because we can evaluate hyperparameter configurations in low-budget regimes (e.g. after a few epochs), then estimate the performance on the full budget using dataset-specific power law models. Concretely, we hypothesize and empirically demonstrate that optimization curves (training epochs versus accuracy, or loss) can be efficiently modeled as simple power law functions. As a result, we introduce Deep Power Law (DPL) ensembles, a probabilistic surrogate for Bayesian optimization (BO) that estimates the performance of a hyperparameter configuration at future budgets using ensembles of deep power law functions. Subsequently, our novel formulation of BO dynami- cally decides which configurations to pause and train incrementally by relying on the performance 37th Conference on Neural Information Processing Systems (NeurIPS 2023). arXiv:2302.00441v3  [cs.LG]  25 Oct 2023estimations of the surrogate. We demonstrate that our method achieves the new state-of-the-art in HPO for DL by comparing against 7 strong HPO baselines, and 59 datasets of three diverse modalities (tabular, image, and natural language processing). Our experimental protocol contains state-of-the-art deep learning architectures such as Transformers [44], XFormer [26], ResNeXt [49] and large language models like GPT-2 [39]. As a result, we believe the proposed method has the potential to finally make HPO for DL a feasible reality. Overall, our contributions can be summarized as follows: • We introduce a novel probabilistic surrogate for multi-fidelity HPO based on ensembles of deep power law functions. • We derive a simple mechanism to combine our surrogate with Bayesian optimization. • Finally, we demonstrate the empirical superiority of our method against the current state-of- the-art in HPO for Deep Learning, with a large-scale HPO experimental protocol. 2 Related Work Multi-fidelity HPO assumes a method has access to the learning curve of a hyperparameter con- figuration. Such a learning curve is the function that maps either training time or dataset size, to the validation performance. The early performance of configurations (i.e. first segment of the learning curve) is used to discard unpromising configurations, before waiting for full convergence. Successive halving [17] is a widely used multi-fidelity method that randomly samples hyperparameter configurations, starts evaluating them, and ends a fraction of them upon reaching a predefined budget. Afterwards, the budget is divided by the fraction of discarded hyperparameter configurations and the process continues until the maximum budget is reached. Although the method relies only on the last observed value of the learning curve, it is very efficient. In recent years, various flavors of successive halving have been suggested, including Hyperband [28], which effectively runs successive halving in parallel with different settings. A major improvement to Hyperband is replacing random search with a more efficient sampling strategy [1, 9]. A more efficient approach is to allocate the budget dynamically to the most promising configurations [47]. However, the only assumption these methods make about the learning curve is that it will improve over time, while recent work [ 4] exploits a power law assumption on the curves. Similarly, we fit surrogates that exploit a power law assumption, however, our method is able to estimate the performance of unobserved configurations through probabilistic surrogates learned jointly for all hyperparameter configurations. Learning curve prediction is a related topic, where the performance of a configuration is predicted based on a partially observed learning curve [36]. Typically, the assumptions about the learning curve are much stronger than those described above. The prediction is often based on the assumption that the performance increases at the beginning and then flattened towards the end. One way to model this behavior is to define a weighted set of parametric functions [8, 23]. Then, the parameters of all functions are determined so that the resulting prediction best matches the observed learning curve. Another approach is to use learning curves from already evaluated configurations and to find an affine transformation that leads to a well-matched learning curve [6]. A more data-driven approach is to learn the typical learning curve behaviour directly from learning curves across different datasets [48]. Learning curve prediction algorithms can be combined with successive halving [2]. In contrast to this research line, we fit ensembles of power law surrogates for conducting multi-fidelity HPO with Bayesian optimization. Scaling laws describe the relationship between the performance of deep learning models as a function of dataset size or model size as a power law [ 14, 16, 41, 40, 11, 20, 15, 33]. Another work tunes hyperparameters on a small-scale model and then transfers them to a large-scale version [ 50]. In contrast to these papers, we directly use the power law assumption for training surrogates in Bayesian optimization for HPO. 3 Preliminaries Hyperparameter Optimization (HPO) demands finding the configurations λ ∈ Λ of a Machine Learning method that achieve the lowest validation loss L(Val) of a model (e.g. a neural network), which is parameterized with θ ∈ Θ and learned to minimize the training loss L(Train) as: 2λ∗ := arg min λ∈Λ L(V al) (λ, θ∗ (λ)) , s.t. θ∗ (λ) := arg min θ∈Θ L(Train ) (λ, θ) (1) For simplicity, we denote the validation loss as our function of interestf(λ) := L(V al) (λ, θ∗ (λ)). The optimal hyperparameter configurations λ∗ of Equation 1 are found via an HPO policy A ∈ P(also called an HPO method) that given a history of N evaluated configurations H(N) := {λi, f(λi)}N i=1 suggests the (N + 1)-th configuration to evaluate as λN+1 := A \u0000 H(N)\u0001 where A : [Λ × R+]N → Λ. The search for an optimal HPO policy is a bi-objective problem in itself, aiming at (i) finding a configuration out of N evaluations that achieves the smallest validation loss f(λ), and (ii) ensuring that the costs of evaluating the N configurations do not exceed a total budget Ω, as shown in Equation 2. arg min A∈P min i∈{1,...,N} f \u0010 λi := A \u0010 H(i−1) \u0011\u0011 , (2) where: H(i) := ( {(λj, f(λj))}i j=1 i >0 ∅ i = 0 , subject to: Ω ≥ NX i=1 cost (f (λi)) Bayesian optimization (BO) is the most popular type of policy for HPO, due to its ability to balance the exploration and exploitation aspects of minimizing the validation loss f in terms of hyperparame- ters λ ∈ Λ. Technically speaking, BO fits a surrogate ˆf(λ; ϕ) parametrized with ϕ ∈ Φ to approximate the observed loss f(λ) using the history H(N), as ϕ∗ := arg maxϕ∈Φ E(λ,f(λ)) ∼pH(N) p(f(λ)|λ, ϕ), where, p represents the probability. Afterwards, BO uses an acquisition/utility function a : Λ → R+ to recommend the next configuration as λN+1 := A \u0000 H(N)\u0001 = arg maxλ∈Λ a \u0010 ˆf(λ; ϕ∗) \u0011 . Multi-fidelity HPO refers to the case where an approximation of the validation loss can be measured at a lower budget b ∈ B, where B := (0, bmax]. For instance, in Deep Learning we can measure the validation loss after a few epochs ( 0 < b < ϵ), rather than wait for a full convergence ( b = bmax). Throughout this paper, the term budget refers to a learning curve step. The evaluation of a configuration λ for a budget b is defined as f (λ, b) : Λ × B → R+. 4 Power Law Surrogates for Bayesian Optimization Prior work has demonstrated that the performance of Machine Learning methods as a function of budgets (i.e. dataset size, number of optimization epochs, model size, image resolution) follows a power law relationship [41, 40, 11, 20, 15]. In this work, we employ this power law dependence between the validation loss and the number of optimization epochs in Deep Learning. We propose a novel multi-fidelity Hyperparameter Optimization method which is based on power law surrogates. We assume that every learning curve f (λ, ·) can be described by a power law function defined by (α, β, γ). Concretely, we define a power law function for the validation loss of a configuration λ at a budget b (a.k.a. the number of epochs) as shown in Equation 3. ˆf (λ, b) := αλ + βλ b−γλ, αλ, βλ, γλ ∈ R (3) Instead of fitting one separate power law function to each learning curve, we fit a single shared power law function across all configurations by conditioning the power law coefficients α, β, γon λ using a parametric neural network g that maps a configuration to the power law coefficients of its learning curve as g : Λ → R3. The network g has three output nodes, corresponding to the power law coefficients, denoted as g(λ)α, g(λ)β, g(λ)γ, as defined in Equation 4. ˆf (λ, b) := g(λ)α + g(λ)β b−g(λ)γ , g : Λ → R3 (4) 3Using a history of learning curve evaluations H(N) := {(λi, bi, f(λi, bi))}N i=1 we can train the power law surrogate to minimize the following loss function using stochastic gradient descent: arg min ˆf E(λ,b,f(λ,b))∼pH(N) \f\f\ff (λi, bi) − ˆf (λi, bi) \f\f\f (5) BO surrogates need to be probabilistic regression models because the acquisition functions require the posterior variance of the predictions. As a result, we train an ensemble of K diverse surrogates ˆf(1)(λ, b), . . . ,ˆf(K)(λ, b) by initializing each surrogate with different weights and by training with a different sequence of mini-batches as in prior work [25, 22]. The posterior mean µ and the posterior variance σ2 of the power law ensemble are trivially computed as: µ ˆf (λ, b) := 1 K KX k=1 ˆf(k)(λ, b), σ 2 ˆf (λ, b) := 1 K KX k=1 \u0010 ˆf(k)(λ, b) − µ ˆf (λ, b) \u00112 (6) A commonly used acquisition function in the domain is Expected Improvement (EI) [ 35] which incorporates both the mean and uncertainty of predictions, applying a trade-off between exploration and exploitation. Consequently, in our work, we use the EI acquisition with the estimated full budget’s (bmax) posterior mean and variance. We briefly define the acquisition function in Equation 7: λN+1 := arg max λ∈Λ EI \u0010 λ, bmax|H(N) \u0011 , (7) EI(λ, bmax|H) := Eˆf(λ,bmax)∼N \u0010 µ ˆf (λ,bmax),σ2 ˆf (λ,bmax) \u0011 h max n ˆf(λ, bmax) − f (λbest, bmax) , 0 oi where, f (λbest, bmax) corresponds to the best observed loss for any budget b′ ≤ bmax from the history H(N). After selecting a configuration with our variant of the EI acquisition, we do not naively run it until convergence. Instead, we propose a novel multi-fidelity strategy that advances the selected λN+1 of Equation 7 by a small budget of bstep, e.g. 1 epoch of training. Therefore, the selected λN+1 will be evaluated at bN+1 as defined in Equation 8. Notice, our proposed strategy also covers new configurations with no learning curve evaluations in H(N). bN+1 := (bstep, ∄λN+1 : (λN+1, ·, ·) ∈ H(N) bstep + max (λN+1,b,·)∈H(N) b, otherwise (8) We provide the detailed pseudocode of our method at Algorithm 1. 5 Experimental Protocol 5.1 Benchmarks LCBench: A benchmark that features 2,000 hyperparameter configurations that parametrize the archi- tecture of simple feedforward neural networks, as well as, the training pipeline [51]. The benchmark features 7 numerical hyperparameters and 35 different datasets from the AutoML benchmark [12]. PD1: A deep learning benchmark [45] that consists of recent DL (including Transformers) architec- tures run on large vision datasets such as CIFAR-10, CIFAR-100, ImageNet, as well as statistical modeling corpora and protein sequence datasets from bioinformatics. Every search space includes varying learning curve lengths, ranging from 5 to 1414, and a different number of evaluated hy- perparameter configurations ranging from 807 to 2807. The search space includes hyperparameter configurations that parametrize the learning rate, the learning rate scheduler, and the momentum. TaskSet: A benchmark that features different optimization tasks evaluated in 5 different search spaces [34]. For our work, we focus on the Adam8p search space, which is among the largest search spaces 4Algorithm 1: Multi-Fidelity HPO with Deep Power Laws Input : Search space Λ, initial design H(init), budget increment bstep Output :Best hyperparameter configuration λ∗ 1 Iteration i ← 0; Evaluate initial configurations and budgets H(i) := H(init); 2 while still budget do 3 Fit a DPL ensemble ˆf(1)(λ, b), . . . ,ˆf(K)(λ, b) from history H(i) using Equation 5; 4 Recommend the next configuration λi+1 and its budget bi+1 using Equation 6, 7 and 8; 5 Train λi+1 until bi+1 and measure the validation loss f (λi+1, bi+1); 6 Append to history Hi+1 ← Hi ∪ {(λi+1, bi+1, f(λi+1, bi+1))}; 7 i ← i + 1 ; 8 end 9 return Best configuration λ∗ with the smallest validation loss min (λ∗,b,f(λ∗,b))∈Hi f (λ∗, b) ; in the benchmark with 1000 hyperparameter configurations. Every hyperparameter configuration features 8 continuous hyperparameters. The hyperparameters control the learning rate, the learning rate schedulers, and the optimizer. For variety among our benchmarks, we focus on 12 RNN text classification tasks that feature different RNN cells, embedding sizes, and batch sizes. For a more detailed explanation of the benchmarks, we refer the reader to Appendix F. 5.2 Baselines Random Search stochastically samples hyperparameter configurations for the largest possible budget. Hyperband uses multiple brackets with different trade-offs of the initial budget and number of epochs to initially train [28]. It then applies Successive Halving (SH) [17] on every bracket. ASHA is an asynchronous version of SH [ 27] that does not wait for all configurations to finish in an SH bracket before starting the next one. Furthermore, BOHB is a follow-up of Hyperband that uses TPE [3] to sample the initial hyperparameter configurations of a bracket [9]. DEHB, on the other hand, modifies Hyperband by using evolutionary strategies to sample the initial hyperparameter configurations [1]. Similarly, multi-fidelity SMAC extends Hyperband but uses random forests to sample the initial hyperparameter configurations for a bracket [ 30]. We also use the Dragonfly Library [19] to compare against BOCA [18], a multi-fidelity method that uses Gaussian Processes to predict the next hyperparameter to evaluate, as well as the fidelity for which it should be evaluated. For all the baselines, we use their official public implementations. We provide additional details in Appendix G. 5.3 Architecture & Training For our method, we use an ensemble of 5 models, where every model consists of a 2-layer feedforward neural network with 128 units per layer and Leaky ReLU for the non-linearity. The architecture of our method is motivated by prior work [25]. Our network has 3 output units, that are then combined with the budget b to yield the power law output. We apply the GLU non-linearity activation only on the β and γ output units, allowing α to take any value. We use the L1 loss to train our network, coupled with Adam featuring an initial learning rate of 10−3. For the first 10 iterations of our multi-fidelity HPO method in Algorithm 1 we train every network of our ensemble for 250 epochs with randomly sampled initial weights. This choice helps the convergence of the weights in the early stage of HPO. Next, we continuously refine the model for 20 epochs every HPO iteration. However, if the optimization stagnates (surrogate fitting loss does not improve) for more than the LC Length + a buffer of 0.2 × LC Length, the training procedure is restarted with random weights, where every model is trained again for 250 epochs and then only refined. During the refining phase, the new data point at an HPO iteration (Line 9 at Algorithm 1) is sampled with repeat on every batch, to learn new data points equally compared to older data points. Since we are working with discrete search spaces, we evaluate the acquisition function exhaustively 5on every hyperparameter configuration. When dealing with continuous search spaces, the acquisition function can either be evaluated exhaustively on a discretized version of the search space, or in a gradient-based way. Our implementation of DPL is publicly available.1 5.4 Protocol In our experiments, we standardize the hyperparameter values by performing min-max scaling for our method and every baseline. If a baseline has a specific preprocessing protocol, we do not apply min-max scaling but we apply the protocol as suggested by the authors. The benchmarks do not support a common evaluation metric for configurations (i.e. the function f). As a consequence, the evaluation metric for LCBench is the balanced accuracy, for TaskSet the log-likelihood loss, while for PD1 the accuracy. Moreover, the benchmarks do not offer learning curves with a common step size. For LCBench and PD1, one step size is equivalent to one epoch, while for TaskSet one step size is 200 batches. The HPO budget is defined as the maximum number of steps needed to fully evaluate 20 hyperparameter configurations. In that context, one unit step of the HPO budget signifies training a particular configuration for one more optimization step (e.g. 200 batches in TaskSet or 1 epoch in LCBench). In the following experiments, for all methods, we report the regret of the best-found configuration as shown in Equation 9: R = f (λbest, bmax) − f (λoracle, bmax) (9) where the oracle is given as f (λoracle, bmax) := min \b f (λ, b) | (λ, b, f(λ, b)) ∈ H(D) ∧ b ≤ bmax\t , and H(D) corresponds to the set of all the exhaustively-evaluated hyperparameter configurations’ performances on a dataset D. If the oracle configuration is not known in advance for the search space, H(D) can be replaced with the history H(N) at the end of the HPO procedure. The only difference between f (λbest, bmax) and f (λoracle, bmax) is that the former only considers the history at the HPO step for which we are reporting the results. In short, the regret is the difference in the evaluation metric performance from the best-found hyperparameter configuration during optimization to the best possible hyperparameter configuration (oracle) on the dataset (in a minimization setting). On a dataset level, we report the average regret across 10 repetitions with different seeds. To be able to aggregate results over datasets, we report the averaged normalized regret. As normalization, we divide the regret by the difference between the performances of the best and the worst hyperparameter configuration on a dataset. Then we compute the mean of the normalized regrets across all the datasets of a benchmark. Moreover, in the experiments that report the average normalized regret over time, we provide results over normalized wall clock time. The wall clock time includes both the method’s overhead (i.e. training the surrogate ˆf and selecting the next hyperparameter configuration to evaluate) and the time taken to evaluate the selected hyperparameter configuration (i.e. evaluating f). Since the methods have different run times, we normalize the individual times by the time it took Random Search (the fastest non-model-based method) to complete the HPO optimization process. To provide a fair any-time comparison, we report results until the time it took Random Search to evaluate 20 hyperparameter configurations. Furthermore, when reporting the learning curve (LC) length fraction, we imply the fraction of the total learning curve length. LCBench and TaskSet have LCs of a fixed length for all datasets, corresponding to 51 epochs for LCBench and 50 epochs for TaskSet. In contrast, PD1 has varying LC lengths for different datasets. In our experiments, all methods start with a history H(init) of 1 randomly sampled hyperparameter configuration evaluated for 1 step/epoch in the case of multi-fidelity techniques (Hyperband, BOHB, DEHB, SMAC, ASHA, Dragonfly; descriptions in Section 5.2), or for the full budget for the black- box technique (Random Search). We ran experiments on a CPU cluster, where every node contains two Intel Xeon E5-2630v4 CPUs with 20 CPU cores running at 2.2 GHz. The total memory of every node is 120GB, and every experiment is limited to 2 cores which offer 12GB. 1https://github.com/releaunifreiburg/DPL 66 Research Hypotheses and Experiments /uni00000013/uni00000011/uni00000014/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000016/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018 /uni0000002f/uni00000026/uni00000003/uni0000002f/uni00000048/uni00000051/uni0000004a/uni00000057/uni0000004b/uni00000003/uni00000029/uni00000055/uni00000044/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000019 /uni00000013/uni00000011/uni0000001b/uni00000026/uni00000052/uni00000055/uni00000055/uni00000048/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni0000001d/uni00000003/uni00000028/uni00000056/uni00000057/uni00000011/uni00000003/uni00000059/uni00000056/uni00000011/uni00000003/uni00000037/uni00000055/uni00000058/uni00000048 /uni00000027/uni00000033/uni0000002f/uni00000026/uni00000052/uni00000051/uni00000047/uni00000003/uni00000031/uni00000031/uni00000033/uni0000002f/uni00000031/uni00000031/uni0000002a/uni00000033 Figure 1: Rank correlations of learning curve fore- casting models, which are given a fraction of the learning curve and estimate the remaining curve segment. DPL: Deep Power Law, Cond NN: Con- ditioned neural network,PL: Power Law,NN: Neu- ral Network, GP: Gaussian processes. Hypothesis 1: The power law assumption im- proves the quality of learning curve forecasting. In this experiment, we evaluate the predictive performance of forecasting models that given a fraction of the observed learning curve, esti- mate the remaining unobserved segment of the curve, on the LCBench benchmark. The results of Figure 1 compare three different forecasting models, concretely, neural networks (NN), Gaus- sian Processes (GP), and Power Law functions (PL). For the three variants (PL, NN, GP) we fitted one model on every learning curve of each hyperparameter configuration (i.e. given b in the x-axis estimate one ˆf(b) separately for every λ). For the other two variants (DPL and Cond NN) we fit a single forecasting model (not an ensem- ble) for all configurations, by conditioning the surrogate on the configuration (i.e. given b and λ, estimate ˆf (λ, b)). The purpose of the experiment is to assess whether a power law function regressor leads to superior predictive accuracy, compared to generic forecasting models, such as neural networks, or Gaussian processes. The evaluation metric of the experiment highlighted in Figure 1 is the rank correlation between the estimated performances at the end of the learning curve and the true performances. We notice that although a Power Law regressor has significantly fewer parameters than a neural network (3 to 288 parameters), PL still achieves higher predictive performance than NN. Furthermore, our conditioning of the power law function to the hyperparameter configuration further improves the predictive quality, as the difference between DLP and PL demonstrates. Lastly, we refer the reader to Appendix H, where we provide an analysis of the distributions for the absolute error rate between the DPL predictions and the ground truth values over the different LC length fractions, showing that DPL does not only retain the ranks but, it also accurately predicts the final performance. Based on the results, we consider Hypothesis 1 to be validated and that DPL is accurate in terms of learning curve forecasting. What about learning curves that do not follow a power law pattern? Although the provided empirical evidence in this section strongly suggests that the presented power law model can accurately forecast learning curves, it is also true that some learning curves have divergent behaviour that does not follow the power law assumption. As a consequence, we investigate two different ways to handle curves that do not follow the power law assumption: i) recently-proposed power law functions that include breaking points [ 5], or shifts in the curve [ 8], and ii) min-smoothing the learning curves to transform them into monotonically decreasing time series. In Appendix A we provide ample empirical evidence showing that although the alternative formulations achieve comparable performances, still they do not outperform our simpler power law formulation. The findings indicate that even though not all learning curves are power laws, most of them are, therefore a power law surrogate is efficient in forecasting the final performance of a partially-observed learning curve. As a result, the forthcoming experiments will provide further empirical evidence that our power law surrogates lead to state-of-the-art HPO results when deployed in the proposed multi-fidelity Bayesian optimization setup. Hypothesis 2: Our method DPL achieves state-of-the-art results in HPO. In Figure 2, we show the performance of the considered methods over the HPO budget, where DPL manages to outperform all the rival baselines. In the case of LCBench, DPL quickly finds well-performing hyperparameter configurations compared to the competitor methods and continues to discover even better configurations until the HPO process ends. Furthermore, we observe the same trend with TaskSet and PD1, where after ca. 25% of the HPO budget, our method DPL converges to a better regret compared to the baselines and increases the lead until HPO ends. For a detailed overview of the performances of all methods on all individual datasets, we point the reader to Appendix H. 7/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000011/uni00000015/uni00000018/uni00000013/uni00000011/uni00000018/uni00000013/uni00000013/uni00000011/uni0000001a/uni00000018/uni00000014/uni00000011/uni00000013/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000031/uni00000052/uni00000055/uni00000050/uni00000044/uni0000004f/uni0000004c/uni0000005d/uni00000048/uni00000047/uni00000003/uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni0000002f/uni00000026/uni00000025/uni00000048/uni00000051/uni00000046/uni0000004b /uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000011/uni00000015/uni00000018/uni00000013/uni00000011/uni00000018/uni00000013/uni00000013/uni00000011/uni0000001a/uni00000018/uni00000014/uni00000011/uni00000013/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000037/uni00000044/uni00000056/uni0000004e/uni00000036/uni00000048/uni00000057 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000011/uni00000015/uni00000018/uni00000013/uni00000011/uni00000018/uni00000013/uni00000013/uni00000011/uni0000001a/uni00000018/uni00000014/uni00000011/uni00000013/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000033/uni00000027/uni00000014 /uni00000027/uni00000033/uni0000002f/uni00000024/uni00000036/uni0000002b/uni00000024/uni00000025/uni00000032/uni0000002b/uni00000025/uni00000027/uni00000028/uni0000002b/uni00000025/uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c/uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047/uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050/uni00000036/uni00000030/uni00000024/uni00000026 Figure 2: DPL discovers better hyperparameter configurations than all rival baselines in terms of regret (distance to oracle). Solid curves and shaded regions represent the mean and standard error of the averaged normalized regret. /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019/uni0000001a/uni0000001b /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000036/uni00000030/uni00000024/uni00000026/uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000033/uni0000002f /uni0000002f/uni00000026/uni00000025/uni00000048/uni00000051/uni00000046/uni0000004b/uni00000023/uni00000018/uni00000013/uni00000008 /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019/uni0000001a/uni0000001b /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c/uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000033/uni0000002f /uni00000037/uni00000044/uni00000056/uni0000004e/uni00000036/uni00000048/uni00000057/uni00000023/uni00000018/uni00000013/uni00000008 /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019/uni0000001a/uni0000001b /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni00000036/uni00000030/uni00000024/uni00000026/uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000033/uni0000002f /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000033/uni00000027/uni00000014/uni00000023/uni00000018/uni00000013/uni00000008 /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019/uni0000001a/uni0000001b /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni00000027/uni00000028/uni0000002b/uni00000025/uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000027/uni00000033/uni0000002f /uni0000002f/uni00000026/uni00000025/uni00000048/uni00000051/uni00000046/uni0000004b/uni00000023/uni00000014/uni00000013/uni00000013/uni00000008 /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019/uni0000001a/uni0000001b /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni00000027/uni00000028/uni0000002b/uni00000025/uni00000036/uni00000030/uni00000024/uni00000026 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000033/uni0000002f /uni00000037/uni00000044/uni00000056/uni0000004e/uni00000036/uni00000048/uni00000057/uni00000023/uni00000014/uni00000013/uni00000013/uni00000008 /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019/uni0000001a/uni0000001b /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000024/uni00000036/uni0000002b/uni00000024/uni00000036/uni00000030/uni00000024/uni00000026 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000027/uni00000033/uni0000002f /uni00000033/uni00000027/uni00000014/uni00000023/uni00000014/uni00000013/uni00000013/uni00000008 Figure 3: The critical difference diagrams at 50% and 100% of the HPO budget. The ranks indicate the sorted position in terms of regret, aggregated across datasets (the lower the better). Thick horizontal lines highlight differences that are not statistically significant. /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000031/uni00000052/uni00000055/uni00000050/uni00000044/uni0000004f/uni0000004c/uni0000005d/uni00000048/uni00000047/uni00000003/uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni0000002f/uni00000026/uni00000025/uni00000048/uni00000051/uni00000046/uni0000004b /uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000013/uni00000011/uni00000017/uni00000013/uni00000013/uni00000011/uni00000019/uni00000013/uni00000013/uni00000011/uni0000001b/uni00000013/uni00000014/uni00000011/uni00000013/uni00000013 /uni00000031/uni00000052/uni00000055/uni00000050/uni00000044/uni0000004f/uni0000004c/uni0000005d/uni00000048/uni00000047/uni00000003/uni0000003a/uni00000044/uni0000004f/uni0000004f/uni00000057/uni0000004c/uni00000050/uni00000048 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000031/uni00000052/uni00000055/uni00000050/uni00000044/uni0000004f/uni0000004c/uni0000005d/uni00000048/uni00000047/uni00000003/uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000033/uni00000027/uni00000014 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 Figure 4: The average normalized regret of DPL and rival methods over the normalized time for all the considered benchmarks. Solid curves and shaded regions represent the mean and standard average normalized regret. In addition, Figure 3 provides the critical differ- ence diagrams of the per-dataset regret ranks at 50% and 100% of the HPO budget. Our method DPL outperforms all baselines in 5 out of 6 cases (in 4 of which with a statistically significant mar- gin), while being second best only at the 50% of the HPO budget on the PD1 benchmark. We investigate the lack of statistical significance in PD1, by analyzing the individual dataset perfor- mances where DPL performs worse compared to other baselines. We notice that the datasets have a skewed distribution of hyperparameter configuration performances, where, a majority of the configurations achieve top performance. Based on the results, we conclude that a lack of statistical significance is the outcome of a search space that includes relatively simple optimiza- tion tasks and not a specific failure state of our method. We provide the detailed results of our analysis in Appendix D. Lastly, we analyse the performance of DPL over time in Figure 4. As it can be observed, DPL manages to outperform the competitors even when the method’s overhead time is included, showing that the overhead of DPL (i.e. fitting surrogate and running the acquisition) is negligible in terms of the quality of the HPO results. For a more detailed information, regarding the DPL overhead time, we point to Appendix E. TaskSet is not included in Figure 4 since the benchmark does not offer runtimes. Given the results, we conclude that Hypothesis 2 is validated and that DPL achieves state-of-the-art performance in HPO. 80.0 0.2 0.4 0.6 0.8 1.0 LC Length Fraction 0.0 0.2 0.4 0.6 Precision of Top Candidates LCBench 0.0 0.2 0.4 0.6 0.8 1.0 LC Length Fraction 0.05 0.10 0.15Average Regret LCBench 0.0 0.2 0.4 0.6 0.8 1.0 LC Length Fraction 0.0 0.2 0.4 0.6 0.8 Fraction of Poor Performer Promotions LCBench 0.0 0.2 0.4 0.6 0.8 1.0 LC Length Fraction 0.00 0.05 0.10 0.15 0.20 0.25 0.30 Precision of Top Candidates TaskSet 0.0 0.2 0.4 0.6 0.8 1.0 LC Length Fraction 0.15 0.20 0.25 0.30Average Regret TaskSet 0.0 0.2 0.4 0.6 0.8 1.0 LC Length Fraction 0.0 0.1 0.2 0.3 0.4 0.5 Fraction of Poor Performer Promotions TaskSet DPL ASHA BOHB DEHB Dragonfly Hyperband Random SMAC BaselineDPL ASHA BOHB DEHB Dragonfly Hyperband Random SMAC Baseline Figure 5: Post-hoc analysis to study DPL’s efficiency. Left: Share of the best candidates selected during training. Middle: Average regret of configurations chosen to be trained at each budget. Right: Share of top third configurations at a given budget which were bottom two third configurations at a previous budget. Hypothesis 3: DPL explores the search space more efficiently compared to the baselines. We conduct further analyses to understand the source of the efficiency of DPL versus the baselines. As a result, we analyze two important aspects, the quality of the evaluated configurations, as well as the exploration capability of our multi-fidelity HPO method. Initially, we measure what fraction of the top 1% configurations (ranked by accuracy) can our method discover. Figure 5 (left) shows that until convergence our method can discover significantly more top configurations compared to the baselines. The middle plots of Figure 5, show the average regret for each configuration promoted to the respective budget. According to the plot, DPL is more efficient and assigns the budget only to configurations with lower regret compared to the other methods. The precision and regret plots demonstrate that the quality of the evaluated configurations is largely better than all baselines, therefore, giving our method a significant lift in the performance rank. Last but not least, the right plot shows the percentage of configurations that were performing poorly in an earlier epoch (i.e. accuracy-wise in the bottom 2/3 of configurations up to the epoch indicated at the x-axis) but performed better at later epochs (i.e. at the top 1/3 of configurations). Furthermore, we added a line labeled with \"Baseline\", which represents the fraction of previously poor-performing configurations of all configurations. This behavior is observed often with learning curves, where for instance, strongly regularized networks converge slowly. For the same analysis regarding the PD1 benchmark, we point the reader to Appendix H. The results indicate that our method explores well the unpromising early configurations, by consider- ing them through the uncertainty estimation of our ensemble and the respective Bayesian optimization mechanism. The results validate Hypothesis 3 and confirm that DPL explores the search space more efficiently. Hypothesis 4: Our method DPL offers an effective tool for HPO in Large Language Models. In this experiment, we consider the case of tuning the hyperparameters of transformers in Large Language Models. To this end, we computed a tabular benchmark by training a smaller GPT-2 [39] model on the OpenWebText dataset [13] for a series of different hyperparameter configurations. We tune three learning rate hyperparameters: the fraction of warmup steps, the maximum learning rate at the end of warmup, and the minimum learning rate at the end of the decay. We repeat the experiments for seven model sizes ranging from 0.3M to 30M total parameters, ablating the embedding size of the multi-head attention layers (details in Appendix B). We follow the common practice of conducting HPO with small transformers and then deploying the discovered optimal configuration on the full-scale transformers [50]. As a result, we search for the optimal hyperparameters of small transformers (embedding size of {6, 12, . . . ,96, 192}) and then evaluate the discovered configurations at a full-scale transformer with an embedding size of 384. 90 3 6 9 12 6.20 6.40 6.60 6.80 7.00Validation Error Embedding size = 12 0 4 8 12 16 HPO Budget [GPU-hours] 6.00 7.00 Embedding size = 48 0 6 12 18 24 5.00 6.00 7.00 Embedding size = 192 12 48 192 3.90 3.92 3.95 3.98Error at Largest Scale Normalized HPO budget = 25% 12 48 192 Small Transformer Scale [embedding size] 3.90 3.92 3.95 3.98 Normalized HPO budget = 50% 12 48 192 3.90 3.92 3.94 3.96 Normalized HPO budget = 100% DPL BOHB Random oracle at small scale oracle at largest scale Figure 6: HPO for Transformer architectures. Top: HPO on small-scale transformers in terms of the embedding size. Bottom: Error on the full-scale transformer, using the hyperparameter configuration discovered by conducting HPO using the small transformers. We present three analyses, ablating the HPO time on the small-scale transformer up to the HPO budget of 2 full function evaluations. Figure 6 shows the HPO results of DPL against Random Search and BOHB (a rival multi-fidelity HPO baseline). In the top row of plots, we observe the performance of the discovered configurations at the small transformers for the indicated embedding size. We observe that our method finds better configurations than the baselines at any proxy space with small embedding sizes. On the other hand, the bottom row of plots presents the performance of the discovered configurations in the small embedding space, by applying such hyperparameter configurations to the full-scale transformers. We observe that the configurations discovered by DPL on the small search space achieve very competitive results on the full-scale transformers, finding the oracle configuration of the full-scale transformers in the majority of cases. It takes DPL 3.6 hours to find the oracle configuration for the largest model via HPO for the smallest model. In turn, it takes 21.52 hours to train the largest model only once. For more details, we refer the reader to Appendix B. The results validate Hypothesis 4 and confirm that DPL is an efficient HPO technique for tuning the hyperparameters of large language models when the HPO is conducted using smaller transformer model sizes. 7 Conclusions Summary. In this work, we introduce Deep Power Law (DPL), a probabilistic surrogate based on an ensemble of power law functions. The proposed surrogate is used within a novel multi-fidelity Hyperparameter Optimization (HPO) method based on Bayesian optimization. In contrast to the prior work, we exploit scaling laws for estimating the performance of Deep Learning (DL) models. Through extensive experiments comprising 7 baselines, 59 datasets, and search spaces of diverse deep learning architectures, we show that DPL outperforms strong HPO baselines for DL by a large margin. As an overarching contribution, we advance the state-of-the-art in the important field of HPO for DL. Limitations and future work. Contrary to the common perception, we experienced that the uncertainty estimation arising from the Deep Ensemble approach [ 25] is suboptimal compared to standard BO surrogates such as Gaussian Processes. In addition, having to train an ensemble has additional computational costs, due to the necessity of training multiple power law models. In the future, we plan to investigate the combination of power laws with Gaussian Processes, as well as investigate additional fidelity types. 10Acknowledgements JG, AK and MJ would like to acknowledge the grant awarded by the Eva-Mayr-Stihl Stiftung. In addition, this research was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under grant number 417962828 and grant INST 39/963-1 FUGG (bwForCluster NEMO). In addition, JG and AK acknowledge the support of the BrainLinks-BrainTools center of excellence. References [1] Noor H. Awad, Neeratyoy Mallik, and Frank Hutter. DEHB: evolutionary hyberband for scalable, robust and efficient hyperparameter optimization. In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI 2021, Virtual Event / Montreal, Canada, 19-27 August 2021, pages 2147–2153, 2021. [2] Bowen Baker, Otkrist Gupta, Ramesh Raskar, and Nikhil Naik. Accelerating neural archi- tecture search using performance prediction. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Workshop Track Proceedings, 2018. [3] James Bergstra, Rémi Bardenet, Yoshua Bengio, and Balázs Kégl. Algorithms for hyper- parameter optimization. Advances in neural information processing systems, 24, 2011. [4] Benedetto J. Buratti and Eli Upfal. Ordalia: Deep learning hyperparameter search via general- ization error bounds extrapolation. In 2019 IEEE International Conference on Big Data (Big Data), pages 180–187, 2019. [5] Ethan Caballero, Kshitij Gupta, Irina Rish, and David Krueger. Broken neural scaling laws, 2022. [6] Akshay Chandrashekaran and Ian R. Lane. Speeding up hyper-parameter optimization by extrapolation of learning curves using previous builds. In Machine Learning and Knowledge Discovery in Databases - European Conference, ECML PKDD 2017, Skopje, Macedonia, September 18-22, 2017, Proceedings, Part I , volume 10534 of Lecture Notes in Computer Science, pages 477–492. Springer, 2017. [7] Yutian Chen, Matthew W. Hoffman, Sergio Gomez Colmenarejo, Misha Denil, Timothy P. Lillicrap, Matthew Botvinick, and Nando de Freitas. Learning to learn without gradient descent by gradient descent. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pages 748–756, 2017. [8] Tobias Domhan, Jost Tobias Springenberg, and Frank Hutter. Speeding up automatic hy- perparameter optimization of deep neural networks by extrapolation of learning curves. In Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence, IJCAI 2015, Buenos Aires, Argentina, July 25-31, 2015, pages 3460–3468. AAAI Press, 2015. [9] Stefan Falkner, Aaron Klein, and Frank Hutter. BOHB: robust and efficient hyperparameter optimization at scale. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, pages 1436–1445, 2018. [10] Luca Franceschi, Michele Donini, Paolo Frasconi, and Massimiliano Pontil. Forward and reverse gradient-based hyperparameter optimization. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017 , pages 1165–1173, 2017. [11] Behrooz Ghorbani, Orhan Firat, Markus Freitag, Ankur Bapna, Maxim Krikun, Xavier Garcia, Ciprian Chelba, and Colin Cherry. Scaling laws for neural machine translation. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. [12] Pieter Gijsbers, Erin LeDell, Janek Thomas, Sébastien Poirier, Bernd Bischl, and Joaquin Vanschoren. An open source automl benchmark. arXiv preprint arXiv:1907.00909, 2019. 11[13] Aaron Gokaslan*, Vanya Cohen*, Ellie Pavlick, and Stefanie Tellex. Openwebtext corpus. http://Skylion007.github.io/OpenWebTextCorpus, 2019. [14] Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory F. Diamos, Heewoo Jun, Hassan Kianinejad, Md. Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling is predictable, empirically. CoRR, abs/1712.00409, 2017. [15] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katherine Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Oriol Vinyals, Jack William Rae, and Laurent Sifre. An empirical analysis of compute-optimal large language model training. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. [16] Achin Jain, Gurumurthy Swaminathan, Paolo Favaro, Hao Yang, Avinash Ravichandran, Hrayr Harutyunyan, Alessandro Achille, Onkar Dabeer, Bernt Schiele, Ashwin Swaminathan, and Stefano Soatto. A meta-learning approach to predicting performance and data requirements. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 3623–3632, June 2023. [17] Kevin G. Jamieson and Ameet Talwalkar. Non-stochastic best arm identification and hyper- parameter optimization. In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, AISTATS 2016, Cadiz, Spain, May 9-11, 2016, pages 240–248, 2016. [18] Kirthevasan Kandasamy, Gautam Dasarathy, Jeff G. Schneider, and Barnabás Póczos. Multi- fidelity bayesian optimisation with continuous approximations. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pages 1799–1808, 2017. [19] Kirthevasan Kandasamy, Karun Raju Vysyaraju, Willie Neiswanger, Biswajit Paria, Christo- pher R. Collins, Jeff Schneider, Barnabás Póczos, and Eric P. Xing. Tuning hyperparameters without grad students: Scalable and robust bayesian optimisation with dragonfly. J. Mach. Learn. Res., 21:81:1–81:27, 2020. [20] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. [21] Andrej Karpathy. nanoGPT. https://github.com/karpathy/nanoGPT, 2023. [22] Abdus Salam Khazi, Sebastian Pineda Arango, and Josif Grabocka. Deep ranking ensembles for hyperparameter optimization. In The Eleventh International Conference on Learning Representations, 2023. [23] Aaron Klein, Stefan Falkner, Jost Tobias Springenberg, and Frank Hutter. Learning curve prediction with bayesian neural networks. In 5th International Conference on Learning Rep- resentations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings, 2017. [24] Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009. [25] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable pre- dictive uncertainty estimation using deep ensembles. In I. Guyon, U. V on Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors,Advances in Neural Informa- tion Processing Systems, volume 30. Curran Associates, Inc., 2017. [26] Benjamin Lefaudeux, Francisco Massa, Diana Liskovich, Wenhan Xiong, Vittorio Caggiano, Sean Naren, Min Xu, Jieru Hu, Marta Tintore, Susan Zhang, Patrick Labatut, and Daniel Haziza. xformers: A modular and hackable transformer modelling library. https://github.com/ facebookresearch/xformers, 2022. 12[27] Liam Li, Kevin Jamieson, Afshin Rostamizadeh, Ekaterina Gonina, Moritz Hardt, Benjamin Recht, and Ameet Talwalkar. Massively parallel hyperparameter tuning. arXiv preprint arXiv:1810.05934, 5, 2018. [28] Lisha Li, Kevin G. Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. Hyperband: A novel bandit-based approach to hyperparameter optimization. J. Mach. Learn. Res., 18:185:1–185:52, 2017. [29] Shibo Li, Wei Xing, Robert M. Kirby, and Shandian Zhe. Multi-fidelity bayesian optimization via deep neural networks. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. [30] Marius Lindauer, Katharina Eggensperger, Matthias Feurer, André Biedenkapp, Difan Deng, Carolin Benjamins, Tim Ruhkopf, René Sass, and Frank Hutter. Smac3: A versatile bayesian optimization package for hyperparameter optimization. Journal of Machine Learning Research (JMLR) – MLOSS, 23(54):1–9, 2022. [31] Jonathan Lorraine, Paul Vicol, and David Duvenaud. Optimizing millions of hyperparameters by implicit differentiation. In The 23rd International Conference on Artificial Intelligence and Statistics, AISTATS 2020, 26-28 August 2020, Online [Palermo, Sicily, Italy], pages 1540–1552, 2020. [32] Dougal Maclaurin, David Duvenaud, and Ryan P. Adams. Gradient-based hyperparameter optimization through reversible learning. In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, pages 2113–2122, 2015. [33] Rafid Mahmood, James Lucas, Jose M. Alvarez, Sanja Fidler, and Marc Law. Optimizing data collection for machine learning. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems , volume 35, pages 29915–29928. Curran Associates, Inc., 2022. [34] Luke Metz, Niru Maheswaranathan, Ruoxi Sun, C. Daniel Freeman, Ben Poole, and Jascha Sohl-Dickstein. Using a thousand optimization tasks to learn hyperparameter search strategies. CoRR, abs/2002.11887, 2020. [35] Jonas Mockus, Vytautas Tiesis, and Antanas Zilinskas. The application of Bayesian methods for seeking the extremum. Towards Global Optimization, 2(117-129):2, 1978. [36] Felix Mohr and Jan N van Rijn. Learning curves for decision making in supervised machine learning–a survey. arXiv preprint arXiv:2201.12150, 2022. [37] OpenAI. Gpt-4 technical report, 2023. [38] Jack Parker-Holder, Vu Nguyen, and Stephen J. Roberts. Provably efficient online hyperparam- eter optimization with population-based bandits. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. [39] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019. [40] Jonathan S. Rosenfeld, Jonathan Frankle, Michael Carbin, and Nir Shavit. On the predictability of pruning across scales. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 9075–9083. PMLR, 2021. [41] Jonathan S. Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. A constructive prediction of the generalization error across scales. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. [42] David Salinas, Matthias Seeger, Aaron Klein, Valerio Perrone, Martin Wistuba, and Cedric Archambeau. Syne tune: A library for large scale hyperparameter tuning and reproducible research. In First Conference on Automated Machine Learning (Main Track), 2022. 13[43] Mingxing Tan and Quoc Le. EfficientNetV2: Smaller models and faster training. github.com/ google/automl/tree/master/efficientnetv2, 2021. [44] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V on Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors,Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. [45] Zi Wang, George E Dahl, Kevin Swersky, Chansoo Lee, Zelda Mariet, Zachary Nado, Justin Gilmer, Jasper Snoek, and Zoubin Ghahramani. Pre-training helps bayesian optimization too. arXiv preprint arXiv:2207.03084, 2022. [46] Ross Wightman. PyTorch image models. https://github.com/rwightman/ pytorch-image-models, 2019. [47] Martin Wistuba, Arlind Kadra, and Josif Grabocka. Supervising the multi-fidelity race of hyperparameter configurations. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. [48] Martin Wistuba and Tejaswini Pedapati. Learning to rank learning curves. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, pages 10303–10312, 2020. [49] Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1492–1500, 2017. [50] Greg Yang, Edward J. Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, and Jianfeng Gao. Tensor programs V: tuning large neural networks via zero-shot hyperparameter transfer. CoRR, abs/2203.03466, 2022. [51] Lucas Zimmer, Marius Lindauer, and Frank Hutter. Auto-pytorch tabular: Multi-fidelity metalearning for efficient and robust autodl. IEEE Transactions on Pattern Analysis and Machine Intelligence, 43(9):3079 – 3090, 2021. 14A Modeling /uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000011/uni00000015/uni00000018/uni00000013/uni00000011/uni00000018/uni00000013/uni00000013/uni00000011/uni0000001a/uni00000018/uni00000014/uni00000011/uni00000013/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000031/uni00000052/uni00000055/uni00000050/uni00000044/uni0000004f/uni0000004c/uni0000005d/uni00000048/uni00000047/uni00000003/uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni0000002f/uni00000026/uni00000025/uni00000048/uni00000051/uni00000046/uni0000004b /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000026/uni00000044/uni00000051/uni00000047/uni0000004c/uni00000047/uni00000044/uni00000057/uni00000048/uni00000003/uni00000014 /uni00000026/uni00000044/uni00000051/uni00000047/uni0000004c/uni00000047/uni00000044/uni00000057/uni00000048/uni00000003/uni00000015 /uni00000025/uni00000055/uni00000052/uni0000004e/uni00000048/uni00000051/uni00000003/uni0000002f/uni00000044/uni0000005a /uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000011/uni00000015/uni00000018/uni00000013/uni00000011/uni00000018/uni00000013/uni00000013/uni00000011/uni0000001a/uni00000018/uni00000014/uni00000011/uni00000013/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000031/uni00000052/uni00000055/uni00000050/uni00000044/uni0000004f/uni0000004c/uni0000005d/uni00000048/uni00000047/uni00000003/uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni0000002f/uni00000026/uni00000025/uni00000048/uni00000051/uni00000046/uni0000004b /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000036/uni00000033/uni0000002f Figure 7: Left: The performance of different power law formulations, as well as, the baselines on the LCBench benchmark over the HPO budget. Right: The performance of the power law formulation when min-smoothing is applied to the learning curve to ensure a monotonically decreasing learning curve. Label Formula DPL α+β · b−γ Candidate 1 α−β · (b +d)−γ Candidate 2 α−β · (e · b +d)−γ Broken Law α+β · b−γ · \u0010 1 + \u0010 b d 1 f \u0011\u0011−c·f Table 1: Alternative power law formulations for the DPL surrogate. To inspect the modeling choices of the power law functions used as our surrogate, we con- sider different formulations for the ensemble members of our surrogate as shown in Table 1. Initially, we consider Candidate 1 which can handle shifts in the learning curve by introduc- ing an additional parameter d. Furthermore, we consider a more complex version, Candidate 2, that allows us to additionally scale the budget, by introducing variable e. Lastly, we consider Broken Laws [5] (BPL), which can handle breaking points in the learning curve. We use a version that can handle one breaking point since the authors of the method suggest it as a sufficient formulation to approximate the majority of cases. We run the DPL surrogate with every individual formulation on the LCBench benchmark to investigate their empirical performance. Figure 7 presents the results, where, our chosen surrogate formulation (DPL) despite being the simplest, outperforms all other formulations. The Candidate 2 formulation does not manage to outperform all competitor methods, the Candidate 1 formulation manages to outperform all rival methods, however, only with a marginal difference. The Broken Law formulation manages to outperform all the rival baselines by a considerable margin, however, it still performs worse than the simple power law formulation used for DPL. We would like to point out that the alternative power law formulations are more difficult to opti- mize/run since they are prone to diverge and fall into a dead state given different combinations of parameter values. The most common is division by zero for e.g. d term in the Broken Law formulation, taking the root of a negative number b + d term in Candidate 1, e · b + d for Candidate 2 and the d term in Broken Law. We additionally consider min-smoothing the learning curve y, by taking at each step b of the learning curve the minimal value of the learning curve ysmooth b where, ysmooth b = min (y0, y1, ..., yb). The min-smoothing allows diverging curves to be formulated as power laws since the diverging phase will be substituted with stagnation. Figure 7 shows that incorporating learning curve min-smoothing for our surrogate (SPL) performs comparably to DPL without learning curve smoothing and manages to beat the other HPO baselines. 15B nanoGPT-Bench In recent years, deep learning research has increasingly focused on large-scale models, particularly Large Language Models (LLMs) like the Generative Pre-trained Transformers (GPTs). To evaluate the effectiveness of search algorithms, we propose a benchmark based on the nanoGPT model [21], reproducing the performance of the small GPT-2 [39] model trained on the OpenWebText dataset [13]. Our experimental setup is designed with the practicalities of real-world hardware constraints in mind. The common practice in the field is to perform Hyperparameter Optimization (HPO) on an informative proxy task that adheres to model size scaling laws [37], and then to apply these optimized parameters to larger models. This pragmatic approach is necessitated by the fact that training larger models would require significantly more expensive computational resources and time. Within these constraints, we focus our experiments on NVIDIA RTX 2080 GPUs. Warmup Max Steps Learning Rate Min Max Figure 8: nanoGPT-Bench search space parametrization. Baseline: We train a small nanoGPT model, a scaled-down variant of the small GPT-2, reducing the parameter count to approximately 30 M from the original 124 M parameters. The model architecture includes 6 transformer layers and 6 attention heads, and the embedding size is set to384. The AdamW optimizer is utilized for training, with the first and second moment estimates configured to 0.9 and 0.98, respectively. The weight decay is set to 10−1, and we apply gradient clipping at a value of 1.0 to prevent large gradients from causing instability in the model training. For the training process, we optimize the cross-entropy loss for next-token prediction. The process involves 350 steps, with each step encompassing 1000 random samples, with a batch size of 12. Each data point has a context size of 512 tokens, encoded using OpenAI’s token embeddings (sized of 50304). This procedure ensures that even the most resource-intensive experiments stay within the limits of a single GPU day. HP Values Max LR [10−5; 10−4; 10−3] Min LR [1%; 10%] of Max LR Warmup Steps [10%; 20%] of Budget Table 2: Search space of nanoGPT- Bench. Search Space: Our hyperparameter search space con- struction involves varying the number of warmup steps, along with the maximum and minimum learning rates for the cosine scheduler. The specific parametrization of the scheduler is illustrated in Figure 8. The discretized choices are presented in detail in Table 2. Fidelity Space: To construct the fidelity space, we focus on two key dimensions: the number of training steps and the transformer’s embedding size, serving as a proxy for model size. In this exploration, the natural fidelity of the number of training steps is visualized by the validation curves during model training as depicted in Figure 9. On the other hand, the end performance correlation between the different fidelities is reflected by the Pearson correlation in Table 3. We establish proxy tasks by sampling embedding size from a log scale,{6, 12, . . . ,96, 192}, with the maximum being 384. Consequently, each configuration has 6 proxy and one target task, leading to a total of 84 unique configurations in our full benchmark. Every configuration is trained for 350 steps. 1650 200 350 8 10 Embedding size = 6 50 200 350 Steps 6 8 10 Embedding size = 12 50 200 350 6 8 10 Embedding size = 24 50 200 350 6 8 10Validation Error Embedding size = 48 50 200 350 Steps 6 8 10 Embedding size = 96 50 200 350 4 6 8 10 Embedding size = 192 50 200 350 Steps 4 6 8 10 Embedding size = 384 Figure 9: Validation loss curves during model training for all nanoGPT-Bench configurations across model fidelity values. 6 48 96 192 384 0 1 2 3Size [bytes] 1e7 6 48 96 192 384 Small Transformer Scale [embedding size] 0.00 0.25 0.50 0.75 1.00FLOPS 1e11 6 48 96 192 384 10 15 20Runtime [GPU-hours] Figure 10: Scaling of model size in relation to bytes, FLOPS, and runtime based on average values across all nanoGPT-Bench configurations. Due to the GPU under-utilization of small model sizes, the runtime scales linearly as the model size scales exponentially. This relationship can be observed in Figure 10 and Figure 11. We expect the runtime to scale in a linear proportion to the model size when larger models are considered. Embedding Size Correlation 6 0 .951 12 0 .880 24 0 .971 48 0 .955 96 0 .987 192 0 .994 384 1 .000 Table 3: Pearson correlation across 7 fidelities. Figure 12 illustrates the effectiveness of DPL, particularly when the number of training steps is considered as a fidelity dimension. Vertical dotted lines denote the iteration at which an algo- rithm identifies the oracle value with an absolute tolerance of 0.01. Figure 13 depicts the results over the different values of the embedding fidelity. We utilized DPL, BOHB, and random search in proxy tasks, incrementing the budget allocation over each run, up to a horizon of 6 full-function evalua- tions. From these proxy tasks, we extracted the incumbent hyperparameters and evaluated their performance on the target task, that correponds to the maximum embedding size of 384. Despite operating within short regimes, DPL consistently outperforms baselines in terms of the mean incumbent value and the explored regime, as evidenced by error bars indicating the range between best and worst incumbents across 10 seeds. It should be noted, however, that the correlation between 176.7 6.8 6.9 7.0 7.10 1 2 3 Embedding size = 6 7.0 7.1 7.2 7.3 7.4 Runtime [GPU-hours] 0 2 4 Embedding size = 12 7.2 7.3 7.4 7.50 1 2 3 Embedding size = 24 7.95 8.00 8.05 8.10 8.150 1 2 3Count Embedding size = 48 8.90 8.95 9.00 9.05 9.10 9.15 Runtime [GPU-hours] 0 1 2 3 Embedding size = 96 13.6 13.7 13.8 13.9 14.00 1 2 3 Embedding size = 192 21.4 21.6 21.8 Runtime [GPU-hours] 0 1 2 3 Embedding size = 384 Figure 11: Distribution of GPU-hours required for training across different model fidelity values. 0 10 20 30 40 6.60 6.70 6.80 6.90 7.00 Embedding size = 6 0 10 20 30 40 HPO Budget [GPU-hours] 6.20 6.40 6.60 6.80 7.00 Embedding size = 12 0 10 20 30 40 6.00 7.00 Embedding size = 24 0 10 20 30 40 6.00 7.00Validation Error Embedding size = 48 0 15 30 45 HPO Budget [GPU-hours] 5.00 6.00 7.00 Embedding size = 96 0 20 40 60 80 5.00 6.00 7.00 Embedding size = 192 0 30 60 90 120 HPO Budget [GPU-hours] 4.00 5.00 6.00 7.00 Embedding size = 384 DPL BOHB Random oracle at small scale Figure 12: The incumbent performance of DPL and other baselines during the HPO budget of 6 full function evaluations for different values of the embedding size fidelity. Dashed lines indicate the point at which the oracle has been evaluated for every algorithm. Solid curves and shaded areas stand for mean value across runs and standard error. proxy and target tasks is not always perfect. This can result in a proxy task incumbent that does not translate to the oracle in the target task, which can be observed particularly at lower fidelity levels. 186 12 24 48 96 192 4.00 4.20 4.40Error at Largest Scale HPO budget = 1 full func evals 6 12 24 48 96 192 Small Transformer Scale [embedding size] 3.90 3.95 4.00 HPO budget = 2 full func evals 6 12 24 48 96 192 3.90 3.95 4.00 HPO budget = 3 full func evals 6 12 24 48 96 192 3.90 3.95 4.00Error at Largest Scale HPO budget = 4 full func evals 6 12 24 48 96 192 Small Transformer Scale [embedding size] 3.90 3.95 4.00 HPO budget = 5 full func evals 6 12 24 48 96 192 3.90 3.95 4.00 HPO budget = 6 full func evals DPL BOHB Random oracle at largest scale Figure 13: The target task performance distribution for DPL and other methods over different HPO budgets ranging from 1 − 6 full function evaluations. C Continuous Search Space 0.00 0.25 0.50 0.75 1.00 HPO Budget 0.1 0.3 0.6 0.9Incumbent Error DPL BOHB Random SMAC Figure 14: The incumbent error of DPL, as well as, the baselines on the CIFAR10 task over the HPO budget. The primary objective of this study is to investigate the efficacy of Deep Power Laws (DPL) in trading off exploration vs exploitation in a continuous HPO search space. In this study, we do not make use of pre-computed tabular tables, but instead we optimize the hyperparameters of an EfficientNetV2 model online, by iteratively pausing unpromising configurations and moving forward only promising hyperparameter configurations during the HPO optimization procedure. To benchmark our findings, we contrast the results against established baseline algorithms such as random search, BOHB, and SMAC. Baseline: We employ EfficientNetV2 [43] as a benchmarking model and train it on the CIFAR10 dataset [24]. Specifically, we train the lightweight variant of EfficientNetV2-b0 from scratch for 50 epochs, using the RMSprop optimizer. The learning rate is initiated at 10−6 and gradually increased over a span of five warmup epochs to reach the learning rate value of5 ·10−4. Following the warmup phase, we employ a cosine learning rate scheduler, with a decay factor of 0.97 applied every 10 epochs. The weight decay is set at 10−5, with no momentum used. Furthermore, the dropout rate is configured to be 10−6 and the model’s moving average exponential decay is set at0.9996. During the training phase, the batch size is set to 64, while for the validation phase, it is reduced to 8. All experiments are performed using the timm library [46]. 19HP Values LR [10−5, 10−2] weight decay [0, 10−1] Table 4: Search space of CIFAR10 task. Search Space: In our experiment, we concen- trate on optimizing the two most critical hy- perparameters, learning rate, and weight decay, while keeping the remaining hyperparameters fixed as per the baseline model. We construct a search space for these two hyperparameters in accordance with common practices (Table 4). To emulate a continuous search space for the acquisition function, we generate 100 equally-sized steps on a logarithmic scale from the lower bound to the upper bound of each dimension. This process yields a search space comprising 104 potential configurations. Our method demonstrates a substantial speedup in terms of anytime performance when compared to baseline algorithms (Figure 14). Acknowledging the practical constraints of evaluating large-scale models, we pragmatically allocated an HPO budget for a maximum of 3 full-function evaluations, equivalent to 150 epochs. The results of our exploration underline the compelling potential of the DPL algorithm to effectively manage HPO tasks within a continuous search space. Exhibiting significant speedup gains, DPL proves itself to be not only viable but an efficient method for identifying optimal hyperparameters. The findings further underscore the adaptability and efficacy of DPL in addressing complex HPO tasks, reinforcing its standing as a valuable tool in the machine learning toolbox. D PD1 Investigation /uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000016/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000011/uni0000001b/uni00000013/uni00000011/uni0000001c /uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c /uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000016/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013 /uni00000018/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000029/uni00000055/uni00000048/uni00000054/uni00000058/uni00000048/uni00000051/uni00000046/uni0000005c /uni00000049/uni00000044/uni00000056/uni0000004b/uni0000004c/uni00000052/uni00000051/uni00000042/uni00000050/uni00000051/uni0000004c/uni00000056/uni00000057/uni00000042/uni00000050/uni00000044/uni0000005b/uni00000042/uni00000053/uni00000052/uni00000052/uni0000004f/uni0000004c/uni00000051/uni0000004a/uni00000042/uni00000046/uni00000051/uni00000051/uni00000042/uni00000057/uni00000044/uni00000051/uni0000004b/uni00000042/uni00000045/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000042/uni00000056/uni0000004c/uni0000005d/uni00000048/uni00000042/uni00000015/uni00000018/uni00000019 /uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000016/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000011/uni0000001b/uni00000013/uni00000011/uni0000001c/uni00000014/uni00000011/uni00000013 /uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c /uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000016/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013 /uni00000018/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013 /uni0000001a/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000029/uni00000055/uni00000048/uni00000054/uni00000058/uni00000048/uni00000051/uni00000046/uni0000005c /uni00000056/uni00000059/uni0000004b/uni00000051/uni00000042/uni00000051/uni00000052/uni00000042/uni00000048/uni0000005b/uni00000057/uni00000055/uni00000044/uni00000042/uni0000005a/uni0000004c/uni00000047/uni00000048/uni00000042/uni00000055/uni00000048/uni00000056/uni00000051/uni00000048/uni00000057/uni00000042/uni00000045/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000042/uni00000056/uni0000004c/uni0000005d/uni00000048/uni00000042/uni00000014/uni00000013/uni00000015/uni00000017 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000014/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000016/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000011/uni0000001b /uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c /uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000016/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013 /uni00000018/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000029/uni00000055/uni00000048/uni00000054/uni00000058/uni00000048/uni00000051/uni00000046/uni0000005c /uni00000046/uni0000004c/uni00000049/uni00000044/uni00000055/uni00000014/uni00000013/uni00000013/uni00000042/uni0000005a/uni0000004c/uni00000047/uni00000048/uni00000042/uni00000055/uni00000048/uni00000056/uni00000051/uni00000048/uni00000057/uni00000042/uni00000045/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000042/uni00000056/uni0000004c/uni0000005d/uni00000048/uni00000042/uni00000015/uni00000018/uni00000019 Figure 15: Configuration test performance histograms for datasets where DPL does not outperform baselines. We investigate the datasets: fashion_mnist_max_pooling_cnn_tanh_batch_size_256, cifar100_wide_resnet_batch_size_256, svhn_no_extra_wide_resnet_batch_size_1024, where DPL does not outperform other methods for the PD1 benchmark as shown in Figure 23. We analyze the test performance of the individual hyperparameter configurations that belong to the aforementioned datasets. Figure 15 shows that the search spaces of the datasets have a skewed distribution of performances, where, there exist a large number of hyperparameter configurations that achieve top performance. In such datasets, even a non-model based technique will quickly find a well-performing configuration, since, there is a high chance for a randomly-sampled configuration to achieve the top performance. For this reason, there is little room for sophisticated HPO techniques in these datasets. To further validate our hypothesis, we investigate two additional datasetsdionis from the LCBench benchmark and uniref50_transformer _batch_size_128 from the PD1 benchmark, where DPL achieves a strong performance compared to baseline HPO methods. The results in Figure 16 show that DPL excels on tasks that are complex and that require optimizations to find hyperparameter configurations that achieve top performance. Based on the results, we conclude that the lack of statistical significance in PD1 is not a specific failure mode of DPL, but a consequence of multiple PD1 datasets where the majority of configurations achieve the top performance. 20/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b /uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c /uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000015/uni00000013/uni00000013/uni00000029/uni00000055/uni00000048/uni00000054/uni00000058/uni00000048/uni00000051/uni00000046/uni0000005c /uni00000047/uni0000004c/uni00000052/uni00000051/uni0000004c/uni00000056 /uni00000013/uni00000011/uni00000014/uni0000001a/uni00000013/uni00000011/uni00000014/uni0000001b/uni00000013/uni00000011/uni00000014/uni0000001c/uni00000013/uni00000011/uni00000015/uni00000013/uni00000013/uni00000011/uni00000015/uni00000014 /uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c /uni00000013 /uni00000018/uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000015/uni00000018/uni00000013 /uni00000016/uni00000013/uni00000013 /uni00000016/uni00000018/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000029/uni00000055/uni00000048/uni00000054/uni00000058/uni00000048/uni00000051/uni00000046/uni0000005c /uni00000058/uni00000051/uni0000004c/uni00000055/uni00000048/uni00000049/uni00000018/uni00000013/uni00000042/uni00000057/uni00000055/uni00000044/uni00000051/uni00000056/uni00000049/uni00000052/uni00000055/uni00000050/uni00000048/uni00000055/uni00000042/uni00000045/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000042/uni00000056/uni0000004c/uni0000005d/uni00000048/uni00000042/uni00000014/uni00000015/uni0000001b Figure 16: Histograms of the distribution of performances for datasets where DPL performs strongly. 0.0 0.5 1.0 0 20 Embedding size = 6 0.0 0.5 1.0 HPO Budget 0 20 Embedding size = 12 0.0 0.5 1.0 0 20 Embedding size = 24 0.0 0.5 1.0 0 20Percentage of Iteration Time Embedding size = 48 0.0 0.5 1.0 HPO Budget 0 10 Embedding size = 96 0.0 0.5 1.0 0 10 Embedding size = 192 0.0 0.5 1.0 HPO Budget 0 10 Embedding size = 384 Figure 17: The percentage taken by the DPL overhead in the total time per iteration for the different embedding sizes in nanoGPT-Bench. E DPL Overhead To investigate the efficiency of DPL in terms of method runtime, we investigate the percentage of time that the DPL overhead contributes in the total time taken to perform one HPO iteration in the nanoGPT-Bench. The total time taken constitutes of the DPL overhead (training the DPL surrogate, calling the acquisition function to suggest the next hyperparameter configuration) and the time taken to run the target algorithm for one more step. Figure 17 shows that the impact of the DPL overhead is negligible in the total time taken. For the smallest embedding of size 6, DPL takes only 10% of the total time taken to perform one HPO iteration after spending half of the optimization budget. At the end, after circa 40 hours of HPO optimization, DPL has an impact of 20% in the total time taken to perform one HPO iteration. The impact is even smaller for the largest embedding of size 384, where DPL has an impact of only 5% in the total time taken per iteration after spending half of the optimization budget and it has an impact of only 10% in the total time per iteration after more than 120 hours of HPO optimization. The findings validate our claim that DPL has a minor time overhead in performing hyperparameter optimization, which explains the strong any-time performance of our method. 21F Details of Considered Benchmarks LCBench: We use the official implementation as the interface for the LCBench benchmark2. As suggested by the authors, we use the benchmark information starting from the second step and we skip the last step of the curve since it is a repeat of the preceding step. TaskSet: The TaskSet benchmark features 1000 diverse tasks. We decide to focus on only 12 NLP tasks from the TaskSet benchmark to add variety to our entire collection of datasets. Our limitation on the number of included tasks is related to the limited compute power, as we are unable to run for the entire suite of tasks offered in TaskSet. TaskSet features a set of 8 hyperparameters, that consists of i) optimizer-specific hyperparameters, such as the learning rate, the exponential decay rate, β1 and β2, and Adam’s constant for numerical stability ε, ii) hyperparameters that control the linear and exponential decay schedulers for the learning rate decay, and lastly iii) hyperparameters that control the L1 and L2 regularization terms. Every hyperparameter in TaskSet except β1 and β2 is sampled logarithmically. PD1: We use the synetune library [42] for our interface to the PD1 benchmark. From the benchmark, we only include datasets that have a learning curve of length greater than 10. We furthermore only include datasets that have a learning curve lower or equal to 50 to have a fair comparison between all benchmarks by having approximately 20 full-function evaluations. PD1 features 4 numerical hyper- parameters, lr_initial_value, lr_power, lr_decay_steps_factor and one_minus_momentum, where lr_initial_value and one_minus_momentum are log sampled. The learning rate decay is applied based on a polynomial schedule, its hyperparameters taken from the search space. G Baselines Random Search: We implemented random search by randomly sampling hyperparameter configura- tions from the benchmarks with the maximal budget. Hyperband, BOHB, LCNet: We use version 0.7.4 of the HpBandSter library as a common codebase for all 3 baselines 3. For the last approach mentioned, despite heavy hyperparameter tuning of the method, we could not get stable results across all the benchmarks and hence dropped the method from our comparison. ASHA: For the implementation of ASHA we use the public implementation from the optuna library, version 2.10.0. DEHB: We use the public implementation offered by the authors 4. MF-DNN: In our experiments we used the official implementation from the authors 5. However, the method crashes which does not allow for full results on all benchmarks. SMAC: For our experiment with SMAC we used the official code base from the authors 6. Dragonfly: We use version 0.1.6 of the publicly available Dragonfly library. For all the multi-fidelity methods considered in the experiments, we use the same minimal and maximal fidelities. In more detail, for the LCBench, TaskSet and PD1 benchmarks we use a minimal fidelity lower bound of 1 and a maximal fidelity lower bound equal to the max budget. H Plots In Hypothesis 1, we prove that DPL achieves a better performance in comparison to other models in estimating the final performance for different hyperparameter configurations based on partial observations. Our analysis shows that DPL manages to retain the ranks of different hyperparameter 2https://github.com/automl/LCBench 3https://github.com/automl/HpBandSter 4https://github.com/automl/DEHB/ 5https://github.com/shib0li/DNN-MFBO 6https://github.com/automl/SMAC3 22/uni00000013/uni00000011/uni00000014/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000016/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018 /uni0000002f/uni00000026/uni00000003/uni0000002f/uni00000048/uni00000051/uni0000004a/uni00000057/uni0000004b/uni00000003/uni00000029/uni00000055/uni00000044/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000014 /uni00000013/uni00000011/uni00000015 /uni00000013/uni00000011/uni00000016 /uni00000013/uni00000011/uni00000017/uni00000024/uni00000045/uni00000056/uni00000052/uni0000004f/uni00000058/uni00000057/uni00000048/uni00000003/uni00000035/uni00000048/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000059/uni00000048/uni00000003/uni00000028/uni00000055/uni00000055/uni00000052/uni00000055 Figure 18: The absolute relative error distribution of DPL over the different learning curve fractions in the LCBench benchmark. The distribution is calculated from the ground truth and prediction values, averaged over all configurations of a dataset. Every distribution is over the datasets. configurations. We complement Hypothesis 1 by additionally investigating the absolute relative error. We measure the difference between the DPL estimation of the final learning curve value for different fractions of available partial observations vs the actual end performance. Figure 18 shows that DPL does not only retain the ranks of the final performances of different hyperparameter configurations, but it also correctly estimates the final performance by attaining a small relative error, where the error is reduced the more partial observations we have from the learning curve. In Hypothesis 3, we investigate the efficiency of DPL in exploring more promising configurations compared to other HPO methods. In Figure 19 we provide the same comparison with regards to the PD1 benchmark. Based on the results, we conclude that DPL explores more promising configurations compared to other HPO methods. 0.005 0.010 0.015 0.020 LC Length Fraction 0.00 0.05 0.10 0.15 0.20 Precision of T op Candidates PD1 0.005 0.010 0.015 0.020 LC Length Fraction 0.02 0.04 0.06 0.08Average Regret PD1 0.005 0.010 0.015 0.020 LC Length Fraction 0.2 0.4 0.6 0.8 Fraction of Poor Performer Promotions PD1 DPL ASHA BOHB DEHB Dragonfly Hyperband Random SMAC Baseline Figure 19: Post-hoc analysis to study DPL’s efficiency.Left: Share of the best candidates selected during training. Middle: Average regret of configurations chosen to be trained at each budget. Right: Share of top third configurations at a given budget which were bottom two third configurations at a previous budget. Lastly, we provide the per-dataset performances of all methods, where we present the mean regret of the incumbent trajectory and the standard error over 10 runs in LCBench (Figure 20 and 21), TaskSet (Figure 22), and PD1 (Figure 23). The results show that DPL consistently outperforms other methods in the majority of cases achieving strong any-time performance and not only a strong final performance. 23/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000024/uni00000033/uni00000036/uni00000029/uni00000044/uni0000004c/uni0000004f/uni00000058/uni00000055/uni00000048 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000024/uni00000050/uni00000044/uni0000005d/uni00000052/uni00000051/uni00000042/uni00000048/uni00000050/uni00000053/uni0000004f/uni00000052/uni0000005c/uni00000048/uni00000048/uni00000042/uni00000044/uni00000046/uni00000046/uni00000048/uni00000056/uni00000056 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000024/uni00000058/uni00000056/uni00000057/uni00000055/uni00000044/uni0000004f/uni0000004c/uni00000044/uni00000051 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000017 /uni00000014/uni00000013/uni00000016 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000029/uni00000044/uni00000056/uni0000004b/uni0000004c/uni00000052/uni00000051/uni00000010/uni00000030/uni00000031/uni0000002c/uni00000036/uni00000037 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni0000002e/uni00000027/uni00000027/uni00000026/uni00000058/uni00000053/uni00000013/uni0000001c/uni00000042/uni00000044/uni00000053/uni00000053/uni00000048/uni00000057/uni00000048/uni00000051/uni00000046/uni0000005c /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000030/uni0000004c/uni00000051/uni0000004c/uni00000025/uni00000052/uni00000052/uni00000031/uni00000028 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000016 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000044/uni00000047/uni00000058/uni0000004f/uni00000057 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000016 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000044/uni0000004c/uni00000055/uni0000004f/uni0000004c/uni00000051/uni00000048/uni00000056 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000044/uni0000004f/uni00000045/uni00000048/uni00000055/uni00000057 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000045/uni00000044/uni00000051/uni0000004e/uni00000010/uni00000050/uni00000044/uni00000055/uni0000004e/uni00000048/uni00000057/uni0000004c/uni00000051/uni0000004a /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000045/uni0000004f/uni00000052/uni00000052/uni00000047/uni00000010/uni00000057/uni00000055/uni00000044/uni00000051/uni00000056/uni00000049/uni00000058/uni00000056/uni0000004c/uni00000052/uni00000051/uni00000010/uni00000056/uni00000048/uni00000055/uni00000059/uni0000004c/uni00000046/uni00000048/uni00000010/uni00000046/uni00000048/uni00000051/uni00000057/uni00000048/uni00000055 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000046/uni00000044/uni00000055 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000046/uni0000004b/uni00000055/uni0000004c/uni00000056/uni00000057/uni0000004c/uni00000051/uni00000048 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000046/uni00000051/uni00000044/uni00000048/uni00000010/uni0000001c /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000016 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000046/uni00000052/uni00000051/uni00000051/uni00000048/uni00000046/uni00000057/uni00000010/uni00000017 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000046/uni00000052/uni00000059/uni00000048/uni00000055/uni00000057/uni0000005c/uni00000053/uni00000048 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000046/uni00000055/uni00000048/uni00000047/uni0000004c/uni00000057/uni00000010/uni0000004a /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000018 /uni00000014/uni00000013/uni00000017 /uni00000014/uni00000013/uni00000016 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000047/uni0000004c/uni00000052/uni00000051/uni0000004c/uni00000056 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 Figure 20: Performance comparison over the number of epochs on a dataset level for LCBench. We plot the mean value over 10 runs and the standard error. 24/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000049/uni00000044/uni00000045/uni00000048/uni00000055/uni00000057 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni0000004b/uni00000048/uni0000004f/uni00000048/uni00000051/uni00000044 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000017 /uni00000014/uni00000013/uni00000016 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni0000004b/uni0000004c/uni0000004a/uni0000004a/uni00000056 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000016 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni0000004d/uni00000044/uni00000051/uni00000051/uni0000004c/uni00000056 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni0000004d/uni00000044/uni00000056/uni00000050/uni0000004c/uni00000051/uni00000048 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000017 /uni00000014/uni00000013/uni00000016 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni0000004d/uni00000058/uni00000051/uni0000004a/uni0000004f/uni00000048/uni00000042/uni00000046/uni0000004b/uni00000048/uni00000056/uni00000056/uni00000042/uni00000015/uni00000053/uni00000046/uni00000056/uni00000042/uni00000055/uni00000044/uni0000005a/uni00000042/uni00000048/uni00000051/uni00000047/uni0000004a/uni00000044/uni00000050/uni00000048/uni00000042/uni00000046/uni00000052/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000057/uni00000048 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni0000004e/uni00000046/uni00000014 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni0000004e/uni00000055/uni00000010/uni00000059/uni00000056/uni00000010/uni0000004e/uni00000053 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000050/uni00000049/uni00000048/uni00000044/uni00000057/uni00000010/uni00000049/uni00000044/uni00000046/uni00000057/uni00000052/uni00000055/uni00000056 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000016 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000051/uni00000052/uni00000050/uni00000044/uni00000052 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000051/uni00000058/uni00000050/uni00000048/uni00000055/uni00000044/uni0000004c/uni00000015/uni0000001b/uni00000011/uni00000019 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000053/uni0000004b/uni00000052/uni00000051/uni00000048/uni00000050/uni00000048 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000056/uni00000048/uni0000004a/uni00000050/uni00000048/uni00000051/uni00000057 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000056/uni0000004b/uni00000058/uni00000057/uni00000057/uni0000004f/uni00000048 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000056/uni0000005c/uni0000004f/uni00000059/uni0000004c/uni00000051/uni00000048 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000016 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000059/uni00000048/uni0000004b/uni0000004c/uni00000046/uni0000004f/uni00000048 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000059/uni00000052/uni0000004f/uni0000004e/uni00000048/uni00000055/uni00000057 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 Figure 21: Performance comparison over the number of epochs on a dataset level for LCBench (cont.). We plot the mean value over 10 runs and the standard error. 25/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000029/uni0000004c/uni0000005b/uni00000048/uni00000047/uni00000037/uni00000048/uni0000005b/uni00000057/uni00000035/uni00000031/uni00000031/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051 /uni0000004c/uni00000050/uni00000047/uni00000045/uni00000042/uni00000053/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000014/uni00000015/uni0000001b/uni00000042/uni0000002f/uni00000036/uni00000037/uni00000030/uni00000014/uni00000015/uni0000001b/uni00000042/uni00000044/uni00000059/uni0000004a/uni00000042/uni00000045/uni00000056/uni00000019 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000014/uni00000017 /uni00000014/uni00000013/uni00000014/uni00000014 /uni00000014/uni00000013/uni0000001b /uni00000014/uni00000013/uni00000018 /uni00000014/uni00000013/uni00000015 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000029/uni0000004c/uni0000005b/uni00000048/uni00000047/uni00000037/uni00000048/uni0000005b/uni00000057/uni00000035/uni00000031/uni00000031/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051 /uni0000004c/uni00000050/uni00000047/uni00000045/uni00000042/uni00000053/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000014/uni00000015/uni0000001b/uni00000042/uni0000002f/uni00000036/uni00000037/uni00000030/uni00000014/uni00000015/uni0000001b/uni00000042/uni00000045/uni00000056/uni00000019 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000029/uni0000004c/uni0000005b/uni00000048/uni00000047/uni00000037/uni00000048/uni0000005b/uni00000057/uni00000035/uni00000031/uni00000031/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051 /uni0000004c/uni00000050/uni00000047/uni00000045/uni00000042/uni00000053/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000014/uni00000015/uni0000001b/uni00000042/uni0000002f/uni00000036/uni00000037/uni00000030/uni00000014/uni00000015/uni0000001b/uni00000042/uni00000048/uni00000050/uni00000045/uni00000048/uni00000047/uni00000014/uni00000015/uni0000001b/uni00000042/uni00000045/uni00000056/uni00000019 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000029/uni0000004c/uni0000005b/uni00000048/uni00000047/uni00000037/uni00000048/uni0000005b/uni00000057/uni00000035/uni00000031/uni00000031/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051 /uni0000004c/uni00000050/uni00000047/uni00000045/uni00000042/uni00000053/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000016/uni00000015/uni00000042/uni0000002a/uni00000035/uni00000038/uni00000014/uni00000015/uni0000001b/uni00000042/uni00000045/uni00000056/uni00000014/uni00000015 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000029/uni0000004c/uni0000005b/uni00000048/uni00000047/uni00000037/uni00000048/uni0000005b/uni00000057/uni00000035/uni00000031/uni00000031/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051 /uni0000004c/uni00000050/uni00000047/uni00000045/uni00000042/uni00000053/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000016/uni00000015/uni00000042/uni0000002a/uni00000035/uni00000038/uni00000019/uni00000017/uni00000042/uni00000044/uni00000059/uni0000004a/uni00000042/uni00000045/uni00000056/uni00000014/uni00000015 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000016 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000029/uni0000004c/uni0000005b/uni00000048/uni00000047/uni00000037/uni00000048/uni0000005b/uni00000057/uni00000035/uni00000031/uni00000031/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051 /uni0000004c/uni00000050/uni00000047/uni00000045/uni00000042/uni00000053/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000016/uni00000015/uni00000042/uni0000002c/uni00000035/uni00000031/uni00000031/uni00000019/uni00000017/uni00000042/uni00000055/uni00000048/uni0000004f/uni00000058/uni00000042/uni00000044/uni00000059/uni0000004a/uni00000042/uni00000045/uni00000056/uni00000014/uni00000015 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000017 /uni00000014/uni00000013/uni00000016 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000029/uni0000004c/uni0000005b/uni00000048/uni00000047/uni00000037/uni00000048/uni0000005b/uni00000057/uni00000035/uni00000031/uni00000031/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051 /uni0000004c/uni00000050/uni00000047/uni00000045/uni00000042/uni00000053/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000016/uni00000015/uni00000042/uni0000002c/uni00000035/uni00000031/uni00000031/uni00000019/uni00000017/uni00000042/uni00000055/uni00000048/uni0000004f/uni00000058/uni00000042/uni0000004f/uni00000044/uni00000056/uni00000057/uni00000042/uni00000045/uni00000056/uni00000014/uni00000015 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000029/uni0000004c/uni0000005b/uni00000048/uni00000047/uni00000037/uni00000048/uni0000005b/uni00000057/uni00000035/uni00000031/uni00000031/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051 /uni0000004c/uni00000050/uni00000047/uni00000045/uni00000042/uni00000053/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000016/uni00000015/uni00000042/uni0000002f/uni00000036/uni00000037/uni00000030/uni00000014/uni00000015/uni0000001b/uni00000042/uni00000028/uni00000014/uni00000015/uni0000001b/uni00000042/uni00000045/uni00000056/uni00000014/uni00000015 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000016 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000029/uni0000004c/uni0000005b/uni00000048/uni00000047/uni00000037/uni00000048/uni0000005b/uni00000057/uni00000035/uni00000031/uni00000031/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051 /uni0000004c/uni00000050/uni00000047/uni00000045/uni00000042/uni00000053/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000016/uni00000015/uni00000042/uni0000002f/uni00000036/uni00000037/uni00000030/uni00000014/uni00000015/uni0000001b/uni00000042/uni00000045/uni00000056/uni00000014/uni00000015 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000013 /uni00000015/uni000000ee/uni00000014/uni00000013/uni00000014 /uni00000016/uni000000ee/uni00000014/uni00000013/uni00000014 /uni00000017/uni000000ee/uni00000014/uni00000013/uni00000014 /uni00000019/uni000000ee/uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000029/uni0000004c/uni0000005b/uni00000048/uni00000047/uni00000037/uni00000048/uni0000005b/uni00000057/uni00000035/uni00000031/uni00000031/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051 /uni0000004c/uni00000050/uni00000047/uni00000045/uni00000042/uni00000053/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000016/uni00000015/uni00000042/uni00000039/uni00000035/uni00000031/uni00000031/uni00000014/uni00000015/uni0000001b/uni00000042/uni00000057/uni00000044/uni00000051/uni0000004b/uni00000042/uni00000045/uni00000056/uni00000014/uni00000015 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000029/uni0000004c/uni0000005b/uni00000048/uni00000047/uni00000037/uni00000048/uni0000005b/uni00000057/uni00000035/uni00000031/uni00000031/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051 /uni0000004c/uni00000050/uni00000047/uni00000045/uni00000042/uni00000053/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000016/uni00000015/uni00000042/uni00000039/uni00000035/uni00000031/uni00000031/uni00000019/uni00000017/uni00000042/uni00000055/uni00000048/uni0000004f/uni00000058/uni00000042/uni00000044/uni00000059/uni0000004a/uni00000042/uni00000045/uni00000056/uni00000014/uni00000015 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000016 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000029/uni0000004c/uni0000005b/uni00000048/uni00000047/uni00000037/uni00000048/uni0000005b/uni00000057/uni00000035/uni00000031/uni00000031/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051 /uni0000004c/uni00000050/uni00000047/uni00000045/uni00000042/uni00000053/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000016/uni00000015/uni00000042/uni00000039/uni00000035/uni00000031/uni00000031/uni00000019/uni00000017/uni00000042/uni00000057/uni00000044/uni00000051/uni0000004b/uni00000042/uni00000044/uni00000059/uni0000004a/uni00000042/uni00000045/uni00000056/uni00000014/uni00000015 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 Figure 22: Performance comparison over the number of steps on a dataset level for TaskSet. We plot the mean value over 10 runs and the standard error. 26/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000046/uni0000004c/uni00000049/uni00000044/uni00000055/uni00000014/uni00000013/uni00000013/uni00000042/uni0000005a/uni0000004c/uni00000047/uni00000048/uni00000042/uni00000055/uni00000048/uni00000056/uni00000051/uni00000048/uni00000057/uni00000042/uni00000045/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000042/uni00000056/uni0000004c/uni0000005d/uni00000048/uni00000042/uni00000015/uni00000018/uni00000019 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000049/uni00000044/uni00000056/uni0000004b/uni0000004c/uni00000052/uni00000051/uni00000042/uni00000050/uni00000051/uni0000004c/uni00000056/uni00000057/uni00000042/uni00000050/uni00000044/uni0000005b/uni00000042/uni00000053/uni00000052/uni00000052/uni0000004f/uni0000004c/uni00000051/uni0000004a/uni00000042/uni00000046/uni00000051/uni00000051/uni00000042/uni00000055/uni00000048/uni0000004f/uni00000058/uni00000042/uni00000045/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000042/uni00000056/uni0000004c/uni0000005d/uni00000048/uni00000042/uni00000015/uni00000018/uni00000019 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000049/uni00000044/uni00000056/uni0000004b/uni0000004c/uni00000052/uni00000051/uni00000042/uni00000050/uni00000051/uni0000004c/uni00000056/uni00000057/uni00000042/uni00000050/uni00000044/uni0000005b/uni00000042/uni00000053/uni00000052/uni00000052/uni0000004f/uni0000004c/uni00000051/uni0000004a/uni00000042/uni00000046/uni00000051/uni00000051/uni00000042/uni00000057/uni00000044/uni00000051/uni0000004b/uni00000042/uni00000045/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000042/uni00000056/uni0000004c/uni0000005d/uni00000048/uni00000042/uni00000015/uni00000018/uni00000019 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000049/uni00000044/uni00000056/uni0000004b/uni0000004c/uni00000052/uni00000051/uni00000042/uni00000050/uni00000051/uni0000004c/uni00000056/uni00000057/uni00000042/uni00000056/uni0000004c/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000042/uni00000046/uni00000051/uni00000051/uni00000042/uni00000045/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000042/uni00000056/uni0000004c/uni0000005d/uni00000048/uni00000042/uni00000015/uni00000018/uni00000019 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000016 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni0000004f/uni00000050/uni00000014/uni00000045/uni00000042/uni00000057/uni00000055/uni00000044/uni00000051/uni00000056/uni00000049/uni00000052/uni00000055/uni00000050/uni00000048/uni00000055/uni00000042/uni00000045/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000042/uni00000056/uni0000004c/uni0000005d/uni00000048/uni00000042/uni00000015/uni00000013/uni00000017/uni0000001b /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000016 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000050/uni00000051/uni0000004c/uni00000056/uni00000057/uni00000042/uni00000050/uni00000044/uni0000005b/uni00000042/uni00000053/uni00000052/uni00000052/uni0000004f/uni0000004c/uni00000051/uni0000004a/uni00000042/uni00000046/uni00000051/uni00000051/uni00000042/uni00000055/uni00000048/uni0000004f/uni00000058/uni00000042/uni00000045/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000042/uni00000056/uni0000004c/uni0000005d/uni00000048/uni00000042/uni00000015/uni00000018/uni00000019 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000016 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000050/uni00000051/uni0000004c/uni00000056/uni00000057/uni00000042/uni00000050/uni00000044/uni0000005b/uni00000042/uni00000053/uni00000052/uni00000052/uni0000004f/uni0000004c/uni00000051/uni0000004a/uni00000042/uni00000046/uni00000051/uni00000051/uni00000042/uni00000057/uni00000044/uni00000051/uni0000004b/uni00000042/uni00000045/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000042/uni00000056/uni0000004c/uni0000005d/uni00000048/uni00000042/uni00000015/uni00000018/uni00000019 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000016 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000050/uni00000051/uni0000004c/uni00000056/uni00000057/uni00000042/uni00000056/uni0000004c/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000042/uni00000046/uni00000051/uni00000051/uni00000042/uni00000045/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000042/uni00000056/uni0000004c/uni0000005d/uni00000048/uni00000042/uni00000015/uni00000018/uni00000019 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000015 /uni00000014/uni00000013/uni00000014 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000056/uni00000059/uni0000004b/uni00000051/uni00000042/uni00000051/uni00000052/uni00000042/uni00000048/uni0000005b/uni00000057/uni00000055/uni00000044/uni00000042/uni0000005a/uni0000004c/uni00000047/uni00000048/uni00000042/uni00000055/uni00000048/uni00000056/uni00000051/uni00000048/uni00000057/uni00000042/uni00000045/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000042/uni00000056/uni0000004c/uni0000005d/uni00000048/uni00000042/uni00000014/uni00000013/uni00000015/uni00000017 /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013 /uni0000002b/uni00000033/uni00000032/uni00000003/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057 /uni00000014/uni00000013/uni00000014 /uni00000014/uni00000013/uni00000013 /uni00000035/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000057 /uni00000058/uni00000051/uni0000004c/uni00000055/uni00000048/uni00000049/uni00000018/uni00000013/uni00000042/uni00000057/uni00000055/uni00000044/uni00000051/uni00000056/uni00000049/uni00000052/uni00000055/uni00000050/uni00000048/uni00000055/uni00000042/uni00000045/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000042/uni00000056/uni0000004c/uni0000005d/uni00000048/uni00000042/uni00000014/uni00000015/uni0000001b /uni00000027/uni00000033/uni0000002f /uni00000024/uni00000036/uni0000002b/uni00000024 /uni00000025/uni00000032/uni0000002b/uni00000025 /uni00000027/uni00000028/uni0000002b/uni00000025 /uni00000027/uni00000055/uni00000044/uni0000004a/uni00000052/uni00000051/uni00000049/uni0000004f/uni0000005c /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050 /uni00000036/uni00000030/uni00000024/uni00000026 Figure 23: Performance comparison over the fraction of the total optimization iterations on a dataset level for PD1. We plot the mean value over 10 runs and the standard error. 27",
      "meta_data": {
        "arxiv_id": "2302.00441v3",
        "authors": [
          "Arlind Kadra",
          "Maciej Janowski",
          "Martin Wistuba",
          "Josif Grabocka"
        ],
        "published_date": "2023-02-01T13:39:07Z",
        "pdf_url": "https://arxiv.org/pdf/2302.00441v3.pdf",
        "github_url": "https://github.com/releaunifreiburg/DPL"
      },
      "llm_extracted_info": {
        "main_contributions": "The main research problem addressed is the high cost and inefficiency of Hyperparameter Optimization (HPO) for Deep Learning (DL), particularly the underutilization of the inherent power-law nature of learning curves by existing Bayesian optimization methods. The paper introduces Deep Power Laws (DPL), a novel multi-fidelity HPO method that uses an ensemble of neural network models. These models are conditioned to predict learning curve performance following a power-law scaling pattern. DPL integrates this probabilistic surrogate with Bayesian Optimization to dynamically decide when to pause and incrementally train hyperparameter configurations through gray-box evaluations. The key findings demonstrate that DPL achieves state-of-the-art performance across diverse benchmarks (tabular, image, NLP, Large Language Models) and deep learning architectures, consistently outperforming 7 strong HPO baselines in terms of both any-time and final performance.",
        "methodology": "Deep Power Laws (DPL) is a multi-fidelity HPO method built upon the power-law relationship between validation loss and training epochs. The core of DPL is an ensemble of K (specifically, 5) probabilistic surrogates. Each surrogate is a neural network `g` that maps a hyperparameter configuration `λ` to three power-law coefficients `(α, β, γ)`. These coefficients define the predicted learning curve `f^(λ, b) = g(λ)α + g(λ)β * b^(-g(λ)γ)`. The ensemble of these neural networks is trained to minimize the L1 loss against observed learning curve data, with individual models initialized with different weights and trained on different mini-batches to provide robust posterior mean and variance estimates. Bayesian Optimization is then employed using the Expected Improvement (EI) acquisition function, which leverages these mean and variance predictions to select the next most promising configuration. A novel multi-fidelity strategy is used where selected configurations are advanced by small, incremental budgets (e.g., one epoch) rather than being fully trained immediately, allowing for dynamic pausing and continuation. The neural networks in the ensemble are 2-layer feedforward networks with 128 units per layer, Leaky ReLU non-linearity, and GLU activation on β and γ output units. Training involves an initial 250 epochs with random weights, followed by 20 epochs of refinement per HPO iteration, with a restart mechanism if optimization stagnates.",
        "experimental_setup": "DPL was evaluated across three primary benchmarks covering diverse modalities: LCBench (2,000 configurations, 7 hyperparameters, 35 tabular datasets, balanced accuracy, 51 epochs), PD1 (807-2807 configurations, 4 hyperparameters, vision and bioinformatics datasets, accuracy, 5-1414 epochs), and TaskSet (1,000 configurations, 8 continuous hyperparameters, 12 RNN text classification tasks, log-likelihood loss, 50 epochs). Additionally, a custom nanoGPT-Bench was created for Large Language Models, involving training smaller GPT-2 models (30M parameters) on OpenWebText to optimize learning rate hyperparameters, with performance then evaluated on larger-scale transformers. A separate experiment was conducted on a continuous search space for EfficientNetV2 models on the CIFAR10 dataset, optimizing learning rate and weight decay. The baselines for comparison included Random Search, Hyperband, ASHA, BOHB, DEHB, multi-fidelity SMAC, and BOCA (from the Dragonfly Library), all using their official public implementations. Performance was measured using the regret of the best-found configuration, averaged and normalized over 10 repetitions with different random seeds. Any-time performance was assessed based on normalized wall clock time. Experiments were conducted on a CPU cluster (Intel Xeon E5-2630v4 CPUs with 2 cores and 12GB memory per experiment), while nanoGPT-Bench experiments utilized NVIDIA RTX 2080 GPUs. The HPO budget was set to allow for the equivalent of 20 full hyperparameter configuration evaluations, with multi-fidelity methods starting with a minimal 1-step evaluation.",
        "limitations": "The study identified several limitations. The uncertainty estimation provided by the Deep Ensemble approach was found to be suboptimal when compared to traditional Bayesian Optimization surrogates like Gaussian Processes. A significant constraint is the additional computational cost associated with training an ensemble, as it requires fitting multiple power law models. While the power law assumption generally holds, some learning curves do exhibit divergent behavior that does not strictly follow this pattern. Attempts to use more complex power law formulations (e.g., with breaking points or shifts) proved difficult to optimize, being prone to divergence and numerical instability (e.g., division by zero or negative roots). Furthermore, in certain datasets within benchmarks like PD1, the distribution of hyperparameter performances was skewed, with a large number of configurations achieving top performance. This characteristic limited the statistical significance of DPL's superior performance, as even non-model-based techniques could quickly find high-performing configurations in such scenarios.",
        "future_research_directions": "Future research will focus on improving DPL's capabilities. Specifically, the authors plan to investigate combining power laws with Gaussian Processes, aiming to enhance the accuracy and robustness of uncertainty estimation within the Bayesian Optimization framework. Additionally, they intend to explore the incorporation of various other fidelity types beyond just training epochs, to further expand the applicability and efficiency of multi-fidelity hyperparameter optimization.",
        "experimental_code": "from copy import deepcopyimport loggingimport osimport timefrom typing import List, Tupleimport numpy as npfrom scipy.stats import normimport torchfrom torch.utils.data import DataLoaderfrom data_loader.tabular_data_loader import WrappedDataLoaderfrom dataset.tabular_dataset import TabularDatasetfrom models.conditioned_power_law import ConditionedPowerLawclass PowerLawSurrogate:    def __init__(        self,        hp_candidates: np.ndarray,        surrogate_configs: dict = None,        seed: int = 11,        max_benchmark_epochs: int = 52,        ensemble_size: int = 5,        nr_epochs: int = 250,        fantasize_step: int = 1,        minimization: bool = True,        total_budget: int = 1000,        device: str = None,        output_path: str = '.',        dataset_name: str = 'unknown',        pretrain: bool = False,        backbone: str = 'power_law',        max_value: float = 100,        min_value: float = 0,        fill_value: str = 'zero',    ):        \"\"\"        Args:            hp_candidates: np.ndarray                The full list of hyperparameter candidates for a given dataset.            surrogate_configs: dict                The model configurations for the surrogate.            seed: int                The seed that will be used for the surrogate.            max_benchmark_epochs: int                The maximal budget that a hyperparameter configuration                has been evaluated in the benchmark for.            ensemble_size: int                The number of members in the ensemble.            nr_epochs: int                Number of epochs for which the surrogate should be                trained.            fantasize_step: int                The number of steps for which we are looking ahead to                evaluate the performance of a hpc.            minimization: bool                If for the evaluation metric, the lower the value the better.            total_budget: int                The total budget given. Used to calculate the initialization                percentage.            device: str                The device where the experiment will be run on.            output_path: str                The path where all the output will be stored.            dataset_name: str                The name of the dataset that the experiment will be run on.            pretrain: bool                If the surrogate will be pretrained before with a synthetic                curve.            backbone: str                The backbone, which can either be 'power_law' or 'nn'.            max_value: float                The maximal value for the dataset.            min_value: float                The minimal value for the dataset.            fill_value: str = 'zero',                The filling strategy for when learning curves are used.                Either 'zero' or 'last' where last represents the last value.        \"\"\"        torch.backends.cudnn.deterministic = True        torch.backends.cudnn.benchmark = False        self.total_budget = total_budget        self.fill_value = fill_value        self.max_value = max_value        self.min_value = min_value        self.backbone = backbone        self.pretrained_path = os.path.join(            output_path,            'power_law',            f'checkpoint_{seed}.pth',        )        self.model_instances = [            ConditionedPowerLaw,            ConditionedPowerLaw,            ConditionedPowerLaw,            ConditionedPowerLaw,            ConditionedPowerLaw,        ]        if device is None:            self.dev = torch.device(                'cuda') if torch.cuda.is_available() else torch.device('cpu')        else:            self.dev = torch.device(device)        self.learning_rate = 0.001        self.batch_size = 64        self.refine_batch_size = 64        self.criterion = torch.nn.L1Loss()        self.hp_candidates = hp_candidates        self.minimization = minimization        self.seed = seed        self.logger = logging.getLogger('power_law')        logging.basicConfig(            filename=f'power_law_surrogate_{dataset_name}_{seed}.log',            level=logging.INFO,            force=True,        )        # with what percentage configurations will be taken randomly instead of being sampled from the model        self.fraction_random_configs = 0.1        self.iteration_probabilities = np.random.rand(self.total_budget)        # the keys will be hyperparameter indices while the value        # will be a list with all the budgets evaluated for examples        # and with all performances for the performances        self.examples = dict()        self.performances = dict()        # set a seed already, so that it is deterministic when        # generating the seeds of the ensemble        torch.manual_seed(seed)        np.random.seed(seed)        self.seeds = np.random.choice(100, ensemble_size, replace=False)        self.max_benchmark_epochs = max_benchmark_epochs        self.ensemble_size = ensemble_size        self.nr_epochs = nr_epochs        self.refine_nr_epochs = 20        self.fantasize_step = fantasize_step        self.pretrain = pretrain        initial_configurations_nr = 1        conf_individual_budget = 1        init_conf_indices = np.random.choice(self.hp_candidates.shape[0], initial_configurations_nr, replace=False)        init_budgets = [i for i in range(1, conf_individual_budget + 1)]        self.rand_init_conf_indices = []        self.rand_init_budgets = []        # basically add every config index up to a certain budget threshold for the initialization        # we will go through both lists during the initialization        for config_index in init_conf_indices:            for config_budget in init_budgets:                self.rand_init_conf_indices.append(config_index)                self.rand_init_budgets.append(config_budget)        self.initial_random_index = 0        if surrogate_configs is None:            self.surrogate_configs = []            for i in range(0, self.ensemble_size):                self.surrogate_configs.append(                    {                        'nr_units': 128,                        'nr_layers': 2,                        'kernel_size': 3,                        'nr_filters': 4,                        'nr_cnn_layers': 2,                        'use_learning_curve': False,                    }                )        else:            self.surrogate_configs = surrogate_configs        self.nr_features = self.hp_candidates.shape[1]        self.best_value_observed = np.inf        self.diverged_configs = set()        # Where the models of the ensemble will be stored        self.models = []        # A tuple which will have the last evaluated point        # It will be used in the refining process        # Tuple(config_index, budget, performance, curve)        self.last_point = None        self.initial_full_training_trials = 10        # a flag if the surrogate should be trained        self.train = True        # the times it was refined        self.refine_counter = 0        # the surrogate iteration counter        self.iterations_counter = 0        # info dict to drop every surrogate iteration        self.info_dict = dict()        # the start time for the overhead of every surrogate iteration        # will be recorded here        self.suggest_time_duration = 0        self.output_path = output_path        self.dataset_name = dataset_name        self.no_improvement_threshold = int(self.max_benchmark_epochs + 0.2 * self.max_benchmark_epochs)        self.no_improvement_patience = 0    def _prepare_dataset(self) -> TabularDataset:        \"\"\"This method is called to prepare the necessary training dataset        for training a model.        Returns:            train_dataset: A dataset consisting of examples, labels, budgets                and learning curves.        \"\"\"        train_examples, train_labels, train_budgets, train_curves = self.history_configurations()        train_curves = self.prepare_training_curves(train_budgets, train_curves)        train_examples = np.array(train_examples, dtype=np.single)        train_labels = np.array(train_labels, dtype=np.single)        train_budgets = np.array(train_budgets, dtype=np.single)        # scale budgets to [0, 1]        train_budgets = train_budgets / self.max_benchmark_epochs        train_dataset = TabularDataset(            train_examples,            train_labels,            train_budgets,            train_curves,        )        return train_dataset    def _refine_surrogate(self):        \"\"\"Refine the surrogate model.        \"\"\"        for model_index, model_seed in enumerate(self.seeds):            train_dataset = self._prepare_dataset()            self.logger.info(f'Started refining model with index: {model_index}')            refined_model = self.train_pipeline(                model_index,                train_dataset,                nr_epochs=self.refine_nr_epochs,                refine=True,                weight_new_example=True,                batch_size=self.refine_batch_size,            )            self.models[model_index] = refined_model    def _train_surrogate(self, pretrain: bool = False):        \"\"\"Train the surrogate model.        Trains all the models of the ensemble        with different initializations and different        data orders.        Args:            pretrain: bool                If we have pretrained weights and we will just                refine the models.        \"\"\"        for model_index, model_seed in enumerate(self.seeds):            train_dataset = self._prepare_dataset()            self.logger.info(f'Started training model with index: {model_index}')            if pretrain:                # refine the models that were already pretrained                trained_model = self.train_pipeline(                    model_index,                    train_dataset,                    nr_epochs=self.refine_nr_epochs,                    refine=True,                    weight_new_example=False,                    batch_size=self.batch_size,                    early_stopping_it=self.refine_nr_epochs,  # basically no early stopping                )                self.models[model_index] = trained_model            else:                # train the models for the first time                trained_model = self.train_pipeline(                    model_index,                    train_dataset,                    nr_epochs=self.nr_epochs,                    refine=False,                    weight_new_example=False,                    batch_size=self.batch_size,                    early_stopping_it=self.nr_epochs,  # basically no early stopping                )                self.models.append(trained_model)    def train_pipeline(        self,        model_index: int,        train_dataset: TabularDataset,        nr_epochs: int,        refine: bool = False,        weight_new_example: bool = True,        batch_size: int = 64,        early_stopping_it: int = 10,        activate_early_stopping: bool = False,    ) -> torch.nn.Module:        \"\"\"Train an algorithm to predict the performance        of the hyperparameter configuration based on the budget.        Args:            model_index: int                The index of the model.            train_dataset: TabularDataset                The tabular dataset featuring the examples, labels,                budgets and curves.            nr_epochs: int                The number of epochs to train the model for.            refine: bool                If an existing model will be refined or if the training                will start from scratch.            weight_new_example: bool                If the last example that was added should be weighted more                by being included in every batch. This is only applicable                when refine is True.            batch_size: int                The batch size to be used for training.            early_stopping_it: int                The early stopping iteration patience.            activate_early_stopping: bool                Flag controlling the activation.        Returns:            model: torch.nn.Module                A trained model.        \"\"\"        if model_index == 0:            self.iterations_counter += 1            self.logger.info(f'Iteration number: {self.iterations_counter}')        surrogate_config = self.surrogate_configs[model_index]        seed = self.seeds[model_index]        torch.manual_seed(seed)        np.random.seed(seed)        if refine:            model = self.models[model_index]        else:            model = self.model_instances[model_index](                nr_initial_features=self.nr_features + 1 if self.backbone == 'nn' else self.nr_features,                nr_units=surrogate_config['nr_units'],                nr_layers=surrogate_config['nr_layers'],                use_learning_curve=surrogate_config['use_learning_curve'],                kernel_size=surrogate_config['kernel_size'],                nr_filters=surrogate_config['nr_filters'],                nr_cnn_layers=surrogate_config['nr_cnn_layers'],            )            model.to(self.dev)        # make the training dataset here        train_dataloader = DataLoader(            train_dataset,            batch_size=batch_size,            shuffle=True,        )        train_dataloader = WrappedDataLoader(train_dataloader, self.dev)        optimizer = torch.optim.Adam(model.parameters(), lr=self.learning_rate)        patience_rounds = 0        best_loss = np.inf        best_state = deepcopy(model.state_dict())        for epoch in range(0, nr_epochs):            running_loss = 0            model.train()            for batch_examples, batch_labels, batch_budgets, batch_curves in train_dataloader:                nr_examples_batch = batch_examples.shape[0]                # if only one example in the batch, skip the batch.                # Otherwise, the code will fail because of batchnormalization.                if nr_examples_batch == 1:                    continue                # zero the parameter gradients                optimizer.zero_grad(set_to_none=True)                # in case we are refining, we add the new example to every                # batch to give it more importance.                if refine and weight_new_example:                    newp_index, newp_budget, newp_performance, newp_curve = self.last_point                    new_example = np.array([self.hp_candidates[newp_index]], dtype=np.single)                    newp_missing_values = self.prepare_missing_values_channel([newp_budget])                    newp_budget = np.array([newp_budget], dtype=np.single) / self.max_benchmark_epochs                    newp_performance = np.array([newp_performance], dtype=np.single)                    modified_curve = deepcopy(newp_curve)                    difference = self.max_benchmark_epochs - len(modified_curve) - 1                    if difference > 0:                        modified_curve.extend([modified_curve[-1] if self.fill_value == 'last' else 0] * difference)                    modified_curve = np.array([modified_curve], dtype=np.single)                    newp_missing_values = np.array(newp_missing_values, dtype=np.single)                    # add depth dimension to the train_curves array and missing_value_matrix                    modified_curve = np.expand_dims(modified_curve, 1)                    newp_missing_values = np.expand_dims(newp_missing_values, 1)                    modified_curve = np.concatenate((modified_curve, newp_missing_values), axis=1)                    new_example = torch.tensor(new_example, device=self.dev)                    newp_budget = torch.tensor(newp_budget, device=self.dev)                    newp_performance = torch.tensor(newp_performance, device=self.dev)                    modified_curve = torch.tensor(modified_curve, device=self.dev)                    batch_examples = torch.cat((batch_examples, new_example))                    batch_budgets = torch.cat((batch_budgets, newp_budget))                    batch_labels = torch.cat((batch_labels, newp_performance))                    batch_curves = torch.cat((batch_curves, modified_curve))                outputs = model(batch_examples, batch_budgets, batch_budgets, batch_curves)                loss = self.criterion(outputs, batch_labels)                loss.backward()                optimizer.step()                # print statistics                running_loss += loss.item()            running_loss = running_loss / len(train_dataloader)            self.logger.info(f'Epoch {epoch +1}, Loss:{running_loss}')            if activate_early_stopping:                if running_loss < best_loss:                    best_state = deepcopy(model.state_dict())                    best_loss = running_loss                    patience_rounds = 0                elif running_loss > best_loss:                    patience_rounds += 1                    if patience_rounds == early_stopping_it:                        model.load_state_dict(best_state)                        self.logger.info(f'Stopping training since validation loss is not improving')                        break        if activate_early_stopping:            model.load_state_dict(best_state)        return model    def _predict(self) -> Tuple[np.ndarray, np.ndarray, List, np.ndarray]:        \"\"\"        Predict the performances of the hyperparameter configurations        as well as the standard deviations based on the ensemble.        Returns:            mean_predictions, std_predictions, hp_indices, real_budgets:            Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]                The mean predictions and the standard deviations over                all model predictions for the given hyperparameter                configurations with their associated indices and budgets.        \"\"\"        configurations, hp_indices, budgets, real_budgets, hp_curves = self.generate_candidate_configurations()        # scale budgets to [0, 1]        budgets = np.array(budgets, dtype=np.single)        hp_curves = self.prepare_training_curves(real_budgets, hp_curves)        budgets = budgets / self.max_benchmark_epochs        real_budgets = np.array(real_budgets, dtype=np.single)        configurations = np.array(configurations, dtype=np.single)        configurations = torch.tensor(configurations)        configurations = configurations.to(device=self.dev)        budgets = torch.tensor(budgets)        budgets = budgets.to(device=self.dev)        hp_curves = torch.tensor(hp_curves)        hp_curves = hp_curves.to(device=self.dev)        network_real_budgets = torch.tensor(real_budgets / self.max_benchmark_epochs)        network_real_budgets.to(device=self.dev)        all_predictions = []        for model in self.models:            model = model.eval()            predictions = model(configurations, budgets, network_real_budgets, hp_curves)            all_predictions.append(predictions.detach().cpu().numpy())        mean_predictions = np.mean(all_predictions, axis=0)        std_predictions = np.std(all_predictions, axis=0)        return mean_predictions, std_predictions, hp_indices, real_budgets    def suggest(self) -> Tuple[int, int]:        \"\"\"Suggest a hyperparameter configuration and a budget        to evaluate.        Returns:            suggested_hp_index, budget: Tuple[int, int]                The index of the hyperparamter configuration to be evaluated                and the budget for what it is going to be evaluated for.        \"\"\"        suggest_time_start = time.time()        if self.initial_random_index < len(self.rand_init_conf_indices):            self.logger.info(                'Not enough configurations to build a model. \\n'                'Returning randomly sampled configuration'            )            suggested_hp_index = self.rand_init_conf_indices[self.initial_random_index]            budget = self.rand_init_budgets[self.initial_random_index]            self.initial_random_index += 1        else:            mean_predictions, std_predictions, hp_indices, real_budgets = self._predict()            best_prediction_index = self.find_suggested_config(                mean_predictions,                std_predictions,            )            # actually do the mapping between the configuration indices and the best prediction            # index            suggested_hp_index = hp_indices[best_prediction_index]            if suggested_hp_index in self.examples:                evaluated_budgets = self.examples[suggested_hp_index]                max_budget = max(evaluated_budgets)                budget = max_budget + self.fantasize_step                if budget > self.max_benchmark_epochs:                    budget = self.max_benchmark_epochs            else:                budget = self.fantasize_step        suggest_time_end = time.time()        self.suggest_time_duration = suggest_time_end - suggest_time_start        return suggested_hp_index, budget    def observe(        self,        hp_index: int,        b: int,        hp_curve: List[float],    ):        \"\"\"Receive information regarding the performance of a hyperparameter        configuration that was suggested.        Args:            hp_index: int                The index of the evaluated hyperparameter configuration.            b: int                The budget for which the hyperparameter configuration was evaluated.            hp_curve: List                The performance of the hyperparameter configuration.        \"\"\"        for index, curve_element in enumerate(hp_curve):            if np.isnan(curve_element):                self.diverged_configs.add(hp_index)                # only use the non-nan part of the curve and the corresponding                # budget to still have the information in the network                hp_curve = hp_curve[0:index + 1]                b = index                break        if not self.minimization:            hp_curve = np.subtract([self.max_value] * len(hp_curve), hp_curve)            hp_curve = hp_curve.tolist()        best_curve_value = min(hp_curve)        self.examples[hp_index] = np.arange(1, b + 1)        self.performances[hp_index] = hp_curve        if self.best_value_observed > best_curve_value:            self.best_value_observed = best_curve_value            self.no_improvement_patience = 0            self.logger.info(f'New Incumbent value found '                             f'{1 - best_curve_value if not self.minimization else best_curve_value}')        else:            self.no_improvement_patience += 1            if self.no_improvement_patience == self.no_improvement_threshold:                self.train = True                self.no_improvement_patience = 0                self.logger.info(                    'No improvement in the incumbent value threshold reached, '                    'restarting training from scratch'                )        initial_empty_value = self.get_mean_initial_value() if self.fill_value == 'last' else 0        if self.initial_random_index >= len(self.rand_init_conf_indices):            performance = self.performances[hp_index]            self.last_point = (hp_index, b, performance[b-1], performance[0:b-1] if b > 1 else [initial_empty_value])            if self.train:                # delete the previously stored models                self.models = []                if self.pretrain:                    # TODO Load the pregiven weights.                    pass                self._train_surrogate(pretrain=self.pretrain)                if self.iterations_counter <= self.initial_full_training_trials:                    self.train = True                else:                    self.train = False            else:                self.refine_counter += 1                self._refine_surrogate()    def prepare_examples(self, hp_indices: List) -> List:        \"\"\"        Prepare the examples to be given to the surrogate model.        Args:            hp_indices: List                The list of hp indices that are already evaluated.        Returns:            examples: List                A list of the hyperparameter configurations.        \"\"\"        examples = []        for hp_index in hp_indices:            examples.append(self.hp_candidates[hp_index])        return examples    def generate_candidate_configurations(self) -> Tuple[List, List, List, List, List]:        \"\"\"Generate candidate configurations that will be        fantasized upon.        Returns:            (configurations, hp_indices, hp_budgets, real_budgets, hp_curves): Tuple                A tuple of configurations, their indices in the hp list,                the budgets that they should be fantasized upon, the maximal                budgets they have been evaluated and their corresponding performance                curves.        \"\"\"        hp_indices = []        hp_budgets = []        hp_curves = []        real_budgets = []        initial_empty_value = self.get_mean_initial_value() if self.fill_value == 'last' else 0        for hp_index in range(0, self.hp_candidates.shape[0]):            if hp_index in self.examples:                budgets = self.examples[hp_index]                # Take the max budget evaluated for a certain hpc                max_budget = budgets[-1]                if max_budget == self.max_benchmark_epochs:                    continue                real_budgets.append(max_budget)                learning_curve = self.performances[hp_index]                hp_curve = learning_curve[0:max_budget-1] if max_budget > 1 else [initial_empty_value]            else:                real_budgets.append(1)                hp_curve = [initial_empty_value]            hp_indices.append(hp_index)            hp_budgets.append(self.max_benchmark_epochs)            hp_curves.append(hp_curve)        configurations = self.prepare_examples(hp_indices)        return configurations, hp_indices, hp_budgets, real_budgets, hp_curves    def history_configurations(self) -> Tuple[List, List, List, List]:        \"\"\"        Generate the configurations, labels, budgets and curves        based on the history of evaluated configurations.        Returns:            (train_examples, train_labels, train_budgets, train_curves): Tuple                A tuple of examples, labels and budgets for the                configurations evaluated so far.        \"\"\"        train_examples = []        train_labels = []        train_budgets = []        train_curves = []        initial_empty_value = self.get_mean_initial_value() if self.fill_value == 'last' else 0        for hp_index in self.examples:            budgets = self.examples[hp_index]            performances = self.performances[hp_index]            example = self.hp_candidates[hp_index]            for budget in budgets:                example_curve = performances[0:budget-1]                train_examples.append(example)                train_budgets.append(budget)                train_labels.append(performances[budget - 1])                train_curves.append(example_curve if len(example_curve) > 0 else [initial_empty_value])        return train_examples, train_labels, train_budgets, train_curves    @staticmethod    def acq(        best_values: np.ndarray,        mean_predictions: np.ndarray,        std_predictions: np.ndarray,        explore_factor: float = 0.25,        acq_choice: str = 'ei',    ) -> np.ndarray:        \"\"\"        Calculate the acquisition function based on the network predictions.        Args:        -----        best_values: np.ndarray            An array with the best value for every configuration.            Depending on the implementation it can be different for every            configuration.        mean_predictions: np.ndarray            The mean values of the model predictions.        std_predictions: np.ndarray            The standard deviation values of the model predictions.        explore_factor: float            The explore factor, when ucb is used as an acquisition            function.        acq_choice: str            The choice for the acquisition function to use.        Returns        -------        acq_values: np.ndarray            The values of the acquisition function for every configuration.        \"\"\"        if acq_choice == 'ei':            z = (np.subtract(best_values, mean_predictions))            difference = deepcopy(z)            not_zero_std_indicator = [False if example_std == 0.0 else True for example_std in std_predictions]            zero_std_indicator = np.invert(not_zero_std_indicator)            z = np.divide(z, std_predictions, where=not_zero_std_indicator)            np.place(z, zero_std_indicator, 0)            acq_values = np.add(np.multiply(difference, norm.cdf(z)), np.multiply(std_predictions, norm.pdf(z)))        elif acq_choice == 'ucb':            # we are working with error rates so we multiply the mean with -1            acq_values = np.add(-1 * mean_predictions, explore_factor * std_predictions)        elif acq_choice == 'thompson':            acq_values = np.random.normal(mean_predictions, std_predictions)        else:            acq_values = mean_predictions        return acq_values    def find_suggested_config(            self,            mean_predictions: np.ndarray,            mean_stds: np.ndarray,    ) -> int:        \"\"\"Return the hyperparameter with the highest acq function value.        Given the mean predictions and mean standard deviations from the DPL        ensemble for every hyperparameter configuraiton, return the hyperparameter        configuration that has the highest acquisition function value.        Args:            mean_predictions: np.ndarray                The mean predictions of the ensemble for every hyperparameter                configuration.            mean_stds: np.ndarray                The standard deviation predictions of the ensemble for every                hyperparameter configuration.        Returns:            max_value_index: int                the index of the maximal value.        \"\"\"        best_values = np.array([self.best_value_observed] * mean_predictions.shape[0])        acq_func_values = self.acq(            best_values,            mean_predictions,            mean_stds,            acq_choice='ei',        )        max_value_index = np.argmax(acq_func_values)        return max_value_index    def calculate_fidelity_ymax(self, fidelity: int) -> float:        \"\"\"Calculate the incumbent for a certain fidelity level.        Args:            fidelity: int                The given budget fidelity.        Returns:            best_value: float                The incumbent value for a certain fidelity level.        \"\"\"        config_values = []        for example_index in self.examples.keys():            try:                performance = self.performances[example_index][fidelity - 1]            except IndexError:                performance = self.performances[example_index][-1]            config_values.append(performance)        # lowest error corresponds to best value        best_value = min(config_values)        return best_value    def patch_curves_to_same_length(self, curves: List):        \"\"\"        Patch the given curves to the same length.        Finds the maximum curve length and patches all        other curves that are shorter with zeroes.        Args:            curves: List                The hyperparameter curves.        \"\"\"        for curve in curves:            difference = self.max_benchmark_epochs - len(curve) - 1            if difference > 0:                fill_value = [curve[-1]] if self.fill_value == 'last' else [0]                curve.extend(fill_value * difference)    def prepare_missing_values_channel(self, budgets: List) -> List:        \"\"\"Prepare an additional channel for learning curves.        The additional channel will represent an existing learning        curve value with a 1 and a missing learning curve value with        a 0.        Args:            budgets: List                A list of budgets for every training point.        Returns:            missing_value_curves: List                A list of curves representing existing or missing                values for the training curves of the training points.        \"\"\"        missing_value_curves = []        for i in range(len(budgets)):            budget = budgets[i]            budget = budget - 1            budget = int(budget)            if budget > 0:                example_curve = [1] * budget            else:                example_curve = []            difference_in_curve = self.max_benchmark_epochs - len(example_curve) - 1            if difference_in_curve > 0:                example_curve.extend([0] * difference_in_curve)            missing_value_curves.append(example_curve)        return missing_value_curves    def get_mean_initial_value(self):        \"\"\"Returns the mean initial value        for all hyperparameter configurations in the history so far.        Returns:            mean_initial_value: float                Mean initial value for all hyperparameter configurations                observed.        \"\"\"        first_values = []        for performance_curve in self.performances.values():            first_values.append(performance_curve[0])        mean_initial_value = np.mean(first_values)        return mean_initial_value    def prepare_training_curves(            self,            train_budgets: List[int],            train_curves: List[float]    ) -> np.ndarray:        \"\"\"Prepare the configuration performance curves for training.        For every configuration training curve, add an extra dimension        regarding the missing values, as well as extend the curve to have        a fixed uniform length for all.        Args:            train_budgets: List                A list of the budgets for all training points.            train_curves: List                A list of curves that pertain to every training point.        Returns:            train_curves: np.ndarray                The transformed training curves.        \"\"\"        missing_value_matrix = self.prepare_missing_values_channel(train_budgets)        self.patch_curves_to_same_length(train_curves)        train_curves = np.array(train_curves, dtype=np.single)        missing_value_matrix = np.array(missing_value_matrix, dtype=np.single)        # add depth dimension to the train_curves array and missing_value_matrix        train_curves = np.expand_dims(train_curves, 1)        missing_value_matrix = np.expand_dims(missing_value_matrix, 1)        train_curves = np.concatenate((train_curves, missing_value_matrix), axis=1)        return train_curvesfrom copy import deepcopyimport loggingimport osimport timefrom typing import List, Tupleimport numpy as npfrom scipy.stats import normimport torchfrom torch.utils.data import DataLoaderfrom data_loader.tabular_data_loader import WrappedDataLoaderfrom dataset.tabular_dataset import TabularDatasetfrom models.conditioned_power_law import ConditionedPowerLawclass PowerLawSurrogate:    def __init__(        self,        hp_candidates: np.ndarray,        surrogate_configs: dict = None,        seed: int = 11,        max_benchmark_epochs: int = 52,        ensemble_size: int = 5,        nr_epochs: int = 250,        fantasize_step: int = 1,        minimization: bool = True,        total_budget: int = 1000,        device: str = None,        output_path: str = '.',        dataset_name: str = 'unknown',        pretrain: bool = False,        backbone: str = 'power_law',        max_value: float = 100,        min_value: float = 0,        fill_value: str = 'zero',    ):        \"\"\"        Args:            hp_candidates: np.ndarray                The full list of hyperparameter candidates for a given dataset.            surrogate_configs: dict                The model configurations for the surrogate.            seed: int                The seed that will be used for the surrogate.            max_benchmark_epochs: int                The maximal budget that a hyperparameter configuration                has been evaluated in the benchmark for.            ensemble_size: int                The number of members in the ensemble.            nr_epochs: int                Number of epochs for which the surrogate should be                trained.            fantasize_step: int                The number of steps for which we are looking ahead to                evaluate the performance of a hpc.            minimization: bool                If for the evaluation metric, the lower the value the better.            total_budget: int                The total budget given. Used to calculate the initialization                percentage.            device: str                The device where the experiment will be run on.            output_path: str                The path where all the output will be stored.            dataset_name: str                The name of the dataset that the experiment will be run on.            pretrain: bool                If the surrogate will be pretrained before with a synthetic                curve.            backbone: str                The backbone, which can either be 'power_law' or 'nn'.            max_value: float                The maximal value for the dataset.            min_value: float                The minimal value for the dataset.            fill_value: str = 'zero',                The filling strategy for when learning curves are used.                Either 'zero' or 'last' where last represents the last value.        \"\"\"        torch.backends.cudnn.deterministic = True        torch.backends.cudnn.benchmark = False        self.total_budget = total_budget        self.fill_value = fill_value        self.max_value = max_value        self.min_value = min_value        self.backbone = backbone        self.pretrained_path = os.path.join(            output_path,            'power_law',            f'checkpoint_{seed}.pth',        )        self.model_instances = [            ConditionedPowerLaw,            ConditionedPowerLaw,            ConditionedPowerLaw,            ConditionedPowerLaw,            ConditionedPowerLaw,        ]        if device is None:            self.dev = torch.device(                'cuda') if torch.cuda.is_available() else torch.device('cpu')        else:            self.dev = torch.device(device)        self.learning_rate = 0.001        self.batch_size = 64        self.refine_batch_size = 64        self.criterion = torch.nn.L1Loss()        self.hp_candidates = hp_candidates        self.minimization = minimization        self.seed = seed        self.logger = logging.getLogger('power_law')        logging.basicConfig(            filename=f'power_law_surrogate_{dataset_name}_{seed}.log',            level=logging.INFO,            force=True,        )        # with what percentage configurations will be taken randomly instead of being sampled from the model        self.fraction_random_configs = 0.1        self.iteration_probabilities = np.random.rand(self.total_budget)        # the keys will be hyperparameter indices while the value        # will be a list with all the budgets evaluated for examples        # and with all performances for the performances        self.examples = dict()        self.performances = dict()        # set a seed already, so that it is deterministic when        # generating the seeds of the ensemble        torch.manual_seed(seed)        np.random.seed(seed)        self.seeds = np.random.choice(100, ensemble_size, replace=False)        self.max_benchmark_epochs = max_benchmark_epochs        self.ensemble_size = ensemble_size        self.nr_epochs = nr_epochs        self.refine_nr_epochs = 20        self.fantasize_step = fantasize_step        self.pretrain = pretrain        initial_configurations_nr = 1        conf_individual_budget = 1        init_conf_indices = np.random.choice(self.hp_candidates.shape[0], initial_configurations_nr, replace=False)        init_budgets = [i for i in range(1, conf_individual_budget + 1)]        self.rand_init_conf_indices = []        self.rand_init_budgets = []        # basically add every config index up to a certain budget threshold for the initialization        # we will go through both lists during the initialization        for config_index in init_conf_indices:            for config_budget in init_budgets:                self.rand_init_conf_indices.append(config_index)                self.rand_init_budgets.append(config_budget)        self.initial_random_index = 0        if surrogate_configs is None:            self.surrogate_configs = []            for i in range(0, self.ensemble_size):                self.surrogate_configs.append(                    {                        'nr_units': 128,                        'nr_layers': 2,                        'kernel_size': 3,                        'nr_filters': 4,                        'nr_cnn_layers': 2,                        'use_learning_curve': False,                    }                )        else:            self.surrogate_configs = surrogate_configs        self.nr_features = self.hp_candidates.shape[1]        self.best_value_observed = np.inf        self.diverged_configs = set()        # Where the models of the ensemble will be stored        self.models = []        # A tuple which will have the last evaluated point        # It will be used in the refining process        # Tuple(config_index, budget, performance, curve)        self.last_point = None        self.initial_full_training_trials = 10        # a flag if the surrogate should be trained        self.train = True        # the times it was refined        self.refine_counter = 0        # the surrogate iteration counter        self.iterations_counter = 0        # info dict to drop every surrogate iteration        self.info_dict = dict()        # the start time for the overhead of every surrogate iteration        # will be recorded here        self.suggest_time_duration = 0        self.output_path = output_path        self.dataset_name = dataset_name        self.no_improvement_threshold = int(self.max_benchmark_epochs + 0.2 * self.max_benchmark_epochs)        self.no_improvement_patience = 0    def _prepare_dataset(self) -> TabularDataset:        \"\"\"This method is called to prepare the necessary training dataset        for training a model.        Returns:            train_dataset: A dataset consisting of examples, labels, budgets                and learning curves.        \"\"\"        train_examples, train_labels, train_budgets, train_curves = self.history_configurations()        train_curves = self.prepare_training_curves(train_budgets, train_curves)        train_examples = np.array(train_examples, dtype=np.single)        train_labels = np.array(train_labels, dtype=np.single)        train_budgets = np.array(train_budgets, dtype=np.single)        # scale budgets to [0, 1]        train_budgets = train_budgets / self.max_benchmark_epochs        train_dataset = TabularDataset(            train_examples,            train_labels,            train_budgets,            train_curves,        )        return train_dataset    def _refine_surrogate(self):        \"\"\"Refine the surrogate model.        \"\"\"        for model_index, model_seed in enumerate(self.seeds):            train_dataset = self._prepare_dataset()            self.logger.info(f'Started refining model with index: {model_index}')            refined_model = self.train_pipeline(                model_index,                train_dataset,                nr_epochs=self.refine_nr_epochs,                refine=True,                weight_new_example=True,                batch_size=self.refine_batch_size,            )            self.models[model_index] = refined_model    def _train_surrogate(self, pretrain: bool = False):        \"\"\"Train the surrogate model.        Trains all the models of the ensemble        with different initializations and different        data orders.        Args:            pretrain: bool                If we have pretrained weights and we will just                refine the models.        \"\"\"        for model_index, model_seed in enumerate(self.seeds):            train_dataset = self._prepare_dataset()            self.logger.info(f'Started training model with index: {model_index}')            if pretrain:                # refine the models that were already pretrained                trained_model = self.train_pipeline(                    model_index,                    train_dataset,                    nr_epochs=self.refine_nr_epochs,                    refine=True,                    weight_new_example=False,                    batch_size=self.batch_size,                    early_stopping_it=self.refine_nr_epochs,  # basically no early stopping                )                self.models[model_index] = trained_model            else:                # train the models for the first time                trained_model = self.train_pipeline(                    model_index,                    train_dataset,                    nr_epochs=self.nr_epochs,                    refine=False,                    weight_new_example=False,                    batch_size=self.batch_size,                    early_stopping_it=self.nr_epochs,  # basically no early stopping                )                self.models.append(trained_model)    def train_pipeline(        self,        model_index: int,        train_dataset: TabularDataset,        nr_epochs: int,        refine: bool = False,        weight_new_example: bool = True,        batch_size: int = 64,        early_stopping_it: int = 10,        activate_early_stopping: bool = False,    ) -> torch.nn.Module:        \"\"\"Train an algorithm to predict the performance        of the hyperparameter configuration based on the budget.        Args:            model_index: int                The index of the model.            train_dataset: TabularDataset                The tabular dataset featuring the examples, labels,                budgets and curves.            nr_epochs: int                The number of epochs to train the model for.            refine: bool                If an existing model will be refined or if the training                will start from scratch.            weight_new_example: bool                If the last example that was added should be weighted more                by being included in every batch. This is only applicable                when refine is True.            batch_size: int                The batch size to be used for training.            early_stopping_it: int                The early stopping iteration patience.            activate_early_stopping: bool                Flag controlling the activation.        Returns:            model: torch.nn.Module                A trained model.        \"\"\"        if model_index == 0:            self.iterations_counter += 1            self.logger.info(f'Iteration number: {self.iterations_counter}')        surrogate_config = self.surrogate_configs[model_index]        seed = self.seeds[model_index]        torch.manual_seed(seed)        np.random.seed(seed)        if refine:            model = self.models[model_index]        else:            model = self.model_instances[model_index](                nr_initial_features=self.nr_features + 1 if self.backbone == 'nn' else self.nr_features,                nr_units=surrogate_config['nr_units'],                nr_layers=surrogate_config['nr_layers'],                use_learning_curve=surrogate_config['use_learning_curve'],                kernel_size=surrogate_config['kernel_size'],                nr_filters=surrogate_config['nr_filters'],                nr_cnn_layers=surrogate_config['nr_cnn_layers'],            )            model.to(self.dev)        # make the training dataset here        train_dataloader = DataLoader(            train_dataset,            batch_size=batch_size,            shuffle=True,        )        train_dataloader = WrappedDataLoader(train_dataloader, self.dev)        optimizer = torch.optim.Adam(model.parameters(), lr=self.learning_rate)        patience_rounds = 0        best_loss = np.inf        best_state = deepcopy(model.state_dict())        for epoch in range(0, nr_epochs):            running_loss = 0            model.train()            for batch_examples, batch_labels, batch_budgets, batch_curves in train_dataloader:                nr_examples_batch = batch_examples.shape[0]                # if only one example in the batch, skip the batch.                # Otherwise, the code will fail because of batchnormalization.                if nr_examples_batch == 1:                    continue                # zero the parameter gradients                optimizer.zero_grad(set_to_none=True)                # in case we are refining, we add the new example to every                # batch to give it more importance.                if refine and weight_new_example:                    newp_index, newp_budget, newp_performance, newp_curve = self.last_point                    new_example = np.array([self.hp_candidates[newp_index]], dtype=np.single)                    newp_missing_values = self.prepare_missing_values_channel([newp_budget])                    newp_budget = np.array([newp_budget], dtype=np.single) / self.max_benchmark_epochs                    newp_performance = np.array([newp_performance], dtype=np.single)                    modified_curve = deepcopy(newp_curve)                    difference = self.max_benchmark_epochs - len(modified_curve) - 1                    if difference > 0:                        modified_curve.extend([modified_curve[-1] if self.fill_value == 'last' else 0] * difference)                    modified_curve = np.array([modified_curve], dtype=np.single)                    newp_missing_values = np.array(newp_missing_values, dtype=np.single)                    # add depth dimension to the train_curves array and missing_value_matrix                    modified_curve = np.expand_dims(modified_curve, 1)                    newp_missing_values = np.expand_dims(newp_missing_values, 1)                    modified_curve = np.concatenate((modified_curve, newp_missing_values), axis=1)                    new_example = torch.tensor(new_example, device=self.dev)                    newp_budget = torch.tensor(newp_budget, device=self.dev)                    newp_performance = torch.tensor(newp_performance, device=self.dev)                    modified_curve = torch.tensor(modified_curve, device=self.dev)                    batch_examples = torch.cat((batch_examples, new_example))                    batch_budgets = torch.cat((batch_budgets, newp_budget))                    batch_labels = torch.cat((batch_labels, newp_performance))                    batch_curves = torch.cat((batch_curves, modified_curve))                outputs = model(batch_examples, batch_budgets, batch_budgets, batch_curves)                loss = self.criterion(outputs, batch_labels)                loss.backward()                optimizer.step()                # print statistics                running_loss += loss.item()            running_loss = running_loss / len(train_dataloader)            self.logger.info(f'Epoch {epoch +1}, Loss:{running_loss}')            if activate_early_stopping:                if running_loss < best_loss:                    best_state = deepcopy(model.state_dict())                    best_loss = running_loss                    patience_rounds = 0                elif running_loss > best_loss:                    patience_rounds += 1                    if patience_rounds == early_stopping_it:                        model.load_state_dict(best_state)                        self.logger.info(f'Stopping training since validation loss is not improving')                        break        if activate_early_stopping:            model.load_state_dict(best_state)        return model    def _predict(self) -> Tuple[np.ndarray, np.ndarray, List, np.ndarray]:        \"\"\"        Predict the performances of the hyperparameter configurations        as well as the standard deviations based on the ensemble.        Returns:            mean_predictions, std_predictions, hp_indices, real_budgets:            Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]                The mean predictions and the standard deviations over                all model predictions for the given hyperparameter                configurations with their associated indices and budgets.        \"\"\"        configurations, hp_indices, budgets, real_budgets, hp_curves = self.generate_candidate_configurations()        # scale budgets to [0, 1]        budgets = np.array(budgets, dtype=np.single)        hp_curves = self.prepare_training_curves(real_budgets, hp_curves)        budgets = budgets / self.max_benchmark_epochs        real_budgets = np.array(real_budgets, dtype=np.single)        configurations = np.array(configurations, dtype=np.single)        configurations = torch.tensor(configurations)        configurations = configurations.to(device=self.dev)        budgets = torch.tensor(budgets)        budgets = budgets.to(device=self.dev)        hp_curves = torch.tensor(hp_curves)        hp_curves = hp_curves.to(device=self.dev)        network_real_budgets = torch.tensor(real_budgets / self.max_benchmark_epochs)        network_real_budgets.to(device=self.dev)        all_predictions = []        for model in self.models:            model = model.eval()            predictions = model(configurations, budgets, network_real_budgets, hp_curves)            all_predictions.append(predictions.detach().cpu().numpy())        mean_predictions = np.mean(all_predictions, axis=0)        std_predictions = np.std(all_predictions, axis=0)        return mean_predictions, std_predictions, hp_indices, real_budgets    def suggest(self) -> Tuple[int, int]:        \"\"\"Suggest a hyperparameter configuration and a budget        to evaluate.        Returns:            suggested_hp_index, budget: Tuple[int, int]                The index of the hyperparamter configuration to be evaluated                and the budget for what it is going to be evaluated for.        \"\"\"        suggest_time_start = time.time()        if self.initial_random_index < len(self.rand_init_conf_indices):            self.logger.info(                'Not enough configurations to build a model. \\n'                'Returning randomly sampled configuration'            )            suggested_hp_index = self.rand_init_conf_indices[self.initial_random_index]            budget = self.rand_init_budgets[self.initial_random_index]            self.initial_random_index += 1        else:            mean_predictions, std_predictions, hp_indices, real_budgets = self._predict()            best_prediction_index = self.find_suggested_config(                mean_predictions,                std_predictions,            )            # actually do the mapping between the configuration indices and the best prediction            # index            suggested_hp_index = hp_indices[best_prediction_index]            if suggested_hp_index in self.examples:                evaluated_budgets = self.examples[suggested_hp_index]                max_budget = max(evaluated_budgets)                budget = max_budget + self.fantasize_step                if budget > self.max_benchmark_epochs:                    budget = self.max_benchmark_epochs            else:                budget = self.fantasize_step        suggest_time_end = time.time()        self.suggest_time_duration = suggest_time_end - suggest_time_start        return suggested_hp_index, budget    def observe(        self,        hp_index: int,        b: int,        hp_curve: List[float],    ):        \"\"\"Receive information regarding the performance of a hyperparameter        configuration that was suggested.        Args:            hp_index: int                The index of the evaluated hyperparameter configuration.            b: int                The budget for which the hyperparameter configuration was evaluated.            hp_curve: List                The performance of the hyperparameter configuration.        \"\"\"        for index, curve_element in enumerate(hp_curve):            if np.isnan(curve_element):                self.diverged_configs.add(hp_index)                # only use the non-nan part of the curve and the corresponding                # budget to still have the information in the network                hp_curve = hp_curve[0:index + 1]                b = index                break        if not self.minimization:            hp_curve = np.subtract([self.max_value] * len(hp_curve), hp_curve)            hp_curve = hp_curve.tolist()        best_curve_value = min(hp_curve)        self.examples[hp_index] = np.arange(1, b + 1)        self.performances[hp_index] = hp_curve        if self.best_value_observed > best_curve_value:            self.best_value_observed = best_curve_value            self.no_improvement_patience = 0            self.logger.info(f'New Incumbent value found '                             f'{1 - best_curve_value if not self.minimization else best_curve_value}')        else:            self.no_improvement_patience += 1            if self.no_improvement_patience == self.no_improvement_threshold:                self.train = True                self.no_improvement_patience = 0                self.logger.info(                    'No improvement in the incumbent value threshold reached, '                    'restarting training from scratch'                )        initial_empty_value = self.get_mean_initial_value() if self.fill_value == 'last' else 0        if self.initial_random_index >= len(self.rand_init_conf_indices):            performance = self.performances[hp_index]            self.last_point = (hp_index, b, performance[b-1], performance[0:b-1] if b > 1 else [initial_empty_value])            if self.train:                # delete the previously stored models                self.models = []                if self.pretrain:                    # TODO Load the pregiven weights.                    pass                self._train_surrogate(pretrain=self.pretrain)                if self.iterations_counter <= self.initial_full_training_trials:                    self.train = True                else:                    self.train = False            else:                self.refine_counter += 1                self._refine_surrogate()    def prepare_examples(self, hp_indices: List) -> List:        \"\"\"        Prepare the examples to be given to the surrogate model.        Args:            hp_indices: List                The list of hp indices that are already evaluated.        Returns:            examples: List                A list of the hyperparameter configurations.        \"\"\"        examples = []        for hp_index in hp_indices:            examples.append(self.hp_candidates[hp_index])        return examples    def generate_candidate_configurations(self) -> Tuple[List, List, List, List, List]:        \"\"\"Generate candidate configurations that will be        fantasized upon.        Returns:            (configurations, hp_indices, hp_budgets, real_budgets, hp_curves): Tuple                A tuple of configurations, their indices in the hp list,                the budgets that they should be fantasized upon, the maximal                budgets they have been evaluated and their corresponding performance                curves.        \"\"\"        hp_indices = []        hp_budgets = []        hp_curves = []        real_budgets = []        initial_empty_value = self.get_mean_initial_value() if self.fill_value == 'last' else 0        for hp_index in range(0, self.hp_candidates.shape[0]):            if hp_index in self.examples:                budgets = self.examples[hp_index]                # Take the max budget evaluated for a certain hpc                max_budget = budgets[-1]                if max_budget == self.max_benchmark_epochs:                    continue                real_budgets.append(max_budget)                learning_curve = self.performances[hp_index]                hp_curve = learning_curve[0:max_budget-1] if max_budget > 1 else [initial_empty_value]            else:                real_budgets.append(1)                hp_curve = [initial_empty_value]            hp_indices.append(hp_index)            hp_budgets.append(self.max_benchmark_epochs)            hp_curves.append(hp_curve)        configurations = self.prepare_examples(hp_indices)        return configurations, hp_indices, hp_budgets, real_budgets, hp_curves    def history_configurations(self) -> Tuple[List, List, List, List]:        \"\"\"        Generate the configurations, labels, budgets and curves        based on the history of evaluated configurations.        Returns:            (train_examples, train_labels, train_budgets, train_curves): Tuple                A tuple of examples, labels and budgets for the                configurations evaluated so far.        \"\"\"        train_examples = []        train_labels = []        train_budgets = []        train_curves = []        initial_empty_value = self.get_mean_initial_value() if self.fill_value == 'last' else 0        for hp_index in self.examples:            budgets = self.examples[hp_index]            performances = self.performances[hp_index]            example = self.hp_candidates[hp_index]            for budget in budgets:                example_curve = performances[0:budget-1]                train_examples.append(example)                train_budgets.append(budget)                train_labels.append(performances[budget - 1])                train_curves.append(example_curve if len(example_curve) > 0 else [initial_empty_value])        return train_examples, train_labels, train_budgets, train_curves    @staticmethod    def acq(        best_values: np.ndarray,        mean_predictions: np.ndarray,        std_predictions: np.ndarray,        explore_factor: float = 0.25,        acq_choice: str = 'ei',    ) -> np.ndarray:        \"\"\"        Calculate the acquisition function based on the network predictions.        Args:        -----        best_values: np.ndarray            An array with the best value for every configuration.            Depending on the implementation it can be different for every            configuration.        mean_predictions: np.ndarray            The mean values of the model predictions.        std_predictions: np.ndarray            The standard deviation values of the model predictions.        explore_factor: float            The explore factor, when ucb is used as an acquisition            function.        acq_choice: str            The choice for the acquisition function to use.        Returns        -------        acq_values: np.ndarray            The values of the acquisition function for every configuration.        \"\"\"        if acq_choice == 'ei':            z = (np.subtract(best_values, mean_predictions))            difference = deepcopy(z)            not_zero_std_indicator = [False if example_std == 0.0 else True for example_std in std_predictions]            zero_std_indicator = np.invert(not_zero_std_indicator)            z = np.divide(z, std_predictions, where=not_zero_std_indicator)            np.place(z, zero_std_indicator, 0)            acq_values = np.add(np.multiply(difference, norm.cdf(z)), np.multiply(std_predictions, norm.pdf(z)))        elif acq_choice == 'ucb':            # we are working with error rates so we multiply the mean with -1            acq_values = np.add(-1 * mean_predictions, explore_factor * std_predictions)        elif acq_choice == 'thompson':            acq_values = np.random.normal(mean_predictions, std_predictions)        else:            acq_values = mean_predictions        return acq_values    def find_suggested_config(            self,            mean_predictions: np.ndarray,            mean_stds: np.ndarray,    ) -> int:        \"\"\"Return the hyperparameter with the highest acq function value.        Given the mean predictions and mean standard deviations from the DPL        ensemble for every hyperparameter configuraiton, return the hyperparameter        configuration that has the highest acquisition function value.        Args:            mean_predictions: np.ndarray                The mean predictions of the ensemble for every hyperparameter                configuration.            mean_stds: np.ndarray                The standard deviation predictions of the ensemble for every                hyperparameter configuration.        Returns:            max_value_index: int                the index of the maximal value.        \"\"\"        best_values = np.array([self.best_value_observed] * mean_predictions.shape[0])        acq_func_values = self.acq(            best_values,            mean_predictions,            mean_stds,            acq_choice='ei',        )        max_value_index = np.argmax(acq_func_values)        return max_value_index    def calculate_fidelity_ymax(self, fidelity: int) -> float:        \"\"\"Calculate the incumbent for a certain fidelity level.        Args:            fidelity: int                The given budget fidelity.        Returns:            best_value: float                The incumbent value for a certain fidelity level.        \"\"\"        config_values = []        for example_index in self.examples.keys():            try:                performance = self.performances[example_index][fidelity - 1]            except IndexError:                performance = self.performances[example_index][-1]            config_values.append(performance)        # lowest error corresponds to best value        best_value = min(config_values)        return best_value    def patch_curves_to_same_length(self, curves: List):        \"\"\"        Patch the given curves to the same length.        Finds the maximum curve length and patches all        other curves that are shorter with zeroes.        Args:            curves: List                The hyperparameter curves.        \"\"\"        for curve in curves:            difference = self.max_benchmark_epochs - len(curve) - 1            if difference > 0:                fill_value = [curve[-1]] if self.fill_value == 'last' else [0]                curve.extend(fill_value * difference)    def prepare_missing_values_channel(self, budgets: List) -> List:        \"\"\"Prepare an additional channel for learning curves.        The additional channel will represent an existing learning        curve value with a 1 and a missing learning curve value with        a 0.        Args:            budgets: List                A list of budgets for every training point.        Returns:            missing_value_curves: List                A list of curves representing existing or missing                values for the training curves of the training points.        \"\"\"        missing_value_curves = []        for i in range(len(budgets)):            budget = budgets[i]            budget = budget - 1            budget = int(budget)            if budget > 0:                example_curve = [1] * budget            else:                example_curve = []            difference_in_curve = self.max_benchmark_epochs - len(example_curve) - 1            if difference_in_curve > 0:                example_curve.extend([0] * difference_in_curve)            missing_value_curves.append(example_curve)        return missing_value_curves    def get_mean_initial_value(self):        \"\"\"Returns the mean initial value        for all hyperparameter configurations in the history so far.        Returns:            mean_initial_value: float                Mean initial value for all hyperparameter configurations                observed.        \"\"\"        first_values = []        for performance_curve in self.performances.values():            first_values.append(performance_curve[0])        mean_initial_value = np.mean(first_values)        return mean_initial_value    def prepare_training_curves(            self,            train_budgets: List[int],            train_curves: List[float]    ) -> np.ndarray:        \"\"\"Prepare the configuration performance curves for training.        For every configuration training curve, add an extra dimension        regarding the missing values, as well as extend the curve to have        a fixed uniform length for all.        Args:            train_budgets: List                A list of the budgets for all training points.            train_curves: List                A list of curves that pertain to every training point.        Returns:            train_curves: np.ndarray                The transformed training curves.        \"\"\"        missing_value_matrix = self.prepare_missing_values_channel(train_budgets)        self.patch_curves_to_same_length(train_curves)        train_curves = np.array(train_curves, dtype=np.single)        missing_value_matrix = np.array(missing_value_matrix, dtype=np.single)        # add depth dimension to the train_curves array and missing_value_matrix        train_curves = np.expand_dims(train_curves, 1)        missing_value_matrix = np.expand_dims(missing_value_matrix, 1)        train_curves = np.concatenate((train_curves, missing_value_matrix), axis=1)        return train_curvesimport torchimport torch.nn as nnclass ConditionedPowerLaw(nn.Module):    def __init__(        self,        nr_initial_features=10,        nr_units=200,        nr_layers=3,        use_learning_curve: bool = True,        kernel_size: int = 3,        nr_filters: int = 4,        nr_cnn_layers: int = 2,    ):        \"\"\"        Args:            nr_initial_features: int                The number of features per example.            nr_units: int                The number of units for every layer.            nr_layers: int                The number of layers for the neural network.            use_learning_curve: bool                If the learning curve should be use in the network.            kernel_size: int                The size of the kernel that is applied in the cnn layer.            nr_filters: int                The number of filters that are used in the cnn layers.            nr_cnn_layers: int                The number of cnn layers to be used.        \"\"\"        super(ConditionedPowerLaw, self).__init__()        self.use_learning_curve = use_learning_curve        self.kernel_size = kernel_size        self.nr_filters = nr_filters        self.nr_cnn_layers = nr_cnn_layers        self.act_func = torch.nn.LeakyReLU()        self.last_act_func = torch.nn.GLU()        self.tan_func = torch.nn.Tanh()        self.batch_norm = torch.nn.BatchNorm1d        layers = []        # adding one since we concatenate the features with the budget        nr_initial_features = nr_initial_features        if self.use_learning_curve:            nr_initial_features = nr_initial_features + nr_filters        layers.append(nn.Linear(nr_initial_features, nr_units))        layers.append(self.act_func)        for i in range(2, nr_layers + 1):            layers.append(nn.Linear(nr_units, nr_units))            layers.append(self.act_func)        last_layer = nn.Linear(nr_units, 3)        layers.append(last_layer)        self.layers = torch.nn.Sequential(*layers)        cnn_part = []        if use_learning_curve:            cnn_part.append(                nn.Conv1d(                    in_channels=2,                    kernel_size=(self.kernel_size,),                    out_channels=self.nr_filters,                ),            )            for i in range(1, self.nr_cnn_layers):                cnn_part.append(self.act_func)                cnn_part.append(                    nn.Conv1d(                        in_channels=self.nr_filters,                        kernel_size=(self.kernel_size,),                        out_channels=self.nr_filters,                    ),                ),            cnn_part.append(nn.AdaptiveAvgPool1d(1))        self.cnn = nn.Sequential(*cnn_part)    def forward(        self,        x: torch.Tensor,        predict_budgets: torch.Tensor,        evaluated_budgets: torch.Tensor,        learning_curves: torch.Tensor,    ):        \"\"\"        Args:            x: torch.Tensor                The examples.            predict_budgets: torch.Tensor                The budgets for which the performance will be predicted for the                hyperparameter configurations.            evaluated_budgets: torch.Tensor                The budgets for which the hyperparameter configurations have been                evaluated so far.            learning_curves: torch.Tensor                The learning curves for the hyperparameter configurations.        \"\"\"        #x = torch.cat((x, torch.unsqueeze(evaluated_budgets, 1)), dim=1)        if self.use_learning_curve:            lc_features = self.cnn(learning_curves)            # revert the output from the cnn into nr_rows x nr_kernels.            lc_features = torch.squeeze(lc_features, 2)            x = torch.cat((x, lc_features), dim=1)        x = self.layers(x)        alphas = x[:, 0]        betas = x[:, 1]        gammas = x[:, 2]        output = torch.add(            alphas,            torch.mul(                self.last_act_func(torch.cat((betas, betas))),                torch.pow(                    predict_budgets,                    torch.mul(self.last_act_func(torch.cat((gammas, gammas))), -1)                )            ),        )        return output",
        "experimental_info": "Ensemble size: 5. Surrogate model architecture: ConditionedPowerLaw with 2-layer feedforward networks, 128 units per layer. Activation functions: Leaky ReLU for hidden layers, GLU for beta and gamma output units. Training epochs: 250 for initial training, 20 epochs for refinement per HPO iteration. Loss function: L1 loss. Optimizer: Adam with a learning rate of 0.001. Batch size: 64 for both initial training and refinement. Acquisition function: Expected Improvement (EI). Multi-fidelity strategy: Incremental budget increase by 1 epoch. Initialization: 1 configuration evaluated for 1 epoch. Restart mechanism: Models are retrained from scratch if the incumbent value does not improve for `max_benchmark_epochs + 0.2 * max_benchmark_epochs` iterations."
      }
    },
    {
      "title": "Hyperparameter Optimization through Neural Network Partitioning",
      "abstract": "Well-tuned hyperparameters are crucial for obtaining good generalization\nbehavior in neural networks. They can enforce appropriate inductive biases,\nregularize the model and improve performance -- especially in the presence of\nlimited data. In this work, we propose a simple and efficient way for\noptimizing hyperparameters inspired by the marginal likelihood, an optimization\nobjective that requires no validation data. Our method partitions the training\ndata and a neural network model into $K$ data shards and parameter partitions,\nrespectively. Each partition is associated with and optimized only on specific\ndata shards. Combining these partitions into subnetworks allows us to define\nthe ``out-of-training-sample\" loss of a subnetwork, i.e., the loss on data\nshards unseen by the subnetwork, as the objective for hyperparameter\noptimization. We demonstrate that we can apply this objective to optimize a\nvariety of different hyperparameters in a single training run while being\nsignificantly computationally cheaper than alternative methods aiming to\noptimize the marginal likelihood for neural networks. Lastly, we also focus on\noptimizing hyperparameters in federated learning, where retraining and\ncross-validation are particularly challenging.",
      "full_text": "Published as a conference paper at ICLR 2023 HYPERPARAMETER OPTIMIZATION THROUGH NEURAL NETWORK PARTITIONING Bruno Mlodozeniec†∗, Matthias Reisser‡, Christos Louizos‡ †University of Cambridge, ‡Qualcomm AI Research bkm28@cam.ac.uk, {mreisser,clouizos}@qti.qualcomm.com ABSTRACT Well-tuned hyperparameters are crucial for obtaining good generalization behavior in neural networks. They can enforce appropriate inductive biases, regularize the model and improve performance — especially in the presence of limited data. In this work, we propose a simple and efﬁcient way for optimizing hyperparameters inspired by the marginal likelihood, an optimization objective that requires no validation data. Our method partitions the training data and a neural network model into K data shards and parameter partitions, respectively. Each partition is associated with and optimized only on speciﬁc data shards. Combining these partitions into subnetworks allows us to deﬁne the “out-of-training-sample” loss of a subnetwork, i.e., the loss on data shards unseen by the subnetwork, as the objective for hyperparameter optimization. We demonstrate that we can apply this objective to optimize a variety of different hyperparameters in a single training run while being signiﬁcantly computationally cheaper than alternative methods aiming to optimize the marginal likelihood for neural networks. Lastly, we also focus on optimizing hyperparameters in federated learning, where retraining and cross-validation are particularly challenging. 1 I NTRODUCTION Due to their remarkable generalization capabilities, deep neural networks have become the de-facto models for a wide range of complex tasks. Combining large models, large-enough datasets, and sufﬁcient computing capabilities enable researchers to train powerful models through gradient descent. Regardless of the data regime, however, the choice of hyperparameters — such as neural architecture, data augmentation strategies, regularization, or which optimizer to choose — plays a crucial role in the ﬁnal model’s generalization capabilities. Hyperparameters allow encoding good inductive biases that effectively constrain the models’ hypothesis space (e.g., convolutions for vision tasks), speed up learning, or prevent overﬁtting in the case of limited data. Whereas gradient descent enables the tuning of model parameters, accessing hyperparameter gradients is more complicated. The traditional and general way to optimize hyperparameters operates as follows; 1) partition the dataset into training and validation data1, 2) pick a set of hyperparameters and optimize the model on the training data, 3) measure the performance of the model on the validation data and ﬁnally 4) use the validation metric as a way to score models or perform search over the space of hyperparameters. This approach inherently requires training multiple models and consequently requires spending resources on models that will be discarded. Furthermore, traditional tuning requires a validation set since optimizing the hyperparameters on the training set alone cannot identify the right inductive biases. A canonical example is data augmentations — they are not expected to improve training set performance, but they greatly help with generalization. In the low data regime, deﬁning a validation set that cannot be used for tuning model parameters is undesirable. Picking the right amount of validation data is a hyperparameter in itself. The conventional rule of thumb to use ∼10% of all data can result in signiﬁcant overﬁtting, as pointed out by Lorraine et al. (2019) , when one has a sufﬁciently large number of hyperparameters to tune. Furthermore, a validation set can be challenging ∗Work done while at Qualcomm AI Research. Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc. and/or its subsidiaries. 1a third partition, the test or holdout set is used to estimate the ﬁnal model performance 1 arXiv:2304.14766v1  [cs.LG]  28 Apr 2023Published as a conference paper at ICLR 2023 to obtain in many use cases. An example is Federated Learning (FL) (McMahan et al., 2017), which we speciﬁcally consider in our experimental section. In FL, each extra training run (for,e.g., a speciﬁc hyperparameter setting) comes with additional, non-trivial costs. Different approaches have been proposed in order to address these challenges. Some schemes optimize hyperparameters during a single training run by making the hyperparameters part of the model (e.g., learning dropout rates with concrete dropout (Gal et al., 2017), learning architectures with DARTs (Liu et al., 2018) and learning data-augmentations with schemes as in Benton et al. (2020); van der Wilk et al. (2018)). In cases where the model does not depend on the hyperparameters directly but only indirectly through their effect on the value of the ﬁnal parameters (through optimization), schemes for differentiating through the training procedures have been proposed, such as Lorraine et al. (2019). Another way of optimizing hyperparameters without a validation set is through the canonical view on model selection (and hence hyperparameter optimization) through the Bayesian lens; the concept of optimizing the marginal likelihood. For deep neural networks, however, the marginal likelihood is difﬁcult to compute. Prior works have therefore developed various approximations for its use in deep learning models and used those to optimize hyperparameters in deep learning, such as those of data augmentation (Schw¨obel et al., 2021; Immer et al., 2022). Still, however, these come at a signiﬁcant added computational expense and do not scale to larger deep learning problems. This paper presents a novel approach to hyperparameter optimization, inspired by the marginal likelihood, that only requires a single training run and no validation set. Our method is more scalable than previous works that rely on marginal likelihood and Laplace approximations (which require computing or inverting a Hessian (Immer et al., 2021)) and is broadly applicable to any hierarchical modelling setup. 2 M ARGINAL LIKELIHOOD AND PRIOR WORK In Bayesian inference, the rules of probability dictate how any unknown, such as parameters w or hyperparameters ψ, should be determined given observed data D. Let p(w) be a prior over w and p(D|w,ψ) be a likelihood for Dwith ψbeing the hyperparameters. We are then interested in the posterior given the data p(w|D,ψ) =p(D|w,ψ)p(w)/p(D|ψ). The denominator term p(D|ψ) is known as the marginal likelihood, as it measures the probability of observing the data given ψ, irrespective of the value of w: p(D|ψ) = ∫ p(w)p(D|w,ψ)dw. Marginal likelihood has many desirable properties that make it a good criterion for model selection and hyperparameter optimization. It intuitively implements the essence of Occam’s Razor principle (MacKay, 2003, § 28). In the PAC-Bayesian literature, it has been shown that higher marginal likelihood gives tighter frequentist upper bounds on the generalization performance of a given model class (McAllester, 1998; Germain et al., 2016). It also has close links to cross-validation (see section 2.1) and can be computed from the training data alone. However, computation of the marginal likelihood in deep learning models is usually prohibitively expensive and many recent works have proposed schemes to approximate the marginal likelihood for differentiable model selection (Lyle et al., 2020; Immer et al., 2021; 2022; Schw¨obel et al., 2021). 2.1 “L EARNING SPEED ” PERSPECTIVE Lyle et al. (2020); Fong and Holmes (2020) pointed out the correspondence between “learning speed” and marginal likelihood. Namely, the marginal likelihood of the data Dconditioned on some hyperparameters ψcan be written as: log p(D|ψ) = ∑ k log Ep(w|D1:k−1,ψ) [p(Dk|w,ψ)] ≥ ∑ k Ep(w|D1:k−1,ψ) [log p(Dk|w,ψ)] (1) where (D1,..., DC) is an arbitrary partitioning of the training dataset Dinto Cshards or chunks2, and p(w|D1:k,ψ) is the posterior over parameters of a function fw : X → Y, from the input domain Xto the target domain Yafter seeing data in shards 1 through k. The right-hand side can be interpreted as a type of cross-validation in which we ﬁx an ordering over the shards and measure the “validation” performance on each shardDk using a model trained on the preceding shards D1:k−1. 2We use the terms “chunk” and “shard” interchangeably. 2Published as a conference paper at ICLR 2023 Alternatively, it can be viewed as the learning speed of a (probabilistic) model: i.e., a measure of how quickly it learns to perform well on new shards of data after only having been ﬁt to the previous shards (through exact Bayesian updating). This perspective neatly illustrates why models with higher marginal likelihood can exhibit good inductive biases, e.g., encoded through ψ, w and fw. Namely, such models can be expected to learn faster and generalize better after seeing fewer samples. For example, if the hypothesis space is constrained3to functions satisfying symmetries present in the data, we need fewer data to identify the correct function (Sokolic et al., 2017; Sannai et al., 2021). We argue that the “learning speed” aspect of marginal likelihood — i.e., measuring how well the model generalizes to new data in the training set, having been trained only on the previous data points — is the key property making marginal likelihood a useful tool for selecting hyperparameters. 2.2 T RAINING SPEED FOR HYPERPARAMETER OPTIMIZATION Computing the “learning speed”, requires samples from the posteriorp(w|D1:k,ψ). Unfortunately, in deep learning settings, such samples are impractical to obtain; thus, prior works have focused on more scalable alternatives. Lyle et al. (2020) propose to approximate the objective in Eq. 1 by looking at the training speed during standard training of a neural network by SGD. Speciﬁcally, they deﬁne the training speed as the reduction in the training loss after a single SGD parameter update, summed over all updates in the ﬁrst epoch. They argue that, during the ﬁrst epoch of training, after the neural network parameters, w, have been updated with SGD steps using data from shards D1:k, they can be approximately used in place of the sample from the posterior p(w|D1:k,ψ) in Eq. 1. They extend the analogy to training past one epoch and use the training speed estimate for model selection (Ru et al., 2021). As pointed out by the authors, however, the analogy between learning speed and training speed somewhat breaks down after 1 epoch of training. The network parameters have “seen” every datapoint in the training set after1 epoch, and hence the connection to measuring the model’s generalization capability is weakened. For the sake of scalability and alignment with deep learning practice, we also focus on simple pointwise approximations qk(w) = δ(w = ˆwk) to the posteriors p(w|D1:k,ψ). However, in contrast to prior work, we explicitly parametrize the learning procedure such that, at any given training iteration, we have access to a model that is trained only on a subset of the dataD1:k. In doing so, we can approximate the objective in Eq. 1, and thus use it to optimize the hyperparameters during the entire training run. 3 P ARTITIONED NEURAL NETWORKS Our goal is to optimize the objective LML (D,ψ) = C∑ k=1 Eqk−1(w) [log p(Dk|w,ψ)] (2) wrt. ψ, which is an approximation to the lower-bound presented in Eq. 1 above. In Appendix A, we show that the left-hand side is also a lower-bound on the marginal likelihood under some unobtrusive conditions. As mentioned in Section 2.2, our goal is to propose an architecture and a training scheme so that we can easily obtain models trained on only subsets of the data D1:k for all k throughout training. We propose that each {qk(w)}C k=1 optimizes a subset of the parameters of the neural network, in a manner that allows us to extract “subnetworks” from the main network that have been trained on speciﬁc chunks of data. We describe the partitioning scheme below. Partitioning the parameters Denote the concatenations of the weights of a neural networkw ∈RN. We can deﬁne a partitioning ((w1,..., wC),P) of the parameters into C partitions, such that w = Pconcat(w1,..., wC) for a permutation matrix P ∈{0,1}N×N. For ease of exposition, we drop the dependence on P, assuming that w is already arranged such that P is identity, P = IN×N. Given the partitioning (w1,..., wC) of the parameters, we then specify Csubnetworks with weights w(1) s ,..., w(C) s such that w(k) s = concat(w1,..., wk, ˆwk+1,..., ˆwC), where ˆwi are some default 3or if the learning algorithm is heavily biased towards returning hypotheses that satisfy a given invariance, e.g., through the use of a prior. 3Published as a conference paper at ICLR 2023 values not optimized during training4. More speciﬁcally, the k-th subnetwork, wk s, retains the ﬁrst kpartitions from the weight partitioning and sets the remaining parameters to ˆwk+1:C. Note that, if each wk is only updated on chunks D1:k, the subnetwork w(k) s is only comprised of weights that have been updated on D1:k. Thus, we can view the parameters of w(k) s as an approximation to qk(w). Although, given that a subset of the parameters in each w(k) s is ﬁxed, this would likely be a poor approximation to the true posterior over the weights given D1:k, it could be, intuitively, a reasonable approximation in function space5. Partitioned training Having partitioned the dataset Dinto Cchunks (D1,..., Dk), we update each partition wk by optimising the negative log-likelihood6on chunks D1:k using subnetwork w(k) s by computing the following gradients: ∇wkL ( D1:k,w(k) s ) = ∑ (x,y)∈D1:k ∇wk log p ( y ⏐⏐⏐x; w(k) s ,ψ ) . (3) We interleave stochastic gradient updates of each partition of the weights with updating the hyperpa- rameters ψusing LML in Eq. 2: ∇ψLML (D,ψ) ≈ C∑ k=2 ∑ (x,y)∈Dk ∇ψlog p ( y ⏐⏐⏐x,w(k−1) s ,ψ ) . (4) This can be seen as the sum of the out-of-sample losses for each subnetwork w(k) s . The scheme is illustrated in Figure 1. For details of how the updates are scheduled in our experiments, see Appendix I. Note that, while we could incorporate the gradient of the ﬁrst term from Eq. 1 corresponding to Eq0(w)[log p(D1|w,ψ)] in Eq. 4, we chose to leave it out. Hence, the gradient of Eq. 4 is of an estimate that can be viewed as an approximation to the conditional marginal likelihood log p(D2:C|D1,ψ). Conditional marginal likelihood has been shown to have many desirable properties for model selection and, in many cases, can be a better proxy for generalization (Lotﬁ et al., 2022). Weights: w = (w1,w2,w3) Alternate: Optimize parameters: log p ( D1 |(w1, ˆw2, ˆw3)   Subnet. 1 ,ψ ) w.r.t. w1 log p ( D1:2|(w1,w2, ˆw3)   Subnet. 2 ,ψ ) w.r.t. w2 log p ( D1:3|(w1,w2,w3)   Subnet. 3 ,ψ ) w.r.t. w3 Optimize hyper parameters ψon: log p ( D2|(w1, ˆw2, ˆw3)   Subnet. 1 ,ψ ) + logp ( D3|(w1,w2, ˆw3)   Subnet. 2 ,ψ ) Figure 1: Best viewed in colour. Illustration of the partitioning scheme for a single hidden layer perceptron with C = 3chunks. This procedure, inspired by the marginal likelihood, has several desirable properties compared to prior work. 1) Our objective is computationally efﬁcient, with a computational cost roughly corresponding to evaluating subnetworks on the training set. There is no need to compute nor invert a Hessian with 4e.g., ˆwi could be the value of the weights at initialization, or ˆwi = 0 corresponding to pruning those parameters and obtaining a proper subnetwork. 5Since a) the mapping from parameters to functions is not bijective and b) neural networks are highly overparameterised and can be heavily pruned while retaining performance (Frankle and Carbin, 2018), obtaining a good ﬁt to a subset of the training data with a subset of the model parameters should be possible. Furthermore, “scaling laws” indicate that the beneﬁt of having more parameters becomes apparent mostly for larger dataset sizes (Kaplan et al., 2020), thus it is reasonable for subnetworks ﬁt to more data to have more learnable parameters. 6Optionally with an added negative log-prior regularization term log p(w(k) s ). 4Published as a conference paper at ICLR 2023 respect to the weights, as in the Laplace approximation (Immer et al., 2021; 2022). 2) Our objective is readily amenable to optimization by stochastic gradient descent; we do not have to iterate over the entire training set to compute a single gradient update for the hyperparameters. 3) Compared to the training speed objective (Lyle et al., 2020), in our method, the training of the weights in each subnetwork progresses independently of the data in future chunks. Hence, it can be seen as more truthfully measuring the generalization capability of a model using a given set of hyperparameters. Partitioning Schemes There are several ways in which the neural network weights can be partitioned. In our experiments in Section 5, we partition the weights before beginning training by assigning a ﬁxed proportion of weights in each layer to a given partition at random. For each subnetwork, for the weight partitions corresponding to future chunks, we use the values of the weights at initialisation. For a discussion of partitioning schemes, see Appendix C. 4 R ELATED WORKS Hyperparameter optimization in deep learning Many works have tackled the challenge of op- timizing hyperparameters in deep learning. Works on implicit differentiation, such as the one by Lorraine et al. (2019), allow for optimizing training hyperparameters such as the learning rate, weight- decay, or other hyperparameters that affect the ﬁnal neural network weights only through the training routine. Other works have proposed ways to parameterize and optimize data-augmentations (Cubuk et al., 2018; Li et al., 2020), search-spaces for neural network architectures, as well as methods to optimize architectures using gradient-based optimization (Liu et al., 2018; Elsken et al., 2019). All of the above works have primarily relied on optimizing hyperparameters on a separate validation set and are compatible with the objective deﬁned in this work. Several works have also aimed to cast learning data augmentations as an invariance learning problem. They do so by parameterizing the model itself with data augmentations, and frame invariance learning as a model selection problem (van der Wilk et al., 2018; Benton et al., 2020; Schw¨obel et al., 2021; Nabarro et al., 2022; Immer et al., 2022). We compare against Benton et al. (2020) (“Augerino”) and Immer et al. (2022) (“Differentiable Laplace”) on this task in the experimental section. Hyperparameter optimization without a validation set A limited number of works consider learning hyperparameters without a validation set in a deep learning context. Benton et al. (2020) propose a simple method for learning invariances without a validation set by regularising invariance hyperparameters to those resulting in higher invariance. They show that the invariances found tend to be insensitive to the regularisation strength, determined by another hyperparameter. However, the method relies on being able to a priori deﬁne which hyperparameters lead to higher invariance through a suitable regularisation function. In more complex invariance learning settings, deﬁning the regulariser can be challenging. For example, if data-augmentation transformations were to be parameterized by a neural network (as proposed in Lorraine et al. (2019)), it is non-trivial to devise an adequate regulariser. We show that our method can be applied to such settings. Other works focus on deriving tractable approximations to the marginal likelihood for deep neural networks. Schw ¨obel et al. (2021) propose only marginalising-out the parameters in the last layer of the neural network by switching it out for a Gaussian Process. They treat the preceding layer effectively as a hyperparameter, and optimize invariance parameters using the marginal likelihood. Although they show promising results on MNIST, they found they “were unable to learn invariances for CIFAR-10” (Schw¨obel et al., 2021, §7) and highlighted the need to marginalise lower layers as well. In contrast, our objective can be seen as being inspired by marginal likelihood where arbitrary network layers can be “marginalised”, and works on datasets like CIFAR-10. Immer et al. (2022) have adapted the Laplace approximation (Immer et al., 2021) to make it tractable for learning data augmentations. In contrast to Schw¨obel et al. (2021), they approximately marginalize out all the network parameters, and performs favourably. Their approximation, however, requires approximations to a Hessian w.r.t. all network parameters; for that reason, their work reports results for architectures only up to a ResNet-14, whereas our method can easily scale to larger architectures. Hyperparameter optimization in FL Improving hyperparameter optimization is especially rele- vant to FL. Given the potential system level constraints (Wang et al., 2021), methods that optimize the hyperparameters and parameters in a single training run are preferred. On this note, Khodak et al. (2021) introduced FedEx and showed that it can successfully optimize the client optimizer 5Published as a conference paper at ICLR 2023 hyperparameters. FedEx relies on a training/validation split on the client level and uses a REIN- FORCE type of gradient (Williams, 1992) estimator, which usually exhibits high variance and needs baselines to reduce it (Mohamed et al., 2020). This is in contrast to partitioned networks, which use standard, low-variance backpropagation for the hyperparameters and no separate validation set per client. To optimize the other hyperparameters, Khodak et al. (2021) wrapped FedEx with a traditional hyperparameter optimization strategy, the successive halving algorithm. This is orthogonal to our method and could be applied to partitioned networks as well. In Zhou et al. (2021), the authors perform a hyperparameter search independently on each client with some off-the-shelf methods and then aggregate the results of the search at the server once in order to identify the best hyperparameter setting. The main drawback of this method compared to partitioned networks is that when the local client datasets are small, a client-speciﬁc validation set is not informative, and the aggregation happens only once. Finally, there is also the recent work from Seng et al. (2022) which performs hyperparameter optimization and neural architecture search in the federated setting. Similarly to prior works, it requires client-speciﬁc validation data in order to optimize the hyperparameters. 5 E XPERIMENTS 1 5 10 15 20 25 30 Num. inputs 0.7 0.8 0.9 1.0 Accuracy 0.5 0.4 0.3 0.2 0.1 0.0 Average Log-likelihoodPosthoc Diagonal Laplace Train T est 1 5 10 15 20 25 30 Num. inputs 103 102 Log Marginal Likelihood Estimate Partitioned (a) 0 400080001200016000200002400028000 Iteration 0 5 10 15 20 25Input Mask Element 0.00 0.25 0.50 0.75 1.00 Mask Probability  (b) Figure 2: (a) Demonstrating the ability of the marginal-likelihood inspired objective LML to identify the correct model on a toy input selection task. We plot the hyperparameter objective, train and test set accuracy, and train and test set log-likelihood with the partitioned networks method (left), and the post-hoc diagonal Laplace method (Immer et al., 2021) (right). (b) Mask over input features learned by partitioned networks over time. The ﬁrst 15 features are correctly identiﬁed. Input Selection To demonstrate that LML is a good objective for model selection that captures the desirable properties of the marginal likelihood, we ﬁrst deploy our method on the toy model selection task of Lyle et al. (2020): there the ﬁrst 15 features are informative, and the remaining15 are spurious y∼Bern (1 2 ) x = [ y+ ϵ1,...,y + ϵ15   Informative ,ϵ16,...,ϵ 30   Spurious ]⊺ ϵ1,...,ϵ 30 iid ∼N(0,1). We specify a ﬁxed mask over the inputs prior to training, where the ﬁrst Kinputs remain unmasked, and the remainder is masked. We expect that, given multiple models with different (ﬁxed) masks over the inputs, the proposed objective will be able to identify the correct one — i.e., the one that keeps only the informative features. We train multiple fully connected neural networks (MLPs) on a training set of 1000 examples using our method and compare the ﬁnal values of the LML objective. The results are shown in Figure 2a. LML correctly identiﬁes 15 input features as the optimum, and correlates well with test accuracy and log-likelihood. Training loss and training accuracy, on the other hand, cannot alone disambiguate whether to use 15 or more input features. Differentiable input selection We further show that we can learn the correct mask over the inputs in a differentiable manner using our method during a single training run. We parameterize a learnable mask over the inputs with a concrete Bernoulli distribution (Maddison et al., 2016) and treat the parameters of the mask distribution as a hyperparameter. We optimize them with respect to the proposed objective using our method. The evolution of the learned mask during training is shown in Figure 2b, where we see that we can correctly identify the ﬁrst 15 informative features. 6Published as a conference paper at ICLR 2023 Learning invariances through data-augmentations Following previous literature on learning soft invariances through learning data augmentations (Nabarro et al., 2022; van der Wilk et al., 2018; Benton et al., 2020; Schw ¨obel et al., 2021; Immer et al., 2022), we show that we can learn useful afﬁne image augmentations, resulting in gains in test accuracy. We specify afﬁne data augmentations as part of a probabilistic model as done by van der Wilk et al. (2018), averaging over multiple data augmentation samples during training and inference. This allows us to treat the data-augmentation distribution as a model hyperparameter rather than a training hyperparameter. For datasets, we consider MNIST, CIFAR10, TinyImagenet along with rotCIFAR10 and rotTinyImagenet, variants where the datapoints are randomly rotated at the beginning of training by angles sampled uniformly from [−π,π] (Immer et al., 2022). Experimental setup details are provided in Appendix I. For the CIFAR10 and rotCIFAR10 datasets, we consider as baselines standard training with no augmentations, Augerino (Benton et al., 2020) and Differentiable Laplace (Immer et al., 2022). Following Immer et al. (2022), we use ﬁxupResNets (Zhang et al., 2019) for the architectures. The results can be seen in Table 1. There, we observe that partitioned networks outperform all baselines in the case of CIFAR10 for both ResNet variants we consider. On RotCIFAR10, we observe that partitioned networks outperform the baseline and Augerino, but it is slightly outperformed by Differentiable Laplace, which optimizes additional prior hyperparameters. To demonstrate the scalability of partitioned networks, for the (rot)TinyImagenet experiments we consider a ResNet-50 architecture with GroupNorm(2). In Table 1 we observe that in both cases, partitioned networks learn invariances successfully and improve upon the baseline. Relative to Augerino, we observe that partitioned networks either improve (TinyImagenet) or are similar (rotTinyImagenet). Table 1: Test accuracy with learning afﬁne augmentations on (rot)CIFAR10 and (rot)TinyImagenet. Method Dataset Architecture Baseline Augerino Diff. Laplace Partitioned RotCIFAR10 ﬁxupResNet-8 54.2±0.4 75.4±0.2 79.5±0.6 79.1±0.0 CIFAR10 ﬁxupResNet-8 74.1±0.5 79.0±1.0 84.2±0.8 86.1±0.4 ﬁxupResNet-14 79.5±0.3 83.0±0.1 88.1±0.2 89.1±0.8 RotTinyImagenet ResNet-50 31.5±0.6 44.5±0.2 OOM7 43.9±0.3 TinyImagenet ResNet-50 44.2±0.5 41.1±0.2 OOM 48.6±0.0 Imbuing a model with useful invariances is particularly useful in the low-data regime, due to better data efﬁciency. To show that, we perform experiments where we artiﬁcially reduce the size of the training dataset. The results can be seen in Figure 3. We see that by learning augmentations with partitioned networks, we can drastically improve performance in the low-data regime upon a baseline that does not learn augmentations, while performing favorably against prior works in most cases. On MNIST, our method outperforms the last-layer marginal-likelihood method (last-layer ML) by Schw¨obel et al. (2021) in the large data regime but underperforms in the low-data regime. That is likely to be expected, as their work ﬁts a Gaussian Process (GP) at the last layer (Wilson et al., 2016), which is better tailored for the low-data regime and results into a more ﬂexible model (due to the GP corresponding to an additional, inﬁnite width, layer). Since the MNIST-CNN is sufﬁciently small to ﬁt multiple networks into memory, we also compare to a variant of our method where, instead of partitioning a single network, we train Cdifferent networks where network kis ﬁt on data D1:k. This serves as an upper bound on the performance of the partitioned networks. We see that by partitioning a single network, we can achieve almost equivalent accuracy. On CIFAR10, partitioned networks outperform all other works on all data sizes we considered. On RotCIFAR10, partitioned networks perform again favourably, but they are marginally outperformed by differentiable Laplace in the low-data regime. Compared to partitioned networks where we only optimize augmentations, differentiable Laplace also optimizes the precision of a Gaussian prior over the weights, which better combats overﬁtting in the low-data regime. On both the TinyImagenet and rotTinyImagenet experiments we observe that partitioned networks either outperform or are similar to the baselines on all data sizes considered. 7Out of memory error on a 32GB Nvidia V100. 7Published as a conference paper at ICLR 2023 5000 20000 60000 Dataset Size 0.98 0.99T est Accuracy Baseline Last-layer ML Augerino Diff. Laplace Partitioned (Ens.) Partitioned (a) MNIST 0.25 0.50 0.75 1 5 10 20 50 Dataset Size (x1000) 0.25 0.50 0.75  (b) (rot)CIFAR10 0.25 0.50 10 50 100 Dataset Size (x1000) 0.2 0.4  (c) (rot)TinyImagenet Figure 3: Learning afﬁne data augmentations on subsets of data. (b) uses a ﬁxupResNet-8 architecture whereas (c) a ResNet-50 architecture. (b,c) Top: normal dataset, bottom: rotated dataset. Comparisons to traditional training / validation split We further perform comparisons between partitioned networks and the more traditional training/validation split (denoted as validation set optimization) with additional ﬁnetuning to the task of learning data augmentations. This is realized as follows; we partition 20kCIFAR10 examples into training and validation data of speciﬁc proportions. We then either train a partitioned network (along with the hyperparameters on LML) on these two chunks of data or train a standard network on the training set while using the validation set loss to obtain gradients for the data augmentation hyperparameters. For the validation set optimization baseline, once the hyperparameters are optimized, the resulting network is ﬁnetuned on the whole dataset for 20 epochs. The results for varying chunk proportions are provided in Table 2. Table 2: Learning afﬁne augmentations with ﬁxupResNet-14 on subset of CIFAR-10 (20kexamples). NaN denotes that a run crashed. Chunk Proportions Method [0.3,0.7] [0 .5,0.5] [0 .7,0.3] [0 .8,0.2] [0 .9,0.1] Partitioned 82.9%±0.3 83.0%±0.01 83.7%±0.2 84.0%±0.6 84.6%±0.05 Validation set optim. NaN 78.9%±0.04 81.5%±0.2 82.6%±0.1 83.4%±0.1 +Finetune NaN 81.3%±0.09 82.5%±0.2 83.5%±0.1 83.8%±0.3 Table 3: Learning a feature extractor (ﬁrst 2 out of 3 stages of a Wide ResNet-20) as a hyperparameter on CIFAR10. Method Chunk Proportions Test accuracy Validation set optim. [0.9,0.1] 59 .6%±0.6 Partitioned [0.1,0.8,0.1] 87.3%±0.8 We can see that partitioned net- works (that do not employ ad- ditional ﬁnetuning) outperform validation set optimization with ﬁnetuning in all settings we tried. The gap does get smaller when we move to the more tra- ditional 90/10 splits for train- ing/validation: a 10% proportion for validation data is enough to optimize a handful of hyper- parameters (just 6 scalars). To corroborate this claim, we set up an additional experiment; we use a Wide ResNet-20 on the full CIFAR10 dataset, where the ﬁrst two out of the three stages (13 convolution layers) are considered as hyperparameters. The results for this setting can be seen in Table 3. We see that 10% validation data are not enough, and the validation set optimization baseline performs poorly. This is in contrast to partitioned networks, where with three chunks, we can learn all of these hyperparameters successfully. Note that, compared to Augerino, applying partitioned networks to this setting is straightforward. To apply Augerino, one would have to come up with a metric that can be used to regularize the feature extractor towards “higher invariance”. Partitioned networks for federated learning We consider federated learning (FL) (McMahan et al., 2017), a setting where data is distributed across many clients. In this setting, there are system properties that make hyperparameter optimization especially challenging (Wang et al., 2021). More speciﬁcally, obtaining a validation set and performing multiple training runs with different 8Published as a conference paper at ICLR 2023 hyperparameter settings might not be possible due to the additional communication and computation costs, and transient client availability (clients join and leave the training process at any time). Optimizing hyperparameters together with the model parameters in a single run is therefore especially beneﬁcial (Wang et al., 2021), and partitioned networks are a good ﬁt for FL. We extend our centralized experimental setup to FL by splitting all N clients into Cnon-overlapping chunks, such that each chunk is understood as the union of all clients’ data shards that belong to that chunk. During federated training, a client belonging to chunk ksequentially optimizes partitions wk:C through sub-networks w(k:C) s and computes a gradient wrt. the hyperparameters ψ. Note that partitions w1:k remain unchanged and do not need to be communicated back to the server. This reduction in upload costs is a welcome property for FL, where upload costs can bottleneck system design. The server receives the (hyper-) parameter updates, averages them, and applies the result as a “gradient” to the server-side model in the traditional federated manner (Reddi et al., 2020). For partitioned networks, the hyperparameters that we optimize are the data augmentation parameters and, since we also include dropout in these architectures, the dropout rates (with the concrete relaxation from Maddison et al. (2016)). As a baseline, we consider the standard federated training without learning hyperparameters (denoted as FedAvg) as well as learning the augmentation parameters with Augerino Benton et al. (2020). Please see Appendix J for a detailed explanation of our FL setup. Table 4 summarizes our results using different sub-sets and variations of MNIST and CIFAR10, where we also included rotMNIST Larochelle et al. (2007) as another dataset. We can see that partitioned networks allow training models that generalize better than both FedAvg and FedAvg with Augerino, at reduced communication costs. Especially when the true data-generating process and underlying source of non-i.i.d.-ness are explicitly accounted for — here in the form of rotation — the beneﬁts of learning the augmentations with partitioned networks become apparent. For example, we observe that on the rotated datasets, partitioned networks learn to correctly increase the rotation angle. Table 4: Validation accuracy averaged over the last10 evaluations, each 10 rounds apart; standard- error is computed across 4 random seeds. All datasets are adapted to the federated setting and are synthetically split to be non-i.i.d. sampled as described in Appendix J.2. Dataset & size ↑MNIST ↑RotMNIST ↓Upload Method 1.25k 5k 50k 1.25k 5k 50k [%] FedAvg 95.4%±0.1 97.4%±0.1 99.0%±0.1 80.5%±0.0 90.4%±0.5 96.8%±0.1 100 FedAvg + Augerino 94.2%±0.5 96.4%±0.1 99.1%±0.0 79.5%±0.3 89.0%±2.0 95.3%±0.2 100 FedAvg + Partitioned97.0%±0.1 98.3%±0.0 99.2%±0.1 85.7%±0.9 93.5%±0.6 97.8%±0.1 77 ↑CIFAR10 ↑RotCIFAR10 ↓Upload 1.25k 5k 45k 1.25k 5k 45k [%] FedAvg 50.2%±0.4 64.5%±0.3 79.2%±0.7 35.6%±0.3 45.2%±0.1 53.9%±1.1 100 FedAvg + Augerino 49.9%±0.8 65.0%±0.2 79.9%±0.4 36.1%±0.2 45.0%±0.2 56.4%±0.7 100 FedAvg + Partitioned50.8%±1.0 64.8%±0.4 81.5%±0.5 37.1%±0.2 45.3%±0.3 60.6%±0.2 91 6 D ISCUSSION We propose partitioned networks as a new method for hyperparameter optimization inspired by the marginal likelihood objective. It provides a general and scalable solution to ﬁnding hyperparameters in a single training run without requiring access to a validation set while introducing less additional overhead to the training task than existing approaches. We showed that partitioned networks are applicable on a wide range of tasks; they can identify the correct model on illustrative toy examples, they can learn data augmentations in a way that improves data efﬁciency, they can optimize general feature extractors as hyperparameters and they can also optimize dropout rates. In the federated setting, partitioned networks allow us to overcome practical challenges, reduce the communication overhead and obtain better models. The notion of partitioned networks we propose in this work is novel to the literature and an orthogonal approach to many existing hyperparameter tuning algorithms. Like any other method, partitioned networks come with their own limitations, e.g., needing a partitioning strategy. We expand upon them in appendix H. We hope to see our method successfully reducing the need to perform hyperparameter search through repeated training and thereby contribute to the community’s effort to reduce its carbon footprint. 9Published as a conference paper at ICLR 2023 REFERENCES Gregory Benton, Marc Finzi, and Andrew G Wilson. Augerino, github, com- mit=fd542eb90ac6b1c0959156c1f6ad2ba8719d8572. https://github.com/g-benton/ learning-invariances/. (on page 18) Gregory Benton, Marc Finzi, Pavel Izmailov, and Andrew G Wilson. Learning invariances in neural networks from training data. Advances in neural information processing systems, 33:17605–17616, 2020. (on page 2, 5, 7, 9, 16, 18, 20, 24, 25) Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment: Learning augmentation policies from data. arXiv preprint arXiv:1805.09501, 2018. (on page 5) Kamal Dys. Cifar10 resnet: 90+% accuracy;less than 5 min. https://www.kaggle.com/code/ kmldas/cifar10-resnet-90-accuracy-less-than-5-min . Accessed: 2022-09- 17. (on page 26) Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural architecture search: A survey. The Journal of Machine Learning Research, 20(1):1997–2017, 2019. (on page 5) Edwin Fong and Chris C Holmes. On the marginal likelihood and cross-validation. Biometrika, 107 (2):489–496, 2020. (on page 2) Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. arXiv preprint arXiv:1803.03635, 2018. (on page 4) Yarin Gal, Jiri Hron, and Alex Kendall. Concrete dropout.Advances in neural information processing systems, 30, 2017. (on page 2) Pascal Germain, Francis Bach, Alexandre Lacoste, and Simon Lacoste-Julien. Pac-bayesian theory meets bayesian inference. Advances in Neural Information Processing Systems, 29, 2016. (on page 2) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectiﬁers: Surpassing human-level performance on imagenet classiﬁcation. In Proceedings of the IEEE international conference on computer vision, pages 1026–1034, 2015. (on page 23) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In European conference on computer vision , pages 630–645. Springer, 2016. (on page 23) Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016. (on page 23) Alexander Immer and Tycho F. A. van der Ouderaa. Learning invariances with laplace ap- proximations (lila), github, commit=c0c4a09a109ed2f55e887def7d854b8a3a2330ef. https: //github.com/tychovdo/lila. (on page 17) Alexander Immer, Matthias Bauer, Vincent Fortuin, Gunnar R¨atsch, and Khan Mohammad Emtiyaz. Scalable marginal likelihood estimation for model selection in deep learning. In International Conference on Machine Learning, pages 4563–4573. PMLR, 2021. (on page 2, 5, 6, 24) Alexander Immer, Tycho F. A. van der Ouderaa, Gunnar R¨atsch, Vincent Fortuin, and Mark van der Wilk. Invariance learning in deep neural networks with differentiable laplace approximations, 2022. URL https://arxiv.org/abs/2202.10638. (on page 2, 5, 7, 15, 16, 17, 18, 22, 23, 24, 25) Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning, pages 448–456. PMLR, 2015. (on page 23) Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. (on page 4) 10Published as a conference paper at ICLR 2023 Mikhail Khodak, Renbo Tu, Tian Li, Liam Li, Maria-Florina F Balcan, Virginia Smith, and Ameet Talwalkar. Federated hyperparameter tuning: Challenges, baselines, and connections to weight- sharing. Advances in Neural Information Processing Systems, 34:19184–19197, 2021. (on page 5, 6) Hugo Larochelle, Dumitru Erhan, Aaron Courville, James Bergstra, and Yoshua Bengio. An empirical evaluation of deep architectures on problems with many factors of variation. In Proceedings of the 24th international conference on Machine learning, pages 473–480, 2007. (on page 9) Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge. CS 231N, 7(7):3, 2015. (on page 24) Yonggang Li, Guosheng Hu, Yongtao Wang, Timothy Hospedales, Neil M Robertson, and Yongxin Yang. Dada: Differentiable automatic data augmentation. arXiv preprint arXiv:2003.03780, 2020. (on page 5) Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. arXiv preprint arXiv:1806.09055, 2018. (on page 2, 5) Jonathan Lorraine, Paul Vicol, and David Duvenaud. Optimizing millions of hyperparameters by implicit differentiation. CoRR, abs/1911.02590, 2019. URL http://arxiv.org/abs/ 1911.02590. (on page 1, 2, 5) Sanae Lotﬁ, Pavel Izmailov, Gregory Benton, Micah Goldblum, and Andrew Gordon Wilson. Bayesian model selection, the marginal likelihood, and generalization. In Kamalika Chaud- huri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine Learning , volume 162 of Pro- ceedings of Machine Learning Research , pages 14223–14247. PMLR, 17–23 Jul 2022. URL https://proceedings.mlr.press/v162/lotfi22a.html. (on page 4) Clare Lyle, Lisa Schut, Robin Ru, Yarin Gal, and Mark van der Wilk. A bayesian perspective on training speed and model selection. Advances in Neural Information Processing Systems , 33: 10396–10408, 2020. (on page 2, 3, 5, 6) David JC MacKay. Information theory, inference and learning algorithms. Cambridge university press, 2003. (on page 2) Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous relaxation of discrete random variables. arXiv preprint arXiv:1611.00712, 2016. (on page 6, 9, 26, 27) David A McAllester. Some pac-bayesian theorems. In Proceedings of the eleventh annual conference on Computational learning theory, pages 230–234, 1998. (on page 2) Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efﬁcient learning of deep networks from decentralized data. In Artiﬁcial intelli- gence and statistics, pages 1273–1282. PMLR, 2017. (on page 2, 8) Shakir Mohamed, Mihaela Rosca, Michael Figurnov, and Andriy Mnih. Monte carlo gradient estimation in machine learning. J. Mach. Learn. Res., 21(132):1–62, 2020. (on page 6) Seth Nabarro, Stoil Ganev, Adri`a Garriga-Alonso, Vincent Fortuin, Mark van der Wilk, and Laurence Aitchison. Data augmentation in bayesian neural networks and the cold posterior effect. In Uncertainty in Artiﬁcial Intelligence, pages 1434–1444. PMLR, 2022. (on page 5, 7) Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and R. Gar- nett, editors, Advances in Neural Information Processing Systems 32, pages 8024–8035. Curran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/9015-pytorch-an- imperative-style-high-performance-deep-learning-library .pdf. (on page 22) 11Published as a conference paper at ICLR 2023 Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Kone ˇcn`y, Sanjiv Kumar, and H Brendan McMahan. Adaptive federated optimization. arXiv preprint arXiv:2003.00295, 2020. (on page 9, 26, 27) Robin Ru, Clare Lyle, Lisa Schut, Miroslav Fil, Mark van der Wilk, and Yarin Gal. Speedy performance estimation for neural architecture search. Advances in Neural Information Processing Systems, 34:4079–4092, 2021. (on page 3) Akiyoshi Sannai, Masaaki Imaizumi, and Makoto Kawano. Improved generalization bounds of group invariant/equivariant deep networks via quotient feature spaces. In Uncertainty in Artiﬁcial Intelligence, pages 771–780. PMLR, 2021. (on page 3) Pola Schw¨obel, Martin Jørgensen, Sebastian W. Ober, and Mark van der Wilk. Last layer marginal likelihood for invariance learning, 2021. URL https://arxiv.org/abs/2106.07512. (on page 2, 5, 7, 15, 16, 23, 24, 26, 27) Jonas Seng, Pooja Prasad, Devendra Singh Dhami, and Kristian Kersting. Hanf: Hyperparameter and neural architecture search in federated learning. arXiv preprint arXiv:2206.12342, 2022. (on page 6) Jure Sokolic, Raja Giryes, Guillermo Sapiro, and Miguel Rodrigues. Generalization error of invariant classiﬁers. In Artiﬁcial Intelligence and Statistics, pages 1094–1103. PMLR, 2017. (on page 3) Mark van der Wilk, Matthias Bauer, ST John, and James Hensman. Learning invariances using the marginal likelihood. Advances in Neural Information Processing Systems, 31, 2018. (on page 2, 5, 7, 16) Bastiaan S Veeling, Jasper Linmans, Jim Winkens, Taco Cohen, and Max Welling. Rotation equiv- ariant cnns for digital pathology. In International Conference on Medical image computing and computer-assisted intervention, pages 210–218. Springer, 2018. (on page 15) Jianyu Wang, Zachary Charles, Zheng Xu, Gauri Joshi, H Brendan McMahan, Maruan Al-Shedivat, Galen Andrew, Salman Avestimehr, Katharine Daly, Deepesh Data, et al. A ﬁeld guide to federated optimization. arXiv preprint arXiv:2107.06917, 2021. (on page 5, 8, 9) Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3):229–256, 1992. (on page 6) Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, and Eric P Xing. Deep kernel learning. In Artiﬁcial intelligence and statistics, pages 370–378. PMLR, 2016. (on page 7) Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European conference on computer vision (ECCV), pages 3–19, 2018. (on page 23, 26) Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016. (on page 23) Hongyi Zhang, Yann N Dauphin, and Tengyu Ma. Fixup initialization: Residual learning without normalization. arXiv preprint arXiv:1901.09321, 2019. (on page 7, 18, 23) Yi Zhou, Parikshit Ram, Theodoros Salonidis, Nathalie Baracaldo, Horst Samulowitz, and Heiko Ludwig. Flora: Single-shot hyper-parameter optimization for federated learning. arXiv preprint arXiv:2112.08524, 2021. (on page 6) 12Published as a conference paper at ICLR 2023 A LML IS A LOWER -BOUND TO THE MARGINAL LIKELIHOOD In this section, we show that the objective in equation 2 is a lower-bound on the marginal likelihood, under a mild assumption on each approximate posterior qk(w). The aim is to approximate: log p(D|ψ) = C∑ k=1 log p(Dk|D1:k−1,ψ) (5) Our partitioned approximation is given by: C∑ k=1 Eqk−1(w) [log p(Dk|w,ψ)] (6) We can get the equation for the gap between quantities in 5 and 6: gap = C∑ k=1 log p(Dk|D1:k−1,ψ) − C∑ k=1 Eqk−1(w) [log p(Dk|w,ψ)] (7) = C∑ k=1 Eqk−1(w) [log p(Dk|D1:k−1,ψ) −log p(Dk|w,ψ)] (8) = C∑ k=1 Eqk−1(w) [ log p(Dk|D1:k−1,ψ) p(Dk|w,ψ) ] (9) = C∑ k=1 Eqk−1(w)  log p(w,Dk|D1:k−1)    p(w|D1:k,ψ)p(Dk|D1:k−1,ψ)p(w|D1:k−1,ψ) p(w|D1:k,ψ)p(Dk|w,ψ)p(w|D1:k−1,ψ)   p(w,Dk|D1:k−1)   (10) = C∑ k=1 Eqk−1(w) [ log p(w|D1:k−1,ψ) p(w|D1:k,ψ) ] (11) = C∑ k=1 DKL [qk−1(w)∥p(w|D1:k,ψ)] −DKL [qk−1(w)∥p(w|D1:k−1,ψ)] (12) We now make two assumptions • DKL [qk−1(w)∥p(w|D1:k,ψ)] ≥DKL [qk(w)∥p(w|D1:k,ψ)]. This is motivated from the fact that qk(w) is trained on all data chunks D1:k so it is expected to be a better approxima- tion to the posterior p(w|D1:k), compared to qk−1(w) which is only trained on D1:k−1. • DKL [qC−1(w)∥p(w|D1:C,ψ)] ≥DKL [q0(w)∥p(w)]. Since we are free to choose the approximate posterior before seeing any data — q0(w)—, we can set it to be equal to the prior p(w) which, together with the positivity of the KL divergence, trivially satisﬁes this assumption. Therefore, by rearranging Eq. 12 and using our two assumptions we have that the gap is positive gap =−DKL [q0(w)∥p(w)] +DKL [qC−1(w)∥p(w|D1:C,ψ)] + C∑ k=1 DKL [qk−1(w)∥p(w|D1:k,ψ)] −DKL [qk(w)∥p(w|D1:k,ψ)] ≥0, (13) and our approximation is a lower bound to the marginal likelihood, i.e., log p(D|ψ) ≥ C∑ k=1 Eqk−1(w) [log p(Dk|w,ψ)] . (14) 13Published as a conference paper at ICLR 2023 B P ARTITIONED NETWORKS AS A SPECIFIC APPROXIMATION TO THE MARGINAL LIKELIHOOD In this section of the appendix, we show that the partitioned neural networks we presented in the paper are a particular instance of the approximation to the marginal likelihood shown in equation 2. Consider a dataset Dcomprised of C shards, i.e. D= (D1,..., DC), along with a model, e.g., a neural network, with parameters w ∈RDw, a prior p(w) = ∏Dw j=1 N(wj|0,λ) and a likelihood p(D|w,ψ) with hyperparameters ψ. Assuming a sequence over the dataset chunks, we can write out the true marginal likelihood as log p(D|ψ) = ∑ k log p(Dk|D1:k−1,ψ) = ∑ k log Ep(w|D1:k−1,ψ) [p(Dk|w,ψ)] (15) ≥ ∑ k Ep(w|D1:k−1,ψ) [log p(Dk|w,ψ)] . (16) Since the true posteriors p(w|D1:j,ψ) for j ∈{1,...,C }are intractable, we can use variational inference to approximate them with qφj(w) for j ∈{1,...,C }, with φj being the to-be-optimized parameters of the j’th variational approximation. Based on the result from Appendix A, whenqφj(w) are optimized to match the respective posteriors p(w|D1:j,ψ), we can use them to approximate the marginal likelihood as log p(D|ψ) ≥ ∑ k Eqφk−1 (w) [log p(Dk|w,ψ)] . (17) Partitioned networks correspond to a speciﬁc choice for the sequence of approximating distribution families qφk(w). Speciﬁcally, we partition the parameter space w into Cchunks, i.e., wk ∈RDwk, such that ∑ kDwk = Dw, and we associate each parameter chunk wk with a data shard Dk. Let rφk(wk) be base variational approximations over wk with parameters φk. Each approximate distribution qφk(w) is then deﬁned in terms of these base approximations, i.e., qφk(w) =   k−1∏ j=1 rφj(wj)  rφk(wk) ( K∏ m=k+1 r0(wm) ) (18) where r0(·) is some base distribution with no free parameters. In accordance with the assumptions in appendix A, we can then ﬁt each qφk(w) by minimising the KL-divergence to p(w|D1:k,ψ) – the posterior after seeing kchunks: DKL [qφk(w)∥p(w|D1:k,ψ)] =−Eqφk(w)[log p(D1:k|w,ψ)] +DKL [qφk(w)∥p(w)] + logp(D1:k|ψ) (19) (20) Finding the optimum with respect to φk: arg min φk DKL [qφk(w)∥p(w|D1:k,ψ)] = (21) = arg min φk −Eqφk(w)[log p(D1:k|w,ψ)] +DKL [qφk(w)∥p(w)] (22) = arg min φk −Eqφk(w)[log p(D1:k|w,ψ)] + DKL     k−1∏ j=1 rφj(wj)  rφk(wk) ( K∏ m=k+1 r0(wm) ) ∥ K∏ i p(wi)   (23) = arg min φk −Eqφk(w)[log p(D1:k|w,ψ)] +DKL [rφk(wk)∥p(wk)] . (24) We can now obtain partitioned networks by assuming that rφk(wk) = N(wk|φk,νI) for k ∈ {1,...,C }, r0(w) = N(w|ˆw,νI), with ˆw being the parameters at initialization (i.e., before we 14Published as a conference paper at ICLR 2023 update them on data) and taking ν →0, i.e., in machine-precision, the weights are deterministic. As noted in Section I.1, we scale the weight-decay regularizer forφk (whenever used) differently for each partition k, such that it can be interpreted as regularization towards a prior. In the experiments where we do not regularize φk according to p(wk) when we optimize them, this implicitly corresponds to λ→∞ (i.e. the limiting behaviour when the variance of p(w) goes to inﬁnity), which makes the contribution of the regularizer negligible. C P ARTITIONING SCHEMES There are several ways in which we could aim to partition the weights of a neural network. Throughout the experimental section 5, we partition the weights by assigning a ﬁxed proportion of weights in each layer to a given partition at random. We call this approach random weight partitioning. We also experimented with other partitioning schemes. For example, we tried assigning a ﬁxed proportion of a layer’s outputs (e.g., channels in a convolution layer) to each partition. All weights in a given layer that a speciﬁc output depends on would then be assigned to that partition. We call this approach node partitioning. Both approaches are illustrated in Figure 4. One beneﬁt of the node partitioning scheme is that it makes it possible to update multiple partitions with a single batch; This is because we can make a forward pass at each linear or convolutional layer with the full network parameters w, and, instead, mask the appropriate inputs and outputs to the layer to retrieve an equivalent computation to that with w(k) s . The gradients also need to be masked on the backward pass adequately. No such simpliﬁcation is possible with the random weight partitioning scheme; if we were to compute a backward pass for a single batch of examples using different subnetworks for each example, the memory overhead would grow linearly with the number of subnetworks used. In initial experiments, we found both random weight partitioning and node partitioning performed similarly. In the experimental section 5, we focused on the former, as it’s easier to reason about with relation to e.g., dropout. Throughout this work, partitioning happens prior to initiating training, and remains ﬁxed throughout. It might also be possible to partition the network parameters dynamically during training, which we leave for future work.   w11 w12 w13 w14 w15 w21 w22 w23 w24 w25 w31 w32 w33 w34 w35 w41 w42 w43 w44 w45 w51 w52 w53 w54 w55 w61 w62 w63 w64 w65   (a) Random weight partitioned In node assignment    Out node assignment      w11 w12 w13 w14 w15 w21 w22 w23 w24 w25 w31 w32 w33 w34 w35 w41 w42 w43 w44 w45 w51 w52 w53 w54 w55 w61 w62 w63 w64 w65   (b) Node partitioned Figure 4: Figures showing how the weights within a single weight matrix W ∈R6×5 for a linear layer would be partitioned. D S CALABILITY In the paper, we claim that our method is scalable compared to Schw¨obel et al. (2021) and Immer et al. (2022). What constraints the scalability of the mentioned prior works, however, is different. For the Last Layer Marginal Likelihood, although the approach works on small datasets such as PCAM (Veeling et al., 2018) and MNIST, the authors report that they were unable to learn invariances 15Published as a conference paper at ICLR 2023 on larger datasets such as CIFAR10. In (Schw¨obel et al., 2021, section 7), they explore the issue of scalability in more detail, and showcase that last layer marginal likelihood is insufﬁcient. Differentiable Laplace performs well, even on more complex datasets, such as CIFAR10. Their scalability, however, is limited by the computational and memory complexity of their method, which we go into in more detail in the section below. D.1 C OMPLEXITY ANALYSIS First, we consider the scalability of our algorithm in terms of computational and memory complexity. In particular, we show that our method scales much more favourably compared to Differentiable Laplace (Immer et al., 2022). We present our analysis for a feed-forward model of depth L, with layer widths D8. In order to directly compare to Immer et al. (2022) and Benton et al. (2020), we consider the complexities in the invariance learning setup (Benton et al., 2020; van der Wilk et al., 2018) withSaugmentation samples. In other experiments, hyperparameter optimization setups, S can be taken to be 1. The notation is summarized in Table 5. N Number of datapoints in dataset D NB Batch size S Number of augmentation samples9 C Output size (number of classes) D Feedforward network layer widths L Feedforward network depth P Number of parameters (s.t. O(P) =O(LD2 + DC)) Table 5: Notation for complexity analysis. We consider the computational and memory costs of 1) obtaining a gradient with respect to the parameters 2) obtaining a gradient with respect to the hyperparameters, and 3) computing the value of the model/hyperparameter selection objective for each method. All analysis assumes computation on a Monte-Carlo estimate of the objective on a single batch of data. In Tables 6 and 7, we assume that C <D, and hence, for the clarity of comparison, sometimes fold a factor depending Cinto a factor depending on Dif it’s clearly smaller. This hiding of the factors was only done for Differentiable Laplace, which is the worst scaling method. D.1.1 C OMPUTATIONAL COMPLEXITY Param. Backward Hyperparam. Backward Hyperparam. Objective Partitioned O(NBPS) O(NBPS) O(NBPS) Augerino O(NBPS) O(NBPS) O(NBPS) Diff. Laplace O(NBPS) O(NBPS+NCP +NCDLS + LD3) O(NPS + NCP +NCDLS + LD3) Table 6: Computational Complexities. The two terms highlighted for Augerino can be computed in a single backward pass. For Differentiable Laplace, the terms in blue can be amortised over multiple hyperparameter backward passes. That is why, in their method, they propose updating the hyperparameters once every epoch on (possibly) multiple batches of data, rather than once on every batch as is done with Partitioned Networks and Augerino. 8This is for the ease of comparison. Same upper bound complexities will hold for a network of variable sizes Dℓ for ℓ∈[L], where D= maxℓ Dℓ 9Only relevant for invariance learning. 16Published as a conference paper at ICLR 2023 D.1.2 M EMORY COMPLEXITY The memory complexities for Partitioned Networks, Augerino, and Differentiable Laplace are shown in Table 7. Crucially, the memory required to update the hyperparameters for Differentiable Laplace scales as O(NBSLD2 + P), with a term depending on the square of the network widths. This can become prohibitively expensive for larger models, and is likely the reason why their paper only considers experiments on architectures with widths up to a maximum of 256. Param. Backward Hyperparam. Backward Hyperparam. Objective Partitioned O(NBSLD+ P) O(NBSLD+ P) O(NBSD+ P) Augerino O(NBSLD+ P) O(NBSLD+ P) O(NBSD+ P) Diff. Laplace O(NBSLD+ P) O(NBSLD2 + P) O(NBSLD2 + P) Table 7: Memory Complexities. Differences are highlighted in red. D.2 P RACTICAL SCALABILITY A complexity analysis in big- Onotation as provided by us in the previous sections allows to understand scalability in the limit, but constant terms that manifest in practice are still of interest. In this section we aim present real timing measurements for our method in comparison to Augerino and Differential Laplace, and elaborate on what overhead might be expected with respect to standard neural network training. The empirical timings measurements on an NVIDIA RTX 3080-10GB GPU are shown in Table 8. We used a batch-size of 250, 200 for the MNIST and CIFAR10 experiments respectively, and 20 augmentation samples, just like in our main experiments in Table 1 and Figure 3. As can be seen, the overhead from using a partitioned network is fairly negligible compared to a standard forward and backward pass. The one difference compared to Augerino is, however, the fact that a separate forward-backward pass needs to be made to update the hyperparameters and regular parameters. This necessity is something that can be side-stepped with alternative partitioning schemes, as preliminarily mentioned in appendix C, and is an interesting direction for future research. MNIST CIFAR10 Method CNN ﬁxupResNet-8 ﬁxupResNet-14 Augerino ×1 ×1 ×1 Diff. Laplace† Param. ×1 ×1 ×1 Hyperparam. ×2015.6 ×18.2 - Partitioned Param. ×1.08 ×1.17 ×1.21 Hyperparam. ×1.08 ×1.08 ×1.09 Table 8: Relative empirical time increase with respect to a regular parameter update during standard training. †The timing multipliers with respect to the baseline for ﬁxupResNet-8 are taken from the timings reported in (Immer et al., 2022, Appendix D.4). On the ResNet-14, we get an out-of- memory error during the hyperparam. update step with Differentiable Laplace on the NVIDIA RTX 3080-10GB GPU when running with the ofﬁcial codebase (Immer and van der Ouderaa). Memory Overhead Our proposed method’s memory consumption scales in the same way as Augerino or vanilla neural network training. There is a minor constant memory overhead due to having to store the assignment of weights to partitions. In general, only log Cbits per parameter are necessary to store the partition assignments, whereCis the number of chunks. In our implementation, we only consider C <28, and hence store the assignments in byte tensors. This means that the partitioned models require extra 25% memory for storing the parameters (when using 32bit ﬂoats to represent the parameters). 17Published as a conference paper at ICLR 2023 If the “default” weight values (i.e. those denoted ˆwi in Figure 1) are non-zero, there is an additional overhead to storing those as well, which doubles the memory required to store the parameters. We observed there was no difference in performance when setting default weight values to 0 in architectures in which normalisation layers are used (i.e. most modern architectures). As such, we would in general recommend to set the default weight values to 0. However, we found setting default values to the initialised values to be necessary for stability of training deep normalisation-free architectures such as the ﬁxup architectures (Zhang et al., 2019) we used to compare with Differentiable Laplace. As their method is not compatible with BatchNorm, we used these architectures in our experiments, and hence used non-zero default values. Lastly, if the default weight values are set to the (random) initialisation values, it is possible to write a cleverer implementation in which only the random seeds are stored in memory, and the default values are re-generated every time they are need in a forward and a backward pass. This would make the memory overhead from storing the default values negligible. E N OTE ON AUGERINO In replicating Augerino (Benton et al., 2020) within our code-base and experimenting with the implementation, we discovered a pathological behaviour that is partly mirrored by the authors of Immer et al. (2022). In particular, note that the loss function (Benton et al., 2020, Equation (5)) proposed by the authors is problematic in the sense that for any regularization strength λ> 0, the optimal loss value is negative inﬁnity since the regularization term (negative L2-norm) is unbounded. In our experiments we observe that for a sufﬁciently-large value of λand after a sufﬁcient number of iterations, this behaviour indeed appears and training diverges. In practice, using Augerino therefore necessitates either careful tuning of λ, clipping the regularisation term (a method that introduces yet another hyperparameter), or other techniques such as early stopping. In the open-source repository for the submission (Benton et al.), it can be seen that on many experiments the authors use a ”safe” variant of the objective, in which they clip the regulariser (without pass-through of the gradient) once the l∞-norm of any of the hyperparameters becomes larger than an arbitrary threshold. Without using this adjustment, we found that the Augerino experiments on MNIST crashed every time with hyperparameters diverging to inﬁnity. F S ENSITIVITY TO PARTITIONING F.1 S ENSITIVITY IN TERMS OF FINAL PERFORMANCE (a) 2 chunks [8,1] [4,1] [1,1] [1,4] [1,8] Param. Proportions [8,1] [4,1] [1,1] [1,4] [1,8]Chunk Proportions 0.990 0.992 0.994 0.996 T est Accuracy  (b) 3 chunks [8,4,1][3,2,1][1,1,1][1,2,3][1,4,8] Param. Proportions [8,4,1] [3,2,1] [1,1,1] [1,2,3] [1,4,8]Chunk Proportions 0.990 0.992 0.994 0.996 T est Accuracy  (c) 4 chunks [8,4,2,1][4,3,2,1][1,1,1,1][1,2,3,4][1,2,4,8] Param. Proportions [8,4,2,1] [4,3,2,1] [1,1,1,1] [1,2,3,4] [1,2,4,8]Chunk Proportions 0.990 0.992 0.994 0.996 T est Accuracy Figure 5: Learning afﬁne augmentations on MNIST with a CNN ﬁt on all data. x- and y−ticks denote the ratios of parameters/datapoints assigned to each partition/chunk respectively. Partitioned networks allow for learning hyperparameters in a single training run, however, they introduce an additional hyperparameter in doing so: the partitioning scheme. The practitioner needs to choose the number of chunks C, the relative proportions of data in each chunk, and the relative proportions of parameters assigned to each of the Cpartitions wk. We investigate the sensitivity to the partitioning scheme here. We show that our results are fairly robust to partitioning through a grid-search over parameter partitions and chunk proportions on the afﬁne augmentation learning task on MNIST with the CNN architecture we use throughout this work. 18Published as a conference paper at ICLR 2023 (a) 2 chunks [8,1] [4,1] [1,1] [1,4] [1,8] Param. Proportions [8,1] [4,1] [1,1] [1,4] [1,8]Chunk Proportions 0.987 0.988 0.989 0.990 T est Accuracy  (b) 3 chunks [8,4,1][3,2,1][1,1,1][1,2,3][1,4,8] Param. Proportions [8,4,1] [3,2,1] [1,1,1] [1,2,3] [1,4,8]Chunk Proportions 0.987 0.988 0.989 0.990 T est Accuracy  (c) 4 chunks [8,4,2,1][4,3,2,1][1,1,1,1][1,2,3,4][1,2,4,8] Param. Proportions [8,4,2,1] [4,3,2,1] [1,1,1,1] [1,2,3,4] [1,2,4,8]Chunk Proportions 0.987 0.988 0.989 0.990 T est Accuracy Figure 6: Learning afﬁne augmentations on RotMNIST with a CNN ﬁt on all data. x- and y−ticks denote the ratios of parameters/datapoints assigned to each partition/chunk respectively. Figure 5 and Figure 6 show the test accuracy for a choice of chunk and parameter proportions across two, three and four chunks. The proportions are to be read as un-normalized distributions; for example, chunk proportions set to [1,8] denotes that there are 8×as many datapoints assigned to the second compared to the ﬁrst. Each conﬁguration was run with 2 random seeds, and we report the mean across those runs in the ﬁgure. The same architecture used was the same as for the main MNIST experiments in section 5 (see Appendix I.4 for details). We observe that for various partition/dataset-chunking conﬁgurations, all models achieve fairly similar ﬁnal test accuracy. There is a trend for models with a lot of parameters assigned to later chunks, but with few datapoints assigned to later chunks, to perform worse. While these results show a high level of robustness against the choice of additional hyperparameters introduced by our method, these results do show an opportunity or necessity for choosing the right partitioning scheme in order to achieve optimal performance. F.2 S ENSITIVITY IN TERMS OF HYPERPARAMETERS FOUND To compare how the different partitioning schemes qualitatively impact the hyperparameters that the method identiﬁes, we also retrain vanilla models from scratch using the hyperparameter values found using partitioned networks. Namely, we take the ﬁnal value of the hyperparameters learned with partitioned networks with a given partitioning scheme, and plot the ﬁnal test set accuracy of a vanilla neural network model trained from scratch with those hyperparameters. The results are shown in Figures 7 and 8. (a) 2 chunks [8,1] [4,1] [1,1] [1,4] [1,8] Param. Proportions [8,1] [4,1] [1,1] [1,4] [1,8]Chunk Proportions 0.990 0.992 0.994 0.996 T est Accuracy  (b) 3 chunks [8,4,1][3,2,1][1,1,1][1,2,3][1,4,8] Param. Proportions [8,4,1] [3,2,1] [1,1,1] [1,2,3] [1,4,8]Chunk Proportions 0.990 0.992 0.994 0.996 T est Accuracy  (c) 4 chunks [8,4,2,1][4,3,2,1][1,1,1,1][1,2,3,4][1,2,4,8] Param. Proportions [8,4,2,1] [4,3,2,1] [1,1,1,1] [1,2,3,4] [1,2,4,8]Chunk Proportions 0.990 0.992 0.994 0.996 T est Accuracy Figure 7: Standard neural network trained onMNIST with a CNN ﬁt on all data, with hyperparameters found using partitioned networks with chunk and parameter proportions corresponding to those in Figure 5. x- and y−ticks denote the ratios of parameters/datapoints assigned to each partition/chunk respectively. G H OW GOOD ARE THE HYPERPARAMETERS FOUND ? Here we show that the hyperparameters found by partitioned networks are also a good set of hyperparameters for vanilla neural networks retrained from scratch. This section expands on the 19Published as a conference paper at ICLR 2023 (a) 2 chunks [8,1] [4,1] [1,1] [1,4] [1,8] Param. Proportions [8,1] [4,1] [1,1] [1,4] [1,8]Chunk Proportions 0.987 0.988 0.989 0.990 T est Accuracy  (b) 3 chunks [8,4,1][3,2,1][1,1,1][1,2,3][1,4,8] Param. Proportions [8,4,1] [3,2,1] [1,1,1] [1,2,3] [1,4,8]Chunk Proportions 0.987 0.988 0.989 0.990 T est Accuracy  (c) 4 chunks [8,4,2,1][4,3,2,1][1,1,1,1][1,2,3,4][1,2,4,8] Param. Proportions [8,4,2,1] [4,3,2,1] [1,1,1,1] [1,2,3,4] [1,2,4,8]Chunk Proportions 0.987 0.988 0.989 0.990 T est Accuracy Figure 8: Standard neural network trained on RotMNIST with a CNN ﬁt on all data, with hyper- parameters found using partitioned networks with chunk and parameter proportions corresponding to those in Figure 6. x- and y−ticks denote the ratios of parameters/datapoints assigned to each partition/chunk respectively. experiment in section F.2. To validate this claim, we conducted a fairly extensive hyperparameter search on the afﬁne augmentation learning task on RotMNIST; we trained 200 models by ﬁrst sampling a set of afﬁne augmentation parameters uniformly at random from a predeﬁned range 10, and then training a neural network model (that averages across augmentation samples at train and test time, as described in Benton et al. (2020)) with standard neural training with those hyperparameters ﬁxed throughout. In Figure 9, we plot the ﬁnal test-set performance of all the models trained with those hyperparameters sampled from a ﬁxed range. Alongside, we show the hyperparameters and test-set performance of the partitioned networks as they progress throughout training. The partitioned networks consistently achieve ﬁnal test-set performance as good as that of the best hyperparameter conﬁgurations iden- tiﬁed through extensive random sampling of the space. We also show the test-set performance of neural network models, trained through standard training, with hyperparameters ﬁxed to the ﬁnal hyperparameter values identiﬁed by the partitioned networks. The hyperparameters identiﬁed by partitioned networks appear to also be good for regular neural networks; the standard neural networks with hyperparameters identiﬁed through partitioned training also outperform the extensive random sampling of the hyperparameter space. Furthermore, Figure 9 shows that partitioned networks do learn full rotation invariance on the RotMNIST task, i.e. when full rotation invariance is present in the data generating distribution. 0.0 0.2 0.4 Translation X 0.96 0.97 0.98 0.99T est Accuracy 0.0 0.2 0.4 Translation Y 0 /2 Rotation Random Sampling Partitioned Runs Partitioned Runs Final Partitioned Runs Final - Retrained 0.0 0.2 0.4 Scale X 0.0 0.2 0.4 0.6 Scale Y 0.0 0.2 0.4 Shear Figure 9: The test-set performance plotted alongside (1D projections of) afﬁne augmentation hyper- parameters on the RotMNIST task with MNIST-CNN. Final test-set accuracies are shown for the hyperparameters sampled randomly for a neural network model trained through standard training with those hyperparameters ﬁxed (+). For multiple partitioned networks runs, the plot shows the progres- sion of the identiﬁed hyperparameters and the test-set performance through the training run ( ), as well as the ﬁnal hyperparameters and test-set performance ( ). Lastly, the plot also shows the ﬁnal test-set accuracies of models trained through standard training on the ﬁnal hyperparameters identiﬁed through partitioned training ( ). 10The ranges were: Uniform(0,π) for the maximum rotation, and Uniform(0,1 2 ) for all the remaining afﬁne augmentation parameters (maximum shear, maximum x−and y−translation, and maximum x−and y−scale). 20Published as a conference paper at ICLR 2023 H L IMITATIONS As mentioned in the main text, our method improves upon existing work, but also comes with its own limitations. Complexity Inherent to our method — as presented in e.g. Figure 1 — is the necessity for an additional forward-backward pass to update the hyperparameters. Consequently, hyperparameter optimization has additional costs which, however, are signiﬁcantly less than the computational costs of existing work, as we discuss in more detail in Appendix D.1 and the experimental section. Furthermore, empirically, partitioned networks usually require more training iterations to converge. Performance Assuming the optimal hyper-parameters are given, training the full, non-partitioned networks based on those optimal values can be expected to yield better performance compared to the ﬁnal model found by partitioned training. Partitioning the network inherently constrains the network capacity, causing some loss of performance. Opportunities for alleviating this performance loss while still enjoying single-run hyperparameter optimization through partitioned training will be left to future work. These include for example adjusting training rounds or increasing network capacity in the ﬁrst place. Partitioning While partitioned networks allows for automatic optimization of, intuitively, hard to tune hyperparameters, such as augmentation parameters, they come with the additional limitation of requiring to partition both the data and the model. This introduces an additional hyperparameter, namely, the partitioning strategy. While our default strategy of assigning more parameters and data to the ﬁrst chunk works reasonably well on all of the experiments we consider, if one targets obtaining the best possible performance on a given task, the partitioning strategy might need additional tuning. We provide some empirical results about the sensitivity to partitioning in appendix F.1 I E XPERIMENTAL DETAILS I.1 P ARTITIONED TRAINING Partitioned parameter update scheduling The gradient computation of Equation 3, as described in the main text, requires that the data-points for updating a given subnetwork w(k) s come from the appropriate dataset chunks (x,y) ∈D1:k for a chunk k. Depending on the partitioning scheme (Appendix C), evaluating different subnetworks for different chunks can or cannot be done in a single mini-batch. More speciﬁcally, the random weight-partitioning we chose for our experiments requires a separate mini-batch per subnetwork (in order to keep the memory cost the same as for standard neural network training). An immediate question arising from a chunked dataset and several partitions is to deﬁne the order and frequency of updates across subnetworks. In our experiments we deﬁne (non-uniform) splits of the training dataset Dacross the Cchunks, which requires a tailored approach to sampling the data. More speciﬁcally, for a given (normalized) ratio of chunk-sizes [u1,...,u C], each iteration of partitioned training proceeds as follows: 1. Sample a partition index k∼Cat(u1,...,u C) 2. Sample a mini-batch ˜Dof examples uniformly from D1:k. 3. Evaluate log p( ˜D|w(k) s ,ψ) using subnetwork w(k) s and 4. compute the (stochastic) gradient wrt. partition parameters wk (Eq. 3). 5. Update partition parameters wk using an optimizer, such as SGD or Adam. This sampling scheme results in a data-point (x,y) ∈Dk from earlier chunks to be sampled more often. Concretely, the probability that an example in chunk kwill be sampled is ∝∑ i≤kui. This is done so that each partition wk is updated with equal probability on each of the examples in D1:k As a result, we use with replacement sampling for the partitioned network training throughout the experimental section. 21Published as a conference paper at ICLR 2023 Gradient optimization of partitioned parameters A consequence of per-partition updates with the random weight partitioning scheme (appendix C) is that, for a chosen partition wk to update, all other partitions do not receive a gradient update. In other words, the gradient at each iteration is sparse. Consequently, many off-the-shelve momentum-based optimizers will not account correctly. Speciﬁcally, we implement modiﬁcations to the PyTorch Paszke et al. (2019) provided optimizers that allow us to track per-partition momenta, number of steps, etc. Note that this creates a disconnect between the number of iterations across all partitions and the number of iterations per-partition. Doing so, however aligns the computational cost of training the partitioned network parameters with the cost of training regular neural network parameters. Regardless, we do not alter the way learning-rate schedulers behave in our experiments and anneal learning-rates according to the total number of iterations. Similarly, we report the total number of iterations when comparing against baselines that update all network-parameters per iteration. While a simple gradient-accumulation scheme across mini-batches would result in a single gradient across all partitions, this approach inherently clashes with non-uniform partitioning [u1,...,u C]. Instead, we chose to sequentially apply gradients computed on a single partition, as described in the previous paragraphs. A further advantage of this approach is that learning progress made by updating partition wk immediately inﬂuences (and can improve) the prediction of subnetworks w(k) s ,w(k+1) s ,..., w(C) s . Gradient optimization of hyperparameters Our partitioned network scheme makes it easy to compute stochastic gradients of the hyperparameter objective LML in Eq. 4 using batch gradient descent optimization methods. After every update to a randomly sampled network partition (see previous paragraph), we update hyperparamters ψas follows: • sample a dataset chunk index k ∼Cat(u2 Z ,..., uC Z ). Ratios are re-normalized to exclude D1. • sample a mini-batch ˜Dof examples uniformly from Dk (Note the choice of Dk instead of D1:k). • Evaluate log p( ˜D|w(k−1) s ,ψ) using subnetwork w(k−1) s and • compute the (stochastic) gradient wrt. hyperparameters ψ(Eq. 4). • Update partition parameters ψusing an optimizer, such as SGD or Adam. The above sampling procedure yields an unbiased estimate of gradients in eq. 4. The fact that we optimize hyperparameters with gradients based on data from a single chunk at a time is again a consequence of the random weight-partitioning scheme for the partitioned networks. It is possible to compute gradients wrt. ψfor mini-batches with examples from multiple chunks at a time. With the random weight partitioning scheme, this would result in an increased memory overhead. Lastly, we could also accumulate gradients from different chunks, similarly to Immer et al. (2022), and this would likely result in a lower-variance estimate per update . It is also possible to reduce the computational overhead of evaluating two mini-batches per iteration (one for updates to wk, one for ψ) as we do in our experiments by interleaving hyperparameter updates at less frequent intervals. We leave an exploration of these design choices to future work. Throughout all experiments, except those in the federated settings (see section J), we use the same batch-size for the hyperparameter udpates as for the regular parameter updates. Weight-decay For partitioned networks, whenever using weight-decay, we scale the weight decay for earlier partitions with the reciprocal of the number of examples in chunks used to optimize them, following the diagonal Gaussian prior interpretation of weight-decay. This makes the training compatible with the variational interpretation in Appendix B. I.2 P ARTITIONED AFFINE TRANSFORMATIONS In Appendix C we described how we realize partitioned versions of fully-connected and convolutional layers. Design choices for other parameterized network layers used in our experiments are described below. 22Published as a conference paper at ICLR 2023 Normalization layers It is common-place in most architectures to follow a normalization layer (such as BatchNorm (Ioffe and Szegedy, 2015), GroupNorm (Wu and He, 2018)) with an element- wise or channel-wise, afﬁne transformation. Namely, such a transformation multiplies its input h by a scale vector s and adds a bias vector b: o = h ∗s + b. For random weight-partitioned networks, we parameterize such afﬁne transformations by deﬁning separate vectors {s1,..., sC} and {b1,..., bC}for each partition; the actual scale and bias used in a given subnetwork w(k) s are s(k) s = ∏ i∈{1,...,k}si and b(k) s = ∑ i∈{1,...,k}bi respectively. This ensures that the ﬁnal afﬁne transformation for each subnetwork w(k) s depends on the parameters in the previous partitions [1,...,k −1]. Doing so increases the parameter count for the partitioned networks in architectures that use those normalization layers by a negligible amount. Scale and bias in FixUp networks The FixUp paper (Zhang et al., 2019) introduces extra scales and biases into the ResNet architecture that transform the entire output of the layers they follow. We turn these into “partitioned” parameters using the same scheme as that for scales and biases of afﬁne transformations following normalization layers. For partitioned networks, through-out the paper, we match the proportion of parameters assigned to each partition kin each layer to the proportion of data examples in the corresponding chunk Dk. I.3 A RCHITECTURE CHOICES Input selection experiments We use a fully-connected feed-forward neural network with2 hidden layers of size [256,256], and with GeLU (Hendrycks and Gimpel, 2016) activation functions. We initialise the weights using the Kaiming uniform scheme (He et al., 2015). For partitioned networks, we use the random-weight partitioning scheme. Fixup Resnet For all experiments using FixUp ResNets we follow Immer et al. (2022); Zhang et al. (2019), and use a 3-stage ResNet with channel-sizes (16,32,64) per stage, with identity skip- connections for the residual blocks as described in He et al. (2016). The residual stages are followed by average pooling and a ﬁnal linear layer with biases. We use 2D average pooling in the residual branches of the downsampling blocks.We initialize all the parameters as described in Zhang et al. (2019). Wide ResNet For all experiments using a Wide-ResNet-N-D (Zagoruyko and Komodakis, 2016), with N being the depth and D the width multiplier, we use a 3 stage ResNet with channel-sizes (16D,32D,64D). We use identity skip-connections for the residual blocks, as described in He et al. (2016), also sometimes known as ResNetV2. ResNet-50 We use the ”V2” version of Wide ResNet as described in (Zagoruyko and Komodakis, 2016) and replace BatchNormalization with GroupNormalization using 2 groups. We use the ’standard’ with withD= 1and three stages of 8 layers for a 50-layer deep ResNet. We use ReLU activations for all ResNet experiments throughout. MNIST CNN For the MNIST experiments, we use the same architecture as Schw¨obel et al. (2021) illustrated in the replicated Table 9. Table 9: CNN architecture for MNIST experiments Layer Speciﬁcation 2D convolution channels=20, kernel size=(5,5), padding=2, activation=ReLU Max pooling pool size=(2,2), stride=2 2D convolution channels=50, kernel size=(5,5), padding=2, activation=ReLU Max pooling pool size=(2,2), stride=2 Fully connected units=500, activation=ReLU Fully connected units=50, activation=ReLU Fully connected units=10, activation=Softmax 23Published as a conference paper at ICLR 2023 I.4 T RAINING DETAILS Learning afﬁne augmentations For the parametrization of the learnable afﬁne augmentation strategies, we follow prior works for a fair comparison. More speciﬁcally, for our MNIST based setup we follow the parametrization proposed in Schw¨obel et al. (2021) whereas for our CIFAR10 based setup we use the generator parametrization from Immer et al. (2022). Input selection experiments For the model selection (non-differentiable) input selection exper- iments, we train all variants with Adam with a learning rate of 0.001 and a batch-size of 256 for 10000 iterations. For both Laplace and partitioned networks, we do early stopping based on the marginal likelihood objective (LML for partitioned networks). We use weight-decay 0.0003 in both cases. For the post-hoc Laplace method, we use the diagonal Hessian approximation, following the recommendation in (Immer et al., 2021). For partitioned networks, we divide the data and parameters into 8 chunks of uniform sizes. We plot results averaged across 3 runs. Mask learning for input selection experiment We use the same optimizer settings as for the input selection experiment. We train for 30000 iterations, and optimize hyperparameters with Adam with a learning rate of 0.001. We divide the data and parameters into 4 uniform chunks. MNIST experiments We follow Schw¨obel et al. (2021), and optimize all methods with Adam with a learning rate of 0.001, no weight decay, and a batch-size of 200. For the partitioned net- works and Augerino results, we use 20 augmentation samples. We use an Adam optimizer for the hyperparameters with a learning rate of 0.001 (and default beta parameters). For Augerino on MNIST, we use the “safe” variant, as otherwise the hyperparameters and the loss diverge on every training run. We elaborate on this phenomenon in Appendix E. Otherwise, we follow the recommended settings from (Benton et al., 2020) and Immer et al. (2022), namely, a regularization strength of 0.01, and a learning rate for the hyperparameters of 0.05. For both MNIST and CIFAR experiments, we found it beneﬁcial to allocate more data to either the earlier, or the later, chunks. Hence, we use 3 chunks with [80%,10%,10%] split of examples for all MNIST and CIFAR experiments. CIFAR variations experiments We again follow Immer et al. (2022), and optimize all ResNet models with SGD with a learning rate of 0.1 decayed by a factor of 100×using Cosine An- nealing, and momentum of 0.9 (as is standard for ResNet models). We use a batch-size of 250. We again use Adam for hyperparameter optimization with a learning rate of 0.001 (and default beta parameters). We train our method for [2400,8000,12000,20000,40000] iterations on subsets [1000,5000,10000,20000,50000] respectively for CIFAR-10, just as in (Immer et al., 2022). For all methods, we used a weight-decay of 1e−4. For partitioned networks, we increase the weight decay for earlier partitions with the square root of the number of examples in chunks used to optimize them, following the diagonal Gaussian prior interpretation of weight-decay. We use3 chunks with [80%,10%,10%] split of examples. For RotCIFAR-10 results, we noticed our method hasn’t fully converged (based on training loss) in this number of iterations, and so we doubled the number of training iterations for the RotMNIST results. This slower convergence can be explained by the fact that, with our method, we only update a fraction of the network parameters at every iteration. TinyImagenet experiments Our experiments with TinyImagenet (Le and Yang, 2015) closely follow the setting for the CIFAR-10 experiments described above. Images are of size64x64 pixels, to be classiﬁed into one of 200 classes. The training-set consists of 100000 images and we compare our method against baselines on subset of [10000,50000,100000] datapoints. For the standard version of TinyImagenet, we train for [80000,80000,40000] steps respectively and for the rotated version of TinyImagenet we train for 120000 steps for all subset sizes. We tuned no other hyper-parameters compared to the CIFAR-10 setup and report our method’s result for a partitioning with[80%,20%] across 2 chunks after ﬁnding it to perform slightly better than a [80%,10%,10%] split across 3 chunks in a preliminary comparison. 24Published as a conference paper at ICLR 2023 Fine-tuning experiments For the ﬁne-tuning experiments in table 2, we trained a FixUp ResNet-14 on a subset of 20000 CIFAR10 examples, while optimizing afﬁne augmentations (following afﬁne augmentations parameterization in (Benton et al., 2020)). We used the same optimizer settings as for all other CIFAR experiments, and trained for 80000 iterations, decaying the learning rate with Cosine Annealing for the ﬁrst 60000 iterations. For ﬁne-tuning of validation-set optimization models, we used SGD with same settings, overriding only the learning rate to 0.01. We tried a learning rate of 0.01 and 0.001, and selected the one that was most favourable for the baseline based on the test accuracy. We also tried training on the full CIFAR-10 dataset, but found that all methods ended up within a standard error of each other when more than 70% of the data was assigned to the ﬁrst chunk (or training set, in the case of validation set optimization). This indicates that CIFAR-10 is sufﬁciently larger that, when combined with afﬁne augmentation learning and the relatively small ResNet-14 architecture used, using the extra data in the 2nd partition (or the validation set) results in negligible gains. I.5 D ATASETS Input selection synthetic dataset For the input selection dataset, we sample 3000 datapoints for the training set as described in section 5, and we use a fresh sample of 1000 datapoints for the test set. RotMNIST Sometimes in the literature, RotMNIST referes to a speciﬁc subset of 12000 MNIST examples, whereas in other works, the full dataset with 60000 examples is used. In this work, following (Benton et al., 2020; Immer et al., 2022) we use the latter. J F EDERATED PARTITIONED TRAINING In this section, we explain how partitioned networks can be applied to the federated setting, as well as the experimental details. J.1 P ARTITIONED NETWORKS IN FL In order to apply partitioned networks to the federated setting, we randomly choose a partition for each client such that the marginal distribution of partitions follows a pre-determined ratio. A given chunk Dk therefore corresponds to the union of several clients’ datasets. Analogous to how “partitioned training” is discussed in the main text and Appendix I, we desire each partition wk to be updated on chunks D1:k. Equation 3 in the main text explains which data chunks are used to compute gradients wrt. parameter partition wk. An analogous perspective to this objective is visualized by the exemplary algorithm in Figure 1 and asks which partitions are inﬂuenced (i,e., updated) by data from chunk Dk: A data chunk Dk is used to compute gradients wrt. partitions wk:C through subnetworks w(k) s to w(C) s respectively. Consequently, a client whose dataset is assigned to chunkDk can compute gradients for all partitions wk:C. Updating network partitions Due to the weight-partitioned construction of the partitioned neural networks, it is not possible to compute gradients with respect to all partitions in a single batched forward-pass through the network. Additionally, a change to the partition parameters wk directly inﬂuences subnetworks w(k+1) s to w(C) s . In order to avoid the choice of ordering indices kto Cfor the client’s local update computation, we update each partition independently while keeping all other partitions initialised to the server-provided values that the client received in that round t: Denote Di,k as the dataset of client iwhere we keep index kto emphasize the client’s assignment to chunkk. Further denote wt+1 j,i as the partition wt j after having been updated by client ion dataset Di,k. wt+1 j,i = arg max wj log p ( Di,k|(wt 1,..., wt j, ˆwt j+1,..., ˆwt j+C),ψ ) ∀j ∈[k,C], (25) where the details of optimization are explained in the following section. We leave an explo- ration for different sequential updating schemes to future work. The ﬁnal update communi- cated by a client to the server consists of the concatenation of all updated parameter partitions 25Published as a conference paper at ICLR 2023 wt+1 .,i = concat(wt+1 k,i ,..., wt+1 C,i ). Note that partitions (wt 1,..., wt k−1) have not been modiﬁed and need not be communicated to the server. The resulting communication reductions make partitioned networks especially attractive to FL as data upload from client to server poses a signiﬁcant bottleneck. In practice, we expect the beneﬁts of these communication reductions to outweigh the additional computation burden of sequentially computing gradients wrt., to multiple partitions. The server receives wt+1 .,i from all clients that participates in round t, computes the delta’s with the global model and proceeds to average them to compute the server-side gradient in the typical federated learning fashion (Reddi et al., 2020). Updating hyperparameters The computation of gradients on a clientiwrt. ψis a straight-forward extension of equation 4 and the exemplary algorithm of Figure 1: ∇ψLML (Di,k,ψ) ≈∇ψlog p ( Di,k|w(t+1),(k−1) s,i ,ψ ) , (26) where Di,k corresponds to client i’s local dataset which is assigned to chunk k and w(t+1),(k−1) s corresponds to the (k−1)’th subnetwork after incorporating all updated partitionsw(t+1),(k−1) s,i = concat(wt 1,..., wt k−1,wt+1 k,i ,..., wt+1 C,i ). Note that we compute a full-batch update to ψin MNIST experiments and use a batch-size equal to the batch-size for the partitioned parameter updates for CIFAR10. Upon receiving these gradients from all clients in this round, the server averages them to form a server-side gradient. Conceptually, this approach to updating ψcorresponds to federated SGD. J.2 F EDERATED SETUP Non-i.i.d. partitioning For our federated experiments, we split the 50kMNIST and 45kCIFAR10 training data-points across 100 clients in a non-i.i.d. way to create the typical challenge to federated learning experiments. In order to simulate label-skew, we follow the recipe proposed in Reddi et al. (2020) with α= 1.0 for CIFAR10 and α= 0.1 for MNIST. Note that with α= 0.1, most clients have data corresponding to only a single digit. For our experiments on rotated versions of CIFAR10 and MNIST, we sample a degree of rotation per data-point and keep it ﬁxed during training. In order to create a non-i.i.d partitioning across the clients, we bin data-points according to their degree of rotation into 10 bins and sample using the same technique as for label-skew with α = 0.1 for both datasets. Learning curves are computed using the 10k MNIST and 5k CIFAR10 validation data-points respectively. For the rotated dataset experiments, we rotate the validation set in the same manner as the training set. Architectures and experimental setup We use the convolutional network provided at Schw¨obel et al. (2021) for MNIST and the ResNet-9 (Dys) model for CIFAR10 but with group normaliza- tion (Wu and He, 2018) instead of batch normalization. We include (learnable) dropout using the continuous relaxation proposed at Maddison et al. (2016) between layers for both architectures. We select 3 chunks for MNIST with a [0.7,0.2,0.1] ratio for both, client-assignments and parameter- partition sizes. For CIFAR10, we found a [0.9,0.1] split across 2 sub-networks to be beneﬁcial. In addition to dropout logits, ψencompasses parameters for afﬁne transformations, i.e., shear, trans- lation, scale and rotation. We report results after 2kand 5krounds, respectively, and the expected communication costs as a percentage of the non-partitioned baseline. Shared setting In order to elaborate on the details to reproduce our results, we ﬁrst focus on the settings that apply across all federated experiments. We randomly sample the corresponding subset of 1.25k, 5kdata-points from the full training set and keep that selection ﬁxed across experiments (i,e., baselines and partitioned networks) as well as seeds. The subsequent partitioning across clients as detailed in the previous paragraph is equally kept ﬁxed across experiments and seeds. Each client computes updates for one epoch of its local dataset, which, for the low data regimes of 1.25k data-points globally, results in single update per client using the entire local dataset. We averaged over 10 augmentation samples for the forward pass in both training and inference. MNIST & RotMNIST For 5k data-points and correspondingly 50 data-points on average per client, most clients perform a single update step. A small selection of clients with more than 64 data- 26Published as a conference paper at ICLR 2023 points performs two updates per round. For the experiments using the full dataset and a mini-batch size of 64, each client performs multiple updates per round. After initial exploration on the baseline FedAvg task, we select a local learning-rate of 5e−2 and apply standard SGD. The server performs Adam Reddi et al. (2020) with a learning rate of 1e−3 for the model parameters. We keep the other parameters of Adam at their standard PyTorch values. We ﬁnd this setting to generalize to the partitioned network experiments but found a higher learning rate of3e−3 for the hyper-parameters to be helpful. We chose the convolutional network from Schw¨obel et al. (2021) with (learned) dropout added between layers. The model’s dropout layers are initialized to drop10% of hidden activations. For the baseline model we keep the dropout-rate ﬁxed and found 10% to be more stable than 30%. CIFAR10 & RotCIFAR10 We ﬁx a mini-batch size of 32, leading to multiple updates per client per round in both, the full dataset regime as well as the5kdata-points setting. Similarly to the MNIST setting, we performed an initial exploration of hyperparameters on the baseline FedAvg task and use the same ones on partitioned networks. We used dropout on the middle layer of each block which was initialized to 0.1 for both the baseline and partitioned networks and whereas partitioned networks optimized it with LML and the concrete relaxation from Maddison et al. (2016), the baseline kept it ﬁxed. For the server side optimizer we used Adam with the default betas and a learning rate of 1e−2, whereas for the hyperparameters we used Adam with the default betas and a learning rate of 1e−3. In both cases we used an ϵ= 1e−7. For the local optimizer we used SGD with a learning rate of 10−0.5 and no momentum. J.3 MNIST LEARNING CURVES In Figure 10 we show learning curves for the three considered dataset sizes on the standard MNIST task. Each learning curve is created by computing a moving average across 10 evaluations, each of which is performed every 10 communication rounds, for each seed. We then compute the average and standard-error across sees and plot those values on the y-axis. On the x-axis we denote the total communication costs (up- and download) to showcase the partitioned networks reduction in communication overhead. We see that especially for the low dataset regime, training has not converged yet and we expect performance to improve for an increased number of iterations. Figure 10: Learning curves for MNIST experiments on 1.25k, 5kand 50kdata-points respectively. 27",
      "meta_data": {
        "arxiv_id": "2304.14766v1",
        "authors": [
          "Bruno Mlodozeniec",
          "Matthias Reisser",
          "Christos Louizos"
        ],
        "published_date": "2023-04-28T11:24:41Z",
        "pdf_url": "https://arxiv.org/pdf/2304.14766v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "This paper proposes Partitioned Neural Networks, a novel and computationally efficient method for hyperparameter optimization (HPO) that does not require a validation set and operates within a single training run. Inspired by the marginal likelihood and the 'learning speed' perspective, the method partitions both the training data into K shards and the neural network parameters into K partitions. It optimizes hyperparameters based on an 'out-of-training-sample' loss of subnetworks (each trained on specific data subsets), which is more scalable and truthful in measuring generalization than prior approaches. Key contributions include demonstrating its effectiveness on various tasks such as differentiable input selection, learning affine data augmentations, optimizing general feature extractors, and learning dropout rates. The method significantly outperforms traditional validation-set-based HPO and prior marginal likelihood approximations, especially in low-data regimes and scales to larger architectures where other methods fail. Furthermore, it is shown to be highly beneficial for Federated Learning, enabling on-device HPO with reduced communication costs and no client-side validation data, leading to better-generalizing models.",
        "methodology": "The core methodology revolves around optimizing an approximation to the marginal likelihood, LML (D,ψ) = C∑ k=1 Eqk-1(w) [log p(Dk|w,ψ)], with respect to hyperparameters ψ. This is achieved by partitioning the neural network's weights (w) into C distinct partitions (w1, ..., wC) and the training dataset (D) into C corresponding shards (D1, ..., DC). For each k, a subnetwork w(k)s is constructed using parameters w1 through wk, while parameters wk+1 through wC are set to default (e.g., initialization) values. This ensures that each subnetwork w(k)s has effectively only been trained on data from shards D1:k. The training procedure interleaves stochastic gradient updates: parameters wk are updated by optimizing the negative log-likelihood on data from D1:k using w(k)s, and hyperparameters ψ are updated using gradients derived from the 'out-of-training-sample' loss of subnetwork w(k-1)s on data shard Dk. This approach provides low-variance gradients for hyperparameters through standard backpropagation, avoiding computationally expensive Hessian computations found in other marginal likelihood approximations. For Federated Learning, clients are assigned to data chunks and compute gradients for the relevant parameter partitions and hyperparameters locally, with only the modified parameters communicated to the server, leading to reduced upload costs.",
        "experimental_setup": "The method was evaluated across a diverse set of tasks and datasets. For **input selection**, a toy synthetic dataset with 15 informative and 15 spurious features was used with fully-connected MLPs. For **invariance learning through data augmentations**, MNIST, CIFAR10, and TinyImagenet datasets were utilized, along with rotated variants (RotMNIST, RotCIFAR10, RotTinyImagenet). Architectures included CNNs for MNIST, fixupResNets (ResNet-8, ResNet-14) for CIFAR10, and ResNet-50 with GroupNorm(2) for TinyImagenet. **Feature extractor learning** experiments used a Wide ResNet-20 on CIFAR10. **Federated Learning** experiments were conducted on non-i.i.d. splits of MNIST, RotMNIST, CIFAR10, and RotCIFAR10 across 100 clients, using a convolutional network for MNIST and a GroupNormalized ResNet-9 for CIFAR10, with learnable dropout. **Baselines** included standard training, Augerino, Differentiable Laplace, Last-layer Marginal Likelihood, traditional validation set optimization (with fine-tuning), and for FL, FedAvg and FedAvg + Augerino. **Validation** involved test accuracy, log-likelihood, and the proposed LML objective. Experiments also explored low-data regimes by using subsets of the training data and investigated sensitivity to partitioning schemes.",
        "limitations": "The proposed method, while offering significant advantages, introduces its own set of limitations. Firstly, it requires an additional forward-backward pass to update the hyperparameters, which introduces some computational overhead, although this is empirically shown to be much less than existing marginal likelihood-based methods. Additionally, partitioned networks typically require more training iterations to converge. Secondly, the act of partitioning the network inherently constrains its capacity, which may result in some performance degradation compared to a fully optimized, non-partitioned network given ideal hyperparameters. Lastly, the partitioning strategy itself (including the number of chunks, and the relative proportions of data and parameters assigned to each) becomes an additional hyperparameter that may need careful tuning to achieve optimal performance, despite empirical evidence suggesting reasonable robustness to these choices.",
        "future_research_directions": "Future research directions include exploring dynamic partitioning strategies for network parameters during training, rather than fixing them beforehand. Another promising area is to investigate alternative design choices for hyperparameter updates, such as accumulating gradients from different chunks or performing less frequent hyperparameter updates, to potentially further reduce computational overhead or variance. Furthermore, research could focus on developing strategies to alleviate the inherent performance loss caused by network partitioning, for example, by adjusting training rounds or proactively increasing the initial network capacity. The authors also express a general hope that their method will contribute to reducing the carbon footprint associated with hyperparameter search through repeated training."
      }
    },
    {
      "title": "PASHA: Efficient HPO and NAS with Progressive Resource Allocation",
      "abstract": "Hyperparameter optimization (HPO) and neural architecture search (NAS) are\nmethods of choice to obtain the best-in-class machine learning models, but in\npractice they can be costly to run. When models are trained on large datasets,\ntuning them with HPO or NAS rapidly becomes prohibitively expensive for\npractitioners, even when efficient multi-fidelity methods are employed. We\npropose an approach to tackle the challenge of tuning machine learning models\ntrained on large datasets with limited computational resources. Our approach,\nnamed PASHA, extends ASHA and is able to dynamically allocate maximum resources\nfor the tuning procedure depending on the need. The experimental comparison\nshows that PASHA identifies well-performing hyperparameter configurations and\narchitectures while consuming significantly fewer computational resources than\nASHA.",
      "full_text": "Published as a conference paper at ICLR 2023 PASHA: E FFICIENT HPO AND NAS WITH PROGRESSIVE RESOURCE ALLOCATION Ondrej Bohdal1∗, Lukas Balles2, Martin Wistuba2, Beyza Ermis3, C´edric Archambeau2, Giovanni Zappella2 1The University of Edinburgh 2AWS, Berlin 3Cohere for AI 1ondrej.bohdal@ed.ac.uk 3beyza@cohere.com 2{balleslb,marwistu,cedrica,zappella}@amazon.com ABSTRACT Hyperparameter optimization (HPO) and neural architecture search (NAS) are methods of choice to obtain the best-in-class machine learning models, but in practice they can be costly to run. When models are trained on large datasets, tuning them with HPO or NAS rapidly becomes prohibitively expensive for practitioners, even when efﬁcient multi-ﬁdelity methods are employed. We propose an approach to tackle the challenge of tuning machine learning models trained on large datasets with limited computational resources. Our approach, named PASHA, extends ASHA and is able to dynamically allocate maximum resources for the tuning procedure depending on the need. The experimental comparison shows that PASHA identiﬁes well-performing hyperparameter conﬁgurations and architectures while consuming signiﬁcantly fewer computational resources than ASHA. 1 I NTRODUCTION Hyperparameter optimization (HPO) and neural architecture search (NAS) yield state-of-the-art models, but often are a very costly endeavor, especially when working with large datasets and models. For example, using the results of (Sharir et al., 2020) we can estimate that evaluating 50 conﬁgurations for a 340-million-parameter BERT model (Devlin et al., 2019) on the 15GB Wikipedia and Book corpora would cost around $500,000. To make HPO and NAS more efﬁcient, researchers explored how we can learn from cheaper evaluations (e.g. on a subset of the data) to later allocate more resources only to promising conﬁgurations. This created a family of methods often described as multi- ﬁdelity methods. Two well-known algorithms in this family are Successive Halving (SH) (Jamieson & Talwalkar, 2016; Karnin et al., 2013) and Hyperband (HB) (Li et al., 2018). Multi-ﬁdelity methods signiﬁcantly lower the cost of the tuning. Li et al. (2018) reported speedups up to 30x compared to standard Bayesian Optimization (BO) and up to 70x compared to random search. Unfortunately, the cost of current multi-ﬁdelity methods is still too high for many practitioners, also because of the large datasets used for training the models. As a workaround, they need to design heuristics which can select a set of hyperparameters or an architecture with a cost comparable to training a single conﬁguration, for example, by training the model with multiple conﬁgurations for a single epoch and then selecting the best-performing candidate. On one hand, such heuristics lack robustness and need to be adapted to the speciﬁc use-cases in order to provide good results. On the other hand, they build on an extensive amount of practical experience suggesting that multi-ﬁdelity methods are often not sufﬁciently aggressive in leveraging early performance measurements and that identifying the best performing set of hyperparameters (or the best architecture) does not require training a model until convergence. For example, Bornschein et al. (2020) show that it is possible to ﬁnd the best hyperparameter – number of channels in ResNet- 101 architecture (He et al., 2015) for ImageNet (Deng et al., 2009) – using only one tenth of the data. However, it is not known beforehand that one tenth of data is sufﬁcient for the task. Our aim is to design a method that consumes fewer resources than standard multi-ﬁdelity algorithms such as Hyperband (Li et al., 2018) or ASHA (Li et al., 2020), and yet is able to identify conﬁgurations ∗Work done during an internship at AWS, Berlin. 1 arXiv:2207.06940v2  [cs.LG]  8 Mar 2023Published as a conference paper at ICLR 2023 that produce models with a similar predictive performance after full retraining from scratch. Models are commonly retrained on a combination of training and validation sets to obtain the best performance after optimizing the hyperparameters. To achieve the speedup, we propose a variant of ASHA, called Progressive ASHA (PASHA), that starts with a small amount of initial maximum resources and gradually increases them as needed. ASHA in contrast has a ﬁxed amount of maximum resources, which is a hyperparameter deﬁned by the user and is difﬁcult to select. Our empirical evaluation shows PASHA can save a signiﬁcant amount of resources while ﬁnding similarly well-performing conﬁgurations as conventional ASHA, reducing the entry barrier to do HPO and NAS. To summarize, our contributions are as follows: 1) We introduce a new approach called PASHA that dynamically selects the amount of maximum resources to allocate for HPO or NAS (up to a certain budget), 2) Our empirical evaluation shows the approach signiﬁcantly speeds up HPO and NAS without sacriﬁcing the performance, and 3) We show the approach can be successfully combined with sample-efﬁcient strategies based on Bayesian Optimization, highlighting the generality of our approach. Our implementation is based on the Syne Tune library (Salinas et al., 2022). 2 R ELATED WORK Real-world machine learning systems often rely on a large number of hyperparameters and require testing many combinations to identify suitable values. This makes data-inefﬁcient techniques such as Grid Search or Random Search (Bergstra & Bengio, 2012) very expensive in most practical scenarios. Various approaches have been proposed to ﬁnd good parameters more quickly, and they can be classiﬁed into two main families: 1) Bayesian Optimization: evaluates the most promising conﬁgurations by modelling their performance. The methods are sample-efﬁcient but often designed for environments with limited amount of parallelism; 2) Multi-ﬁdelity: sequentially allocates more resources to conﬁgurations with better performance and allows high level of parallelism during the tuning. Multi-ﬁdelity methods have typically been faster when run at scale and will be the focus of this work. Ideas from these two families can also be combined together, for example as done in BOHB by Falkner et al. (2018), and we will test a similar method in our experiments. Successive Halving (SH) (Karnin et al., 2013; Jamieson & Talwalkar, 2016) is conceptually the simplest multi-ﬁdelity method. Its key idea is to run all conﬁgurations using a small amount of resources and then successively promote only a fraction of the most promising conﬁgurations to be trained using more resources. Another popular multi-ﬁdelity method, called Hyperband (Li et al., 2018), performs SH with different early schedules and number of candidate conﬁgurations. ASHA (Li et al., 2020) extends the simple and very efﬁcient idea of successive halving by introducing asynchronous evaluation of different conﬁgurations, which leads to further practical speedups thanks to better utilisation of workers in a parallel setting. Related to the problem of efﬁciency in HPO, cost-aware HPO explicitly accounts for the cost of the evaluations of different conﬁgurations. Previous work on cost-aware HPO for multi-ﬁdelity algorithms such as CAHB (Ivkin et al., 2021) keeps a tight control on the budget spent during the HPO process. This is different from our work, as we reduce the budget spent by terminating the HPO procedure early instead of allocating the compute budget in its entirety. Moreover, PASHA could be combined with CAHB to leverage the cost-based resources allocation. Recently, researchers considered dataset subsampling to speedup HPO and NAS. Shim et al. (2021) have combined coresets with PC-DARTS (Xu et al., 2020) and showed that they can ﬁnd well- performing architectures using only 10% of the data and 8.8x less search time. Similarly, Visalpara et al. (2021) have combined subset selection methods with the Tree-structured Parzen Estimator (TPE) for HPO (Bergstra et al., 2011). With a 5% subset they obtained between an 8x to 10x speedup compared to standard TPE. However, in both cases it is difﬁcult to say in advance what subsampling ratio to use. For example, the 10% ratio in (Shim et al., 2021) incurs no decrease in accuracy, while reducing further to 2% leads to a substantial (2.6%) drop in accuracy. In practice, it is difﬁcult to ﬁnd a trade-off between the time required for tuning (proportional to the subset size) and the loss of performance for the ﬁnal model because these change, sometimes wildly, between datasets. Further, Zhou et al. (2020) have observed that for a ﬁxed number of iterations, rank consistency is better if we use more training samples and fewer epochs rather than fewer training samples and more epochs. This observation gives further motivation for using the whole dataset for HPO/NAS and design new approaches, like PASHA, to save computational resources. 2Published as a conference paper at ICLR 2023 3 P ROBLEM SETUP The problem of selecting the best conﬁguration of a machine learning algorithm to be trained is formalized in (Jamieson & Talwalkar, 2016) as a non-stochastic bandit problem. In this setting the learner (the hyperparameter optimizer) receives N hyperparameter conﬁgurations and it has to identify the best performing one with the constraint of not spending more than a ﬁxed amount of resources R(e.g. total number of training epochs) on a speciﬁc conﬁguration. Ris considered given, but in practice users do not have a good way for selecting it, which can have undesirable consequences: if the value is too small, the model performance will be sub-optimal, while if the budget is too large, the user will incur a signiﬁcant cost without any practical return. This leads users to overestimate R, setting it to a large amount of resources in order to guarantee the convergence of the model. We maintain the concept of maximum amount of resources in our algorithm but we prefer to interpret Ras a “safety net”, a cost not to be surpassed (e.g. in case an error prevents a normal behaviour of the algorithm), instead of the exact amount of resources spent for the optimization. This setting could be extended with additional assumptions, based on empirical observation, removing some extreme cases and leading to a more practical setup. In particular, when working with large datasets we observe that the curve of the loss for conﬁgurations (called arms in the bandit literature) continuously decreases (in expectation). Moreover, “crossing points” between the curves are rare (excluding noise), and they are almost always in the very initial part of the training procedure. Viering & Loog (2021); Mohr & van Rijn (2022) provide an analysis of learning curves and note that in practice most learning curves are well-behaved, with Bornschein et al. (2020); Domhan et al. (2015) reporting similar ﬁndings. More formally, let us deﬁne Ras the maximum number of resources needed to train an ML algorithm to convergence. Given πm(i) the ranking of conﬁguration iafter using mresources for training, there exists minimum R∗much smaller than Rsuch that for all amounts of resources rlarger than R∗the rankings of conﬁgurations trained with r resources remain the same: ∃R∗ ≪R : ∀i ∈ {conﬁgurations},∀r > R∗,πR∗(i) =πr(i). The existence of such a quantity, limited to the best performing conﬁguration, is also assumed by Jamieson & Talwalkar (2016), and it is leveraged to quantify the budget required to identify the best performing conﬁguration. If we knew R∗, it would be sufﬁcient to run all conﬁgurations with exactly that amount of resources to identify the best one and then just train the model from scratch with all the data using that conﬁguration. Unfortunately that quantity is unknown and can only be estimated during the optimization procedure. Note that in practice there is noise involved in training of neural networks, so similarly performing conﬁgurations will repeatedly swap their ranks. 4 M ETHOD PASHA is an extension of ASHA (Li et al., 2020) inspired by the “doubling trick” (Auer et al., 1995). PASHA targets improvements for hyperparameter tuning on large datasets by hinging on the assumptions made about the crossing points of the learning curves in Section 3. The algorithm starts with a small initial amount of resources and progressively increases them if the ranking of the conﬁgurations in the top two rungs (rounds of promotion) has not stabilized. The ability of our approach to stop early automatically is the key beneﬁt. We illustrate the approach in Figure 1, showing how we stop evaluating conﬁgurations for additional rungs if rankings are stable. We describe the details of our proposed approach in Algorithm 1. Given η, a hyperparameter used both in ASHA and PASHA to control the fraction of conﬁgurations to prune, PASHA sets the current maximum resources Rt to be used for evaluating a conﬁguration using the reduction factor ηand the minimum amount of resources rto be used (Kt is the current maximum rung). The approach increases the maximum number of resources allocated to promising conﬁgurations each time the ranking of conﬁgurations in the top two rungs becomes inconsistent. For example, if we can currently train conﬁgurations up to rung 2 and the ranking of conﬁgurations in rung 1 and rung 2 is not consistent, then we allow training part of the conﬁgurations up to rung 3, i.e. one additional rung. The minimum amount of resources ris a hyperparameter to be set by the user. It is signiﬁcantly easier to set compared to Ras ris the minimum amount of resources required to see a meaningful difference in the performance of the models, and it can be easily estimated empirically by running a few small-scale experiments. 3Published as a conference paper at ICLR 2023 Figure 1: Illustration of how PASHA stops early if the ranking of conﬁgurations has stabilized. Left: the ranking of the conﬁgurations (displayed inside the circles) has stabilized, so we can select the best conﬁguration and stop the search. Right: the ranking has not stabilized, so we continue. Algorithm 1Progressive Asynchronous Successive Halving (PASHA) 1: input minimum resource r, reduction factor η 2: function PASHA() 3: t= 0,R0 = ηr, K0 = ⌊logη(R0/r)⌋ 4: while desired do 5: for each free worker do 6: (θ,k) =get job() 7: run then return val loss(θ,rηk) 8: end for 9: for completed job (θ, k) with loss ldo 10: Update conﬁguration θin rung kwith loss l 11: if k≥Kt −1 then 12: πk = configuration ranking(k) 13: end if 14: if k= Kt and πk ̸≡πk−1 then 15: t= t+ 1 16: Rt = ηtR0 17: Kt = ⌊logη(Rt/r)⌋ 18: end if 19: end for 20: end while 21: end function 22: function get job() 23: // Check if there is a promotable config 24: for k= Kt −1,..., 1,0 do 25: candidates = top k(rung k,|rung k|/η) 26: promotable = {c∈candidates : cnot promoted} 27: if |promotable|>0 then 28: return promotable[0],k + 1 29: end if 30: // If not, grow bottom rung 31: Draw random conﬁguration θ 32: return θ,0 33: end for 34: end function We also set a maximum amount of resources Rso that PASHA can default to ASHA if needed and avoid increasing the resources indeﬁnitely. While it is not generally reached, it provides a safety net. 4.1 S OFT RANKING Due to the noise present in the training process, negligible differences in the measured predictive performance of different conﬁgurations can lead to signiﬁcantly different rankings. For these reasons we adopt what we call “soft ranking”. In soft ranking, conﬁgurations are still sorted by predictive performance but are considered equivalent if the performance difference is smaller 4Published as a conference paper at ICLR 2023 than a value ϵ (or equal to it). Instead of producing a sorted list of conﬁguration, this provides a list of lists where for every position of the ranking there is a list of equivalent conﬁgurations. The concept is explained graphically in Figure 2, and we also provide a formal deﬁnition. For a set of n conﬁgurations c1,c2,··· ,ci,··· ,cn and performance metric f (e.g. accuracy) with f(c1) ≤f(c2) ≤···≤ f(ci) ≤···≤ f(cn), soft rank at position iis deﬁned as soft ranki = {cj ∈conﬁgurations : |f(ci) −f(cj)|≤ ϵ}. When deciding on if to increase the resources, we go through the ranked list of conﬁgurations in the top rung and check if the current conﬁguration at the given rank was in the list of conﬁgurations for that rank in the previous rung. If there is a conﬁguration which does not satisfy the condition, we increase resources. Figure 2: Illustration of soft ranking. There are three lists with the ﬁrst two containing two items because the scores of the two conﬁgurations are closer to each other than ϵ. 4.2 A UTOMATIC ESTIMATION OF ϵBY MEASURING NOISE IN RANKINGS Every operation involving randomization gives slightly different results when repeated, the training process and the measurement of performance on the validation set are no exception. In an ideal world, we could repeat the process multiple times to compute empirical mean and variance to make a better decision. Unfortunately this is not possible in our case since the repeating portions of the training process will defeat the purpose of our work: speeding up the tuning process. Understanding when the differences between the performance measured for different conﬁgurations are “signiﬁcant” is crucial for ranking them correctly. We devise a method to estimate a threshold below which differences are meaningless. Our intuition is that conﬁgurations with different performance maintain their relative ranking over time. On the other hand, conﬁgurations that repeatedly swap their rankings perform similarly well and the performance difference in the current epoch or rung is simply due to noise. We want to measure this noise and use it to automatically estimate the threshold value ϵto be used in the soft-ranking described above. Formally we can deﬁne a set of pairs of conﬁgurations that perform similarly well by the following: S : {(c,c′) : ( πrj (c) >πrj (c′) ∧πrk (c) <πrk (c′) ∧πrl (c) >πrl (c′) ) ∨ ( πrj (c) <πrj (c′) ∧πrk (c) >πrk (c′) ∧πrl (c) <πrl (c′) ) }, (1) for resource levels (e.g. epochs – not rungs) rj > rk > rl, using the same notation as earlier to refer to resources. In practice we have per-epoch validation performance statistics and use these to ﬁnd resource levels rj,rk,rl that have conﬁgurations with the criss-crossing behaviour (there can be several epochs between such resource levels). We only consider conﬁgurations (c,c′) that made it to the latest rung, so rηKt−1 ≥rj >rη Kt−2. However, we allow for the criss-crossing to happen across epochs from any rungs. The value of ϵcan then be calculated as the N-th percentile of distances between the performances of conﬁgurations in S: ϵ= PN,(c,c′)∈S|frj (c) −frj (c′)|. The exact value of rj depends on the considered pair of conﬁgurations (c,c′). To uniquely deﬁne frj , we take the maximum resources rj currently available for both conﬁgurations in the consid- ered pair (c,c′). Let us consider the following example setup: the top rung has 8 epochs and 5Published as a conference paper at ICLR 2023 the next one has 4 epochs, there are three conﬁgurations ca,cb,cc that made it to the top rung and were trained for 8, 8 and 6 epochs so far respectively. Assuming there was criss-crossing within each pair (ca,cb), (ca,cc) and (cb,cc), the set of distances between conﬁgurations in S is {|f8(ca) −f8(cb)|,|f6(ca) −f6(cc)|,|f6(cb) −f6(cc)|}. The value of ϵis recalculated every time we receive new information about the performances of conﬁgurations. Initially the value of ϵis set to 0, which means that we check for exact ranking if we cannot yet calculate the value of ϵ. 5 E XPERIMENTS In this section we empirically evaluate the performance of PASHA. Its goal is not to provide a model with a higher accuracy, but to identify the best conﬁguration in a shorter amount of time so that we can then re-train the model from scratch. Overall, we target a signiﬁcantly faster tuning time and on-par predictive performance when comparing with the models identiﬁed by state-of-the-art optimizers like ASHA. Re-training after HPO or NAS is important because HPO and NAS in general require to reserve a signiﬁcant part of the data (often around 20 or 30%) to be used as a validation set. Training with fewer data is not desirable because in practice it is observed that training a model on the union of training and validation sets provides better results. We tested our method on two different sets of experiments. The ﬁrst set evaluates the algorithm on NAS problems and uses NASBench201 (Dong & Yang, 2020), while the second set focuses on HPO and was run on two large-scale tasks from PD1 benchmark (Wang et al., 2021). 5.1 S ETUP Our experimental setup consists of two phases: 1) run the hyperparameter optimizer until N = 256 candidate conﬁgurations are evaluated; and 2) use the best conﬁguration identiﬁed in the ﬁrst phase to re-train the model from scratch. For the purpose of these experiments we re-train all the models using only the training set. This avoids introducing an arbitrary choice on the validation set size and allows us to leverage standard benchmarks such as NASBench201. In real-world applications the model can be trained on both training and validation sets. All our results report only the time invested in identifying the best conﬁguration since the re-training time is comparable for all optimizers. All results are averaged over multiple repetitions, with the details speciﬁed for each set of experiments separately. We use N = 90-th percentile when calculating the value of ϵ. We use 4 workers to perform parallel and asynchronous evaluations. The choice of Ris sensitive for ASHA as it can make the optimizer consume too many resources and penalize the performance. For a fair comparison, we make Rdataset-dependent taking the maximum amount of resources in the considered benchmarks. ris also dataset-dependent and η, the halving factor, is set to 3 unless otherwise speciﬁed. The same values are used for both ASHA and PASHA. Runtime reported is the time spent on HPO (without retraining), including the time for computing validation set performance. We compare PASHA with ASHA (Li et al., 2020), a widely-adopted approach for multi-ﬁdelity HPO, and other relevant baselines. In particular, we consider “one-epoch baseline” that trains all conﬁgurations for one epoch (the minimum available resources) and then selects the most promising conﬁguration, and “random baseline” that randomly selects the conﬁguration without any training. For both one-epoch and random baselines we sample N = 256conﬁgurations, using the same scheduler and seeds as for PASHA and ASHA. All reported accuracies are after retraining for R= 200epochs. In addition, two, three and ﬁve-epoch baselines are evaluated in Appendix A. 5.2 NAS EXPERIMENTS For our NAS experiments we leverage the well-known NASBench201 (Dong & Yang, 2020) bench- mark. The task is to identify the network structure providing the best accuracy on three different datasets (CIFAR-10, CIFAR-100 and ImageNet16-120) independently. We use r = 1epoch and R= 200epochs. We repeat the experiments using 5 random seeds for the scheduler and 3 random seeds for NASBench201 (all that are available), resulting in 15 repetitions. Some conﬁgurations in NASBench201 do not have all seeds available, so we impute them by averaging over the available seeds. To measure the predictive performance we report the best accuracy on the combined validation and test set provided by the creators of the benchmark. 6Published as a conference paper at ICLR 2023 Table 1: NASBench201 results. PASHA leads to large improvements in runtime, while achieving similar accuracy as ASHA. Dataset Approach Accuracy (%) Runtime Speedup factor Max resources CIFAR-10 ASHA 93.85 ±0.25 3.0h±0.6h 1.0x 200.0 ±0.0 PASHA 93.57 ±0.75 1.3h±0.6h 2.3x 36.1 ±50.0 One-epoch baseline 93.30±0.61 0.3h±0.0h 8.5x 1.0 ±0.0 Random baseline 72.88±19.20 0.0h±0.0h N/A 0.0 ±0.0 CIFAR-100 ASHA 71.69 ±1.05 3.2h±0.9h 1.0x 200.0 ±0.0 PASHA 71.84 ±1.41 0.9h±0.4h 3.4x 20.5 ±48.3 One-epoch baseline 65.57±5.53 0.3h±0.0h 9.2x 1.0 ±0.0 Random baseline 42.83±18.20 0.0h±0.0h N/A 0.0 ±0.0 ImageNet16-120 ASHA 45.63 ±0.81 8.8h±2.2h 1.0x 200.0 ±0.0 PASHA 45.13 ±1.51 2.9h±1.7h 3.1x 21.3 ±48.1 One-epoch baseline 41.42±4.98 1.0h±0.0h 8.8x 1.0 ±0.0 Random baseline 20.75±9.97 0.0h±0.0h N/A 0.0 ±0.0 The results in Table 1 suggest PASHA consistently leads to strong improvements in runtime, while achieving similar accuracy values as ASHA. The one-epoch baseline has noticeably worse accuracies than ASHA or PASHA, suggesting that PASHA does a good job of deciding when to continue increasing the resources – it does not stop too early. Random baseline is a lot worse than the one- epoch baseline, so there is value in performing NAS. We also report the maximum resources used to ﬁnd how early the ranking becomes stable in PASHA. The large variances are caused by stopping HPO at different rung levels for different seeds (e.g. 27 and 81 epochs). Note that the time required to train a model is about 1.3h for CIFAR-10 and CIFAR-100, and about 4.1h for ImageNet16-120, making the total tuning time of PASHA comparable or faster than the training time. We also ran additional experiments testing PASHA with a reduction factor ofη= 2and η= 4instead of η = 3, the usage of PASHA as a scheduler in MOBSTER (Klein et al., 2020) and alternative ranking functions. These experiments provided similar ﬁndings as the above and are described next. 5.2.1 R EDUCTION FACTOR An important parameter for the performance of multi-ﬁdelity algorithms like ASHA is the reduction factor. This hyperparameter controls the fraction of pruned candidates at every rung. The optimal theoretical value is eand it is typically set to 2 or 3. In Table 2 we report the results of the different algorithms ran with η= 2and η= 4on CIFAR-100 (the full set of results is in Appendix B). The gains are consistent also for η= 2and η= 4, with a larger speedup when using η= 2as that allows PASHA to make more decisions and identify earlier that it can stop the search. Table 2: NASBench201 – CIFAR-100 results with various reduction factorsη. The speedup is large for both η= 2and η= 4, and accuracy similar to ASHA is retained. Dataset Reduction factor Approach Accuracy (%) Runtime Speedup factor Max resources CIFAR-100 η= 2 ASHA 71.67 ±0.84 3.8h ±1.0h 1.0x 200.0 ±0.0 PASHA 71.65±1.42 0.9h ±0.1h 4.2x 5.9 ±2.0 η= 4 ASHA 71.43 ±1.13 2.7h ±0.9h 1.0x 200.0 ±0.0 PASHA 72.09±1.22 1.0h ±0.4h 2.8x 25.1 ±49.0 5.2.2 B AYESIAN OPTIMIZATION Bayesian Optimization combined with multi-ﬁdelity methods such as Successive Halving can improve the predictive performance of the ﬁnal model (Klein et al., 2020). In this set of experiments, we verify PASHA can speedup also these kinds of methods. Our results are reported in Table 3, where we can clearly see PASHA obtains a similar accuracy result as ASHA with signiﬁcant speedup. 7Published as a conference paper at ICLR 2023 Table 3: NASBench201 results for ASHA with Bayesian Optimization searcher – MOBSTER (Klein et al., 2020) and similarly extended version of PASHA. The results show PASHA can be successfully combined with a smarter conﬁguration selection strategy. Dataset Approach Accuracy (%) Runtime Speedup factor Max resources CIFAR-10 MOBSTER 94.21±0.28 5.0h ±1.1h 1.0x 200.0 ±0.0 PASHA BO 94.00±0.20 2.6h ±1.8h 2.0x 70.7 ±81.6 CIFAR-100 MOBSTER 72.79±0.68 5.7h ±1.4h 1.0x 200.0 ±0.0 PASHA BO 72.16±1.07 1.6h ±0.5h 3.7x 13.0 ±8.7 ImageNet16-120MOBSTER 46.21±0.70 15.1h±4.0h 1.0x 200.0 ±0.0 PASHA BO 45.36±1.06 3.9h ±1.2h 3.9x 11.8 ±7.9 5.2.3 A LTERNATIVE RANKING FUNCTIONS We have considered a variety of alternative ranking functions in addition to the soft ranking function that automatically estimates the value of ϵby measuring noise in rankings. These include simple ranking (equivalent to soft ranking withϵ= 0.0), soft ranking with ﬁxed values of ϵor obtained using various heuristics (for example based on the standard deviation of objective values in the previous rung), Rank Biased Overlap (RBO) (Webber et al., 2010), and our own reciprocal rank regret metric (RRR) that considers the objective values of conﬁgurations. Details of the ranking functions and additional results are in Appendix C. Table 4 shows a selection of the results on CIFAR-100 with full results in the appendix. We can see there are also other ranking functions that work well and that simple ranking is not sufﬁciently robust – some benevolence is needed. However, the ranking function that estimates the value of ϵ by measuring noise in rankings (to which we refer simply as PASHA) remains the easiest to use, is well-motivated and offers both excellent performance and large speedup. Table 4: NASBench201 – CIFAR-100 results for a variety of ranking functions, showing there are also other well-performing options, even though those are harder to use and are less interpretable. Approach Accuracy (%) Runtime (s) Speedup factor Max resources ASHA 71.69 ±1.05 3.2h±0.9h 1.0x 200.0 ±0.0 PASHA 71.84 ±1.41 0.9h±0.4h 3.4x 20.5 ±48.3 PASHA direct ranking 71.69 ±1.05 2.8h±0.7h 1.1x 200.0 ±0.0 PASHA soft rankingϵ= 2.5% 71.41±1.15 1.5h±0.7h 2.1x 88.3 ±74.4 PASHA soft rankingϵ= 2σ 71.14±0.97 1.9h±0.7h 1.7x 136.4 ±75.8 PASHA RBO 71.51 ±0.93 2.4h±0.7h 1.3x 180.5 ±50.6 PASHA RRR 71.42 ±1.51 1.2h±0.5h 2.6x 39.3 ±51.4 One-epoch baseline 65.57 ±5.53 0.3h±0.0h 9.2x 1.0 ±0.0 Random baseline 42.83 ±18.20 0.0h±0.0h N/A 0.0 ±0.0 5.3 HPO EXPERIMENTS We further utilize the PD1 HPO benchmark (Wang et al., 2021) to show the usefulness of PASHA in large-scale settings. In particular, we take WMT15 German-English (Bojar et al., 2015) and ImageNet (Deng et al., 2009) datasets that use xformer (Lefaudeux et al., 2021) and ResNet50 (He et al., 2015) models. Both of them are datasets with a large amount of training examples, in particular WMT15 German-English has about 4.5M examples, while ImageNet has about 1.3M examples. In PD1 we optimize four hyperparameters: base learning rate η∈ [ 10−5,10.0 ] (log scale), momen- tum 1 −β ∈ [ 10−3,1.0 ] (log scale), polynomial learning rate decay schedule power p∈[0.1,2.0] (linear scale) and decay steps fraction λ∈[0.01,0.99] (linear scale). The minibatch size used for WMT experiments is 64, while the minibatch size for ImageNet experiments is 512. There are 1414 epochs available for WMT and 251 for ImageNet. There are also other datasets in PD1, but these only have a small number of epochs with 1 epoch being the minimum amount of resources. As a result there would not be enough rungs to see beneﬁts of the early stopping provided by PASHA. 8Published as a conference paper at ICLR 2023 If resources could be deﬁned in terms of fractions of epochs, PASHA could be beneﬁcial there too. Most public benchmarks have resources deﬁned in terms of epochs, but in practice it is possible to deﬁne resources also in alternative ways. We use 1-NN as a surrogate model for the PD1 benchmark. We repeat our experiments using 5 random seeds and there is only one dataset seed available. The results in Table 5 show that PASHA leads to large speedups on both WMT and ImageNet datasets. The speedup is particularly impressive for the signiﬁcantly larger WMT dataset where it is about 15.5x, highlighting how PASHA can signiﬁcantly accelerate the HPO search on datasets with millions of training examples (WMT has about 4.5M training examples). The one-epoch baseline obtains similar accuracy as ASHA and PASHA for WMT, but performs signiﬁcantly worse on ImageNet dataset. This result suggests that simple approaches such as the one-epoch baseline are not robust and solutions such as PASHA are needed (which we also saw on NASBench201). Selecting the hyperparameters randomly leads to signiﬁcantly worse performance than any of the other approaches. Table 5: Results of the HPO experiments on WMT and ImageNet tasks from the PD1 benchmark. Mean and std of the best validation accuracy (or its equivalent as given in the PD1 benchmark). Dataset Approach Accuracy (%) Runtime Speedup factor Max resources WMT ASHA 62.72 ±1.41 43.7h±37.2h 1.0x 1357.4 ±80.4 PASHA 62.04 ±2.05 2.8h ±0.6h 15.5x 37.8 ±21.6 One-epoch baseline 62.36±1.40 0.6h ±0.0h 67.3x 1.0 ±0.0 Random baseline 33.93±21.96 0.0h ±0.0h N/A 0.0 ±0.0 ImageNet ASHA 75.10 ±2.03 7.3h ±1.2h 1.0x 251.0 ±0.0 PASHA 73.37 ±2.71 3.8h ±1.0h 1.9x 45.0 ±30.1 One-epoch baseline 63.40±9.91 1.1h ±0.0h 6.7x 1.0 ±0.0 Random baseline 36.94±31.05 0.0h ±0.0h N/A 0.0 ±0.0 6 L IMITATIONS PASHA is designed to speed up ﬁnding the best conﬁguration, making HPO and NAS more accessible. To do so, PASHA interrupts the tuning process when it considers the ranking of conﬁgurations to be sufﬁciently stable, not spending resources on evaluating conﬁgurations in later rungs. However, the beneﬁts of such mechanism will be small in some circumstances. When the number of rungs is small, there will be few opportunities for PASHA to interrupt the tuning and provide large speedups. This phenomenon is demonstrated in Appendix D on the LCBench benchmark (Zimmer et al., 2021). Public benchmarks usually ﬁx the minimum resources to one epoch, while the maximum is benchmark-dependent (e.g. 200 epochs for NASBench201 and 50 for LCBench), leaving little control for algorithms like PASHA in some cases. Appendix E analyses the impact of these choices. For practical usage, we recommend having a maximum amount of resources at least 100 times larger than the minimum amount of resources when using η= 3(default). This can be achieved by measuring resources with higher granularity (e.g. in terms of gradient updates) if needed. 7 C ONCLUSIONS In this work we have introduced a new variant of Successive Halving called PASHA. Despite its simplicity, PASHA leads to strong improvements in the tuning time. For example, in many cases it reduces the time needed to about one third compared to ASHA without a noticeable impact on the quality of the found conﬁguration. For benchmarks with a small number of rungs (LCBench), PASHA provides more modest speedups but this limitation can be mitigated in practice by adopting a more granular unit of measure for resources. Further work could investigate the deﬁnition of rungs and resource levels, with the aim of understanding how they impact the decisions of the algorithm. More broadly this applies not only to PASHA but also to multi-ﬁdelity algorithms in general. PASHA can also be successfully combined with more advanced search strategies based on Bayesian Optimization to obtain improvements in accuracy at a fraction of the time. In the future, we would like to test combinations of PASHA with transfer-learning techniques for multi-ﬁdelity such as RUSH (Zappella et al., 2021) to further decrease the tuning time. 9Published as a conference paper at ICLR 2023 REPRODUCIBILITY STATEMENT We include the code for our approach at https://github.com/ondrejbohdal/pasha, including details for how to run the experiments. We use pre-computed benchmarks that make it possible to run the NAS and HPO experiments even without large computational resources. In addition, PASHA is available as part of the Syne Tune library (Salinas et al., 2022). ACKNOWLEDGEMENTS We would like to thank the Syne Tune developers for providing us with a library to easily extend and use in our experiments. We would like to thank Aaron Klein, Matthias Seeger and David Salinas for their support on questions regarding Syne Tune and hyperparameter optimization more broadly. We would also like to thank Valerio Perrone, Sanyam Kapoor and Aditya Rawal for insightful discussions when working on the project. Further, we are thankful to the anonymous reviewers for helping us improve our paper. REFERENCES Peter Auer, Nicol´o Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. Gambling in a rigged casino: The adversarial multi-armed bandit problem. In Annual Foundations of Computer Science, 1995. James Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. JMLR, 2012. James Bergstra, R´emi Bardenet, Yoshua Bengio, and Bal´azs K´egl. Algorithms for hyper-parameter optimization. In NIPS, 2011. Ondˇrej Bojar, Rajen Chatterjee, Christian Federmann, Barry Haddow, Matthias Huck, Chris Hokamp, Philipp Koehn, Varvara Logacheva, Christof Monz, Matteo Negri, Matt Post, Carolina Scarton, Lucia Specia, and Marco Turchi. Findings of the 2015 workshop on statistical machine translation. In Proceedings of the Tenth Workshop on Statistical Machine Translation, 2015. Jorg Bornschein, Francesco Visin Visin, and Simon Osindero. Small data, big decisions: Model selection in the small-data regime. In ICML, 2020. Jia Deng, Wei Dong, Richard Socher, Li-jia Li, Kai Li, and Li Fei-fei. Imagenet: A large-scale hierarchical image database, 2009. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In ACL, 2019. Tobias Domhan, Jost Tobias Springenberg, and Frank Hutter. Speeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves. In IJCAI, 2015. Xuanyi Dong and Yi Yang. Nas-bench-201: Extending the scope of reproducible neural architecture search. In ICLR, 2020. Stefan Falkner, Aaron Klein, and Frank Hutter. BOHB: Robust and efﬁcient hyperparameter opti- mization at scale. In ICML, 2018. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2015. Nikita Ivkin, Zohar Karnin, Valerio Perrone, and Giovanni Zappella. Cost-aware adversarial best arm identiﬁcation. In ICLR NAS Workshop, 2021. Kevin Jamieson and Ameet Talwalkar. Non-stochastic best arm identiﬁcation and hyperparameter optimization. In AISTATS, 2016. Zohar Karnin, Tomer Koren, and Oren Somekh. Almost optimal exploration in multi-armed bandits. In ICML, 2013. 10Published as a conference paper at ICLR 2023 Aaron Klein, Louis C. Tiao, Thibaut Lienart, Cedric Archambeau, and Matthias Seeger. Model-based asynchronous hyperparameter and neural architecture search. In arXiv, 2020. Benjamin Lefaudeux, Francisco Massa, Diana Liskovich, Wenhan Xiong, Vittorio Caggiano, Sean Naren, Min Xu, Jieru Hu, Marta Tintore, and Susan Zhang. xformers: A modular and hackable transformer modelling library, 2021. Liam Li, Kevin Jamieson, Afshin Rostamizadeh, Ekaterina Gonina, Moritz Hardt Hardt, Benjamin Recht, and Ameet Talwalkar. A system for massively parallel hyperparameter tuning. In MLSys, 2020. Lisha Li, Kevin G. Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. Hyper- band: A novel bandit-based approach to hyperparameter optimization. JMLR, 2018. Felix Mohr and Jan N. van Rijn. Learning curves for decision making in supervised machine learning - a survey. In arXiv, 2022. David Salinas, Matthias Seeger, Aaron Klein, Valerio Perrone, Martin Wistuba, and Cedric Archam- beau. Syne tune: A library for large scale hyperparameter tuning and reproducible research. In First Conference on Automated Machine Learning (Main Track), 2022. Or Sharir, Barak Peleg, and Yoav Shoham. The cost of training nlp models: A concise overview. In arXiv, 2020. Jae-hun Shim, Kyeongbo Kong, and Suk-Ju Kang. Core-set sampling for efﬁcient neural architecture search. In ICML Workshop on Subset Selection in ML, 2021. Tom Viering and Marco Loog. The shape of learning curves: a review. In arXiv, 2021. Savan Visalpara, Krishnateja Killamsetty, and Rishabh Iyer. A data subset selection framework for efﬁcient hyper-parameter tuning and automatic machine learning. In ICML Workshop on Subset Selection in ML, 2021. Zi Wang, George E. Dahl, Kevin Swersky, Chansoo Lee, Zelda Mariet, Zachary Nado, Justin Gilmer, Jasper Snoek, and Zoubin Ghahramani. Pre-trained Gaussian processes for Bayesian Optimization. In arXiv, 2021. William Webber, Alistair Moffat, and Justin Zobel. A similarity measure for indeﬁnite rankings. In ACM Transactions on Information Systems, 2010. Yuhui Xu, Lingxi Xie, Xiaopeng Zhang, Xin Chen, Guo-Jun Qi, Qi Tian, and Hongkai Xiong. PC-DARTS: Partial channel connections for memory-efﬁcient architecture search. In ICLR, 2020. Giovanni Zappella, David Salinas, and C´edric Archambeau. A resource-efﬁcient method for repeated HPO and NAS problems. In ICML AutoML Workshop, 2021. Dongzhan Zhou, Xinchi Zhou, Wenwei Zhang, Chen Change Loy, Shuai Yi, Xuesen Zhang, and Wanli Ouyang. EcoNAS: Finding proxies for economical neural architecture search. In CVPR, 2020. Lucas Zimmer, Marius Lindauer, and Frank Hutter. Auto-PyTorch Tabular: Multi-ﬁdelity metalearn- ing for efﬁcient and robust AutoDL. TPAMI, 2021. 11Published as a conference paper at ICLR 2023 A A DDITIONAL BASELINES We consider additional baselines that evaluate how good two, three and ﬁve-epoch baselines are compared to PASHA. From Table 6 and 7 we see that while these usually get closer to the performance of ASHA and PASHA than the one-epoch baseline, they are still relatively far compared to PASHA. Moreover, it is crucial to observe that such baselines cannot dynamically allocate resources and decide when to stop, and as a result PASHA can outperform them both in terms of speedup and the quality of the found conﬁguration. Table 6: NASBench201 results. PASHA leads to large improvements in runtime, while achieving similar accuracy as ASHA. Dataset Approach Accuracy (%) Runtime Speedup factor Max resources CIFAR-10 ASHA 93.85 ±0.25 3.0h ±0.6h 1.0x 200.0 ±0.0 PASHA 93.57 ±0.75 1.3h ±0.6h 2.3x 36.1 ±50.0 One-epoch baseline 93.30±0.61 0.3h ±0.0h 8.5x 1.0 ±0.0 Two epoch baseline 92.75±0.91 0.7h ±0.0h 4.2x 2.0 ±0.0 Three epoch baseline 93.47±0.71 1.0h ±0.0h 2.8x 3.0 ±0.0 Five epoch baseline 93.38±0.90 1.7h ±0.0h 1.7x 5.0 ±0.0 Random baseline 72.88 ±19.20 0.0h±0.0h N/A 0.0 ±0.0 CIFAR-100 ASHA 71.69 ±1.05 3.2h ±0.9h 1.0x 200.0 ±0.0 PASHA 71.84 ±1.41 0.9h ±0.4h 3.4x 20.5 ±48.3 One-epoch baseline 65.57±5.53 0.3h ±0.0h 9.2x 1.0 ±0.0 Two-epoch baseline 68.28±4.25 0.7h ±0.0h 4.6x 2.0 ±0.0 Three-epoch baseline 70.47±1.60 1.0h ±0.0h 3.1x 3.0 ±0.0 Five-epoch baseline 70.95±0.95 1.7h ±0.0h 1.8x 5.0 ±0.0 Random baseline 42.83 ±18.20 0.0h±0.0h N/A 0.0 ±0.0 ImageNet16-120 ASHA 45.63 ±0.81 8.8h ±2.2h 1.0x 200.0 ±0.0 PASHA 45.13 ±1.51 2.9h ±1.7h 3.1x 21.3 ±48.1 One-epoch baseline 41.42±4.98 1.0h ±0.0h 8.8x 1.0 ±0.0 Two-epoch baseline 42.99±1.89 2.0h ±0.0h 4.4x 2.0 ±0.0 Three-epoch baseline 44.65±0.95 3.0h ±0.0h 2.9x 3.0 ±0.0 Five-epoch baseline 44.75±1.03 5.0h ±0.1h 1.8x 5.0 ±0.0 Random baseline 20.75 ±9.97 0.0h ±0.0h N/A 0.0 ±0.0 Table 7: Results of the HPO experiments on WMT and ImageNet tasks from the PD1 benchmark. Mean and std of the best validation accuracy (or its equivalent as given in the PD1 benchmark). Dataset Approach Accuracy (%) Runtime Speedup factor Max resources WMT ASHA 62.72 ± 1.41 43.7h ±37.2h 1.0x 1357.4 ±80.4 PASHA 62.04 ± 2.05 2.8h ± 0.6h 15.5x 37.8 ±21.6 One-epoch baseline 62.36 ± 1.40 0.6h ± 0.0h 67.3x 1.0 ± 0.0 Two-epoch baseline 60.16± 3.58 1.1h ± 0.0h 39.1x 2.0 ± 0.0 Three-epoch baseline 61.12± 3.47 1.6h ± 0.0h 27.6x 3.0 ± 0.0 Five-epoch baseline 57.89 ± 5.33 2.5h ± 0.0h 17.3x 5.0 ± 0.0 Random baseline 33.93 ±21.96 0.0h ± 0.0h N/A 0.0 ± 0.0 ImageNet ASHA 75.10 ± 2.03 7.3h ± 1.2h 1.0x 251.0 ± 0.0 PASHA 73.37 ± 2.71 3.8h ± 1.0h 1.9x 45.0 ±30.1 One-epoch baseline 63.40 ± 9.91 1.1h ± 0.0h 6.7x 1.0 ± 0.0 Two-epoch baseline 64.61±10.81 1.7h ± 0.0h 4.2x 2.0 ± 0.0 Three-epoch baseline 68.74± 3.79 2.3h ± 0.1h 3.2x 3.0 ± 0.0 Five-epoch baseline 65.91 ± 3.99 3.7h ± 0.1h 2.0x 5.0 ± 0.0 Random baseline 36.94 ±31.05 0.0h ± 0.0h N/A 0.0 ± 0.0 B R EDUCTION FACTOR Table 8 shows the full set of results for our experiments with different reduction factors. The behaviour is the same across all cases. 12Published as a conference paper at ICLR 2023 Table 8: NASBench201 results with various reduction factors η. Dataset Reduction factor Approach Accuracy (%) Runtime Speedup factor Max resources CIFAR-10 η= 2 ASHA 93.88 ±0.27 3.6h ±1.1h 1.0x 200.0 ±0.0 PASHA 93.53 ±0.76 1.0h ±0.3h 3.5x 9.1 ±8.1 η= 4 ASHA 93.75 ±0.28 2.4h ±0.6h 1.0x 200.0 ±0.0 PASHA 93.65 ±0.65 1.1h ±0.5h 2.3x 32.3 ±50.2 CIFAR-100 η= 2 ASHA 71.67 ±0.84 3.8h ±1.0h 1.0x 200.0 ±0.0 PASHA 71.65 ±1.42 0.9h ±0.1h 4.2x 5.9 ±2.0 η= 4 ASHA 71.43 ±1.13 2.7h ±0.9h 1.0x 200.0 ±0.0 PASHA 72.09 ±1.22 1.0h ±0.4h 2.8x 25.1 ±49.0 ImageNet16-120 η= 2 ASHA 46.09 ±0.68 11.9h ±4.0h 1.0x 200.0 ±0.0 PASHA 45.35 ±1.52 2.8h ±0.6h 4.2x 9.3 ±7.1 η= 4 ASHA 45.43 ±0.98 7.9h ±3.0h 1.0x 200.0 ±0.0 PASHA 45.52 ±1.30 2.9h ±1.1h 2.8x 18.4 ±18.7 C E XPERIMENTS WITH ALTERNATIVE RANKING FUNCTIONS C.1 D ESCRIPTION PASHA employs a ranking function whose choice is completely arbitrary. In our main set of experiments we used soft ranking that automatically estimates the value of ϵby measuring noise in rankings. In this set of experiments we would like to evaluate different criteria to deﬁne the ranking of the candidates. We describe the functions considered next. C.1.1 D IRECT RANKING As a baseline, we study if we can use the simple ranking of conﬁgurations by predictive performance (e.g., sorting from the ones with the highest accuracy to the ones with the lowest). If any of the conﬁgurations change their order, we consider the ranking unstable and increase the resources. C.1.2 S OFT RANKING VARIATIONS We consider several variations of soft ranking. The ﬁrst variation is to ﬁx the value of the ϵparameter. We have considered values 0.01, 0.02, 0.025, 0.03, 0.05. The second set of variations aim to estimate the value of ϵautomatically, using various heuristics. The heuristics we have evaluated include: • Standard deviation: calculate the standard deviation of the considered performance measure (e.g. accuracy) of the conﬁgurations in the previous rung and set a multiple of it as the value of ϵ– we tried multiples of 1, 2 and 3. • Mean distance: value of ϵis set as the mean distance between the score of the conﬁgurations in the previous rung. • Median distance: similar to the mean distance, but using the median distance. There are various beneﬁts for estimating the value of ϵby measuring noise in rankings, as presented in our paper: • There is no need to set the value of ϵmanually. • Estimation of ϵhas an intuitive motivation that makes sense. • The value of ϵcan dynamically adapt to the different stages of hyperparameter optimization. • The approach works well in practice. C.1.3 R ANK BIASED OVERLAP (RBO) (W EBBER ET AL ., 2010) A score that can be broadly interpreted as a weighted correlation between rankings. We can specify how much we want to prioritize the top of the ranking using parameter pthat is between 0.0 and 1.0, 13Published as a conference paper at ICLR 2023 with a smaller value giving more priority to the top of the ranking. The best value is 1.0, while it gives value of 0.0 for rankings that are completely the opposite. We compute the RBO value and then compare it to the selected threshold t, increasing the resources if the value is less than the threshold. C.1.4 R ECIPROCAL RANK REGRET (RRR) A key insight is that conﬁgurations can be very similar to each other and differences in their rankings will not affect the quality of the found solution signiﬁcantly. To account for this we look at the objective values of the conﬁgurations (e.g. accuracy) and compute the relative regret that we would pay at the current rung if we would have assumed the ranking at the previous rung correct. We deﬁne reciprocal rank regret (RRR) as: RRR = n−1∑ i=0 (fi −f′ i) fi wi, where f represents the ordered scores in the top rung, f′represents the reordered scores from the top rung according to the second rung, nis the number of conﬁgurations in the top rung and pis the parameter that says how much attention to give to the top of the ranking. The weights wi sum to 1 and can be selected in different ways to e.g. give more priority to the top of the ranking. For example, we could use the following weights: wi = pi ∑n−1 i=0 pi The metric has an intuitive interpretation: it is the average relative regret with priority on top of the ranking. The best value of RRR is 0.0, while the worst possible value is 1.0. We also consider a version of RRR which considers the absolute values of the differences in the objectives - Absolute RRR (ARRR). We have evaluated these additional ranking functions using NASBench201 benchmark. C.2 R ESULTS We report the results in Table 9, 10 and 11. We see there are also several other variations that achieve strong results across a variety of datasets within NASBench201, most notably soft ranking 2σand variations based on RRR. In these cases we obtain similar performance as ASHA, but at a signiﬁcantly shorter time. We additionally also give a similar analysis in Table 12 (analogous to Table 4), where we analyse a selection of the most interesting ranking functions for the PD1 benchmark. 14Published as a conference paper at ICLR 2023 Table 9: NASBench201 – CIFAR-10 results for a variety of ranking functions. Approach Accuracy (%) Runtime Speedup factor Max resources ASHA 93.85 ± 0.25 3.0h ±0.6h 1.0x 200.0 ± 0.0 PASHA 93.57 ± 0.75 1.3h ±0.6h 2.3x 36.1 ±50.0 PASHA direct ranking 93.79 ± 0.26 2.7h ±0.6h 1.1x 198.4 ± 6.0 PASHA soft rankingϵ= 0.01 93.79± 0.26 2.6h ±0.5h 1.1x 194.3 ±21.2 PASHA soft rankingϵ= 0.02 93.78± 0.31 2.4h ±0.5h 1.2x 152.4 ±58.3 PASHA soft rankingϵ= 0.025 93.78± 0.31 2.3h ±0.5h 1.3x 144.5 ±59.4 PASHA soft rankingϵ= 0.03 93.78± 0.32 2.2h ±0.6h 1.3x 128.6 ±58.3 PASHA soft rankingϵ= 0.05 93.79± 0.49 1.8h ±0.7h 1.6x 76.0 ±66.0 PASHA soft ranking1σ 93.75± 0.32 2.4h ±0.5h 1.2x 186.4 ±35.2 PASHA soft ranking2σ 93.88± 0.28 1.9h ±0.5h 1.5x 132.7 ±68.7 PASHA soft ranking3σ 93.56± 0.69 0.9h ±0.3h 3.1x 16.2 ±19.9 PASHA soft ranking mean distance 93.73± 0.52 2.3h ±0.4h 1.3x 184.1 ±40.5 PASHA soft ranking median distance 93.82± 0.26 2.3h ±0.5h 1.3x 169.2 ±51.2 PASHA RBO p=1.0, t=0.5 93.49 ± 0.78 0.7h ±0.1h 4.2x 4.6 ± 6.0 PASHA RBO p=0.5, t=0.5 93.77 ± 0.35 2.2h ±0.6h 1.3x 144.0 ±71.2 PASHA RRR p=1.0, t=0.05 93.49 ± 0.78 0.7h ±0.0h 4.4x 3.0 ± 0.0 PASHA RRR p=0.5, t=0.05 93.76 ± 0.31 2.1h ±0.6h 1.4x 140.9 ±69.7 PASHA ARRR p=1.0, t=0.05 93.71 ± 0.35 2.4h ±0.4h 1.2x 179.0 ±42.9 PASHA ARRR p=0.5, t=0.05 93.81 ± 0.30 2.5h ±0.4h 1.2x 181.0 ±40.9 One-epoch baseline 93.30 ± 0.61 0.3h ±0.0h 8.5x 1.0 ± 0.0 Random baseline 72.88 ±19.20 0.0h ±0.0h N/A 0.0 ± 0.0 Table 10: NASBench201 – CIFAR-100 results for a variety of ranking functions. Approach Accuracy (%) Runtime (s) Speedup factor Max resources ASHA 71.69 ± 1.05 3.2h ±0.9h 1.0x 200.0 ± 0.0 PASHA 71.84 ± 1.41 0.9h ±0.4h 3.4x 20.5 ±48.3 PASHA direct ranking 71.69 ± 1.05 2.8h ±0.7h 1.1x 200.0 ± 0.0 PASHA soft rankingϵ= 0.01 71.55± 1.04 2.5h ±0.7h 1.3x 198.3 ± 6.5 PASHA soft rankingϵ= 0.02 70.94± 0.85 2.0h ±0.5h 1.6x 160.5 ±62.9 PASHA soft rankingϵ= 0.025 71.41± 1.15 1.5h ±0.7h 2.1x 88.3 ±74.4 PASHA soft rankingϵ= 0.03 71.00± 1.38 1.0h ±0.5h 3.2x 39.4 ±63.4 PASHA soft rankingϵ= 0.05 70.71± 1.66 0.7h ±0.0h 4.9x 3.0 ± 0.0 PASHA soft ranking1σ 71.56± 1.03 2.5h ±0.6h 1.3x 184.1 ±40.5 PASHA soft ranking2σ 71.14± 0.97 1.9h ±0.7h 1.7x 136.4 ±75.8 PASHA soft ranking3σ 71.63± 1.60 1.0h ±0.3h 3.3x 20.2 ±25.3 PASHA soft ranking mean distance 71.51± 0.99 2.4h ±0.5h 1.4x 189.8 ±30.3 PASHA soft ranking median distance 71.52± 0.98 2.4h ±0.6h 1.3x 189.5 ±30.6 PASHA RBO p=1.0, t=0.5 70.69 ± 1.67 0.7h ±0.1h 4.6x 3.8 ± 2.0 PASHA RBO p=0.5, t=0.5 71.51 ± 0.93 2.4h ±0.7h 1.3x 180.5 ±50.6 PASHA RRR p=1.0, t=0.05 70.71 ± 1.66 0.7h ±0.0h 4.9x 3.0 ± 0.0 PASHA RRR p=0.5, t=0.05 71.42 ± 1.51 1.2h ±0.5h 2.6x 39.3 ±51.4 PASHA ARRR p=1.0, t=0.05 70.80 ± 1.70 0.8h ±0.4h 3.8x 22.9 ±51.3 PASHA ARRR p=0.5, t=0.05 71.41 ± 1.05 1.8h ±0.6h 1.7x 110.0 ±68.7 One-epoch baseline 65.57 ± 5.53 0.3h ±0.0h 9.2x 1.0 ± 0.0 Random baseline 42.83 ±18.20 0.0h ±0.0h N/A 0.0 ± 0.0 15Published as a conference paper at ICLR 2023 Table 11: NASBench201 – ImageNet16-120 results for a variety of ranking functions. Approach Accuracy (%) Runtime (s) Speedup factor Max resources ASHA 45.63 ± 0.81 8.8h ±2.2h 1.0x 200.0 ± 0.0 PASHA 45.13 ± 1.51 2.9h ±1.7h 3.1x 21.3 ±48.1 PASHA direct ranking 45.63 ± 0.81 8.3h ±2.5h 1.1x 200.0 ± 0.0 PASHA soft rankingϵ= 0.01 45.52± 0.89 7.0h ±1.5h 1.3x 185.7 ±36.1 PASHA soft rankingϵ= 0.02 45.79± 1.16 4.4h ±1.4h 2.0x 71.4 ±50.8 PASHA soft rankingϵ= 0.025 46.01± 1.00 3.2h ±1.0h 2.8x 28.6 ±27.7 PASHA soft rankingϵ= 0.03 45.62± 1.48 2.4h ±0.7h 3.6x 11.0 ±10.0 PASHA soft rankingϵ= 0.05 44.90± 1.42 1.8h ±0.0h 5.0x 3.0 ± 0.0 PASHA soft ranking1σ 45.63± 0.89 6.5h ±1.3h 1.4x 177.1 ±44.2 PASHA soft ranking2σ 45.39± 1.22 4.5h ±1.4h 1.9x 91.2 ±58.0 PASHA soft ranking3σ 44.90± 1.42 1.8h ±0.0h 5.0x 3.0 ± 0.0 PASHA soft ranking mean distance 45.50± 1.12 6.2h ±1.5h 1.4x 157.7 ±54.7 PASHA soft ranking median distance 45.67± 0.95 6.3h ±1.6h 1.4x 156.3 ±52.2 PASHA RBO p=1.0, t=0.5 44.90 ± 1.42 1.8h ±0.0h 5.0x 3.0 ± 0.0 PASHA RBO p=0.5, t=0.5 45.24 ± 1.13 6.4h ±1.3h 1.4x 148.3 ±56.9 PASHA RRR p=1.0, t=0.05 44.90 ± 1.42 1.8h ±0.0h 5.0x 3.0 ± 0.0 PASHA RRR p=0.5, t=0.05 44.90 ± 1.42 1.8h ±0.0h 5.0x 3.0 ± 0.0 PASHA ARRR p=1.0, t=0.05 44.90 ± 1.42 1.8h ±0.0h 5.0x 3.0 ± 0.0 PASHA ARRR p=0.5, t=0.05 44.90 ± 1.42 1.8h ±0.0h 5.0x 3.0 ± 0.0 One-epoch baseline 41.42 ± 4.98 1.0h ±0.0h 8.8x 1.0 ± 0.0 Random baseline 20.75 ± 9.97 0.0h ±0.0h N/A 0.0 ± 0.0 Table 12: Results of the HPO experiments on WMT and ImageNet tasks from the PD1 benchmark, using a selection of the most interesting candidates for ranking functions. Mean and std of the best validation accuracy (or its equivalent as given in the PD1 benchmark). Dataset Approach Accuracy (%) Runtime Speedup factor Max resources WMT ASHA 62.72 ±1.41 43.7h±37.2h 1.0x 1357.4 ±80.4 PASHA 62.04 ±2.05 2.8h ±0.6h 15.5x 37.8 ±21.6 PASHA direct ranking 62.16 ±1.75 39.3h±38.3h 1.1x 1024.0 ±466.6 PASHA soft rankingϵ= 2.5% 62.09±2.04 1.3h ±0.4h 33.4x 4.2 ± 2.4 PASHA soft ranking2σ 62.52±2.18 1.1h ±0.1h 38.8x 3.0 ± 0.0 PASHA RBO p=0.5, t=0.5 61.44±1.23 6.7h ±7.8h 6.5x 147.6 ±113.2 PASHA RRR p=0.5, t=0.05 62.52±2.18 1.1h ±0.1h 38.8x 3.0 ± 0.0 One-epoch baseline 62.36 ±1.40 0.6h ±0.0h 67.3x 1.0 ± 0.0 Random baseline 33.93 ±21.96 0.0h ±0.0h N/A 0.0 ± 0.0 ImageNet ASHA 75.10 ±2.03 7.3h ±1.2h 1.0x 251.0 ± 0.0 PASHA 73.37 ±2.71 3.8h ±1.0h 1.9x 45.0 ±30.1 PASHA direct ranking 75.10 ±2.03 6.8h ±0.7h 1.1x 247.8 ± 3.9 PASHA soft rankingϵ= 2.5% 74.73±1.99 4.3h ±2.5h 1.7x 140.4 ±112.8 PASHA soft ranking2σ 75.82±0.82 5.0h ±1.6h 1.5x 133.0 ±96.8 PASHA RBO p=0.5, t=0.5 74.80±2.19 4.4h ±2.1h 1.6x 117.4 ±109.4 PASHA RRR p=0.5, t=0.05 74.98±2.12 1.6h ±0.0h 4.7x 3.0 ± 0.0 One-epoch baseline 63.40 ±9.91 1.1h ±0.0h 6.7x 1.0 ± 0.0 Random baseline 36.94 ±31.05 0.0h ±0.0h N/A 0.0 ± 0.0 16Published as a conference paper at ICLR 2023 D A DDITIONAL RESULTS ON LCB ENCH We additionally evaluate PASHA on the LCBench benchmark (Zimmer et al., 2021) where only modest speedups can be expected due to a small number of epochs (and hence rungs) available. LCBench limits the maximum amount of resources per conﬁguration to 50 epochs, so when using and setting the minimum resource level to 1 epoch, it is a challenging testbed for an algorithm like PASHA. The hyperparameters optimized include number of layers ∈[1,5], max. number of units ∈[64,1024] (log scale), batch size ∈[16,512] (log scale), learning rate ∈ [ 10−4,10−1] (log scale), weight decay ∈ [ 10−5,10−1] , momentum ∈[0.1,0.99] and max. value of dropout ∈[0.0,1.0]. Similarly as in our other experiments, we use η= 3and stop after sampling 256 candidates. Overall, the results in Table 13 conﬁrm an accuracy level on-par with ASHA. While, as expected, the speedup is reduced compared to the experiments on NASBench, in several cases PASHA achieves a 20+% speedup with peaks around 40%. If only a small number of epochs is sufﬁcient for training the model on the given dataset, then HPO can be performed on a sub-epoch basis, e.g. deﬁning the rung levels in terms of iterations instead of epochs. PASHA would then be able to give a large speedup even in cases with smaller numbers of epochs – an example of which is LCBench. Table 13: Results of the HPO experiments on the LCBench benchmark. Mean and std of the test accuracy across ﬁve random seeds. PASHA achieves similar accuracies as ASHA, but gives only modest speedups because of the limited number of rung levels and opportunities to stop the HPO early. To enable large speedup from PASHA, we could redeﬁne the rung levels in terms of neural network weights updates rather than epochs. Dataset ASHA accuracy (%) PASHA accuracy (%) PASHA speedup APSFailure 97.52 ± 0.92 97.01 ±0.75 1.3x Amazonemployeeaccess 94.01 ± 0.40 94.21 ±0.00 1.1x Australian 83.35 ± 0.33 83.35 ±0.51 1.1x Fashion-MNIST 86.70 ± 1.87 86.34 ±1.32 1.1x KDDCup09appetency 98.22 ± 0.00 98.22 ±0.00 1.1x MiniBooNE 86.13 ± 1.57 86.24 ±1.62 1.4x Adult 79.14 ± 0.85 79.14 ±0.85 1.2x Airlines 59.57 ± 1.32 59.22 ±0.76 1.4x Albert 64.31 ± 0.99 64.23 ±0.61 1.2x Bank-marketing 88.34 ± 0.07 88.38 ±0.00 1.2x Blood-transfusion-service-center 79.92 ± 0.20 76.99 ±6.00 1.1x Car 86.60 ± 6.41 86.60 ±6.41 1.1x Christine 71.05 ± 1.17 70.15 ±1.85 1.2x Cnae-9 94.10 ± 0.31 94.44 ±0.11 1.0x Connect-4 62.28 ± 6.87 65.69 ±0.04 1.2x Covertype 59.76 ± 3.24 61.64 ±1.64 1.2x Credit-g 70.30 ± 0.84 70.79 ±0.68 1.1x Dionis 64.58 ± 9.89 64.58 ±9.89 1.1x Fabert 56.11 ±10.89 53.47 ±9.75 1.1x Helena 19.16 ± 3.20 19.16 ±3.20 1.1x Higgs 66.48 ± 3.16 66.48 ±3.16 1.1x Jannis 58.92 ± 2.38 59.63 ±2.81 1.4x Jasmine 75.85 ± 0.36 75.55 ±0.68 1.0x Junglechess2pcsrawendgamecomplete 72.86 ± 4.69 74.94 ±7.84 1.3x Kc1 80.32 ± 4.37 80.86 ±3.37 1.2x Kr-vs-kp 92.50 ± 3.93 90.95 ±4.70 1.0x Mfeat-factors 98.21 ± 0.15 98.15 ±0.15 1.1x Nomao 94.12 ± 0.60 94.25 ±0.64 1.1x Numerai28.6 52.03 ± 0.55 52.30 ±0.24 1.3x Phoneme 76.65 ± 2.65 75.42 ±2.87 1.1x Segment 83.15 ± 2.54 83.15 ±2.54 1.0x Sylvine 90.57 ± 1.87 90.89 ±2.04 1.0x Vehicle 71.76 ± 2.57 71.76 ±2.57 1.1x V olkert 50.72 ± 1.91 50.72 ±1.91 1.1x 17Published as a conference paper at ICLR 2023 E I NVESTIGATION WITH VARIABLE MAXIMUM RESOURCES We analyse the impact of variable maximum resources (number of epochs) on how large speedup PASHA provides over ASHA. More speciﬁcally, we change the maximum resources available for ASHA and also the upper boundary on maximum resources for PASHA. We utilize NASBench201 benchmark for these experiments and set the number of epochs to 200 (default) or 50 (other details are the same as earlier). The results in Table 14 conﬁrm that PASHA leads to larger speedups when there are more epochs (and rung levels) available. This analysis also explains the modest speedups on LCBench analysed earlier. If the model is trained for a small number of epochs, it is worth redesigning the HPO so that there are more rung levels available, enabling PASHA to give larger speedups. This can be achieved by using sub-epoch resource levels – specifying the rung levels and the minimum resources in terms of the number of iterations (neural network weights updates). Based on the results observed across various benchmarks, we would recommend having at least 5 rung levels in ASHA, with more rung levels leading to larger speedups from PASHA over ASHA. Table 14: NASBench201 results. PASHA leads to larger speedups if the models are trained with more epochs. Dataset Number of epochs Approach Accuracy (%) Runtime Speedup factor Max resources CIFAR-10 200 ASHA 93.85 ±0.25 3.0h±0.6h 1.0x 200.0 ±0.0 PASHA 93.57±0.75 1.3h±0.6h 2.3x 36.1 ±50.0 50 ASHA 93.78 ±0.39 1.8h±0.2h 1.0x 50.0 ±0.0 PASHA 93.58±0.75 1.2h±0.4h 1.5x 22.0 ±16.8 CIFAR-100 200 ASHA 71.69 ±1.05 3.2h±0.9h 1.0x 200.0 ±0.0 PASHA 71.84±1.41 0.9h±0.4h 3.4x 20.5 ±48.3 50 ASHA 72.24 ±0.87 1.8h±0.3h 1.0x 50.0 ±0.0 PASHA 71.91±1.32 0.9h±0.3h 2.0x 10.5 ±12.1 ImageNet16-120 200 ASHA 45.63 ±0.81 8.8h±2.2h 1.0x 200.0 ±0.0 PASHA 45.13±1.51 2.9h±1.7h 3.1x 21.3 ±48.1 50 ASHA 45.97 ±0.99 5.2h±0.7h 1.0x 50.0 ±0.0 PASHA 45.09±1.52 2.7h±1.0h 1.9x 11.3 ±11.7 F A NALYSIS OF LEARNING CURVES We analyse the NASBench201 learning curves in Figure 3 and 4. To make the analysis realistic and easier to grasp, we ﬁrst sample a random subset of 256 conﬁgurations, similarly as we do for our NAS experiments. Figure 3 shows the learning curves of the top three conﬁgurations (selected in terms of their ﬁnal performance). We see that these learning curves are very close to each other and frequently cross due to noise in the training, allowing us to estimate a meaningful value of ϵ parameter (conﬁgurations that repeatedly swap their order are very likely to be similarly good, so we can select any of them because the goal is to ﬁnd a strong conﬁguration quickly rather than the very best one). Figure 4 shows all learning curves from the same random sample of 256 conﬁgurations. In this case we can see that the learning curves are relatively well-behaved (especially the ones at the top), and any exceptions are rare. 18Published as a conference paper at ICLR 2023 0 50 100 150 200 Epoch 40 50 60 70 80 90Validation accuracy (%) CIFAR-10 0 50 100 150 200 Epoch 20 40 60Validation accuracy (%) CIFAR-100 0 50 100 150 200 Epoch 10 20 30 40Validation accuracy (%) ImageNet16-120 Figure 3: Analysis of how the learning curves of the top three conﬁgurations (in terms of ﬁnal validation accuracy; from a random sample of 256 conﬁgurations) evolve across epochs. We see that such similar conﬁgurations frequently change their ranks, enabling us to calculate a meaningful value of ϵparameter. 0 50 100 150 200 Epoch 20 40 60 80Validation accuracy (%) CIFAR-10 0 50 100 150 200 Epoch 0 20 40 60Validation accuracy (%) CIFAR-100 0 50 100 150 200 Epoch 0 10 20 30 40Validation accuracy (%) ImageNet16-120 Figure 4: Analysis of what the learning curves look like for a random sample of 256 conﬁgurations. We see that the learning curves are relatively well-behaved (especially the ones at the top), and any exceptions are rare. G I NVESTIGATION OF HOW VALUE ϵEVOLVES We analyse how the value of ϵthat is used for calculating soft ranking develops during the HPO process. We show the results in Figure 5 for the three different datasets available in NASBench201 (taking one seed). The results show the obtained values of ϵare relatively small. 0 500 1000 1500 2000 Number of updates received 0.00 0.05 0.10 0.15 0.20 0.25 0.30Value of  CIFAR-10 0 200 400 600 Number of updates received 0.00 0.05 0.10 0.15Value of  CIFAR-100 0 200 400 600 800 Number of updates received 0.00 0.02 0.04 0.06Value of  ImageNet16-120 Figure 5: Analysis of how the value of ϵevolves as we receive additional updates about the perfor- mances of candidate conﬁgurations. Note that most of the updates are obtained in the top rung due to how multi-ﬁdelity methods work. 19Published as a conference paper at ICLR 2023 H I NVESTIGATION OF PERCENTILE VALUE N We investigate the impact of using various percentile valuesN used for estimating the value of ϵin Table 15. The intuition is that we want to take some value on the top end rather than the maximum distance in case there are some outliers. We see that the results are relatively stable, even though larger value of N can lead to further speedups. However, from the point of view of a practitioner we would still take N = 90in case there are any outliers in the speciﬁc new use-case. Table 15: NASBench201 results. PASHA leads to large improvements in runtime, while achieving similar accuracy as ASHA. Investigation of various percentile values ( N) to use for calculating parameter ϵ. Dataset Approach Accuracy (%) Runtime Speedup factor Max resources CIFAR-10 ASHA 93.85 ±0.25 3.0h ±0.6h 1.0x 200.0 ±0.0 PASHAN= 100%93.70±0.61 1.0h ±0.4h 3.0x 13.8 ±19.5 PASHAN= 95% 93.64±0.59 1.0h ±0.4h 2.8x 15.4 ±19.5 PASHAN= 90% 93.57±0.75 1.3h ±0.6h 2.3x 36.1 ±50.0 PASHAN= 80% 93.86±0.53 1.5h ±0.6h 1.9x 60.9 ±60.7 One-epoch baseline 93.30±0.61 0.3h ±0.0h 8.5x 1.0 ±0.0 Random baseline 72.88±19.20 0.0h ±0.0h N/A 0.0 ±0.0 CIFAR-100 ASHA 71.69 ±1.05 3.2h ±0.9h 1.0x 200.0 ±0.0 PASHAN= 100%71.84±1.41 0.8h ±0.1h 3.9x 6.6 ±2.9 PASHAN= 95% 71.84±1.41 0.8h ±0.1h 3.9x 6.6 ±2.9 PASHAN= 90% 71.91±1.32 0.9h ±0.3h 3.5x 12.6 ±19.2 PASHAN= 80% 71.78±1.31 1.2h ±0.6h 2.6x 56.0 ±76.2 One-epoch baseline 65.57±5.53 0.3h ±0.0h 9.2x 1.0 ±0.0 Random baseline 42.83±18.20 0.0h ±0.0h N/A 0.0 ±0.0 ImageNet16-120 ASHA 45.63 ±0.81 8.8h ±2.2h 1.0x 200.0 ±0.0 PASHAN= 100%45.09±1.61 2.3h ±0.4h 3.7x 7.0 ±2.8 PASHAN= 95% 45.26±1.58 2.4h ±0.4h 3.7x 7.4 ±2.7 PASHAN= 90% 45.13±1.51 2.9h ±1.7h 3.1x 21.3 ±48.1 PASHAN= 80% 45.36±1.38 3.6h ±1.2h 2.5x 40.5 ±47.7 One-epoch baseline 41.42±4.98 1.0h ±0.0h 8.8x 1.0 ±0.0 Random baseline 20.75±9.97 0.0h ±0.0h N/A 0.0 ±0.0 20",
      "meta_data": {
        "arxiv_id": "2207.06940v2",
        "authors": [
          "Ondrej Bohdal",
          "Lukas Balles",
          "Martin Wistuba",
          "Beyza Ermis",
          "Cédric Archambeau",
          "Giovanni Zappella"
        ],
        "published_date": "2022-07-14T14:06:15Z",
        "pdf_url": "https://arxiv.org/pdf/2207.06940v2.pdf",
        "github_url": "https://github.com/ondrejbohdal/pasha"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the challenge of high computational cost in Hyperparameter Optimization (HPO) and Neural Architecture Search (NAS), especially for models trained on large datasets. It proposes PASHA (Progressive Asynchronous Successive Halving), an extension of ASHA, that dynamically allocates maximum resources during the tuning procedure. The main contributions are: 1) Introducing PASHA, a new approach that dynamically selects the maximum resources for HPO or NAS, up to a certain budget. 2) Demonstrating empirically that PASHA significantly speeds up HPO and NAS (e.g., 2.3x to 15.5x faster than ASHA) without sacrificing predictive performance. 3) Showing that PASHA can be successfully combined with sample-efficient strategies like Bayesian Optimization, highlighting its generality.",
        "methodology": "PASHA is an extension of ASHA, inspired by the 'doubling trick' concept. It starts with a small initial maximum amount of resources and progressively increases them only if the ranking of configurations in the top two rungs (rounds of promotion) has not stabilized. The key idea is to stop early when rankings are stable. To handle noise in the training process, PASHA employs a 'soft ranking' mechanism, where configurations are considered equivalent if their performance difference is smaller than a dynamically estimated value ϵ. This ϵ value is automatically estimated by identifying pairs of configurations that repeatedly swap their ranks across different resource levels (criss-crossing behavior) and calculating it as the N-th (90th in experiments) percentile of the performance differences of these criss-crossing pairs.",
        "experimental_setup": "The evaluation consists of two phases: 1) Running the hyperparameter optimizer until 256 candidate configurations are evaluated. 2) Retraining the best identified configuration from scratch, typically on the training set (or combined training/validation in practice). Experiments were conducted on two sets of tasks: NAS problems using the NASBench201 benchmark (for CIFAR-10, CIFAR-100, and ImageNet16-120 datasets) and HPO problems on two large-scale tasks from the PD1 benchmark (WMT15 German-English and ImageNet, using xformer and ResNet50 models respectively). The experiments used 4 workers for parallel asynchronous evaluations, a default reduction factor η=3, and an N=90th percentile for ϵ calculation. Results were averaged over multiple random seeds (5 for scheduler, 3 for NASBench201; 5 for HPO on PD1). Baselines for comparison included ASHA, 'one-epoch baseline', 'random baseline', and additional two, three, and five-epoch baselines. PASHA was also tested with Bayesian Optimization searchers (MOBSTER) and various alternative ranking functions.",
        "limitations": "The benefits of PASHA are reduced in scenarios where the number of rungs (resource levels, e.g., epochs) is small, as it provides fewer opportunities for the algorithm to interrupt tuning and achieve large speedups. This limitation was observed on benchmarks like LCBench. Public benchmarks often fix minimum and maximum resource levels, which can further constrain PASHA's ability to demonstrate its full potential. For practical usage, the authors recommend having a maximum amount of resources at least 100 times larger than the minimum amount when using a reduction factor η=3. This can be mitigated by defining resources with higher granularity, such as in terms of gradient updates instead of epochs.",
        "future_research_directions": "Future work could investigate how the definition of rungs and resource levels impacts the decisions of multi-fidelity algorithms, including PASHA, to further optimize their performance. Additionally, the authors plan to test combinations of PASHA with transfer-learning techniques for multi-fidelity HPO and NAS, such as RUSH, to achieve even greater reductions in tuning time.",
        "experimental_code": "class PASHARungSystem(PromotionRungSystem):\n    \"\"\"\n    Implements PASHA algorithm. It is very similar to ASHA, but it progressively\n    extends the maximum resources if the ranking in the top two current rungs changes.\n\n    A report introducing and evaluating the approach is available at\n    TODO: add link\n    \"\"\"\n\n    def __init__(\n        self,\n        rung_levels,\n        promote_quantiles,\n        metric,\n        mode,\n        resource_attr,\n        max_t,\n        ranking_criterion,\n        epsilon,\n        epsilon_scaling,\n    ):\n        super().__init__(\n            rung_levels, promote_quantiles, metric, mode, resource_attr, max_t\n        )\n        self.ranking_criterion = ranking_criterion\n        # define the index of the current top rung, starting from 1 for the lowest rung\n        #\n        self.current_rung_idx = 2\n        self.rung_levels = rung_levels\n\n        # initialize current maximum resources\n        self.current_max_t = rung_levels[self.current_rung_idx - 1]\n\n        self.epsilon = epsilon\n        self.epsilon_scaling = epsilon_scaling\n        if ranking_criterion == 'soft_ranking_auto':\n            self.per_epoch_results = {}\n            self.epoch_to_trials = {}\n            self.current_max_epoch = -1\n\n    # overriding the method in HB promotion to accomodate the increasing max resources level\n    def _effective_max_t(self):\n        return self.current_max_t\n\n    def _get_top_rungs_rankings(self, num_rungs=2):\n        \"\"\"\n        Look at the current top two rungs and get the rankings of the configurations.\n        The rungs can be empty, in which case we will return a list with 0 or 1 elements.\n        Normally the list will include rankings for both rungs.\n\n        The rankings are stored as a list of tuples (trial_id, rank, value).\n\n        Lower values have lower ranks, starting from zero. For example:\n        [('0', 0, 10.0), ('1', 3, 19.6), ('2', 2, 14.3), ('3', 1, 11.6)]\n\n        :param num_rungs: int describing how many top rungs to return\n        :return: rankings\n            List of at most two lists with tuple(trial_id, rank, score)\n        \"\"\"\n        rankings = []\n        # be careful, self._rungs is ordered with the highest resources level in the beginning\n        rungs = [self._rungs[-self.current_rung_idx + e] for e in range(num_rungs)]\n        for rung in rungs:\n            if rung.data != {}:\n                trial_ids = rung.data.keys()\n                values = []\n                for trial_id in trial_ids:\n                    values.append(rung.data[trial_id][0])\n                # order specifies where the value should be placed in the sorted list\n                values_order = np.array(values).argsort()\n                # calling argsort on the order will give us the ranking\n                values_ranking = values_order.argsort()\n                ranking = list(zip(trial_ids, values_ranking, values))\n\n                rankings.append(ranking)\n\n        return rankings\n\n    def _get_sorted_top_rungs(self, rankings):\n        \"\"\"\n        Sort the configurations in the top rung and the previous rung.\n        Filter out the configurations from the previous rung that\n        are not in the top rung.\n\n        :param rankings: list of at most two lists with tuple(trial_id, rank, score)\n        return: sorted_top_rung, sorted_previous_rung\n        \"\"\"\n        # filter only the relevant configurations from the earlier rung\n        top_rung_keys = set([e[0] for e in rankings[0]])\n        corresponding_previous_rung_trials = filter(\n            lambda e: e[0] in top_rung_keys, rankings[1]\n        )\n        # if we try to maximize the objective, we need to reverse the ranking\n        if self._mode == \"max\":\n            reverse = True\n        else:\n            reverse = False\n\n        sorted_top_rung = sorted(rankings[0], key=lambda e: e[1], reverse=reverse)\n        sorted_previous_rung = sorted(\n            corresponding_previous_rung_trials, key=lambda e: e[1], reverse=reverse\n        )\n        return sorted_top_rung, sorted_previous_rung\n\n    def _evaluate_soft_ranking(self, sorted_top_rung, sorted_previous_rung) -> bool:\n        \"\"\"\n        Soft ranking creates groups of similarly performing configurations\n        and increases the resources only if a configuration goes outside of\n        its group.\n\n        :param sorted_top_rung: list of tuple(trial_id, rank, score)\n        :param sorted_previous_rung: list of tuple(sorted_top_rung, rank, score)\n        :return: keep_current_budget\n        \"\"\"\n        keep_current_budget = True\n        if len(sorted_previous_rung) < 2:\n            epsilon = 0.0\n        elif self.ranking_criterion == \"soft_ranking_std\":\n            epsilon = (\n                np.std([e[2] for e in sorted_previous_rung]) * self.epsilon_scaling\n            )\n        elif (\n            self.ranking_criterion == \"soft_ranking_median_dst\"\n            or self.ranking_criterion == \"soft_ranking_mean_dst\"\n        ):\n            scores = [e[2] for e in sorted_previous_rung]\n            distances = [\n                abs(e1 - e2)\n                for idx1, e1 in enumerate(scores)\n                for idx2, e2 in enumerate(scores)\n                if idx1 != idx2\n            ]\n            if self.ranking_criterion == \"soft_ranking_mean_dst\":\n                epsilon = np.mean(distances) * self.epsilon_scaling\n            elif self.ranking_criterion == \"soft_ranking_median_dst\":\n                epsilon = np.median(distances) * self.epsilon_scaling\n            else:\n                raise ValueError(\n                    \"Ranking criterion {} is not supported\".format(\n                        self.ranking_criterion\n                    )\n                )\n        else:\n            epsilon = self.epsilon\n\n        # create groups of configurations with similar performance\n        previous_rung_groups = []\n        for idx, item in enumerate(sorted_previous_rung):\n            current_rung_group = [item[0]]\n            # add configurations that are after the current configuration\n            for idx_after in range(idx + 1, len(sorted_previous_rung)):\n                new_item = sorted_previous_rung[idx_after]\n\n                if self._mode == \"max\":\n                    if new_item[2] < item[2] - epsilon:\n                        break\n                else:\n                    if new_item[2] > item[2] + epsilon:\n                        break\n                current_rung_group.append(new_item[0])\n            # add configurations that are before the current configuration\n            for idx_before in range(idx - 1, -1, -1):\n                new_item = sorted_previous_rung[idx_before]\n                if self._mode == \"max\":\n                    if new_item[2] > item[2] + epsilon:\n                        break\n                else:\n                    if new_item[2] < item[2] - epsilon:\n                        break\n                current_rung_group.append(new_item[0])\n            previous_rung_groups.append(set(current_rung_group))\n\n        # evaluate if a configuration has switched its group\n        for idx, item in enumerate(sorted_top_rung):\n            if item[0] not in previous_rung_groups[idx]:\n                keep_current_budget = False\n                break\n\n        return keep_current_budget\n\n    def _update_epsilon(self):\n        \"\"\"\n        This function is used to automatically calculate the value of epsilon.\n        It finds the configurations which swapped their rankings across rungs\n        and estimates the value of epsilon as the 90th percentile of the difference\n        between their performance in the previous rung.\n\n        The original value of epsilon is kept if no suitable configurations were found.\n        \"\"\"\n\n        seen_pairs = set()\n        noisy_cfg_distances = []\n        top_epoch = min(self.current_max_epoch, self._rungs[-self.current_rung_idx].level)\n        bottom_epoch = min(self._rungs[-self.current_rung_idx+1].level, self.current_max_epoch)\n        for epoch in range(top_epoch, bottom_epoch, -1):\n            if len(self.epoch_to_trials[epoch]) > 1:\n                for pair in itertools.combinations(self.epoch_to_trials[epoch], 2):\n                    c1, c2 = pair[0], pair[1]\n                    if (c1, c2) not in seen_pairs:\n                        seen_pairs.add((c1, c2))\n                        p1, p2 = self.per_epoch_results[c1][epoch], self.per_epoch_results[c2][epoch]\n                        cond = p1 > p2\n\n                        opposite_order = False\n                        same_order_after_opposite = False\n                        # now we need to check the earlier epochs to see if at any point they had a different order\n                        for prev_epoch in range(epoch - 1, 0, -1):\n                            pp1, pp2 = self.per_epoch_results[c1][prev_epoch], self.per_epoch_results[c2][prev_epoch]\n                            p_cond = pp1 > pp2\n                            if p_cond == (not cond):\n                                opposite_order = True\n                            if opposite_order and p_cond == cond:\n                                same_order_after_opposite = True\n                                break\n\n                        if opposite_order and same_order_after_opposite:\n                            noisy_cfg_distances.append(abs(p1 - p2))\n\n        if len(noisy_cfg_distances) > 0:\n            self.epsilon = np.percentile(noisy_cfg_distances, 90)\n            if str(self.epsilon) == 'nan':\n                raise ValueError('Epsilon became nan') \n\n    def _update_per_epoch_results(self, trial_id, result):\n        if trial_id not in self.per_epoch_results:\n            self.per_epoch_results[trial_id] = {}\n        self.per_epoch_results[trial_id][result[self._resource_attr]] = result[self._metric]\n\n        if result[self._resource_attr] not in self.epoch_to_trials:\n            self.epoch_to_trials[result[self._resource_attr]] = set() \n        self.epoch_to_trials[result[self._resource_attr]].add(trial_id)\n\n        if result[self._resource_attr] > self.current_max_epoch:\n            self.current_max_epoch = result[self._resource_attr]\n\n    def _decide_resource_increase(self, rankings) -> bool:\n        \"\"\"\n        Decide if to increase the resources given the current rankings.\n        Currently we look at the rankings and if elements in the first list\n        have the same order also in the second list, we keep the current resource\n        budget. If the rankings are different, we will increase the budget.\n\n        The rankings can only be incorrect if we have rankings for both rungs.\n\n        :param rankings: list of at most two lists with tuple(trial_id, rank, score)\n        return: not keep_current_budget\n        \"\"\"\n        if len(rankings) == 2:\n            sorted_top_rung, sorted_previous_rung = self._get_sorted_top_rungs(rankings)\n        else:\n            return False\n\n        keep_current_budget = self._evaluate_soft_ranking(\n            sorted_top_rung, sorted_previous_rung\n        )\n\n        return not keep_current_budget\n\n    def on_task_report(self, trial_id: str, result: dict, skip_rungs: int) -> dict:\n        \"\"\"\n        Apart from calling the superclass method, we also check the rankings\n        and decides if to increase the current maximum resources.\n        \"\"\"\n        ret_dict = super().on_task_report(trial_id, result, skip_rungs)\n\n        if self.ranking_criterion == \"soft_ranking_auto\":\n            self._update_per_epoch_results(trial_id, result)\n            self._update_epsilon()\n\n        # check the rankings and decide if to increase the current maximum resources\n        rankings = self._get_top_rungs_rankings(num_rungs=2)\n        increase_resources = self._decide_resource_increase(rankings)\n\n        # we have a maximum amount of resources that PASHA can use\n        # the resources should not increase indefinitely\n        if increase_resources:\n            if self.current_rung_idx < len(self._rungs):\n                self.current_rung_idx += 1\n                # be careful, self.rung_levels is ordered with the highest resources level at the end\n                # moreover, since we use rung levels for counting both from the beginning and from the end of the list\n                # we need to remember that counting from the beginning it's zero indexed\n                self.current_max_t = self.rung_levels[self.current_rung_idx - 1]\n            else:\n                self.current_max_t = self.max_t\n\n        return ret_dict",
        "experimental_info": "PASHA is an extension of ASHA that dynamically adjusts the maximum amount of resources (`current_max_t`) based on the stability of rankings between configurations in the top two rungs. The ranking stability is assessed using a 'soft ranking' mechanism, where configurations are considered equivalent if their performance difference is smaller than an `epsilon` value.\n\nKey configurable parameters for PASHA, set via `rung_system_kwargs` when initializing the `HyperbandScheduler` (with `type=\"pasha\"`), include:\n\n- **`ranking_criterion`**: Determines how ranking stability is evaluated. Supported options are:\n    - `'soft_ranking'`: Uses a fixed `epsilon` value.\n    - `'soft_ranking_std'`, `'soft_ranking_median_dst'`, `'soft_ranking_mean_dst'`: These dynamically estimate `epsilon` based on statistical properties (standard deviation, median distance, mean distance) of performance differences among configurations, with an optional `epsilon_scaling` factor.\n    - `'soft_ranking_auto'`: Dynamically estimates `epsilon` by identifying configurations that repeatedly swap ranks across different resource levels ('criss-crossing pairs'). `epsilon` is then calculated as the 90th percentile of the performance differences of these criss-crossing pairs. When this criterion is selected, a provided `epsilon` value in `rung_system_kwargs` is ignored.\n\n- **`epsilon`**: A float value used directly for the `'soft_ranking'` criterion to define the performance difference threshold for considering configurations equivalent.\n\n- **`epsilon_scaling`**: A float factor applied to the dynamically estimated `epsilon` when using criteria like `'soft_ranking_std'`, `'soft_ranking_median_dst'`, `'soft_ranking_mean_dst'`, or `'soft_ranking_auto'`.\n\n**Operational Details:**\n- The algorithm starts by considering the top two current rungs (`num_rungs=2` in `_get_top_rungs_rankings`).\n- Initially, `current_rung_idx` is set to 2, and `current_max_t` is set to `rung_levels[1]` (the second rung level from the lowest, assuming `rung_levels` is 0-indexed and sorted ascendingly for levels).\n- Resources (`current_max_t`) are increased by moving to the next rung level (`self.current_rung_idx += 1` and `self.current_max_t = self.rung_levels[self.current_rung_idx - 1]`) only if the ranking is deemed unstable (`_decide_resource_increase` returns True). This process continues up to the `max_t` defined for Hyperband.\n- When `ranking_criterion='soft_ranking_auto'`, the `_update_epsilon` method automatically computes `epsilon` as the 90th percentile of the absolute performance differences observed in configurations that exhibit 'criss-crossing' behavior (repeatedly swap ranks across different resource levels)."
      }
    },
    {
      "title": "Reshuffling Resampling Splits Can Improve Generalization of Hyperparameter Optimization",
      "abstract": "Hyperparameter optimization is crucial for obtaining peak performance of\nmachine learning models. The standard protocol evaluates various hyperparameter\nconfigurations using a resampling estimate of the generalization error to guide\noptimization and select a final hyperparameter configuration. Without much\nevidence, paired resampling splits, i.e., either a fixed train-validation split\nor a fixed cross-validation scheme, are often recommended. We show that,\nsurprisingly, reshuffling the splits for every configuration often improves the\nfinal model's generalization performance on unseen data. Our theoretical\nanalysis explains how reshuffling affects the asymptotic behavior of the\nvalidation loss surface and provides a bound on the expected regret in the\nlimiting regime. This bound connects the potential benefits of reshuffling to\nthe signal and noise characteristics of the underlying optimization problem. We\nconfirm our theoretical results in a controlled simulation study and\ndemonstrate the practical usefulness of reshuffling in a large-scale, realistic\nhyperparameter optimization experiment. While reshuffling leads to test\nperformances that are competitive with using fixed splits, it drastically\nimproves results for a single train-validation holdout protocol and can often\nmake holdout become competitive with standard CV while being computationally\ncheaper.",
      "full_text": "Reshuffling Resampling Splits Can Improve Generalization of Hyperparameter Optimization Thomas Nagler∗ Lennart Schneider∗ Bernd Bischl Matthias Feurer t.nagler@lmu.de Department of Statistics, LMU Munich Munich Center for Machine Learning (MCML) Abstract Hyperparameter optimization is crucial for obtaining peak performance of ma- chine learning models. The standard protocol evaluates various hyperparameter configurations using a resampling estimate of the generalization error to guide opti- mization and select a final hyperparameter configuration. Without much evidence, paired resampling splits, i.e., either a fixed train-validation split or a fixed cross- validation scheme, are often recommended. We show that, surprisingly, reshuffling the splits for every configuration often improves the final model’s generalization performance on unseen data. Our theoretical analysis explains how reshuffling affects the asymptotic behavior of the validation loss surface and provides a bound on the expected regret in the limiting regime. This bound connects the potential benefits of reshuffling to the signal and noise characteristics of the underlying optimization problem. We confirm our theoretical results in a controlled simula- tion study and demonstrate the practical usefulness of reshuffling in a large-scale, realistic hyperparameter optimization experiment. While reshuffling leads to test performances that are competitive with using fixed splits, it drastically improves results for a single train-validation holdout protocol and can often make holdout become competitive with standard CV while being computationally cheaper. 1 Introduction Hyperparameters have been shown to strongly influence the performance of machine learning models (van Rijn & Hutter, 2018; Probst et al., 2019). The primary goal of hyperparameter optimization (HPO; also called tuning) is the identification and selection of a hyperparameter configuration (HPC) that minimizes the estimated generalization error (Feurer & Hutter, 2019; Bischl et al., 2023). Typically, this task is challenged by the absence of a closed-form mathematical description of the objective function, the unavailability of an analytic gradient, and the large cost to evaluate HPCs, categorizing HPO as a noisy, black-box optimization problem. An HPC is evaluated via resampling, such as a holdout split or M-fold cross-validation (CV), during tuning. These resampling splits are usually constructed in a fixed and instantiated manner, i.e., the same training and validation splits are used for the internal evaluation of all configurations. On the one hand, this is an intuitive approach, as it should facilitate a fair comparison between HPCs and reduce the variance in the comparison.1 On the other hand, such a fixing of train and validation splits might steer the optimization, especially after a substantial budget of evaluations, towards favoring HPCs ∗Equal contribution. 1This approach likely originates from the concept of paired statistical tests and the resulting variance reduction, but in our literature search we did not find any references discussing this in the context of HPO. For example, when comparing the performance of two classifiers on one dataset, paired tests are commonly 38th Conference on Neural Information Processing Systems (NeurIPS 2024). arXiv:2405.15393v2  [stat.ML]  7 Nov 2024which are specifically tailored to the chosen splits. Such and related effects, where we \"overoptimize\" the validation performance without effective reward in improved generalization performance have been sometimes dubbed \"overtuning\" or \"oversearching\". For a more detailed discussion of this topic, including related work, see Section 5 and Appendix B. The practice of reshuffling resampling splits during HPO is generally neither discussed in the scientific literature nor HPO software tools.2 To the best of our knowledge, only Lévesque (2018) investigated reshuffling train-validation splits for every new HPC. For both holdout and M-fold CV using reshuffled resampling splits resulted in, on average, slightly lower generalization error when used in combination with Bayesian optimization (BO, Garnett, 2023) or CMA-ES (Hansen & Ostermeier, 2001) as HPO algorithms. Additionally, reshuffling was used by a solution to the NeurIPS 2006 performance prediction challenge to estimate the final generalization performance (Guyon et al., 2006). Recently, in the context of evolutionary optimization, reshuffling was applied after every generation (Larcher & Barbosa, 2022). In this paper, we systematically examine the effect of reshuffling on HPO performance. Our contribu- tions can be summarized as follows: 1. We show theoretically that reshuffling resampling splits during HPO can result in finding a configuration with better overall generalization performance, especially when the loss surface is rather flat and its estimate is noisy (Section 2). 2. We confirm these theoretical insights through controlled simulation studies (Section 3). 3. We demonstrate in realistic HPO benchmark experiments that reshuffling splits can lead to a real-world improvement of HPO (Section 4). Especially in the case of reshuffled holdout, we find that the final generalization performance is often on par with 5-fold CV under a wide range of settings. We discuss results, limitations, and avenues for future research in Section 5. 2 Theoretical Analysis 2.1 Problem Statement and Setup Machine learning (ML) aims to fit a model to data, so that it generalizes well to new observations of the same distribution. Let D = {Zi}n i=1 be the observed dataset consisting of i.i.d. random variables from a distribution P, i.e., in the supervised setting Zi = ( Xi, Yi).3,4 Formally, an inducer g configured by an HPC λ ∈ Λ maps a dataset D to a model from our hypothesis space h = gλ(D) ∈ H. During HPO, we want to find a HPC that minimizes the expected generalization error, i.e., find λ∗ = arg min λ∈Λ µ(λ), where µ(λ) = E[ℓ(Z, gλ(D))], where ℓ(Z, h) is the loss of model h on a fresh observation Z. In practice, there is usually a limited computational budget for each HPO run, so we assume that there is only a finite number of distinct HPCs Λ = {λ1, . . . ,λJ} to be evaluated, which also simplifies the subsequent analysis. Naturally, we cannot optimize the generalization error directly, but only an estimate of it. To do so, a resampling is constructed. For every HPC λj, draw M random sets I1,j, . . . ,IM,j ⊂ {1, . . . , n} of validation indices with nvalid = ⌈αn⌉ instances each. The random index draws are assumed to be independent of the observed data. The data is then split accordingly into pairs Vm,j = {Zi}i∈Im,j , Tm,j = {Zi}i/∈Im,j of disjoint validation and training sets. Define the validation loss on the m-th fold L(Vm,j, gλj (Tm,j)) = 1 nvalid X i∈Im,j ℓ(Zi, gλj (Tm,j)), employed that implicitly assume that differences between the performance of classifiers on a given CV fold are comparable (Dietterich, 1998; Nadeau & Bengio, 1999, 2003; Demšar, 2006). 2In Appendix B, we present an overview of how resampling is addressed in tutorials and examples of standard HPO libraries and software. We conclude that usually fixed splits are used or recommended. 3Throughout, we use bold letters to indicate (fixed and random) vectors. 4We provide a notation table for symbols used in the main paper in Table 2 in the appendix. 2and the M-fold validation loss as bµ(λj) = 1 M MX m=1 L(Vm,j, gλj (Tm,j)). Since µ is unknown, we minimize bλ = arg minλ∈Λ bµ(λ), hoping that µ(bλ) will also be small. Typically, the same splits are used for every HPC, so Im,j = Im for all j = 1, . . . , Jand m = 1, . . . , M. In the following, we investigate how reshuffling train-validation splits (i.e., Im,j ̸= Im,j′ for j ̸= j′) affects the HPO problem. 2.2 How Reshuffling Affects the Loss Surface We first investigate how different validation and reshuffling strategies affect the empirical loss surface bµ. In particular, we derive the limiting distribution of the sequence √n(bµ(λj) − µ(λj))J j=1. This limiting regime will not only reveal the effect of reshuffling on the loss surface, but also give us a tractable setting to study HPO performance. Theorem 2.1. Under regularity conditions stated in Appendix C.1, it holds √n (bµ(λj) − µ(λj))J j=1 → N(0, Σ) in distribution, where Σi,j = τi,j,M K(λi, λj), τ i,j,M = lim n→∞ 1 nM2α2 nX s=1 MX m=1 MX m′=1 Pr(s ∈ Im,i ∩ Im′,j), and K(λi, λj) = lim n→∞ Cov[¯ℓn(Z′, λi), ¯ℓn(Z′, λj)], ¯ℓn(z, λ) = E[ℓ(z, gλ(T ))] − E[ℓ(Z, gλ(T ))], where the expectation is taken over a training set T of size n and two fresh samples Z, Z′ from the same distribution. The regularity conditions are rather mild and discussed further in Appendix C.1. The kernel K reflects the (co-)variability of the losses caused by validation samples. The contribution of training samples only has a higher-order effect. The validation scheme enters the distribution through the quantities τi,j,M . In what follows, we compute explicit expressions for some popular examples. The following list provides formal definitions for the index sets Im,j. (i) (holdout) Let M = 1 and I1,j = I1 for all j = 1, . . . , J, and some size-⌈αn⌉ index set I1. (ii) (reshuffled holdout) Let M = 1 and I1,1, . . . ,I1,J be independently drawn from the uniform distribution over all size-⌈αn⌉ subsets from {1, . . . , n}. (iii) (M-fold CV) Let α = 1/M and I1, . . . ,IM be a disjoint partition of {1, . . . , n}, and Im,j = Im for all j = 1, . . . , J. (iv) (reshuffled M-fold CV) Let α = 1/M and (I1,j, . . . ,IM,j ), j= 1, . . . , J, be independently drawn from the uniform distribution over disjoint partitions of {1, . . . , n}. (v) (M-fold holdout) LetIm, m= 1, . . . , M, be independently drawn from the uniform distribution over size-⌈αn⌉ subsets of {1, . . . , n} and set Im,j = Im for all m = 1, . . . , M, j= 1, . . . , J. (vi) (reshuffled M-fold holdout) Let Im,j, m= 1, . . . , M, j= 1, . . . , J, be independently drawn from the uniform distribution over size-⌈αn⌉ subsets of {1, . . . , n}. The value of τi,j,M for each example is computed explicitly in Appendix E. In all these examples, we in fact have τi,j,M = \u001aσ2, i = j τ2σ2, i ̸= j. , (1) for some method-dependent parameters σ, τshown in Table 1. The parameter σ2 captures any increase in variance caused by omitting an observation from the validation sets. The parameter τ quantifies a potential decrease in correlation in the loss surface due to reshuffling. More precisely, 3Table 1: Exemplary parametrizations in Equation (1) for resamplings; see Appendix E for details. Method σ2 τ2 holdout (HO) 1/α 1 reshuffled HO 1/α α M-fold CV 1 1 reshuffled M-fold CV 1 1 M-fold HO (subsampling / Monte Carlo CV) 1 + (1− α)/Mα 1 reshuffled M-fold HO 1 + (1− α)/Mα 1/(1 + (1− α)/Mα) the observed losses bµ(λi), bµ(λj) at distinct HPCs λi ̸= λj become less correlated when τ is small. Generally, an increase in variance leads to worse generalization performance. The effect of a correlation decrease is less obvious and is studied in detail in the following section. We make the following observations about the differences between methods in Table 1: • M-fold CV incurs no increase in variance (σ2 = 1) and — because every HPC uses the same folds — no decrease in correlation. Interestingly, the correlation does not even decrease when reshuffling the folds. In any case, all samples are used exactly once as validation and training instance. At least asymptotically, this leads to the same behavior, and reshuffling should have almost no effect on M-fold CV . • The two (1-fold) holdout methods bear the same 1/α increase in variance. This is caused by only using a fraction α of the data as validation samples. Reshuffled holdout also decreases the correlation parameter τ2. In fact, if HPCs λi ̸= λj are evaluated on largely distinct samples, the validation losses bµ(λi) and bµ(λj) become almost independent. • M-fold holdout also increases the variance, because some samples may still be omitted from validation sets. This increase is much smaller for large M. Accordingly, the correlation is also decreased by less in the reshuffled variant. 2.3 How Reshuffling Affects HPO Performance In practice, we are mainly interested in the performance of a model trained with the optimal HPC bλ. To simplify the analysis, we explore this in the large-sample regime derived in the previous section. Assume bµ(λj) = µ(λj) + ϵ(λj) (2) where ϵ(λ) is a zero-mean Gaussian process with covariance kernel Cov(ϵ(λ), ϵ(λ′)) = \u001aK(λ, λ) if λ = λ′, τ2K(λ, λ′) else. (3) Let Λ ⊆ {λ ∈ Rd : ∥λ∥ ≤1} with |Λ| = J <∞ be the set of hyperparameters. Theorem 2.2 ahead gives a bound on the expected regretE[µ(bλ)−µ(λ∗)]. It depends on several quantities characterizing the difficulty of the HPO problem. The constant κ = sup ∥λ∥,∥λ′∥≤1 |K(λ, λ) − K(λ, λ′)| K(λ, λ)∥λ − λ′∥2 . can be interpreted as a measure of correlation of the process ϵ. In particular, Corr(ϵ(λ), ϵ(λ′)) ≥ 1 − κ∥λ − λ′∥2. The constant is small when ϵ is strongly correlated, and large otherwise. Further, define η as the minimal number such that any η-ball contained in {∥λ∥ ≤1} contains at least one element of Λ. It measures how densely the set of candidate HPCsΛ covers set of all possible HPCs. If Λ is a deterministic uniform grid, we have about η ≈ J−1/d. Similarly, Lemma D.1 in the Appendix shows that η ≲ J−1/2d when randomly sampling HPCs. Finally, the constant m = sup λ∈Λ |µ(λ) − µ(λ∗)| ∥λ − λ∗∥2 , 4−2 −1 0 1 2 3 0.00 0.25 0.50 0.75 1.00 λ Loss Surface T ype T rue Empirical ( τ = 1 ) Empirical ( τ = 0.3 ) (a) High signal-to-noise ratio −2 0 2 0.00 0.25 0.50 0.75 1.00 λ Loss Surface T ype T rue Empirical ( τ = 1 ) Empirical ( τ = 0.3 ) (b) Low signal-to-noise ratio Figure 1: Example of reshuffled empirical loss yielding a worse (left) and better (right) minimizer. measures the local curvature at the minimum of the loss surface µ. Finding an HPC λ close to the theoretical optimum λ∗ is easier when the minimum is more pronounced (large m). On the other hand, the regret µ(λ) − µ(λ∗) is also punishing mistakes more quickly. Defining log(x)+ = max{0, log(x)}, we can now state our main result. Theorem 2.2. Let bµ follow the Gaussian process model(2). Suppose κ <∞, 0 < σ2 ≤ Var[ϵ(λ)] ≤ σ2 < ∞ for all λ ∈ Λ, and m >0. Then E[µ(bλ) − µ(λ∗)] ≤ σ √ d[8 + B(τ) − A(τ)]. where B(τ) = 48 hp 1 − τ2 p log J + τ p 1 + log(3κ)+ i , A (τ) = p 1 − τ2(σ/σ) s log \u0012 σ 2mη2 \u0013 + . The numeric constants result from several simplifications in a worst-case analysis, which lowers their practical relevance. A qualitative analysis of the bound is still insightful. The bound is increasing in σ and d, indicating that the HPO problem is harder when there is a lot of noise or there are many parameters to tune. The terms B(τ) and A(τ) have conceptual interpretations: • The term B(τ) quantifies how likely it is to pick a bad bλ because of bad luck: a λ far away from λ∗ had such a small ϵ(λ) that it outweighs the increase in µ. Such events are more likely when the process ϵ is weakly correlated. Accordingly, B(τ) is decreasing in τ and increasing in κ. • The term A(τ) quantifies how likely it is to pick a good bλ by luck: a λ close to λ∗ had such a small ϵ(λ) that it overshoots all the other fluctuations. Also such events are more likely when the process ϵ is weakly correlated. Accordingly, the term A(τ) is decreasing in τ. The B, as stated, is unbounded, but a closer inspection of the proof shows that it is upper bounded by √log J. This bound is attained only in the unrealistic scenario when the validation losses are essentially uncorrelated across all HPCs. The term A is bounded from below by zero, which is also the worst case because the term enters our regret bound with a negative sign. Both A and B are decreasing in the reshuffling parameter τ. There are two regimes. If σ/2mη2 ≤ e, then A(τ) = 0 and reshuffling cannot lead to an improvement of the bound. The term σ/mη2 can be interpreted as noise-to-signal ratio (relative to the grid density). If the signal is much stronger than the noise, the HPO problem is so easy that reshuffling will not help. This situation is illustrated in Figure 1a. If on the other hand σ/mη2 > e, the terms A(τ) and B(τ) enter the bound with opposing signs. This creates tension: reshuffling between HPCs increases B(τ), which is countered by a decrease in A(τ). So which scenarios favor reshuffling? When the process ϵ is strongly correlated, κ is small and reshuffling (decreasing τ) incurs a high cost in B(τ). This is intuitive: When there is strong 5correlation, the validation loss surface bµ is essentially just a vertical shift of µ. Finding the optimal λ is then almost as easy as if we would know µ, and decorrelating the surface through reshuffling would make it unnecessarily hard. When ϵ is less correlated (κ large) however, reshuffling does not hurt the term B(τ) as much, but we can reap all the benefits of increasing A(τ). Here, the effect of reshuffling can be interpreted as hedging against the catastrophic case where all bµ(λ) close to the optimal λ∗ are simultaneously dominated by a region of bad hyperparameters. This is illustrated in Figure 1b. 3 Simulation Study To test our theoretical understanding of the potential benefits of reshuffling resampling splits during HPO, we conduct a simulation study. This study helps us explore the effects of reshuffling in a controlled setting. 3.1 Design We construct a univariate quadratic loss surface function µ : Λ ⊂ R 7→ R, λ→ m(λ − 0.5)2/2 which we want to minimize. The global minimum is given at µ(0.5) = 0 . Combined with a kernel for the noise process ϵ as in Equation (3), this allows us to simulate an objective as ob- served during HPO by sampling bµ(λ) = µ(λ) + ϵ(λ). We use a squared exponential kernel K(λ, λ′) = σ2 K exp (−κ(λ − λ′)2/2) that is plugged into the covariance kernel of the noise process ϵ in Equation (3). The parameters m and κ in our simulation setup correspond exactly to the curva- ture and correlation constants from the previous sections. Recall that Theorem 2.2 states that the effect of reshuffling strongly depends on the curvature m of the loss surface µ (a larger m implies a stronger curvature) and the constant κ as a measure of correlation of the noise ϵ (a larger κ implies weaker correlation). Combined with the possibility to vary τ in the covariance kernel of ϵ, we can systematically investigate how curvature of the loss surface, correlation of the noise and the extent of reshuffling affect optimization performance. In each simulation run, we simulate the observed objective ˆµ(λ), identify the minimizer ˆλ = arg minλ∈Λ ˆµ(λ), and calculate its true risk, µ(ˆλ). We repeat this process 10000 times for various combinations of τ, m, and κ. 3.2 Results Figure 2 visualizes the true risk of the configuration ˆλ that minimizes the observed objective. We observe that for a loss surface with low curvature (i.e., m ≤ 2), reshuffling is beneficial (lower values of τ resulting in a better true risk of the configuration that optimizes the observed objective) as long as the noise process is not too correlated (i.e., κ ≥ 1). As soon as the noise process is more strongly correlated, even flat valleys of the true risk µ remain clearly visible in the observed risk bµ, and reshuffling starts to hurt the optimization performance. Moving to scenarios of high curvature, the general relationship of m and κ remains the same, but reshuffling starts to hurt optimization performance already with weaker correlation in the noise. In summary, the simulations show that in cases of low curvature of the loss surface, reshuffling (reducing τ) tends to improve the true risk of the optimized configuration, especially when the loss surface is flat (small m) and the noise is not strongly correlated (i.e., κ is large). This exactly confirms our theoretical predictions from the previous section. 4 Benchmark Experiments In this section, we present benchmark experiments of real-world HPO problems where we investigate the effect of reshuffling resampling splits during HPO. First, we discuss the experimental setup. Second, we present results for HPO using random search (Bergstra & Bengio, 2012). Third, we also show the effect of reshuffling when applied in BO using HEBO (Cowen-Rivers et al., 2022) and SMAC3 (Lindauer et al., 2022). Recall that our theoretical insight suggests that 1) reshuffling might be beneficial during HPO and 2) holdout should be affected the most by reshuffling and other resamplings should only be affected to a lesser extent. 6m: 20 κ: 0.04 m: 20 κ: 1 m: 20 κ: 4 m: 20 κ: 100 m: 10 κ: 0.04 m: 10 κ: 1 m: 10 κ: 4 m: 10 κ: 100 m: 2 κ: 0.04 m: 2 κ: 1 m: 2 κ: 4 m: 2 κ: 100 0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00 0.070 0.075 0.080 0.085 0.090 0.175 0.200 0.225 0.250 0.275 0.21 0.24 0.27 0.30 0.08 0.10 0.12 0.18 0.19 0.20 0.100 0.125 0.150 0.175 0.200 0.07 0.08 0.09 0.10 0.11 0.12 0.08 0.12 0.16 0.05 0.10 0.15 0.20 0.02 0.04 0.06 0.00 0.05 0.10 0.15 0.00 0.05 0.10 0.15 0.20 τ μ(λ^) Figure 2: Mean true risk (lower is better) of the configuration minimizing the observed objective systematically varied with respect to curvature m, correlation strength κ of the noise (a larger κ implying weaker correlation), and extent of reshuffling τ (lower τ increasing reshuffling). A τ of 1 indicates no reshuffling. Error bars represent standard errors. 4.1 Experimental Setup As benchmark tasks, we use a set of standard HPO problems defined on small- to medium-sized tabular datasets for binary classification. We suspect the effect of the resampling variant used and whether the resampling is reshuffled to be larger for smaller datasets, where the variance of the validation loss estimator is naturally higher. Furthermore, from a practical perspective, this also ensures computational feasibility given the large number of HPO runs in our experiments. We systematically vary the learning algorithm, optimized performance metric, resampling method, whether the resampling is reshuffled, and the size of the dataset used for training and validation during HPO. Below, we outline the general experimental design and refer to Appendix F for details. We used a subset of the datasets defined by the AutoML benchmark (Gijsbers et al., 2024), treating these as data generating processes (DGPs; Hothorn et al., 2005). We only considered datasets with less than 100 features to reduce the required computation time and required the number of observations to be between 10000 and 1000000; for further details see Appendix F.1. Our aim was to robustly measure the generalization performance when varying the size n, which, as defined in Section 2 denotes the size of the combined data for model selection, so one training and validation set combined. First, we sampled 5000 data points per dataset for robust assessment of the generalization error; these points are not used during HPO in any way. Then, from the remaining points we sampled tasks with n ∈ {500, 1000, 5000}. We selected CatBoost (Prokhorenkova et al., 2018) and XGBoost (Chen & Guestrin, 2016) for their state-of-the-art performance on tabular data (Grinsztajn et al., 2022; Borisov et al., 2022; McElfresh et al., 2023; Kohli et al., 2024). Additionally, we included an Elastic Net (Zou & Hastie, 2005) to represent a linear baseline with a smaller search space and a funnel-shaped MLP (Zimmer et al., 2021) as a cost-effective neural network baseline. We provide details regarding training pipelines and search spaces in Appendix F.2. We conduct a random search with500 HPC evaluations for every resampling strategy we described in Table 1, for both fixed and reshuffled splits. We always use 80/20 train-validation splits for holdout 7500 1000 5000 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 −0.730 −0.725 −0.720 −0.715 −0.70 −0.69 −0.68 −0.67 −0.66 −0.68 −0.67 −0.66 −0.65 −0.64 −0.63 No. HPC Evaluations Mean T est Performance Reshuffling FALSE TRUE Resampling Holdout 5−fold CV 5−fold Holdout 5x 5−fold CV Figure 3: Average test performance (negative ROC AUC) of the incumbent for XGBoost on dataset albert for increasing n (train-validation sizes, columns). Shaded areas represent standard errors. and 5-fold CVs, so that training set size (and negative estimation bias) are the same. Anytime test performance of an HPO run is assessed by re-training the current incumbent (i.e. the best HPC until the current HPO iteration based on validation performance) on all available train and validation data and evaluating its performance on the outer test set. Note we do this for scientific evaluation in this experiment; obviously, this is not possible in practice. Using random search allows us to record various metrics and afterwards simulate optimizing for different ones, specifically, we recorded accuracy, area under the ROC curve (ROC AUC) and logloss. We also investigated the effect of reshuffling on two state-of-the-art BO variants (Eggensperger et al., 2021; Turner et al., 2021), namely HEBO (Cowen-Rivers et al., 2022) and SMAC3 (Lindauer et al., 2022). The experimental design was the same as for random search, except for the budget, which we reduced from 500 HPCs to 250 HPCs, and only optimized ROC AUC. 4.2 Experimental Results In the following, we focus on the results obtained using ROC AUC. We present aggregated results over different tasks, learning algorithms and replications to get a general understanding of the effects. Unaggregated results and results involving accuracy and logloss can be found in Appendix G. Results of Reshuffling Different Resamplings For each resampling (holdout, 5-fold holdout, 5-fold CV , and 5x 5-fold CV), we empirically analyze the effect of reshuffling train and validation splits during HPO. In Figure 3 we exemplarily show how test performance develops over the course of an HPO run on a single task for different resamplings (with and without reshuffling). Naturally, test performance does not necessarily increase in a monotonic fashion, and especially holdout without reshuffling tends to be unstable. Its reshuffled version results in substantially better test performance. Next, we look at the relative improvement (compared to standard 5-fold CV , which we consider our baseline) with respect to test ROC AUC performance of the incumbent over time in Figure 4, i.e., the difference in test performance of the incumbent between standard 5-fold CV and a different resampling protocol; hence a positive difference tells us how much better in test error we are, if we would have chosen the other protocol instead 5-fold CV . We observe that reshuffling generally results in equal or better performance compared to the same resampling protocol without reshuffling. For 5-fold holdout and especially 5-fold CV and 5x 5-fold CV , reshuffling has a smaller effect on relative test performance improvement, as expected. Holdout is affected the most by reshuffling and results in substantially better relative test performance compared to standard holdout. We also observe that an HPO protocol based on reshuffled holdout results in similar final test performance as standard 5-fold CV while overall being substantially cheaper due to requiring less model fits per HPC evaluation. In Appendix G.2, we further provide an ablation study on the number of folds when using M-fold holdout, where we observed that – in line with our theory – the more folds are used, the less reshuffling affects M-fold holdout. 8500 1000 5000 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 −0.50 −0.25 0.00 0.25 −1.2 −0.8 −0.4 0.0 0.4 −1.0 −0.5 0.0 0.5 No. HPC Evaluations Mean T est Improvement Reshuffling FALSE TRUE Resampling Holdout 5−fold CV 5−fold Holdout 5x 5−fold CV Figure 4: Average improvement (compared to standard 5-fold CV) with respect to test performance (ROC AUC) of the incumbent over different tasks, learning algorithms and replications separately for increasing n (train-validation sizes, columns). Shaded areas represent standard errors. However, this general trend can vary for certain combinations of classifier and performance metric, see Appendix G. Especially for logloss, we observed that reshuffling rarely is beneficial; see the discussion in Section 5. Finally, the different resamplings generally behave as expected. The more we are willing to invest compute resources into a more intensive resampling like 5-fold CV or 5x 5-fold CV , the better the generalization performance of the final incumbent. Results for BO and Reshuffling Figure 5 shows that, generally HEBO and SMAC3 outperform random search with respect to generalization performance (i.e., comparing HEBO and SMAC3 to random search under standard holdout, or comparing under reshuffled holdout). More interestingly, HEBO, SMAC3 and random search all strongly benefit from reshuffling. Moreover, the performance gap between HEBO and random search but also SMAC3 and random search narrows when the resampling is reshuffled, which is an interesting finding of its own: As soon as we are concerned with generalization performance of HPO and not only investigate validation performance during optimization, the choice of optimizer might have less impact on final generalization performance compared to other choices such as whether the resampling is reshuffled during HPO or not. We present results for BO and reshuffling for different resamplings in Appendix G. 500 1000 5000 1 50 100 150 200 250 1 50 100 150 200 250 1 50 100 150 200 250 0.0 0.5 1.0 1.5 −0.5 0.0 0.5 1.0 −0.5 0.0 0.5 1.0 No. HPC Evaluations Mean T est Improvement Optimizer Random Search HEBO SMAC3 Reshuffling FALSE TRUE Figure 5: Average improvement (compared to random search on standard holdout) with respect to test performance (ROC AUC) of the incumbent over tasks, learning algorithms and replications for different n (train-validation sizes, columns). Shaded areas represent standard errors. 5 Discussion In the previous sections, we have shown theoretically and empirically that reshuffling can enhance generalization performance of HPO. The main purpose of this article is to draw attention to this 9surprising fact about a technique that is simple but rarely discussed. Our work goes beyond a preliminary experimental study on reshuffling (Lévesque, 2018), in that we also study the effect of reshuffling on random search, multiple metrics and learning algorithms, and most importantly, for the first time, we provide a theoretical analysis that explains why reshuffling can be beneficial. Limitations To unveil the mechanisms underlying the reshuffling procedures, our theoretical analysis relies on an asymptotic approximation of the empirical loss surface. This allows us to operate on Gaussian loss surfaces, which exhibit convenient concentration and anti-concentration properties required in our proof. The latter are lacking for general distributions, which explains our asymptotic approach. The analysis was further facilitated by a loss stability assumption regarding the learning algorithms that is generally rather mild; see the discussion in Bayle et al. (2020). However, it typically fails for highly sensitive losses, which has practical consequences. In fact, Figure 9 in Appendix G shows that reshuffling usually hurts generalization for the logloss and small sample sizes. It is still an open question whether this problem can be fixed by less naive implementations of the technique. Another limitation is our focus on generalization after search through a fixed, finite set of candidates. This largely ignores the dynamic nature of many HPO algorithms, which would greatly complicate our analysis. Finally, our experiments are limited in that we restricted ourselves to tabular data and binary classification and we avoided extremely small or large datasets. Relation to Overfitting The fact that generalization performance can decrease during HPO (or computational model selection in general) is sometimes known as oversearching, overtuning, or overfitting to the validation set (Quinlan & Cameron-Jones, 1995; Escalante et al., 2009; Koch et al., 2010; Igel, 2012; Bischl et al., 2023), but has arguably not been studied very thoroughly. Given recent theoretical (Feldman et al., 2019) and empirical (Purucker & Beel, 2023) findings, we expect less overtuning on multi-class datasets, making it interesting to see how reshuffling would affect the generalization performance. Several works suggest strategies to counteract this effect. First, LOOCVCV proposes a conservative choice of incumbents (Ng, 1997) at the cost of leave-one-out analysis or an additional hyperparameter. Second, it is possible to use an extra selection set (Igel, 2012; Lévesque, 2018; Mohr et al., 2018) at the cost of reduced training data, which was found to lead to reduced overall performance (Lévesque, 2018). Third, by using early stopping one can stop hyperparameter optimization before the generaliza- tion performance degrades again. This was so far demonstrated to be able to save compute budget at only marginally reduced performance, but also requires either a sensitivity hyperparameter or correct estimation of the variance of the generalization estimate and was only developed for cross-validation so far (Makarova et al., 2022). Reshuffling itself is orthogonal to these proposals and a combination with the above-mentioned methods might result in further improvements. Outlook Generally, the related literature detects overfitting to the validation set either visually (Ng, 1997) or by measuring it (Koch et al., 2010; Igel, 2012; Fabris & Freitas, 2019). Developing a unified formal definition of the above-mentioned terms and thoroughly analyzing the effect of decreased generalization performance after many HPO iterations and how it relates to our measurements of the validation performance is an important direction for future work. We further found, both theoretically and experimentally, that investing more resources when evaluating each HPC can result in better final HPO performance. To reduce the computational burden on HPO again, we suggest further investigating the use of adaptive CV techniques, as proposed by Auto- WEKA (Thornton et al., 2013) or under the name Lazy Paired Hyperparameter Tuning (Zheng & Bilenko, 2013). Designing more advanced HPO algorithms exploiting the reshuffling effect should be a promising avenue for further research. Acknowledgments and Disclosure of Funding We thank Martin Binder and Florian Karl for helpful discussions. Lennart Schneider is supported by the Bavarian Ministry of Economic Affairs, Regional Development and Energy through the Center for Analytics - Data - Applications (ADACenter) within the framework of BAYERN DIGITAL II (20-3410-2-9-8). Lennart Schneider acknowledges funding from the LMU Mentoring Program of the Faculty of Mathematics, Informatics and Statistics. 10References Arlot, S. and Celisse, A. A survey of cross-validation procedures for model selection. Statistics Surveys, 4:40 – 79, 2010. B Austern, M. and Zhou, W. Asymptotics of cross-validation. arXiv:2001.11111 [math.ST], 2020. C.1 Awad, N., Mallik, N., and Hutter, F. DEHB: Evolutionary hyberband for scalable, robust and efficient Hyperparameter Optimization. In Zhou, Z. (ed.), Proceedings of the 30th International Joint Conference on Artificial Intelligence (IJCAI’21), pp. 2147–2153, 2021. B Bayle, P., Bayle, A., Janson, L., and Mackey, L. Cross-validation confidence intervals for test error. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M.-F., and Lin, H. (eds.),Proceedings of the 33rd International Conference on Advances in Neural Information Processing Systems (NeurIPS’20), pp. 16339–16350. Curran Associates, 2020. 5, C.1, C.1, C.1 Bergman, E., Purucker, L., and Hutter, F. Don’t waste your time: Early stopping cross-validation. In Eggensperger, K., Garnett, R., Vanschoren, J., Lindauer, M., and Gardner, J. (eds.),Proceedings of the Third International Conference on Automated Machine Learning, volume 256 of Proceedings of Machine Learning Research, pp. 9/1–31. PMLR, 2024. B Bergstra, J. and Bengio, Y . Random search for hyper-parameter optimization.Journal of Machine Learning Research, 13:281–305, 2012. 4, B Bischl, B., Binder, M., Lang, M., Pielok, T., Richter, J., Coors, S., Thomas, J., Ullmann, T., Becker, M., Boulesteix, A., Deng, D., and Lindauer, M. Hyperparameter optimization: Foundations, algorithms, best practices, and open challenges. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, pp. e1484, 2023. 1, 5, B Blum, A., Kalai, A., and Langford, J. Beating the hold-out: Bounds for k-fold and progressive cross-validation. In Proceedings of the Twelfth Annual Conference on Computational Learning Theory, COLT ’99, pp. 203–208, 1999. B Borisov, V ., Leemann, T., Seßler, K., Haug, J., Pawelczyk, M., and Kasneci, G. Deep neural networks and tabular data: A survey. IEEE Transactions on Neural Networks and Learning Systems, pp. 1–21, 2022. 4.1 Bouckaert, Remcoand Frank, E. Evaluating the Replicability of Significance Tests for Comparing Learning Algorithms. In Dai, H., Srikant, R., and Zhang, C. (eds.), Advances in Knowledge Discovery and Data Mining, pp. 3–12. Springer, 2004. B Bousquet, O. and Zhivotovskiy, N. Fast classification rates without standard margin assumptions. Information and Inference: A Journal of the IMA, 10(4):1389–1421, 2021. C.1 Bouthillier, X., Delaunay, P., Bronzi, M., Trofimov, A., Nichyporuk, B., Szeto, J., Sepahvand, N. M., Raff, E., Madan, K., V oleti, V ., Kahou, S. E., Michalski, V ., Arbel, T., Pal, C., Varoquaux, G., and Vincent, P. Accounting for variance in machine learning benchmarks. In Smola, A., Dimakis, A., and Stoica, I. (eds.), Proceedings of Machine Learning and Systems 3, volume 3, pp. 747–769, 2021. B Buczak, P., Groll, A., Pauly, M., Rehof, J., and Horn, D. Using sequential statistical tests for efficient hyperparameter tuning. AStA Advances in Statistical Analysis, 108(2):441–460, 2024. B Cawley, G. and Talbot, N. On Overfitting in Model Selection and Subsequent Selection Bias in Performance Evaluation. Journal of Machine Learning Research, 11:2079–2107, 2010. B Chen, T. and Guestrin, C. XGBoost: A scalable tree boosting system. In Krishnapuram, B., Shah, M., Smola, A., Aggarwal, C., Shen, D., and Rastogi, R. (eds.), Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD’16) , pp. 785–794. ACM Press, 2016. 4.1 Cowen-Rivers, A., Lyu, W., Tutunov, R., Wang, Z., Grosnit, A., Griffiths, R., Maraval, A., Jianye, H., Wang, J., Peters, J., and Ammar, H. HEBO: Pushing the limits of sample-efficient hyper-parameter optimisation. Journal of Artificial Intelligence Research, 74:1269–1349, 2022. 4, 4.1, B 11Demšar, J. Statistical comparisons of classifiers over multiple data sets. Journal of Machine Learning Research, 7:1–30, 2006. 1 Dietterich, T. G. Approximate statistical tests for comparing supervised classification learning algorithms. Neural Computation, 10(7):1895–1923, 1998. 1 Dunias, Z., Van Calster, B., Timmerman, D., Boulesteix, A.-L., and van Smeden, M. A comparison of hyperparameter tuning procedures for clinical prediction models: A simulation study. Statistics in Medicine, 43(6):1119–1134, 2024. B Eggensperger, K., Lindauer, M., Hoos, H., Hutter, F., and Leyton-Brown, K. Efficient benchmarking of algorithm configurators via model-based surrogates. Machine Learning, 107(1):15–41, 2018. 5 Eggensperger, K., Lindauer, M., and Hutter, F. Pitfalls and best practices in algorithm configuration. Journal of Artificial Intelligence Research, pp. 861–893, 2019. B Eggensperger, K., Müller, P., Mallik, N., Feurer, M., Sass, R., Klein, A., Awad, N., Lindauer, M., and Hutter, F. HPOBench: A collection of reproducible multi-fidelity benchmark problems for HPO. In Vanschoren & Yeung (2021). 4.1, B Escalante, H., Montes, M., and Sucar, E. Particle Swarm Model Selection. Journal of Machine Learning Research, 10:405–440, 2009. 5 Fabris, F. and Freitas, A. Analysing the overfit of the auto-sklearn automated machine learning tool. In Nicosia, G., Pardalos, P., Umeton, R., Giuffrida, G., and Sciacca, V . (eds.), Machine Learning, Optimization, and Data Science, volume 11943 of Lecture Notes in Computer Science, pp. 508–520, 2019. 5 Falkner, S., Klein, A., and Hutter, F. BOHB: Robust and efficient Hyperparameter Optimization at scale. In Dy, J. and Krause, A. (eds.), Proceedings of the 35th International Conference on Machine Learning (ICML’18), volume 80, pp. 1437–1446. Proceedings of Machine Learning Research, 2018. B Feldman, V ., Frostig, R., and Hardt, M. The advantages of multiple classes for reducing overfitting from test set reuse. In Chaudhuri, K. and Salakhutdinov, R. (eds.), Proceedings of the 36th Interna- tional Conference on Machine Learning (ICML’19), volume 97, pp. 1892–1900. Proceedings of Machine Learning Research, 2019. 5 Feurer, M. and Hutter, F. Hyperparameter Optimization. In Hutter et al. (2019), chapter 1, pp. 3 – 38. Available for free at http://automl.org/book. 1, B Feurer, M., Eggensperger, K., Falkner, S., Lindauer, M., and Hutter, F. Auto-Sklearn 2.0: Hands-free automl via meta-learning. Journal of Machine Learning Research, 23(261):1–61, 2022. B Garnett, R. Bayesian Optimization. Cambridge University Press, 2023. 1, B Gijsbers, P., Bueno, M., Coors, S., LeDell, E., Poirier, S., Thomas, J., Bischl, B., and Vanschoren, J. AMLB: an automl benchmark. Journal of Machine Learning Research, 25(101):1–65, 2024. 4.1 Giné, E. and Nickl, R. Mathematical Foundations of Infinite-Dimensional Statistical Models, vol- ume 40. Cambridge University Press, 2016. C.2 Grinsztajn, L., Oyallon, E., and Varoquaux, G. Why do tree-based models still outperform deep learning on typical tabular data? In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks, pp. 507–520, 2022. 4.1 Guyon, I., Alamdari, A., Dror, G., and Buhmann, J. Performance prediction challenge. In The 2006 IEEE International Joint Conference on Neural Network Proceedings, 2006. 1 Guyon, I., Saffari, A., Dror, G., and Cawley, G. Model selection: Beyond the Bayesian/Frequentist divide. Journal of Machine Learning Research, 11:61–87, 2010. B 12Guyon, I., Bennett, K., Cawley, G., Escalante, H. J., Escalera, S., Ho, T. K., Macià, N., Ray, B., Saeed, M., Statnikov, A., and Viegas, E. Design of the 2015 ChaLearn AutoML challenge. In 2015 International Joint Conference on Neural Networks (IJCNN’15), pp. 1–8. International Neural Network Society and IEEE Computational Intelligence Society, IEEE, 2015. B Guyon, I., Sun-Hosoya, L., Boullé, M., Escalante, H., Escalera, S., Liu, Z., Jajetic, D., Ray, B., Saeed, M., Sebag, M., Statnikov, A., Tu, W., and Viegas, E. Analysis of the AutoML Challenge Series 2015-2018. In Hutter et al. (2019), chapter 10, pp. 177–219. Available for free at http: //automl.org/book. B Guyon, I., Lindauer, M., van der Schaar, M., Hutter, F., and Garnett, R. (eds.). Proceedings of the First International Conference on Automated Machine Learning, 2022. Proceedings of Machine Learning Research. 5 Hansen, N. and Ostermeier, A. Completely derandomized self-adaptation in evolution strategies. Evolutionary C., 9(2):159–195, 2001. 1 Hothorn, T., Leisch, F., Zeileis, A., and Hornik, K. The design and analysis of benchmark experiments. Journal of Computational and Graphical Statistics, 14(3):675–699, 2005. 4.1, F.1 Hutter, F., Kotthoff, L., and Vanschoren, J. (eds.).Automated Machine Learning: Methods, Systems, Challenges. Springer, 2019. Available for free at http://automl.org/book. 5 Igel, C. A note on generalization loss when evolving adaptive pattern recognition systems. IEEE Transactions on Evolutionary Computation, 17(3):345–352, 2012. 5, 5 Jamieson, K. and Talwalkar, A. Non-stochastic best arm identification and Hyperparameter Op- timization. In Gretton, A. and Robert, C. (eds.), Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics (AISTATS’16) , volume 51. Proceedings of Machine Learning Research, 2016. B Kadra, A., Janowski, M., Wistuba, M., and Grabocka, J. Scaling laws for hyperparameter optimization. In Oh, A., Naumann, T., Globerson, A., Saenko, K., Hardt, M., and Levine, S. (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 47527–47553, 2023. B Kallenberg, O. Foundations of modern probability, volume 2. Springer, 1997. D Klein, A., Falkner, S., Bartels, S., Hennig, P., and Hutter, F. Fast Bayesian optimization of machine learning hyperparameters on large datasets. In Singh, A. and Zhu, J. (eds.), Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics (AISTATS’17), volume 54. Proceedings of Machine Learning Research, 2017. B Koch, P., Konen, W., Flasch, O., and Bartz-Beielstein, T. Optimizing support vector machines for stormwater prediction. Technical Report TR10-2-007, Technische Universität Dortmund, 2010. Proceedings of Workshop on Experimental Methods for the Assessment of Computational Systems joint to PPSN2010. 5, 5 Kohli, R., Feurer, M., Bischl, B., Eggensperger, K., and Hutter, F. Towards quantifying the effect of datasets for benchmarking: A look at tabular machine learning. In Data-centric Machine Learning (DMLR) workshop at the International Conference on Learning Representations (ICLR), 2024. 4.1 Lang, M., Kotthaus, H., Marwedel, P., Weihs, C., Rahnenführer, J., and Bischl, B. Automatic model selection for high-dimensional survival analysis. Journal of Statistical Computation and Simulation, 85:62–76, 2015. B Larcher, C. and Barbosa, H. Evaluating models with dynamic sampling holdout in auto-ml. SN Computer Science, 3(506), 2022. 1 Li, L., Jamieson, K., DeSalvo, G., Rostamizadeh, A., and Talwalkar, A. Hyperband: A novel bandit-based approach to Hyperparameter Optimization. Journal of Machine Learning Research, 18(185):1–52, 2018. B Lindauer, M., Eggensperger, K., Feurer, M., Biedenkapp, A., Deng, D., Benjamins, C., Ruhkopf, T., Sass, R., and Hutter, F. SMAC3: A versatile bayesian optimization package for Hyperparameter Optimization. Journal of Machine Learning Research, 23(54):1–9, 2022. 4, 4.1, B 13Loshchilov, I. and Hutter, F. CMA-ES for Hyperparameter Optimization of deep neural networks. In International Conference on Learning Representations Workshop track, 2016. Published online: iclr.cc. B Lévesque, J. Bayesian Hyperparameter Optimization: Overfitting, Ensembles and Conditional Spaces. PhD thesis, Université Laval, 2018. 1, 5, 5 Makarova, A., Shen, H., Perrone, V ., Klein, A., Faddoul, J., Krause, A., Seeger, M., and Archambeau, C. Automatic termination for hyperparameter optimization. In Guyon et al. (2022). 5 Mallik, N., Bergman, E., Hvarfner, C., Stoll, D., Janowski, M., Lindauer, M., Nardi, L., and Hutter, F. PriorBand: Practical hyperparameter optimization in the age of deep learning. In Oh et al. (2023). B McElfresh, D., Khandagale, S., Valverde, J., Prasad C., V ., Ramakrishnan, G., Goldblum, M., and White, C. When do neural nets outperform boosted trees on tabular data? In Oh et al. (2023), pp. 76336–76369. 4.1, F.2 Mohr, F., Wever, M., and Hüllermeier, E. ML-Plan: Automated machine learning via hierarchical planning. Machine Learning, 107(8-10):1495–1515, 2018. 5, B Molinaro, A., Simon, R., and Pfeiffer, R. Prediction error estimation: A comparison of resampling methods. Bioinformatics, 21(15):3301–3307, 2005. B Nadeau, C. and Bengio, Y . Inference for the generalization error. In Solla, S., Leen, T., and Müller, K. (eds.), Proceedings of the 13th International Conference on Advances in Neural Information Processing Systems (NeurIPS’99). The MIT Press, 1999. 1 Nadeau, C. and Bengio, Y . Inference for the generalization error.Machine Learning, 52:239–281, 2003. 1 Ng, A. Preventing “overfitting”’ of cross-validation data. In Fisher, D. H. (ed.), Proceedings of the Fourteenth International Conference on Machine Learning (ICML’97), pp. 245–253. Morgan Kaufmann Publishers, 1997. 5, 5 Oh, A., Neumann, T., Globerson, A., Saenko, K., Hardt, M., and Levine, S. (eds.). Proceedings of the 36th International Conference on Advances in Neural Information Processing Systems (NeurIPS’23), 2023. Curran Associates. 5 Pfisterer, F., Schneider, L., Moosbauer, J., Binder, M., and Bischl, B. YAHPO Gym – an efficient multi-objective multi-fidelity benchmark for hyperparameter optimization. In Guyon et al. (2022). B, 5 Pineda Arango, S., Jomaa, H., Wistuba, M., and Grabocka, J. HPO-B: A large-scale reproducible benchmark for black-box HPO based on OpenML. In Vanschoren & Yeung (2021). B, 5 Probst, P., Boulesteix, A., and Bischl, B. Tunability: Importance of hyperparameters of machine learning algorithms. Journal of Machine Learning Research, 20(53):1–32, 2019. 1 Prokhorenkova, L., Gusev, G., V orobev, A., Dorogush, A., and Gulin, A. Catboost: Unbiased boosting with categorical features. In Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R. (eds.), Proceedings of the 31st International Conference on Advances in Neural Information Processing Systems (NeurIPS’18), pp. 6639–6649. Curran Associates, 2018. 4.1 Purucker, L. and Beel, J. CMA-ES for post hoc ensembling in automl: A great success and salvageable failure. In Faust, A., Garnett, R., White, C., Hutter, F., and Gardner, J. R. (eds.),Proceedings of the Second International Conference on Automated Machine Learning, volume 224 of Proceedings of Machine Learning Research, pp. 1/1–23. PMLR, 2023. 5 Quinlan, J. and Cameron-Jones, R. Oversearching and layered search in empirical learning. In Proceedings of the 14th International Joint Conference on Artificial Intelligence , volume 2 of IJCAI’95, pp. 1019–1024, 1995. 5 14Rao, R., Fung, G., and Rosales, R. On the dangers of cross-validation. an experimental evaluation. In Proceedings of the 2008 SIAM International Conference on Data Mining (SDM), pp. 588–596, 2008. B Salinas, D., Seeger, M., Klein, A., Perrone, V ., Wistuba, M., and Archambeau, C. Syne Tune: A library for large scale hyperparameter tuning and reproducible research. In Guyon et al. (2022), pp. 16–1. B Schaffer, C. Selecting a classification method by cross-validation. Machine Learning Journal, 13: 135–143, 1993. B Swersky, K., Snoek, J., and Adams, R. Freeze-thaw Bayesian optimization. arXiv:1406.3896 [stats.ML], 2014. B Talagrand, M. The generic chaining: upper and lower bounds of stochastic processes . Springer Science & Business Media, 2005. C.2 Thornton, C., Hutter, F., Hoos, H., and Leyton-Brown, K. Auto-WEKA: Combined selection and Hyperparameter Optimization of classification algorithms. In Dhillon, I., Koren, Y ., Ghani, R., Senator, T., Bradley, P., Parekh, R., He, J., Grossman, R., and Uthurusamy, R. (eds.), The 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD’13), pp. 847–855. ACM Press, 2013. 5, B Turner, R., Eriksson, D., McCourt, M., Kiili, J., Laaksonen, E., Xu, Z., and Guyon, I. Bayesian optimization is superior to random search for machine learning hyperparameter tuning: Analysis of the Black-Box Optimization Challenge 2020. In Escalante, H. and Hofmann, K. (eds.),Proceedings of the Neural Information Processing Systems Track Competition and Demonstration, pp. 3–26. Curran Associates, 2021. 4.1 van der Vaart, A. Asymptotic statistics, volume 3. Cambridge university press, 2000. C.1 van Erven, T., Grünwald, P., Mehta, N., Reid, M., and Williamson, R. Fast rates in statistical and online learning. Journal of Machine Learning Research, 16(54):1793–1861, 2015. C.1 van Rijn, J. and Hutter, F. Hyperparameter importance across datasets. In Guo, Y . and Farooq, F. (eds.), Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD’18), pp. 2367–2376. ACM Press, 2018. 1 Vanschoren, J. and Yeung, S. (eds.).Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks, 2021. Curran Associates. 5 Vanschoren, J., van Rijn, J., Bischl, B., and Torgo, L. OpenML: Networked science in machine learning. SIGKDD Explorations, 15(2):49–60, 2014. 4 Wainer, J. and Cawley, G. Empirical Evaluation of Resampling Procedures for Optimising SVM Hyperparameters. Journal of Machine Learning Research, 18:1–35, 2017. B Wainwright, M. High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cambridge university press, 2019. C.2 Wistuba, M., Schilling, N., and Schmidt-Thieme, L. Scalable Gaussian process-based transfer surrogates for Hyperparameter Optimization. Machine Learning, 107(1):43–78, 2018. G.1 Wu, J., Toscano-Palmerin, S., Frazier, P., and Wilson, A. Practical multi-fidelity Bayesian optimization for hyperparameter tuning. In Peters, J. and Sontag, D. (eds.), Proceedings of The 36th Uncertainty in Artificial Intelligence Conference (UAI’20), pp. 788–798. PMLR, 2020. B Zheng, A. and Bilenko, M. Lazy paired hyper-parameter tuning. In Rossi, F. (ed.), Proceedings of the 23rd International Joint Conference on Artificial Intelligence (IJCAI’13), pp. 1924–1931, 2013. 5, B Zimmer, L., Lindauer, M., and Hutter, F. Auto-Pytorch: Multi-fidelity metalearning for efficient and robust AutoDL. IEEE Transactions on Pattern Analysis and Machine Intelligence, 43:3079–3090, 2021. 4.1, F.2 Zou, H. and Hastie, T. Regularization and variable selection via the elastic net. Journal of the Royal Statistical Society Series B: Statistical Methodology, 67(2):301–320, 2005. 4.1 15A Notation Table 2: Notation table. We discuss all symbols used in the main paper. Xi Random vector, describing the features Yi Random variable, describing the target Zi = (Xi, Yi) Data point D = {Zi}n i=1 Dataset consisting of iid random variables n Number of observations g Inducer/ML algorithm h Model, created by the inducer via h = gλ(D) λ Hyperparameter configuration Λ Finite set of all hyperparameter configurations J |Λ|, i.e., the number of hyperparameter configurations gλj Hyperparameterized inducer µ(λ) Expected loss of a hyperparameterized inducer on the distribution of a dataset ℓ(Z, h) Loss of a model h on a fresh observation Z M Number of folds in M-fold cross-validation α Percentage of samples to be used for validation I1,j, . . . ,IM,j ⊂ {1, . . . , n} M sets of validation indices, to be used for evaluating λj Vm,j Validation data for fold m and configuration λj Tm,j Training data for fold m and configuration λj L(Vm,j, gλj (Tm,j)) Validation loss for fold m and configuration λj bµ(λj) M-fold validation loss σ2 Increase in variance of validation loss caused by resampling τ2 Decrease in correlation among validation losses caused by reshuffling τi,j,M Resampling-related component of validation loss covariance K(·, ·) Kernel capturing the covariance of the pointwise losses between two HPCs ϵ(λj) Zero-mean Gaussian process, see Equation (2) d Number of hyperparameters κ Curvature constant of covariance kernel η Density of hyperparameter set Λ m Local curvature at the minimum of the loss surface µ σ Lower bound on the noise level B(τ) Part of the regret bound penalizing reshuffling A(τ) Part of the regret bound rewarding reshuffling B Extended Related Work Due to the black box nature of the HPO problem (Feurer & Hutter, 2019; Bischl et al., 2023), gradient free, zeroth-order optimization algorithms such as BO (Garnett, 2023), Evolutionary Strate- gies (Loshchilov & Hutter, 2016) or a simple random search (Bergstra & Bengio, 2012) have become standard optimization algorithms to tackle vanilla HPO problems. In the last decade, most research on HPO has been concerned with constructing new algorithms that excel at finding configurations with a low estimated generalization error. Examples include BO variants such as as HEBO (Cowen-Rivers et al., 2022) or SMAC3 (Lindauer et al., 2022). Another direction of HPO research has been concerned with speeding up the HPO process to allow more efficient spending of compute resources. Multifidelity HPO, for example, turns the black box optimization problem into a gray box one by making use of lower fidelity approximations to the target function, i.e., using fewer numbers of epochs or subsets of the data for cheap low-fidelity evaluations that approximate the costly high-fidelity evaluation. Examples include bandit-based budget allocation algorithms such as Successive Halving (Jamieson & Talwalkar, 2016), Hyperband (Li et al., 2018) and their extensions that use non-random search mechanisms (Falkner et al., 2018; Awad et al., 2021; Mallik et al., 2023) or algorithms making use of multi-fidelity information in the context of BO (Swersky et al., 2014; Klein et al., 2017; Wu et al., 2020; Kadra et al., 2023). Several works address the problem of speeding up cross-validation techniques and use techniques that could be described as grey box optimization techniques. Besides the ones mentioned in the main paper (Thornton et al., 2013; Zheng & Bilenko, 2013), it is possible to employ racing techniques for model selection in machine learning as demonstrated by Lang et al. (2015), and there has been a recent interest in methods that adapt the cost of running full cross-validation procedures (Bergman et al., 2024; Buczak et al., 2024). When addressing the problem of HPO, we must acknowledge an inherent mismatch between the explicit objective we optimize – namely, the estimated generalization performance of a model – and the actual implicit optimization goal, which is to identify a configuration that yields the best 16generalization performance on new, unseen data. Typically, evaluations and comparisons of different HPO algorithms focus exclusively on the final best validation performance (i.e., the objective that is directly optimized), even though an unbiased estimate of performance on an external unseen test set might be available. While this approach is logical for assessing the efficacy of an optimization algorithm based on the metric it seeks to improve, relying solely on finding an optimal validation configuration is beneficial only if there is reason to assume a strong correlation between the optimized validation performance and true generalization ability on new, unseen test data. This discrepancy can be found deeply within the HPO community, where the evaluation of HPO algorithms on standard benchmark libraries is usually done solely with respect to the validation performance (Eggensperger et al., 2021; Pineda Arango et al., 2021; Salinas et al., 2022; Pfisterer et al., 2022).5 This relationship between validation performance (i.e., the estimated generalization error derived from resampling) and true generalization performance (e.g., assessed through an outer holdout test set or additional resampling) of an optimal validation configuration found during HPO remains a largely unexplored area of research. In general, little research has focused on the selection of resampling types, let alone the automated selection of resampling types (Guyon et al., 2010; Feurer et al., 2022). While we usually expect that a more intensive resampling will reduce the variance of the estimated generalization error and thereby improve the (rank) correlation between optimized validation and unbiased outer test performance within HPCs, this benefit is naturally offset by a higher computational expense. Overall, there is little research on which resampling method to use in practice for model selection, and we only know of a study for support vector machines (Wainer & Cawley, 2017), a simulation study for clinical prediction models (Dunias et al., 2024), a study on feature selection (Molinaro et al., 2005) and a study on fast CV (Bergman et al., 2024). In addition, ML-Plan (Mohr et al., 2018) proposed a two-stage procedure. In a first stage (search), the tool uses planning on hierarchical task networks to find promising machine learning pipelines on 70% of the training data. In a second step (selection), it uses 100% of the training data and retrains the most promising candidates from the search step. Finally, it uses a combination of the internal generalization error estimation that was used during search and the 0.75 percentile of the generalization error estimation from the selection step to make a more unbiased selection of the final model. The paper found that this improves performance over using only regular cross-validation for search and selection. The general consensus, that is in agreement with our findings, is that CV or repeated CV generally leads to better generalization performance. In addition, while there are theoretical works that compare the accuracy of estimating the generalization error of holdout and CV (Blum et al., 1999), our goals is to correctly identify a single solution, which generalizes well, see the excellent survey by Arlot & Celisse (2010) for a discussion on this topic. Bouthillier et al. (2021) studied the sources of variance in machine learning experiments, and find that the split into training and test data has the largest impact. Consequently, they suggest to reshuffle the data prior to splitting it into the training, which is then used for HPO, and the test set. We followed their suggestion when designing our experiments and draw a new test sample for every replication, see Section 4.1 and Appendix F. This dependence on the exact split was further already discussed in the context of how much the outcome of a statistical test on results of machine learning experiments depended on the exact train-test split (Bouckaert, 2004). Finally, the first warning against comparing too many hypothesis using cross-validation was raised by Schaffer (1993), and in addition to the works discussed in Section 5 in the main paper, also picked up by Rao et al. (2008); Cawley & Talbot (2010). Moreover, the problem of finding a correct \"upper objective\" in a bilevel optimization problem has been noted (Guyon et al., 2010, 2015, 2019). Also, in the related field of algorithm configuration the problem has been identified (Eggensperger et al., 2019). B.1 Current Treatment of Resamplings in HPO Libraries and Software In Table 3, we provide a brief summary of how resampling is handled in popular HPO libraries and software.6 For each library, we checked whether the core functionality, examples, or tutorials mention 5We admit that these benchmark libraries implement efficient benchmarking methods such as surro- gate (Eggensperger et al., 2018; Pfisterer et al., 2022) or tabular benchmarks (Pineda Arango et al., 2021). It would be possible to adapt them to return the test performance, however, changes in the HPO evaluation protocol, such as the one we propose, would not be feasible. 6This summary is not exhaustive but reflects the general consensus observed in widely-used software. 17the possibility of reshuffling the resampling during HPO or if the resampling is considered fixed. If reshuffling is used in an example, mentioned, or if core functionality uses it, we mark it with a ✓. If it is unclear or inconsistent across examples and core functionality, we mark it with a ?. Otherwise, we use a ✗. Our conclusion is that the concept of reshuffling resampling generally receives little attention. Table 3: Exemplary Treatment of Resamplings in HPO Libraries and Software Software Reshuffled? Reference(s) sklearn ✗ GridSearchCV1/ RandomizedSearchCV2 HEBO ✗ sklearn_tuner3 optuna ? Inconsistency between examples 4,5,6 bayesian-optimization ✗ sklearn Example7,8 ax ✗ CNN Example9 spearmint ✗ No official HPO Examples scikit-optimize ✗ BO for GBT Example7,10 SMAC3 ✗ SVM Example7,11 dragonfly ✗ Tree Based Ensemble Example12 aws sagemaker ✗ Blog Post13 raytune ? Inconsistency between examples 14, 15 hyperopt(-sklearn) ? Cost Function Logic 16 ✗: no reshuffling, ?: both reshuffling and no reshuffling or unclear, ✓: reshuffling 1 https://github.com/scikit-learn/scikit-learn/blob/8721245511de2f225ff5f9aa5f5fadce663cd4a3/sklearn/m odel_selection/_search.py#L1263 2 https://github.com/scikit-learn/scikit-learn/blob/8721245511de2f225ff5f9aa5f5fadce663cd4a3/sklearn/m odel_selection/_search.py#L1644 3 https://github.com/huawei-noah/HEBO/blob/b60f41aa862b4c5148e31ab4981890da6d41f2b1/HEBO/hebo/sklearn_t uner.py#L73 4 https://github.com/optuna/optuna-integration/blob/15e6b0ec6d9a0d7f572ad387be8478c56257bef7/optuna_in tegration/sklearn/sklearn.py#L223 here sklearn’s cross_validate is used which by default does not reshuffle the resampling https://github.com/scikit-learn/scikit-learn/blob/8721245511de2f225ff5f9aa5f5fadce663cd4a3/sklearn/m odel_selection/_validation.py#L186 5 https://github.com/optuna/optuna-examples/blob/dd56b9692e6d1f4fa839332edbcdd93fd48c16d8/pytorch/py torch_simple.py#L79 here, data loaders for train and valid are instantiated within the objective of the trial but the data within the loaders is fixed 6 https://github.com/optuna/optuna-examples/blob/dd56b9692e6d1f4fa839332edbcdd93fd48c16d8/xgboost/xgbo ost_simple.py#L22 here, the train validation split is performed within the objective of the trial and no seed is set which results in reshuffling https://github.com/scikit-learn/scikit-learn/blob/8721245511de2f225ff5f9aa5f5fadce663cd4a3/s klearn/model_selection/_split.py#L2597 7 functionality relies on sklearn’s cross_val_score which by default does not reshuffle the resampling https://github.com/sciki t-learn/scikit-learn/blob/8721245511de2f225ff5f9aa5f5fadce663cd4a3/sklearn/model_selection/_validati on.py#L631 8 https://github.com/bayesian-optimization/BayesianOptimization/blob/c7e5c3926944fc6011ae7ace29f7b5ed0f 9c983b/examples/sklearn_example.py#L32 9 https://github.com/facebook/Ax/blob/ac44a6661f535dd3046954f8fd8701327f4a53e2/tutorials/tune_cnn_serv ice.ipynb#L39 and https://github.com/facebook/Ax/blob/ac44a6661f535dd3046954f8fd8701327f4a53e2/ax/util s/tutorials/cnn_utils.py#L154 10 https://github.com/scikit-optimize/scikit-optimize/blob/a2369ddbc332d16d8ff173b12404b03fea472492/ex amples/hyperparameter-optimization.py#L82C21-L82C36 11 https://github.com/automl/SMAC3/blob/9aaa8e94a5b3a9657737a87b903ee96c683cc42c/examples/1_basics/2_sv m_cv.py#L63 12 https://github.com/dragonfly/dragonfly/blob/3eef7d30bcc2e56f2221a624bd8ec7f933f81e40/examples/tree_r eg/skltree.py#L111 13 https://aws.amazon.com/blogs/architecture/field-notes-build-a-cross-validation-machine-learning-mod el-pipeline-at-scale-with-amazon-sagemaker/ 14 https://github.com/ray-project/ray/blob/3f5aa5c4642eeb12447d9de5dce22085512312f3/doc/source/tune/exa mples/tune-pytorch-cifar.ipynb#L120 here, data loaders for train and valid are instantiated within the objective but the data within the loaders are fixed 15 https://github.com/ray-project/ray/blob/3f5aa5c4642eeb12447d9de5dce22085512312f3/doc/source/tune/exa mples/tune-xgboost.ipynb#L335 here, the train validation split is performed within the objective and no seed is set which results in reshuffling https://github.com/scikit-learn/scikit-learn/blob/8721245511de2f225ff5f9aa5f5fadce663cd4a3 /sklearn/model_selection/_split.py#L2597 16 https://github.com/hyperopt/hyperopt-sklearn/blob/4bc286479677a0bfd2178dac4546ea268b3f3b77/hpsklearn /estimator/_cost_fn.py#L144 dependence on random seed which by default is not set and there is no discussion of reshuffling and behavior is somewhat unclear 18C Proofs of the Main Results C.1 Proof of Theorem 2.1 We impose stability assumptions on the learning algorithm similar to Bayle et al. (2020); Austern & Zhou (2020). Let Z, Z1, . . . ,Zn, Z′ 1, be iid random variables. Define T = {Zi}n i=1, and T ′ as T but with Zn replaced by the independent copy Z′ n. Define eℓn(z, λ) = ℓ(z, gλ(T )) − E[ℓ(Z, gλ(T )) | T], assume that each gλ(T ) is invariant to the ordering in T , ℓ is bounded, and max λ∈Λ E{[eℓ(Z, gλ(T )) − eℓ(Z, gλ(T ′))]2} = o(1/n). (4) This loss stability assumption is rather mild, see Bayle et al. (2020) for an extensive discussion. Further, define the risk R(g) = E[ℓ(Z, g)] and assume that for every λ ∈ Λ, there is a prediction rule g∗ λ such that max λ∈Λ E[|R(gλ(T )) − R(g∗ λ)|] = o(1/√n). (5) This assumption requires gλ(T ) to converge to some fixed prediction rule sufficiently fast and serves as a reasonable working condition for our purposes. It is satisfied, for example, when ℓ is the square loss and gλ is an empirical risk minimizer over a hypothesis class Gλ with finite VC-dimension. For further examples, see, e.g., Bousquet & Zhivotovskiy (2021), van Erven et al. (2015), and references therein. The assumption could be relaxed, but this would lead to a more complicated limiting distribution but with the same essential interpretation. Theorem C.1. Under assumptions (4) and (5), it holds √n (bµ(λj) − µ(λj))J j=1 →d N(0, Σ), where Σj,j′ = τi,j,M lim n→∞ Cov[¯ℓn(Z, λj), ¯ℓn(Z, λj′ )], τj,j′,M = lim n→∞ 1 nM2α2 nX i=1 MX m=1 MX m′=1 Pr(i ∈ Im,j ∩ Im′,j′ ). Proof. Define eµ(λj) = 1 M MX m=1 E[L(Vm,j, gλj (Tm,j)) | Tm,j]. By the triangle inequality (first and second step), Jensen’s inequality (third step), and (5) (last step), E[|eµ(λj) − µ(λj)|] ≤ max 1≤m≤M E \u0002\f\fE[L(Vm,j, gλj (Tm,j)) | Tm,j] − E[L(Vm,j, gλj (Tm,j))] \f\f\u0003 ≤ max 1≤m≤M E h\f\f\fE[L(Vm,j, gλj (Tm,j)) | Tm,j] − E[L(Vm,j, g∗ λj )] \f\f\f i + max 1≤m≤M E h\f\f\fE[L(Vm,j, gλj (Tm,j))] − E[L(Vm,j, g∗ λj )] \f\f\f i ≤ 2 max 1≤m≤M E h\f\f\fE[L(Vm,j, gλj (Tm,j)) | Tm,j] − E[L(Vm,j, g∗ λj )] \f\f\f i = 2 max 1≤m≤M E h\f\f\fR(gλj (Tm,j)) − R(g∗ λj ) \f\f\f i = o(1/√n). Next, assumption (4) together with Theorem 2 and Proposition 3 of Bayle et al. (2020) yield √n (bµ(λj) − eµ(λj)) − 1 M MX m=1 1 α√n X i∈Im,j ¯ℓn(Zi, λj) →p 0. 19Now rewrite 1 Mα√n MX m=1 X i∈Im,j ¯ℓn(Zi, λj) = 1 Mα√n nX i=1 MX m=1 1(i ∈ Im,j)¯ℓn(Zi, λj) | {z } :=ξ(j) i,n . The sequence (ξi,n)n i=1 = (ξ(j) i,n, . . . , ξ(j) i,n)n i=1 is a triangular array of independent, centered, and bounded random vectors. Because 1(Zi ∈ Vm,j) and Zi are independent, it holds Cov(ξ(j) i,n, ξ(j′) i,n ) = MX m=1 MX m′=1 E[1(i ∈ Im,j ∩ Im′,j′ )]E[¯ℓn(Zi, λj)¯ℓn(Zi, λj′ )], so lim n→∞ Cov \" 1 Mα√n nX i=1 ξ(j) i,n, 1 Mα√n nX i=1 ξ(j′) i,n # = lim n→∞ 1 nM2α2 nX i=1 Cov h ξ(j) i,n, ξ(j′) i,n i = Σj,j′ . Now the result follows from Lindeberg’s central limit theorem for triangular arrays (e.g., van der Vaart, 2000, Proposition 2.27). C.2 Proof of Theorem 2.2 We want to bound the probability thatµ(ˆλ) −µ(λ∗) is large. For some δ >0, define the set of ‘good’ hyperparameters Λδ = {λj : µ(λj) − µ(λ∗) ≤ δ}. Now Pr \u0010 µ(bλ) − µ(λ∗) > δ \u0011 = Pr \u0010 bλ /∈ Λδ \u0011 = Pr \u0012 min λ/∈Λδ bµ(λ) < min λ∈Λδ bµ(λ) \u0013 ≤ Pr \u0012 min λ/∈Λδ bµ(λ) < min λ∈Λδ/2 bµ(λ) \u0013 = Pr \u0012 min λ/∈Λδ µ(λ) + ϵ(λ) < min λ∈Λδ/2 µ(λ) + ϵ(λ) \u0013 ≤ Pr \u0012 δ + min λ/∈Λδ ϵ(λ) < δ/2 + min λ∈Λδ/2 ϵ(λ) \u0013 = Pr \u0012 min λ/∈Λδ ϵ(λ) − min λ∈Λδ/2 ϵ(λ) < −δ/2 \u0013 = Pr \u0012 max λ/∈Λδ ϵ(λ) − max λ∈Λδ/2 ϵ(λ) > δ/2 \u0013 . (ϵ d = −ϵ) There is a tension between the two maxima. The more λ’s there are in Λδ/2 and the less they are correlated, the more likely it is to find one ϵ(λ) that is large. This makes the probability small. However, the less ϵ is correlated, the larger is maxλ/∈Λδ ϵ(λ), making the probability large. To formalize this, use the Gaussian concentration inequality (Talagrand, 2005, Lemma 2.1.3): Pr \u0012 max λ/∈Λδ ϵ(λ) − max λ∈Λδ/2 ϵ(λ) > δ/2 \u0013 ≤ Pr \u0012 2 \f\f\f\fmax λ∈Λ ϵ(λ) − E \u0014 max λ∈Λ ϵ(λ) \u0015\f\f\f\f > δ/2 − E \u0014 max λ∈Λδ/2 ϵ(λ) \u0015 + E \u0014 max λ/∈Λδ ϵ(λ) \u0015\u0013 ≤ 2 exp ( − \u0000 δ/2 − E \u0002 maxλ∈Λδ/2 ϵ(λ) \u0003 + E[maxλ/∈Λδ ϵ(λ)] \u00012 8σ2 ) , provided δ/2−E \u0002 maxλ∈Λδ/2 ϵ(λ) \u0003 +E[maxλ/∈Λδ ϵ(λ)] ≥ 0. We bound the two maxima separately. 20Lower Bound for Maximum over the Good Set Recall the definition of m right before Theorem 2.2 and observe Λδ/2 = {λ: µ(λ) − µ(λ∗) ≤ δ/2} ⊃ {λ: m∥λ − λ∗∥2 ≤ δ/2} = {λ: ∥λ − λ∗∥ ≤(δ/2m)1/2} = B(λ∗, (δ/2m)1/2). Pack the ball B(λ∗, (δ/2m)1/2) with smaller balls with radius η. We can always construct such a packing with at least (δ/2mη2)d/2 elements. By assumption, each small ball contains at least one element of Λ. Pick one element from each small ball and collect them into the set Λ′ δ/2. By construction, |Λ′ δ/2| ≥(δ/2mη2)d/2 and min λ̸=λ′∈Λ′ δ/2| ∥λ − λ′∥ ≥η. Sudakov’s minoration principle (e.g., Wainwright, 2019, Theorem 5.30) gives E \u0014 max λ∈Λδ/2 ϵ(λ) \u0015 ≥ 1 2 q log |Λ′ δ/2| min {λ̸=λ′}∩Λ′ δ/2 p Var[ϵ(λ) − ϵ(λ′)] ≥ 1 2 q log |Λ′ δ/2| min ∥λ−λ′∥≥η p Var[ϵ(λ) − ϵ(λ′)]. In general, Var[ϵ(λ) − ϵ(λ′)] = K(λ, λ) + K(λ′, λ′) − 2τ2K(λ, λ′) = (1 − τ2)[K(λ, λ) + K(λ′, λ′)] + τ2[K(λ, λ) − K(λ, λ′)] + τ2[K(λ′, λ′) − K(λ, λ′)] ≥ 2σ2(1 − τ2). Hence, we have min ∥λ−λ′∥≥η Var[ϵ(λ) − ϵ(λ′)] ≥ 2σ2(1 − τ2), which implies E \u0014 max λ∈Λδ/2 ϵ(λ) \u0015 ≥ 1 2σ √ d p 1 − τ2 p log(δ/2mη2) =: σ √ dA(τ, δ)/2. Upper Bound for Maximum over the Bad Set Dudley’s entropy bound (e.g., Giné & Nickl, 2016, Theorem 2.3.6) gives E \u0014 max λ/∈Λδ ϵ(λ) \u0015 ≤ 12 Z ∞ 0 p log N(s)ds, where N(s) is the minimum number of points λ1, . . . ,λN(s) such that sup λ∈Λ min 1≤k≤N(s) p Var[ϵ(λ) − ϵ(λk)] ≤ s. Note that sup λ,λ′∈Λ p Var[ϵ(λ) − ϵ(λ′)] ≤ 2σ, so N(s) = 1 for all s ≥ 2σ. For s2 ≤ 4σ2(1 − τ2), we can use the trivial bound N(s) ≤ J. For s2 > 4σ2(1 − τ2), cover Λ with ℓ2-balls of size (s/2στκ ). We can do this with less than N(s) ≤ (6σκ/s)d ∨ 1 such balls. Let λ1, . . . ,λN be the centers of these balls. In general, it holds Var[ϵ(λ) − ϵ(λ′)] = K(λ, λ) + K(λ′, λ′) − 2τ2K(λ, λ′) = (1 − τ2)[K(λ, λ) + K(λ′, λ′)] + τ2[K(λ, λ) − K(λ, λ′)] + τ2[K(λ′, λ′) − K(λ, λ′)] ≤ 2(1 − τ2)σ2 + 2τ2σ2κ2∥λ − λ′∥2. 21For s2 > 4σ2(1 − τ2), we thus have sup λ∈Λ min 1≤k≤N(s) Var[ϵ(λ) − ϵ(λk)] ≤ sup ∥λ−λ′∥2≤(s/2τσκ)2 Var[ϵ(λ) − ϵ(λ′)] ≤ 2(1 − τ2)σ2 + 2τ2σ2κ2(s/2τσκ )2 ≤ s2, as desired. Now decompose the integral Z ∞ 0 p log N(s)ds = Z 2σ √ 1−τ2 0 p log N(s)ds + Z 2σ 2σ √ 1−τ2 p log N(s)ds ≤ 2σ √ d p 1 − τ2 p log J + Z 2σ 2σ √ 1−τ2 p log N(s)ds. For the second term, compute Z 2σ σ √ 1−τ2 p log N(s)ds ≤ √ d Z 2σ 2σ √ 1−τ2 p log(6σκ/s)+ ds = σ √ d Z 2 2 √ 1−τ2 p log(6κ/s)+ ds ≤ σ √ d \u0012Z 2 0 log(6κ/s)+ ds \u00131/2 \u0010 2(1 − p 1 − τ2) \u00111/2 = σ √ d p 2 + 2 log(3κ)+ \u0010 2(1 − p 1 − τ2) \u00111/2 = 2σ √ d p 1 + log(3κ)+ τ (1 + √ 1 − τ2)1/2 ≤ 2σ √ dτ p 1 + log(3κ)+. We have shown that E \u0014 max λ/∈Λδ ϵ(λ) \u0015 ≤ 24σ √ d hp 1 − τ2 p log J + τ p 1 + log(3κ)+ i =: σ √ dB(τ)/4. Integrating Probabilities Summarizing the two previous steps, we have Pr \u0010 µ(bλ) − µ(λ∗) > δ \u0011 ≤ 2 exp    − \u0010 δ − σ √ d[B(τ) − A(τ, δ)] \u00112 36σ2    , provided t ≥ σ √ d[B(τ) − A(τ, δ)]. Now for any s ≥ 0 and t ≥ 2es2 mη2, it holds A(τ, s) ≥ (σ/σ) p 1 − τ2s =: A(τ)s. In particular, if t ≥ 2es2 mη2 + σ √ d[B(τ) − A(τ)s] =: C, we have Pr \u0010 µ(bλ) − µ(λ∗) > δ \u0011 ≤ 4 exp    − \u0010 δ − σ √ d[B(τ) − A(τ)s] \u00112 36σ2    . 22Integrating the probability gives E[µ(bλ) − µ(λ∗)] = Z ∞ 0 Pr \u0010 µ(bλ) − µ(λ∗) > δ \u0011 dδ = Z C 0 Pr \u0010 µ(bλ) − µ(λ∗) > δ \u0011 dδ + Z ∞ C Pr \u0010 µ(bλ) − µ(λ∗) > δ \u0011 dδ ≤ C + Z ∞ C exp    − \u0010 δ − σ √ d[B(τ) − A(τ)s] \u00112 36σ2    dδ ≤ C + √ 36σ = 2es2 mη2 + σ √ d[B(τ) − A(τ)s] + 6σ. Simplifying The bound can be optimized with respect to s, but the solution involves the Lambert W-function, which has no analytical expression. Instead choose s for simplicity as s = s log \u0012 σ 2mη2 \u0013 + . which gives E[µ(bλ) − µ(λ∗)] ≤ σ √ d \" 8 + B(τ) − A(τ) s log \u0012 σ 2mη2 \u0013# . D Additional Results on the Density of Random HPC Grids Lemma D.1. Suppose that the J elements in Λ are drawn independently from a continuous density p with c := min∥λ∥≤1 p(λ) > 0. Then with probability at least 1 − δ, η ≲ \u0010p log(1/δ)/J \u00111/d , and with probability 1, η ≲ \u0010p log(J)/J \u00111/d , for all J sufficiently large. Proof. We want to bound the probability that there is a λ such that |B(λ, η) ∩ Λ| = 0. In what follows λ is silently understood to have norm bounded by 1. Let eλ1, . . . ,eλN the centers of η/2-balls covering {∥λ∥ ≤1}, for which we may assume N ≤ (6/η)d. For eλk the closest center to λ, it holds ∥λ′ − λ∥ ≤ ∥λ′ − eλk∥ + ∥eλk − λ∥ ≤ ∥λ′ − eλk∥ + η/2, so ∥λ′ − eλk∥ ≤η/2 implies ∥λ′ − λ∥ ≤η. We thus have Pr(∃λ: |B(λ, η) ∩ Λ| = 0) = Pr   inf λ JX i=1 1{∥λi − λ∥ ≤η} ≤0 ! ≤ Pr   min 1≤k≤N JX i=1 1{∥λi − eλk∥ ≤η/2} ≤0 ! . 23Further Pr   min 1≤k≤N JX i=1 1{∥λi − eλk∥ ≤η/2} ≤0 ! = Pr   max 1≤k≤N JX i=1 −1{∥λi − eλk∥ ≤η/2} ≥0 ! ≤ Pr   max 1≤k≤N JX i=1 E h 1{∥λi − eλk∥ ≤η/2} i − 1{∥λi − eλk∥ ≤η/2} ≥J inf λ E[1{∥λi − λ∥ ≤η/2}] ! . It holds E[1{∥λi − λ∥ ≤η/2}] = Pr (∥λi − λ∥ ≤η/2) = Z ∥λ′−λ∥≤η/2 p(λ′)dλ′ ≥ c vol(B(0, η/2)) = cvd(η/2)d, where vd = vol(B(0, 1)). Now the union bound and Hoeffding’s inequality give Pr   min 1≤k≤N JX i=1 1{∥λi − eλk∥ ≤η/2} ≤0 ! ≤ N exp \u0012 −Jc2v2 d(η/2)2d 2 \u0013 ≤ (6/η)d exp \u0012 −Jc2v2 d(η/2)2d 2 \u0013 . Choosing η = 2 \u0012q 2 log(3d √ Jcvd/δ)/ √ Jcvd \u00131/d gives Pr(∃λ: |B(λ, η) ∩ Λ| = 0) ≤ δ/ q 2 log(3d √ Jcvd), which is bounded by δ when √ J ≥ e1/2/3dcvd. Further, setting η = 2( p 6 log(J)/ √ Jcvd)1/d gives Pr   min 1≤k≤N JX i=1 1{∥λi − eλk∥ ≤η/2} ≤0 ! ≲ J−5/2, so that ∞X J=1 Pr   min 1≤j≤J min 1≤k≤N jX i=1 1{∥λi − eλk∥ ≤η/2} ≤0 ! ≤ ∞X J=1 J Pr   min 1≤k≤N JX i=1 1{∥λi − eλk∥ ≤η/2} ≤0 ! ≲ ∞X J=1 1 J3/2 < ∞. Now the Borel-Cantelli lemma (e.g., Kallenberg, 1997, Theorem 4.18) implies that, with probability 1, |B(λ, η) ∩ Λ| ≥1, for all J sufficiently large. 24E Selected Validation Schemes E.1 Definition of Index Sets Recall: (i) (holdout) Let M = 1 and I1,j = I1 for all j = 1, . . . , J, and some size-⌈αn⌉ index set I1. (ii) (reshuffled holdout) Let M = 1 and I1,1, . . . ,I1,J be independently drawn from the uniform distribution over all size-⌈αn⌉ subsets from {1, . . . , n}. (iii) (M-fold CV) Let α = 1/M and I1, . . . ,IM be a disjoint partition of {1, . . . , n}, and Im,j = Im for all j = 1, . . . , J. (iv) (reshuffled M-fold CV) Let α = 1/M and (I1,j, . . . ,IM,j ), j= 1, . . . , J, be independently drawn from the uniform distribution over disjoint partitions of {1, . . . , n}. (v) (M-fold holdout) LetIm, m= 1, . . . , M, be independently drawn from the uniform distribution over size-⌈αn⌉ subsets of {1, . . . , n} and set Im,j = Im for all m = 1, . . . , M, j= 1, . . . , J. (vi) (reshuffled M-fold holdout) Let Im,j, m= 1, . . . , M, j= 1, . . . , J, be independently drawn from the uniform distribution over size-⌈αn⌉ subsets of {1, . . . , n}. E.2 Derivation of Reshuffling Parameters in Limiting Distribution Recall τi,j,M = 1 nM2α2 nX s=1 MX m=1 MX m′=1 Pr(s ∈ Im,i ∩ Im′,j). For all schemes in the proposition, the probabilities are independent of the index s, so the average over s = 1, . . . , ncan be omitted. We now verify the constants σ, τfrom Table 1. (i) It holds Pr(s ∈ I1,i ∩ I1,j) = Pr(s ∈ I1) = α. Hence, τi,j,1 = 1/α = 1/α × 1 = σ2 × τ2. (ii) (reshuffled holdout) This is a special case of part (vi) with M = 1. (iii) ( M-fold CV) It holds Pr(s ∈ Im,i ∩ Im′,j) = Pr(s ∈ Im ∩ Im′ ) = \u001a1/M, m = m′, 0, m ̸= m′. Only M probabilities in the double sum are non-zero, whence τi,j,M = 1 M2α2 × M/M = 1/α2M2 = 1 × 1 = σ2 × τ2, where we used α = 1/M. (iv) (reshuffled M-fold CV) It holds Pr(s ∈ Im,i ∩ Im′,j) =    1/M, m = m′, i= j 0, m ̸= m′, i= j 1/M2, m = m′, i̸= j 1/M2, m ̸= m′, i̸= j. For i = j, only M probabilities in the double sum are non-zero. Also using α = 1/M, we get τi,j,M = 1 M2α2 × M × 1/M = 1 = σ2. For i ̸= j, τi,j,M = 1 M2α2 × M2 × 1/M2 = 1 × 1 = σ2 × τ2. 25(v) ( M-fold holdout) It holds Pr(s ∈ Im,i ∩ Im′,j) = Pr(s ∈ Im ∩ Im′ ) = \u001aα, m = m′, α2, else. This gives τi,j,M = 1 M2α2 × [M × α + (M − 1)M × α2] = [1/αM + (M − 1)/M] × 1 = σ2 × τ2. for all i, j. (vi) (reshuffled M-fold holdout) It holds Pr(s ∈ Im,i ∩ Im′,j) = \u001aα, m = m′, i= j α2, else. For i = j, this gives τi,j,M = 1 M2α2 × [M × α + (M − 1)M × α2] = 1/αM + (M − 1)/M. For i ̸= j, τi,j,M = 1 M2α2 × (M2 × α2) = 1. This implies that (1) holds with σ2 = 1/Mα + (M − 1)/M, τ2 = 1/(1/Mα + (M − 1)/M). Remark E.1. Although not technically covered by Theorem 2.1, performing independent bootstraps for each λj correspond to reshuffled n-fold holdout with α = 1 /n. Accordingly, σ ≈ √ 2 and τ ≈ p 1/2. F Details Regarding Benchmark Experiments F.1 Datasets We list all datasets used in the benchmark experiments in Table 4. Table 4: List of datasets used in benchmark experiments. All information can be found on OpenML (Vanschoren et al., 2014). OpenML Dataset ID Dataset Name Size ( n × p) 23517 numerai28.6 96320 × 21 1169 airlines 539383 × 7 41147 albert 425240 × 78 4135 Amazon_employee_access 32769 × 9 1461 bank-marketing 45211 × 16 1590 adult 48842 × 14 41150 MiniBooNE 130064 × 50 41162 kick 72983 × 32 42733 Click_prediction_small 39948 × 11 42742 porto-seguro 595212 × 57 Note that datasets serve as data generating processes (DGPs; Hothorn et al., 2005). As we are mostly concerned with the actual generalization performance of the final best HPC found during HPO based on validation performance we rely on a comparably large held out test set that is not used during HPO. We therefore use 5000 data points sampled from a DGP as an outer test set. To further be able to measure the generalization performance robustly for varying data sizes available during HPO, we construct concrete tasks based on the DGPs by sampling subsets of (train_valid; n) size 500, 1000 and 5000 from the DGPs. This results in 30 tasks in total (10 DGPS × 3 train_valid sizes). For more details and the concrete implementation of this procedure, see Appendix F.3. We also collected another 5000 data points as an external validation set, but did not use it. Therefore, we had to tighten the restriction to 10000 data points mentioned in the main paper to 15000 data points as the lower bound on data points. To allow for stronger variation over different replications, we decided to use 20000 as the final lower bound. 26F.2 Learning Algorithms Here we briefly present training pipeline details and search spaces of the learning algorithms used in our benchmark experiments. The funnel-shaped MLP is based on sklearn’s MLP Classifier and is constructed in the following way: The hidden layer size for each layer is determined by num_layers and max_units. We start with max_units and half the number of units for every subsequent layer to create a funnel. max_batch_size is the largest power of 2 that is smaller than the number of training samples available. We use ReLU as activation function and train the network optimizing logloss as a loss function via SGD using a constant learning rate and Nesterov momentum for 100 epochs. Table 5 lists the search space (inspired from Zimmer et al. (2021)) used during HPO. The Elastic Net is based on sklearn’s Logistic Regression Classifier. We train it for a maximum of 1000 iterations using the \"saga\" solver. Table 6 lists the search space used during HPO. The XGBoost and CatBoost search spaces are listed in Table 7 and Table 8, both inspired from their search spaces used in McElfresh et al. (2023). For both the Elastic Net and Funnel MLP, missing values are imputed in the preprocessing pipeline (mean imputation for numerical features and adding a new level for categorical features). Categorical features are target encoded in a cross-validated manner using a 5-fold CV . Features are then scaled to zero mean and unit variance via a standard scaler. For XGBoost, we impute missing values for categorical features (adding a new level) and target encode them in a cross-validated manner using a 5-fold CV . For CatBoost, no preprocessing is performed. XGBoost and CatBoost models are trained for 2000 iterations and stop early if the validation loss (using the default internal loss function used during training, i.e., logloss) does not improve over a horizon of 20 iterations. For retraining the best configuration on the whole train and validation data, the number of boosting iterations is set to the number of iterations used to find the best validation performance prior to the stopping mechanism taking action.7 F.3 Exact Implementation In the following, we outline the exact implementation of performing one HPO run for a given learning algorithm on a concrete task (dataset × train_valid size) and a given resampling. We release all code to replicate benchmark results and reproduce our analyses via https://github.com/slds-l mu/paper_2024_reshuffling. For a given replication (in total 10): 1. We sample (without replacement) train_valid size (500, 1000 or 5000 points) and test size (always 5000) points from the DGP (i.e. a concrete dataset in Table 4). These are shared for every learning algorithm (i.e. all learning algorithms are evaluated on the same data). 2. A given HPC is evaluated in the following way: • The resampling operates on the train validation 8 set of size train_valid. • The learning algorithm is configured by the HPC. • The learning algorithm is trained on training splits and evaluated on validation splits according to the resampling strategy. In case reshuffling is turned on, the training and validation splits are recreated for every HPO. We compute the Accuracy, ROC AUC and logloss when using a random search and compute ROC AUC when using HEBO or SMAC3 and average performance over all folds for resamplings involving multiple folds. • For each HPC we then always re-train the model on all train_valid data being available and evaluate the model on the held-out test set to compute an outer estimate of generalization performance for each HPC (regardless of whether it is the incumbent for a given iteration or not). 7For CV and repeated holdout we take the average number of boosting iterations over the models trained on the different folds. 8With train validation we refer to all data being available during HPO which is then further split by a resampling into train and validation sets. 27Table 5: Search Space for Funnel-Shaped MLP Classifier. Parameter Type Range Log num_layers Int. 1 to 5 No max_units Int. 64, 128, 256, 512 No learning_rate Num. 1 × 10−4 to 1 × 10−1 Yes batch_size Int. 16, 32, ..., max_batch_size No momentum Num. 0.1 to 0.99 No alpha Num. 1 × 10−6 to 1 × 10−1 Yes Table 6: Search Space for Elastic Net Classifier. Parameter Type Range Log C Num. 1 × 10−6 to 1 × 104 Yes l1_ratio Num. 0.0 to 1.0 No Table 7: Search Space for XGBoost Classifier. Parameter Type Range Log max_depth Int. 2 to 12 Yes alpha Num. 1 × 10−8 to 1.0 Yes lambda Num. 1 × 10−8 to 1.0 Yes eta Num. 0.01 to 0.3 Yes Table 8: Search Space for CatBoost Classifier. Parameter Type Range Log learning_rate Num. 0.01 to 0.3 Yes depth Int. 2 to 12 Yes l2_leaf_reg Num. 0.5 to 30 Yes 3. We evaluate 500 HPCs when using random search and 250 HPC when using HEBO or SMAC3 (SMAC4HPO facade). As resamplings, we use holdout with a 80/20 train-validation split and 5 folds for CV , so that the holdout strategy is just one fold of the CV and the fraction of data points being used for training and respectively validation are the same across different resampling strategies. 5-fold holdout simply repeats the holdout procedure five times and 5x 5-fold CV repeats the 5-fold CV five times. Each of the four resamplings can be reshuffled or not (standard). As mentioned above, the test set is only varied for each of the 10 replica (repetitions with different seeds), but consistent for different tasks (i.e. the different learning algorithms are evaluated on the same test set, similarly, also the different dataset subsets all share the same test set). This allows for fair comparisons of different resamplings on a concrete problem (i.e. a given dataset, train_valid size and learning algorithm). Additionally, for the random search, the 500 HPCs evaluated for a given learning algorithm are also fixed over different dataset and train_valid size combinations. This is done to allow for an isolation of the effect, the concrete resampling (and whether it is reshuffled or not) has on generalization performance, reducing noise arising due to different HPCs. Learning algorithms themselves are not explicitly seeded to allow for variation during model training over different replications. Resamplings and partitioning of data are always performed in a stratified manner with respect to the target variable. For the random search, we only ran (standard and reshuffled) holdout and (standard and reshuffled) 5x 5-fold CV experiments (because we can simulate 5-fold CV and 5-fold holdout experiments based 28on the results obtained from the 5x 5-fold CV (by only considering the first repeat or the first fold for each of the five repeats).9 For running HEBO or SMAC3, each resampling (standard and reshuffled for holdout, 5-fold holdout, 5-fold CV , 5x 5-fold CV) has to be actually run due to the adaptive nature of BO. For the random search experiments, this results in 10 (DGPs)× 3 (train_valid sizes) × 4 (learning algorithms) × 2 (holdout or 5x 5-fold CV) × 2 (standard or reshuffled) × 10 (replications) = 4800 HPO runs,10 each involving the evaluation of 500 HPCs and each evaluation of an HPC involving either 2 (for holdout; due to retraining on train validation data) or 26 (for 5x 5-fold CV; due to retraining on train validation data) model fits. In summary, the random search experiments involve the evaluation of 2.4 Million HPCs with in total 33.6 Million model fits. Similarly, for the HEBO and SMAC3 experiments, this each results in 10 (DGPs)× 3 (train_valid sizes) × 4 (learning algorithms) × 4 (holdout, 5-fold CV , 5x 5-fold CV or 5-fold holdout) × 2 (standard or reshuffled) × 10 (replications) = 9600 HPO runs 11, each involving the evaluation of 250 HPCs and each evaluation of an HPC involving either 2 (for holdout; due to retraining on train validation data), 6 (for 5-fold CV or 5-fold holdout; due to retraining on train validation data) or 26 (for 5x 5-fold CV; due to retraining on train validation data) model fits. In summary, the HEBO and SMAC3 experiments each involve the evaluation of 2.4 Million HPCs with in total 24 Million model fits. F.4 Compute Resources We estimate our total compute time for the random search, HEBO and SMAC3 experiments to be roughly 11.86 CPU years. Benchmark experiments were run on an internal HPC cluster equipped with a mix of Intel Xeon E5-2670, Intel Xeon E5-2683 and Intel Xeon Gold 6330 instances. Jobs were scheduled to use a single CPU core and were allowed to use up to 16GB RAM. Total emissions are estimated to be an equivalent of roughly 6508.67 kg CO2. G Additional Benchmark Results Visualizations G.1 Main Experiments In this section, we provide additional visualizations of the results of our benchmark experiments. Figure 6 illustrates the trade-off between the final number of model fits required by different resam- plings and the final average normalized test performance (AUC ROC) after running random search for a budget of 500 hyperparameter configurations. We can see that the reshuffled holdout on average comes close to the final test performance of the overall more expensive 5-fold CV . Below, we give an overview of the different types of additional analyses and visualizations we provide. Normalized metrics, i.e., normalized validation or test performance refer to the measure being scaled to [0, 1] based on the empirical observed minimum and maximum values obtained on the raw results level (ADTM; see Wistuba et al., 2018). More concretely, for each scenario consisting of a learning algorithm that is run on a given task (dataset × train_valid size) given a certain performance metric, the performance values (validation or test) for all resamplings and optimizers are normalized on the replication level to [0, 1] by subtracting the empirical best value and dividing by the range of performance values. Therefore a normalized performance value of 0 is best and 1 is worst. Note that we additionally provide further aggregated results on the learning algorithm level and raw results of validation and test performance via https://github.com/slds-lmu/paper_2024_reshuffl ing. • Random search – Normalized validation performance in Figure 7. 9We even could have simulated the vanilla holdout from the 5x 5-fold CV experiments by choosing an arbitrary fold and repeat but choose not to do so, to have some sanity checks regarding our implementation by being able to compare the \"true\" holdout with a the simulated holdout. 10Note that we do not have to take the 3 different metrics into account because random search allows us to simulate runs for different metric post hoc. 11Note that HEBO and SMAC3 were only run for ROC AUC as the performance metric. 29500 1000 5000 300 1000 3000 10000 300 1000 3000 10000 300 1000 3000 10000 0.20 0.25 0.30 0.35 0.40 0.25 0.30 0.35 0.40 0.45 0.30 0.35 0.40 0.45 No. Final Model Fits Mean Normalized T est Performance Reshuffling FALSE TRUE Resampling Holdout 5−fold CV 5−fold Holdout 5x 5−fold CV Figure 6: Trade-off between the final number of model fits required by different resamplings and the final average normalized test performance (AUC ROC) after running random search for a budget of 500 hyperparameter configurations. Averaged over different tasks, learning algorithms and replications separately for increasing n (train-validation sizes, columns). Shaded areas represent standard errors. – Normalized test performance in Figure 8. – Improvement in test performance over 5-fold CV in Figure 9. – Rank w.r.t. test performance in Figure 10. • HEBO and SMAC3 vs. random search holdout – Normalized validation performance in Figure 11. – Normalized test performance in Figure 12. – Improvement in test performance over standard holdout in Figure 13. – Rank w.r.t. test performance in Figure 14. • HEBO and SMAC3 vs. random search 5-fold holdout – Normalized validation performance in Figure 15. – Normalized test performance in Figure 16. – Improvement in test performance over standard 5-fold holdout in Figure 17. – Rank w.r.t. test performance in Figure 18. • HEBO and SMAC3 vs. random search 5-fold CV – Normalized validation performance in Figure 19. – Normalized test performance in Figure 20. – Improvement in test performance over 5-fold CV in Figure 21. – Rank w.r.t. test performance in Figure 22. • HEBO and SMAC3 vs. random search 5x 5-fold CV – Normalized validation performance in Figure 23. – Normalized test performance in Figure 24. – Improvement in test performance over 5x 5-fold CV in Figure 25. – Rank w.r.t. test performance in Figure 26. 30Logloss, 500 Logloss, 1000 Logloss, 5000 ROC AUC, 500 ROC AUC, 1000 ROC AUC, 5000 Accuracy , 500 Accuracy , 1000 Accuracy , 5000 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 0.00 0.25 0.50 0.75 0.00 0.25 0.50 0.75 0.00 0.25 0.50 0.75 0.00 0.25 0.50 0.75 0.00 0.25 0.50 0.75 0.00 0.25 0.50 0.75 0.25 0.50 0.75 0.00 0.25 0.50 0.75 0.00 0.25 0.50 0.75 No. HPC Evaluations Mean Normalized Validation Performance Reshuffling FALSE TRUE Resampling Holdout 5−fold CV 5−fold Holdout 5x 5−fold CV Figure 7: Random search. Average normalized performance over tasks, learners and replications for different n (train-validation sizes, columns). Shaded areas represent standard errors. Logloss, 500 Logloss, 1000 Logloss, 5000 ROC AUC, 500 ROC AUC, 1000 ROC AUC, 5000 Accuracy , 500 Accuracy , 1000 Accuracy , 5000 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 0.3 0.4 0.5 0.2 0.3 0.4 0.5 0.6 0.2 0.3 0.4 0.5 0.6 0.2 0.3 0.4 0.5 0.3 0.4 0.5 0.1 0.2 0.3 0.4 0.5 0.3 0.4 0.25 0.30 0.35 0.40 0.45 0.50 0.1 0.2 0.3 0.4 0.5 No. HPC Evaluations Mean Normalized T est Performance Reshuffling FALSE TRUE Resampling Holdout 5−fold CV 5−fold Holdout 5x 5−fold CV Figure 8: Random search. Average normalized test performance over tasks, learners and replications for different n (train-validation sizes, columns). Shaded areas represent standard errors. 31Logloss, 500 Logloss, 1000 Logloss, 5000 ROC AUC, 500 ROC AUC, 1000 ROC AUC, 5000 Accuracy , 500 Accuracy , 1000 Accuracy , 5000 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 −0.4 −0.3 −0.2 −0.1 0.0 0.1 −0.50 −0.25 0.00 0.25 −0.3 −0.2 −0.1 0.0 0.1 −0.4 −0.2 0.0 −1.2 −0.8 −0.4 0.0 0.4 −1.5 −1.0 −0.5 0.0 −0.75 −0.50 −0.25 0.00 0.25 −1.0 −0.5 0.0 0.5 −2.0 −1.5 −1.0 −0.5 0.0 No. HPC Evaluations Mean T est Improvement Reshuffling FALSE TRUE Resampling Holdout 5−fold CV 5−fold Holdout 5x 5−fold CV Figure 9: Random search. Average improvement (compared to standard 5-fold CV) with respect to test performance of the incumbent over tasks, learners and replications for differentn (train-validation sizes, columns). Shaded areas represent standard errors. Logloss, 500 Logloss, 1000 Logloss, 5000 ROC AUC, 500 ROC AUC, 1000 ROC AUC, 5000 Accuracy , 500 Accuracy , 1000 Accuracy , 5000 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 4.0 4.5 5.0 5.5 3.5 4.0 4.5 5.0 5.5 6.0 4 5 4.0 4.5 5.0 5.5 3.5 4.0 4.5 5.0 5.5 6.0 4 5 6 4.0 4.5 5.0 4.0 4.5 5.0 5.5 4 5 6 No. HPC Evaluations Mean Rank (T est Performance) Reshuffling FALSE TRUE Resampling Holdout 5−fold CV 5−fold Holdout 5x 5−fold CV Figure 10: Random search. Average ranks (lower is better) with respect to test performance over tasks, learners and replications for different n (train-validation sizes, columns). Shaded areas represent standard errors. 32500 1000 5000 1 50 100 150 200 250 1 50 100 150 200 250 1 50 100 150 200 250 0.0 0.2 0.4 0.6 0.8 0.2 0.4 0.6 0.8 0.2 0.4 0.6 0.8 No. HPC Evaluations Mean Normalized Validation Performance Optimizer Random Search HEBO SMAC3 Reshuffling FALSE TRUE Figure 11: HEBO and SMAC3 vs. random search for holdout. Average normalized validation performance (ROC AUC) over tasks, learners and replications for differentn (train-validation sizes, columns). Shaded areas represent standard errors. 500 1000 5000 1 50 100 150 200 250 1 50 100 150 200 250 1 50 100 150 200 250 0.2 0.3 0.4 0.5 0.3 0.4 0.5 0.30 0.35 0.40 0.45 0.50 No. HPC Evaluations Mean Normalized T est Performance Optimizer Random Search HEBO SMAC3 Reshuffling FALSE TRUE Figure 12: HEBO and SMAC3 vs. random search for holdout. Average normalized test performance (ROC AUC) over tasks, learners and replications for different n (train-validation sizes, columns). Shaded areas represent standard errors. 500 1000 5000 1 50 100 150 200 250 1 50 100 150 200 250 1 50 100 150 200 250 0.0 0.5 1.0 1.5 −0.5 0.0 0.5 1.0 −0.5 0.0 0.5 1.0 No. HPC Evaluations Mean T est Improvement Optimizer Random Search HEBO SMAC3 Reshuffling FALSE TRUE Figure 13: HEBO and SMAC3 vs. random search for holdout. Average improvement (compared to standard holdout) with respect to test performance (ROC AUC) of the incumbent over tasks, learners and replications for different n (train-validation sizes, columns). Shaded areas represent standard errors. 33500 1000 5000 1 50 100 150 200 250 1 50 100 150 200 250 1 50 100 150 200 250 3.25 3.50 3.75 4.00 3.00 3.25 3.50 3.75 4.00 3.00 3.25 3.50 3.75 4.00 No. HPC Evaluations Mean Rank (T est Performance) Optimizer Random Search HEBO SMAC3 Reshuffling FALSE TRUE Figure 14: HEBO and SMAC3 vs. random search for holdout. Average ranks (lower is better) with respect to test performance (ROC AUC) of the incumbent over tasks, learners and replications for different n (train-validation sizes, columns). Shaded areas represent standard errors. 500 1000 5000 1 50 100 150 200 250 1 50 100 150 200 250 1 50 100 150 200 250 0.2 0.4 0.6 0.2 0.4 0.6 0.8 0.2 0.4 0.6 0.8 No. HPC Evaluations Mean Normalized Validation Performance Optimizer Random Search HEBO SMAC3 Reshuffling FALSE TRUE Figure 15: HEBO and SMAC3 vs. random search for 5-fold holdout. Average normalized validation performance (ROC AUC) over tasks, learners and replications for differentn (train-validation sizes, columns). Shaded areas represent standard errors. 500 1000 5000 1 50 100 150 200 250 1 50 100 150 200 250 1 50 100 150 200 250 0.25 0.35 0.45 0.55 0.3 0.4 0.5 0.25 0.30 0.35 0.40 0.45 0.50 No. HPC Evaluations Mean Normalized T est Performance Optimizer Random Search HEBO SMAC3 Reshuffling FALSE TRUE Figure 16: HEBO and SMAC3 vs. random search for 5-fold holdout. Average normalized test performance (ROC AUC) over tasks, learners and replications for differentn (train-validation sizes, columns). Shaded areas represent standard errors. 34500 1000 5000 1 50 100 150 200 250 1 50 100 150 200 250 1 50 100 150 200 250 0.0 0.5 1.0 1.5 −0.5 0.0 0.5 1.0 1.5 −0.5 0.0 0.5 No. HPC Evaluations Mean T est Improvement Optimizer Random Search HEBO SMAC3 Reshuffling FALSE TRUE Figure 17: HEBO and SMAC3 vs. random search for 5-fold holdout. Average improvement (compared to standard 5-fold holdout) with respect to test performance (ROC AUC) of the incumbent over tasks, learners and replications for different n (train-validation sizes, columns). Shaded areas represent standard errors. 500 1000 5000 1 50 100 150 200 250 1 50 100 150 200 250 1 50 100 150 200 250 3.00 3.25 3.50 3.75 4.00 3.25 3.50 3.75 3.25 3.50 3.75 No. HPC Evaluations Mean Rank (T est Performance) Optimizer Random Search HEBO SMAC3 Reshuffling FALSE TRUE Figure 18: HEBO and SMAC3 vs. random search for 5-fold holdout. Average ranks (lower is better) with respect to test performance (ROC AUC) of the incumbent tasks, learners and replications for different n (train-validation sizes, columns). Shaded areas represent standard errors. 500 1000 5000 1 50 100 150 200 250 1 50 100 150 200 250 1 50 100 150 200 250 0.2 0.4 0.6 0.2 0.4 0.6 0.8 0.2 0.4 0.6 0.8 No. HPC Evaluations Mean Normalized Validation Performance Optimizer Random Search HEBO SMAC3 Reshuffling FALSE TRUE Figure 19: HEBO and SMAC3 vs. random search for 5-fold CV . Average normalized validation performance (ROC AUC) over tasks, learners and replications for differentn (train-validation sizes, columns). Shaded areas represent standard errors. 35500 1000 5000 1 50 100 150 200 250 1 50 100 150 200 250 1 50 100 150 200 250 0.2 0.3 0.4 0.5 0.3 0.4 0.5 0.30 0.35 0.40 0.45 0.50 No. HPC Evaluations Mean Normalized T est Performance Optimizer Random Search HEBO SMAC3 Reshuffling FALSE TRUE Figure 20: HEBO and SMAC3 vs. random search for 5-fold CV . Average normalized test performance (ROC AUC) over tasks, learners and replications for different n (train-validation sizes, columns). Shaded areas represent standard errors. 500 1000 5000 1 50 100 150 200 250 1 50 100 150 200 250 1 50 100 150 200 250 0.0 0.5 1.0 1.5 0.0 0.5 1.0 1.5 −0.5 0.0 0.5 1.0 No. HPC Evaluations Mean T est Improvement Optimizer Random Search HEBO SMAC3 Reshuffling FALSE TRUE Figure 21: HEBO and SMAC3 vs. random search for 5-fold CV . Average improvement (compared to standard 5-fold CV) with respect to test performance (ROC AUC) of the incumbent over tasks, learners and replications for different n (train-validation sizes, columns). Shaded areas represent standard errors. 500 1000 5000 1 50 100 150 200 250 1 50 100 150 200 250 1 50 100 150 200 250 3.25 3.50 3.75 3.2 3.4 3.6 3.8 3.3 3.5 3.7 No. HPC Evaluations Mean Rank (T est Performance) Optimizer Random Search HEBO SMAC3 Reshuffling FALSE TRUE Figure 22: HEBO and SMAC3 vs. random search for 5-fold CV . Average ranks (lower is better) with respect to test performance (ROC AUC) of the incumbent over tasks, learners and replications for different n (train-validation sizes, columns). Shaded areas represent standard errors. 36500 1000 5000 1 50 100 150 200 250 1 50 100 150 200 250 1 50 100 150 200 250 0.0 0.2 0.4 0.6 0.2 0.4 0.6 0.2 0.4 0.6 0.8 No. HPC Evaluations Mean Normalized Validation Performance Optimizer Random Search HEBO SMAC3 Reshuffling FALSE TRUE Figure 23: HEBO and SMAC3 vs. random search for 5x 5-fold CV . Average normalized validation performance (ROC AUC) over tasks, learners and replications for differentn (train-validation sizes, columns). Shaded areas represent standard errors. 500 1000 5000 1 50 100 150 200 250 1 50 100 150 200 250 1 50 100 150 200 250 0.2 0.3 0.4 0.5 0.3 0.4 0.5 0.3 0.4 0.5 No. HPC Evaluations Mean Normalized T est Performance Optimizer Random Search HEBO SMAC3 Reshuffling FALSE TRUE Figure 24: HEBO and SMAC3 vs. random search for 5x 5-fold CV . Average normalized test performance (ROC AUC) over tasks, learners and replications for differentn (train-validation sizes, columns). Shaded areas represent standard errors. 500 1000 5000 1 50 100 150 200 250 1 50 100 150 200 250 1 50 100 150 200 250 0.0 0.5 1.0 1.5 0.0 0.5 1.0 1.5 −0.4 0.0 0.4 0.8 1.2 No. HPC Evaluations Mean T est Improvement Optimizer Random Search HEBO SMAC3 Reshuffling FALSE TRUE Figure 25: HEBO and SMAC3 vs. random search for 5x 5-fold CV . Average improvement (compared to standard 5x 5-fold CV) with respect to test performance (ROC AUC) of the incumbent over tasks, learners and replications for different n (train-validation sizes, columns). Shaded areas represent standard errors. 37500 1000 5000 1 50 100 150 200 250 1 50 100 150 200 250 1 50 100 150 200 250 3.00 3.25 3.50 3.75 3.25 3.50 3.75 4.00 3.2 3.4 3.6 3.8 No. HPC Evaluations Mean Rank (T est Performance) Optimizer Random Search HEBO SMAC3 Reshuffling FALSE TRUE Figure 26: HEBO and SMAC3 vs. random search for 5x 5-fold CV . Average ranks (lower is better) with respect to test performance (ROC AUC) of the incumbent over tasks, learners and replications for different n (train-validation sizes, columns). Shaded areas represent standard errors. 38G.2 Ablation on M-fold holdout Based on the 5x 5-fold CV results we further simulated different M-fold holdout resamplings (standard and reshuffled) by taking M repeats from the first fold of the 5x 5-fold CV . This allows us to get an understanding of the effect more folds have on M-fold holdout, especially in the context of reshuffling. Regarding normalized validation performance we observe that more folds generally result in a less optimistically biased validation performance (see Figure 27). Looking at normalized test performance (Figure 28) we observe the general trend that more folds result in better test performance – which is expected. Reshuffling generally results in better test performance compared to the standard resampling (with the exception of logloss where especially in the case of a single holdout, reshuffling can hurt generalization performance). This effect is smaller, the more folds are used, which is in line with our theoretical results presented in Table 1. Looking at improvement compared to standard 5-fold holdout with respect to test performance and ranks with respect to test performance, we observe that often reshuffled 2-fold holdout results that are highly competitive with standard 3, 4 or 5-fold holdout. Logloss, 500 Logloss, 1000 Logloss, 5000 ROC AUC, 500 ROC AUC, 1000 ROC AUC, 5000 Accuracy , 500 Accuracy , 1000 Accuracy , 5000 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 0.00 0.25 0.50 0.75 0.00 0.25 0.50 0.75 0.00 0.25 0.50 0.75 0.25 0.50 0.75 0.00 0.25 0.50 0.75 0.00 0.25 0.50 0.75 0.25 0.50 0.75 0.0 0.2 0.4 0.6 0.8 0.25 0.50 0.75 No. HPC Evaluations Mean Normalized Validation Performance Holdout 1−fold 2−fold 3−fold 4−fold 5−fold Reshuffling FALSE TRUE Figure 27: Random search. Average normalized validation performance over tasks, learners and replications for different n (train-validation sizes, columns). Shaded areas represent standard errors. 39Logloss, 500 Logloss, 1000 Logloss, 5000 ROC AUC, 500 ROC AUC, 1000 ROC AUC, 5000 Accuracy , 500 Accuracy , 1000 Accuracy , 5000 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 0.25 0.30 0.35 0.40 0.45 0.50 0.2 0.3 0.4 0.5 0.6 0.2 0.3 0.4 0.5 0.6 0.2 0.3 0.4 0.5 0.3 0.4 0.5 0.1 0.2 0.3 0.4 0.5 0.25 0.30 0.35 0.40 0.45 0.30 0.35 0.40 0.45 0.50 0.1 0.2 0.3 0.4 0.5 No. HPC Evaluations Mean Normalized T est Performance Holdout 1−fold 2−fold 3−fold 4−fold 5−fold Reshuffling FALSE TRUE Figure 28: Random search. Average normalized test performance over tasks, learners and replications for different n (train-validation sizes, columns). Shaded areas represent standard errors. Logloss, 500 Logloss, 1000 Logloss, 5000 ROC AUC, 500 ROC AUC, 1000 ROC AUC, 5000 Accuracy , 500 Accuracy , 1000 Accuracy , 5000 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 −0.4 −0.3 −0.2 −0.1 0.0 0.1 −0.6 −0.4 −0.2 0.0 0.2 −0.3 −0.2 −0.1 0.0 0.1 −0.4 −0.3 −0.2 −0.1 0.0 0.1 −1.0 −0.5 0.0 −1.5 −1.0 −0.5 0.0 −0.50 −0.25 0.00 0.25 0.50 −1.0 −0.5 0.0 −2.0 −1.5 −1.0 −0.5 0.0 No. HPC Evaluations Mean T est Improvement Holdout 1−fold 2−fold 3−fold 4−fold 5−fold Reshuffling FALSE TRUE Figure 29: Random search. Average improvement (compared to standard 5-fold holdout) with respect to test performance of the incumbent over tasks, learners and replications for different n (train-validation sizes, columns). Shaded areas represent standard errors. 40Logloss, 500 Logloss, 1000 Logloss, 5000 ROC AUC, 500 ROC AUC, 1000 ROC AUC, 5000 Accuracy , 500 Accuracy , 1000 Accuracy , 5000 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 5.0 5.5 6.0 6.5 5.0 5.5 6.0 6.5 7.0 4.5 5.0 5.5 6.0 6.5 5.0 5.5 6.0 6.5 4.5 5.0 5.5 6.0 6.5 7.0 5 6 7 4.8 5.2 5.6 6.0 5.0 5.5 6.0 6.5 5 6 7 No. HPC Evaluations Mean Rank (T est Performance) Holdout 1−fold 2−fold 3−fold 4−fold 5−fold Reshuffling FALSE TRUE Figure 30: Random search. Average ranks (lower is better) with respect to test performance of the incumbent over tasks, learners and replications for different n (train-validation sizes, columns). Shaded areas represent standard errors. 41NeurIPS Paper Checklist 1. Claims Question: Do the main claims made in the abstract and introduction accurately reflect the paper’s contributions and scope? Answer: [Yes] Justification: We outline our three main contributions in the introduction (Section 1). We do not discuss generalization in the introduction, but rather in the discussion in Section 5. Guidelines: • The answer NA means that the abstract and introduction do not include the claims made in the paper. • The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. • The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. • It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. 2. Limitations Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: The paper provides an analysis of reshuffling data in the context of estimating the generalization error for hyperparameter optimization. Our theoretical analysis explains why reshuffling works, and we experimentally verify the theoretical analysis. We discuss the limitations of our work in Section 5. Guidelines: • The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. • The authors are encouraged to create a separate \"Limitations\" section in their paper. • The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. • The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. • The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. • The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. • If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. • While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren’t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an impor- tant role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. 3. Theory Assumptions and Proofs 42Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: Full assumptions and proofs for our main results (Theorem 2.1 and Theo- rem 2.2) are given in Appendix C.1 and Appendix C.2, respectively. Derivations for the parameters in Table 1 are provided in Appendix E. The additional results for the grid density are stated and proven directly in Appendix D. Guidelines: • The answer NA means that the paper does not include theoretical results. • All the theorems, formulas, and proofs in the paper should be numbered and cross- referenced. • All assumptions should be clearly stated or referenced in the statement of any theorems. • The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. • Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. • Theorems and Lemmas that the proof relies upon should be properly referenced. 4. Experimental Result Reproducibility Question: Does the paper fully disclose all the information needed to reproduce the main ex- perimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provide thorough details on the experimental setup in Section 4.1 and Appendix F. Moreover, we provide code to reproduce our results under an open source license at https://github.com/slds-lmu/paper_2024_reshuffling. Guidelines: • The answer NA means that the paper does not include experiments. • If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. • If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. • Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. • While NeurIPS does not require releasing code, the conference does require all submis- sions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 43(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. 5. Open access to data and code Question: Does the paper provide open access to the data and code, with sufficient instruc- tions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: Regarding datasets, we rely on OpenML.org. We provide thorough details on the experimental setup in Section 4.1 and Appendix F. Moreover, we provide code to reproduce our results under an open source license at https://github.com/slds-lmu /paper_2024_reshuffling. Guidelines: • The answer NA means that paper does not include experiments requiring code. • Please see the NeurIPS code and data submission guidelines ( https://nips.cc/pu blic/guides/CodeSubmissionPolicy) for more details. • While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). • The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. • The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. • The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. • At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). • Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. 6. Experimental Setting/Details Question: Does the paper specify all the training and test details (e.g., data splits, hyper- parameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We provide thorough details on the experimental setup in Section 4.1 and Appendix F. Moreover, we provide code to reproduce our results under an open source license at https://github.com/slds-lmu/paper_2024_reshuffling. Guidelines: • The answer NA means that the paper does not include experiments. • The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. • The full details can be provided either with the code, in appendix, or as supplemental material. 7. Experiment Statistical Significance Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] 44Justification: We report the standard error in every analysis. Guidelines: • The answer NA means that the paper does not include experiments. • The authors should answer \"Yes\" if the results are accompanied by error bars, confi- dence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. • The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). • The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) • The assumptions made should be given (e.g., Normally distributed errors). • It should be clear whether the error bar is the standard deviation or the standard error of the mean. • It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. • For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). • If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. 8. Experiments Compute Resources Question: For each experiment, does the paper provide sufficient information on the com- puter resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We provide details in Appendix F.4. Guidelines: • The answer NA means that the paper does not include experiments. • The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. • The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. • The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn’t make it into the paper). 9. Code Of Ethics Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Our work provides a study on reshuffling data when estimating the generaliza- tion error in hyperparameter tuning. Therefore, our work is applicable wherever standard machine learning is applicable, and we do not see any ethical concerns in our method. Guidelines: • The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. • If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. • The authors should make sure to preserve anonymity (e.g., if there is a special consid- eration due to laws or regulations in their jurisdiction). 10. Broader Impacts 45Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: The paper conducts fundamental research that is not tied to particular applica- tions, let alone deployment. Guidelines: • The answer NA means that there is no societal impact of the work performed. • If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. • Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. • The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. • The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. • If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). 11. Safeguards Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper conducts fundamental research that is not tied to particular applica- tions, let alone deployment. The paper does not develop models that have a high risk for misuse. Guidelines: • The answer NA means that the paper poses no such risks. • Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. • Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. • We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. 12. Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] 46Justification: We used datasets from OpenML.org and reference the dataset pages. Further information of the datasets, including their licenses, are available at OpenML.org. Guidelines: • The answer NA means that the paper does not use existing assets. • The authors should cite the original paper that produced the code package or dataset. • The authors should state which version of the asset is used and, if possible, include a URL. • The name of the license (e.g., CC-BY 4.0) should be included for each asset. • For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. • If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. • For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. • If this information is not available online, the authors are encouraged to reach out to the asset’s creators. 13. New Assets Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We provide code as a new asset and describe how we make our code available in Point 5 of the NeurIPS Paper Checklist. Guidelines: • The answer NA means that the paper does not release new assets. • Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. • The paper should discuss whether and how consent was obtained from people whose asset is used. • At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. 14. Crowdsourcing and Research with Human Subjects Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does neither involve crowdsourcing nor research with human subjects. Guidelines: • The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Including this information in the supplemental material is fine, but if the main contribu- tion of the paper involves human subjects, then as much detail as possible should be included in the main paper. • According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects 47Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does neither involve crowdsourcing nor research with human subjects. Guidelines: • The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. • We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. • For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. 48",
      "meta_data": {
        "arxiv_id": "2405.15393v2",
        "authors": [
          "Thomas Nagler",
          "Lennart Schneider",
          "Bernd Bischl",
          "Matthias Feurer"
        ],
        "published_date": "2024-05-24T09:48:18Z",
        "pdf_url": "https://arxiv.org/pdf/2405.15393v2.pdf",
        "github_url": "https://github.com/slds-lmu/paper_2024_reshuffling"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the standard practice in hyperparameter optimization (HPO) of using fixed resampling splits for evaluating configurations, and surprisingly demonstrates that reshuffling these splits for every configuration often improves the final model's generalization performance on unseen data. The key contributions include a theoretical analysis explaining how reshuffling impacts the asymptotic behavior of the validation loss surface and providing a bound on the expected regret, connecting benefits to signal and noise characteristics. This theoretical insight is confirmed through controlled simulation studies and practical usefulness is shown in large-scale HPO experiments. Reshuffling drastically improves results for single train-validation holdout protocols, making it competitive with standard cross-validation while being computationally cheaper.",
        "methodology": "The methodology involves both theoretical analysis and empirical studies. Theoretically, the paper investigates how reshuffling affects the empirical loss surface by deriving the limiting distribution of the sequence of validation losses. It models the observed loss as a true loss plus a zero-mean Gaussian process with a covariance kernel that changes with reshuffling. A bound on the expected regret is derived, dependent on the loss surface's curvature, noise correlation, and a reshuffling parameter (τ). Empirically, a simulation study uses a univariate quadratic loss surface with a squared exponential kernel for the noise process to systematically vary curvature (m), noise correlation (κ), and reshuffling extent (τ). Benchmark experiments are conducted using random search, HEBO, and SMAC3 as HPO algorithms on various real-world tabular datasets and learning algorithms (CatBoost, XGBoost, Elastic Net, MLP) to measure generalization performance using ROC AUC, accuracy, and logloss. Resampling strategies like holdout, M-fold CV, M-fold holdout, and their reshuffled variants are compared.",
        "experimental_setup": "For the simulation study, a univariate quadratic loss surface µ(λ) = m(λ - 0.5)^2/2 is minimized, combined with a squared exponential kernel K(λ, λ') = σ^2_K exp(-κ(λ - λ')^2/2) for the noise process ϵ. In each run, the observed objective is simulated, the minimizer identified, and its true risk calculated, repeated 10,000 times for various combinations of τ, m, and κ. For benchmark experiments, a subset of AutoML benchmark tabular datasets (10 DGPs with 10k-1M observations, <100 features) are used. Tasks are created by sampling train-validation sizes n ∈ {500, 1000, 5000} points from DGPs, with an additional 5000 points reserved for robust assessment of generalization error (outer test set). Learning algorithms include CatBoost, XGBoost, Elastic Net, and a funnel-shaped MLP, with detailed training pipelines and search spaces provided. Random search is performed with 500 HPC evaluations, while Bayesian optimization (HEBO, SMAC3) is run with 250 HPC evaluations. Resampling methods are 80/20 train-validation holdout, 5-fold CV, 5-fold holdout, and 5x 5-fold CV, each with fixed and reshuffled splits. Test performance is assessed by retraining the incumbent HPC on all available train/validation data and evaluating on the outer test set. Experiments are replicated 10 times, with metrics including ROC AUC, accuracy, and logloss.",
        "limitations": "The theoretical analysis relies on an asymptotic approximation of the empirical loss surface, assuming Gaussian loss surfaces for tractability, which may not hold for general distributions. A loss stability assumption regarding learning algorithms is made, which is generally mild but can fail for highly sensitive losses (e.g., logloss at small sample sizes, where reshuffling sometimes hurts generalization). The focus is on generalization after search through a fixed, finite set of candidates, ignoring the dynamic nature of many HPO algorithms. Experiments are limited to tabular data and binary classification, avoiding extremely small or large datasets, which might affect the generalizability of findings to other data types or scales.",
        "future_research_directions": "Future work could focus on developing a unified formal definition of 'oversearching', 'overtuning', or 'overfitting to the validation set' and thoroughly analyzing its relationship to validation performance measurements. Investigating less naive implementations of reshuffling to address its negative impact on highly sensitive losses like logloss is another direction. Further research into adaptive cross-validation (CV) techniques to reduce the computational burden of HPO while exploiting the benefits of more intensive resamplings is suggested. Finally, designing more advanced HPO algorithms that explicitly exploit the reshuffling effect, potentially in combination with existing methods like LOOCVCV or early stopping, could lead to further improvements. Exploring reshuffling's effect on multi-class datasets where less overtuning is expected is also mentioned.",
        "experimental_code": "def simulate_gp(x: torch.Tensor, mu: callable, cov: callable) -> torch.Tensor:\n    \"\"\"\n    Simulate a Gaussian process.\n\n    :param x: A tensor of shape (n, d) where n is the number of points and d is the dimensionality of each point.\n    :param mu: Function to compute the mean of the Gaussian process.\n    :param cov: Function to compute the covariance matrix of the Gaussian process.\n\n    :return: A tensor of shape (n, d) representing the simulated Gaussian process.\n    \"\"\"\n    sigma_2 = 1e-5\n    n = x.shape[0]\n    d = x.shape[1]\n    mu_x = mu(x)\n    K_x = cov(x, x)\n    K_x += sigma_2 * torch.eye(n)\n    eigenvalues, eigenvectors = torch.linalg.eigh(K_x)\n    positive_eigenvalues = torch.clamp(eigenvalues, min=0)\n    sqrt_K_x = eigenvectors @ torch.diag(positive_eigenvalues.sqrt()) @ eigenvectors.T\n    z = torch.normal(0, 1, (n, d))\n    return mu_x + sqrt_K_x @ z\n\n\ndef mu_factory(x: torch.Tensor, alpha: float) -> torch.Tensor:\n    \"\"\"\n    The mean function of the Gaussian process.\n\n    :param x: A tensor of shape (n, d) where n is the number of points and d is the dimensionality of each point.\n    :param alpha: The alpha parameter of the mean function.\n\n    :return: A tensor of shape (n, 1) representing the mean of the Gaussian process at each point.\n    \"\"\"\n    values = alpha * (x - 0.5).pow(2)\n    return values\n\n\ndef cov_factory(\n    x1: torch.Tensor,\n    x2: torch.Tensor,\n    lengthscale: float,\n    tau: float = None,\n    shuffled: bool = False,\n) -> torch.Tensor:\n    \"\"\"\n    Vectorized computation of the covariance matrix of the Gaussian process.\n\n    :param x1: A tensor of shape (n, d) where n is the number of points and d is the dimensionality of each point.\n    :param x2: A tensor of shape (n, d) where n is the number of points and d is the dimensionality of each point.\n    :param lengthscale: The lengthscale parameter of the covariance function.\n    :param shuffled: Whether to assume a shuffled version of the covariance function.\n    :param tau: The tau parameter of the shuffled covariance function.\n\n    :return: A tensor of shape (n, n) representing the covariance matrix of the Gaussian process.\n    \"\"\"\n    sq_dist = torch.sum((x1[:, None, :] - x2[None, :, :]) ** 2, dim=-1)\n    K = torch.exp(-sq_dist / (2 * (lengthscale**2)))\n    if shuffled:\n        K = (1 - torch.eye(K.shape[0])) * (tau**2) * K + torch.eye(K.shape[0]) * K\n    return K\n\n\nif __name__ == \"__main__\":\n    # ... (code for argument parsing and setup)\n    # Relevant part of the simulation loop:\n    # Varying alpha (curvature), lengthscale (noise correlation), and tau (reshuffling extent)\n    # ...\n    taus = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n    # ...\n    for tau in taus:\n        # ... (inner loop for replicates)\n            y = simulate_gp(x, mu, cov)\n            y_shuffled = simulate_gp(x, mu, partial(cov, shuffled=True, tau=tau))\n            y_mu_y = mu(x[y.argmin(dim=0)])\n            y_mu_y_shuffled = mu(x[y_shuffled.argmin(dim=0)])\n            # ... (store results)\n\n\n# From reshufflebench/learner/learner_random_cv.py\nclass LearnerRandomCV(LearnerRandom):\n    # ... (init and other methods)\n\n    def objective(self, trial: Trial) -> float:\n        \"\"\"\n        Objective function for the optimization.\n        \"\"\"\n        # construct classifier pipeline\n        self.classifier.construct_pipeline(\n            trial,\n            refit=False,\n            cat_features=self.cat_features,\n            num_features=self.num_features,\n            n_train_samples=self.train_size,\n        )\n\n        if self.reshuffle:\n            self.cv = [\n                StratifiedKFold(\n                    n_splits=self.n_splits,\n                    shuffle=True,\n                    random_state=self.seed + (trial.number * 500000) + (i * 1000),\n                )\n                for i in range(self.n_repeats)\n            ]\n            self.cv_splits = []\n            for cv in self.cv:\n                self.cv_splits.append(\n                    list(cv.split(self.x_valid_train, self.y_valid_train))\n                )\n\n            # partition add_valid_use data into n_splits folds and repeat it n_repeats times\n            self.cv_add_valid = [\n                StratifiedKFold(\n                    n_splits=self.n_splits,\n                    shuffle=True,\n                    random_state=self.seed + (trial.number * 500000) + (i * 1000),\n                )\n                for i in range(self.n_repeats)\n            ]\n            self.cv_splits_add_valid = []\n            for cv in self.cv_add_valid:\n                self.cv_splits_add_valid.append(\n                    list(cv.split(self.x_add_valid_use, self.y_add_valid_use))\n                )\n\n        # ... (code for storing splits, y_train_hist, y_valid_hist, etc.)\n\n        # for each repeat and each fold fit the classifier and predict and compute metrics\n        # ...\n        for repeat in range(self.n_repeats):\n            for fold in range(self.n_splits):\n                train_index, valid_index = self.cv_splits[repeat][fold]\n                # ... (construct x_train, x_valid, y_train, y_valid)\n                # ... (construct x_add_valid, y_add_valid for additional validation)\n\n                self.classifier.fit(\n                    trial=trial,\n                    x_train=x_train,\n                    y_train=y_train,\n                    x_valid=x_valid,\n                    y_valid=y_valid,\n                    cat_features=self.cat_features,\n                )\n                # ... (predict and compute metrics for train, valid, add_valid, test sets)\n\n        # ... (code for storing predictions and metrics)\n\n        # refit on the train_valid set and predict on train_valid and test_retrained\n        self.classifier.construct_pipeline(\n            trial,\n            refit=True,\n            cat_features=self.cat_features,\n            num_features=self.num_features,\n        )\n        self.classifier.fit(\n            trial=trial,\n            x_train=self.x_valid_train,\n            y_train=self.y_valid_train,\n            cat_features=self.cat_features,\n        )\n\n        # ... (predict on valid_train and test_retrained, compute metrics)\n\n        self.classifier.reset()\n\n        return metrics_valid[\"accuracy\"]\n\n\n# From analyze/result_analyzer.py\nclass ResultAnalyzer(object):\n    # ... (init and other methods)\n\n    def calculate_curvature(self) -> None:\n        \"\"\"\n        Fit a GP on observed values and calculate some curvature metrics at the empirical optimum.\n        \"\"\"\n        for metric in self.params[\"metrics\"]:\n            dat = self.results_raw[metric]\n            relevant_columns_valid = [\n                column for column in dat.columns if \"params_\" in column\n            ] + [\"valid\"]\n            dat_valid = dat.loc[:, relevant_columns_valid]\n            dat_valid.rename(columns={\"valid\": \"y\"}, inplace=True)\n            X = dat_valid.drop(columns=[\"y\"])\n            y = dat_valid[\"y\"].values.reshape(-1, 1)\n            # ... (prepare search space and model config for GP)\n            model = get_model(\"gp\", space.num_numeric, space.num_categorical, 1, **model_config)\n            model.fit(X, Xe, y)\n\n            empirical_argmin = model.predict(X, Xe)[0].argmin()\n            X_argmin = X[empirical_argmin, :].unsqueeze(0)\n            Xe_argmin = Xe[empirical_argmin, :].unsqueeze(0)\n\n            def posterior_mean_wrapper(x, model, Xe_argmin):\n                x_tensor = torch.FloatTensor(x).unsqueeze(0).requires_grad_(True)\n                return model.predict(x_tensor, Xe_argmin)[0][0, 0].detach().numpy()\n\n            x0 = X[empirical_argmin, :].numpy()\n            result = opt.minimize(\n                posterior_mean_wrapper,\n                x0,\n                args=(model, Xe_argmin),\n                bounds=bounds,\n                method=\"Nelder-Mead\",\n            )\n\n            x_optimal = result.x\n            hessian_function = numdifftools.Hessian(posterior_mean_wrapper)\n            hessian_optimal = hessian_function(x_optimal, model, Xe_argmin)\n\n            def make_psd(matrix):\n                eigenvalues, eigenvectors = np.linalg.eigh(matrix)\n                already_is_psd = np.all(eigenvalues >= 0)\n                eigenvalues[eigenvalues < 0] = 0\n                return (\n                    already_is_psd,\n                    eigenvectors @ np.diag(eigenvalues) @ eigenvectors.T,\n                )\n\n            already_is_psd, hessian_optimal = make_psd(hessian_optimal)\n\n            det_hessian = np.linalg.det(hessian_optimal)\n            trace_hessian = np.trace(hessian_optimal)\n            eigenvalues_hessian = np.linalg.eigvals(hessian_optimal)\n            smallest_eigenvalue_hessian = np.min(eigenvalues_hessian)\n            biggest_eigenvalue_hessian = np.max(eigenvalues_hessian)\n\n            # ... (store curvature metrics)\n\n\n# From reshufflebench/metrics/metrics.py\ndef compute_accuracy(\n    y_true: np.array,\n    y_pred: np.array,\n    y_pred_proba: np.array,\n    labels: List[int],\n    multiclass: bool,\n) -> float:\n    \"\"\"\n    Compute accuracy score.\n    \"\"\"\n    return accuracy_score(y_true, y_pred)\n\n\ndef compute_balanced_accuracy(\n    y_true: np.array,\n    y_pred: np.array,\n    y_pred_proba: np.array,\n    labels: List[int],\n    multiclass: bool,\n) -> float:\n    \"\"\"\n    Compute balanced accuracy score.\n    \"\"\"\n    return balanced_accuracy_score(y_true, y_pred)\n\n\ndef compute_logloss(\n    y_true: np.array,\n    y_pred: np.array,\n    y_pred_proba: np.array,\n    labels: List[int],\n    multiclass: bool,\n) -> float:\n    \"\"\"\n    Compute logloss score.\n    \"\"\"\n    return log_loss(y_true, y_pred_proba, labels=labels)\n\n\ndef compute_auc(\n    y_true: np.array,\n    y_pred: np.array,\n    y_pred_proba: np.array,\n    labels: List[int],\n    multiclass: bool,\n) -> float:\n    \"\"\"\n    Compute AUC score.\n    \"\"\"\n    if multiclass:\n        return roc_auc_score(\n            y_true, y_pred_proba, average=\"macro\", multi_class=\"ovo\", labels=labels\n        )\n    else:\n        return roc_auc_score(y_true, y_pred_proba[:, 1])\n\n\ndef compute_metric(\n    y_true: np.array,\n    y_pred: np.array,\n    y_pred_proba: np.array,\n    metric: str,\n    labels: List[int],\n    multiclass: bool,\n) -> float:\n    \"\"\"\n    Compute a metric.\n    \"\"\"\n    if metric == \"accuracy\":\n        return compute_accuracy(\n            y_true,\n            y_pred=y_pred,\n            y_pred_proba=y_pred_proba,\n            labels=labels,\n            multiclass=multiclass,\n        )\n    elif metric == \"balanced_accuracy\":\n        return compute_balanced_accuracy(\n            y_true,\n            y_pred=y_pred,\n            y_pred_proba=y_pred_proba,\n            labels=labels,\n            multiclass=multiclass,\n        )\n    elif metric == \"logloss\":\n        return compute_logloss(\n            y_true,\n            y_pred=y_pred,\n            y_pred_proba=y_pred_proba,\n            labels=labels,\n            multiclass=multiclass,\n        )\n    elif metric == \"auc\":\n        return compute_auc(\n            y_true,\n            y_pred=y_pred,\n            y_pred_proba=y_pred_proba,\n            labels=labels,\n            multiclass=multiclass,\n        )\n    else:\n        raise ValueError(f\"Unknown metric: {metric}\")",
        "experimental_info": "The methodology involves both theoretical analysis and empirical studies.The theoretical analysis investigates how reshuffling affects the empirical loss surface by deriving the limiting distribution of the sequence of validation losses. It models the observed loss as a true loss plus a zero-mean Gaussian process with a covariance kernel that changes with reshuffling. A bound on the expected regret is derived, dependent on the loss surface's curvature (m), noise correlation (κ), and a reshuffling parameter (τ).\n\nEmpirical Simulation Study Settings:\n- **Loss Surface**: Univariate quadratic loss surface, defined by `mu_factory(x, alpha) = alpha * (x - 0.5).pow(2)`, where `alpha` represents curvature.\n- **Noise Process**: Squared exponential kernel, defined by `cov_factory` with `lengthscale` influencing noise correlation.\n- **Reshuffling**: `tau` parameter in `cov_factory` controls reshuffling extent. If `shuffled=True`, `K = (1 - torch.eye(K.shape[0])) * (tau**2) * K + torch.eye(K.shape[0]) * K`.\n- **Parameters Varied**:\n  - `alpha` (curvature): [0.5, 1, 5, 10]\n  - `lengthscale` (noise correlation): [0.1, 0.5, 1, 5]\n  - `tau` (reshuffling extent): [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n- **Simulation Details**:\n  - Number of replicates (`n_replicates`): 10000 per (alpha, lengthscale, tau) combination.\n  - `x` domain: `torch.linspace(0, 1, 101)` (1D points).\n  - Gaussian Process noise variance (`sigma_2`): `1e-5`.\n\nBenchmark Experiment Settings:\n- **HPO Algorithms**: Random Search, HEBO, SMAC3.\n- **Learning Algorithms**: CatBoost, XGBoost, Elastic Net (Logistic Regression), MLP (FunnelMLP).\n- **Real-world Tabular Datasets (OpenML data_ids)**: [23517, 1169, 41147, 4135, 1461, 1590, 41150, 41162, 42733, 42742], plus two synthetic datasets (99999, 11111) for testing.\n- **Data Splitting**: Datasets split into training + validation, test, and additional validation sets.\n  - `train_valid_size`: [500, 1000, 5000]\n  - `test_size`: 5000\n  - `add_valid_size`: 5000\n- **Resampling Strategies**:\n  - **Holdout**: `valid_frac=0.2` (80% train, 20% validation).\n  - **M-fold Cross-Validation (M-fold CV)**: `n_splits=5`.\n    - For HEBO/SMAC, `n_repeats=1` by default (single 5-fold CV).\n    - For Random Search, `n_repeats=5` by default (5-times repeated 5-fold CV).\n  - **M-fold Holdout (Repeated Holdout)**: `valid_frac=0.2`, `n_repeats=5` (5-times repeated holdout, also referred to as Monte Carlo CV).\n  - All resampling strategies are compared with `reshuffle=True` and `reshuffle=False`.\n- **Number of Trials**:\n  - `n_trials=500` for Random Search.\n  - `n_trials=250` for HEBO and SMAC3.\n- **Seeds**: `range(42, 52)` (i.e., 42, 43, ..., 51) are used for reproducibility across experiments.\n- **Generalization Performance Metrics**: ROC AUC, Accuracy, Balanced Accuracy, LogLoss. Note: HEBO and SMAC3 were optimized using AUC as the primary metric."
      }
    },
    {
      "title": "Differentiable Expected Hypervolume Improvement for Parallel Multi-Objective Bayesian Optimization",
      "abstract": "In many real-world scenarios, decision makers seek to efficiently optimize\nmultiple competing objectives in a sample-efficient fashion. Multi-objective\nBayesian optimization (BO) is a common approach, but many of the\nbest-performing acquisition functions do not have known analytic gradients and\nsuffer from high computational overhead. We leverage recent advances in\nprogramming models and hardware acceleration for multi-objective BO using\nExpected Hypervolume Improvement (EHVI)---an algorithm notorious for its high\ncomputational complexity. We derive a novel formulation of q-Expected\nHypervolume Improvement (qEHVI), an acquisition function that extends EHVI to\nthe parallel, constrained evaluation setting. qEHVI is an exact computation of\nthe joint EHVI of q new candidate points (up to Monte-Carlo (MC) integration\nerror). Whereas previous EHVI formulations rely on gradient-free acquisition\noptimization or approximated gradients, we compute exact gradients of the MC\nestimator via auto-differentiation, thereby enabling efficient and effective\noptimization using first-order and quasi-second-order methods. Our empirical\nevaluation demonstrates that qEHVI is computationally tractable in many\npractical scenarios and outperforms state-of-the-art multi-objective BO\nalgorithms at a fraction of their wall time.",
      "full_text": "Differentiable Expected Hypervolume Improvement for Parallel Multi-Objective Bayesian Optimization Samuel Daulton Facebook sdaulton@fb.com Maximilian Balandat Facebook balandat@fb.com Eytan Bakshy Facebook ebakshy@fb.com Abstract In many real-world scenarios, decision makers seek to efﬁciently optimize multiple competing objectives in a sample-efﬁcient fashion. Multi-objective Bayesian opti- mization (BO) is a common approach, but many of the best-performing acquisition functions do not have known analytic gradients and suffer from high computational overhead. We leverage recent advances in programming models and hardware acceleration for multi-objective BO using Expected Hypervolume Improvement (EHVI )—an algorithm notorious for its high computational complexity. We derive a novel formulation of q-Expected Hypervolume Improvement (qEHVI ), an acqui- sition function that extends EHVI to the parallel, constrained evaluation setting. qEHVI is an exact computation of the joint EHVI of qnew candidate points (up to Monte-Carlo (MC) integration error). Whereas previous EHVI formulations rely on gradient-free acquisition optimization or approximated gradients, we compute exact gradients of the MC estimator via auto-differentiation, thereby enabling efﬁ- cient and effective optimization using ﬁrst-order and quasi-second-order methods. Our empirical evaluation demonstrates that qEHVI is computationally tractable in many practical scenarios and outperforms state-of-the-art multi-objective BO algorithms at a fraction of their wall time. 1 Introduction The problem of optimizing multiple competing objectives is ubiquitous in scientiﬁc and engineering applications. For example in automobile design, an automaker will want to maximize vehicle durability and occupant safety, while using lighter materials that afford increased fuel efﬁciency and lower manufacturing cost [44, 72]. Evaluating the crash safety of an automobile design experimentally is expensive due to both the manufacturing time and the destruction of a vehicle. In such a scenario, sample efﬁciency is paramount. For a different example, video streaming web services commonly use adaptive control policies to determine the bitrate as the stream progresses in real time [47]. A decision maker may wish to optimize the control policy to maximize the quality of the video stream, while minimizing the stall time. Policy evaluation typically requires using the suggested policy on segments of live trafﬁc, which is subject to opportunity costs. If long evaluation times are the limiting factor, multiple designs may be evaluated in parallel to signiﬁcantly decrease end-to-end optimization time. For example, an automaker could manufacture multiple vehicle designs in parallel or a web service could deploy several control policies to different segments of trafﬁc at the same time. 1.1 Background Multi-Objective Optimization: In this work, we address the problem of optimizing a vector-valued objective f(x) : Rd →RM with f(x) = ( f(1)(x),...,f (M)(x) ) over a bounded set X ⊂Rd. We consider the scenario in which the f(i) are expensive-to-evaluate black-box functions with 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada. arXiv:2006.05078v3  [stat.ML]  23 Oct 2020no known analytical expression, and no observed gradients. Multi-objective (MO) optimization problems typically do not have a single best solution; rather, the goal is to identify the set of Pareto optimal solutions such that any improvement in one objective means deteriorating another. Without loss of generality, we assume the goal is to maximize all objectives. We say a solution f(x) Pareto dominates another solution f(x′) if f(m)(x) ≥ f(m)(x′) ∀m = 1 ,...,M and there exists m′ ∈{1,...,M }such that f(m′)(x) > f(m′)(x′). We write f(x) ≻f(x′). Let P∗= {f(x) s.t. ∄ x′∈X : f(x′) ≻f(x)}and X∗= {x ∈X s.t. f(x) ∈P∗}denote the set of Pareto optimal solutions and Pareto optimal inputs, respectively. Provided with the Pareto set, decision-makers can select a solution with an objective trade-off according to their preferences. A common approach for solving MO problems is to use evolutionary algorithms (e.g. NSGA-II), which are robust multi-objective optimizers, but require a large number of function evaluations [14]. Bayesian optimization (BO) offers a far more sample-efﬁcient alternative [57]. Bayesian Optimization: BO [38] is an established method for optimizing expensive-to-evaluate black-box functions. BO relies on a probabilistic surrogate model, typically a Gaussian Process (GP) [55], to provide a posterior distribution P(f|D) over the true function values f given the observed data D= {(xi,yi)}n i=1. An acquisition function α : Xcand ↦→R employs the surrogate model to assign a utility value to a set of candidates Xcand = {xi}q i=1 to be evaluated on the true function. While the true f may be expensive-to-evaluate, the surrogate-based acquisition function is not, and can thus be efﬁciently optimized to yield a set of candidates Xcand to be evaluated on f. If gradients of α(Xcand) are available, gradient-based methods can be utilized. If not, gradients are either approximated (e.g. with ﬁnite differences) or gradient-free methods (e.g. DIRECT [ 37] or CMA-ES [32]) are used. 1.2 Limitations of current approaches In the single-objective (SO) setting, a large body of work focuses on practical extensions to BO for supporting parallel evaluation and outcome constraints [49, 30, 66, 25, 43]. Less attention has been given to such extensions in the MO setting. Moreover, the existing constrained and parallel MO BO options have limitations: 1) many rely on scalarizations to transform the MO problem into a SO one [40]; 2) many acquisition functions are computationally expensive to compute [52, 21, 6, 71]; 3) few have known analytical gradients or are differentiable [19, 62, 33]; 4) many rely on heuristics to extend sequential algorithms to the parallel setting [27, 62]. A natural acquisition function for MO BO is Expected Hypervolume Improvement (EHVI ). Max- imizing the hypervolume ( HV) has been shown to produce Pareto fronts with excellent cover- age [73, 12, 69]. However, there has been little work on EHVI in the parallel setting, and the work that has been done resorts to approximate methods [71, 28, 62]. A vast body of literature has focused on efﬁcient EHVI computation [34, 20, 67], but the time complexity for computing EHVI is exponential in the number of objectives—in part due the hypervolume indicator itself incurring a time complexity that scales super-polynomially with the number of objectives [68]. Our core insight is that by exploiting advances in auto-differentiation and highly parallelized hardware [51], we can make EHVI computations fast and practical. 1.3 Contributions In this work, we derive a novel formulation of the parallelq-Expected Hypervolume Improvement acquisition function (qEHVI ) that is exact up to Monte-Carlo (MC) integration error. We compute the exact gradient of the MC estimator of qEHVI using auto-differentiation, which allows us to employ efﬁcient and effective gradient-based optimization methods. Rather than using ﬁrst-order gradient methods, we instead leverage the sample average approximation (SAA) approach from [5] to use higher-order deterministic optimization methods, and we prove theoretical convergence guarantees under the SAA approach. Our formulation of qEHVI is embarrassingly parallel, and despite its computational cost would achieve constant time complexity given inﬁnite processing cores. We demonstrate that, using modern GPU hardware and computing exact gradients, optimizing qEHVI is faster than existing state-of-the art methods in many practical scenarios. Moreover, we extend qEHVI to support auxiliary outcome constraints, making it practical in many real-world scenarios. Lastly, we demonstrate how modern auto-differentiation can be used to compute exact gradients of analytic EHVI , which has never been done before for M >2 objectives. Our empirical evaluation 2shows that qEHVI outperforms state-of-the-art multi-objective BO algorithms while using only a fraction of their wall time. 2 Related Work Yang et al. [69] is the only previous work to consider exact gradients of EHVI, but the authors only derive an analytical gradient for the unconstrained two-objective, sequential optimization setting. All other works either do not optimize EHVI (e.g. they use it for pre-screening candidates [ 18]), optimize it with gradient-free methods [68], or using approximate gradients [62]. In contrast, we use exact gradients and demonstrate that optimizing EHVI using this gradient information is far more efﬁcient. There are many alternatives to EHVI for MO BO. For example, ParEGO [ 40] and TS-TCH [50] randomly scalarize the objectives and use Expected Improvement [38] and Thompson Sampling [61], respectively. SMS-EGO [53] uses HV in a UCB-based acquisition function and is more scalable than EHVI [54]. ParEGO and SMS-EGO have only been considered for the q= 1, unconstrained setting. Predictive entropy search for MO BO (PESMO) [ 33] has been shown to be another competitive alternative and has been extended to handle constraints [ 26] and parallel evaluations [ 27]. MO max-value entropy search (MO-MES) has been shown to achieve superior optimization performance and faster wall times than PESMO, but is limited to q= 1. Wilson et al. [65] empirically and theoretically show that sequential greedy selection of qcandidates achieves performance comparable to jointly optimizing qcandidates for many acquisition functions (including [63, 66]). The sequential greedy approach integrates over the posterior of the unobserved outcomes corresponding to the previously selected candidates in the q-batch. Sequential greedy optimization often yields better empirical results because the optimization problem has a lower dimension: din each step, rather than qdin the joint problem. Most prior works in the MO setting use a sequential greedy approximation or heuristics [62, 71, 28, 10], but impute the unobserved outcomes with the posterior mean rather than integrating over the posterior [30]. For many joint acquisition functions involving expectations, this shortcut sacriﬁces the theoretical error bound on the sequential greedy approximation because the exact joint acquisition function overx1,..., xi, 1 ≤i≤qrequires integration over the joint posterior P(f(x1),..., f(xq)|D) and is not computed for i> 1. Garrido-Merchán and Hernández-Lobato [27] and Wada and Hino [62] jointly optimize the qcandi- dates and, noting the difﬁculty of the optimization, both papers focus on deriving gradients to aid in the optimization. Wada and Hino [62] deﬁned the qEHVI acquisition function, but after ﬁnding it challenging to optimize qcandidates jointly (without exact gradients), the authors propose optimizing an alternative acquisition function instead of exactqEHVI . In contrast, our novel qEHVI formulation allows for gradient-based parallel and sequential greedy optimization, with proper integration over the posterior for the latter. Feliot et al. [22] and Abdolshah et al. [1] proposed extensions of EHVI to the constrained q = 1 setting, but neither considers the batch setting and both rely on gradient-free optimization. 3 Differentiable q-Expected Hypervolume Improvement In this section, we review HVI and EHVI computation by means of box decompositions, and explain our novel formulation for the parallel setting. Deﬁnition 1. Given a reference point r ∈RM , the hypervolume indicator (HV) of a ﬁnite approxi- mate Pareto set Pis the M-dimensional Lebesgue measure λM of the space dominated by Pand bounded from below byr: HV(P,r) = λM (⋃|P| i=1[r,yi] ) , where [r,yi] denotes the hyper-rectangle bounded by vertices r and yi. Deﬁnition 2. Given a Pareto set Pand reference point r, the hypervolume improvement (HVI) of a set of points Yis: HVI (Y,P,r) = HV(P∪Y ,r) −HV(P,r).1 EHVI is the expectation of HVI over the posterior P(f,D): αEHVI (Xcand) = E [ HVI (f(Xcand)) ] . In the sequential setting, and assuming the objectives are independent and modeled with independent 1In this work, we omit the arguments Pand rwhen referring to HVI for brevity. 3GPs, EHVI can be expressed in closed form [69]. In other settings, EHVI can be approximated with MC integration. Following previous work, we assume that the reference point is known and speciﬁed by the decision maker [69] (see Appendix E.1.1 for additional discussion). 3.1 A review of hypervolume improvement computation using box decompositions Deﬁnition 3. For a set of objective vectors {f(xi)}q i=1, a reference point r ∈RM , and a non- dominated set P, let ∆({f(xi)}q i=1,P,r) ⊂RM denote the set of points (i) are dominated by {f(xi)}q i=1, dominate r, and are not dominated by P. Given P,r, the HVI of a new point f(x) is the HV of the intersection of space dominated by P∪{ f(x)}and the non-dominated space. Figure 1b illustrates this for one new point f(x) for M = 2. The yellow region is ∆({f(x)},P,r) and the hypervolume improvement is the volume covered by ∆({f(x)},P,r). Since ∆({f(x)},P,r) is often a non-rectangular polytope, HVI is typically computed by partitioning the non-dominated space into disjoint axis-parallel rectangles [12, 68] (see Figure 1a) and using piece-wise integration [18]. Let {Sk}K k=1 be a partitioning the of non-dominated space into disjoint hyper-rectangles, where each Sk is deﬁned by a pair of lower and upper vertices lk ∈RM and uk ∈RM ∪{∞}. The high level idea is to sum the HV ofSk ∩∆({f(x)},P,r) over all Sk. For each hyper-rectangle Sk, the intersec- tion of Sk and ∆({f(x)},P,r) is a hyper-rectangle where the lower bound vertex islk and the upper bound vertex is the component-wise minimum ofuk and the new pointf(x): zk := min [ uk,f(x) ] . r l3 S3 u3u2 S2 l2 l4 S4 u4u1 l1 S1 f (2)(x) f (1)(x) (a) f(x1) r l3 S3 u3u2 S2 l2 l4 S4 u4u1 l1 S1 f(2)(x ) f(1)(x ) (b) f(x1) f(2)(x ) f(1)(x )r l3 S3 u3u2 S2 l2 l4 S4 u4u1 l1 S1 f(x2)  (c) Figure 1: For M=2, (a) the dominated space (red) and the non-dominated space partitioned into disjoint boxes (white), (b) the HVI of one new point f(x), and (c) the HVI of two new points f(x1),f(x2). Hence, the HVI of a single outcome vector f(x) within Sk is given by HVI k ( f(x),lk,uk ) = λM ( Sk ∩∆({f(x)},P,r) ) = ∏M m=1 [ z(m) k −l(m) k ] +, where u(m) k ,l(m) k ,f(m)(x), and z(m) k denote the mth component of the corresponding vector and [·]+ denotes the min(·,0) operation. Summing over rectangles yields HVI ( f(x) ) = K∑ k=1 HVIk ( f(x),lk,uk ) = K∑ k=1 M∏ m=1 [ z(m) k −l(m) k ] + (1) 3.2 Computing q-Hypervolume Improvement via the Inclusion-Exclusion Principle Figure 1c illustrates the HVI in the q = 2 setting. Given q new points {f(xi)}q i=1, let Ai := ∆({f(xi)},P,r) for i= 1,...,q be the space dominated by f(xi) but not dominated by P, independently of the other q−1 points. Note that λM (Ai) = HVI (f(xi)). The union of the subsets Ai is the space dominated jointly by the qnew points: ⋃q i=1 Ai = ⋃q i=1 ∆({f(xi)},P,r), and the Lebesgue measure λM (⋃q i=1 Ai ) is the joint HVI from the qnew points. Since each subspace Ai is bounded, the restricted Lebesgue measure is ﬁnite and we may compute λM (⋃q i=1 Ai ) using the inclusion-exclusion principle [13, 59]: HVI ({f(xi)}q i=1) = λM ( q⋃ i=1 Ai ) = q∑ j=1 (−1)j+1 ∑ 1≤i1≤...≤ij≤q λM ( Ai1 ∩···∩ Aij ) (2) 4Since {Sk}K k=1 is a disjoint partition, λM (Ai1 ∩···∩ Aij ) = ∑K k=1 λM (Sk ∩Ai1 ∩···∩ Aij ), we can compute λM (Ai1 ∩···∩ Aij ) in a piece-wise fashion across the Khyper-rectangles {Sk}K k=1 as the HV of the intersection of Ai1 ∩···∩ Aij with each hyper-rectangle Sk. The inclusion- exclusion principle has been proposed for computingHV (not HVI) [45], but it is rarely used because complexity scales exponentially with the number of elements. However, the inclusion-exclusion principle is practical for computing the joint HVI of qpoints since typically q <<|P|. This formulation has three advantages. First, while the new dominated space Ai can be a non-rectangular polytope, the intersection Ai ∩Sk is a rectangular polytope, which simpliﬁes computation of overlapping hypervolume. Second, the vertices deﬁning the hyper-rectangle Sk ∩Ai1 ∩···∩ Aij are easily derived. The lower bound is simply the lk lower bound of Sk, and the upper bound is the component-wise minimum zk,i1,...ij := min [ uk,f(xi1 ),..., f(xij ) ] . Third, computation can be across all intersections of subsets Ai1 ∩···∩ Aij for 1 ≤ij ≤... ≤ij ≤qand across all Khyper-rectangles can be performed in parallel. Explicitly, the HVI is computed as: HVI ({f(xi)}q i=1) = K∑ k=1 q∑ j=1 ∑ Xj∈Xj (−1)j+1 M∏ m=1 [ z(m) k,Xj −l(m) k ] + (3) where Xj := {Xj ⊂ Xcand : |Xj| = j}is the superset of all subsets of Xcand of size j, and z(m) k,Xj := z(m) k,i1,...ij for Xj = {xi1 ,..., xij }. See Appendix A for further details of the derivation. 3.3 Computing Expected q-Hypervolume Improvement The above approach for computing HVI assumes that we know the true objective values f(Xcand) = {f(xi)}q i=1. In BO, we instead compute qEHVI as the expectation over the posterior model posterior: αqEHVI (Xcand) = E [ HVI (f(Xcand)) ] = ∫ ∞ −∞ HVI(f(Xcand))df. (4) Since no known analytical form is known [ 70] for q > 1 (or in the case of correlated out- comes), we estimate (4) using MC integration with samples from the joint posterior {ft(xi)}q i=1 ∼ P ( f(x1),..., f(xq)|D ) ,t = 1,...N . Let z(m) k,Xj,t := min [ uk,minx′∈Xj ft(x′) ] . Then, ˆαN qEHVI (Xcand) = 1 N N∑ t=1 HVI(ft(Xcand)) = 1 N N∑ t=1 K∑ k=1 q∑ j=1 ∑ Xj∈Xj (−1)j+1 M∏ m=1 [ z(m) k,Xj,t −l(m) k ] + (5) Provided that {Sk}K k=1 is an exact partitioning, (5) is an exact computation of qEHVI up to the MC estimation error, which scales as 1/ √ N when using iidMC samples regardless of the dimension of the search space [18]. In practice, we use randomized quasi MC methods [8] to reduce the variance and empirically observe low estimation error (see Figure 5a in the Appendix for a comparison of analytic EHVI and (quasi-)MC-based qEHVI). qEHVI requires computing the volume of 2q −1 hyper-rectangles (the number of subsets of q) for each of Khyper-rectangles and N MC samples. Given posterior samples, the time complexity on a single-threaded machine is: T1 = O(MNK(2q −1)). In the two-objective case, K = |P|+ 1, but K is super-polynomial in M [68]. The number of boxes required for a decomposition of the non-dominated space is unknown for M ≥4 [68]. qEHVI is agnostic to the partitioning algorithm used, and in F.4, we demonstrate using qEHVI in higher-dimensional objective spaces using an approximate box decomposition algorithm [11]. Despite the daunting workload, the critical work path—the time complexity of the smallest non-parallelizable unit—is constant: T∞= O(1).2 On highly-threaded many-core hardware (e.g. GPUs), our formulation achieves tractable wall times in many practical scenarios: as is shown in Figure 11 in the Appendix, the computation time is nearly constant with increasing quntil an inﬂection point at which the workload saturates the available cores. For additional discussion of both time and memory complexity of qEHVI see Appendix A.4. 3.4 Outcome Constraints Our proposed qEHVI acquisition function is easily extended to constraints on auxiliary outcomes. We consider the scenario where we receive observations of M objectives f(x) ∈RM and V constraints 2As evident from (5), the critical path consists of 3 multiplications and 5 summations. 5c(v) ∈RV , all of which are assumed to be “black-box”. We assume w.l.o.g. that c(v) is feasible iff c(v) ≥0. In the constrained optimization setting, we aim to identify the feasible Pareto set: Pfeas = {f(x) s.t. c(x) ≥0, ∄ x′ : c(x′) ≥0, f(x′) ≻f(x)}. The natural improvement measure in the constrained setting is feasible HVI, which we deﬁne for a single candidate point x as HVI C(f(x),c(x)) := HVI [f(x)] ·1 [c(x) ≥0]. Taking expectations, the constrained expected HV can be seen to be the HV weighted by the probability of feasibility. In Appendix A.3, we detail how performing feasibility-weighting on the sample-level allows us to include such auxiliary outcome constraints into our MC formulation in a straightforward way. 4 Optimizing q-Expected Hypervolume Improvement 4.1 Differentiability While an analytic formula for the gradient of EHVI exists for the M = 2 objective case in the unconstrained, sequential ( q = 1 ) setting, no such formula is known in 1) the case of M > 2 objectives, 2) the constrained setting, and 3) for q >1. Leveraging the re-parameterization trick [39, 64] and auto-differentiation, we are able to automatically compute exact gradients of the MC- estimator qEHVI in all of the above settings, as well as the gradient of analytic EHVI for M ≥2 (see Figure 5b in the Appendix for a comparison of the exact gradients of EHVI and the sample average gradients of qEHVI for M = 3).3,4 4.2 Optimization via Sample Average Approximation We show in Appendix C that if mean and covariance function of the GP are sufﬁciently regular, the gradient of the MC estimator (5) is an unbiased estimate of the gradient of the exact acquisition function (4). To maximize qEHVI , we could therefore directly apply stochastic optimization methods, as has previously been done for single-outcome acquisition functions [ 64, 66]. Instead, we opt to use the sample average approximation (SAA) approach from Balandat et al. [5], which allows us to employ deterministic, higher-order optimizers to achieve faster convergence rates. Informally (see Appendix C for the formal statement), if ˆx∗ N ∈arg maxx∈X ˆαN qEHVI (x), we can show under some regularity conditions that, as N →∞, (i) ˆαN qEHVI (ˆx∗ N ) →maxx∈XαqEHVI (x) a.s., and (ii) dist ( ˆx∗ N ,arg maxx∈XαqEHVI (x) ) →0 a.s.. These results hold for any covariance function satisfying the regularity conditions, including such ones that model correlation between outcomes. In particular, our results do not require the outputs to be modeled by independent GPs. Figure 2a demonstrates the importance of using exact gradients for efﬁciently and effectively op- timizing EHVI and qEHVI by comparing the following optimization methods: L-BFGS-B with exact gradients, L-BFGS-B with gradients approximated via ﬁnite differences, and CMA-ES (without gradients). The cumulative time spent optimizing the acquisition function is an order of magnitude less when using exact gradients rather than approximate gradients or zeroth order methods. 4.3 Sequential Greedy and Joint Batch Optimization Jointly optimizing qcandidates increases in difﬁculty with qbecause the problem dimension is dq. An alternative is to sequentially and greedily select candidates and condition the acquisition function on the previously selected pending points when selecting the next point [65]. Using a submodularity argument similar to that in Wilson et al. [64], the sequential greedy approximation of qEHVI enjoys regret of no more than 1 e α∗ qEHVI , where α∗ qEHVI is the optima of αqEHVI [23] (see Appendix B). Although sequential greedy approaches have been considered for many acquisition functions [65], no previous work has proposed a proper sequential greedy approach (with integration over the posterior) for parallel EHVI , as this would require computing the Pareto front under each sample ft from the joint posterior before computing the hypervolume improvement. These operations would be computationally expensive for even modest N and non-differentiable. qEHVI avoids determining the Pareto set for each sample by using inclusion-exclusion principle to compute the joint HVI over the pending points x1,..., xi−1 and the new candidate xi for each MC sample. Figure 2b empirically 3Technically,min and max are only sub-differentiable, but are known to be well-behaved [64]. In our MC setting with GP posteriors, qEHVI is differentiable w.p. 1 if xcontains no repeated points. 4For the constrained case, we replace the indicator with a differentiable sigmoid approximation. 60 5000 10000 15000 20000 Average/uni00A0Cumulative/uni00A0Acquisition/uni00A0Optimization/uni00A0Wall/uni00A0Time/uni00A0(s) 0.40 0.35 0.30 0.25 0.20 0.15 0.10 log/uni00A0HV/uni00A0difference EHVI/uni00A0/uni00AD/uni00A0Exact/uni00A0Gradient EHVI/uni00A0/uni00AD/uni00A0Approx./uni00A0Gradient EHVI/uni00A0/uni00AD/uni00A0Gradient/uni00A0Free qEHVI/uni00A0(q=2)/uni00A0/uni00AD/uni00A0Exact/uni00A0Gradient qEHVI/uni00A0(q=2)/uni00A0/uni00AD/uni00A0Approx./uni00A0Gradient qEHVI/uni00A0(q=2)/uni00A0/uni00AD/uni00A0Gradient/uni00A0Free (a) 0 20 40 60 80 100 Batch/uni00A0Iteration 1.8 1.6 1.4 1.2 1.0 0.8 0.6 0.4 log/uni00A0HV/uni00A0difference qEHVI/uni00A0Joint/uni00A0q=2 qEHVI/uni00A0Joint/uni00A0q=4 qEHVI/uni00A0Joint/uni00A0q=8 qEHVI/uni00A0Post./uni00A0Mean/uni00A0q=2 qEHVI/uni00A0Post./uni00A0Mean/uni00A0q=4 qEHVI/uni00A0Post./uni00A0Mean/uni00A0q=8 qEHVI/uni00A0Seq./uni00A0Greedy/uni00A0q=2 qEHVI/uni00A0Seq./uni00A0Greedy/uni00A0q=4 qEHVI/uni00A0Seq./uni00A0Greedy/uni00A0q=8 qEHVI/uni00A0q=1 (b) Figure 2: (a) A comparison of EHVI and qEHVI (q = 2) optimized with L-BFGS-B using exact gradients, L-BFGS-B using gradients approximated using ﬁnite differences, and CMA-ES, a gradient- free method. (b) A comparison of joint optimization, sequential greedy optimization with proper integration at the pending points, and sequential greedy using the posterior mean. Both plots show optimization performance on a DTLZ2 problem (d= 6,M = 2) with a budget of 100 evaluations (plus the initial quasi-random design). We report means and 2 standard errors across 20 trials. demonstrates the improved optimization performance from properly integrating over the unobserved outcomes rather than using the posterior mean or jointly optimizing the qcandidates. 5 Benchmarks We empirically evaluate qEHVI on synthetic and real world optimization problems. We compare qEHVI 5 against existing state-of-the-art methods including SMS-EGO6, PESMO6, TS-TCH5, and analytic EHVI [68] with gradients5. Additionally, we compare against a novel extension of ParEGO [40] that supports parallel evaluation and constraints (neither of which have been done before to our knowledge); we call this method qPAREGO5. Additionally, we include a quasi-random baseline that selects candidates from a scrambled Sobol sequence. See Appendix E.1 for details on all baseline algorithms. Synthetic Benchmarks We evaluate optimization performance on four benchmark problems in terms of log hypervolume difference, which is deﬁned as the difference between the hypervolume of the true (feasible) Pareto front and the hypervolume of the approximate (feasible) Pareto front based on the observed data; in the case that the true Pareto front is unknown (or not easily approximated), we evaluate the hypervolume indicator. All references points and search spaces are provided in Appendix E.2. For synthetic problems, we consider the Branin-Currin problem ( d = 2,M = 2, convex Pareto front) [6] and the C2-DTLZ2 (d= 12,M = 2,V = 1, concave Pareto front), which is a standard constrained benchmark from the MO literature [16] (see Appendix F.1 for additional synthetic benchmarks). Real-World Benchmarks Structural Optimization in Automobile Safety Design (VEHICLE SAFETY ): Vehicle crash safety is an important consideration in the structural design of automobiles. A lightweight car is preferable because of its potentially lower manufacturing cost and better fuel economy, but lighter material can fare worse than sturdier alternatives in a collision, potentially leading to increased vehicle damage and more severe injury to the vehicle occupants [72]. We consider the problem designing the thickness of 5 reinforced parts of the frontal frame of a vehicle that considerably affect crash safety. The goal is to minimize: 1) the mass of the vehicle; 2) the collision acceleration in a full frontal crash—a proxy for bio-mechanical trauma to the vehicle occupants from the acceleration; and 3) the toe-board 5Acquisition functions are available as part of the open-source library BoTorch [ 5]. Code is available at https://github.com/pytorch/botorch. 6We leverage existing implementations from the Spearmint library. The code is available at https:// github.com/HIPS/Spearmint/tree/PESM. 70 20 40 60 80 100 Function/uni00A0Evaluations 0.0 0.5 1.0 1.5log/uni00A0HV/uni00A0DifferenceSobol EHVI qEHVI qParEGO TS/uni00ADTCH PESMO SMS/uni00ADEGO (a) 0 20 40 60 80 100 Function/uni00A0Evaluations 1.0 0.9 0.8 0.7 0.6 0.5 0.4 log/uni00A0HV/uni00A0Difference Sobol qEHVI qParEGO (b) 0 20 40 60 80 100 Function/uni00A0Evaluations 0.0 0.5 1.0 1.5 2.0log/uni00A0HV/uni00A0DifferenceSobol EHVI qEHVI qParEGO TS/uni00ADTCH PESMO SMS/uni00ADEGO (c) 0 20 40 60 80 100 Function/uni00A0Evaluations 3.0 3.1 3.2 3.3 3.4 3.5 3.6HV 1e6 Sobol EHVI qEHVI qParEGO TS/uni00ADTCH PESMO SMS/uni00ADEGO (d) Figure 3: Sequential optimization performance on (a) on the Branin-Currin problem (q= 1), (b) the C2-DTLZ2 problem, (c) the vehicle crash safety problem (q= 1), and (d) the ABR control problem (q= 1). We report the means and 2 standard errors across 20 trials. intrusion—a measure of the most extreme mechanical damage to the vehicle in an off-frontal collision [44]. For this problem, we optimize the surrogate from Tanabe and Ishibuchi [60]. Policy Optimization for Adaptive Bitrate Control(ABR ): Many web services adapt video playback quality adaptively based on the receiver’s network bandwith to maintain steady, high quality stream with minimal stalls and buffer periods [47]. Previous works have proposed controllers with different scalarized objective functions [46], but in many cases, engineers may prefer to learn the set of optimal trade-offs between their metrics of interest, rather than specifying a scalarized objective in advance. In this problem, we decompose the objective function proposed in Mao et al. [46] into its constituent metrics and optimize 4 parameters of an ABR control policy on the Park simulator [48] to maximize video quality (bitrate) and minimize stall time. See Appendix E.2 for details. 5.1 Results Figure 3 shows thatqEHVI outperforms all baselines in terms of sequential optimization performance on all evaluated problems. Table 1 shows that qEHVI achieves wall times that are an order of magnitude smaller than those of PESMO on a CPU in sequential optimization, and maintains competitive wall times even relative toqPAREGO (which has a signiﬁcantly smaller workload) for large q on a GPU. TS-TCH has by far the fastest wall time, but this comes at the cost of inferior optimization performance. Figure 4 illustrates optimization performance of parallel acquisition functions for varying batch sizes. Increasing the level of parallelism leads to faster convergence for all algorithms (Figure 4a). In contrast with other algorithms, qEHVI ’s sample complexity does not deteriorate substantially when high levels of parallelism are used (Figure 4b). 8Table 1: Acquisition Optimization wall time in seconds on a CPU (2x Intel Xeon E5-2680 v4 @ 2.40GHz) and a GPU (Tesla V100-SXM2-16GB). We report the mean and 2 standard errors across 20 trials. NA indicates that the algorithm does not support constraints. CPU BRANIN CURRIN C2DTLZ2 ABR V EHICLE SAFETY PESMO ( q=1) 249.16 (±19.35) NA 214.16 (±18.38) 492 .64 (±58.98) SMS-EGO ( q=1) 146.1 (±8.57) NA 89.54 (±5.79) 115 .11 (±8.21) TS-TCH ( q=1) 2.82 (±0.03) NA 17.22 (±0.04) 47 .46 (±0.05) qPAREGO ( q=1) 1.56 (±0.16) 4 .01 (±0.77) 7 .47 (±0.67) 1 .74 (±0.27) EHVI ( q=1) 3.04 (±0.16) NA 2.48 (±0.19) 15 .18 (±2.24) qEHVI ( q=1) 3.63 (±0.23) 5 .4 (±1.18) 6 .15 (±0.71) 67 .54 (±10.45) GPU BRANIN CURRIN C2DTLZ2 ABR V EHICLE SAFETY TS-TCH ( q=1) 0.07 (±0.00) NA 0.16 (±0.00) 0 .32 (±0.0) TS-TCH ( q=2) 0.07 (±0.00) NA 0.15 (±0.00) 0 .34 (±0.01) TS-TCH ( q=4) 0.09 (±0.01) NA 0.15 (±0.00) 0 .31 (±0.01) TS-TCH ( q=8) 0.08 (±0.00) NA 0.16 (±0.00) 0 .34 (±0.01) qPAREGO ( q=1) 3.2 (±0.37) 3 .85 (±0.91) 9 .64 (±0.96) 3 .44 (±0.51) qPAREGO ( q=2) 7.12 (±0.81) 12 .1 (±2.77) 21 .19 (±1.53) 7 .32 (±0.97) qPAREGO ( q=4) 15.34 (±1.69) 39 .71 (±7.40) 35 .46 (±2.32) 17 .2 (±2.29) qPAREGO ( q=8) 32.11 (±4.14) 99 .58 (±15.20) 72 .52 (±5.04) 39 .72 (±7.13) EHVI ( q=1) 4.53 (±0.23) NA 6.82 (±0.55) 8 .95 (±0.64) qEHVI ( q=1) 5.98 (±0.28) 3 .36 (±0.94) 7 .71 (±0.67) 10 .43 (±0.64) qEHVI ( q=2) 11.37 (±0.56) 21 .56 (±3.45) 18 .32 (±1.48) 17 .67 (±1.54) qEHVI ( q=4) 25.29 (±1.51) 89 .18 (±10.86) 44 .44 (±3.53) 54 .25 (±4.17) qEHVI ( q=8) 102.46 (±9.22) 215 .74 (±15.85) 100 .64 (±7.22) 255 .72 (±23.73) 0 20 40 60 80 100 Batch/uni00A0Iteration 3.0 3.1 3.2 3.3 3.4 3.5 3.6HV 1e6 Sobol/uni00A0q=8 TS/uni00A0q=1 TS/uni00A0q=2 TS/uni00A0q=4 TS/uni00A0q=8 qEHVI/uni00A0q=1 qEHVI/uni00A0q=2 qEHVI/uni00A0q=4 qEHVI/uni00A0q=8 qParEGO/uni00A0q=1 qParEGO/uni00A0q=2 qParEGO/uni00A0q=4 qParEGO/uni00A0q=8 (a) 0 20 40 60 80 100 Function/uni00A0Evaluations 3.0 3.1 3.2 3.3 3.4 3.5 3.6HV 1e6 Sobol/uni00A0q=8 TS/uni00A0q=1 TS/uni00A0q=2 TS/uni00A0q=4 TS/uni00A0q=8 qEHVI/uni00A0q=1 qEHVI/uni00A0q=2 qEHVI/uni00A0q=4 qEHVI/uni00A0q=8 qParEGO/uni00A0q=1 qParEGO/uni00A0q=2 qParEGO/uni00A0q=4 qParEGO/uni00A0q=8 (b) Figure 4: Parallel optimization performance on the ABR problem with varying batch sizes (q) by (a) batch BO iterations and (b) function evaluations. 6 Discussion We present a practical and efﬁcient acquisition function, qEHVI , for parallel, constrained multi- objective Bayesian optimization. Leveraging differentiable programming, modern parallel hardware, and the Sample Average Approximation, we efﬁciently optimizeqEHVI via quasi second-order meth- ods and provide theoretical convergence guarantees for our approach. Empirically, we demonstrate that our method out-performs state-of-the-art multi-objective Bayesian optimization methods. One limitation of our approach is that it currently assumes noiseless observations, which, to our knowledge, is the case with all formulations of EHVI . Integrating over the uncertainty around the previous observations [43] by using MC samples over the new candidates and the training points, one may be able to account for the noise.Another limitation of qEHVI is that its scalability is limited the partitioning algorithm, precluding its use in high-dimensional objective spaces. More scalable partitioning algorithms, either approximate algorithms (e.g. the algorithm proposed by Couckuyt et al. [11], which we examine brieﬂy in Appendix F.4) or more efﬁcient exact algorithms that result in fewer disjoint hyper-rectangles (e.g. [41, 17, 69]), will improve the scalability and computation time of of qEHVI . We hope this work encourages researchers to consider more improvements from applying modern computational paradigms and tooling to Bayesian optimization. 97 Statement of Broader Impact Optimizing a single outcome commonly comes at the expense of other secondary outcomes. In some cases, decision makers may be able to form a scalarization of their objectives in advance, but in the researcher’s experience, formulating such trade-offs in advance is difﬁcult for most. Improvements to the optimization performance and practicality of multi-objective Bayesian optimization have the potential to allow decision makers to better understand and make more informed decisions across multiple trade-offs. We expect these directions to be particularly important as Bayesian optimization is increasingly used for applications such as recommender systems [42], where auxiliary goals such as fairness must be accounted for. Of course, at the end of the day, exactly what objectives decision makers choose to optimize, and how they balance those trade-offs (and whether that is done in equitable fashion) is up to the individuals themselves. Acknowledgments We would like to thank Daniel Jiang for helpful discussions around our theoretical results. References [1] M. Abdolshah, A. Shilton, S. Rana, S. Gupta, and S. Venkatesh. Expected hypervolume improvement with constraints. In 2018 24th International Conference on Pattern Recognition (ICPR), pages 3238–3243, 2018. [2] Arash Asadpour, Hamid Nazerzadeh, and Amin Saberi. Stochastic submodular maximization. In Christos Papadimitriou and Shuzhong Zhang, editors, Internet and Network Economics. Springer Berlin Heidelberg, 2008. [3] R. Astudillo and P. Frazier. Bayesian optimization of composite functions. Forthcoming, in Proceedings of the 35th International Conference on Machine Learning, 2019. [4] Anne Auger, Johannes Bader, Dimo Brockhoff, and Eckart Zitzler. Theory of the hypervolume indicator: Optimal mu-distributions and the choice of the reference point. In Proceedings of the Tenth ACM SIGEVO Workshop on Foundations of Genetic Algorithms, FOGA ’09, page 87–102, New York, NY , USA, 2009. Association for Computing Machinery. [5] Maximilian Balandat, Brian Karrer, Daniel R. Jiang, Samuel Daulton, Benjamin Letham, Andrew Gordon Wilson, and Eytan Bakshy. BoTorch: A Framework for Efﬁcient Monte-Carlo Bayesian Optimization. In Advances in Neural Information Processing Systems 33, 2020. [6] Syrine Belakaria, Aryan Deshwal, and Janardhan Rao Doppa. Max-value entropy search for multi-objective bayesian optimization. In Advances in Neural Information Processing Systems 32, 2019. [7] Eric Bradford, Artur Schweidtmann, and Alexei Lapkin. Efﬁcient multiobjective optimization employing gaussian processes, spectral sampling and a genetic algorithm. Journal of Global Optimization, 71, 02 2018. doi: 10.1007/s10898-018-0609-2. [8] Russel E Caﬂisch. Monte carlo and quasi-monte carlo methods. Acta numerica, 7:1–49, 1998. [9] Mauro Cerasoli and Aniello Fedullo. The inclusion-exclusion principle. Journal of Interdisciplinary Mathematics, 5(2):127–141, 2002. [10] Anirban Chaudhuri, Raphael Haftka, Peter Ifju, Kelvin Chang, Christopher Tyler, and Tony Schmitz. Experimental ﬂapping wing optimization and uncertainty quantiﬁcation using limited samples. Structural and Multidisciplinary Optimization, 51, 11 2014. doi: 10.1007/s00158-014-1184-x. [11] I. Couckuyt, D. Deschrijver, and T. Dhaene. Towards efﬁcient multiobjective optimization: Multiobjective statistical criterions. In 2012 IEEE Congress on Evolutionary Computation, pages 1–8, 2012. [12] Ivo Couckuyt, Dirk Deschrijver, and Tom Dhaene. Fast calculation of multiobjective probability of improvement and expected improvement criteria for pareto optimization. J. of Global Optimization, 60(3): 575–594, November 2014. [13] Daniel A. da Silva. Proprietades geraes. J. de l’Ecole Polytechnique, cah. 30. I, 1854. 10[14] K. Deb, A. Pratap, S. Agarwal, and T. Meyarivan. A fast and elitist multiobjective genetic algorithm: Nsga-ii. IEEE Transactions on Evolutionary Computation, 6(2):182–197, 2002. [15] Kalyan Deb, L. Thiele, Marco Laumanns, and Eckart Zitzler. Scalable multi-objective optimization test problems. volume 1, pages 825–830, 06 2002. ISBN 0-7803-7282-4. doi: 10.1109/CEC.2002.1007032. [16] Kalyanmoy Deb. Constrained Multi-objective Evolutionary Algorithm, pages 85–118. Springer Interna- tional Publishing, Cham, 2019. [17] Kerstin Dächert, Kathrin Klamroth, Renaud Lacour, and Daniel Vanderpooten. Efﬁcient computation of the search region in multi-objective optimization. European Journal of Operational Research, 260(3):841 – 855, 2017. [18] M. T. M. Emmerich, K. C. Giannakoglou, and B. Naujoks. Single- and multiobjective evolutionary opti- mization assisted by gaussian random ﬁeld metamodels. IEEE Transactions on Evolutionary Computation, 10(4):421–439, 2006. [19] M. T. M. Emmerich, A. H. Deutz, and J. W. Klinkenberg. Hypervolume-based expected improvement: Monotonicity properties and exact computation. In 2011 IEEE Congress of Evolutionary Computation (CEC), pages 2147–2154, 2011. [20] Michael Emmerich, Kaifeng Yang, André Deutz, Hao Wang, and Carlos M. Fonseca. A Multicriteria Generalization of Bayesian Global Optimization, pages 229–242. Springer International Publishing, 2016. [21] Michael T. M. Emmerich and Carlos M. Fonseca. Computing hypervolume contributions in low dimensions: Asymptotically optimal algorithm and complexity results. In Ricardo H. C. Takahashi, Kalyanmoy Deb, Elizabeth F. Wanner, and Salvatore Greco, editors, Evolutionary Multi-Criterion Optimization , pages 121–135, Berlin, Heidelberg, 2011. Springer Berlin Heidelberg. [22] Paul Feliot, Julien Bect, and Emmanuel Vazquez. A bayesian approach to constrained single- and multi- objective optimization. Journal of Global Optimization, 67(1-2):97–133, Apr 2016. ISSN 1573-2916. doi: 10.1007/s10898-016-0427-3. URL http://dx.doi.org/10.1007/s10898-016-0427-3 . [23] M. L. Fisher, G. L. Nemhauser, and L. A. Wolsey. An analysis of approximations for maximizing submodular set functions—II , pages 73–87. Springer Berlin Heidelberg, Berlin, Heidelberg, 1978. [24] Tobias Friedrich and Frank Neumann. Maximizing submodular functions under matroid constraints by multi-objective evolutionary algorithms. In Thomas Bartz-Beielstein, Jürgen Branke, Bogdan Filipiˇc, and Jim Smith, editors, Parallel Problem Solving from Nature – PPSN XIII , pages 922–931, Cham, 2014. Springer International Publishing. ISBN 978-3-319-10762-2. [25] Jacob Gardner, Matt Kusner, Zhixiang, Kilian Weinberger, and John Cunningham. Bayesian optimization with inequality constraints. In Proceedings of the 31st International Conference on Machine Learning, volume 32 of Proceedings of Machine Learning Research, pages 937–945, Beijing, China, 22–24 Jun 2014. PMLR. [26] Eduardo C Garrido-Merchán and Daniel Hernández-Lobato. Predictive entropy search for multi-objective bayesian optimization with constraints. Neurocomputing, 361:50–68, 2019. [27] Eduardo C Garrido-Merchán and Daniel Hernández-Lobato. Parallel predictive entropy search for multi- objective bayesian optimization with constraints, 2020. [28] David Gaudrie, Rodolphe Le Riche, Victor Picheny, Benoît Enaux, and Vincent Herbert. Targeting solutions in bayesian multi-objective optimization: sequential and batch versions. Annals of Mathematics and Artiﬁcial Intelligence, 88(1-3):187–212, Aug 2019. ISSN 1573-7470. doi: 10.1007/s10472-019-09644-8. URL http://dx.doi.org/10.1007/s10472-019-09644-8 . [29] Michael A. Gelbart, Jasper Snoek, and Ryan P. Adams. Bayesian optimization with unknown constraints. In Proceedings of the 30th Conference on Uncertainty in Artiﬁcial Intelligence, UAI, 2014. [30] David Ginsbourger, Rodolphe Le Riche, and Laurent Carraro. Kriging Is Well-Suited to Parallelize Optimization, pages 131–162. Springer Berlin Heidelberg, Berlin, Heidelberg, 2010. [31] P. Glasserman. Performance continuity and differentiability in monte carlo optimization. In 1988 Winter Simulation Conference Proceedings, pages 518–524, 1988. [32] Nikolaus Hansen. The CMA Evolution Strategy: A Comparing Review, volume 192, pages 75–102. 06 2007. doi: 10.1007/3-540-32494-1_4. 11[33] Daniel Hernández-Lobato, José Miguel Hernández-Lobato, Amar Shah, and Ryan P. Adams. Predictive entropy search for multi-objective bayesian optimization, 2015. [34] Iris Hupkens, Andre Deutz, Kaifeng Yang, and Michael Emmerich. Faster exact algorithms for computing expected hypervolume improvement. In Antonio Gaspar-Cunha, Carlos Henggeler Antunes, and Car- los Coello Coello, editors, Evolutionary Multi-Criterion Optimization, pages 65–79. Springer International Publishing, 2015. [35] Hisao Ishibuchi, Naoya Akedo, and Yusuke Nojima. A many-objective test problem for visually examining diversity maintenance behavior in a decision space. In Proceedings of the 13th Annual Conference on Genetic and Evolutionary Computation , GECCO ’11, page 649–656, New York, NY , USA, 2011. Association for Computing Machinery. ISBN 9781450305570. doi: 10.1145/2001576.2001666. URL https://doi.org/10.1145/2001576.2001666. [36] Hisao Ishibuchi, Ryo Imada, Yu Setoguchi, and Yusuke Nojima. How to specify a reference point in hypervolume calculation for fair performance comparison. Evol. Comput., 26(3):411–440, September 2018. [37] Donald Jones, C. Perttunen, and B. Stuckman. Lipschitzian optimisation without the lipschitz constant. Journal of Optimization Theory and Applications, 79:157–181, 01 1993. doi: 10.1007/BF00941892. [38] Donald R. Jones, Matthias Schonlau, and William J. Welch. Efﬁcient global optimization of expensive black-box functions. Journal of Global Optimization, 13:455–492, 1998. [39] Diederik P Kingma and Max Welling. Auto-Encoding Variational Bayes. arXiv e-prints , page arXiv:1312.6114, Dec 2013. [40] J. Knowles. Parego: a hybrid algorithm with on-line landscape approximation for expensive multiobjective optimization problems. IEEE Transactions on Evolutionary Computation, 10(1):50–66, 2006. [41] Renaud Lacour, Kathrin Klamroth, and Carlos M. Fonseca. A box decomposition algorithm to compute the hypervolume indicator. Computers & Operations Research, 79:347 – 360, 2017. [42] Benjamin Letham and Eytan Bakshy. Bayesian optimization for policy search via online-ofﬂine experimen- tation. Journal of Machine Learning Research, 20(145):1–30, 2019. URL http://jmlr.org/papers/ v20/18-225.html. [43] Benjamin Letham, Brian Karrer, Guilherme Ottoni, and Eytan Bakshy. Constrained bayesian optimization with noisy experiments. Bayesian Analysis, 14(2):495–519, 06 2019. doi: 10.1214/18-BA1110. [44] Xingtao Liao, Qing Li, Xujing Yang, Weigang Zhang, and Wei Li. Multiobjective optimization for crash safety design of vehicles using stepwise regression model. Structural and Multidisciplinary Optimization, 35:561–569, 06 2008. doi: 10.1007/s00158-007-0163-x. [45] Edgar Manoatl Lopez, Luis Miguel Antonio, and Carlos A. Coello Coello. A gpu-based algorithm for a faster hypervolume contribution computation. In António Gaspar-Cunha, Carlos Henggeler Antunes, and Carlos Coello Coello, editors, Evolutionary Multi-Criterion Optimization, pages 80–94. Springer International Publishing, 2015. [46] Hongzi Mao, Ravi Netravali, and Mohammad Alizadeh. Neural adaptive video streaming with pensieve. In Proceedings of the Conference of the ACM Special Interest Group on Data Communication, SIGCOMM ’17, page 197–210, New York, NY , USA, 2017. Association for Computing Machinery. ISBN 9781450346535. doi: 10.1145/3098822.3098843. URL https://doi.org/10.1145/3098822.3098843. [47] Hongzi Mao, Shannon Chen, Drew Dimmery, Shaun Singh, Drew Blaisdell, Yuandong Tian, Mohammad Alizadeh, and Eytan Bakshy. Real-world video adaptation with reinforcement learning. 2019. [48] Hongzi Mao, Parimarjan Negi, Akshay Narayan, Hanrui Wang, Jiacheng Yang, Haonan Wang, Ryan Marcus, Ravichandra Addanki, Mehrdad Khani Shirkoohi, Songtao He, Vikram Nathan, Frank Cangialosi, Shaileshh Bojja Venkatakrishnan, Wei-Hung Weng, Shu-Wen Han, Tim Kraska, and Mohammad Alizadeh. Park: An open platform for learning-augmented computer systems. In NeurIPS, 2019. [49] Sébastien Marmin, Clément Chevalier, and David Ginsbourger. Differentiating the multipoint expected improvement for optimal batch design. In Panos Pardalos, Mario Pavone, Giovanni Maria Farinella, and Vincenzo Cutello, editors, Machine Learning, Optimization, and Big Data , pages 37–48, Cham, 2015. Springer International Publishing. [50] B. Paria, K. Kandasamy, and B. Póczos. A Flexible Multi-Objective Bayesian Optimization Approach using Random Scalarizations. ArXiv e-prints, May 2018. 12[51] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in PyTorch. 2017. [52] Victor Picheny. Multiobjective optimization using gaussian process emulators via stepwise uncertainty reduction. Statistics and Computing, 25, 10 2013. doi: 10.1007/s11222-014-9477-x. [53] Wolfgang Ponweiser, Tobias Wagner, Dirk Biermann, and Markus Vincze. Multiobjective optimization on a limited budget of evaluations using model-assisted s-metric selection. In Günter Rudolph, Thomas Jansen, Nicola Beume, Simon Lucas, and Carlo Poloni, editors, Parallel Problem Solving from Nature – PPSN X, pages 784–794, Berlin, Heidelberg, 2008. Springer Berlin Heidelberg. [54] Alma A. M. Rahat, Richard M. Everson, and Jonathan E. Fieldsend. Alternative inﬁll strategies for expensive multi-objective optimisation. In Proceedings of the Genetic and Evolutionary Computation Con- ference, GECCO ’17, page 873–880, New York, NY , USA, 2017. Association for Computing Machinery. ISBN 9781450349208. [55] Carl Edward Rasmussen. Gaussian Processes in Machine Learning , pages 63–71. Springer Berlin Heidelberg, Berlin, Heidelberg, 2004. [56] Jerry Segercrantz. Inclusion-exclusion and characteristic functions.Mathematics Magazine, 71(3):216–218, 1998. ISSN 0025570X, 19300980. URL http://www.jstor.org/stable/2691209. [57] B. Shahriari, K. Swersky, Z. Wang, R. P. Adams, and N. de Freitas. Taking the human out of the loop: A review of bayesian optimization. Proceedings of the IEEE, 104(1):148–175, 2016. [58] Niranjan Srinivas, Andreas Krause, Sham Kakade, and Matthias Seeger. Gaussian process optimization in the bandit setting: No regret and experimental design. In Proceedings of the 27th International Conference on International Conference on Machine Learning, ICML’10, page 1015–1022, Madison, WI, USA, 2010. Omnipress. ISBN 9781605589077. [59] J. Sylvester. Note sur la théorème de legendre. Comptes Rendus Acad. Sci., 96:463–465, 1883. [60] Ryoji Tanabe and Hisao Ishibuchi. An easy-to-use real-world multi-objective optimization problem suite. Applied Soft Computing, 89:106078, 2020. ISSN 1568-4946. doi: https://doi.org/10.1016/j.asoc.2020. 106078. [61] William R. Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 25(3/4):285–294, 1933. [62] Takashi Wada and Hideitsu Hino. Bayesian optimization for multi-objective optimization and multi-point search, 2019. [63] Jialei Wang, Scott C. Clark, Eric Liu, and Peter I. Frazier. Parallel bayesian global optimization of expensive functions, 2016. [64] J. T. Wilson, R. Moriconi, F. Hutter, and M. P. Deisenroth. The reparameterization trick for acquisition functions. ArXiv e-prints, December 2017. [65] James Wilson, Frank Hutter, and Marc Deisenroth. Maximizing acquisition functions for bayesian optimization. In Advances in Neural Information Processing Systems 31, pages 9905–9916. 2018. [66] Jian Wu and Peter I. Frazier. The parallel knowledge gradient method for batch bayesian optimization. In Proceedings of the 30th International Conference on Neural Information Processing Systems, NIPS’16, page 3134–3142, Red Hook, NY , USA, 2016. Curran Associates Inc. ISBN 9781510838819. [67] Kaifeng Yang, Michael Emmerich, André Deutz, and Carlos M. Fonseca. Computing 3-d expected hypervolume improvement and related integrals in asymptotically optimal time. In 9th International Conference on Evolutionary Multi-Criterion Optimization - Volume 10173, EMO 2017, page 685–700, Berlin, Heidelberg, 2017. Springer-Verlag. [68] Kaifeng Yang, Michael Emmerich, André H. Deutz, and Thomas Bäck. Efﬁcient computation of expected hypervolume improvement using box decomposition algorithms. CoRR, abs/1904.12672, 2019. [69] Kaifeng Yang, Michael Emmerich, André Deutz, and Thomas Bäck. Multi-objective bayesian global optimization using expected hypervolume improvement gradient. Swarm and Evolutionary Computation, 44:945 – 956, 2019. ISSN 2210-6502. doi: https://doi.org/10.1016/j.swevo.2018.10.007. URL http: //www.sciencedirect.com/science/article/pii/S2210650217307861. 13[70] Kaifeng Yang, Pramudita Palar, Michael Emmerich, Koji Shimoyama, and Thomas Bäck. A multi- point mechanism of expected hypervolume improvement for parallel multi-objective bayesian global optimization. pages 656–663, 07 2019. doi: 10.1145/3321707.3321784. [71] Kaifeng Yang, Pramudita Satria Palar, Michael Emmerich, Koji Shimoyama, and Thomas Bäck. A multi-point mechanism of expected hypervolume improvement for parallel multi-objective bayesian global optimization. In Proceedings of the Genetic and Evolutionary Computation Conference, GECCO ’19, page 656–663, New York, NY , USA, 2019. Association for Computing Machinery. ISBN 9781450361118. doi: 10.1145/3321707.3321784. URL https://doi.org/10.1145/3321707.3321784. [72] R. J. Yang, N. Wang, C. H. Tho, J. P. Bobineau, and B. P. Wang. Metamodeling Development for Vehicle Frontal Impact Simulation. Journal of Mechanical Design, 127(5):1014–1020, 01 2005. [73] E. Zitzler, L. Thiele, M. Laumanns, C. M. Fonseca, and V . G. da Fonseca. Performance assessment of multiobjective optimizers: an analysis and review. IEEE Transactions on Evolutionary Computation, 7(2): 117–132, 2003. 14Appendix to: Differentiable Expected Hypervolume Improvement for Parallel Multi-Objective Bayesian Optimization A Derivation of q-Expected Hypervolume Improvement A.1 Hypervolume Improvement via the Inclusion-Exclusion Principle The hypervolume improvement of f(x) within the hyper-rectangle Sk is the volume of Sk ∩∆({f(x)},P,r) and is given by: HVIk ( f(x),lk,uk ) = λM ( Sk ∩∆({f(x)},P,r) ) = M∏ m=1 [ z(m) k −l(m) k ] +, where u(m) k ,l(m) k ,f(m)(x), and z(m) k denote the mth component of the corresponding vector and [·]+ denotes the min(·,0) operation. Summing over all Sk gives the total hypervolume improvement: HVI ( f(x) ) = K∑ k=1 HVIk ( f(x),lk,uk ) = K∑ k=1 λM ( Sk ∩∆({f(x)},P,r) ) = K∑ k=1 M∏ m=1 [ z(m) k −l(m) k ] +. We can extend the HVI computation to the q >1 case using the inclusion-exclusion principle. Principle 1. The inclusion-exclusion principle[13, 59, 9] Given a ﬁnite measure space (B,A,µ) and a ﬁnite sequence of potentially empty or overlapping sets {Ai}i = 1n where Ai ∈A and µ(B) <∞, then, λM ( p⋃ i=1 Ai ) = p∑ j=1 (−1)j+1 ∑ 1≤i1≤...≤ij≤p λM ( Ai1 ∩...∩Aij ) In the context of computing the joint HVI of q new points{f(xi)}q i=1, each subset Ai for i = 1,...,q is the set of points contained in ∆({f(xi)},P,r) — independently of the other q−1 points. λM (Ai) is the hypervolume improvement from the new point f(xi): λM (Ai) = HVI(f(xi)). The union of these subsets is the set of points in the new space dominated by the qnew points: ⋃q i=1 Ai = ⋃q i=1 ∆({f(xi)},P,r). The hypervolume of ⋃q i=1 ∆({f(xi)},P,r) is the hypervolume improvement from the qnew points: HVI({f(xi)}q i=1) = λM ( q⋃ i=1 Ai ) = q∑ j=1 (−1)j+1 ∑ 1≤i1≤...≤ij≤q λM ( Ai1 ∩···∩ Aij ) To compute λM (Ai1 ∩···∩ Aij ), we partition the space covered by Ai1 ∩···∩ Aij across the K hyper- rectangles {Sk}K k=1 and compute the hypervolume of the overlapping space of Ai1 ∩···∩ Aij with each Sk independently. Since {Sk}K k=1 is a disjoint partition, summing overKgives the hypervolume ofAi1 ∩···∩ Aij : λM ( Ai1 ∩···∩ Aij ) = K∑ k=1 λM ( Sk ∩Ai1 ∩···∩ Aij ) This has two advantages. First, the new dominated space Ai can be a non-rectangular polytope, but the intersection Ai ∩Sk is a rectangular polytope, which simpliﬁes computation of overlapping hypervolume. 15Second, the vertices deﬁning the hyper-rectangle encapsulated by Sk ∩Ai1 ∩···∩ Aij are easily derived. The lower bound is simply the lk lower bound of Sk and the upper bound is the component-wise minimum zk,i1,...ij = min [ uk,f(xi1 ),..., f(xij ) ] . Importantly, this is computationally tractable because this speciﬁc approach enables parallelizing computation across all intersections of subsets Ai1 ∩···∩ Aij for 1 ≤ij ≤... ≤ij ≤qand across all Khyper-rectangles. Explicitly, the HVI is computed as: HVI({f(xi)}q i=1) = λM ( p⋃ i=1 Ai ) = q∑ j=1 ∑ 1≤i1≤...≤ij≤q (−1)j+1λM ( Ai1 ∩···∩ Aij ) = K∑ k=1 q∑ j=1 ∑ 1≤i1≤...≤ij≤q (−1)j+1λM ( Sk ∩Ai1 ∩···∩ Aij ) = K∑ k=1 q∑ j=1 ∑ 1≤i1≤...≤ij≤q (−1)j+1λM ( Sk ∩∆({f(xi1 )},P,r) ∩... ∩∆({f(xij )},P,r) ) = K∑ k=1 q∑ j=1 ∑ 1≤i1≤...≤ij≤q (−1)j+1 M∏ m=1 [ z(m) k,i1,...ij −l(m) k ] + = K∑ k=1 q∑ j=1 ∑ Xj∈Xj (−1)j+1 M∏ m=1 [ z(m) k,Xj −l(m) k ] + where Xj is the superset all subsets of Xcand of size j: Xj = {Xj ⊂Xcand : |Xj|= j}and z(m) k,Xj = z(m) k,i1,...ij for Xj = {xi1 ,..., xij }. A.2 Computing Expected Hypervolume Improvement The above approach for computing HVI assumes we know the true objective values {f(xi)}q i=1. Since we do not know the true function values {f(xi)}q i=1, we compute qEHVI as the expectation over the GP posterior. αqEHVI = E [ HVI({f(xi)}q i=1) ] = ∫ RM HVI({f(xi)}q i=1)df (6) In the sequential setting and under the assumption of independent outcomes, qEHVI is simply EHVI and can be expressed in closed form [ 69]. However when q > 1, there is no known analytical formulation [70]. Instead, we estimate the expectation in (6) using MC integration with samples from the joint posterior P ( f(x1),..., f(xq)|D): αqEHVI = E [ HVI({f(xi)}q i=1) ] ≈ 1 N N∑ t=1 HVI({ft(xi)}q i=1) (7) = 1 N N∑ t=1 K∑ k=1 q∑ j=1 ∑ Xj∈Xj (−1)j+1 M∏ m=1 [ z(m) k,Xj,t −l(m) k ] + (8) where {ft(xi)}q i=1 ∼P ( f(x1),..., f(xq)|X,Y ) is the tth sample from the joint posterior over Xcand and z(m) k,Xj,t = min [ uk,minx′∈Xj ft(x′) ] . A.3 Supporting Outcome Constraints Recall that we deﬁned the constrained hypervolume improvement as HVI C (f(x),c(x)) = HVI[f(x)] ·1 [c(x) ≥0]. (9) For q= 1 and assuming independence of the objectives and the constraints, the expected HVI C is the product of the expected HVI and the probability of feasibility (the expectation of 1 [c(x) ≥0]) [22]. However, requiring objectives and constraints to be independent is unnecessary when estimating the expectation with MC integration using samples from the joint posterior. 16In the parallel setting, if all constraints are satisﬁed for all q candidates Xcand = {xi}q i=1, HVI C is simply HVI . If a subset V⊂X cand,V̸= ∅ of the candidates violate at least one of the constraints, then the feasible HVI is the HVI of the set of feasible candidates: HVI C (Xcand) = HVI(Xcand \\V). That is, the hypervolume contribution (i.e. the marginal HVI) of an infeasible point is zero. In our formulation, HVI can be computed by multiplying (5) with an additional factor ∏ x′∈Xj ∏V v=1 1 [c(v)(x′) ≥0]: HVI C ({f(xi),c(xi)}q i=1) = K∑ k=1 q∑ j=1 ∑ Xj∈Xj (−1)j+1 [( M∏ m=1 [ z(m) k,Xj −l(m) k ] + ) ∏ x′∈Xj V∏ v=1 1 [c(v)(x′) ≥0] ] . (10) The additional factor ∏ x′∈Xj ∏V v=1 1 [c(v)(xa) ≥0] indicates whether all constraints are satisﬁed for all candidates in a given subset Xj. Thus HVI C can be computed in the same fashion as HVI , but with the additional step of setting the HV of all subsets containing x′to zero if x′violates any constraint. We can now again perform MC integration as in (5) to compute the expected constrained hypervolume improvement. In this formulation, the marginal hypervolume improvement from a candidate is weighted by the probability that the candidate is feasible. The marginal hypervolume improvements are highly dependent on the outcomes of the other candidates. Importantly, the MC-based approach enables us to properly estimate the marginal hypervolume improvements across candidates by sampling from the joint posterior. Note that while the expected constrained hypervolume E [ HVI C ({f(xi),c(xi)}q i=1) ] is differentiable, we may not differentiate inside the expectation (hence we cannot expect simply differentiating (10) on the sample-level to provide proper gradients). We therefore replace the indicator with a sigmoid function with temperature parameter ϵ, which provides a differentiable relaxation 1 [c(v)(x′) ≥0] ≈s(c(v)(x′); ϵ) := 1 1 + exp(−c(v)(x′)/ϵ) (11) that becomes exact in the limit ϵ↘0. As in the unconstrained parallel scenario, there is no known analytical expression for the expected feasible hypervolume improvement. Therefore, we again use MC integration to approximate the expectation: αqEHVI C (x) = E [ HVI C ({f(xi),c(xi)}q i=1) ] (12a) ≈ 1 N N∑ t=1 HVI C ({ft(xi),ct(xi)}q i=1) (12b) ≈ 1 N N∑ t=1 K∑ k=1 q∑ j=1 ∑ Xj∈Xj (−1)j+1 [( M∏ m=1 [ z(m) k,Xj,t −l(m) k ] + ) ∏ x′∈Xj V∏ v=1 s(c(v)(x′); ϵ) ] (12c) A.3.1 Inclusion Exclusion principle for HVI C Equation (10) holds when the indicator function because HVI C is equivalent to HVI with the subset of feasible points. However, the sigmoid approximation can result in non-zero error. The error function ε: 2Xcand →R can be expressed as ε(X) = ∏ x′∈X V∏ v=1 1 [c(x′) >0] − ∏ x′∈X V∏ v=1 s(c(x′),ϵ) The error function gives a value to each to each element of 2Xcand . Weight functions have been studied in conjunction with the inclusion-exclusion principle [56], but under the assumption of that the weight of a set is the sum of the weights of its elements: w(A) = ∑ a∈A w(a). In our case, the weight function of a set Ais the product the weights of its elements. There, it is not obvious whether the inclusion-exclusion principle will hold in this case. Theorem 1. Given a feasible Pareto frontPfeas, a partitioning {(lk,uk}K k=1 of the objective space RM that is not dominated by the Pfeas, then for a set of points Xcand with objective values f(Xcand) and constraint values c(Xcand), HVI C (f(Xcand),c(Xcand),P,r) = HVI(f′(Xcand),P′,r′) where f′(Xcand) is the set of objective-constraint vectors for each candidate point f′(x) ∈RM+V , P′is the set of vectors [f(1)(x),...,f (M)(x),0V ] ∈RM+V , and r′= [r(1),...,r (M),0V ] ∈RM+V . Proof. Recall equation 10, HVI C ({f(xi),c(xi)}q i=1) = K∑ k=1 q∑ j=1 ∑ Xj∈Xj (−1)j+1 [( M∏ m=1 [ z(m) k,Xj −l(m) k ] + ) ∏ x′∈Xj V∏ v=1 1 [c(v)(x′) ≥0] ] . 17Note that the constraint product ∏ x′∈Xj V∏ v=1 1 [c(v)(x′) ≥0] = V∏ v=1 ∏ x′∈Xj 1 [c(v)(x′) ≥0] = V∏ v=1 min x′∈Xj 1 [c(v)(x′) ≥0] = V∏ v=1 min [ 1, min x′∈Xj 1 [c(v)(x′) ≥0] ] = V∏ v=1 [ min [ 1, min x′∈Xj 1 [c(v)(x′) ≥0] ] −0 ] . (13) For v = 1 ,...,V , k = 1 ,...K, let l(M+v) k = 0 and u(M+v) k = 1 . Then, substituting into the following expression from Equation 13 gives min [ 1, min x′∈Xj 1 [c(v)(x′) ≥0] ] = min [ u(M+v) k , min x′∈Xj 1 [c(v)(x′) ≥0] ] Recall from Section 4, that zis deﬁned as: zk := min [ uk,f(x) ] . The high-level idea is that if we consider the indicator of the slack constraints 1 [c(v)(x′) ≥0] as objectives, then the above expression is consistent with the deﬁnition of zat the beginning of section 4. For v= 1,...,V , z(M+v) k,Xj = min [ 1, min x′∈Xj 1 [c(v)(x′) ≥0] ] Thus, ∏ x′∈Xj V∏ v=1 1 [c(v)(x′) ≥0] = V∏ v=1 [ min [ 1, min x′∈Xj 1 [c(v)(x′) ≥0] ] −0 ] = V∏ v=1 [ z(M+v) k,Xj −l(M+v) k ] + Returning to the HVI C equation, we have HVI C ({f(xi),c(xi)}q i=1) = K∑ k=1 q∑ j=1 ∑ Xj∈Xj (−1)j+1 [( M∏ m=1 [ z(m) k,Xj −l(m) k ] + ) ∏ x′∈Xj V∏ v=1 1 [c(v)(x′) ≥0] ] = K∑ k=1 q∑ j=1 ∑ Xj∈Xj (−1)j+1 [( M∏ m=1 [ z(m) k,Xj −l(m) k ] + ) M+V∏ v=M+1 [ z(v) k,Xj −l(M+v) k ] + ] = K∑ k=1 q∑ j=1 ∑ Xj∈Xj (−1)j+1 [M+V∏ m=1 [ z(m) k,Xj −l(m) k ] + ] (14) Now consider the case when a sigmoid approximation 1 [c(v)(x′) ≥0] ≈s(c(v)(x′); ϵ) is used. The only change to Equation 14 is that z(m) k,Xj ≈ˆz(m) k,Xj = min [ u(M+v) k , min x′∈Xj S[c(v)(x′),ϵ] ] . If S[c(v)(x′),ϵ] = 1 [c(v)(x′) ≥0] for all v,x′, then HVI is computed exactly without approximation error. If S[c(v)(x′),ϵ]1 [c(v)(x′) ≥0] for any v,x′, then there is approximation error: the hypervolume improvement from all subsets containing x′is proportional to ∏V v=1 minx′∈X s(c(x′),ϵ). Since the constraint outcomes are directly considered as components in the hypervolume computation, the inclusion-exclusion principle incorporates the approximate indicator properly. 18A.4 Complexity Recall from Section 3.3 that, given posterior samples, the time complexity on a single-threaded machine is T1 = O(MNK(2q −1)). The space complexity required for maximum parallelism is also is T1 (ignoring the space required by the models), which does limit scalability to larger M and q, but difﬁculty scaling to large M is a known limitaiton of EHVI [69]. To reduce memory load, rectangles could be materialized and processed in chunks at the cost of additional runtime. In addition, our implementation of qEHVI uses the box decomposition algorithm from Couckuyt et al. [11], but we emphasize qEHVI is agnostic to the choice of partitioning algorithm and using a more efﬁcient partitioning algorithm (e.g. [69, 17, 41]) may signiﬁcantly improve memory footprint on GPU and enable larger using qin many scenarios. B Error Bound on Sequential Greedy Approximation If the acquisition function L(Xcand) is a normalized, monotone, submodular set function (where submodular means that the increase in L(Xcand) is non-increasing as elements are added to Xcand and normalized means that L(∅) = 0), then the sequential greedy approximation of Lenjoys regret of no more than 1 e L∗, where L∗is the optima of L[23]. We have αqEHVI (Xcand) = L(Xcand) = Ef ( HVI [ f(Xcand) ]) . Since HVI is a submodular set function [24] and the expectation of a stochastic submodular function is also submodular [ 2], αqEHVI (Xcand) is also submodular and therefore its sequential greedy approximation enjoys regret of no more than 1 e α∗ qEHVI . Using the result from Wilson et al. [65], the MC-based approximation ˆαqEHVI (Xcand) = ∑N t=1 HVI [ ft(Xcand) ] also enjoys the same regret bound since HVI is a normalized submodular set function.7 C Convergence Results For the purpose of stating our convergence results, we recall some concepts and notation from Balandat et al. [5]. First, consider a sample {ft(x1)}q i=1 from the multi-output posterior of the GP surrogate model. Let x∈Rqd be the stacked set of candidates Xcand and let ft(x) := [ft(x1)T ,...,f t(xq)T ]T be the stacked set of corresponding objective vectors. It is well known that, using the reparameterization trick, we can write ft(x) = µ(x) + L(x)ϵt, (15) where µ: Rqd →RqM is the mean function of the multi-output GP, L(x) ∈RqM×qM is a root decomposition (typically the Cholesky decomposition) of the multi-output GP’s posterior covarianceΣ(x) ∈RqM×qM , and ϵt ∈RqM with ϵt ∼N(0,IqM ). For x∈X , consider the MC-approximation ˆαN qEHVI (x) from (5). Denote by ∇x ˆαN qEHVI (x) the gradient of ˆαN qEHVI (x), obtained by averaging the gradients on the sample-level: ∇x ˆαN qEHVI (x) := 1 N N∑ t=1 ∇xHVI({ft(xi)}q i=1) (16) Let α∗ qEHVI := max x∈XαqEHVI (x) denote the maximum of the true acquisition function qEHVI , and let X∗:= arg maxx∈XαqEHVI (x) denote the set of associated maximizers. Theorem 2. Suppose that Xis compact and thatfhas a Multi-Output Gaussian Process prior with continuously differentiable mean and covariance functions. If the base samples {ϵt}N t=1 are drawn i.i.d. from N(0,IqM ), and if ˆx∗ N ∈arg maxx∈X ˆαN qEHVI (x), then (1) αqEHVI (ˆx∗ N ) →α∗ qEHVI a.s. (2) dist(ˆx∗ N ,X∗) →0 a.s. In addition to the almost sure convergence in Theorem 2, deriving a result on the convergence rate of the optimizer, similar to the one obtained in [5], should be possible. We leave this to future work. Moreover, the results in Theorem 2 can also be extended to the situation in which the base samples are generated using a particular class of randomized QMC methods (see similar results in [5]). Proof. We consider the setting from Balandat et al. [5, Section D.5]. Let ϵ ∼N(0,IqM ), so that we can write the posterior over outcome mat xas the random variable f(m)(x,ϵ) = S{ij,m}(µ(x) + L(x)ϵ), where µ(x) 7As noted in Wilson et al. [65], submodularity technically requires the search space Xto be ﬁnite, whereas in BO, it will typically be inﬁnite. Wilson et al. [65] note that in similar scenarios, submodularity has been extended to inﬁnite sets X(e.g. Srinivas et al. [58]). 19and L(x) are the (vector-valued) posterior mean and the Cholesky factor of posterior covariance, respectively, and S{ij,m}is an appropriate selection matrix (in particular, ∥S{ij,m}∥∞≤1 for all ij and m). Let A(x,ϵ) = K∑ k=1 q∑ j=1 ∑ Xj∈Xj (−1)j+1 M∏ m=1 [ z(m) k,Xj (ϵ) −l(m) k ] + where z(m) k,Xj (ϵ) = min [ u(m) k ,f(m)(xi1 ,ϵ),...,f (m)(xij ,ϵ) ] and Xj = {xi1 ,..., xij }. Following [5, Theorem 3], we need to show that there exists an integrable function ℓ: Rq×M ↦→R such that for almost every ϵand all x,y⊆X,x,y∈Rq×d, |A(x,ϵ) −A(y,ϵ)|≤ ℓ(ϵ)∥x−y∥. (17) Let us deﬁne ˜akmjXj (x,ϵ) := [ min [ u(m) k ,f(m)(xi1 ,ϵ),...,f (m)(xij ,ϵ) ] −l(m) k ] + . Linearity implies that it sufﬁces to show that this condition holds for ˜A(x,ϵ) := M∏ m=1 ˜akmjXj (x,ϵ) = M∏ m=1 [ min [ u(m) k ,f(m)(xi1 ,ϵ),...,f (m)(xij ,ϵ) ] −l(m) k ] + (18) for all k, j, and Xj. Observe that ˜akmjXj (x,ϵ) ≤ ⏐⏐⏐min [ u(m) k ,f(m)(xi1 ,ϵ),...,f (m)(xij ,ϵ) ] −l(m) k ⏐⏐⏐ ≤|l(m) k |+ ⏐⏐⏐min [ u(m) k ,f(m)(xi1 ,ϵ),...,f (m)(xij ,ϵ) ]⏐⏐⏐. Note that if u(m) k = ∞, then min[u(m) k ,f(x,ϵ)(m) i1 ,...f(m)(xij ,ϵ)] = min[f(m)(xi1 ,ϵ),...f(m)(xij ,ϵ)]. If u(m) k < ∞, then min[u(m) k ,f(m)(xi1 ,ϵ),...f(m)(xij ,ϵ)] < ⏐⏐min[f(m)(xi1 ,ϵ),...f(m)(xij ,ϵ)] ⏐⏐+ ⏐⏐u(m) k ⏐⏐. Let w(m) k = u(m) k if u(m) k <∞and 0 otherwise. Then ˜akmjXj (x,ϵ) ≤|l(m) k |+ |w(m) k |+ ⏐⏐min [ f(m)(xi1 ,ϵ),...,f (m)(xij ,ϵ) ]⏐⏐ ≤|l(m) k |+ |w(m) k |+ ∑ i1,...,ij ⏐⏐f(m)(xij ,ϵ) ⏐⏐. We therefore have that |˜akmjXj (x,ϵ)|≤| l(m) k |+ |w(m) k |+ |Xj| ( ∥µ(m)(x)∥+ ∥L(m)(x)∥∥ϵ∥ ) for all k,m,j,X j, where |Xj|denotes the cardinality of the set Xj. Under our assumptions (compactness of X, continuous differentiability of mean and covariance function), both µ(x) and L(x), as well as their respective gradients w.r.t. x, are uniformly bounded. In particular there exist C1,C2 <∞such that |˜akmjXj (x,ϵ)|≤ C1 + C2∥ϵ∥ for all k,m,j,X j. Dropping indices k,j,X j for simplicity, observe that ⏐⏐˜A(x,ϵ) −˜A(y,ϵ) ⏐⏐= ⏐⏐˜a1(x,ϵ)˜a2(x,ϵ) −˜a1(y,ϵ)˜a2(y,ϵ) ⏐⏐ (19a) = ⏐⏐˜a1(x,ϵ) ( ˜a2(x,ϵ) −˜a2(y,ϵ) ) + ˜a2(y,ϵ) ( ˜a1(x,ϵ) −˜a1(y,ϵ) )⏐⏐ (19b) ≤|˜a1(x,ϵ)| ⏐⏐˜a2(x,ϵ) −˜a2(y,ϵ) ⏐⏐+ |˜a2(y,ϵ)| ⏐⏐˜a1(x,ϵ) −˜a1(y,ϵ) ⏐⏐. (19c) Furthermore, |˜akmjXj (x,ϵ) −˜akmjXj (y,ϵ)|≤ ∑ i1,...,ij ⏐⏐S{ij,m}(µ(x) + L(x)ϵ) −S{ij,m}(µ(y) + L(y)ϵ) ⏐⏐ ≤|Xj| ( ∥µ(x) −µ(y)∥+ ∥L(x) −L(y)∥∥ϵ∥ ) . Since µand Lhave uniformly bounded gradients, they are Lipschitz. Therefore, there exist C3,C4 <∞such that |˜akmjXj (x,ϵ) −˜akmjXj (y,ϵ)|≤ (C3 + C4∥ϵ∥)∥x−y∥ 20for all x,y,k,m,j,X j. Plugging this into (19) above, we ﬁnd that ⏐⏐˜A(x,ϵ) −˜A(y,ϵ) ⏐⏐≤2 ( C1C3 + (C1C4 + C2C3)∥ϵ∥+ C2C4∥ϵ∥2 ) ∥x−y∥ for all x,yand ϵ. For M > 2 we generalize the idea from (19), making sure to telescope the respective expressions. It is not hard to see that with this, there exist C <∞such that ⏐⏐˜A(x,ϵ) −˜A(y,ϵ) ⏐⏐≤C M∑ m=1 ∥ϵ∥m∥x−y∥ Letting ℓ(ϵ) := C∑M m=1 ∥ϵ∥m, we observe that ℓ(ϵ) is integrable (since all absolute moments exist for the Normal distribution). The result now follows from in Balandat et al. [5, Theorem 3]. Besides the above convergence result, we can also show that the sample average gradient of the MC approximation of qEHVI is an unbiased estimator of the true gradient of qEHVI: Proposition 1. Suppose that the GP mean and covariance function are continuously differentiable. Suppose further that the candidate set xhas no duplicates, and that the sample-level gradients ∇xHVI({ft(xi)}q i=1) are obtained using the reparameterization trick as in [5]. Then E [ ∇x ˆαN qEHVI (x) ] = ∇xαqEHVI (x), (20) that is, the averaged sample-level gradient is an unbiased estimate of the gradient of the true acquisition function. Proof. This proof follows the arguments Wang et al.[63, Theorem 1], which leverages Glasserman[31, Theorem 1]. We verify the conditions of Glasserman [31, Theorem 1] below. Using the arguments from [5], we know that, under the assumption of differentiable mean and covariance functions, the samples ft(x) are continuously differentiable w.r.t. x(since there are no duplicates, and thus the covariance Σ(x) is non-singular). Hence, Glasserman [31, A1] is satisﬁed. Furthermore, it is easy to see from(1) that HVI({f(xi)}q i=1) is a.s. continuous and is differentiable w.r.t. ft(x) on RM , except on the edges of the hyper-rectangle decomposition {Sk}K k=1 of the non-dominated space, which satisﬁes [31, A3]. The set of points deﬁned by the union of these edges clearly has measure zero under any non-degenerate (non-singular covariance) GP posterior on RM , so Glasserman [31, A4] holds. Therefore Glasserman [31, Lemma 2] holds, so HVI({f(xi)}q i=1) is a.s. piece-wise differentiable w.r.t. x. Lastly, we need to show that the result in Glasserman [31, Lemma 3] holds: E [ sup xci /∈˜D |A′(x,ϵ)| ] <∞. As in Wang et al.[63, Theorem 1], we ﬁx xexcept for xci where xci is the cth component of the ith point, We need to show that E [ supxci /∈˜D |A′(x,ϵ)| ] <∞. By linearity, it sufﬁces to show that E [ supxci /∈˜D |˜A′(x,ϵ)| ] <∞. We have E [ sup xci /∈˜D |˜A′(x,ϵ)| ] = E [ sup xci /∈˜D ⏐⏐⏐⏐ ∂˜A(x,ϵ) ∂xci ⏐⏐⏐⏐ ] . Consider the M = 2 case. We have ˜A(x,ϵ) = a1(x,ϵ)a2(x,ϵ), where am(x,ϵ) = [ min [ u(m) k ,f(m)(xi1 ,ϵ),...,f (m)(xij ,ϵ) ] −l(m) k ] + . The partial derivative of ˜A(x,ϵ) with respect to xci is ∂˜A(x,ϵ) ∂xci = ∂a1(x,ϵ) ∂xci a2(x,ϵ) + a1(x,ϵ)∂a2(x,ϵ) ∂xci , and therefore ⏐⏐⏐∂˜A(x,ϵ) ∂xci ⏐⏐⏐≤ ⏐⏐⏐∂a1(x,ϵ) ∂xci ⏐⏐⏐· ⏐⏐⏐a2(x,ϵ) ⏐⏐⏐+ ⏐⏐⏐a1(x,ϵ) ⏐⏐⏐· ⏐⏐⏐∂a2(x,ϵ) ∂xci ⏐⏐⏐ Since we are only concerned with xci /∈ ˜D, am(x,ϵ) = [ min [ f(m)(xi1 ,ϵ),...,f (m)(xij ,ϵ) ] −l(1) k ] + . 21As in the proof of Theorem 2, we write the posterior over outcome mat xas the random variable f(m)(x,ϵ) = S{ij,m}(µ(x) + L(x)ϵ), where ϵ ∼N(0,IqM ) and S{ij,m}is an appropriate selection matrix. With this, am(x,ϵ) = [ min [ S{i1,1} ( µ(x) + L(x)ϵ ) ,...,S {ij,1} ( µ(x) + L(x)ϵ )] −l(1) k ] + . Since the interval X is compact and the mean, covariance, and Cholesky factor of the covariance µ(x),C(x),L(x) are continuously differentiable, for all mwe have sup xci ⏐⏐⏐⏐ ∂µ(m)(xa) ∂xci ⏐⏐⏐⏐= µ∗,(m) a <∞, sup xci ⏐⏐⏐⏐ ∂L(m)(x) ∂xci ⏐⏐⏐⏐= L∗,(m) ca <∞. Let µ(m) ∗∗ = maxa µ∗,(m) a , L(m) ∗∗ = maxa,b L∗,(m) ab (x), where L(m) ab is the element at row a, column bin L(m), the Cholesky factor for outcome m. Let ϵ(m) ∈Rq denote the vector of i.i.d. N(0,1) samples corresponding to outcome m. Then we have⏐⏐⏐⏐ ∂ ∂xci [ [min [ S{i1,1} ( µ(x) + L(x)ϵ ) ,...,S {ij,1} ( µ(x) + L(x)ϵ )] −l(1) k ] + ⏐⏐⏐⏐ ≤ ⏐⏐⏐ [ µ(m) ∗∗ + L(m) ∗∗ ||ϵ(m)||1 −l(m) k ] + ⏐⏐⏐ ≤ ⏐⏐⏐µ(m) ∗∗ + L(m) ∗∗ ||ϵ(m)||1 ⏐⏐⏐+ ⏐⏐⏐l(m) k ⏐⏐⏐. Under our assumptions (compactness of X, continuous differentiability of mean and covariance function) both µ(x) and L(x), as well as their respective gradients, are uniformly bounded. In particular there exist C(m) 1 ,C(m) 2 <∞such that ⏐⏐S{a,m} ( µ(x) + L(x)ϵ ) −l(m) k ⏐⏐≤C(m) 1 + C(m) 2 ||ϵ(m)||1 for all a= i1,...,i j. Hence, ⏐⏐⏐⏐ ∂˜A(x,ϵ) ∂xci ⏐⏐⏐⏐≤ [⏐⏐⏐µ(1) ∗∗ + C(1) ∗∗||ϵ(1)||1 ⏐⏐⏐+ ⏐⏐⏐l(1) k ⏐⏐⏐ ][ C(2) 1 + C(2) 2 ||ϵ(2)||1 ] + [ C(1) 1 + C(1) 2 ||ϵ(1)||1 ][⏐⏐⏐µ(2) ∗∗ + C(2) ∗∗||ϵ(2)||1 ⏐⏐⏐+ ⏐⏐⏐l(2) k ⏐⏐⏐ ] Since ϵis absolutely integrable, E (⏐⏐⏐⏐ ∂˜A(x,ϵ) ∂xci ⏐⏐⏐⏐ ) <∞. Hence, E [ supxci /∈˜D |A′(x,ϵ)| ] <∞. This can be extended to M >2 in the same manner using the product rule to obtain E (∂˜A(x,ϵ) ∂xci ) ≤ M∑ m=1 ([⏐⏐⏐µ(m) ∗∗ + C(m) ∗∗ E[||ϵ(m)||1] ⏐⏐⏐+ ⏐⏐⏐l(1) k ⏐⏐⏐ ] M∏ n=1,n̸=m [ C(n) 1 + C(n) 2 E[||ϵ(n)||1] ]) ≤ M∑ m=1 ([⏐⏐⏐µ(m) ∗∗ + π 2 qC(m) ∗∗ ⏐⏐⏐+ ⏐⏐⏐l(1) k ⏐⏐⏐ ] M∏ n=1,n̸=m [ C(n) 1 + π 2 qC(n) 2 ] ]) . Hence, E [ supxci /∈˜D |A′(x,ϵ)| ] <∞for M ≥2 and Glasserman [31, Theorem 1] holds. 22D Monte-Carlo Approximation Figure 5b shows the gradient of analytic EHVI and the MC estimator qEHVI on slice of a 3-objective problem. Even using only N = 32 QMC samples, the average sample gradient has very low variance. Moreover, ﬁxing the base samples also greatly reduces the variance without introducing bias. 0.0 0.1EHVI MC,/uni00A0N=32 analytic qMC,/uni00A0N=32 analytic 0.00 0.25 0.50 0.75 1.00 0.0 0.1EHVI MC,/uni00A0N=32/uni00A0(fixed) analytic 0.00 0.25 0.50 0.75 1.00 qMC,/uni00A0N=32/uni00A0(fixed) analytic (a) A comparison of the analytic EHVI acquisition function and the MC-based qEHVI for q= 1. 0.1 0.0 0.1 /uni00A0EHVI MC,/uni00A0N=32 analytic qMC,/uni00A0N=32 analytic 0.00 0.25 0.50 0.75 1.00 0.1 0.0 0.1 /uni00A0EHVI MC,/uni00A0N=32/uni00A0(fixed) analytic 0.00 0.25 0.50 0.75 1.00 qMC,/uni00A0N=32/uni00A0(fixed) analytic (b) A comparison of the exact gradient of analytic EHVI and the exact sample average gradient of the MC-based qEHVI for q= 1. Figure 5: A comparison of (a) the analytic EHVI and MC-based qEHVI for q = 1 and (b) a comparison of the exact gradient ∇αEHVI of analytic EHVI and average sample gradient of the MC-estimator ∇ˆαqEHVI over a slice of the input space on a DTLZ2 problem (q= 1, M = 3, d= 6) [15]. x(0) is varied across 0 ≤λ≤1, while x(i) for 1,...D are held constant. In each of (a) and (b), the top row show qEHVI where the (quasi-)standard normal base samples are resampled for each value of x(0). The solid line is one sample average (across (q)MC samples) and the shaded area is the mean plus 2 standard errors across 50 repetitions. The bottom row uses the same base samples for evaluating each test point and the sample average for each of 50 repetitions is plotted. E Experiment Details E.1 Algorithms For TS-TCH, we draw a sample from the joint posterior over a discrete set of 1000dpoints sampled from a scrambled Sobol sequence. For PESMO, we follow [27] and use a Pareto set of size 10 for each sampled GP, which is optimized over a discrete set of 1000dpoints sampled from a scrambled Sobol sequence. The current 23Table 2: Reference points for all benchmark problems. Assuming minimization. In our benchmarks, equivalently maximize the negative objectives and multiply the reference points by -1. PROBLEM REFERENCE POINT BRANIN CURRIN (18.0, 6.0) DTLZ2 (1.1,..., 1.1) ∈RM ABR (-150.0, 3500.0, 5.1) VEHICLE CRASH SAFETY (1864.72022, 11.81993945, 0.2903999384) CONSTRAINED BRANIN CURRIN (90.0, 10.0) C2-DTLZ2 (1.1,..., 1.1) ∈RM Pareto front is approximated by optimizing the posterior means over a grid as is done in Garrido-Merchán and Hernández-Lobato [26, 27]. For SMS-EGO, we use the observed Pareto front. All acquisition functions are optimized with L-BFGS-B (with a maximum of 200 iterations); SMS-EGO [53] and PESMO [26] use gradients approximated by ﬁnite differences and all other methods use exact gradients. For all methods, each outcome is modeled with an independent Gaussian process with a Matern 5/2 ARD kernel. The methods implemented in Spearmint use a fully Bayesian treatment of the hyperparameters with 10 samples from posterior over the hyperparamters, and the methods implemented in BoTorch use maximum a posteriori estimates of the GP hyperparameters. All methods are initialized with 2(d+ 1)points from a scrambled Sobol sequence. qPAREGO and qEHVI use N = 128 QMC samples. E.1.1 Reference point speciﬁcation There is a large body of literature on the effects of reference point speciﬁcation [4, 35, 36]. The hypervolume indicator is sensitive to speciﬁed the reference point: a reference point that is far away from the Pareto front will favor extreme points, where as reference point that is close to the Pareto front gives more weight to less extreme points [36]. Sensitivity to the reference point is affects both the evaluation of different MO methods and the utility function for methods that rely HV. In practice, a decision maker may be able to specify a reference point that satisﬁes their preference with domain knowledge. If a reference point is provided by the decision maker, previous work has suggested heuristics for choosing reference points for use in an algorithm’s utility function [35, 53]. We follow previous work [69, 68] and assume that the reference point is known. We also considered (but did not use in our experiments) a dynamic reference point strategy where at each BO iteration, the reference point is selected to be a point slightly worse than the nadir (component-wise minimum) point of the current observed Pareto front for computing the acquisition function: r = ynadir −0.1 ·|ynadir| where ynadir = ( miny(1)∈D(1) y(1),..., miny(m)∈D(m) y(m)) . This reference point is used in SMS-EMOA in Ishibuchi et al. [35]), and we ﬁnd similar average performance (but higher variance) on problems to using a known reference point with continuous Pareto fronts. If the Pareto front is discontinuous, then it is possible not all sections of the Pareto front will be reached. E.1.2 qPAREGO Previous work has only considered unconstrained sequential optimization with ParEGO [40, 7] and ParEGO is often optimized with gradient-free methods [ 53]. To the best of our knowledge, qPAREGO is the ﬁrst to support parallel and constrained optimization. Moreover, we compute exact gradients via auto-differentiation for acquisition optimization. ParEGO is typically implemented by applying augmented Chebyshev scalarization and modeling the scalarized outcome [40]. However, recent work has shown that composite objectives offer improved optimization performance [3]. qPAREGO uses a MC-based Expected Improvement [38] acquisition function, where the objectives are modeled independently and the augmented Chebyshev scalarization [40] is applied to the posterior samples as a composite objective. This approach enables the use of sequential greedy optimization of qcandidates with proper integration over the posterior at the pending points. Importantly, the sequential greedy approach allows for using different random scalarization weights for selecting each of the q candidates. qPAREGO is extended to the constrained setting by weighting the EI by the probability of feasibility [25]. We estimate the probability of feasiblity using the posterior samples and approximate the indicator function with a sigmoid to maintain differentiablity as in constrained qEHVI . qPAREGO is trivially extended to the noisy setting using Noisy Expected Improvement [43, 5], but we use Expected Improvement in our experiments as all of the problems are noiseless. E.2 Benchmark Problems The details for the benchmark problems below assume minimization of all objectives. Table 2 provides the reference points used for all benchmark problems. 24Branin-Currin f(1)(x′ 1,x′ 2) = (x2 −5.1 4π2 x2 1 + 5 πx1 −r)2 + 10(1 − 1 8π) cos(x1) + 10 f(2)(x1,x2) = [ 1 −exp ( − 1 (2x2) )]2300x3 1 + 1900x2 1 + 2092x1 + 60 100x3 1 + 500x2 1 + 4x1 + 20 where x1,x2 ∈[0,1], x′ 1 = 15x1 −5, and x′ 2 = 15x2. The constrained Branin-Currin problem uses the following disk constraint from [29]: c(x′ 1,x′ 2) = 50 −(x′ 1 −2.5)2 −(x′ 2 −7.5)2) ≥0 DTLZ2 The objectives are given by [15]: f1(x) = (1 + g(xM )) cos (π 2 x1 ) ··· cos (π 2 xM−2 ) cos (π 2 xM−1 ) f2(x) = (1 + g(xM )) cos (π 2 x1 ) ··· cos (π 2 xM−2 ) sin (π 2 xM−1 ) f3(x) = (1 + g(xM )) cos (π 2 x1 ) ··· sin (π 2 xM−2 ) ... fM (x) = (1 + g(xM )) sin (π 2 x1 ) where g(x) = ∑ xi∈xM (xi −0.5)2,x∈[0,1]d,and xM represents the last d−M + 1 elements of x. The C2-DTLZ2 problem adds the following constraint [16]: c(x) = −min [ M min i=1 ( (fi(x) −1)2 + M∑ j=1,j=i (f2 j −r2) ) , ( M∑ i=1 ( (fi(x) − 1√ M )2 −r2))] ≥0 Vehicle Crash Safety The objectives are given by [60]: f1(x) = 1640.2823 + 2.3573285x1 + 2.3220035x2 + 4.5688768x3 + 7.7213633x4 + 4.4559504x5 f2(x) = 6.5856 + 1.15x1 −1.0427x2 + 0.9738x3 + 0.8364x4 −0.3695x1x4 + 0.0861x1x5 + 0.3628x2x4 + 0.1106x2 1 −0.3437x2 3 + 0.1764x2 4 f3(x) = −0.0551 + 0.0181x1 + 0.1024x2 + 0.0421x3 −0.0073x1x2 + 0.024x2x3 −0.0118x2x4 −0.0204x3x4 −0.008x3x5 −0.0241x2 2 + 0.0109x2 4 where x∈[1,3]5. Policy Optimization for Adaptive Bitrate Control The controller is given by: at = x0 ˆzbd,t + x2zbf,t + x3, where ˆzbd,t = ∑ ti<t zbd,ti exp(−x1ti) ∑ ti<t exp(−x1ti) is estimated bandwidth at time tusing an exponential moving average, zbf,t is the buffer occupancy at time t, and x0,...x3 are the parameters we seek to optimize. We evaluate each policy on a set of 400 videos, where the number of time steps (chunks) in each video stream trajectory depends on the size of the video. 25Table 3: Acquisition Optimization wall time in seconds on a CPU (2x Intel Xeon E5-2680 v4 @ 2.40GHz) and on a GPU (Tesla V100-SXM2-16GB). The mean and two standard errors are reported. NA indicates that the algorithm does not support constraints. CPU CONSTRAINED BRANIN CURRIN DTLZ2 PESMO ( q=1) NA 278.53 (±25.66) SMS-EGO ( q=1) NA 104.26 (±7.66) TS-TCH ( q=1) NA 52.55 (±0.06) qPAREGO ( q=1) 2.4 (±0.37) 4 .68 (±0.46) EHVI ( q=1) NA 3.58 (±0.28) qEHVI ( q=1) 5.69 (±0.43) 5 .95 (±0.45) GPU CONSTRAINED BRANIN CURRIN DTLZ2 TS-TCH ( q=1) NA 0.25 (±0.00) TS-TCH ( q=2) NA 0.27 (±0.00) TS-TCH ( q=4) NA 0.28 (±0.00) TS-TCH ( q=8) NA 0.32 (±0.01) qPAREGO ( q=1) 3.52 (±0.34) 9 .04 (±0.93) qPAREGO ( q=2) 6.0 (±0.56) 14 .23 (±1.55) qPAREGO ( q=4) 12.07 (±0.98) 40 .5 (±3.21) qPAREGO ( q=8) 33.1 (±3.32) 84 .15 (±6.9) EHVI ( q=1) NA 84.15 (±6.9) qEHVI ( q=1) 5.61 (±0.17) 10 .21 (±0.58) qEHVI ( q=2) 19.06 (±5.88) 17 .75 (±0.97) qEHVI ( q=4) 29.26 (±2.01) 40 .41 (±2.78) qEHVI ( q=8) 91.56 (±5.51) 106 .51 (±7.69) F Additional Empirical Results F.1 Additional Sequential Optimization Results We include results for an additional synthetic benchmark: the DTLZ2 problem from the MO literature [ 15] (d= 6,M = 2). Figure 6 shows that qEHVI outperforms all other baseline algorithms on the DTLZ2 in terms of sequential optimization performance with competitive wall times as shown in 3. 0 20 40 60 80 100 Function/uni00A0Evaluations 1.8 1.6 1.4 1.2 1.0 0.8 0.6 0.4 log/uni00A0HV/uni00A0DifferenceSobol EHVI qEHVI qParEGO TS/uni00ADTCH PESMO SMS/uni00ADEGO Figure 6: Optimization performance on the DTLZ2 synthetic function (d= 6,M = 2). F.2 Performance with Increasing Parallelism Figure 7 shows that that the performance of qEHVI performance does not degrade substantially, whereas performance does degrade for qPAREGO and TS-TCH on some benchmark problems. We include results for all problems in Section 5 and Appendix F.1 as well as a Constrained Branin-Currin problem (which is described in Appendix E.2). 260 20 40 60 80 100 Batch/uni00A0Iteration 0.0 0.5 1.0 1.5 2.0log/uni00A0HV/uni00A0Difference Sobol/uni00A0q=8 TS/uni00A0q=1 TS/uni00A0q=2 TS/uni00A0q=4 TS/uni00A0q=8 qEHVI/uni00A0q=1 qEHVI/uni00A0q=2 qEHVI/uni00A0q=4 qEHVI/uni00A0q=8 qParEGO/uni00A0q=1 qParEGO/uni00A0q=2 qParEGO/uni00A0q=4 qParEGO/uni00A0q=8 (a) VEHICLE SAFETY 0 20 40 60 80 100 Function/uni00A0Evaluations 0.0 0.5 1.0 1.5 2.0log/uni00A0HV/uni00A0Difference Sobol/uni00A0q=8 TS/uni00A0q=1 TS/uni00A0q=2 TS/uni00A0q=4 TS/uni00A0q=8 qEHVI/uni00A0q=1 qEHVI/uni00A0q=2 qEHVI/uni00A0q=4 qEHVI/uni00A0q=8 qParEGO/uni00A0q=1 qParEGO/uni00A0q=2 qParEGO/uni00A0q=4 qParEGO/uni00A0q=8 (b) VEHICLE SAFETY 0 20 40 60 80 100 Batch/uni00A0Iteration 1.1 1.0 0.9 0.8 0.7 0.6 0.5 0.4 log/uni00A0HV/uni00A0Difference Sobol/uni00A0q=8 qEHVI/uni00A0q=1 qEHVI/uni00A0q=2 qEHVI/uni00A0q=4 qEHVI/uni00A0q=8 qParEGO/uni00A0q=1 qParEGO/uni00A0q=2 qParEGO/uni00A0q=4 qParEGO/uni00A0q=8 (c) C2DTLZ2 0 20 40 60 80 100 Function/uni00A0Evaluations 1.1 1.0 0.9 0.8 0.7 0.6 0.5 0.4 log/uni00A0HV/uni00A0Difference Sobol/uni00A0q=8 qEHVI/uni00A0q=1 qEHVI/uni00A0q=2 qEHVI/uni00A0q=4 qEHVI/uni00A0q=8 qParEGO/uni00A0q=1 qParEGO/uni00A0q=2 qParEGO/uni00A0q=4 qParEGO/uni00A0q=8 (d) C2DTLZ2 0 20 40 60 80 100 Batch/uni00A0Iteration 0.0 0.5 1.0 1.5log/uni00A0HV/uni00A0Difference Sobol/uni00A0q=8 TS/uni00A0q=1 TS/uni00A0q=2 TS/uni00A0q=4 TS/uni00A0q=8 qEHVI/uni00A0q=1 qEHVI/uni00A0q=2 qEHVI/uni00A0q=4 qEHVI/uni00A0q=8 qParEGO/uni00A0q=1 qParEGO/uni00A0q=2 qParEGO/uni00A0q=4 qParEGO/uni00A0q=8 (e) BRANIN CURRIN 0 20 40 60 80 100 Function/uni00A0Evaluations 0.0 0.5 1.0 1.5log/uni00A0HV/uni00A0Difference Sobol/uni00A0q=8 TS/uni00A0q=1 TS/uni00A0q=2 TS/uni00A0q=4 TS/uni00A0q=8 qEHVI/uni00A0q=1 qEHVI/uni00A0q=2 qEHVI/uni00A0q=4 qEHVI/uni00A0q=8 qParEGO/uni00A0q=1 qParEGO/uni00A0q=2 qParEGO/uni00A0q=4 qParEGO/uni00A0q=8 (f) BRANIN CURRIN Figure 7: Optimization performance of parallel acquisition functions over batch BO iterations (left) and function evaluations (right) for benchmark problems in Section 5. 270 20 40 60 80 100 Batch/uni00A0Iteration 0.5 1.0 1.5 2.0 2.5log/uni00A0HV/uni00A0Difference Sobol/uni00A0q=8 qEHVI/uni00A0q=1 qEHVI/uni00A0q=2 qEHVI/uni00A0q=4 qEHVI/uni00A0q=8 qParEGO/uni00A0q=1 qParEGO/uni00A0q=2 qParEGO/uni00A0q=4 qParEGO/uni00A0q=8 (a) CONSTRAINED BRANIN CURRIN 0 20 40 60 80 100 Function/uni00A0Evaluations 0.5 1.0 1.5 2.0 2.5log/uni00A0HV/uni00A0Difference Sobol/uni00A0q=8 qEHVI/uni00A0q=1 qEHVI/uni00A0q=2 qEHVI/uni00A0q=4 qEHVI/uni00A0q=8 qParEGO/uni00A0q=1 qParEGO/uni00A0q=2 qParEGO/uni00A0q=4 qParEGO/uni00A0q=8 (b) CONSTRAINED BRANIN CURRIN 0 20 40 60 80 100 Batch/uni00A0Iteration 1.8 1.6 1.4 1.2 1.0 0.8 0.6 0.4 log/uni00A0HV/uni00A0Difference Sobol/uni00A0q=8 TS/uni00A0q=1 TS/uni00A0q=2 TS/uni00A0q=4 TS/uni00A0q=8 qEHVI/uni00A0q=1 qEHVI/uni00A0q=2 qEHVI/uni00A0q=4 qEHVI/uni00A0q=8 qParEGO/uni00A0q=1 qParEGO/uni00A0q=2 qParEGO/uni00A0q=4 qParEGO/uni00A0q=8 (c) DTLZ2 ( M = 2,d = 6) 0 20 40 60 80 100 Function/uni00A0Evaluations 1.8 1.6 1.4 1.2 1.0 0.8 0.6 0.4 log/uni00A0HV/uni00A0Difference Sobol/uni00A0q=8 TS/uni00A0q=1 TS/uni00A0q=2 TS/uni00A0q=4 TS/uni00A0q=8 qEHVI/uni00A0q=1 qEHVI/uni00A0q=2 qEHVI/uni00A0q=4 qEHVI/uni00A0q=8 qParEGO/uni00A0q=1 qParEGO/uni00A0q=2 qParEGO/uni00A0q=4 qParEGO/uni00A0q=8 (d) DTLZ2 ( M = 2,d = 6) Figure 8: Optimization performance of parallel acquisition functions over batch BO iterations (left) and function evaluations (right) for additional benchmark problems. 28F.3 Noisy Observations Although neither qEHVI nor any variant of expected hypervolume improvement (to our knowledge) directly account for noisy observations, noisy observations are a practical challenge. We empirically evaluate the performance of all algorithms on a Branin-Currin function where observations have additive, zero-mean, iid Gaussian noise; the unknown standard deviation of the noise is set to be 1% of the range of each objective. Fig 9 shows that qEHVI performs favorably in the presence of noise, besting all algorithms including Noisy qPAREGO (qNParego) (described in Appendix E.1.2), PESMO and TS-TCH, all of which account for noise. 0 20 40 60 80 100 Function/uni00A0Evaluations 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8log/uni00A0HV/uni00A0Difference Sobol EHVI qEHVI qParEGO qNParEGO TS/uni00ADTCH Figure 9: Sequential optimization performance on a noisy Branin-Currin problem. F.4 Approximate Box Decompositions EHVI becomes prohibitively computationally expensive in many scenarios with ≥4 objectives because of the wall time of partitioning the non-dominated space into disjoint rectangles [ 11]. Therefore, in addition to providing an exact binary partitioning algorithm, Couckuyt et al. [11] propose an approximation that terminates the partitioning algorithm when the new additional set of hyper-rectangles in the partitioning has a total hypervolume of less than a predetermined fraction ζof the hypervolume dominated by the Pareto front. While qEHVI is guaranteed to be exact when an exact partitioning of the non-dominated space is used, qEHVI is agnostic to the partitioning algorithm used and is compatible with more scalable approximate methods. We evaluate the performance ofqEHVI with approximation of various ﬁdelities ζon DTLZ2 problems with 3 and 4 objectives (with d = 6 ). ζ = 0 corresponds to an exact partitioning and the approximation is monotonically worse as ζincreases. Larger values of ζdegrade optimization performance (Figure 10), but can result in substantial speedups (Table 4). Even with coarser levels of approximation, qEHVI () performs better than qPAREGO with respect to log hypervolume difference, while achieving wall time improvements of 2-7x compared to exact qEHVI. 0 20 40 60 80 100 Function/uni00A0Evaluations 1.0 0.8 0.6 0.4 0.2 log/uni00A0HV/uni00A0Difference qEHVI/uni00A0( = 10 3) qEHVI/uni00A0( = 10 4) qEHVI/uni00A0( = 10 5) qEHVI/uni00A0( = 10 6) qEHVI/uni00A0(exact) qParEGO (a) 0 20 40 60 80 100 Function/uni00A0Evaluations 0.7 0.6 0.5 0.4 0.3 0.2 0.1 log/uni00A0HV/uni00A0DifferenceqEHVI/uni00A0( = 10 3) qEHVI/uni00A0( = 10 4) qEHVI/uni00A0( = 10 5) qEHVI/uni00A0( = 10 6) qEHVI/uni00A0(exact) qParEGO (b) Figure 10: Optimization performance on DTLZ2 problems (d= 6) with approximate partitioning using various approximation levels ζfor (a) M = 3 objectives and (b) M = 4 objectives. 29CPU DTLZ2 ( M = 3) DTLZ2 ( M = 4) qPAREGO 5.86 (±0.51) 5 .6 (±0.53) qEHVI ( ζ = 10−3) 6.89 (±0.41) 9 .53 (±0.49) qEHVI ( ζ = 10−4) 9.83 (±0.9) 17 .47 (±1.2) qEHVI ( ζ = 10−5) 18.99 (±2.72) 60 .27 (±3.57) qEHVI ( ζ = 10−6) 37.9 (±7.47) 136 .15 (±12.88) qEHVI ( EXACT ) 45.52 (±9.83) 459 .33 (±77.95) Table 4: Acquisition function optimization wall time with approximate hypervolume computation, in seconds on a CPU (2x Intel Xeon E5-2680 v4 @ 2.40GHz). The mean and two standard errors are reported. F.5 Acquisition Computation Time Figure 11 show the acquisition computation time for different M and q. The inﬂection points corresponds to available processor cores becoming saturated. For large M an qon the GPU, memory becomes an issue, but we discuss ways of mitigating the issue in Appendix A.4. 2.5 5.0 7.5 10.0 12.5 15.0 17.5 q 0 1 2 3 4 5 6Acquisition/uni00A0Computation/uni00A0Time/uni00A0(s) M=2/uni00A0(CPU) M=3/uni00A0(CPU) M=4/uni00A0(CPU) M=2/uni00A0(GPU) M=3/uni00A0(GPU) M=4/uni00A0(GPU) Figure 11: Acquisition computation time for different batch sizes qand numbers of objectives M (this excludes the time required to compute the acquisition function given box decomposition of the non-dominated space). This uses N = 512 MC samples, d= 6, |P|= 10, and 20 training points. CPU time was measured on 2x Intel Xeon E5-2680 v4 @ 2.40GHz and GPU time was measured on a Tesla V100-SXM2-16GB GPU using 64-bit ﬂoating point precision. The mean and 2 standard errors over 1000 trials are reported. 30",
      "meta_data": {
        "arxiv_id": "2006.05078v3",
        "authors": [
          "Samuel Daulton",
          "Maximilian Balandat",
          "Eytan Bakshy"
        ],
        "published_date": "2020-06-09T06:57:47Z",
        "venue": "Advances in Neural Information Processing Systems 33, 2020",
        "pdf_url": "https://arxiv.org/pdf/2006.05078v3.pdf",
        "github_url": "https://github.com/pytorch/botorch"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces q-Expected Hypervolume Improvement (qEHVI), a novel acquisition function for parallel and constrained multi-objective Bayesian optimization (MO BO). It provides an exact computation of joint EHVI (up to Monte-Carlo integration error) and leverages auto-differentiation to compute exact gradients of its Monte-Carlo estimator. This enables efficient optimization using first-order and quasi-second-order methods. The work demonstrates qEHVI's computational tractability and superior performance compared to state-of-the-art MO BO algorithms, often at a fraction of their wall time. Additionally, qEHVI supports auxiliary outcome constraints and proves theoretical convergence guarantees under the Sample Average Approximation (SAA) approach. It also shows how auto-differentiation can be used for exact gradients of analytic EHVI for more than two objectives.",
        "methodology": "The methodology centers on `qEHVI`, an extension of Expected Hypervolume Improvement (EHVI). `qEHVI` calculates the joint Hypervolume Improvement (HVI) of `q` candidate points using box decompositions and the inclusion-exclusion principle to handle non-rectangular polytopes in the non-dominated space. The expected value of HVI is estimated via Monte-Carlo (MC) integration, sampling from the joint posterior of a Gaussian Process (GP) surrogate model; randomized quasi-MC methods are employed to reduce variance. Exact gradients of the MC estimator are computed using auto-differentiation and the re-parameterization trick, enabling gradient-based optimization. The Sample Average Approximation (SAA) approach is utilized for faster convergence with deterministic, higher-order optimizers. For constrained optimization, feasibility weighting is applied at the sample level, and indicator functions are replaced with differentiable sigmoid approximations to maintain differentiability.",
        "experimental_setup": "The empirical evaluation uses both synthetic and real-world optimization problems. Synthetic benchmarks include the Branin-Currin problem (2 objectives, 2 dimensions), the C2-DTLZ2 constrained problem (2 objectives, 12 dimensions, 1 constraint), and DTLZ2 problems (2, 3, and 4 objectives, 6 dimensions). Real-world benchmarks are Structural Optimization in Automobile Safety Design (3 objectives, 5 design parameters) and Policy Optimization for Adaptive Bitrate Control (2 objectives, 4 parameters). Performance is measured by log hypervolume difference. Baselines include SMS-EGO, PESMO, TS-TCH, analytic EHVI, a novel qPAREGO extension, and a quasi-random baseline. All outcomes are modeled with independent Gaussian processes with Matern 5/2 ARD kernels. Initialization uses 2(d+1) points from a scrambled Sobol sequence. `qEHVI` and `qPAREGO` use N=128 QMC samples. Experiments compare optimization on CPU (2x Intel Xeon E5-2680 v4) and GPU (Tesla V100-SXM2-16GB). An additional experiment evaluates performance on a noisy Branin-Currin function with additive Gaussian noise. Approximate box decompositions with varying fidelity (ζ) are tested on DTLZ2 problems (3 and 4 objectives).",
        "limitations": "One limitation is that `qEHVI` currently assumes noiseless observations, a common characteristic among existing EHVI formulations. Its scalability is constrained by the underlying partitioning algorithm, which limits its applicability to high-dimensional objective spaces (particularly for M >= 4). Furthermore, memory usage can become a challenge for large numbers of objectives (M) and batch sizes (q) when running on GPUs.",
        "future_research_directions": "Future research could explore integrating over the uncertainty of previous observations to account for noise in the model. Another key direction is to investigate more scalable partitioning algorithms, including approximate or more efficient exact algorithms, to enhance `qEHVI`'s performance and reduce computation time in higher-dimensional objective spaces. The authors also hope this work encourages further application of modern computational paradigms and tooling to Bayesian optimization. Further theoretical work could focus on deriving results on the convergence rate of the optimizer for the SAA approach and extending current convergence results to randomized Quasi-Monte Carlo methods for base samples.",
        "experimental_code": "from botorch.acquisition.multi_objective.base import MultiObjectiveMCAcquisitionFunction\nfrom botorch.acquisition.multi_objective.objective import MCMultiOutputObjective\nfrom botorch.models.model import Model\nfrom botorch.sampling.base import MCSampler\nfrom torch import Tensor\nfrom botorch.acquisition.logei import TAU_MAX, TAU_RELU\nfrom botorch.acquisition.multi_objective.base import MultiObjectiveMCAcquisitionFunction\nfrom botorch.acquisition.multi_objective.objective import MCMultiOutputObjective\nfrom botorch.models.model import Model\nfrom botorch.sampling.base import MCSampler\nfrom botorch.utils.multi_objective.box_decompositions.non_dominated import (\n    NondominatedPartitioning,\n)\nfrom botorch.utils.multi_objective.hypervolume import (\n    NoisyExpectedHypervolumeMixin,\n    SubsetIndexCachingMixin,\n)\nfrom botorch.utils.objective import compute_smoothed_feasibility_indicator\nfrom botorch.utils.safe_math import (\n    fatmin,\n    log_fatplus,\n    log_softplus,\n    logdiffexp,\n    logmeanexp,\n    logplusexp,\n    logsumexp,\n    smooth_amin,\n)\nfrom botorch.utils.transforms import (\n    average_over_ensemble_models,\n    concatenate_pending_points,\n    t_batch_mode_transform,\n)\nfrom torch import Tensor\nfrom botorch.exceptions.warnings import legacy_ei_numerics_warning\nfrom botorch.utils.multi_objective.box_decompositions.non_dominated import (\n    NondominatedPartitioning,\n)\nfrom botorch.utils.multi_objective.hypervolume import (\n    NoisyExpectedHypervolumeMixin,\n    SubsetIndexCachingMixin,\n)\nfrom botorch.utils.objective import compute_smoothed_feasibility_indicator\nfrom botorch.utils.transforms import (\n    average_over_ensemble_models,\n    concatenate_pending_points,\n    t_batch_mode_transform,\n)\nfrom torch import Tensor\n\n\nclass MultiObjectiveMCAcquisitionFunction(AcquisitionFunction, MCSamplerMixin, ABC):\n\n    _default_sample_shape = torch.Size([128])\n\n    def __init__(\n        self,\n        model: Model,\n        sampler: MCSampler | None = None,\n        objective: MCMultiOutputObjective | None = None,\n        constraints: list[Callable[[Tensor], Tensor]] | None = None,\n        eta: Tensor | float = 1e-3,\n        X_pending: Tensor | None = None,\n    ) -> None:\n        super().__init__(model=model)\n        MCSamplerMixin.__init__(self, sampler=sampler)\n        if objective is None:\n            objective = IdentityMCMultiOutputObjective()\n        elif not isinstance(objective, MCMultiOutputObjective):\n            raise UnsupportedError(\n                \"Only objectives of type MCMultiOutputObjective are supported for \"\n                \"Multi-Objective MC acquisition functions.\"\n            )\n        if (\n            hasattr(model, \"input_transform\")\n            and isinstance(model.input_transform, InputPerturbation)\n            and constraints is not None\n        ):\n            raise UnsupportedError(\n                \"Constraints are not supported with input perturbations, due to\"\n                \"sample q-batch shape being different than that of the inputs.\"\n                \"Use a composite objective that applies feasibility weighting to\"\n                \"samples before calculating the risk measure.\"\n            )\n        self.add_module(\"objective\", objective)\n        self.constraints = constraints\n        if constraints:\n            if type(eta) is not Tensor:\n                eta = torch.full((len(constraints),), eta)\n            self.register_buffer(\"eta\", eta)\n        self.X_pending = None\n        if X_pending is not None:\n            self.set_X_pending(X_pending)\n\n    @abstractmethod\n    def forward(self, X: Tensor) -> Tensor:\n        pass # pragma: no cover\n\n\nclass qExpectedHypervolumeImprovement(\n    MultiObjectiveMCAcquisitionFunction, SubsetIndexCachingMixin\n):\n    def __init__(\n        self,\n        model: Model,\n        ref_point: list[float] | Tensor,\n        partitioning: NondominatedPartitioning,\n        sampler: MCSampler | None = None,\n        objective: MCMultiOutputObjective | None = None,\n        constraints: list[Callable[[Tensor], Tensor]] | None = None,\n        X_pending: Tensor | None = None,\n        eta: Tensor | float = 1e-3,\n        fat: bool = False,\n    ) -> None:\n        legacy_ei_numerics_warning(legacy_name=type(self).__name__)\n        if len(ref_point) != partitioning.num_outcomes:\n            raise ValueError(\n                \"The length of the reference point must match the number of outcomes. \"\n                f\"Got ref_point with {len(ref_point)} elements, but expected \"\n                f\"{partitioning.num_outcomes}.\"\n            )\n        ref_point = torch.as_tensor(\n            ref_point,\n            dtype=partitioning.pareto_Y.dtype,\n            device=partitioning.pareto_Y.device,\n        )\n        super().__init__(\n            model=model,\n            sampler=sampler,\n            objective=objective,\n            constraints=constraints,\n            eta=eta,\n            X_pending=X_pending,\n        )\n        self.register_buffer(\"ref_point\", ref_point)\n        cell_bounds = partitioning.get_hypercell_bounds()\n        self.register_buffer(\"cell_lower_bounds\", cell_bounds[0])\n        self.register_buffer(\"cell_upper_bounds\", cell_bounds[1])\n        SubsetIndexCachingMixin.__init__(self)\n        self.fat = fat\n\n    def _compute_qehvi(self, samples: Tensor, X: Tensor | None = None) -> Tensor:\n        obj = self.objective(samples, X=X)\n        q = obj.shape[-2]\n        if self.constraints is not None:\n            feas_weights = compute_smoothed_feasibility_indicator(\n                constraints=self.constraints,\n                samples=samples,\n                eta=self.eta,\n                fat=self.fat,\n            ) # `sample_shape x batch-shape x q`\n        device = self.ref_point.device\n        q_subset_indices = self.compute_q_subset_indices(q_out=q, device=device)\n        batch_shape = obj.shape[:-2]\n        areas_per_segment = torch.zeros(\n            *batch_shape,\n            self.cell_lower_bounds.shape[-2],\n            dtype=obj.dtype,\n            device=device,\n        )\n        cell_batch_ndim = self.cell_lower_bounds.ndim - 2\n        sample_batch_view_shape = torch.Size(\n            [\n                batch_shape[0] if cell_batch_ndim > 0 else 1,\n                *[1 for _ in range(len(batch_shape) - max(cell_batch_ndim, 1))],\n                *self.cell_lower_bounds.shape[1:-2],\n            ]\n        )\n        view_shape = (\n            *sample_batch_view_shape,\n            self.cell_upper_bounds.shape[-2],\n            1,\n            self.cell_upper_bounds.shape[-1],\n        )\n        for i in range(1, self.q_out + 1):\n            q_choose_i = q_subset_indices[f\"q_choose_{i}\"]\n            obj_subsets = obj.index_select(dim=-2, index=q_choose_i.view(-1))\n            obj_subsets = obj_subsets.view(\n                obj.shape[:-2] + q_choose_i.shape + obj.shape[-1:]\n            )\n            overlap_vertices = obj_subsets.min(dim=-2).values\n            overlap_vertices = torch.min(\n                overlap_vertices.unsqueeze(-3), self.cell_upper_bounds.view(view_shape)\n            )\n            lengths_i = (\n                overlap_vertices - self.cell_lower_bounds.view(view_shape)\n            ).clamp_min(0.0)\n            areas_i = lengths_i.prod(dim=-1)\n            if self.constraints is not None:\n                feas_subsets = feas_weights.index_select(\n                    dim=-1, index=q_choose_i.view(-1)\n                ).view(feas_weights.shape[:-1] + q_choose_i.shape)\n                areas_i = areas_i * feas_subsets.unsqueeze(-3).prod(dim=-1)\n            areas_i = areas_i.sum(dim=-1)\n            areas_per_segment += (-1) ** (i + 1) * areas_i\n        return areas_per_segment.sum(dim=-1).mean(dim=0)\n\n    @concatenate_pending_points\n    @t_batch_mode_transform()\n    @average_over_ensemble_models\n    def forward(self, X: Tensor) -> Tensor:\n        posterior = self.model.posterior(X)\n        samples = self.get_posterior_samples(posterior)\n        return self._compute_qehvi(samples=samples, X=X)\n\n\nTAU_RELU = 1e-6\nTAU_MAX = 1e-2\n\n\nclass qLogExpectedHypervolumeImprovement(\n    MultiObjectiveMCAcquisitionFunction, SubsetIndexCachingMixin\n):\n    _log: bool = True\n\n    def __init__(\n        self,\n        model: Model,\n        ref_point: list[float] | Tensor,\n        partitioning: NondominatedPartitioning,\n        sampler: MCSampler | None = None,\n        objective: MCMultiOutputObjective | None = None,\n        constraints: list[Callable[[Tensor], Tensor]] | None = None,\n        X_pending: Tensor | None = None,\n        eta: Tensor | float = 1e-2,\n        fat: bool = True,\n        tau_relu: float = TAU_RELU,\n        tau_max: float = TAU_MAX,\n    ) -> None:\n        if len(ref_point) != partitioning.num_outcomes:\n            raise ValueError(\n                \"The dimensionality of the reference point must match the number of \"\n                f\"outcomes. Got ref_point with {len(ref_point)} elements, but expected \"\n                f\"{partitioning.num_outcomes}.\"\n            )\n        ref_point = torch.as_tensor(\n            ref_point,\n            dtype=partitioning.pareto_Y.dtype,\n            device=partitioning.pareto_Y.device,\n        )\n        super().__init__(\n            model=model,\n            sampler=sampler,\n            objective=objective,\n            constraints=constraints,\n            eta=eta,\n            X_pending=X_pending,\n        )\n        self.register_buffer(\"ref_point\", ref_point)\n        cell_bounds = partitioning.get_hypercell_bounds()\n        self.register_buffer(\"cell_lower_bounds\", cell_bounds[0])\n        self.register_buffer(\"cell_upper_bounds\", cell_bounds[1])\n        SubsetIndexCachingMixin.__init__(self)\n        self.tau_relu = tau_relu\n        self.tau_max = tau_max\n        self.fat = fat\n\n    def _compute_log_qehvi(self, samples: Tensor, X: Tensor | None = None) -> Tensor:\n        obj = self.objective(samples, X=X)  # mc_samples x batch_shape x q x m\n        q = obj.shape[-2]\n        if self.constraints is not None:\n            log_feas_weights = compute_smoothed_feasibility_indicator(\n                constraints=self.constraints,\n                samples=samples,\n                eta=self.eta,\n                log=True,\n                fat=self.fat,\n            )\n        device = self.ref_point.device\n        q_subset_indices = self.compute_q_subset_indices(q_out=q, device=device)\n        batch_shape = obj.shape[:-2]  # mc_samples x batch_shape\n        log_areas_per_segment = torch.full(\n            size=(\n                *batch_shape,\n                self.cell_lower_bounds.shape[-2],  # num_cells\n                2,  # for even and odd terms\n            ),\n            fill_value=-torch.inf,\n            dtype=obj.dtype,\n            device=device,\n        )\n\n        cell_batch_ndim = self.cell_lower_bounds.ndim - 2\n        sample_batch_view_shape = torch.Size(\n            [\n                batch_shape[0] if cell_batch_ndim > 0 else 1,\n                *[1 for _ in range(len(batch_shape) - max(cell_batch_ndim, 1))],\n                *self.cell_lower_bounds.shape[1:-2],\n            ]\n        )\n        view_shape = (\n            *sample_batch_view_shape,\n            self.cell_upper_bounds.shape[-2],  # num_cells\n            1,  # adding for q_choose_i dimension\n            self.cell_upper_bounds.shape[-1],  # num_objectives\n        )\n\n        for i in range(1, self.q_out + 1):\n            q_choose_i = q_subset_indices[f\"q_choose_{i}\"]  # q_choose_i x i\n            obj_subsets = obj.index_select(dim=-2, index=q_choose_i.view(-1))\n            obj_subsets = obj_subsets.view(\n                obj.shape[:-2] + q_choose_i.shape + obj.shape[-1:]\n            )\n\n            log_improvement_i = self._log_improvement(obj_subsets, view_shape)\n\n            log_improvement_i = self._smooth_min(\n                log_improvement_i,\n                dim=-2,\n            )  # mc_samples x batch_shape x num_cells x q_choose_i x m\n\n            log_lengths_i = self._log_cell_lengths(log_improvement_i, view_shape)\n\n            log_areas_i = log_lengths_i.sum(dim=-1)  # areas_i = lengths_i.prod(dim=-1)\n\n            if self.constraints is not None:\n                log_feas_subsets = log_feas_weights.index_select(\n                    dim=-1, index=q_choose_i.view(-1)\n                ).view(log_feas_weights.shape[:-1] + q_choose_i.shape)\n                log_areas_i = log_areas_i + log_feas_subsets.unsqueeze(-3).sum(dim=-1)\n\n            log_areas_i = logsumexp(log_areas_i, dim=-1)  # areas_i.sum(dim=-1)\n\n            log_areas_per_segment[..., i % 2] = logplusexp(\n                log_areas_per_segment[..., i % 2],\n                log_areas_i,\n            )\n\n        log_areas_per_segment = logdiffexp(\n            log_a=log_areas_per_segment[..., 0], log_b=log_areas_per_segment[..., 1]\n        )\n\n        return logmeanexp(logsumexp(log_areas_per_segment, dim=-1), dim=0)\n\n    def _log_improvement(\n        self, obj_subsets: Tensor, view_shape: tuple | torch.Size\n    ) -> Tensor:\n        obj_subsets = obj_subsets.unsqueeze(-4)\n        cell_lower_bounds = self.cell_lower_bounds.view(view_shape).unsqueeze(-3)\n        Z = obj_subsets - cell_lower_bounds\n        log_Zi = self._log_smooth_relu(Z)\n        return log_Zi  # mc_samples x batch_shape x num_cells x q_choose_i x i x m\n\n    def _log_cell_lengths(\n        self, log_improvement_i: Tensor, view_shape: tuple | torch.Size\n    ) -> Tensor:\n        cell_upper_bounds = self.cell_upper_bounds.clamp_max(\n            1e10 if log_improvement_i.dtype == torch.double else 1e8\n        )  # num_cells x num_objectives\n        log_cell_lengths = (\n            (cell_upper_bounds - self.cell_lower_bounds).log().view(view_shape)\n        )  # (mc_samples = 1) x (batch_shape = 1) x n_cells x (q_choose_i = 1) x m\n        return self._smooth_minimum(\n            log_improvement_i,\n            log_cell_lengths,\n        )\n\n    def _log_smooth_relu(self, X: Tensor) -> Tensor:\n        f = log_fatplus if self.fat else log_softplus\n        return f(X, tau=self.tau_relu)\n\n    def _smooth_min(self, X: Tensor, dim: int, keepdim: bool = False) -> Tensor:\n        f = fatmin if self.fat else smooth_amin\n        return f(X, tau=self.tau_max, dim=dim)\n\n    def _smooth_minimum(self, X: Tensor, Y: Tensor) -> Tensor:\n        XY = torch.stack(torch.broadcast_tensors(X, Y), dim=-1)\n        return self._smooth_min(XY, dim=-1, keepdim=False)\n\n    @concatenate_pending_points\n    @t_batch_mode_transform()\n    @average_over_ensemble_models\n    def forward(self, X: Tensor) -> Tensor:\n        posterior = self.model.posterior(X)\n        samples = self.get_posterior_samples(posterior)\n        return self._compute_log_qehvi(samples=samples, X=X)\n\n\ndef compute_smoothed_feasibility_indicator(\n    constraints: list[Callable[[Tensor], Tensor]] | None,\n    samples: Tensor,\n    eta: Tensor | float = 1e-3,\n    log: bool = False,\n    fat: bool = False,\n) -> Tensor:\n    if constraints is None:\n        return torch.zeros(samples.shape[:-2], device=samples.device, dtype=samples.dtype)\n\n    if not log:\n        if fat:\n            soft_clamp = fatplus\n        else:\n            soft_clamp = softplus\n        indicator = 1.0\n    else:\n        soft_clamp = log_fatplus if fat else log_softplus\n        indicator = 0.0\n\n    if type(eta) is not Tensor:\n        eta = torch.full((len(constraints),), eta, device=samples.device, dtype=samples.dtype)\n\n    for i, constraint in enumerate(constraints):\n        constrained_obj = -constraint(samples)\n        if not log:\n            indicator = indicator * soft_clamp(constrained_obj, beta=1.0 / eta[i])\n        else:\n            indicator = indicator + soft_clamp(constrained_obj, beta=1.0 / eta[i])\n    return indicator\n",
        "experimental_info": "The methodology centers on `qEHVI`, an extension of Expected Hypervolume Improvement (EHVI), or its log-transformed variant `qLogEHVI`.\n\n**Hypervolume Improvement (HVI) Calculation:**\n-   `qEHVI` calculates the joint Hypervolume Improvement (HVI) of `q` candidate points using box decompositions and the inclusion-exclusion principle to handle non-rectangular polytopes in the non-dominated space.\n-   A `NondominatedPartitioning` or `FastNondominatedPartitioning` module is used to partition the non-dominated space.\n-   The approximation level `alpha` for partitioning is `0.0` (exact) for up to 4 objectives, `10^-3` for 5-6 objectives, and `10^-2` for more than 6 objectives (determined by `get_default_partitioning_alpha`).\n\n**Expected Value Estimation (Monte-Carlo Integration):**\n-   The expected value of HVI is estimated via Monte-Carlo (MC) integration.\n-   Sampling is performed from the joint posterior of a Gaussian Process (GP) surrogate model.\n-   Randomized quasi-MC methods (`SobolQMCNormalSampler`) are employed by default to reduce variance.\n-   The default number of MC samples (`mc_samples`) for multi-objective acquisition functions is `128` (e.g., in `construct_inputs_qEHVI`), although a general `get_acquisition_function` might default to `512`.\n-   The default sample shape for multi-objective MC acquisition functions is `torch.Size([128])`.\n\n**Gradient-Based Optimization:**\n-   Exact gradients of the MC estimator are computed using auto-differentiation and the re-parameterization trick.\n-   The Sample Average Approximation (SAA) approach is utilized for faster convergence with deterministic, higher-order optimizers.\n\n**Constrained Optimization:**\n-   Feasibility weighting is applied at the sample level.\n-   Indicator functions for constraints are replaced with differentiable sigmoid approximations.\n-   The temperature parameter `eta` for the sigmoid approximation is `1e-3` for `qEHVI` and `1e-2` for `qLogEHVI`.\n-   The `fat` parameter, which toggles the logarithmic/linear asymptotic behavior of the smooth approximation, is `False` for `qEHVI` and `True` for `qLogEHVI`.\n\n**Log-space Calculations (`qLogEHVI` specific):**\n-   `qLogEHVI` computes the logarithm of the Expected Hypervolume Improvement in a numerically robust manner.\n-   It uses specific temperature parameters for smooth approximations:\n    -   `TAU_RELU = 1e-6` for smoothing the ReLU function.\n    -   `TAU_MAX = 1e-2` for smoothing the `max` operator.\n\n**Global Numerical Settings (GPyTorch/LinearOperator):**\n-   Fast computations in `linear_operator` are turned off by default (`_fast_covar_root_decomposition`, `_fast_log_prob`, `_fast_solves` are `False`).\n-   `max_cholesky_size` and `max_eager_kernel_size` are set to `4096`.\n-   `cholesky_max_tries` is set to `6`.\n-   A warning is raised for legacy EI acquisition functions due to known numerical issues, recommending the use of their `LogEI` counterparts."
      }
    },
    {
      "title": "Hyperparameter Optimization through Neural Network Partitioning",
      "abstract": "Well-tuned hyperparameters are crucial for obtaining good generalization\nbehavior in neural networks. They can enforce appropriate inductive biases,\nregularize the model and improve performance -- especially in the presence of\nlimited data. In this work, we propose a simple and efficient way for\noptimizing hyperparameters inspired by the marginal likelihood, an optimization\nobjective that requires no validation data. Our method partitions the training\ndata and a neural network model into $K$ data shards and parameter partitions,\nrespectively. Each partition is associated with and optimized only on specific\ndata shards. Combining these partitions into subnetworks allows us to define\nthe ``out-of-training-sample\" loss of a subnetwork, i.e., the loss on data\nshards unseen by the subnetwork, as the objective for hyperparameter\noptimization. We demonstrate that we can apply this objective to optimize a\nvariety of different hyperparameters in a single training run while being\nsignificantly computationally cheaper than alternative methods aiming to\noptimize the marginal likelihood for neural networks. Lastly, we also focus on\noptimizing hyperparameters in federated learning, where retraining and\ncross-validation are particularly challenging.",
      "full_text": "Published as a conference paper at ICLR 2023 HYPERPARAMETER OPTIMIZATION THROUGH NEURAL NETWORK PARTITIONING Bruno Mlodozeniec†∗, Matthias Reisser‡, Christos Louizos‡ †University of Cambridge, ‡Qualcomm AI Research bkm28@cam.ac.uk, {mreisser,clouizos}@qti.qualcomm.com ABSTRACT Well-tuned hyperparameters are crucial for obtaining good generalization behavior in neural networks. They can enforce appropriate inductive biases, regularize the model and improve performance — especially in the presence of limited data. In this work, we propose a simple and efﬁcient way for optimizing hyperparameters inspired by the marginal likelihood, an optimization objective that requires no validation data. Our method partitions the training data and a neural network model into K data shards and parameter partitions, respectively. Each partition is associated with and optimized only on speciﬁc data shards. Combining these partitions into subnetworks allows us to deﬁne the “out-of-training-sample” loss of a subnetwork, i.e., the loss on data shards unseen by the subnetwork, as the objective for hyperparameter optimization. We demonstrate that we can apply this objective to optimize a variety of different hyperparameters in a single training run while being signiﬁcantly computationally cheaper than alternative methods aiming to optimize the marginal likelihood for neural networks. Lastly, we also focus on optimizing hyperparameters in federated learning, where retraining and cross-validation are particularly challenging. 1 I NTRODUCTION Due to their remarkable generalization capabilities, deep neural networks have become the de-facto models for a wide range of complex tasks. Combining large models, large-enough datasets, and sufﬁcient computing capabilities enable researchers to train powerful models through gradient descent. Regardless of the data regime, however, the choice of hyperparameters — such as neural architecture, data augmentation strategies, regularization, or which optimizer to choose — plays a crucial role in the ﬁnal model’s generalization capabilities. Hyperparameters allow encoding good inductive biases that effectively constrain the models’ hypothesis space (e.g., convolutions for vision tasks), speed up learning, or prevent overﬁtting in the case of limited data. Whereas gradient descent enables the tuning of model parameters, accessing hyperparameter gradients is more complicated. The traditional and general way to optimize hyperparameters operates as follows; 1) partition the dataset into training and validation data1, 2) pick a set of hyperparameters and optimize the model on the training data, 3) measure the performance of the model on the validation data and ﬁnally 4) use the validation metric as a way to score models or perform search over the space of hyperparameters. This approach inherently requires training multiple models and consequently requires spending resources on models that will be discarded. Furthermore, traditional tuning requires a validation set since optimizing the hyperparameters on the training set alone cannot identify the right inductive biases. A canonical example is data augmentations — they are not expected to improve training set performance, but they greatly help with generalization. In the low data regime, deﬁning a validation set that cannot be used for tuning model parameters is undesirable. Picking the right amount of validation data is a hyperparameter in itself. The conventional rule of thumb to use ∼10% of all data can result in signiﬁcant overﬁtting, as pointed out by Lorraine et al. (2019) , when one has a sufﬁciently large number of hyperparameters to tune. Furthermore, a validation set can be challenging ∗Work done while at Qualcomm AI Research. Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc. and/or its subsidiaries. 1a third partition, the test or holdout set is used to estimate the ﬁnal model performance 1 arXiv:2304.14766v1  [cs.LG]  28 Apr 2023Published as a conference paper at ICLR 2023 to obtain in many use cases. An example is Federated Learning (FL) (McMahan et al., 2017), which we speciﬁcally consider in our experimental section. In FL, each extra training run (for,e.g., a speciﬁc hyperparameter setting) comes with additional, non-trivial costs. Different approaches have been proposed in order to address these challenges. Some schemes optimize hyperparameters during a single training run by making the hyperparameters part of the model (e.g., learning dropout rates with concrete dropout (Gal et al., 2017), learning architectures with DARTs (Liu et al., 2018) and learning data-augmentations with schemes as in Benton et al. (2020); van der Wilk et al. (2018)). In cases where the model does not depend on the hyperparameters directly but only indirectly through their effect on the value of the ﬁnal parameters (through optimization), schemes for differentiating through the training procedures have been proposed, such as Lorraine et al. (2019). Another way of optimizing hyperparameters without a validation set is through the canonical view on model selection (and hence hyperparameter optimization) through the Bayesian lens; the concept of optimizing the marginal likelihood. For deep neural networks, however, the marginal likelihood is difﬁcult to compute. Prior works have therefore developed various approximations for its use in deep learning models and used those to optimize hyperparameters in deep learning, such as those of data augmentation (Schw¨obel et al., 2021; Immer et al., 2022). Still, however, these come at a signiﬁcant added computational expense and do not scale to larger deep learning problems. This paper presents a novel approach to hyperparameter optimization, inspired by the marginal likelihood, that only requires a single training run and no validation set. Our method is more scalable than previous works that rely on marginal likelihood and Laplace approximations (which require computing or inverting a Hessian (Immer et al., 2021)) and is broadly applicable to any hierarchical modelling setup. 2 M ARGINAL LIKELIHOOD AND PRIOR WORK In Bayesian inference, the rules of probability dictate how any unknown, such as parameters w or hyperparameters ψ, should be determined given observed data D. Let p(w) be a prior over w and p(D|w,ψ) be a likelihood for Dwith ψbeing the hyperparameters. We are then interested in the posterior given the data p(w|D,ψ) =p(D|w,ψ)p(w)/p(D|ψ). The denominator term p(D|ψ) is known as the marginal likelihood, as it measures the probability of observing the data given ψ, irrespective of the value of w: p(D|ψ) = ∫ p(w)p(D|w,ψ)dw. Marginal likelihood has many desirable properties that make it a good criterion for model selection and hyperparameter optimization. It intuitively implements the essence of Occam’s Razor principle (MacKay, 2003, § 28). In the PAC-Bayesian literature, it has been shown that higher marginal likelihood gives tighter frequentist upper bounds on the generalization performance of a given model class (McAllester, 1998; Germain et al., 2016). It also has close links to cross-validation (see section 2.1) and can be computed from the training data alone. However, computation of the marginal likelihood in deep learning models is usually prohibitively expensive and many recent works have proposed schemes to approximate the marginal likelihood for differentiable model selection (Lyle et al., 2020; Immer et al., 2021; 2022; Schw¨obel et al., 2021). 2.1 “L EARNING SPEED ” PERSPECTIVE Lyle et al. (2020); Fong and Holmes (2020) pointed out the correspondence between “learning speed” and marginal likelihood. Namely, the marginal likelihood of the data Dconditioned on some hyperparameters ψcan be written as: log p(D|ψ) = ∑ k log Ep(w|D1:k−1,ψ) [p(Dk|w,ψ)] ≥ ∑ k Ep(w|D1:k−1,ψ) [log p(Dk|w,ψ)] (1) where (D1,..., DC) is an arbitrary partitioning of the training dataset Dinto Cshards or chunks2, and p(w|D1:k,ψ) is the posterior over parameters of a function fw : X → Y, from the input domain Xto the target domain Yafter seeing data in shards 1 through k. The right-hand side can be interpreted as a type of cross-validation in which we ﬁx an ordering over the shards and measure the “validation” performance on each shardDk using a model trained on the preceding shards D1:k−1. 2We use the terms “chunk” and “shard” interchangeably. 2Published as a conference paper at ICLR 2023 Alternatively, it can be viewed as the learning speed of a (probabilistic) model: i.e., a measure of how quickly it learns to perform well on new shards of data after only having been ﬁt to the previous shards (through exact Bayesian updating). This perspective neatly illustrates why models with higher marginal likelihood can exhibit good inductive biases, e.g., encoded through ψ, w and fw. Namely, such models can be expected to learn faster and generalize better after seeing fewer samples. For example, if the hypothesis space is constrained3to functions satisfying symmetries present in the data, we need fewer data to identify the correct function (Sokolic et al., 2017; Sannai et al., 2021). We argue that the “learning speed” aspect of marginal likelihood — i.e., measuring how well the model generalizes to new data in the training set, having been trained only on the previous data points — is the key property making marginal likelihood a useful tool for selecting hyperparameters. 2.2 T RAINING SPEED FOR HYPERPARAMETER OPTIMIZATION Computing the “learning speed”, requires samples from the posteriorp(w|D1:k,ψ). Unfortunately, in deep learning settings, such samples are impractical to obtain; thus, prior works have focused on more scalable alternatives. Lyle et al. (2020) propose to approximate the objective in Eq. 1 by looking at the training speed during standard training of a neural network by SGD. Speciﬁcally, they deﬁne the training speed as the reduction in the training loss after a single SGD parameter update, summed over all updates in the ﬁrst epoch. They argue that, during the ﬁrst epoch of training, after the neural network parameters, w, have been updated with SGD steps using data from shards D1:k, they can be approximately used in place of the sample from the posterior p(w|D1:k,ψ) in Eq. 1. They extend the analogy to training past one epoch and use the training speed estimate for model selection (Ru et al., 2021). As pointed out by the authors, however, the analogy between learning speed and training speed somewhat breaks down after 1 epoch of training. The network parameters have “seen” every datapoint in the training set after1 epoch, and hence the connection to measuring the model’s generalization capability is weakened. For the sake of scalability and alignment with deep learning practice, we also focus on simple pointwise approximations qk(w) = δ(w = ˆwk) to the posteriors p(w|D1:k,ψ). However, in contrast to prior work, we explicitly parametrize the learning procedure such that, at any given training iteration, we have access to a model that is trained only on a subset of the dataD1:k. In doing so, we can approximate the objective in Eq. 1, and thus use it to optimize the hyperparameters during the entire training run. 3 P ARTITIONED NEURAL NETWORKS Our goal is to optimize the objective LML (D,ψ) = C∑ k=1 Eqk−1(w) [log p(Dk|w,ψ)] (2) wrt. ψ, which is an approximation to the lower-bound presented in Eq. 1 above. In Appendix A, we show that the left-hand side is also a lower-bound on the marginal likelihood under some unobtrusive conditions. As mentioned in Section 2.2, our goal is to propose an architecture and a training scheme so that we can easily obtain models trained on only subsets of the data D1:k for all k throughout training. We propose that each {qk(w)}C k=1 optimizes a subset of the parameters of the neural network, in a manner that allows us to extract “subnetworks” from the main network that have been trained on speciﬁc chunks of data. We describe the partitioning scheme below. Partitioning the parameters Denote the concatenations of the weights of a neural networkw ∈RN. We can deﬁne a partitioning ((w1,..., wC),P) of the parameters into C partitions, such that w = Pconcat(w1,..., wC) for a permutation matrix P ∈{0,1}N×N. For ease of exposition, we drop the dependence on P, assuming that w is already arranged such that P is identity, P = IN×N. Given the partitioning (w1,..., wC) of the parameters, we then specify Csubnetworks with weights w(1) s ,..., w(C) s such that w(k) s = concat(w1,..., wk, ˆwk+1,..., ˆwC), where ˆwi are some default 3or if the learning algorithm is heavily biased towards returning hypotheses that satisfy a given invariance, e.g., through the use of a prior. 3Published as a conference paper at ICLR 2023 values not optimized during training4. More speciﬁcally, the k-th subnetwork, wk s, retains the ﬁrst kpartitions from the weight partitioning and sets the remaining parameters to ˆwk+1:C. Note that, if each wk is only updated on chunks D1:k, the subnetwork w(k) s is only comprised of weights that have been updated on D1:k. Thus, we can view the parameters of w(k) s as an approximation to qk(w). Although, given that a subset of the parameters in each w(k) s is ﬁxed, this would likely be a poor approximation to the true posterior over the weights given D1:k, it could be, intuitively, a reasonable approximation in function space5. Partitioned training Having partitioned the dataset Dinto Cchunks (D1,..., Dk), we update each partition wk by optimising the negative log-likelihood6on chunks D1:k using subnetwork w(k) s by computing the following gradients: ∇wkL ( D1:k,w(k) s ) = ∑ (x,y)∈D1:k ∇wk log p ( y ⏐⏐⏐x; w(k) s ,ψ ) . (3) We interleave stochastic gradient updates of each partition of the weights with updating the hyperpa- rameters ψusing LML in Eq. 2: ∇ψLML (D,ψ) ≈ C∑ k=2 ∑ (x,y)∈Dk ∇ψlog p ( y ⏐⏐⏐x,w(k−1) s ,ψ ) . (4) This can be seen as the sum of the out-of-sample losses for each subnetwork w(k) s . The scheme is illustrated in Figure 1. For details of how the updates are scheduled in our experiments, see Appendix I. Note that, while we could incorporate the gradient of the ﬁrst term from Eq. 1 corresponding to Eq0(w)[log p(D1|w,ψ)] in Eq. 4, we chose to leave it out. Hence, the gradient of Eq. 4 is of an estimate that can be viewed as an approximation to the conditional marginal likelihood log p(D2:C|D1,ψ). Conditional marginal likelihood has been shown to have many desirable properties for model selection and, in many cases, can be a better proxy for generalization (Lotﬁ et al., 2022). Weights: w = (w1,w2,w3) Alternate: Optimize parameters: log p ( D1 |(w1, ˆw2, ˆw3)   Subnet. 1 ,ψ ) w.r.t. w1 log p ( D1:2|(w1,w2, ˆw3)   Subnet. 2 ,ψ ) w.r.t. w2 log p ( D1:3|(w1,w2,w3)   Subnet. 3 ,ψ ) w.r.t. w3 Optimize hyper parameters ψon: log p ( D2|(w1, ˆw2, ˆw3)   Subnet. 1 ,ψ ) + logp ( D3|(w1,w2, ˆw3)   Subnet. 2 ,ψ ) Figure 1: Best viewed in colour. Illustration of the partitioning scheme for a single hidden layer perceptron with C = 3chunks. This procedure, inspired by the marginal likelihood, has several desirable properties compared to prior work. 1) Our objective is computationally efﬁcient, with a computational cost roughly corresponding to evaluating subnetworks on the training set. There is no need to compute nor invert a Hessian with 4e.g., ˆwi could be the value of the weights at initialization, or ˆwi = 0 corresponding to pruning those parameters and obtaining a proper subnetwork. 5Since a) the mapping from parameters to functions is not bijective and b) neural networks are highly overparameterised and can be heavily pruned while retaining performance (Frankle and Carbin, 2018), obtaining a good ﬁt to a subset of the training data with a subset of the model parameters should be possible. Furthermore, “scaling laws” indicate that the beneﬁt of having more parameters becomes apparent mostly for larger dataset sizes (Kaplan et al., 2020), thus it is reasonable for subnetworks ﬁt to more data to have more learnable parameters. 6Optionally with an added negative log-prior regularization term log p(w(k) s ). 4Published as a conference paper at ICLR 2023 respect to the weights, as in the Laplace approximation (Immer et al., 2021; 2022). 2) Our objective is readily amenable to optimization by stochastic gradient descent; we do not have to iterate over the entire training set to compute a single gradient update for the hyperparameters. 3) Compared to the training speed objective (Lyle et al., 2020), in our method, the training of the weights in each subnetwork progresses independently of the data in future chunks. Hence, it can be seen as more truthfully measuring the generalization capability of a model using a given set of hyperparameters. Partitioning Schemes There are several ways in which the neural network weights can be partitioned. In our experiments in Section 5, we partition the weights before beginning training by assigning a ﬁxed proportion of weights in each layer to a given partition at random. For each subnetwork, for the weight partitions corresponding to future chunks, we use the values of the weights at initialisation. For a discussion of partitioning schemes, see Appendix C. 4 R ELATED WORKS Hyperparameter optimization in deep learning Many works have tackled the challenge of op- timizing hyperparameters in deep learning. Works on implicit differentiation, such as the one by Lorraine et al. (2019), allow for optimizing training hyperparameters such as the learning rate, weight- decay, or other hyperparameters that affect the ﬁnal neural network weights only through the training routine. Other works have proposed ways to parameterize and optimize data-augmentations (Cubuk et al., 2018; Li et al., 2020), search-spaces for neural network architectures, as well as methods to optimize architectures using gradient-based optimization (Liu et al., 2018; Elsken et al., 2019). All of the above works have primarily relied on optimizing hyperparameters on a separate validation set and are compatible with the objective deﬁned in this work. Several works have also aimed to cast learning data augmentations as an invariance learning problem. They do so by parameterizing the model itself with data augmentations, and frame invariance learning as a model selection problem (van der Wilk et al., 2018; Benton et al., 2020; Schw¨obel et al., 2021; Nabarro et al., 2022; Immer et al., 2022). We compare against Benton et al. (2020) (“Augerino”) and Immer et al. (2022) (“Differentiable Laplace”) on this task in the experimental section. Hyperparameter optimization without a validation set A limited number of works consider learning hyperparameters without a validation set in a deep learning context. Benton et al. (2020) propose a simple method for learning invariances without a validation set by regularising invariance hyperparameters to those resulting in higher invariance. They show that the invariances found tend to be insensitive to the regularisation strength, determined by another hyperparameter. However, the method relies on being able to a priori deﬁne which hyperparameters lead to higher invariance through a suitable regularisation function. In more complex invariance learning settings, deﬁning the regulariser can be challenging. For example, if data-augmentation transformations were to be parameterized by a neural network (as proposed in Lorraine et al. (2019)), it is non-trivial to devise an adequate regulariser. We show that our method can be applied to such settings. Other works focus on deriving tractable approximations to the marginal likelihood for deep neural networks. Schw ¨obel et al. (2021) propose only marginalising-out the parameters in the last layer of the neural network by switching it out for a Gaussian Process. They treat the preceding layer effectively as a hyperparameter, and optimize invariance parameters using the marginal likelihood. Although they show promising results on MNIST, they found they “were unable to learn invariances for CIFAR-10” (Schw¨obel et al., 2021, §7) and highlighted the need to marginalise lower layers as well. In contrast, our objective can be seen as being inspired by marginal likelihood where arbitrary network layers can be “marginalised”, and works on datasets like CIFAR-10. Immer et al. (2022) have adapted the Laplace approximation (Immer et al., 2021) to make it tractable for learning data augmentations. In contrast to Schw¨obel et al. (2021), they approximately marginalize out all the network parameters, and performs favourably. Their approximation, however, requires approximations to a Hessian w.r.t. all network parameters; for that reason, their work reports results for architectures only up to a ResNet-14, whereas our method can easily scale to larger architectures. Hyperparameter optimization in FL Improving hyperparameter optimization is especially rele- vant to FL. Given the potential system level constraints (Wang et al., 2021), methods that optimize the hyperparameters and parameters in a single training run are preferred. On this note, Khodak et al. (2021) introduced FedEx and showed that it can successfully optimize the client optimizer 5Published as a conference paper at ICLR 2023 hyperparameters. FedEx relies on a training/validation split on the client level and uses a REIN- FORCE type of gradient (Williams, 1992) estimator, which usually exhibits high variance and needs baselines to reduce it (Mohamed et al., 2020). This is in contrast to partitioned networks, which use standard, low-variance backpropagation for the hyperparameters and no separate validation set per client. To optimize the other hyperparameters, Khodak et al. (2021) wrapped FedEx with a traditional hyperparameter optimization strategy, the successive halving algorithm. This is orthogonal to our method and could be applied to partitioned networks as well. In Zhou et al. (2021), the authors perform a hyperparameter search independently on each client with some off-the-shelf methods and then aggregate the results of the search at the server once in order to identify the best hyperparameter setting. The main drawback of this method compared to partitioned networks is that when the local client datasets are small, a client-speciﬁc validation set is not informative, and the aggregation happens only once. Finally, there is also the recent work from Seng et al. (2022) which performs hyperparameter optimization and neural architecture search in the federated setting. Similarly to prior works, it requires client-speciﬁc validation data in order to optimize the hyperparameters. 5 E XPERIMENTS 1 5 10 15 20 25 30 Num. inputs 0.7 0.8 0.9 1.0 Accuracy 0.5 0.4 0.3 0.2 0.1 0.0 Average Log-likelihoodPosthoc Diagonal Laplace Train T est 1 5 10 15 20 25 30 Num. inputs 103 102 Log Marginal Likelihood Estimate Partitioned (a) 0 400080001200016000200002400028000 Iteration 0 5 10 15 20 25Input Mask Element 0.00 0.25 0.50 0.75 1.00 Mask Probability  (b) Figure 2: (a) Demonstrating the ability of the marginal-likelihood inspired objective LML to identify the correct model on a toy input selection task. We plot the hyperparameter objective, train and test set accuracy, and train and test set log-likelihood with the partitioned networks method (left), and the post-hoc diagonal Laplace method (Immer et al., 2021) (right). (b) Mask over input features learned by partitioned networks over time. The ﬁrst 15 features are correctly identiﬁed. Input Selection To demonstrate that LML is a good objective for model selection that captures the desirable properties of the marginal likelihood, we ﬁrst deploy our method on the toy model selection task of Lyle et al. (2020): there the ﬁrst 15 features are informative, and the remaining15 are spurious y∼Bern (1 2 ) x = [ y+ ϵ1,...,y + ϵ15   Informative ,ϵ16,...,ϵ 30   Spurious ]⊺ ϵ1,...,ϵ 30 iid ∼N(0,1). We specify a ﬁxed mask over the inputs prior to training, where the ﬁrst Kinputs remain unmasked, and the remainder is masked. We expect that, given multiple models with different (ﬁxed) masks over the inputs, the proposed objective will be able to identify the correct one — i.e., the one that keeps only the informative features. We train multiple fully connected neural networks (MLPs) on a training set of 1000 examples using our method and compare the ﬁnal values of the LML objective. The results are shown in Figure 2a. LML correctly identiﬁes 15 input features as the optimum, and correlates well with test accuracy and log-likelihood. Training loss and training accuracy, on the other hand, cannot alone disambiguate whether to use 15 or more input features. Differentiable input selection We further show that we can learn the correct mask over the inputs in a differentiable manner using our method during a single training run. We parameterize a learnable mask over the inputs with a concrete Bernoulli distribution (Maddison et al., 2016) and treat the parameters of the mask distribution as a hyperparameter. We optimize them with respect to the proposed objective using our method. The evolution of the learned mask during training is shown in Figure 2b, where we see that we can correctly identify the ﬁrst 15 informative features. 6Published as a conference paper at ICLR 2023 Learning invariances through data-augmentations Following previous literature on learning soft invariances through learning data augmentations (Nabarro et al., 2022; van der Wilk et al., 2018; Benton et al., 2020; Schw ¨obel et al., 2021; Immer et al., 2022), we show that we can learn useful afﬁne image augmentations, resulting in gains in test accuracy. We specify afﬁne data augmentations as part of a probabilistic model as done by van der Wilk et al. (2018), averaging over multiple data augmentation samples during training and inference. This allows us to treat the data-augmentation distribution as a model hyperparameter rather than a training hyperparameter. For datasets, we consider MNIST, CIFAR10, TinyImagenet along with rotCIFAR10 and rotTinyImagenet, variants where the datapoints are randomly rotated at the beginning of training by angles sampled uniformly from [−π,π] (Immer et al., 2022). Experimental setup details are provided in Appendix I. For the CIFAR10 and rotCIFAR10 datasets, we consider as baselines standard training with no augmentations, Augerino (Benton et al., 2020) and Differentiable Laplace (Immer et al., 2022). Following Immer et al. (2022), we use ﬁxupResNets (Zhang et al., 2019) for the architectures. The results can be seen in Table 1. There, we observe that partitioned networks outperform all baselines in the case of CIFAR10 for both ResNet variants we consider. On RotCIFAR10, we observe that partitioned networks outperform the baseline and Augerino, but it is slightly outperformed by Differentiable Laplace, which optimizes additional prior hyperparameters. To demonstrate the scalability of partitioned networks, for the (rot)TinyImagenet experiments we consider a ResNet-50 architecture with GroupNorm(2). In Table 1 we observe that in both cases, partitioned networks learn invariances successfully and improve upon the baseline. Relative to Augerino, we observe that partitioned networks either improve (TinyImagenet) or are similar (rotTinyImagenet). Table 1: Test accuracy with learning afﬁne augmentations on (rot)CIFAR10 and (rot)TinyImagenet. Method Dataset Architecture Baseline Augerino Diff. Laplace Partitioned RotCIFAR10 ﬁxupResNet-8 54.2±0.4 75.4±0.2 79.5±0.6 79.1±0.0 CIFAR10 ﬁxupResNet-8 74.1±0.5 79.0±1.0 84.2±0.8 86.1±0.4 ﬁxupResNet-14 79.5±0.3 83.0±0.1 88.1±0.2 89.1±0.8 RotTinyImagenet ResNet-50 31.5±0.6 44.5±0.2 OOM7 43.9±0.3 TinyImagenet ResNet-50 44.2±0.5 41.1±0.2 OOM 48.6±0.0 Imbuing a model with useful invariances is particularly useful in the low-data regime, due to better data efﬁciency. To show that, we perform experiments where we artiﬁcially reduce the size of the training dataset. The results can be seen in Figure 3. We see that by learning augmentations with partitioned networks, we can drastically improve performance in the low-data regime upon a baseline that does not learn augmentations, while performing favorably against prior works in most cases. On MNIST, our method outperforms the last-layer marginal-likelihood method (last-layer ML) by Schw¨obel et al. (2021) in the large data regime but underperforms in the low-data regime. That is likely to be expected, as their work ﬁts a Gaussian Process (GP) at the last layer (Wilson et al., 2016), which is better tailored for the low-data regime and results into a more ﬂexible model (due to the GP corresponding to an additional, inﬁnite width, layer). Since the MNIST-CNN is sufﬁciently small to ﬁt multiple networks into memory, we also compare to a variant of our method where, instead of partitioning a single network, we train Cdifferent networks where network kis ﬁt on data D1:k. This serves as an upper bound on the performance of the partitioned networks. We see that by partitioning a single network, we can achieve almost equivalent accuracy. On CIFAR10, partitioned networks outperform all other works on all data sizes we considered. On RotCIFAR10, partitioned networks perform again favourably, but they are marginally outperformed by differentiable Laplace in the low-data regime. Compared to partitioned networks where we only optimize augmentations, differentiable Laplace also optimizes the precision of a Gaussian prior over the weights, which better combats overﬁtting in the low-data regime. On both the TinyImagenet and rotTinyImagenet experiments we observe that partitioned networks either outperform or are similar to the baselines on all data sizes considered. 7Out of memory error on a 32GB Nvidia V100. 7Published as a conference paper at ICLR 2023 5000 20000 60000 Dataset Size 0.98 0.99T est Accuracy Baseline Last-layer ML Augerino Diff. Laplace Partitioned (Ens.) Partitioned (a) MNIST 0.25 0.50 0.75 1 5 10 20 50 Dataset Size (x1000) 0.25 0.50 0.75  (b) (rot)CIFAR10 0.25 0.50 10 50 100 Dataset Size (x1000) 0.2 0.4  (c) (rot)TinyImagenet Figure 3: Learning afﬁne data augmentations on subsets of data. (b) uses a ﬁxupResNet-8 architecture whereas (c) a ResNet-50 architecture. (b,c) Top: normal dataset, bottom: rotated dataset. Comparisons to traditional training / validation split We further perform comparisons between partitioned networks and the more traditional training/validation split (denoted as validation set optimization) with additional ﬁnetuning to the task of learning data augmentations. This is realized as follows; we partition 20kCIFAR10 examples into training and validation data of speciﬁc proportions. We then either train a partitioned network (along with the hyperparameters on LML) on these two chunks of data or train a standard network on the training set while using the validation set loss to obtain gradients for the data augmentation hyperparameters. For the validation set optimization baseline, once the hyperparameters are optimized, the resulting network is ﬁnetuned on the whole dataset for 20 epochs. The results for varying chunk proportions are provided in Table 2. Table 2: Learning afﬁne augmentations with ﬁxupResNet-14 on subset of CIFAR-10 (20kexamples). NaN denotes that a run crashed. Chunk Proportions Method [0.3,0.7] [0 .5,0.5] [0 .7,0.3] [0 .8,0.2] [0 .9,0.1] Partitioned 82.9%±0.3 83.0%±0.01 83.7%±0.2 84.0%±0.6 84.6%±0.05 Validation set optim. NaN 78.9%±0.04 81.5%±0.2 82.6%±0.1 83.4%±0.1 +Finetune NaN 81.3%±0.09 82.5%±0.2 83.5%±0.1 83.8%±0.3 Table 3: Learning a feature extractor (ﬁrst 2 out of 3 stages of a Wide ResNet-20) as a hyperparameter on CIFAR10. Method Chunk Proportions Test accuracy Validation set optim. [0.9,0.1] 59 .6%±0.6 Partitioned [0.1,0.8,0.1] 87.3%±0.8 We can see that partitioned net- works (that do not employ ad- ditional ﬁnetuning) outperform validation set optimization with ﬁnetuning in all settings we tried. The gap does get smaller when we move to the more tra- ditional 90/10 splits for train- ing/validation: a 10% proportion for validation data is enough to optimize a handful of hyper- parameters (just 6 scalars). To corroborate this claim, we set up an additional experiment; we use a Wide ResNet-20 on the full CIFAR10 dataset, where the ﬁrst two out of the three stages (13 convolution layers) are considered as hyperparameters. The results for this setting can be seen in Table 3. We see that 10% validation data are not enough, and the validation set optimization baseline performs poorly. This is in contrast to partitioned networks, where with three chunks, we can learn all of these hyperparameters successfully. Note that, compared to Augerino, applying partitioned networks to this setting is straightforward. To apply Augerino, one would have to come up with a metric that can be used to regularize the feature extractor towards “higher invariance”. Partitioned networks for federated learning We consider federated learning (FL) (McMahan et al., 2017), a setting where data is distributed across many clients. In this setting, there are system properties that make hyperparameter optimization especially challenging (Wang et al., 2021). More speciﬁcally, obtaining a validation set and performing multiple training runs with different 8Published as a conference paper at ICLR 2023 hyperparameter settings might not be possible due to the additional communication and computation costs, and transient client availability (clients join and leave the training process at any time). Optimizing hyperparameters together with the model parameters in a single run is therefore especially beneﬁcial (Wang et al., 2021), and partitioned networks are a good ﬁt for FL. We extend our centralized experimental setup to FL by splitting all N clients into Cnon-overlapping chunks, such that each chunk is understood as the union of all clients’ data shards that belong to that chunk. During federated training, a client belonging to chunk ksequentially optimizes partitions wk:C through sub-networks w(k:C) s and computes a gradient wrt. the hyperparameters ψ. Note that partitions w1:k remain unchanged and do not need to be communicated back to the server. This reduction in upload costs is a welcome property for FL, where upload costs can bottleneck system design. The server receives the (hyper-) parameter updates, averages them, and applies the result as a “gradient” to the server-side model in the traditional federated manner (Reddi et al., 2020). For partitioned networks, the hyperparameters that we optimize are the data augmentation parameters and, since we also include dropout in these architectures, the dropout rates (with the concrete relaxation from Maddison et al. (2016)). As a baseline, we consider the standard federated training without learning hyperparameters (denoted as FedAvg) as well as learning the augmentation parameters with Augerino Benton et al. (2020). Please see Appendix J for a detailed explanation of our FL setup. Table 4 summarizes our results using different sub-sets and variations of MNIST and CIFAR10, where we also included rotMNIST Larochelle et al. (2007) as another dataset. We can see that partitioned networks allow training models that generalize better than both FedAvg and FedAvg with Augerino, at reduced communication costs. Especially when the true data-generating process and underlying source of non-i.i.d.-ness are explicitly accounted for — here in the form of rotation — the beneﬁts of learning the augmentations with partitioned networks become apparent. For example, we observe that on the rotated datasets, partitioned networks learn to correctly increase the rotation angle. Table 4: Validation accuracy averaged over the last10 evaluations, each 10 rounds apart; standard- error is computed across 4 random seeds. All datasets are adapted to the federated setting and are synthetically split to be non-i.i.d. sampled as described in Appendix J.2. Dataset & size ↑MNIST ↑RotMNIST ↓Upload Method 1.25k 5k 50k 1.25k 5k 50k [%] FedAvg 95.4%±0.1 97.4%±0.1 99.0%±0.1 80.5%±0.0 90.4%±0.5 96.8%±0.1 100 FedAvg + Augerino 94.2%±0.5 96.4%±0.1 99.1%±0.0 79.5%±0.3 89.0%±2.0 95.3%±0.2 100 FedAvg + Partitioned97.0%±0.1 98.3%±0.0 99.2%±0.1 85.7%±0.9 93.5%±0.6 97.8%±0.1 77 ↑CIFAR10 ↑RotCIFAR10 ↓Upload 1.25k 5k 45k 1.25k 5k 45k [%] FedAvg 50.2%±0.4 64.5%±0.3 79.2%±0.7 35.6%±0.3 45.2%±0.1 53.9%±1.1 100 FedAvg + Augerino 49.9%±0.8 65.0%±0.2 79.9%±0.4 36.1%±0.2 45.0%±0.2 56.4%±0.7 100 FedAvg + Partitioned50.8%±1.0 64.8%±0.4 81.5%±0.5 37.1%±0.2 45.3%±0.3 60.6%±0.2 91 6 D ISCUSSION We propose partitioned networks as a new method for hyperparameter optimization inspired by the marginal likelihood objective. It provides a general and scalable solution to ﬁnding hyperparameters in a single training run without requiring access to a validation set while introducing less additional overhead to the training task than existing approaches. We showed that partitioned networks are applicable on a wide range of tasks; they can identify the correct model on illustrative toy examples, they can learn data augmentations in a way that improves data efﬁciency, they can optimize general feature extractors as hyperparameters and they can also optimize dropout rates. In the federated setting, partitioned networks allow us to overcome practical challenges, reduce the communication overhead and obtain better models. The notion of partitioned networks we propose in this work is novel to the literature and an orthogonal approach to many existing hyperparameter tuning algorithms. Like any other method, partitioned networks come with their own limitations, e.g., needing a partitioning strategy. We expand upon them in appendix H. We hope to see our method successfully reducing the need to perform hyperparameter search through repeated training and thereby contribute to the community’s effort to reduce its carbon footprint. 9Published as a conference paper at ICLR 2023 REFERENCES Gregory Benton, Marc Finzi, and Andrew G Wilson. Augerino, github, com- mit=fd542eb90ac6b1c0959156c1f6ad2ba8719d8572. https://github.com/g-benton/ learning-invariances/. (on page 18) Gregory Benton, Marc Finzi, Pavel Izmailov, and Andrew G Wilson. Learning invariances in neural networks from training data. Advances in neural information processing systems, 33:17605–17616, 2020. (on page 2, 5, 7, 9, 16, 18, 20, 24, 25) Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment: Learning augmentation policies from data. arXiv preprint arXiv:1805.09501, 2018. (on page 5) Kamal Dys. Cifar10 resnet: 90+% accuracy;less than 5 min. https://www.kaggle.com/code/ kmldas/cifar10-resnet-90-accuracy-less-than-5-min . Accessed: 2022-09- 17. (on page 26) Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural architecture search: A survey. The Journal of Machine Learning Research, 20(1):1997–2017, 2019. (on page 5) Edwin Fong and Chris C Holmes. On the marginal likelihood and cross-validation. Biometrika, 107 (2):489–496, 2020. (on page 2) Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. arXiv preprint arXiv:1803.03635, 2018. (on page 4) Yarin Gal, Jiri Hron, and Alex Kendall. Concrete dropout.Advances in neural information processing systems, 30, 2017. (on page 2) Pascal Germain, Francis Bach, Alexandre Lacoste, and Simon Lacoste-Julien. Pac-bayesian theory meets bayesian inference. Advances in Neural Information Processing Systems, 29, 2016. (on page 2) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectiﬁers: Surpassing human-level performance on imagenet classiﬁcation. In Proceedings of the IEEE international conference on computer vision, pages 1026–1034, 2015. (on page 23) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In European conference on computer vision , pages 630–645. Springer, 2016. (on page 23) Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016. (on page 23) Alexander Immer and Tycho F. A. van der Ouderaa. Learning invariances with laplace ap- proximations (lila), github, commit=c0c4a09a109ed2f55e887def7d854b8a3a2330ef. https: //github.com/tychovdo/lila. (on page 17) Alexander Immer, Matthias Bauer, Vincent Fortuin, Gunnar R¨atsch, and Khan Mohammad Emtiyaz. Scalable marginal likelihood estimation for model selection in deep learning. In International Conference on Machine Learning, pages 4563–4573. PMLR, 2021. (on page 2, 5, 6, 24) Alexander Immer, Tycho F. A. van der Ouderaa, Gunnar R¨atsch, Vincent Fortuin, and Mark van der Wilk. Invariance learning in deep neural networks with differentiable laplace approximations, 2022. URL https://arxiv.org/abs/2202.10638. (on page 2, 5, 7, 15, 16, 17, 18, 22, 23, 24, 25) Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning, pages 448–456. PMLR, 2015. (on page 23) Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. (on page 4) 10Published as a conference paper at ICLR 2023 Mikhail Khodak, Renbo Tu, Tian Li, Liam Li, Maria-Florina F Balcan, Virginia Smith, and Ameet Talwalkar. Federated hyperparameter tuning: Challenges, baselines, and connections to weight- sharing. Advances in Neural Information Processing Systems, 34:19184–19197, 2021. (on page 5, 6) Hugo Larochelle, Dumitru Erhan, Aaron Courville, James Bergstra, and Yoshua Bengio. An empirical evaluation of deep architectures on problems with many factors of variation. In Proceedings of the 24th international conference on Machine learning, pages 473–480, 2007. (on page 9) Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge. CS 231N, 7(7):3, 2015. (on page 24) Yonggang Li, Guosheng Hu, Yongtao Wang, Timothy Hospedales, Neil M Robertson, and Yongxin Yang. Dada: Differentiable automatic data augmentation. arXiv preprint arXiv:2003.03780, 2020. (on page 5) Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. arXiv preprint arXiv:1806.09055, 2018. (on page 2, 5) Jonathan Lorraine, Paul Vicol, and David Duvenaud. Optimizing millions of hyperparameters by implicit differentiation. CoRR, abs/1911.02590, 2019. URL http://arxiv.org/abs/ 1911.02590. (on page 1, 2, 5) Sanae Lotﬁ, Pavel Izmailov, Gregory Benton, Micah Goldblum, and Andrew Gordon Wilson. Bayesian model selection, the marginal likelihood, and generalization. In Kamalika Chaud- huri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine Learning , volume 162 of Pro- ceedings of Machine Learning Research , pages 14223–14247. PMLR, 17–23 Jul 2022. URL https://proceedings.mlr.press/v162/lotfi22a.html. (on page 4) Clare Lyle, Lisa Schut, Robin Ru, Yarin Gal, and Mark van der Wilk. A bayesian perspective on training speed and model selection. Advances in Neural Information Processing Systems , 33: 10396–10408, 2020. (on page 2, 3, 5, 6) David JC MacKay. Information theory, inference and learning algorithms. Cambridge university press, 2003. (on page 2) Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous relaxation of discrete random variables. arXiv preprint arXiv:1611.00712, 2016. (on page 6, 9, 26, 27) David A McAllester. Some pac-bayesian theorems. In Proceedings of the eleventh annual conference on Computational learning theory, pages 230–234, 1998. (on page 2) Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efﬁcient learning of deep networks from decentralized data. In Artiﬁcial intelli- gence and statistics, pages 1273–1282. PMLR, 2017. (on page 2, 8) Shakir Mohamed, Mihaela Rosca, Michael Figurnov, and Andriy Mnih. Monte carlo gradient estimation in machine learning. J. Mach. Learn. Res., 21(132):1–62, 2020. (on page 6) Seth Nabarro, Stoil Ganev, Adri`a Garriga-Alonso, Vincent Fortuin, Mark van der Wilk, and Laurence Aitchison. Data augmentation in bayesian neural networks and the cold posterior effect. In Uncertainty in Artiﬁcial Intelligence, pages 1434–1444. PMLR, 2022. (on page 5, 7) Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and R. Gar- nett, editors, Advances in Neural Information Processing Systems 32, pages 8024–8035. Curran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/9015-pytorch-an- imperative-style-high-performance-deep-learning-library .pdf. (on page 22) 11Published as a conference paper at ICLR 2023 Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Kone ˇcn`y, Sanjiv Kumar, and H Brendan McMahan. Adaptive federated optimization. arXiv preprint arXiv:2003.00295, 2020. (on page 9, 26, 27) Robin Ru, Clare Lyle, Lisa Schut, Miroslav Fil, Mark van der Wilk, and Yarin Gal. Speedy performance estimation for neural architecture search. Advances in Neural Information Processing Systems, 34:4079–4092, 2021. (on page 3) Akiyoshi Sannai, Masaaki Imaizumi, and Makoto Kawano. Improved generalization bounds of group invariant/equivariant deep networks via quotient feature spaces. In Uncertainty in Artiﬁcial Intelligence, pages 771–780. PMLR, 2021. (on page 3) Pola Schw¨obel, Martin Jørgensen, Sebastian W. Ober, and Mark van der Wilk. Last layer marginal likelihood for invariance learning, 2021. URL https://arxiv.org/abs/2106.07512. (on page 2, 5, 7, 15, 16, 23, 24, 26, 27) Jonas Seng, Pooja Prasad, Devendra Singh Dhami, and Kristian Kersting. Hanf: Hyperparameter and neural architecture search in federated learning. arXiv preprint arXiv:2206.12342, 2022. (on page 6) Jure Sokolic, Raja Giryes, Guillermo Sapiro, and Miguel Rodrigues. Generalization error of invariant classiﬁers. In Artiﬁcial Intelligence and Statistics, pages 1094–1103. PMLR, 2017. (on page 3) Mark van der Wilk, Matthias Bauer, ST John, and James Hensman. Learning invariances using the marginal likelihood. Advances in Neural Information Processing Systems, 31, 2018. (on page 2, 5, 7, 16) Bastiaan S Veeling, Jasper Linmans, Jim Winkens, Taco Cohen, and Max Welling. Rotation equiv- ariant cnns for digital pathology. In International Conference on Medical image computing and computer-assisted intervention, pages 210–218. Springer, 2018. (on page 15) Jianyu Wang, Zachary Charles, Zheng Xu, Gauri Joshi, H Brendan McMahan, Maruan Al-Shedivat, Galen Andrew, Salman Avestimehr, Katharine Daly, Deepesh Data, et al. A ﬁeld guide to federated optimization. arXiv preprint arXiv:2107.06917, 2021. (on page 5, 8, 9) Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3):229–256, 1992. (on page 6) Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, and Eric P Xing. Deep kernel learning. In Artiﬁcial intelligence and statistics, pages 370–378. PMLR, 2016. (on page 7) Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European conference on computer vision (ECCV), pages 3–19, 2018. (on page 23, 26) Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016. (on page 23) Hongyi Zhang, Yann N Dauphin, and Tengyu Ma. Fixup initialization: Residual learning without normalization. arXiv preprint arXiv:1901.09321, 2019. (on page 7, 18, 23) Yi Zhou, Parikshit Ram, Theodoros Salonidis, Nathalie Baracaldo, Horst Samulowitz, and Heiko Ludwig. Flora: Single-shot hyper-parameter optimization for federated learning. arXiv preprint arXiv:2112.08524, 2021. (on page 6) 12Published as a conference paper at ICLR 2023 A LML IS A LOWER -BOUND TO THE MARGINAL LIKELIHOOD In this section, we show that the objective in equation 2 is a lower-bound on the marginal likelihood, under a mild assumption on each approximate posterior qk(w). The aim is to approximate: log p(D|ψ) = C∑ k=1 log p(Dk|D1:k−1,ψ) (5) Our partitioned approximation is given by: C∑ k=1 Eqk−1(w) [log p(Dk|w,ψ)] (6) We can get the equation for the gap between quantities in 5 and 6: gap = C∑ k=1 log p(Dk|D1:k−1,ψ) − C∑ k=1 Eqk−1(w) [log p(Dk|w,ψ)] (7) = C∑ k=1 Eqk−1(w) [log p(Dk|D1:k−1,ψ) −log p(Dk|w,ψ)] (8) = C∑ k=1 Eqk−1(w) [ log p(Dk|D1:k−1,ψ) p(Dk|w,ψ) ] (9) = C∑ k=1 Eqk−1(w)  log p(w,Dk|D1:k−1)    p(w|D1:k,ψ)p(Dk|D1:k−1,ψ)p(w|D1:k−1,ψ) p(w|D1:k,ψ)p(Dk|w,ψ)p(w|D1:k−1,ψ)   p(w,Dk|D1:k−1)   (10) = C∑ k=1 Eqk−1(w) [ log p(w|D1:k−1,ψ) p(w|D1:k,ψ) ] (11) = C∑ k=1 DKL [qk−1(w)∥p(w|D1:k,ψ)] −DKL [qk−1(w)∥p(w|D1:k−1,ψ)] (12) We now make two assumptions • DKL [qk−1(w)∥p(w|D1:k,ψ)] ≥DKL [qk(w)∥p(w|D1:k,ψ)]. This is motivated from the fact that qk(w) is trained on all data chunks D1:k so it is expected to be a better approxima- tion to the posterior p(w|D1:k), compared to qk−1(w) which is only trained on D1:k−1. • DKL [qC−1(w)∥p(w|D1:C,ψ)] ≥DKL [q0(w)∥p(w)]. Since we are free to choose the approximate posterior before seeing any data — q0(w)—, we can set it to be equal to the prior p(w) which, together with the positivity of the KL divergence, trivially satisﬁes this assumption. Therefore, by rearranging Eq. 12 and using our two assumptions we have that the gap is positive gap =−DKL [q0(w)∥p(w)] +DKL [qC−1(w)∥p(w|D1:C,ψ)] + C∑ k=1 DKL [qk−1(w)∥p(w|D1:k,ψ)] −DKL [qk(w)∥p(w|D1:k,ψ)] ≥0, (13) and our approximation is a lower bound to the marginal likelihood, i.e., log p(D|ψ) ≥ C∑ k=1 Eqk−1(w) [log p(Dk|w,ψ)] . (14) 13Published as a conference paper at ICLR 2023 B P ARTITIONED NETWORKS AS A SPECIFIC APPROXIMATION TO THE MARGINAL LIKELIHOOD In this section of the appendix, we show that the partitioned neural networks we presented in the paper are a particular instance of the approximation to the marginal likelihood shown in equation 2. Consider a dataset Dcomprised of C shards, i.e. D= (D1,..., DC), along with a model, e.g., a neural network, with parameters w ∈RDw, a prior p(w) = ∏Dw j=1 N(wj|0,λ) and a likelihood p(D|w,ψ) with hyperparameters ψ. Assuming a sequence over the dataset chunks, we can write out the true marginal likelihood as log p(D|ψ) = ∑ k log p(Dk|D1:k−1,ψ) = ∑ k log Ep(w|D1:k−1,ψ) [p(Dk|w,ψ)] (15) ≥ ∑ k Ep(w|D1:k−1,ψ) [log p(Dk|w,ψ)] . (16) Since the true posteriors p(w|D1:j,ψ) for j ∈{1,...,C }are intractable, we can use variational inference to approximate them with qφj(w) for j ∈{1,...,C }, with φj being the to-be-optimized parameters of the j’th variational approximation. Based on the result from Appendix A, whenqφj(w) are optimized to match the respective posteriors p(w|D1:j,ψ), we can use them to approximate the marginal likelihood as log p(D|ψ) ≥ ∑ k Eqφk−1 (w) [log p(Dk|w,ψ)] . (17) Partitioned networks correspond to a speciﬁc choice for the sequence of approximating distribution families qφk(w). Speciﬁcally, we partition the parameter space w into Cchunks, i.e., wk ∈RDwk, such that ∑ kDwk = Dw, and we associate each parameter chunk wk with a data shard Dk. Let rφk(wk) be base variational approximations over wk with parameters φk. Each approximate distribution qφk(w) is then deﬁned in terms of these base approximations, i.e., qφk(w) =   k−1∏ j=1 rφj(wj)  rφk(wk) ( K∏ m=k+1 r0(wm) ) (18) where r0(·) is some base distribution with no free parameters. In accordance with the assumptions in appendix A, we can then ﬁt each qφk(w) by minimising the KL-divergence to p(w|D1:k,ψ) – the posterior after seeing kchunks: DKL [qφk(w)∥p(w|D1:k,ψ)] =−Eqφk(w)[log p(D1:k|w,ψ)] +DKL [qφk(w)∥p(w)] + logp(D1:k|ψ) (19) (20) Finding the optimum with respect to φk: arg min φk DKL [qφk(w)∥p(w|D1:k,ψ)] = (21) = arg min φk −Eqφk(w)[log p(D1:k|w,ψ)] +DKL [qφk(w)∥p(w)] (22) = arg min φk −Eqφk(w)[log p(D1:k|w,ψ)] + DKL     k−1∏ j=1 rφj(wj)  rφk(wk) ( K∏ m=k+1 r0(wm) ) ∥ K∏ i p(wi)   (23) = arg min φk −Eqφk(w)[log p(D1:k|w,ψ)] +DKL [rφk(wk)∥p(wk)] . (24) We can now obtain partitioned networks by assuming that rφk(wk) = N(wk|φk,νI) for k ∈ {1,...,C }, r0(w) = N(w|ˆw,νI), with ˆw being the parameters at initialization (i.e., before we 14Published as a conference paper at ICLR 2023 update them on data) and taking ν →0, i.e., in machine-precision, the weights are deterministic. As noted in Section I.1, we scale the weight-decay regularizer forφk (whenever used) differently for each partition k, such that it can be interpreted as regularization towards a prior. In the experiments where we do not regularize φk according to p(wk) when we optimize them, this implicitly corresponds to λ→∞ (i.e. the limiting behaviour when the variance of p(w) goes to inﬁnity), which makes the contribution of the regularizer negligible. C P ARTITIONING SCHEMES There are several ways in which we could aim to partition the weights of a neural network. Throughout the experimental section 5, we partition the weights by assigning a ﬁxed proportion of weights in each layer to a given partition at random. We call this approach random weight partitioning. We also experimented with other partitioning schemes. For example, we tried assigning a ﬁxed proportion of a layer’s outputs (e.g., channels in a convolution layer) to each partition. All weights in a given layer that a speciﬁc output depends on would then be assigned to that partition. We call this approach node partitioning. Both approaches are illustrated in Figure 4. One beneﬁt of the node partitioning scheme is that it makes it possible to update multiple partitions with a single batch; This is because we can make a forward pass at each linear or convolutional layer with the full network parameters w, and, instead, mask the appropriate inputs and outputs to the layer to retrieve an equivalent computation to that with w(k) s . The gradients also need to be masked on the backward pass adequately. No such simpliﬁcation is possible with the random weight partitioning scheme; if we were to compute a backward pass for a single batch of examples using different subnetworks for each example, the memory overhead would grow linearly with the number of subnetworks used. In initial experiments, we found both random weight partitioning and node partitioning performed similarly. In the experimental section 5, we focused on the former, as it’s easier to reason about with relation to e.g., dropout. Throughout this work, partitioning happens prior to initiating training, and remains ﬁxed throughout. It might also be possible to partition the network parameters dynamically during training, which we leave for future work.   w11 w12 w13 w14 w15 w21 w22 w23 w24 w25 w31 w32 w33 w34 w35 w41 w42 w43 w44 w45 w51 w52 w53 w54 w55 w61 w62 w63 w64 w65   (a) Random weight partitioned In node assignment    Out node assignment      w11 w12 w13 w14 w15 w21 w22 w23 w24 w25 w31 w32 w33 w34 w35 w41 w42 w43 w44 w45 w51 w52 w53 w54 w55 w61 w62 w63 w64 w65   (b) Node partitioned Figure 4: Figures showing how the weights within a single weight matrix W ∈R6×5 for a linear layer would be partitioned. D S CALABILITY In the paper, we claim that our method is scalable compared to Schw¨obel et al. (2021) and Immer et al. (2022). What constraints the scalability of the mentioned prior works, however, is different. For the Last Layer Marginal Likelihood, although the approach works on small datasets such as PCAM (Veeling et al., 2018) and MNIST, the authors report that they were unable to learn invariances 15Published as a conference paper at ICLR 2023 on larger datasets such as CIFAR10. In (Schw¨obel et al., 2021, section 7), they explore the issue of scalability in more detail, and showcase that last layer marginal likelihood is insufﬁcient. Differentiable Laplace performs well, even on more complex datasets, such as CIFAR10. Their scalability, however, is limited by the computational and memory complexity of their method, which we go into in more detail in the section below. D.1 C OMPLEXITY ANALYSIS First, we consider the scalability of our algorithm in terms of computational and memory complexity. In particular, we show that our method scales much more favourably compared to Differentiable Laplace (Immer et al., 2022). We present our analysis for a feed-forward model of depth L, with layer widths D8. In order to directly compare to Immer et al. (2022) and Benton et al. (2020), we consider the complexities in the invariance learning setup (Benton et al., 2020; van der Wilk et al., 2018) withSaugmentation samples. In other experiments, hyperparameter optimization setups, S can be taken to be 1. The notation is summarized in Table 5. N Number of datapoints in dataset D NB Batch size S Number of augmentation samples9 C Output size (number of classes) D Feedforward network layer widths L Feedforward network depth P Number of parameters (s.t. O(P) =O(LD2 + DC)) Table 5: Notation for complexity analysis. We consider the computational and memory costs of 1) obtaining a gradient with respect to the parameters 2) obtaining a gradient with respect to the hyperparameters, and 3) computing the value of the model/hyperparameter selection objective for each method. All analysis assumes computation on a Monte-Carlo estimate of the objective on a single batch of data. In Tables 6 and 7, we assume that C <D, and hence, for the clarity of comparison, sometimes fold a factor depending Cinto a factor depending on Dif it’s clearly smaller. This hiding of the factors was only done for Differentiable Laplace, which is the worst scaling method. D.1.1 C OMPUTATIONAL COMPLEXITY Param. Backward Hyperparam. Backward Hyperparam. Objective Partitioned O(NBPS) O(NBPS) O(NBPS) Augerino O(NBPS) O(NBPS) O(NBPS) Diff. Laplace O(NBPS) O(NBPS+NCP +NCDLS + LD3) O(NPS + NCP +NCDLS + LD3) Table 6: Computational Complexities. The two terms highlighted for Augerino can be computed in a single backward pass. For Differentiable Laplace, the terms in blue can be amortised over multiple hyperparameter backward passes. That is why, in their method, they propose updating the hyperparameters once every epoch on (possibly) multiple batches of data, rather than once on every batch as is done with Partitioned Networks and Augerino. 8This is for the ease of comparison. Same upper bound complexities will hold for a network of variable sizes Dℓ for ℓ∈[L], where D= maxℓ Dℓ 9Only relevant for invariance learning. 16Published as a conference paper at ICLR 2023 D.1.2 M EMORY COMPLEXITY The memory complexities for Partitioned Networks, Augerino, and Differentiable Laplace are shown in Table 7. Crucially, the memory required to update the hyperparameters for Differentiable Laplace scales as O(NBSLD2 + P), with a term depending on the square of the network widths. This can become prohibitively expensive for larger models, and is likely the reason why their paper only considers experiments on architectures with widths up to a maximum of 256. Param. Backward Hyperparam. Backward Hyperparam. Objective Partitioned O(NBSLD+ P) O(NBSLD+ P) O(NBSD+ P) Augerino O(NBSLD+ P) O(NBSLD+ P) O(NBSD+ P) Diff. Laplace O(NBSLD+ P) O(NBSLD2 + P) O(NBSLD2 + P) Table 7: Memory Complexities. Differences are highlighted in red. D.2 P RACTICAL SCALABILITY A complexity analysis in big- Onotation as provided by us in the previous sections allows to understand scalability in the limit, but constant terms that manifest in practice are still of interest. In this section we aim present real timing measurements for our method in comparison to Augerino and Differential Laplace, and elaborate on what overhead might be expected with respect to standard neural network training. The empirical timings measurements on an NVIDIA RTX 3080-10GB GPU are shown in Table 8. We used a batch-size of 250, 200 for the MNIST and CIFAR10 experiments respectively, and 20 augmentation samples, just like in our main experiments in Table 1 and Figure 3. As can be seen, the overhead from using a partitioned network is fairly negligible compared to a standard forward and backward pass. The one difference compared to Augerino is, however, the fact that a separate forward-backward pass needs to be made to update the hyperparameters and regular parameters. This necessity is something that can be side-stepped with alternative partitioning schemes, as preliminarily mentioned in appendix C, and is an interesting direction for future research. MNIST CIFAR10 Method CNN ﬁxupResNet-8 ﬁxupResNet-14 Augerino ×1 ×1 ×1 Diff. Laplace† Param. ×1 ×1 ×1 Hyperparam. ×2015.6 ×18.2 - Partitioned Param. ×1.08 ×1.17 ×1.21 Hyperparam. ×1.08 ×1.08 ×1.09 Table 8: Relative empirical time increase with respect to a regular parameter update during standard training. †The timing multipliers with respect to the baseline for ﬁxupResNet-8 are taken from the timings reported in (Immer et al., 2022, Appendix D.4). On the ResNet-14, we get an out-of- memory error during the hyperparam. update step with Differentiable Laplace on the NVIDIA RTX 3080-10GB GPU when running with the ofﬁcial codebase (Immer and van der Ouderaa). Memory Overhead Our proposed method’s memory consumption scales in the same way as Augerino or vanilla neural network training. There is a minor constant memory overhead due to having to store the assignment of weights to partitions. In general, only log Cbits per parameter are necessary to store the partition assignments, whereCis the number of chunks. In our implementation, we only consider C <28, and hence store the assignments in byte tensors. This means that the partitioned models require extra 25% memory for storing the parameters (when using 32bit ﬂoats to represent the parameters). 17Published as a conference paper at ICLR 2023 If the “default” weight values (i.e. those denoted ˆwi in Figure 1) are non-zero, there is an additional overhead to storing those as well, which doubles the memory required to store the parameters. We observed there was no difference in performance when setting default weight values to 0 in architectures in which normalisation layers are used (i.e. most modern architectures). As such, we would in general recommend to set the default weight values to 0. However, we found setting default values to the initialised values to be necessary for stability of training deep normalisation-free architectures such as the ﬁxup architectures (Zhang et al., 2019) we used to compare with Differentiable Laplace. As their method is not compatible with BatchNorm, we used these architectures in our experiments, and hence used non-zero default values. Lastly, if the default weight values are set to the (random) initialisation values, it is possible to write a cleverer implementation in which only the random seeds are stored in memory, and the default values are re-generated every time they are need in a forward and a backward pass. This would make the memory overhead from storing the default values negligible. E N OTE ON AUGERINO In replicating Augerino (Benton et al., 2020) within our code-base and experimenting with the implementation, we discovered a pathological behaviour that is partly mirrored by the authors of Immer et al. (2022). In particular, note that the loss function (Benton et al., 2020, Equation (5)) proposed by the authors is problematic in the sense that for any regularization strength λ> 0, the optimal loss value is negative inﬁnity since the regularization term (negative L2-norm) is unbounded. In our experiments we observe that for a sufﬁciently-large value of λand after a sufﬁcient number of iterations, this behaviour indeed appears and training diverges. In practice, using Augerino therefore necessitates either careful tuning of λ, clipping the regularisation term (a method that introduces yet another hyperparameter), or other techniques such as early stopping. In the open-source repository for the submission (Benton et al.), it can be seen that on many experiments the authors use a ”safe” variant of the objective, in which they clip the regulariser (without pass-through of the gradient) once the l∞-norm of any of the hyperparameters becomes larger than an arbitrary threshold. Without using this adjustment, we found that the Augerino experiments on MNIST crashed every time with hyperparameters diverging to inﬁnity. F S ENSITIVITY TO PARTITIONING F.1 S ENSITIVITY IN TERMS OF FINAL PERFORMANCE (a) 2 chunks [8,1] [4,1] [1,1] [1,4] [1,8] Param. Proportions [8,1] [4,1] [1,1] [1,4] [1,8]Chunk Proportions 0.990 0.992 0.994 0.996 T est Accuracy  (b) 3 chunks [8,4,1][3,2,1][1,1,1][1,2,3][1,4,8] Param. Proportions [8,4,1] [3,2,1] [1,1,1] [1,2,3] [1,4,8]Chunk Proportions 0.990 0.992 0.994 0.996 T est Accuracy  (c) 4 chunks [8,4,2,1][4,3,2,1][1,1,1,1][1,2,3,4][1,2,4,8] Param. Proportions [8,4,2,1] [4,3,2,1] [1,1,1,1] [1,2,3,4] [1,2,4,8]Chunk Proportions 0.990 0.992 0.994 0.996 T est Accuracy Figure 5: Learning afﬁne augmentations on MNIST with a CNN ﬁt on all data. x- and y−ticks denote the ratios of parameters/datapoints assigned to each partition/chunk respectively. Partitioned networks allow for learning hyperparameters in a single training run, however, they introduce an additional hyperparameter in doing so: the partitioning scheme. The practitioner needs to choose the number of chunks C, the relative proportions of data in each chunk, and the relative proportions of parameters assigned to each of the Cpartitions wk. We investigate the sensitivity to the partitioning scheme here. We show that our results are fairly robust to partitioning through a grid-search over parameter partitions and chunk proportions on the afﬁne augmentation learning task on MNIST with the CNN architecture we use throughout this work. 18Published as a conference paper at ICLR 2023 (a) 2 chunks [8,1] [4,1] [1,1] [1,4] [1,8] Param. Proportions [8,1] [4,1] [1,1] [1,4] [1,8]Chunk Proportions 0.987 0.988 0.989 0.990 T est Accuracy  (b) 3 chunks [8,4,1][3,2,1][1,1,1][1,2,3][1,4,8] Param. Proportions [8,4,1] [3,2,1] [1,1,1] [1,2,3] [1,4,8]Chunk Proportions 0.987 0.988 0.989 0.990 T est Accuracy  (c) 4 chunks [8,4,2,1][4,3,2,1][1,1,1,1][1,2,3,4][1,2,4,8] Param. Proportions [8,4,2,1] [4,3,2,1] [1,1,1,1] [1,2,3,4] [1,2,4,8]Chunk Proportions 0.987 0.988 0.989 0.990 T est Accuracy Figure 6: Learning afﬁne augmentations on RotMNIST with a CNN ﬁt on all data. x- and y−ticks denote the ratios of parameters/datapoints assigned to each partition/chunk respectively. Figure 5 and Figure 6 show the test accuracy for a choice of chunk and parameter proportions across two, three and four chunks. The proportions are to be read as un-normalized distributions; for example, chunk proportions set to [1,8] denotes that there are 8×as many datapoints assigned to the second compared to the ﬁrst. Each conﬁguration was run with 2 random seeds, and we report the mean across those runs in the ﬁgure. The same architecture used was the same as for the main MNIST experiments in section 5 (see Appendix I.4 for details). We observe that for various partition/dataset-chunking conﬁgurations, all models achieve fairly similar ﬁnal test accuracy. There is a trend for models with a lot of parameters assigned to later chunks, but with few datapoints assigned to later chunks, to perform worse. While these results show a high level of robustness against the choice of additional hyperparameters introduced by our method, these results do show an opportunity or necessity for choosing the right partitioning scheme in order to achieve optimal performance. F.2 S ENSITIVITY IN TERMS OF HYPERPARAMETERS FOUND To compare how the different partitioning schemes qualitatively impact the hyperparameters that the method identiﬁes, we also retrain vanilla models from scratch using the hyperparameter values found using partitioned networks. Namely, we take the ﬁnal value of the hyperparameters learned with partitioned networks with a given partitioning scheme, and plot the ﬁnal test set accuracy of a vanilla neural network model trained from scratch with those hyperparameters. The results are shown in Figures 7 and 8. (a) 2 chunks [8,1] [4,1] [1,1] [1,4] [1,8] Param. Proportions [8,1] [4,1] [1,1] [1,4] [1,8]Chunk Proportions 0.990 0.992 0.994 0.996 T est Accuracy  (b) 3 chunks [8,4,1][3,2,1][1,1,1][1,2,3][1,4,8] Param. Proportions [8,4,1] [3,2,1] [1,1,1] [1,2,3] [1,4,8]Chunk Proportions 0.990 0.992 0.994 0.996 T est Accuracy  (c) 4 chunks [8,4,2,1][4,3,2,1][1,1,1,1][1,2,3,4][1,2,4,8] Param. Proportions [8,4,2,1] [4,3,2,1] [1,1,1,1] [1,2,3,4] [1,2,4,8]Chunk Proportions 0.990 0.992 0.994 0.996 T est Accuracy Figure 7: Standard neural network trained onMNIST with a CNN ﬁt on all data, with hyperparameters found using partitioned networks with chunk and parameter proportions corresponding to those in Figure 5. x- and y−ticks denote the ratios of parameters/datapoints assigned to each partition/chunk respectively. G H OW GOOD ARE THE HYPERPARAMETERS FOUND ? Here we show that the hyperparameters found by partitioned networks are also a good set of hyperparameters for vanilla neural networks retrained from scratch. This section expands on the 19Published as a conference paper at ICLR 2023 (a) 2 chunks [8,1] [4,1] [1,1] [1,4] [1,8] Param. Proportions [8,1] [4,1] [1,1] [1,4] [1,8]Chunk Proportions 0.987 0.988 0.989 0.990 T est Accuracy  (b) 3 chunks [8,4,1][3,2,1][1,1,1][1,2,3][1,4,8] Param. Proportions [8,4,1] [3,2,1] [1,1,1] [1,2,3] [1,4,8]Chunk Proportions 0.987 0.988 0.989 0.990 T est Accuracy  (c) 4 chunks [8,4,2,1][4,3,2,1][1,1,1,1][1,2,3,4][1,2,4,8] Param. Proportions [8,4,2,1] [4,3,2,1] [1,1,1,1] [1,2,3,4] [1,2,4,8]Chunk Proportions 0.987 0.988 0.989 0.990 T est Accuracy Figure 8: Standard neural network trained on RotMNIST with a CNN ﬁt on all data, with hyper- parameters found using partitioned networks with chunk and parameter proportions corresponding to those in Figure 6. x- and y−ticks denote the ratios of parameters/datapoints assigned to each partition/chunk respectively. experiment in section F.2. To validate this claim, we conducted a fairly extensive hyperparameter search on the afﬁne augmentation learning task on RotMNIST; we trained 200 models by ﬁrst sampling a set of afﬁne augmentation parameters uniformly at random from a predeﬁned range 10, and then training a neural network model (that averages across augmentation samples at train and test time, as described in Benton et al. (2020)) with standard neural training with those hyperparameters ﬁxed throughout. In Figure 9, we plot the ﬁnal test-set performance of all the models trained with those hyperparameters sampled from a ﬁxed range. Alongside, we show the hyperparameters and test-set performance of the partitioned networks as they progress throughout training. The partitioned networks consistently achieve ﬁnal test-set performance as good as that of the best hyperparameter conﬁgurations iden- tiﬁed through extensive random sampling of the space. We also show the test-set performance of neural network models, trained through standard training, with hyperparameters ﬁxed to the ﬁnal hyperparameter values identiﬁed by the partitioned networks. The hyperparameters identiﬁed by partitioned networks appear to also be good for regular neural networks; the standard neural networks with hyperparameters identiﬁed through partitioned training also outperform the extensive random sampling of the hyperparameter space. Furthermore, Figure 9 shows that partitioned networks do learn full rotation invariance on the RotMNIST task, i.e. when full rotation invariance is present in the data generating distribution. 0.0 0.2 0.4 Translation X 0.96 0.97 0.98 0.99T est Accuracy 0.0 0.2 0.4 Translation Y 0 /2 Rotation Random Sampling Partitioned Runs Partitioned Runs Final Partitioned Runs Final - Retrained 0.0 0.2 0.4 Scale X 0.0 0.2 0.4 0.6 Scale Y 0.0 0.2 0.4 Shear Figure 9: The test-set performance plotted alongside (1D projections of) afﬁne augmentation hyper- parameters on the RotMNIST task with MNIST-CNN. Final test-set accuracies are shown for the hyperparameters sampled randomly for a neural network model trained through standard training with those hyperparameters ﬁxed (+). For multiple partitioned networks runs, the plot shows the progres- sion of the identiﬁed hyperparameters and the test-set performance through the training run ( ), as well as the ﬁnal hyperparameters and test-set performance ( ). Lastly, the plot also shows the ﬁnal test-set accuracies of models trained through standard training on the ﬁnal hyperparameters identiﬁed through partitioned training ( ). 10The ranges were: Uniform(0,π) for the maximum rotation, and Uniform(0,1 2 ) for all the remaining afﬁne augmentation parameters (maximum shear, maximum x−and y−translation, and maximum x−and y−scale). 20Published as a conference paper at ICLR 2023 H L IMITATIONS As mentioned in the main text, our method improves upon existing work, but also comes with its own limitations. Complexity Inherent to our method — as presented in e.g. Figure 1 — is the necessity for an additional forward-backward pass to update the hyperparameters. Consequently, hyperparameter optimization has additional costs which, however, are signiﬁcantly less than the computational costs of existing work, as we discuss in more detail in Appendix D.1 and the experimental section. Furthermore, empirically, partitioned networks usually require more training iterations to converge. Performance Assuming the optimal hyper-parameters are given, training the full, non-partitioned networks based on those optimal values can be expected to yield better performance compared to the ﬁnal model found by partitioned training. Partitioning the network inherently constrains the network capacity, causing some loss of performance. Opportunities for alleviating this performance loss while still enjoying single-run hyperparameter optimization through partitioned training will be left to future work. These include for example adjusting training rounds or increasing network capacity in the ﬁrst place. Partitioning While partitioned networks allows for automatic optimization of, intuitively, hard to tune hyperparameters, such as augmentation parameters, they come with the additional limitation of requiring to partition both the data and the model. This introduces an additional hyperparameter, namely, the partitioning strategy. While our default strategy of assigning more parameters and data to the ﬁrst chunk works reasonably well on all of the experiments we consider, if one targets obtaining the best possible performance on a given task, the partitioning strategy might need additional tuning. We provide some empirical results about the sensitivity to partitioning in appendix F.1 I E XPERIMENTAL DETAILS I.1 P ARTITIONED TRAINING Partitioned parameter update scheduling The gradient computation of Equation 3, as described in the main text, requires that the data-points for updating a given subnetwork w(k) s come from the appropriate dataset chunks (x,y) ∈D1:k for a chunk k. Depending on the partitioning scheme (Appendix C), evaluating different subnetworks for different chunks can or cannot be done in a single mini-batch. More speciﬁcally, the random weight-partitioning we chose for our experiments requires a separate mini-batch per subnetwork (in order to keep the memory cost the same as for standard neural network training). An immediate question arising from a chunked dataset and several partitions is to deﬁne the order and frequency of updates across subnetworks. In our experiments we deﬁne (non-uniform) splits of the training dataset Dacross the Cchunks, which requires a tailored approach to sampling the data. More speciﬁcally, for a given (normalized) ratio of chunk-sizes [u1,...,u C], each iteration of partitioned training proceeds as follows: 1. Sample a partition index k∼Cat(u1,...,u C) 2. Sample a mini-batch ˜Dof examples uniformly from D1:k. 3. Evaluate log p( ˜D|w(k) s ,ψ) using subnetwork w(k) s and 4. compute the (stochastic) gradient wrt. partition parameters wk (Eq. 3). 5. Update partition parameters wk using an optimizer, such as SGD or Adam. This sampling scheme results in a data-point (x,y) ∈Dk from earlier chunks to be sampled more often. Concretely, the probability that an example in chunk kwill be sampled is ∝∑ i≤kui. This is done so that each partition wk is updated with equal probability on each of the examples in D1:k As a result, we use with replacement sampling for the partitioned network training throughout the experimental section. 21Published as a conference paper at ICLR 2023 Gradient optimization of partitioned parameters A consequence of per-partition updates with the random weight partitioning scheme (appendix C) is that, for a chosen partition wk to update, all other partitions do not receive a gradient update. In other words, the gradient at each iteration is sparse. Consequently, many off-the-shelve momentum-based optimizers will not account correctly. Speciﬁcally, we implement modiﬁcations to the PyTorch Paszke et al. (2019) provided optimizers that allow us to track per-partition momenta, number of steps, etc. Note that this creates a disconnect between the number of iterations across all partitions and the number of iterations per-partition. Doing so, however aligns the computational cost of training the partitioned network parameters with the cost of training regular neural network parameters. Regardless, we do not alter the way learning-rate schedulers behave in our experiments and anneal learning-rates according to the total number of iterations. Similarly, we report the total number of iterations when comparing against baselines that update all network-parameters per iteration. While a simple gradient-accumulation scheme across mini-batches would result in a single gradient across all partitions, this approach inherently clashes with non-uniform partitioning [u1,...,u C]. Instead, we chose to sequentially apply gradients computed on a single partition, as described in the previous paragraphs. A further advantage of this approach is that learning progress made by updating partition wk immediately inﬂuences (and can improve) the prediction of subnetworks w(k) s ,w(k+1) s ,..., w(C) s . Gradient optimization of hyperparameters Our partitioned network scheme makes it easy to compute stochastic gradients of the hyperparameter objective LML in Eq. 4 using batch gradient descent optimization methods. After every update to a randomly sampled network partition (see previous paragraph), we update hyperparamters ψas follows: • sample a dataset chunk index k ∼Cat(u2 Z ,..., uC Z ). Ratios are re-normalized to exclude D1. • sample a mini-batch ˜Dof examples uniformly from Dk (Note the choice of Dk instead of D1:k). • Evaluate log p( ˜D|w(k−1) s ,ψ) using subnetwork w(k−1) s and • compute the (stochastic) gradient wrt. hyperparameters ψ(Eq. 4). • Update partition parameters ψusing an optimizer, such as SGD or Adam. The above sampling procedure yields an unbiased estimate of gradients in eq. 4. The fact that we optimize hyperparameters with gradients based on data from a single chunk at a time is again a consequence of the random weight-partitioning scheme for the partitioned networks. It is possible to compute gradients wrt. ψfor mini-batches with examples from multiple chunks at a time. With the random weight partitioning scheme, this would result in an increased memory overhead. Lastly, we could also accumulate gradients from different chunks, similarly to Immer et al. (2022), and this would likely result in a lower-variance estimate per update . It is also possible to reduce the computational overhead of evaluating two mini-batches per iteration (one for updates to wk, one for ψ) as we do in our experiments by interleaving hyperparameter updates at less frequent intervals. We leave an exploration of these design choices to future work. Throughout all experiments, except those in the federated settings (see section J), we use the same batch-size for the hyperparameter udpates as for the regular parameter updates. Weight-decay For partitioned networks, whenever using weight-decay, we scale the weight decay for earlier partitions with the reciprocal of the number of examples in chunks used to optimize them, following the diagonal Gaussian prior interpretation of weight-decay. This makes the training compatible with the variational interpretation in Appendix B. I.2 P ARTITIONED AFFINE TRANSFORMATIONS In Appendix C we described how we realize partitioned versions of fully-connected and convolutional layers. Design choices for other parameterized network layers used in our experiments are described below. 22Published as a conference paper at ICLR 2023 Normalization layers It is common-place in most architectures to follow a normalization layer (such as BatchNorm (Ioffe and Szegedy, 2015), GroupNorm (Wu and He, 2018)) with an element- wise or channel-wise, afﬁne transformation. Namely, such a transformation multiplies its input h by a scale vector s and adds a bias vector b: o = h ∗s + b. For random weight-partitioned networks, we parameterize such afﬁne transformations by deﬁning separate vectors {s1,..., sC} and {b1,..., bC}for each partition; the actual scale and bias used in a given subnetwork w(k) s are s(k) s = ∏ i∈{1,...,k}si and b(k) s = ∑ i∈{1,...,k}bi respectively. This ensures that the ﬁnal afﬁne transformation for each subnetwork w(k) s depends on the parameters in the previous partitions [1,...,k −1]. Doing so increases the parameter count for the partitioned networks in architectures that use those normalization layers by a negligible amount. Scale and bias in FixUp networks The FixUp paper (Zhang et al., 2019) introduces extra scales and biases into the ResNet architecture that transform the entire output of the layers they follow. We turn these into “partitioned” parameters using the same scheme as that for scales and biases of afﬁne transformations following normalization layers. For partitioned networks, through-out the paper, we match the proportion of parameters assigned to each partition kin each layer to the proportion of data examples in the corresponding chunk Dk. I.3 A RCHITECTURE CHOICES Input selection experiments We use a fully-connected feed-forward neural network with2 hidden layers of size [256,256], and with GeLU (Hendrycks and Gimpel, 2016) activation functions. We initialise the weights using the Kaiming uniform scheme (He et al., 2015). For partitioned networks, we use the random-weight partitioning scheme. Fixup Resnet For all experiments using FixUp ResNets we follow Immer et al. (2022); Zhang et al. (2019), and use a 3-stage ResNet with channel-sizes (16,32,64) per stage, with identity skip- connections for the residual blocks as described in He et al. (2016). The residual stages are followed by average pooling and a ﬁnal linear layer with biases. We use 2D average pooling in the residual branches of the downsampling blocks.We initialize all the parameters as described in Zhang et al. (2019). Wide ResNet For all experiments using a Wide-ResNet-N-D (Zagoruyko and Komodakis, 2016), with N being the depth and D the width multiplier, we use a 3 stage ResNet with channel-sizes (16D,32D,64D). We use identity skip-connections for the residual blocks, as described in He et al. (2016), also sometimes known as ResNetV2. ResNet-50 We use the ”V2” version of Wide ResNet as described in (Zagoruyko and Komodakis, 2016) and replace BatchNormalization with GroupNormalization using 2 groups. We use the ’standard’ with withD= 1and three stages of 8 layers for a 50-layer deep ResNet. We use ReLU activations for all ResNet experiments throughout. MNIST CNN For the MNIST experiments, we use the same architecture as Schw¨obel et al. (2021) illustrated in the replicated Table 9. Table 9: CNN architecture for MNIST experiments Layer Speciﬁcation 2D convolution channels=20, kernel size=(5,5), padding=2, activation=ReLU Max pooling pool size=(2,2), stride=2 2D convolution channels=50, kernel size=(5,5), padding=2, activation=ReLU Max pooling pool size=(2,2), stride=2 Fully connected units=500, activation=ReLU Fully connected units=50, activation=ReLU Fully connected units=10, activation=Softmax 23Published as a conference paper at ICLR 2023 I.4 T RAINING DETAILS Learning afﬁne augmentations For the parametrization of the learnable afﬁne augmentation strategies, we follow prior works for a fair comparison. More speciﬁcally, for our MNIST based setup we follow the parametrization proposed in Schw¨obel et al. (2021) whereas for our CIFAR10 based setup we use the generator parametrization from Immer et al. (2022). Input selection experiments For the model selection (non-differentiable) input selection exper- iments, we train all variants with Adam with a learning rate of 0.001 and a batch-size of 256 for 10000 iterations. For both Laplace and partitioned networks, we do early stopping based on the marginal likelihood objective (LML for partitioned networks). We use weight-decay 0.0003 in both cases. For the post-hoc Laplace method, we use the diagonal Hessian approximation, following the recommendation in (Immer et al., 2021). For partitioned networks, we divide the data and parameters into 8 chunks of uniform sizes. We plot results averaged across 3 runs. Mask learning for input selection experiment We use the same optimizer settings as for the input selection experiment. We train for 30000 iterations, and optimize hyperparameters with Adam with a learning rate of 0.001. We divide the data and parameters into 4 uniform chunks. MNIST experiments We follow Schw¨obel et al. (2021), and optimize all methods with Adam with a learning rate of 0.001, no weight decay, and a batch-size of 200. For the partitioned net- works and Augerino results, we use 20 augmentation samples. We use an Adam optimizer for the hyperparameters with a learning rate of 0.001 (and default beta parameters). For Augerino on MNIST, we use the “safe” variant, as otherwise the hyperparameters and the loss diverge on every training run. We elaborate on this phenomenon in Appendix E. Otherwise, we follow the recommended settings from (Benton et al., 2020) and Immer et al. (2022), namely, a regularization strength of 0.01, and a learning rate for the hyperparameters of 0.05. For both MNIST and CIFAR experiments, we found it beneﬁcial to allocate more data to either the earlier, or the later, chunks. Hence, we use 3 chunks with [80%,10%,10%] split of examples for all MNIST and CIFAR experiments. CIFAR variations experiments We again follow Immer et al. (2022), and optimize all ResNet models with SGD with a learning rate of 0.1 decayed by a factor of 100×using Cosine An- nealing, and momentum of 0.9 (as is standard for ResNet models). We use a batch-size of 250. We again use Adam for hyperparameter optimization with a learning rate of 0.001 (and default beta parameters). We train our method for [2400,8000,12000,20000,40000] iterations on subsets [1000,5000,10000,20000,50000] respectively for CIFAR-10, just as in (Immer et al., 2022). For all methods, we used a weight-decay of 1e−4. For partitioned networks, we increase the weight decay for earlier partitions with the square root of the number of examples in chunks used to optimize them, following the diagonal Gaussian prior interpretation of weight-decay. We use3 chunks with [80%,10%,10%] split of examples. For RotCIFAR-10 results, we noticed our method hasn’t fully converged (based on training loss) in this number of iterations, and so we doubled the number of training iterations for the RotMNIST results. This slower convergence can be explained by the fact that, with our method, we only update a fraction of the network parameters at every iteration. TinyImagenet experiments Our experiments with TinyImagenet (Le and Yang, 2015) closely follow the setting for the CIFAR-10 experiments described above. Images are of size64x64 pixels, to be classiﬁed into one of 200 classes. The training-set consists of 100000 images and we compare our method against baselines on subset of [10000,50000,100000] datapoints. For the standard version of TinyImagenet, we train for [80000,80000,40000] steps respectively and for the rotated version of TinyImagenet we train for 120000 steps for all subset sizes. We tuned no other hyper-parameters compared to the CIFAR-10 setup and report our method’s result for a partitioning with[80%,20%] across 2 chunks after ﬁnding it to perform slightly better than a [80%,10%,10%] split across 3 chunks in a preliminary comparison. 24Published as a conference paper at ICLR 2023 Fine-tuning experiments For the ﬁne-tuning experiments in table 2, we trained a FixUp ResNet-14 on a subset of 20000 CIFAR10 examples, while optimizing afﬁne augmentations (following afﬁne augmentations parameterization in (Benton et al., 2020)). We used the same optimizer settings as for all other CIFAR experiments, and trained for 80000 iterations, decaying the learning rate with Cosine Annealing for the ﬁrst 60000 iterations. For ﬁne-tuning of validation-set optimization models, we used SGD with same settings, overriding only the learning rate to 0.01. We tried a learning rate of 0.01 and 0.001, and selected the one that was most favourable for the baseline based on the test accuracy. We also tried training on the full CIFAR-10 dataset, but found that all methods ended up within a standard error of each other when more than 70% of the data was assigned to the ﬁrst chunk (or training set, in the case of validation set optimization). This indicates that CIFAR-10 is sufﬁciently larger that, when combined with afﬁne augmentation learning and the relatively small ResNet-14 architecture used, using the extra data in the 2nd partition (or the validation set) results in negligible gains. I.5 D ATASETS Input selection synthetic dataset For the input selection dataset, we sample 3000 datapoints for the training set as described in section 5, and we use a fresh sample of 1000 datapoints for the test set. RotMNIST Sometimes in the literature, RotMNIST referes to a speciﬁc subset of 12000 MNIST examples, whereas in other works, the full dataset with 60000 examples is used. In this work, following (Benton et al., 2020; Immer et al., 2022) we use the latter. J F EDERATED PARTITIONED TRAINING In this section, we explain how partitioned networks can be applied to the federated setting, as well as the experimental details. J.1 P ARTITIONED NETWORKS IN FL In order to apply partitioned networks to the federated setting, we randomly choose a partition for each client such that the marginal distribution of partitions follows a pre-determined ratio. A given chunk Dk therefore corresponds to the union of several clients’ datasets. Analogous to how “partitioned training” is discussed in the main text and Appendix I, we desire each partition wk to be updated on chunks D1:k. Equation 3 in the main text explains which data chunks are used to compute gradients wrt. parameter partition wk. An analogous perspective to this objective is visualized by the exemplary algorithm in Figure 1 and asks which partitions are inﬂuenced (i,e., updated) by data from chunk Dk: A data chunk Dk is used to compute gradients wrt. partitions wk:C through subnetworks w(k) s to w(C) s respectively. Consequently, a client whose dataset is assigned to chunkDk can compute gradients for all partitions wk:C. Updating network partitions Due to the weight-partitioned construction of the partitioned neural networks, it is not possible to compute gradients with respect to all partitions in a single batched forward-pass through the network. Additionally, a change to the partition parameters wk directly inﬂuences subnetworks w(k+1) s to w(C) s . In order to avoid the choice of ordering indices kto Cfor the client’s local update computation, we update each partition independently while keeping all other partitions initialised to the server-provided values that the client received in that round t: Denote Di,k as the dataset of client iwhere we keep index kto emphasize the client’s assignment to chunkk. Further denote wt+1 j,i as the partition wt j after having been updated by client ion dataset Di,k. wt+1 j,i = arg max wj log p ( Di,k|(wt 1,..., wt j, ˆwt j+1,..., ˆwt j+C),ψ ) ∀j ∈[k,C], (25) where the details of optimization are explained in the following section. We leave an explo- ration for different sequential updating schemes to future work. The ﬁnal update communi- cated by a client to the server consists of the concatenation of all updated parameter partitions 25Published as a conference paper at ICLR 2023 wt+1 .,i = concat(wt+1 k,i ,..., wt+1 C,i ). Note that partitions (wt 1,..., wt k−1) have not been modiﬁed and need not be communicated to the server. The resulting communication reductions make partitioned networks especially attractive to FL as data upload from client to server poses a signiﬁcant bottleneck. In practice, we expect the beneﬁts of these communication reductions to outweigh the additional computation burden of sequentially computing gradients wrt., to multiple partitions. The server receives wt+1 .,i from all clients that participates in round t, computes the delta’s with the global model and proceeds to average them to compute the server-side gradient in the typical federated learning fashion (Reddi et al., 2020). Updating hyperparameters The computation of gradients on a clientiwrt. ψis a straight-forward extension of equation 4 and the exemplary algorithm of Figure 1: ∇ψLML (Di,k,ψ) ≈∇ψlog p ( Di,k|w(t+1),(k−1) s,i ,ψ ) , (26) where Di,k corresponds to client i’s local dataset which is assigned to chunk k and w(t+1),(k−1) s corresponds to the (k−1)’th subnetwork after incorporating all updated partitionsw(t+1),(k−1) s,i = concat(wt 1,..., wt k−1,wt+1 k,i ,..., wt+1 C,i ). Note that we compute a full-batch update to ψin MNIST experiments and use a batch-size equal to the batch-size for the partitioned parameter updates for CIFAR10. Upon receiving these gradients from all clients in this round, the server averages them to form a server-side gradient. Conceptually, this approach to updating ψcorresponds to federated SGD. J.2 F EDERATED SETUP Non-i.i.d. partitioning For our federated experiments, we split the 50kMNIST and 45kCIFAR10 training data-points across 100 clients in a non-i.i.d. way to create the typical challenge to federated learning experiments. In order to simulate label-skew, we follow the recipe proposed in Reddi et al. (2020) with α= 1.0 for CIFAR10 and α= 0.1 for MNIST. Note that with α= 0.1, most clients have data corresponding to only a single digit. For our experiments on rotated versions of CIFAR10 and MNIST, we sample a degree of rotation per data-point and keep it ﬁxed during training. In order to create a non-i.i.d partitioning across the clients, we bin data-points according to their degree of rotation into 10 bins and sample using the same technique as for label-skew with α = 0.1 for both datasets. Learning curves are computed using the 10k MNIST and 5k CIFAR10 validation data-points respectively. For the rotated dataset experiments, we rotate the validation set in the same manner as the training set. Architectures and experimental setup We use the convolutional network provided at Schw¨obel et al. (2021) for MNIST and the ResNet-9 (Dys) model for CIFAR10 but with group normaliza- tion (Wu and He, 2018) instead of batch normalization. We include (learnable) dropout using the continuous relaxation proposed at Maddison et al. (2016) between layers for both architectures. We select 3 chunks for MNIST with a [0.7,0.2,0.1] ratio for both, client-assignments and parameter- partition sizes. For CIFAR10, we found a [0.9,0.1] split across 2 sub-networks to be beneﬁcial. In addition to dropout logits, ψencompasses parameters for afﬁne transformations, i.e., shear, trans- lation, scale and rotation. We report results after 2kand 5krounds, respectively, and the expected communication costs as a percentage of the non-partitioned baseline. Shared setting In order to elaborate on the details to reproduce our results, we ﬁrst focus on the settings that apply across all federated experiments. We randomly sample the corresponding subset of 1.25k, 5kdata-points from the full training set and keep that selection ﬁxed across experiments (i,e., baselines and partitioned networks) as well as seeds. The subsequent partitioning across clients as detailed in the previous paragraph is equally kept ﬁxed across experiments and seeds. Each client computes updates for one epoch of its local dataset, which, for the low data regimes of 1.25k data-points globally, results in single update per client using the entire local dataset. We averaged over 10 augmentation samples for the forward pass in both training and inference. MNIST & RotMNIST For 5k data-points and correspondingly 50 data-points on average per client, most clients perform a single update step. A small selection of clients with more than 64 data- 26Published as a conference paper at ICLR 2023 points performs two updates per round. For the experiments using the full dataset and a mini-batch size of 64, each client performs multiple updates per round. After initial exploration on the baseline FedAvg task, we select a local learning-rate of 5e−2 and apply standard SGD. The server performs Adam Reddi et al. (2020) with a learning rate of 1e−3 for the model parameters. We keep the other parameters of Adam at their standard PyTorch values. We ﬁnd this setting to generalize to the partitioned network experiments but found a higher learning rate of3e−3 for the hyper-parameters to be helpful. We chose the convolutional network from Schw¨obel et al. (2021) with (learned) dropout added between layers. The model’s dropout layers are initialized to drop10% of hidden activations. For the baseline model we keep the dropout-rate ﬁxed and found 10% to be more stable than 30%. CIFAR10 & RotCIFAR10 We ﬁx a mini-batch size of 32, leading to multiple updates per client per round in both, the full dataset regime as well as the5kdata-points setting. Similarly to the MNIST setting, we performed an initial exploration of hyperparameters on the baseline FedAvg task and use the same ones on partitioned networks. We used dropout on the middle layer of each block which was initialized to 0.1 for both the baseline and partitioned networks and whereas partitioned networks optimized it with LML and the concrete relaxation from Maddison et al. (2016), the baseline kept it ﬁxed. For the server side optimizer we used Adam with the default betas and a learning rate of 1e−2, whereas for the hyperparameters we used Adam with the default betas and a learning rate of 1e−3. In both cases we used an ϵ= 1e−7. For the local optimizer we used SGD with a learning rate of 10−0.5 and no momentum. J.3 MNIST LEARNING CURVES In Figure 10 we show learning curves for the three considered dataset sizes on the standard MNIST task. Each learning curve is created by computing a moving average across 10 evaluations, each of which is performed every 10 communication rounds, for each seed. We then compute the average and standard-error across sees and plot those values on the y-axis. On the x-axis we denote the total communication costs (up- and download) to showcase the partitioned networks reduction in communication overhead. We see that especially for the low dataset regime, training has not converged yet and we expect performance to improve for an increased number of iterations. Figure 10: Learning curves for MNIST experiments on 1.25k, 5kand 50kdata-points respectively. 27",
      "meta_data": {
        "arxiv_id": "2304.14766v1",
        "authors": [
          "Bruno Mlodozeniec",
          "Matthias Reisser",
          "Christos Louizos"
        ],
        "published_date": "2023-04-28T11:24:41Z",
        "pdf_url": "https://arxiv.org/pdf/2304.14766v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper proposes a novel and efficient hyperparameter optimization method, called Partitioned Neural Networks, that is inspired by marginal likelihood and requires no validation data. It allows for optimizing a variety of hyperparameters (e.g., neural architecture elements, data augmentation strategies, dropout rates) in a single training run, significantly reducing computational expense compared to other marginal likelihood approximation methods for neural networks. The method also addresses the challenges of hyperparameter optimization in federated learning by reducing communication overhead and achieving better model generalization, especially in low-data regimes and for non-i.i.d. data.",
        "methodology": "The core methodology involves partitioning the training data into K data shards and a neural network model into K parameter partitions. Each parameter partition is optimized only on specific data shards (D1:k for partition wk). By combining these partitions, subnetworks are formed. The hyperparameter optimization objective, LML, is defined as the 'out-of-training-sample' loss of a subnetwork, i.e., the loss on data shards unseen by that subnetwork (Dk for subnetwork trained on D1:k-1). This objective is an approximation to a lower-bound on the marginal likelihood. Parameter updates for each partition are interleaved with hyperparameter updates, computed via stochastic gradient descent. The network weights can be partitioned randomly or by assigning fixed proportions of a layer's outputs to each partition. Default weight values for unoptimized partitions are typically set to initialization values or zero.",
        "experimental_setup": "The method was validated on several tasks: a toy input selection task (identifying informative features), learning invariances through data augmentations, optimizing feature extractors as hyperparameters, and hyperparameter optimization in federated learning. Datasets included MNIST, CIFAR10, TinyImagenet, and their rotated variants (rotCIFAR10, rotTinyImagenet, rotMNIST), often with reduced training data sizes. Architectures used were MLPs, Fixup ResNets (ResNet-8, ResNet-14), ResNet-50 with GroupNorm, Wide ResNet-20, and a CNN for MNIST. Baselines for comparison included standard training (no augmentations), Augerino, Differentiable Laplace, Last-layer ML, traditional training/validation split optimization (with fine-tuning), and FedAvg. Federated learning experiments involved non-i.i.d. data partitioning (label-skew and rotation-skew) across 100 clients.",
        "limitations": "The method introduces additional computational costs due to the necessity of an extra forward-backward pass for hyperparameter updates, although these are generally lower than existing marginal likelihood methods. Partitioned networks typically require more training iterations to converge. The partitioning of the network inherently constrains its capacity, potentially leading to some performance loss compared to a full, non-partitioned network trained with optimal, known hyperparameters. The practitioner must also choose a partitioning strategy (number of chunks, relative data/parameter proportions), which introduces an additional hyperparameter, though empirical results suggest robustness to these choices.",
        "future_research_directions": "Future work could explore alternative partitioning schemes to potentially side-step the necessity for a separate forward-backward pass for hyperparameters. Investigating methods to alleviate the performance loss caused by network partitioning, such as adjusting training rounds or increasing network capacity, is another direction. Dynamically partitioning network parameters during training could also be explored."
      }
    },
    {
      "title": "Federated Hyperparameter Tuning: Challenges, Baselines, and Connections to Weight-Sharing",
      "abstract": "Tuning hyperparameters is a crucial but arduous part of the machine learning\npipeline. Hyperparameter optimization is even more challenging in federated\nlearning, where models are learned over a distributed network of heterogeneous\ndevices; here, the need to keep data on device and perform local training makes\nit difficult to efficiently train and evaluate configurations. In this work, we\ninvestigate the problem of federated hyperparameter tuning. We first identify\nkey challenges and show how standard approaches may be adapted to form\nbaselines for the federated setting. Then, by making a novel connection to the\nneural architecture search technique of weight-sharing, we introduce a new\nmethod, FedEx, to accelerate federated hyperparameter tuning that is applicable\nto widely-used federated optimization methods such as FedAvg and recent\nvariants. Theoretically, we show that a FedEx variant correctly tunes the\non-device learning rate in the setting of online convex optimization across\ndevices. Empirically, we show that FedEx can outperform natural baselines for\nfederated hyperparameter tuning by several percentage points on the\nShakespeare, FEMNIST, and CIFAR-10 benchmarks, obtaining higher accuracy using\nthe same training budget.",
      "full_text": "Federated Hyperparameter Tuning: Challenges, Baselines, and Connections to Weight-Sharing Mikhail Khodak, Renbo Tu, Tian Li Carnegie Mellon University {khodak,renbo,tianli}@cmu.edu Liam Li Hewlett Packard Enterprise me@liamcli.com Maria-Florina Balcan, Virginia Smith Carnegie Mellon University ninamf@cs.cmu.edu,smithv@cmu.edu Ameet Talwalkar Carnegie Mellon University & Hewlett Packard Enterprise talwalkar@cmu.edu Abstract Tuning hyperparameters is a crucial but arduous part of the machine learning pipeline. Hyperparameter optimization is even more challenging in federated learn- ing, where models are learned over a distributed network of heterogeneous devices; here, the need to keep data on device and perform local training makes it difﬁcult to efﬁciently train and evaluate conﬁgurations. In this work, we investigate the prob- lem of federated hyperparameter tuning. We ﬁrst identify key challenges and show how standard approaches may be adapted to form baselines for the federated setting. Then, by making a novel connection to the neural architecture search technique of weight-sharing, we introduce a new method, FedEx, to accelerate federated hyperparameter tuning that is applicable to widely-used federated optimization methods such as FedAvg and recent variants. Theoretically, we show that a FedEx variant correctly tunes the on-device learning rate in the setting of online convex optimization across devices. Empirically, we show that FedEx can outperform natural baselines for federated hyperparameter tuning by several percentage points on the Shakespeare, FEMNIST, and CIFAR-10 benchmarks—obtaining higher accuracy using the same training budget. 1 Introduction Federated learning (FL) is a popular distributed computational setting where training is performed locally or privately [ 33, 39] and where hyperparameter tuning has been identiﬁed as a critical problem [20]. Although general hyperparameter optimization has been the subject of intense study [4, 18, 29], several unique aspects of the federated setting make tuning hyperparameters especially challenging. However, to the best of our knowledge there has been no dedicated study on the speciﬁc challenges and solutions in federated hyperparameter tuning. In this work, we ﬁrst formalize the problem of hyperparameter optimization in FL, introducing the following three key challenges: 1. Federated validation data: In federated networks, as the validation data is split across devices, the entire dataset is not available at any one time; instead a central server is given access to some number of devices at each communication round, for one or at most a few runs of local training and validation. Thus, because the standard measure of complexity in FL is the number of communication rounds, computing validation metrics exactly dramatically increases the cost. 35th Conference on Neural Information Processing Systems (NeurIPS 2021). arXiv:2106.04502v2  [cs.LG]  4 Nov 2021Figure 1: FedEx can be applied to any local training-based FL method, e.g. FedAvg, by interleaving standard updates to model weights (computed by aggregating results of local training) with exponen- tiated gradient updates to hyperparameters (computed by aggregating results of local validation). 2. Extreme resource limitations: FL applications often involve training using devices with very limited computational and communication capabilities. Furthermore, many require the use of privacy techniques such as differential privacy that limit the number times user data can be accessed. Thus we cannot depend on being able to run many different conﬁgurations to completion. 3. Evaluating personalization: Finally, even with non-federated data, applying common hyperpa- rameter optimization methods to standard personalized FL approaches (such as ﬁnetuning) can be costly because evaluation may require performing many additional training steps locally. With these challenges1 in mind, we propose reasonable baselines for federated hyperparameter tuning by showing how to adapt standard non-federated algorithms. We further study the challenge of noisy validation signal due to federation, and show that simple state-estimation-based ﬁxes do not help. Our formalization and analysis of this problem leads us to develop FedEx, a method that exploits a novel connection between hyperparameter tuning in FL and the weight-sharing technique widely used in neural architecture search (NAS) [5, 37, 43]. In particular, we observe that weight-sharing is a natural way of addressing the three challenges above for federated hyperparameter tuning, as it incorporates noisy validation signal, simultaneously tunes and trains the model, and evaluates personalization as part of training rather than as a costly separate step. Although standard weight- sharing only handles architectural hyperparameters such as the choice of layer or activation, and not critical settings such as those of local stochastic gradient descent (SGD), we develop a formulation that allows us to tune most of these as well via the relationship between local-training and ﬁne-tuning- based personalization. This make FedEx a general hyperparameter tuning algorithm applicable to many local training-based FL methods, e.g. FedAvg [39], FedProx [34], and SCAFFOLD [21]. In Section 4, we next conduct a theoretical study of FedEx in a simple setting: tuning the client step-size. Using the ARUBA framework for analyzing meta-learning [22], we show that a variant of FedEx correctly tunes the on-device step-size to minimize client-averaged regret by adapting to the intrinsic similarity between client data. We improve the convergence rate compared to some past meta-learning theory [22, 28] while not depending on knowing the (usually unknown) task-similarity. Finally, in Section 5, we instantiate our baselines and FedEx to tune hyperparameters of FedAvg, FedProx, and Reptile, evaluating on three standard FL benchmarks: Shakespeare, FEMNIST, and CIFAR-10 [6, 39]. While our baselines already obtain performance similar to past hand-tuning, FedEx further surpasses them in most settings examined, including by 2-3% on Shakespeare. Related Work To the best of our knowledge, we are the ﬁrst to systematically analyze the formu- lation and challenges of hyperparameter optimization in the federated setting. Several papers have explored limited aspects of hyperparameter tuning in FL [8, 26, 41], focusing on a small number of hyperparameters (e.g. the step-size and sometimes one or two more) in less general settings (studying small-scale problems or assuming server-side validation data). In contrast our methods are able to tune a wide range of hyperparameters in realistic federated networks. Some papers also discussed the challenges of ﬁnding good conﬁgurations while studying other aspects of federated training [ 44]. We argue that it is critical to properly address the challenges of federated hyperparameter optimization in practical settings, as we discuss in detail in Section 2. Methodologically, our approach draws on the fact that local training-based methods such as FedAvg can be viewed as optimizing a surrogate objective for personalization [22], and more broadly leverages the similarity of the personalized FL setup and initialization-based meta-learning [ 7, 12, 19, 28]. While FedEx’s formulation and guarantees use this relationship, the method itself is general-purpose 1A further challenge we do not address is that of the time-dependency of federated evaluation, c.f. [10]. 2and applicable to federated training of a single global model. Many recent papers address FL personalization more directly [15, 32, 38, 47, 51]. This connection and our use of NAS techniques also makes research connecting NAS and meta-learning relevant [11, 36], but unlike these methods we focus on tuning non-architectural parameters. In fact, we believe our work is the ﬁrst to apply weight- sharing to regular hyperparameter search. Furthermore, meta-learning does not have the data-access and computational restrictions of FL, where such methods using the DARTS mixture relaxation [37] are less practical. Instead, FedEx employs the lower-overhead stochastic relaxation [9, 31], and its exponentiated update is similar to the recently proposed GAEA approach for NAS [30]. Running NAS itself in federated settings has also been studied [14, 17, 50]; while our focus is on non-architectural hyperparameters, in-principle our algorithms can also be used for federated NAS. Theoretically, our work makes use of the average regret-upper-bound analysis (ARUBA) frame- work [22] to derive guarantees for learning the initialization, i.e. the global model, while simul- taneously tuning the step-size of the local algorithm. The step-size of gradient-based algorithms has also been tuned on its own in the settings of data-driven algorithm design [16] and of statistical learning-to-learn [49]. 2 Federated Hyperparameter Optimization In this section we formalize the problem of hyperparameter optimization for FL and discuss the con- nection of its personalized variant to meta-learning. We also reviewFedAvg [39], a common federated optimization method, and present a reasonable baseline approach for tuning its hyperparameters. Global and Personalized FL In FL we are concerned with optimizing over a network of heteroge- neous clients i= 1,...,n , each with training, validation, and testing sets Ti, Vi, and Ei, respectively. We use LS(w) to denote the average loss over a dataset Sof some w-parameterized ML model, for w ∈Rd some real vector. For hyperparameter optimization, we assume a class of algorithms Alga hyperparameterized by a∈A that use federated access to training sets Ti to output some element of Rd. Here by “federated access\" we mean that each iteration corresponds to a communication round at which Alga has access to a batch of Bclients2 that can do local training and validation. Speciﬁcally, we assume Alga can be described by two subroutines with hyperparameters encoded by b∈B and c∈C, so that a= (b,c) and A= B×C . Here cencodes settings of a local training algorithm Locc that take a training set S and initialization w ∈Rd as input and outputs a model Locc(S,w) ∈Rd, while b sets those of an aggregation Aggb that takes the initialization w and outputs of Locc as input and returns a model parameter. For example, in standard FedAvg, Locc is T steps of gradient descent with step-size η and Aggb takes a weighted average of the outputs of Locc across clients; here c = (η,T ) and b = (). As detailed in the appendix, many FL methods can be decomposed this way, including well-known ones such as FedAvg [39], FedProx [34], SCAFFOLD [21], and Reptile [42] as well as more recent methods [1, 2, 32]. Our analysis and our proposed FedEx algorithm will thus apply to all of them, up to an assumption detailed next. Starting from this decomposition, the global hyperparameter optimization problem can be written as min a∈A n∑ i=1 |Vi|LVi(Alga({Tj}n j=1)) (1) In many cases we are also interested in obtaining a device-speciﬁc local model, where we take a model trained on all clients and ﬁnetune it on each individual client before evaluating. A key assumption we make is that the ﬁnetuning algorithm will be the same as the local training algorithm Locc used by Alga. This assumption can be justiﬁed by recent work in meta-learning that shows that algorithms that aggregate the outputs of local SGD can be viewed as optimizing for personalization using local SGD [22]. Then, in the personalized setting, the tuning objective becomes min a=(b,c)∈A n∑ i=1 |Vi|LVi(Locc(Ti,Alga({Tj}n j=1)) (2) Our approach will focus on the setting where the hyperparameters cof local training make up a signiﬁcant portion of all hyperparameters a= (b,c); by considering the personalization objective we will be able to treat such hyperparameters as architectural and thus apply weight-sharing. 3Algorithm 1: Successive halving algorithm (SHA) ap- plied to personalized FL. For the non-personalized ob- jective (1), replace LVti(wi) by LVti(wa). For random search (RS) with N samples, set η= N and R= 1. Input: distribution Dover hyperparameters A, elimination rate η∈N, elimination rounds τ0 = 0,τ1,...,τ R sample set of ηR hyperparameters H ∼D[ηR] initialize a model wa ∈Rd for each a∈H for elimination round r∈[R] do for setting a= (b,c) ∈H do for comm. round t= τr−1 + 1,...,τ r do for client i= 1,...,B do send wa,c to client wi ←Locc(Tti,wa) send wi,LVti(wi) to server wa ←Aggb(wa,{wi}B i=1) sa ←∑B i=1 |Vti|LVti(wi)/∑B i=1 |Vti| H ←{a∈H : sa ≤1 η-quantile({sa : a∈H})} Output: remaining a∈H and associated model wa Figure 2: Tuning FL with SHA but mak- ing elimination decisions based on vali- dation estimates using different discount factors. On both FEMNIST (top) and CI- FAR (bottom) using more of the validation data does not improve upon just using the most recent round’s validation error. Tuning FL Methods: Challenges and Baselines In the non-federated setting, the objective (1) is amenable to regular hyperparameter optimization methods; for example, a random search approach would repeatedly sample a setting afrom some distribution over A, run Alga to completion, and eval- uate the objective, saving the best setting and output [4]. With a reasonable distribution and enough samples this is guaranteed to converge and can be accelerated using early stopping methods [29], in which Alga is not always run to completion if the desired objective is poor at intermediate stages, or by adapting the sampling distribution using the results of previous objective evaluations [48]. As mentioned in the introduction, applying such methods to FL is inherently challenging due to 1. Federated validation data: Separating data across devices means we cannot immediately get a good estimate of the model’s validation performance, as we only have access to a possibly small batch of devices at a time. This means that decisions such as which models to ﬂag for early stopping will be noisy and may not fully incorporate all the available validation signal. 2. Extreme resource limitations: As FL algorithms can take a very long time to run in-practice due to the weakness and spotty availability of devices, we often cannot afford to conduct many training runs to evaluate different conﬁgurations. This issue is made more salient in cases where we use privacy techniques that only allow a limited number of accesses to the data of any individual user. 3. Evaluating personalization: While personalization is important in FL due to client heterogeneity, checking the performance of the current model on the personalization objective(2) is computation- ally intensive because computing may require running local training multiple times. In particular, while regular validation losses require computing one forward pass per data point, personalized losses require several forward-backward passes, making it many times more expensive if this loss is needed to make a tuning decision such as eliminating a conﬁguration from consideration. Despite these challenges, we can still devise sensible baselines for tuning hyperparameters in FL, most straightforward of which is to use a regular hyperparameter method but use validation data from a single round as a noisy surrogate for the full validation objective. Speciﬁcally, one can use random search (RS)—repeatedly evaluate random conﬁgurations—and a simple generalization called successive halving (SHA), in which we sample a set of conﬁgurations and partially run all of them for some number of communication rounds before eliminating all but the best 1 η fraction, repeating until only one conﬁguration remains. Note both are equivalent to a “bracket” in Hyperband [29] and their adaptation to FL is detailed in Algorithm 1. 2For simplicity the number of clients per round is ﬁxed, but all methods can be easily generalized to varying B. 4As shown in Section 5, SHA performs reasonably well on the benchmarks we consider. However, by using validation data from one round it may make noisy elimination decisions, early-stopping potentially good conﬁgurations because of a difﬁcult set of clients on a particular round. Here the problem is one of insufﬁcient utilization of the validation data to estimate model performance. A reasonable approach to use more is to try some type of state-estimation: using the performance from previous rounds to improve the noisy measurement of the current one. For example, instead of using only the most recent round for elimination decisions we can use a weighted sum of the performances at all past rounds. To investigate this, we study a power decay weighting, where a round is discounted by some constant factor for each time step it is in the past. We consider factors 0.0 (taking the most recent performance only, as before), 0.5, and 1.0 (taking the average). However, in Figure 2 we show that incorporating more validation data this way than is used by Algorithm 1 by default does not signiﬁcantly affect results. Thus we may need a better algorithm to use more of the validation signal, most of which is discarded by using the most recent round’s performance. We next proposeFedEx, a new method that does so by using validation on each round to update a client hyperparameters distribution used to sample conﬁgurations to send to devices. Thus it alleviates issue (1) above by updating at each step, not waiting for an elimination round as in RS or SHA. By simultaneously training the model and tuning (client) hyperparameters, it also moves towards a fully single-shot procedure in which we only train once (we must still run multiple times due to server hyperparameters), which would solve issue (2). Finally, FedEx addresses issue (3) by using local training to both update the model and to estimate personalized validation loss, thus not spending extra computation on this more expensive objective. 3 Weight-Sharing for Federated Learning We now present FedEx, a way to tune local FL hyperparameters. This section contains the general algorithm and its connection to weight-sharing; we instantiate it on several FL methods in Section 5. Weight-Sharing for Architecture Search We ﬁrst review the weight-sharing approach in NAS, which for a set Cof network conﬁgurations is often posed as the bilevel optimization min c∈C Lvalid(w,c) s.t. w ∈arg min u∈Rd Ltrain(u,c) (3) where Ltrain,Lvalid evaluate a single conﬁguration with the given weights. If, as in NAS, all hyperpa- rameters are architectural, then they are effectively themselves trainable model parameters [30], so we could instead consider solving the following “single-level\" empirical risk minimization (ERM): min c∈C,w∈Rd L(w,c) = min c∈C,w∈Rd Ltrain(w,c) + Lvalid(w,c) (4) Solving this instead of the bilevel problem (3) has been proposed in several recent papers [27, 30]. Early approaches to solving either formulation of NAS were costly due to the need for full or partial training of many architectures in a very large search space. The weight-sharing paradigm [43] reduces the problem to that of training a single architecture, a “supernet\" containing all architectures in the search space C. A straightforward way of constructing a supernet is via a “stochastic relaxation\" where the loss is an expectation w.r.t. sampling cfrom some distribution over C[9]. Then the shared weights can be updated using SGD by ﬁrst sampling an architecture cand using an unbiased estimate of ∇wL(w,c) to update w. The distribution over Cmay itself be adapted or stay ﬁxed. We focus on the former case, adapting some θ-parameterized distribution Dθ; this yields the stochastic relaxation objective min θ∈Θ,w∈Rd Ec∼DθL(w,c) (5) Since architectural hyperparameters are often discrete decisions, e.g. a choice of which of a ﬁxed number of operations to use, a natural choice of Dθ is as a product of categorical distributions over simplices. In this case, any discretization of an optimum θof the relaxed objective (5) whose support is in the support of θ will be an optimum of the original objective (4). A natural update scheme here is exponentiated gradient [ 25], where each successive θ is proportional to θ⊙exp(−η˜∇), ηis a step-size, and ˜∇an unbiased estimate of ∇θEc∼DθL(w,c) that can be computed using the re-parameterization trick [45]. By alternating this exponentiated update with the standard SGD update to w discussed earlier we obtain a simple block-stochastic minimization scheme that is guaranteed to converge, under certain conditions, to the ERM objective, and also performs well in practice [30]. 5The FedEx Method To obtain FedEx from weight-sharing we restrict to the case of tuning only the hyperparameters cof local training Locc.3 Our goal then is just to ﬁnd the best initialization w ∈Rd and local hyperparameters c∈C, i.e. we replace the personalized objective (2) by min c∈C,w∈Rd n∑ i=1 |Vi|LVi(Locc(Ti,w)) (6) Note Alga outputs an element of Rd, so this new objective is upper-bounded by the original (2), i.e. any solution will be at least as good for the original objective. Note also that for ﬁxed cthis is equivalent to the classic train-validation split objective for meta-learning withLoccas the base-learner. More importantly for us, it is also in the form of the r.h.s. of the weight-sharing objective (4), i.e. it is a single-level function of w and c. We thus apply a NAS-like stochastic relaxation: min θ∈Θ,w∈Rd n∑ i=1 |Vi|Ec∈DθLVi(Locc(Ti,w)) (7) In NAS we would now set the distribution to be a product of categorical distributions over different architectures, thus making θan element of a product of simplices and making the optimum of the original objective (6) equivalent to the optimum of the relaxed objective(7) as an extreme point of the simplex. Unlike in NAS, FL hyperparameters such as the learning rate are not extreme points of a simplex and so it is less clear what parameterized distribution Dθ to use. Nevertheless, we ﬁnd that crudely imposing a categorical distribution over k> 1 random samples from some distribution (e.g. uniform) over Cand updating θusing exponentiated gradient over the resulting k-simplex works well. We alternate this with updating w ∈Rd, which in a NAS algorithm involves an SGD update using an unbiased estimate of the gradient at the current w and θ. We call this alternating method for solving (7) FedEx and describe it for a general Alga consisting of sub-routines Aggb and Locc in Algorithm 2; recall from Section 2 that many FL methods can be decomposed this way, so our approach is widely applicable. FedEx has a minimal overhead, consisting only of the last four lines of the outer loop updating θ. Thus, as with weight-sharing, FedEx can be viewed as reducing the complexity of tuning local hyperparameters to that of training a single model. Each update to θrequires a step-size ηt and an approximation ˜∇of the gradient w.r.t. θ; for the latter we obtain an estimate ˜∇j of each gradient entry via the reparameterization trick, whose variance we reduce by subtracting a baseline λt. How we set ηt and λt is detailed in the Appendix. To see how FedEx is approximately optimizing the relaxed objective (7), we can consider the case where Alga is Reptile [42], which was designed to optimize some approximation of (6) for ﬁxed c, or equivalently the relaxed objective for an atomic distributionDθ. The theoretical literature on meta-learning [22, 23] shows that Reptile can be interpreted as optimizing a surrogate objective minimizing the squared distance between w and the optimum of each task i, with the latter being replaced by the last iterate in practice. It is also shown that the surrogate objective is useful for personalization in the online convex setting. 4 As opposed to this past work, FedEx makes two gradient updates in the outer loop, on two disjoint sets of variables: the ﬁrst is the sub-routine Aggb of Alga that aggregates the outputs of local training and is using the gradient of the surrogate objective, since the derivative of the squared distance is the difference between the initialization w and the parameter at the last iterate of Locc; the second is the exponentiated gradient update that is directly using an unbiased estimate of the derivative of the second objective w.r.t. the distribution parametersθ. Thus, roughly speaking FedEx runs simultaneous stochastic gradient descent on the relaxed objective (7), although for the variables w we are using a ﬁrst-order surrogate. In the theoretical portion of this work we employ this interpretation to show the approach works for tuning the step-size of online gradient descent in the online convex optimizations setting. Wrapping FedEx We can view FedEx as an algorithm of the form tuned by Algorithm 1 that implements federated training of a supernet parameter (w,θ), with the local training routine Loc including a step for sampling c∼Dθ and the server aggregation routine including an exponentiated update of θ. Thus we can wrap FedEx in Algorithm 1, which we ﬁnd useful for a variety of reasons: • The wrapper can tune the settings of bfor the aggregation step Aggb, which FedEx cannot. • FedEx itself has a few hyperparameters, e.g. how to set the baseline λt, which can be tuned. 3We will use some wrapper algorithm to tune the hyperparameters b of Aggb. 4Formally they study a sequence of upper bounds and not a surrogate objective, as their focus is online learning. 6Algorithm 2: FedEx Input: conﬁgurations c1,...,c k ∈C, setting bfor Aggb, schemes for setting step-size ηt and baseline λt, total number of steps τ ≥1 initialize θ1 = 1k/kand shared weights w1 ∈Rd for comm. round t= 1,...,τ do for client i= 1,...,B do send wt,θt to client sample cti ∼Dθt wti ←Loccti(Tti,wt) send wti,cti,LVti(wti) to server wt+1 ←Aggb(w,{wti}B i=1) ˜∇j ← ∑B i=1 |Vti|(LVti(wti)−λt)1cti=cj θt[j] ∑B i=1 |Vti| ∀j θt+1 ←θt ⊙exp(−ηt˜∇) θt+1 ←θt+1/∥θt+1∥1 Output: model w, hyperparameter distribution θ Figure 3: Comparison of the range of perfor- mance values attained using different pertur- bation settings. Although the range is much smaller for ϵ= 0.1 than for ϵ= 1.0 (the lat- ter is the entire space), it still covers a large (roughly 10-20%) range of different perfor- mance levels on both FEMNIST (left) and CIFAR (right). • By running multiple seeds and potentially using early stopping, we can run FedEx using more aggressive steps-sizes and the wrapper will discard cases where this leads to poor results. • We can directly compare FedEx to a regular hyperparameter optimization scheme run over the original algorithm, e.g. FedAvg, by using the same scheme to both wrap FedEx and tune FedAvg. • Using the wrapper allows us to determine the conﬁgurations c1,...,c k given to Algorithm 2 using a local perturbation scheme (detailed next) while still exploring the entire hyperparameter space. Local Perturbation It remains to specify how to select the conﬁgurations c1,...,c k ∈C to pass to Algorithm 2. While the simplest approach is to draw fromUnifk(C), we ﬁnd that this leads to unstable behavior if the conﬁgurations are too distinct from each other. To interpolate between sampling ci independently and setting them to be identical (which would just be equivalent to the baseline algorithm), we use a simple local perturbation method in which c1 is sampled from Unif(C) and c2,...,c k are sampled uniformly from a local neighborhood of C. For continuous hyperparameters (e.g. step-size, dropout) drawn from an interval [a,b] ⊂R the local neighborhood is [c±(b−a)ε] for some ε≥0, i.e. a scaled ε-ball; for discrete hyperparameters (e.g. batch-size, epochs) drawn from a set {a,...,b }⊂ Z, the local neighborhood is similarly {c−⌊(b−a)ε⌋,...,c + ⌈(b−a)ε⌉}; in our experiments we set ε= 0.1, which works well, but run ablation studies varying these values in the appendix showing that a wide range of them leads to improvement. Note that while local perturbation does limit the size of the search space explored by each instance of FedEx, as shown in Figure 3 the difference in performance between different conﬁgurations in the same ball is still substantial. Limitations of FedEx While FedEx is applicable to many important FL algorithms, those that cannot be decomposed into local ﬁne-tuning and aggregation should instead be tuned by one of our baselines, e.g. SHA. FedEx is also limited in that it is forced to rely on such algorithms as wrappers for tuning its own hyperparameters and certain FL hyperparameters such as server learning rate. 4 Theoretical Analysis for Tuning the Step-Size in an Online Setting As noted in Section 3, FedEx can be viewed as alternating minimization, with a gradient step on a surrogate personalization loss and an exponentiated gradient update of the conﬁguration distribution θ. We make this formal and prove guarantees for a simple variant of FedEx in the setting where the server has one client per round, to which the server sends an initialization to solve an online convex optimization (OCO) problem using online gradient descent (OGD) on a sequence of m adversarial convex losses (i.e. one SGD epoch in the stochastic case). Note we use “client” and “task” interchangeably, as the goal is a meta-learning (personalization) result. The performance measure here is task-averaged regret, which takes the average over τ clients of the regret they incur on its loss: 7¯Rτ = 1 τ τ∑ t=1 m∑ i=1 ℓt,i(wt,i) −ℓt,i(w∗ t) (8) Here ℓt,i is the ith loss of client t, wt,i the parameter chosen on its ith round from a compact parameter space W, and w∗ t ∈arg minw∈W ∑m i=1 ℓt,i(w) the task optimum. In this setting, the Average Regret-Upper-Bound Analysis (ARUBA) framework [22] can be used to show guarantees for a Reptile (i.e. FedEx with a server step-size) variant in which at each round the initialization is updated as wt+1 ←(1 −αt)wt + αtw∗ t for server step-size αt = 1/t. Observe that the only difference between this update and FedEx’s is that the task-toptimum w∗ t is used rather than the last iterate of OGD on that task. Speciﬁcally they bound task-averaged regret by ¯Rτ ≤ ˜O ( 1 4√τ + V )√m for V2 = min w∈W 1 τ τ∑ t=1 ∥w −w∗ t∥2 2 (9) Here V—the average deviation of the optimal actions w∗ t across tasks—is a measure of task-similarity: V is small when the tasks (clients) have similar data and thus can be solved by similar parameters inW but large when their data is different and so the optimum parameters to use are very different. Thus the bound in (9) shows that as the server (meta-learning) sees more and more clients (tasks), their regret on each decays with rate 1/4√τ to depend only on the task-similarity, which is hopefully small if the client data is similar enough that transfer learning makes sense, in particular ifV ≪diam(W). Since single-task regret has lower bound Ω(D√m), achieving asymptotic regret V√mthus demonstrates successful learning of a useful initialization in Wthat can be used for personalization. Note that such bounds can also be converted to obtain guarantees in the statistical meta-learning setting as well [22]. A drawback of past results using the ARUBA framework is that they either assume the task-similarity V is known in order to set the client step-size [28] or they employ an OCO method to learn the local step-size that cannot be applied to other potential algorithmic hyperparameters [22]. In contrast, we prove results for using bandit exponentiated gradient to tune the client step-size, which is precisely the FedEx update. In particular, Theorem 4.1 shows that by using a discretization of potential client step-sizes as the conﬁgurations in Algorithms 2 we can obtain the following task-averaged regret: Theorem 4.1. Let W⊂ Rd be convex and compact with diameter D = diam(W) and let ℓt,i be a sequence of mτ b-bounded convex losses— mfor each of τ tasks—with Lipschitz constant ≤G. We assume that the adversary is oblivious within-task. Suppose we run Algorithm 2 with B = 1, conﬁgurations cj = D Gj√m for each j = 1,...,k determining the local step-size of single-epoch SGD (OGD), wti = w∗ t, regret∑m i=1 ℓt,i(wt,i)−ℓt,i(wt) used in place ofLVti(wti), and λt = 0 ∀t∈[τ]. Then if ηt = 1 mb √ log k kτ ∀t ∈[τ], k 3 2 = DG b √ τ 2m, and Aggb(w,w∗ t) = (1 −αt)w + αtw∗ t for αt = 1/t∀t∈[τ] we have (taking expectations over sampling from Dθt) E ¯Rτ ≤ ˜O ( 3 √ m/τ + V )√m (10) The proof of this result, given in the supplement, follows the ARUBA framework of using meta OCO algorithm to optimize the initialization-dependent upper bound on the regret of OGD; in addition we bound errors to the bandit setting and discretization of the step-sizes. Theorem 4.1 demonstrates that FedEx is a sensible algorithm for tuning the step-size in the meta-learning setting where each task is an OCO problem, with the average regret across tasks (clients) converging to depend only on the task-similarity V, which we hope is small in the setting where personalization is useful. As we can see by comparing to the bound in (9), besides holding for a more generally-applicable algorithm our bound also improves the dependence on τ, albeit at the cost of an additional m 1 3 factor. Note that that the sublinear term can be replaced by 1/√τ in the full-information setting, i.e. where required the client to try SGD with each conﬁguration cj at each round to obtain regret for all of them. 8Table 1: Final test error obtained when tuning using a standard hyperparameter tuning algorithm (SHA or RS) alone, or when using it for server (aggregation) hyperparameters while FedEx tunes client (on-device training) hyperparameters. The target model is the one used to compute on-device validation error by the wrapper method, as well as the one used to compute test error after tuning. Note that this table reports the ﬁnal error results corresponding to the online evaluations reported in Figure 4, which measure performance as more of the computational budget is expended. Wrapper Target Tuning Shakespeare FEMNIST CIFAR-10 method model method i.i.d. non-i.i.d. i.i.d. non-i.i.d. i.i.d. global RS (server & client)60.32±10.03 64.36±14.19 22.81±4.56 22.98±3.41 30.46±9.44 Random + FedEx(client) 53.94±9.13 57.70±17.57 20.96±4.77 22.30±3.66 34.83±14.74 Search person- RS (server & client) 61.10±9.32 61 .71±9.08 17.45±2.82 17.77±2.63 34.89±10.56 (RS) alized + FedEx(client) 54.90±9.97 56.48±13.60 16.31±3.77 15.93±3.06 39.13±15.13 global SHA (server & client)47.38±3.40 46 .79±3.51 18.64±1.68 20.30±1.66 21.62±2.51 Successive + FedEx(client) 44.52±1.68 45.24±3.31 19.22±2.05 19.43±1.45 20.82±1.37 Halving person- SHA (server & client)46.77±3.61 48 .04±3.72 14.79±1.55 14.78±1.31 24.81±6.13 (SHA) alized + FedEx(client) 46.08±2.57 45 .89±3.76 14.97±1.31 14.76±1.70 21.77±2.83 5 Empirical Results In our experiments, we instantiate FedEx on the problem of tuning FedAvg, FedProx, and Reptile; the ﬁrst is the most popular algorithm for federated training, the second is an extension designed for heterogeneous devices, and the last is a compatible meta-learning method used for learning initializations for personalization. At communication round tthese algorithms use the aggregation Aggb(w,{wi}B i=1) = (1 −αt)w + αt ∑B i=1 |Tti| B∑ i=1 |Tti|wi (11) for some learning rate αt >0 that can vary through time; in the case of FedAvg we have αt = 1 ∀t. The local training sub-routine Locc is SGD with hyperparameters cover some objective deﬁned by the training data Tti, which can also depend on c. For example, to include FedProx we include in c an additional local hyperparameter for the proximal term compared with that of FedAvg. We tune several hyperparameters of both aggregation and local training; for the former we tune the server learning rate schedule and momentum, found to be helpful for personalization [19]; for the latter we tune the learning rate, momentum, weight-decay, the number of local epochs, the batch-size, dropout, and proximal regularization. Please see the supplementary material for the exact hyperparameter space considered. While we mainly evaluate FedEx in cross-device federated settings, which is generally more difﬁcult than cross-silo in terms of hyperparameter optimization, FedEx can be naturally applied to cross-silo settings, where the challenges of heterogeneity, privacy requirements, and personalization remain. Because our baseline is running Algorithm 1, a standard hyperparameter tuning algorithm, to tune all hyperparameters, and because we need to also wrap FedEx in such an algorithm for the reasons described in Section 3, our empirical results will test the following question: doesFedEx, wrapped by random search (RS) or a successive halving algorithm (SHA), do better than RS or SHA run with the same settings directly? Here “better” will mean both the ﬁnal test accuracy obtained and the online evaluation setting, which tests how well hyperparameter optimization is doing at intermediate phases. Furthermore, we also investigate whether FedEx can improve upon the wrapper alone even when targeting a good global and not personalized model, i.e. when elimination decisions are made using the average global validation loss. We run Algorithm 1 on the personalized objective and use RS and SHA with elimination rate η= 3, the latter following Hyperband [29]. To both wrappers we allocate the same (problem-dependent) tuning budget. To obtain the elimination rounds in Algorithm 1 for SHA, we set the number of eliminations to R= 3, ﬁx a total communication round budget, and ﬁx a maximum number of rounds to be allocated to any conﬁguration a; as detailed in the Appendix, this allows us to determine T1,...,T R so as to use up as much of the budget as possible. We evaluate the performance ofFedEx on three datasets (Shakespeare, FEMNIST, and CIFAR-10) on both vision and language tasks. We consider the following two different partitions of data: 1. Each device holds i.i.d. data. While overall data across the entire network can be non-i.i.d., we randomly shufﬂe local data within each device before splitting into train, validation, and test sets. 9Figure 4: Online evaluation of FedEx on the Shakespeare next-character prediction dataset (left), the FEMNIST image classiﬁcation dataset (middle), and the CIFAR-10 image classiﬁcation dataset (right) in the fully non-i.i.d. setting (except CIFAR-10). We report global model performance on the top and personalized performance on the bottom. All evaluations are run for three trials. 2. Each device holds non-i.i.d. data. In Shakespeare, each device is an actor and the local data is split according to the temporal position in the play; in FEMNIST, each device is the digit writer and the local data is split randomly; in CIFAR-10, we do not consider a non-i.i.d. setting. For Shakespeare and FEMNIST we use 80% of the data for training and 10% each for validation and testing. In CIFAR-10 we hold out 10K examples from the usual training/testing split for validation. The backbone models used for Shakespeare and CIFAR-10 follow from the FedAvg evaluation [39] and use 4K communications rounds (at most 800 round for each arm), while that of FEMNIST follows from LEAF [6] and uses 2K communication rounds (at most 200 for each arm). Table 1 presents our main results, displaying the ﬁnal test error of the target model after tuning using either a wrapper algorithm alone or its combination with FedEx. The evaluation shows that using FedEx on the client parameters is either equally or more effective in most cases; in particular, a FedEx-modiﬁed method performs best everywhere except i.i.d. FEMNIST, where it is very close. Furthermore, FedEx frequently improves upon the wrapper algorithm by 2 or more percentage points. We further present online evaluation results in Figure 4, where we display the test error of FedEx wrapped with SHA compared to SHA alone as a function of communication rounds. Here we see that for most of training FedEx is either around the same or better then the alternative, except at the beginning; the former is to be expected since the randomness of FedEx leads to less certain updates at initialization. Nevertheless FedEx is usually better than the SHA baseline by the halfway point. 6 Conclusion In this paper we study the problem of hyperparameter optimization in FL, starting with identifying the key challenges and proposing reasonable baselines that adapts standard approaches to the federated setting. We further make a novel connection to the weight-sharing paradigm from NAS—to our knowledge the ﬁrst instance of this being used for regular (non-architectural) hyperparameters— and use it to introduce FedEx. This simple, low-overhead algorithm for accelerating the tuning of hyperparameters in federated learning can be theoretically shown to successfully tune the step-size for multi-task OCO problems and effectively tunes FedAvg, FedProx, and Reptile on standard benchmarks. The scope of application of FedEx is very broad, including tuning actual architectural hyperparameters rather than just settings of local SGD, i.e. doing federated NAS, and tuning initialization-based meta-learning algorithms such as Reptile and MAML. Lastly, any work on FL comes with privacy and fairness risks due its frequent use of sensitive data; thus any application of our work must consider tools being developed by the community for mitigating such issues [35, 40]. 10Acknowledgments This material is based on work supported by the National Science Foundation under grants CCF- 1535967, CCF-1910321, IIS-1618714, IIS-1901403, SES-1919453, IIS-1705121, IIS-1838017, IIS-2046613 and IIS-2112471; the Defense Advanced Research Projects Agency under cooperative agreements HR00112020003 and FA875017C0141; an AWS Machine Learning Research Award; an Amazon Research Award; a Bloomberg Research Grant; a Microsoft Research Faculty Fellowship; an Amazon Web Services Award; a Facebook Faculty Research Award; funding from Booz Allen Hamilton Inc.; a Block Center Grant; and a Two Sigma Fellowship Award. Any opinions, ﬁndings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reﬂect the views of any of these funding agencies. References [1] Durmus Alp Emre Acar, Yue Zhao, Ramon Matas Navarro, Matthew Mattina, Paul N. What- mough, and Venkatesh Saligrama. Federated learning based on dynamic regularization. In Proceedings of the 9th International Conference on Learning Representations, 2021. [2] Maruan Al-Shedivat, Jennifer Gillenwater, Eric Xing, and Afshin Rostamizadeh. Federated learning via posterior averaging: A new perspective and practical algorithms. In Proceedings of the 9th International Conference on Learning Representations, 2021. [3] Peter L. Bartlett, Elad Hazan, and Alexander Rakhlin. Adaptive online gradient descent. In Advances in Neural Information Processing Systems, 2008. [4] James Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. Journal of Machine Learning Research, 13:281–305, 2012. [5] Han Cai, Ligeng Zhu, and Song Han. ProxylessNAS: Direct neural architecture search on target task and hardware. In Proceedings of the 7th International Conference on Learning Representations, 2019. [6] Sebastian Caldas, Peter Wu, Tian Li, Jakub Koneˇcný, H. Brendan McMahan, Virginia Smith, and Ameet Talwalkar. LEAF: A benchmark for federated settings. arXiv, 2018. [7] Fei Chen, Zhenhua Dong, Zhenguo Li, and Xiuqiang He. Federated meta-learning for recom- mendation. arXiv, 2018. [8] Zhongxiang Dai, Kian Hsiang Low, and Patrick Jaillet. Federated bayesian optimization via thompson sampling. In Advances in Neural Information Processing Systems, 2020. [9] Xuanyi Dong and Yi Yang. Searching for a robust neural architecture in four GPU hours. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019. [10] Hubert Eichner, Tomer Koren, H. Brendan McMahan, Nathan Srebro, and Kunal Talwar. Semi- cyclic stochastic gradient descent. In Proceedings of the 36th International Conference on Machine Learning, 2019. [11] Thomas Elsken, Benedikt Stafﬂer, Jan Hendrik Metzen, and Frank Hutter. Meta-learning of neural architectures for few-shot learning. arXiv, 2019. [12] Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar. Personalized federated learning: A meta-learning approach. In Advances in Neural Information Processing Systems, 2020. [13] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adap- tation of deep networks. In Proceedings of the 34th International Conference on Machine Learning, 2017. [14] Anubhav Garg, Amit Kumar Saha, and Debo Dutta. Direct federated neural architecture search. arXiv, 2020. [15] Avishek Ghosh, Jichan Chung, Dong Yin, and Kannan Ramchandran. An efﬁcient framework for clustered federated learning. arXiv, 2020. 11[16] Rishi Gupta and Tim Roughgarden. A PAC approach to application-speciﬁc algorithm selection. SIAM Journal on Computing, 46(3):992–1017, 2017. [17] Chaoyang He, Murali Annavaram, and Salman Avestimehr. Towards non-i.i.d. and invisible data with FedNAS: Federated deep learning via neural architecture search. arXiv, 2020. [18] Frank Hutter, Holger H. Hoos, and Kevin Leyton-Brown. Sequential model-based optimization for general algorithm conﬁguration. In Proceedings of the International Conference on Learning and Intelligent Optimization, 2011. [19] Yihan Jiang, Jakub Koneˇcný, Keith Rush, and Sreeram Kannan. Improving federated learning personalization via model agnostic meta learning. arXiv, 2019. [20] Peter Kairouz, H. Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi Bennis, Ar- jun Nitin Bhagoji, Keith Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, Rafael G. L. D’Oliveira, Salim El Rouayheb, David Evans, Josh Gardner, Zachary Garrett, Adrià Gascón, Badih Ghazi, Phillip B. Gibbons, Marco Gruteser, Zaid Harchaoui, Chaoyang He, Lie He, Zhouyuan Huo, Ben Hutchinson, Justin Hsu, Martin Jaggi, Tara Javidi, Gauri Joshi, Mikhail Khodak, Jakub Kone ˇcný, Aleksandra Korolova, Farinaz Koushanfar, Sanmi Koyejo, Tancrède Lepoint, Yang Liu, Prateek Mittal, Mehryar Mohri, Richard Nock, Ayfer Özgür, Rasmus Pagh, Mariana Raykova, Hang Qi, Daniel Ramage, Ramesh Raskar, Dawn Song, Weikang Song, Sebastian U. Stich, Ziteng Sun, Ananda Theertha Suresh, Florian Tramèr, Praneeth Vepakomma, Jianyu Wang, Li Xiong, Zheng Xu, Qiang Yang, Felix X. Yu, Han Yu, and Sen Zhao. Advances and open problems in federated learning. arXiv, 2019. [21] Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank J. Reddi, Sebastian U. Stich, and Ananda Theertha Suresh. SCAFFOLD: Stochastic controlled averaging for federated learning. In Proceedings of the 37th International Conference on Machine Learning, 2020. [22] Mikhail Khodak, Maria-Florina Balcan, and Ameet Talwalkar. Adaptive gradient-based meta- learning methods. In Advances in Neural Information Processing Systems, 2019. [23] Mikhail Khodak, Maria-Florina Balcan, and Ameet Talwalkar. Provable guarantees for gradient- based meta-learning. In Proceedings of the 36th International Conference on Machine Learning, 2019. [24] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. InProceedings of the 3rd International Conference on Learning Representations, 2015. [25] Jyrki Kivinen and Manfred K. Warmuth. Exponentiated gradient versus gradient descent for linear predictors. Information and Computation, 132:1–63, 1997. [26] Antti Koskela and Antti Honkela. Learning rate adaptation for federated and differentially private learning. arXiv, 2018. [27] Guilin Li, Xing Zhang, Zitong Wang, Zhenguo Li, and Tong Zhang. StacNAS: Towards stable and consistent differentiable neural architecture search. arXiv, 2019. [28] Jeffrey Li, Mikhail Khodak, Sebastian Caldas, and Ameet Talwalkar. Differentially private meta-learning. In Proceedings of the 8th International Conference on Learning Representations, 2020. [29] Liam Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. Hy- perband: A novel bandit-based approach to hyperparameter optimization. Journal of Machine Learning Research, 18(185):1–52, 2018. [30] Liam Li, Mikhail Khodak, Maria-Florina Balcan, and Ameet Talwalkar. Geometry-aware gradient algorithms for neural architecture search. In Proceedings of the 9th International Conference on Learning Representations, 2021. [31] Liam Li and Ameet Talwalkar. Random search and reproducibility for neural architecture search. In Proceedings of the Conference on Uncertainty in Artiﬁcial Intelligence, 2019. 12[32] Tian Li, Shengyuan Hu, Ahmad Beirami, and Virginia Smith. Ditto: Fair and robust federated learning through personalization. arXiv, 2020. [33] Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. Federated learning: Chal- lenges, methods, and future directions. IEEE Signal Processing Magazine, 37, 2020. [34] Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated optimization in heterogeneous networks. In Proceedings of the Conference on Machine Learning and Systems, 2020. [35] Tian Li, Maziar Sanjabi, Ahmad Beirami, and Virginia Smith. Fair resource allocation in feder- ated learning. In Proceedings of the 6th International Conference on Learning Representations, 2020. [36] Dongze Lian, Yin Zheng, Yintao Xu, Yanxiong Lu, Leyu Lin, Peilin Zhao, Junzhou Huang, and Shenghua Gao. Towards fast adaptation of neural architectures with meta-learning. In Proceedings of the 8th International Conference on Learning Representations, 2020. [37] Hanxiao Liu, Karen Simonyan, and Yiming Yang. DARTS: Differentiable architecture search. In Proceedings of the 7th International Conference on Learning Representations, 2019. [38] Yishay Mansour, Mehryar Mohri, Jae Ro, and Ananda Theertha Suresh. Three approaches for personalization with applications to federated learning. arXiv, 2020. [39] H. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efﬁcient learning of deep networks from decentralized data. In Proceedings of the 20th International Conference on Artiﬁcal Intelligence and Statistics, 2017. [40] H. Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li Zhang. Learning differentially private recurrent language models. In Proceedings of the 6th International Conference on Learning Representations, 2018. [41] Hesham Mostafa. Robust federated learning through representation matching and adaptive hyper-parameters. arXiv, 2019. [42] Alex Nichol, Joshua Achiam, and John Schulman. On ﬁrst-order meta-learning algorithms. arXiv, 2018. [43] Hieu Pham, Melody Y . Guan, Barret Zoph, Quoc V . Le, and Jeff Dean. Efﬁcient neural architecture search via parameter sharing. In Proceedings of the 35th International Conference on Machine Learning, 2018. [44] Sashank J. Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Koneˇcný, Sanjiv Kumar, and H. Brendan McMahan. Adaptive federated optimization. In Proceedings of the 9th International Conference on Learning Representations, 2021. [45] Reuven Y . Rubinstein and Alexander Shapiro.Discrete Event Systems: Sensitivity Analysis and Stochastic Optimization by the Score Function Method. John Wiley & Sons, Inc., 1993. [46] Shai Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends in Machine Learning, 4(2):107—-194, 2011. [47] Virginia Smith, Chao-Kai Chiang, Maziar Sanjabi, and Ameet Talwalkar. Federated multi-task learning. In Advances in Neural Information Processing Systems, 2017. [48] Jasper Snoek, Hugo Larochelle, and Ryan P. Adams. Practical bayesian optimization of machine learning algorithms. In Advances in Neural Information Processing Systems, 2012. [49] Xiang Wang, Shuai Yuan, Chenwei Wu, and Rong Ge. Guarantees for tuning the step size using a learning-to-learn approach. In Proceedings of the 38th International Conference on Machine Learning, 2021. [50] Mengwei Xu, Yuxin Zhao, Kaigui Bian, Gang Huang, Qiaozhu Mei, and Xuanzhe Liu. Federated neural architecture search. arXiv, 2020. [51] Tao Yu, Eugene Bagdasaryan, and Vitaly Shmatikov. Salvaging federated learning by local adaptation. arXiv, 2020. 13A Proof of Theorem 4.1 Proof. Let γt ∼Dθt be the step-size chosen at time t. Then we have that τE ¯Rτ = τ∑ t=1 Eγt m∑ i=1 ℓt,i ( w(wt,γt) t,i ) − m∑ i=1 ℓt,i(w∗ t) = τ∑ t=1 k∑ j=1 θt[j] m∑ i=1 ℓt,i ( w(wt,cj) t,i ) − m∑ i=1 ℓt,i(w∗ t) ≤log k η + ηkτm2b2 + min j∈[k] τ∑ t=1 m∑ i=1 ℓt,i ( w(wt,cj) t,i ) −min w∈W m∑ i=1 ℓt,i(w∗ t) ≤2mb √ τklog k+ min j∈[k] τ∑ t=1 1 2cj ∥wt −w∗ t∥2 2 + cjmG2 ≤2mb √ τklog k+ min j∈[k] D2(1 + logτ) 2cj + (V2 2cj + cjmG2 ) τ ≤2mb √ τklog k+ D2(1 + logτ) + V2τ 2γ∗ + γ∗mG2τ + min j∈[k] (1 cj − 1 γ∗ )D2(1 + logτ) + V2τ 2 + (cj −γ∗)mG2τ ≤2mb √ τklog k+ 4D √ τ + τlog τ 2 + ( 2V + D k ) Gτ √m 2 = mb √ 2τlog τ + 4D √ τ + τlog τ 2 + (DG+ 2GVτ ) √m 2 where the second line uses linearity of expectations over γt ∼Dθt, the third substitutes the bandit regret of EG [ 46, Corollary 4.2], the fourth substitutes η = 1 mb √ log k τk and the regret of OGD [46, Corollary 2.7], the ﬁfth substitutes the regret guarantee of Adaptive OGD over functions 1 2 ∥wt−w∗ t∥2 2 [3, Theorem 2.1] with step-size αt = 1/tand the deﬁnition of V, the sixth substitutes the best discretized step-size cj for the optimal γ∗ ∈ ( 0, D G √ 2m ] , and the seventh substitutes V 2G √ 2m + D 2G √ 1+log τ 2mτ for γ∗and arg minj:cj≥γ∗ for arg minjcj. Setting k 3 2 = DG b √ τ 2m and dividing both sides by τ yields the result. 14B Decomposing Federated Optimization Methods As detailed in Section 2 our analysis and use of FedEx to tune local training hyperparameters depends on a formulation that decomposes FL methods into two subroutines: a local training routine Locc(S,w) with hyperparameters cover data S and starting from initialization w and an aggregation routine Aggbwith hyperparameters b. In this section we discuss how a variety of federated optimization methods, including several of the best-known, can be decomposed in this manner. This enables the application of FedEx to tune their hyperparameters. B.1 FedAvg [39] The best-known FL method, FedAvg runs SGD on each client in a batch starting from a shared initialization and then updates to the average of the last iterate of the clients, often weighted by the number of data points each client has. The decomposition here is: Locc Local SGD (or another gradient-based algorithm, e.g. Adam [24]), with cbeing the standard hyperparameters such as step-size, momentum, weight-decay, etc. Aggb Weighted averaging, with no hyperparameters in b. B.2 FedProx [34] FedProx has the same decomposition as FedAvg except local SGD is replaced by a proximal version that regularizes the routine to be closer to the initialization, adding another hyperparameter to c governing the strength of this regularization. B.3 Reptile [42] A well-known meta-learning algorithm, Reptile has the same decomposition as FedAvg except the averaged aggregation is replaced by a convex combination of the initialization and the average of the last iterates, as in Equation 11. This adds another hyperparameter to bgoverning the tradeoff between the two. B.4 SCAFFOLD [21] SCAFFOLD comes in two variants, both of which compute and aggregate control variates in parallel to the model weights. The decomposition here is: Locc Local SGD starting from a weight initialization with a control variate, which can be merged to form the local training initialization. The hyperparameters in care the same as in FedAvg. Aggb Weighted averaging of both the initialization and the control variates, with the same hyper- parameters as Reptile. B.5 FedDyn [1] In addition to a FedAvg/FedProx/Reptile-like training routine, this algorithm maintains a regu- larizer on each device that affects the local training routine. While this statefulness cannot strictly be subsumed in our decomposition, since it does not introduce any additional hyperparameters the remaining hyperparameters can be tuned in the same manner as we do forFedAvg/FedProx/Reptile. In order to choose between using FedDyn or not, one can introduce a binary hyperparameter to c specifying whether or not Locc uses that term in the objective it optimizes or not, allowing it also to be tuned via FedEx. B.6 FedPA [2] This algorithm replaces local SGD in FedAvg by a local Markov-chain Monte Carlo (MCMC) routine starting from the initialization given by aggregating the previous round’s MCMC routines. The decomposition is then just a replacement of local SGD and its associated hyperparameters by local MCMC and its hyperparameters, with the aggregation routine remaining the same. 15B.7 Ditto [32] Although it depends on what solver is used for the local solver and aggregation routines, in the simplest formulation, the local optimization of personalized models involves an additional regular- ization hyperparameter. While the updating rule of Ditto is different from that of FedProx, the hyperparameters can be decomposed and tuned in a similar manner. B.8 MAML [13] A well-known meta-learning algorithm, MAML takes one or more full-batch gradient descent (GD) steps locally and updates the global model using a second-order gradient using validation data. The decomposition here is : Locc Local SGD starting from a weight initialization. The hyperparameters in care the same as in FedAvg. The algorithm also returns second-order information required to compute the meta-gradient. Aggb Meta-gradient computation, summation, and updating using a standard descent method like Adam [24]. The hyperparameters in bare the hyperparameters of the latter. C FedEx Details C.1 Stochastic Gradient used by FedEx Below is a simple calculation showing that the stochastic gradient used to update the categorical architecture distribution of FedEx is an unbiased approximation of the true gradient w.r.t. its parameters. ∇θjEcij|θLVti(wi) = ∇θjEcij|θ(LVti(wi) −λ) = Ecij|θ ( (LVti(wi) −λ)∇θj log Pθ(cij) ) = Ecij|θ ( (LVti( ˆwk) −λ)∇θj log n∏ i=1 Pθ(cij = cj) ) = Ecij|θ ( (LVti(wi) −λ) n∑ i=1 ∇θj log Pθ(cij = cj) ) = Ecij|θ ((LVti(wi) −λ)1cij=cj θj ) Note that this use of the reparameterization trick has some similarity with a recent RL approach to tune the local step-size and number of epochs [41]; however, FedEx can be rigorously formulated as an optimization over the personalization objective, has provable guarantees in a simple setting, uses a different conﬁguration distribution that leads to our exponentiated update, and crucially for practical deployment does not depend on obtaining aggregate reward signal on each round. 16C.2 FedEx wrapped with SHA For completeness, we present the pseudo code of wrapping FedEx with SHA in Algorithm 3 below. Algorithm 3: FedEx wrapped with SHA Input: distribution Dover hyperparameters A, elimination rate η∈N, elimination rounds τ0 = 0,τ1,...,τ R sample set of ηR hyperparameters H ∼D[ηR] initialize a model wa ∈Rd for each a∈H for elimination round r∈[R] do for setting a= (b,c) ∈H do sa,wa,θa ←FedEx (wa,b,c,θ a,τr+1 −τr) H ←{a∈H : sa ≤1 η-quantile({sa : a∈H})} Output: remaining a∈H and associated model wa FedEx (w,b, {c1,...,c k},θ,τ ≥1): initialize θ1 ←θ initialize shared weights w1 ←w for comm. round t= 1,...,τ do for client i= 1,...,B do send wt,θt to client sample cti ∼Dθt wti ←Loccti(Tti,wt) send wti,cti,LVti(wti) to server wt+1 ←Aggb(w,{wti}B i=1) set step size ηt and baseline λt ˜∇j ← ∑B i=1 |Vti|(LVti(wti)−λt)1cti=cj θt[j] ∑B i=1 |Vti| ∀j θt+1 ←θt ⊙exp(−ηt˜∇) θt+1 ←θt+1/∥θt+1∥1 s←∑B i=1 |Vti|LVti/∑B i=1 |Vti| Return s, model w, hyperparameter distribution θ C.3 Hyperparameters of FedEx We tune the computation of the baseline λt, which we set to λt = 1∑ s<tγt−s ∑ s<t γt−s ∑B i=1 |Vti| B∑ i=1 LVti(wi) for discount factor γ ∈[0,1]. As discussed in Section 3, the local perturbation factor is set to ε= 0.1. 27 conﬁgurations are used in each arm for SHA and RS. The number of conﬁguration used per arm of FedEx (i.e. the dimensionality of θ) is the same (27). 17D Experimental Details Code implementing FedEx is available at https://github.com/mkhodak/fedex. The code auto- matically downloads CIFAR-10 data, while Shakespeare and FEMNIST data is made available by the LEAF repository: https://github.com/TalwalkarLab/leaf. D.1 Settings of the Baseline/Wrapper Algorithm We use the same settings of Algorithm 1 for both tuning FedAvg and for wrapping FedEx. Given an elimination rate η, number of elimination rounds R, resource budget B, and maximum rounds per arm M, we assign T1,...,T R s.t. Ti −Ti−1 = T −M ηn+1−1 η−1 −n−1 (recall T0 = 0) and assign any remaining resources to maximize resource use. All remaining details were noted in Section 5. D.2 Hyperparameters of FedAvg/FedProx/Reptile Server hyperparameters (learning rate αt = γt): log10 lr : Unif[ −1,1] momentum : Unif[0 ,0.9] log10(1 −γ) : Unif[ −4,−2] Local training hyperparameters (note we only use 1 epoch for Shakespeare to conserve computation): log10(lr) : Unif[ −4,0] momentum : Unif[0 .0,1.0] log10(weight-decay) : Unif[ −5,−1] epoch : Unif {1,2,3,4,5} log2(batch) : Unif {3,4,5,6,7} dropout : Unif[0 ,0.5] E Conﬁdence Intervals Table 2: Final test error obtained when tuning using a standard hyperparameter tuning algorithm (SHA or RS) alone, or when using it for server (aggregation) hyperparameters while FedEx tunes client (on-device training) hyperparameters. The target model is the one used to compute on-device validation error by the wrapper method, as well as the one used to compute test error after tuning. The conﬁdence intervals displayed are 90% Student-t conﬁdence intervals for the mean estimates from Table 1, with 5 independent trials for Shakespeare, 10 for FEMNIST, 10 for RS on CIFAR, and 6 for SHA on CIFAR. Wrapper Target Tuning Shakespeare FEMNIST CIFAR-10 method model method i.i.d. non-i.i.d. i.i.d. non-i.i.d. i.i.d. global RS (server & client)60.32±9.56 64.36±13.53 22.81±2.64 22.98±1.98 30.46±5.47 Random + FedEx(client) 53.94±8.70 57.70±16.75 20.96±2.77 22.30±2.12 34.83±8.54 Search person- RS (server & client) 61.10±8.89 61.71±8.66 17.45±1.63 17.77±1.52 34.89±6.12 (RS) alized + FedEx(client) 54.90±9.50 56.48±12.97 16.31±2.19 15.93±1.77 39.13±8.77 global SHA (server & client)47.38±3.24 46.79±3.35 18.64±0.97 20.30±0.96 21.62±1.45 Successive + FedEx(client) 44.52±1.60 45.24±3.16 19.22±1.19 19.43±0.84 20.82±0.79 Halving person- SHA (server & client)46.77±3.44 48.04±3.54 14.79±0.90 14.78±0.75 24.81±3.55 (SHA) alized + FedEx(client) 46.08±2.45 45.89±3.58 14.97±0.76 14.76±0.99 21.77±1.64 18Figure 5: Comparison of different εsettings for the local perturbation component of FedEx from Section 3. Figure 6: Comparison of step-size schedules for ηt in FedEx. In practice we chose the ‘aggres- sive’ schedule, which exhibits faster convergence to favorable conﬁgurations. F Ablation Studies We now discuss two design choices of FedEx and how they affect performance of the algorithm. First, the choice of the local perturbation ε = 0.1 discussed in Section 3; we choose this setting due to its consistent performance across several settings. In Figure 5 we plot the performance of FedEx on CIFAR-10 between ε= 0.0 (no FedEx, i.e. SHA only) and ε= 1.0 (full FedEx, i.e. client conﬁgurations are chosen independently) and show that while the use of a nonzero εis important, performance at fairly low values of εis roughly similar. We further investigated the setting of the step-size ηt for the exponentiated gradient update in FedEx. We examine three different approaches: a constant rate of ηt = √2 logk, an ‘adaptive’ schedule of ηt = √2 logk/ √∑ s≤t∥˜∇s∥2∞, and an ‘aggressive’ schedule ofηt = √2 logk/∥˜∇t∥∞. Here ˜∇t is the stochastic gradient w.r.t. θcomputed in Algorithm 2 at step tand the form of the step-size is derived from standard settings for exponentiated gradient in online learning [46]. We found that the ‘aggressive’ schedule works best in practice, as shown in Figure 6. A key issue with using the ‘constant’ and ‘adaptive’ approaches is that they continue to assign high probability to several conﬁgurations late in the tuning process; this slows down training of the shared weights. One could consider a tradeoff between allowingFedEx to run longer than while keeping the total budget constant, but for simplicity we chose the more effective ‘aggressive’ schedule. 19",
      "meta_data": {
        "arxiv_id": "2106.04502v2",
        "authors": [
          "Mikhail Khodak",
          "Renbo Tu",
          "Tian Li",
          "Liam Li",
          "Maria-Florina Balcan",
          "Virginia Smith",
          "Ameet Talwalkar"
        ],
        "published_date": "2021-06-08T16:42:37Z",
        "pdf_url": "https://arxiv.org/pdf/2106.04502v2.pdf",
        "github_url": "https://github.com/mkhodak/fedex"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper investigates federated hyperparameter tuning, identifying key challenges such as federated validation data, extreme resource limitations, and evaluating personalization. It introduces FedEx, a novel method that connects federated hyperparameter tuning to the neural architecture search technique of weight-sharing. FedEx accelerates tuning for widely-used federated optimization methods like FedAvg and its variants. Theoretically, a FedEx variant is shown to correctly tune the on-device learning rate in the setting of online convex optimization across devices. Empirically, FedEx outperforms natural baselines for federated hyperparameter tuning by several percentage points on Shakespeare, FEMNIST, and CIFAR-10 benchmarks, achieving higher accuracy using the same training budget.",
        "methodology": "FedEx leverages a novel connection between hyperparameter tuning in Federated Learning (FL) and the weight-sharing paradigm from Neural Architecture Search (NAS). It formalizes the personalized FL objective as a single-level empirical risk minimization, enabling a stochastic relaxation approach similar to NAS. Instead of architectural hyperparameters, FedEx tunes local training hyperparameters (e.g., learning rate, momentum, epochs) by setting up a categorical distribution over a fixed number of sampled configurations (often drawn using a local perturbation scheme around an initial sample). This distribution is then updated using exponentiated gradient updates, alternating with standard SGD updates to the shared model weights. The method applies to FL algorithms decomposable into local training (Locc) and aggregation (Aggb) subroutines (e.g., FedAvg, FedProx, SCAFFOLD, Reptile). For theoretical guarantees, FedEx's approach is analyzed within the Average Regret-Upper-Bound Analysis (ARUBA) framework for online convex optimization.",
        "experimental_setup": "Experiments instantiate FedEx on FedAvg, FedProx, and Reptile. It is compared against standard hyperparameter tuning baselines: Random Search (RS) and Successive Halving Algorithm (SHA), both when run alone and when wrapping FedEx. The tuned hyperparameters include server learning rate schedule and momentum, as well as local training parameters such as learning rate, momentum, weight-decay, number of local epochs, batch-size, dropout, and proximal regularization. Evaluations are performed on three standard FL benchmarks: Shakespeare (next-character prediction), FEMNIST (image classification), and CIFAR-10 (image classification). Both i.i.d. and non-i.i.d. data partitions are considered for Shakespeare and FEMNIST, while CIFAR-10 uses i.i.d. data. Performance is measured by final test error and online evaluation (test error as a function of communication rounds). Experiments utilize 4K communication rounds for Shakespeare and CIFAR-10 (at most 800 rounds per arm) and 2K for FEMNIST (at most 200 per arm). Confidence intervals are derived from 5 to 10 independent trials.",
        "limitations": "FedEx is applicable only to FL algorithms that can be decomposed into local fine-tuning and aggregation routines. It requires wrapper algorithms (such as RS or SHA) to tune its own internal hyperparameters (e.g., the baseline λt for gradient estimation) and certain server-side FL hyperparameters (e.g., server learning rate). The local perturbation scheme, while useful for stability, can limit the size of the search space explored by each individual instance of FedEx. The theoretical analysis for tuning the step-size is conducted in a simplified setting (one client per round, online convex optimization), and the theoretical bound on average regret incurs an additional m^(1/3) factor compared to a full-information setting. The paper also acknowledges general privacy and fairness risks associated with FL applications, which are not directly addressed by FedEx.",
        "future_research_directions": "Future research directions include extending FedEx to tune architectural hyperparameters, effectively enabling federated Neural Architecture Search (NAS). It can also be applied to tune initialization-based meta-learning algorithms such as MAML. While FedEx is naturally applicable to cross-silo settings, further dedicated exploration in this domain could be beneficial. Addressing the time-dependency of federated evaluation is another challenge not fully explored. Furthermore, any practical application of this work must consider and integrate tools being developed by the community for mitigating privacy and fairness issues in FL.",
        "experimental_code": "class FedEx:\n    '''runs hyperparameter optimization given a federated learning server'''\n\n    def entropy(self):\n\n        entropy = 0.0\n        for probs in product(*(theta[theta>0.0] for theta in self._theta)):\n            prob = np.prod(probs)\n            entropy -= prob * np.log(prob)\n        return entropy\n\n    def mle(self):\n    \n        return np.prod([theta.max() for theta in self._theta])\n\n    def __init__(\n                 self, \n                 server, \n                 configs, \n                 eta0='auto', \n                 sched='auto', \n                 cutoff=0.0, \n                 baseline=0.0, \n                 diff=False,\n                 ):\n        '''\n        Args:\n            server: Object that implements two methods, 'communication_round' and 'full_evaluation'\n                    taking as input a single argument, 'get_config', itself a function that takes \n                    no inputs and outputs an element of the provided list 'configs'. \n                    - 'communication_round' samples a batch of clients, assigns a config to each \n                    using 'get_config', and runs local training using that config. It then \n                    aggregates the local models to to take a training step and returns three lists \n                    or arrays: a list of each client's validation error before local training, a \n                    list of each client's validation error after local training, and a list of each \n                    client's weight (e.g. size of its validation set). \n                    - 'full_evaluation' assigns a config to each client using 'get_config' and runs\n                    local training using that config. It then returns three lists or arrays: a list\n                    of each client's test error before local training, a list of each client's test\n                    error after local training, and a list of each client's weight (e.g. size of \n                    its test set).\n            configs: list of configs used for local training and testing by 'server' \n                     OR dict of (string, list) pairs denoting a grid of configs\n            eta0: base exponentiated gradient step size; if 'auto' uses sqrt(2*log(len(configs)))\n            sched: learning rate schedule for exponentiated gradient:\n                    - 'adaptive': uses eta0 / sqrt(sum of squared gradient l-infinity norms)\n                    - 'aggressive': uses eta0 / gradient l-infinity norm\n                    - 'auto': uses eta0 / sqrt(t) for t the number of rounds\n                    - 'constant': uses eta0\n                    - 'scale': uses sched * sqrt(2 * log(len(configs)))\n            cutoff: entropy level below which to stop updating the config probability and use MLE\n            baseline: discount factor when computing baseline; 0.0 is most recent, 1.0 is mean\n            diff: if True uses performance difference; otherwise uses absolute performance\n        '''\n\n        self._server = server\n        self._configs = configs\n        self._grid = [] if type(configs) == list else sorted(configs.keys())\n\n        sizes = [len(configs[param]) for param in self._grid] if self._grid else [len(configs)]\n        self._eta0 = [np.sqrt(2.0 * np.log(size)) if eta0 == 'auto' else eta0 for size in sizes]\n        self._sched = sched\n        self._cutoff = cutoff\n        self._baseline = baseline\n        self._diff = diff\n        self._z = [np.full(size, -np.log(size)) for size in sizes]\n        self._theta = [np.exp(z) for z in self._z]\n\n        self._store = [0.0 for _ in sizes]\n        self._stopped = False\n        self._trace = {'global': [], 'refine': [], 'entropy': [self.entropy()], 'mle': [self.mle()]}\n\n    def stop(self):\n\n        self._stopped = True\n\n    def sample(self, mle=False, _index=[]):\n        '''samples from configs using current probability vector'''\n\n        if mle or self._stopped:\n            if self._grid:\n                return {param: self._configs[param][theta.argmax()] \n                        for theta, param in zip(self._theta, self._grid)}\n            return self._configs[self._theta[0].argmax()]\n        _index.append([np.random.choice(len(theta), p=theta) for theta in self._theta])\n\n        if self._grid:\n            return {param: self._configs[param][i] for i, param in zip(_index[-1], self._grid)}\n        return self._configs[_index[-1][0]]\n\n    def settings(self):\n        '''returns FedEx input settings'''\n\n        output = {'configs': deepcopy(self._configs)}\n        output['eta0'], output['sched'] = self._eta0, self._sched\n        output['cutoff'], output['baseline'] = self._cutoff, self._baseline \n        if self._trace['refine']:\n            output['theta'] = self.theta()\n        return output\n\n    def step(self):\n        '''takes exponentiated gradient step (calls 'communication_round' once)'''\n\n        index = []\n        before, after, weight = self._server.communication_round(lambda: self.sample(_index=index))        \n        before, after = np.array(before), np.array(after)\n        weight = np.array(weight, dtype=np.float64) / sum(weight)\n\n        if self._trace['refine']:\n            trace = self.trace('refine')\n            if self._diff:\n                trace -= self.trace('global')\n            baseline = discounted_mean(trace, self._baseline)\n        else:\n            baseline = 0.0\n        self._trace['global'].append(np.inner(before, weight))\n        self._trace['refine'].append(np.inner(after, weight))\n        if not index:\n            self._trace['entropy'].append(0.0)\n            self._trace['mle'].append(1.0)\n            return\n\n        for i, (z, theta) in enumerate(zip(self._z, self._theta)):\n            grad = np.zeros(len(z))\n            for idx, s, w in zip(index, after-before if self._diff else after, weight):\n                grad[idx[i]] += w * (s - baseline) / theta[idx[i]]\n            if self._sched == 'adaptive':\n                self._store[i] += norm(grad, float('inf')) ** 2\n                denom = np.sqrt(self._store[i])\n            elif self._sched == 'aggressive':\n                denom = 1.0 if np.all(grad == 0.0) else norm(grad, float('inf'))\n            elif self._sched == 'auto':\n                self._store[i] += 1.0\n                denom = np.sqrt(self._store[i])\n            elif self._sched == 'constant':\n                denom = 1.0\n            elif self._sched == 'scale':\n                denom = 1.0 / np.sqrt(2.0 * np.log(len(grad))) if len(grad) > 1 else float('inf')\n            else:\n                raise NotImplementedError\n            eta = self._eta0[i] / denom\n            z -= eta * grad\n            z -= logsumexp(z)\n            self._theta[i] = np.exp(z)\n\n        self._trace['entropy'].append(self.entropy())\n        self._trace['mle'].append(self.mle())\n        if self._trace['entropy'][-1] < self._cutoff:\n            self.stop()\n\n    def test(self, mle=False):\n        '''evaluates found config (calls 'full_evaluation' once)\n        Args:\n            mle: use MLE config instead of sampling\n        Returns:\n            output of 'full_evaluation'\n        '''\n\n        before, after, weight = self._server.full_evaluation(lambda: self.sample(mle=mle))\n        return {'global': np.inner(before, weight) / weight.sum(),\n                'refine': np.inner(after, weight) / weight.sum()}\n\n    def theta(self):\n        '''returns copy of config probability vector'''\n\n        return deepcopy(self._theta)\n\n    def trace(self, key):\n        '''returns trace of one of three tracked quantities\n        Args:\n            key: 'entropy', 'global', or 'refine'\n        Returns:\n            numpy vector with length equal to number of calls to 'step'\n        '''\n\n        return np.array(self._trace[key])\n\ndef wrapped_fedex(\n                  get_server,\n                  get_client,\n                  num_configs=1,\n                  prod=False,\n                  stepsize_init='auto', \n                  stepsize_sched='aggressive', \n                  cutoff=1E-4, \n                  baseline_discount=-1.0, \n                  diff=False,\n                  mle=False, \n                  logdir=None,\n                  val_discount=0.0, \n                  last_stop=False,\n                  eval_global=False,\n                  **kwargs,\n                  ):\n    '''evaluates FedEx wrapped with successive elimination algorithm;\n       uses FedAvg when num_configs = 1 and prod = False\n    Args:\n        get_server: function that takes no input and returns an object that can be passed as the \n                    first argument to FedEx.__init__, e.g. a Server object\n        get_client: function that takes no input and returns a dict of local training configs, a\n                    list of which is passed as the second argument to 'FedEx.__init__'; can also\n                    return a dict of (string, list) pairs to be passed directly to 'FedEx.__init__'\n        num_configs: determines number of configs in the list passed to 'FedEx.__init__':\n                     - >0: use this value directly\n                     - =0: value drawn at random from Unif[1, number of arms given by the wrapper]\n                     - =-1: use the number of arms given by the wrapper\n                     - else: value drawn at random from Unif{1, ..., abs(num_configs)}\n        prod: run FedEx over a product set of single-parameter grids; must be 'True' in the case\n                  when 'get_client' returns an object to be passed directly to 'FedEx.__init__'\n        stepsize_init: passed to 'eta0' kwarg of 'FedEx.__init__'\n        stepsize_sched: passed to 'sched' kwarg of 'FedEx.__init__'\n        baseline_discount: determines 'baseline' kwarg of 'FedEx.__init__':\n                           - >0.0: use this value directly\n                           - else: value drawn at random from Unif[0.0, abs(baseline_discount)]\n        diff: passed to 'diff' kwarg of 'FedEx.__init__'\n        mle: passed to 'mle' kwarg of 'FedEx.test' via the kwargs of 'successive_elimination'\n        logdir: passed to 'logdir' kwarg of 'successive_elimination'\n        val_discount: passed to 'val_discount' kwarg of 'successive_elimination'\n        last_stop: if True sets 'last_round' kwarg of 'successive_elimination' to 'stop'\n        kwargs: passed to 'get_schedule'\n    Returns:\n        FedEx object\n    '''\n\n    elim_rate, elim_sched, eval_sched = get_schedule(**kwargs)\n    print('Wrapping with', 'random search' if len(elim_sched) == 1 else 'successive elimination')\n\n    if num_configs < -1:\n        samples = lambda n: random.randint(1, -num_configs)\n    elif num_configs == -1:\n        samples = lambda n: n\n    elif num_configs == 0:\n        samples = lambda n: random.randint(1, n)\n    else:\n        samples = lambda n: num_configs\n\n    if baseline_discount < 0.0:\n        baseline = lambda: random.uniform(0.0, -baseline_discount)\n    else:\n        baseline = lambda: baseline_discount\n\n    def sampler(n):\n\n        for _ in range(n):\n            yield FedEx(\n                        get_server(), \n                        get_client() if prod else get_client(samples(n)),\n                        eta0=stepsize_init, \n                        sched=stepsize_sched, \n                        cutoff=cutoff, \n                        baseline=baseline(),\n                        diff=diff,\n                        )\n\n    return successive_elimination(\n                                  sampler, \n                                  ['refine', 'global'], \n                                  logdir=logdir, \n                                  val_discount=val_discount,\n                                  elim_rate=elim_rate, \n                                  elim_sched=elim_sched, \n                                  eval_sched=eval_sched,\n                                  traces=['entropy', 'mle', 'global', 'refine'], \n                                  last_round='stop' if last_stop else None,\n                                  mle=mle,\n                                  eval_global=eval_global,\n                                  )\n\ndef get_client(n_clients=1):\n    '''performs local tuning for each hyperparameter'''\n    # Example from cifar.py, similar in other dataset files\n    if args.lr_only:\n        return [SIMPLE_CLIENT()]\n\n    initial_client = CLIENT()\n    client_arr = [initial_client]\n    eps = args.eps\n\n    for i in range(n_clients-1):\n        other_client = deepcopy(initial_client)\n        \n        log_lr = np.log10(other_client['lr'])\n        other_client['lr'] = 10 ** np.clip(log_lr + np.random.uniform(4*-eps, 4*eps), -4.0, 0.0)\n        \n        other_client['momentum'] = np.clip(initial_client['momentum'] + np.random.uniform(-eps, eps), 0, 1.0)\n        \n        log_wd = np.log10(other_client['weight_decay'])\n        other_client['weight_decay'] = 10 ** np.clip(log_wd + np.random.uniform(4*-eps, 4*eps),-5.0, -1.0)\n        \n        epochs_range = math.ceil(eps * 4)\n        other_client['epochs'] = np.clip(np.random.choice(np.arange(initial_client['epochs']-epochs_range, initial_client['epochs']+epochs_range+1)), 1, 5)\n\n        log_batch = int(np.log2(other_client['batch']))\n        batch_range = math.ceil(eps * 4)\n        other_client['batch'] = 2 ** np.clip(np.random.choice(np.arange(log_batch-batch_range, log_batch+batch_range+1)), 3, 7)\n\n        \n        log_mu = np.log10(other_client['mu'])\n        other_client['mu'] = 10 ** np.clip(log_mu + np.random.uniform(5*-eps, 5*eps), -5.0 , 0.0)\n        \n        other_client['dropout'] = np.clip(initial_client['dropout'] + np.random.uniform(0.5*-eps, 0.5*eps),0, 0.5)\n\n        client_arr.append(other_client)\n\n    return [UNIFORM()] if args.uniform else [RANDOM()] if args.random else client_arr",
        "experimental_info": "The FedEx method tunes local training hyperparameters for personalized Federated Learning. The specific settings for the CIFAR-10 experiment are as follows:\n\n**1. Local Training Hyperparameter Search Space (Client Configuration):**\nFedEx optimizes over a categorical distribution of client configurations, which are generated by perturbing an initial random sample. The initial random sampling ranges for client hyperparameters are:\n-   **Learning Rate (lr):** Uniformly sampled from 10^(-4.0) to 10^(0.0) (log-uniform).\n-   **Momentum:** Uniformly sampled from 0.0 to 1.0.\n-   **Weight Decay:** Uniformly sampled from 10^(-5.0) to 10^(-1.0) (log-uniform).\n-   **Epochs:** Randomly chosen integer from 1 to 5.\n-   **Batch Size (batch):** Randomly chosen power of 2 from 2^3 to 2^7.\n-   **Proximal Term (mu):** Uniformly sampled from 10^(-5.0) to 10^(0.0) (log-uniform), used in FedProx (if mu > 0.0).\n-   **Dropout:** Uniformly sampled from 0.0 to 0.5.\n\nWhen `args.eps > 0.0`, additional client configurations are generated by perturbing the `initial_client`'s hyperparameters using `np.random.uniform(-eps * factor, eps * factor)` where `factor` varies per hyperparameter (e.g., 4 for log_lr, 1 for momentum, 5 for log_mu, 0.5 for dropout). For discrete parameters like epochs and batch size, a range `math.ceil(eps * 4)` is used for random choices.\n\n**2. FedEx Algorithm Settings:**\n-   `--configs`: Number of configurations to optimize over. Default is 1 (FedAvg). Can be set to sample a random number of configs or use the number of arms from the wrapper.\n-   `--eps`: Multiplicative perturbation factor (default 0.0 for FedAvg) applied during client config generation, enabling the local perturbation scheme.\n-   `--uniform`: Flag to run FedEx over a product set of single-parameter uniform grids.\n-   `--random`: Flag to run FedEx over a product set of single-parameter random grids.\n-   `--eta0`: Initial step size for exponentiated gradient updates (default 'auto').\n-   `--sched`: Step size schedule for exponentiated gradient ('aggressive', 'adaptive', 'auto', 'constant', 'scale'). Default is 'aggressive'.\n-   `--cutoff`: Entropy level below which FedEx stops updating the config probability distribution (default 0.0).\n-   `--baseline`: Discount factor for computing the baseline in exponentiated gradient (default -1.0, samples from [0.0, 1.0)).\n-   `--diff`: If set, uses the performance difference between refined and global models as the FedEx objective.\n-   `--stop`: If set, stops updating the FedEx config distribution after the last elimination round.\n-   `--mle`: If set, uses the Maximum Likelihood Estimate (MLE) configuration at test time.\n-   `--loss`: If set, uses loss instead of error as the evaluation metric.\n\n**3. Wrapper Algorithm (Successive Elimination) Settings:**\n-   `--rounds`: Maximum number of communication rounds (resources assigned to a single arm, default 800 for CIFAR-10).\n-   `--total`: Total number of communication rounds/resources (default 4000 for CIFAR-10).\n-   `--rate`: Elimination rate (multiplicative, default 3).\n-   `--elim`: Number of elimination rounds (default 0, runs random search if 0).\n-   `--eval`: Number of evaluation rounds (default 1).\n-   `--discount`: Discount factor for computing the validation score of an arm (default 0.0, uses most recent value).\n-   `--batch`: Number of clients sampled per communication round for FedEx (default 10).\n-   `--eval_global`: If set, uses global error as the elimination metric instead of the refined error.\n\n**4. Dataset and Model Settings (CIFAR-10 Specific):**\n-   **Dataset:** CIFAR-10. Input images are 32x32. Preprocessing includes random horizontal flip, random crop, color jitter, ToTensor, and normalization (mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]).\n-   **Model:** A Convolutional Neural Network (CNN) with three convolutional layers (32, 64, 64 filters respectively), ReLU activations, MaxPool2d, followed by a dropout layer and two fully connected layers (1024 to 64, then 64 to 10 for 10 classes).\n-   `--val`: Proportion of training data used for validation (default 0.2).\n-   `--num-clients`: Number of clients (default 500). Each client receives a partition of the CIFAR-10 dataset (50000 images for train, 10000 for test, split across clients)."
      }
    },
    {
      "title": "Reshuffling Resampling Splits Can Improve Generalization of Hyperparameter Optimization",
      "abstract": "Hyperparameter optimization is crucial for obtaining peak performance of\nmachine learning models. The standard protocol evaluates various hyperparameter\nconfigurations using a resampling estimate of the generalization error to guide\noptimization and select a final hyperparameter configuration. Without much\nevidence, paired resampling splits, i.e., either a fixed train-validation split\nor a fixed cross-validation scheme, are often recommended. We show that,\nsurprisingly, reshuffling the splits for every configuration often improves the\nfinal model's generalization performance on unseen data. Our theoretical\nanalysis explains how reshuffling affects the asymptotic behavior of the\nvalidation loss surface and provides a bound on the expected regret in the\nlimiting regime. This bound connects the potential benefits of reshuffling to\nthe signal and noise characteristics of the underlying optimization problem. We\nconfirm our theoretical results in a controlled simulation study and\ndemonstrate the practical usefulness of reshuffling in a large-scale, realistic\nhyperparameter optimization experiment. While reshuffling leads to test\nperformances that are competitive with using fixed splits, it drastically\nimproves results for a single train-validation holdout protocol and can often\nmake holdout become competitive with standard CV while being computationally\ncheaper.",
      "full_text": "Reshuffling Resampling Splits Can Improve Generalization of Hyperparameter Optimization Thomas Nagler∗ Lennart Schneider∗ Bernd Bischl Matthias Feurer t.nagler@lmu.de Department of Statistics, LMU Munich Munich Center for Machine Learning (MCML) Abstract Hyperparameter optimization is crucial for obtaining peak performance of ma- chine learning models. The standard protocol evaluates various hyperparameter configurations using a resampling estimate of the generalization error to guide opti- mization and select a final hyperparameter configuration. Without much evidence, paired resampling splits, i.e., either a fixed train-validation split or a fixed cross- validation scheme, are often recommended. We show that, surprisingly, reshuffling the splits for every configuration often improves the final model’s generalization performance on unseen data. Our theoretical analysis explains how reshuffling affects the asymptotic behavior of the validation loss surface and provides a bound on the expected regret in the limiting regime. This bound connects the potential benefits of reshuffling to the signal and noise characteristics of the underlying optimization problem. We confirm our theoretical results in a controlled simula- tion study and demonstrate the practical usefulness of reshuffling in a large-scale, realistic hyperparameter optimization experiment. While reshuffling leads to test performances that are competitive with using fixed splits, it drastically improves results for a single train-validation holdout protocol and can often make holdout become competitive with standard CV while being computationally cheaper. 1 Introduction Hyperparameters have been shown to strongly influence the performance of machine learning models (van Rijn & Hutter, 2018; Probst et al., 2019). The primary goal of hyperparameter optimization (HPO; also called tuning) is the identification and selection of a hyperparameter configuration (HPC) that minimizes the estimated generalization error (Feurer & Hutter, 2019; Bischl et al., 2023). Typically, this task is challenged by the absence of a closed-form mathematical description of the objective function, the unavailability of an analytic gradient, and the large cost to evaluate HPCs, categorizing HPO as a noisy, black-box optimization problem. An HPC is evaluated via resampling, such as a holdout split or M-fold cross-validation (CV), during tuning. These resampling splits are usually constructed in a fixed and instantiated manner, i.e., the same training and validation splits are used for the internal evaluation of all configurations. On the one hand, this is an intuitive approach, as it should facilitate a fair comparison between HPCs and reduce the variance in the comparison.1 On the other hand, such a fixing of train and validation splits might steer the optimization, especially after a substantial budget of evaluations, towards favoring HPCs ∗Equal contribution. 1This approach likely originates from the concept of paired statistical tests and the resulting variance reduction, but in our literature search we did not find any references discussing this in the context of HPO. For example, when comparing the performance of two classifiers on one dataset, paired tests are commonly 38th Conference on Neural Information Processing Systems (NeurIPS 2024). arXiv:2405.15393v2  [stat.ML]  7 Nov 2024which are specifically tailored to the chosen splits. Such and related effects, where we \"overoptimize\" the validation performance without effective reward in improved generalization performance have been sometimes dubbed \"overtuning\" or \"oversearching\". For a more detailed discussion of this topic, including related work, see Section 5 and Appendix B. The practice of reshuffling resampling splits during HPO is generally neither discussed in the scientific literature nor HPO software tools.2 To the best of our knowledge, only Lévesque (2018) investigated reshuffling train-validation splits for every new HPC. For both holdout and M-fold CV using reshuffled resampling splits resulted in, on average, slightly lower generalization error when used in combination with Bayesian optimization (BO, Garnett, 2023) or CMA-ES (Hansen & Ostermeier, 2001) as HPO algorithms. Additionally, reshuffling was used by a solution to the NeurIPS 2006 performance prediction challenge to estimate the final generalization performance (Guyon et al., 2006). Recently, in the context of evolutionary optimization, reshuffling was applied after every generation (Larcher & Barbosa, 2022). In this paper, we systematically examine the effect of reshuffling on HPO performance. Our contribu- tions can be summarized as follows: 1. We show theoretically that reshuffling resampling splits during HPO can result in finding a configuration with better overall generalization performance, especially when the loss surface is rather flat and its estimate is noisy (Section 2). 2. We confirm these theoretical insights through controlled simulation studies (Section 3). 3. We demonstrate in realistic HPO benchmark experiments that reshuffling splits can lead to a real-world improvement of HPO (Section 4). Especially in the case of reshuffled holdout, we find that the final generalization performance is often on par with 5-fold CV under a wide range of settings. We discuss results, limitations, and avenues for future research in Section 5. 2 Theoretical Analysis 2.1 Problem Statement and Setup Machine learning (ML) aims to fit a model to data, so that it generalizes well to new observations of the same distribution. Let D = {Zi}n i=1 be the observed dataset consisting of i.i.d. random variables from a distribution P, i.e., in the supervised setting Zi = ( Xi, Yi).3,4 Formally, an inducer g configured by an HPC λ ∈ Λ maps a dataset D to a model from our hypothesis space h = gλ(D) ∈ H. During HPO, we want to find a HPC that minimizes the expected generalization error, i.e., find λ∗ = arg min λ∈Λ µ(λ), where µ(λ) = E[ℓ(Z, gλ(D))], where ℓ(Z, h) is the loss of model h on a fresh observation Z. In practice, there is usually a limited computational budget for each HPO run, so we assume that there is only a finite number of distinct HPCs Λ = {λ1, . . . ,λJ} to be evaluated, which also simplifies the subsequent analysis. Naturally, we cannot optimize the generalization error directly, but only an estimate of it. To do so, a resampling is constructed. For every HPC λj, draw M random sets I1,j, . . . ,IM,j ⊂ {1, . . . , n} of validation indices with nvalid = ⌈αn⌉ instances each. The random index draws are assumed to be independent of the observed data. The data is then split accordingly into pairs Vm,j = {Zi}i∈Im,j , Tm,j = {Zi}i/∈Im,j of disjoint validation and training sets. Define the validation loss on the m-th fold L(Vm,j, gλj (Tm,j)) = 1 nvalid X i∈Im,j ℓ(Zi, gλj (Tm,j)), employed that implicitly assume that differences between the performance of classifiers on a given CV fold are comparable (Dietterich, 1998; Nadeau & Bengio, 1999, 2003; Demšar, 2006). 2In Appendix B, we present an overview of how resampling is addressed in tutorials and examples of standard HPO libraries and software. We conclude that usually fixed splits are used or recommended. 3Throughout, we use bold letters to indicate (fixed and random) vectors. 4We provide a notation table for symbols used in the main paper in Table 2 in the appendix. 2and the M-fold validation loss as bµ(λj) = 1 M MX m=1 L(Vm,j, gλj (Tm,j)). Since µ is unknown, we minimize bλ = arg minλ∈Λ bµ(λ), hoping that µ(bλ) will also be small. Typically, the same splits are used for every HPC, so Im,j = Im for all j = 1, . . . , Jand m = 1, . . . , M. In the following, we investigate how reshuffling train-validation splits (i.e., Im,j ̸= Im,j′ for j ̸= j′) affects the HPO problem. 2.2 How Reshuffling Affects the Loss Surface We first investigate how different validation and reshuffling strategies affect the empirical loss surface bµ. In particular, we derive the limiting distribution of the sequence √n(bµ(λj) − µ(λj))J j=1. This limiting regime will not only reveal the effect of reshuffling on the loss surface, but also give us a tractable setting to study HPO performance. Theorem 2.1. Under regularity conditions stated in Appendix C.1, it holds √n (bµ(λj) − µ(λj))J j=1 → N(0, Σ) in distribution, where Σi,j = τi,j,M K(λi, λj), τ i,j,M = lim n→∞ 1 nM2α2 nX s=1 MX m=1 MX m′=1 Pr(s ∈ Im,i ∩ Im′,j), and K(λi, λj) = lim n→∞ Cov[¯ℓn(Z′, λi), ¯ℓn(Z′, λj)], ¯ℓn(z, λ) = E[ℓ(z, gλ(T ))] − E[ℓ(Z, gλ(T ))], where the expectation is taken over a training set T of size n and two fresh samples Z, Z′ from the same distribution. The regularity conditions are rather mild and discussed further in Appendix C.1. The kernel K reflects the (co-)variability of the losses caused by validation samples. The contribution of training samples only has a higher-order effect. The validation scheme enters the distribution through the quantities τi,j,M . In what follows, we compute explicit expressions for some popular examples. The following list provides formal definitions for the index sets Im,j. (i) (holdout) Let M = 1 and I1,j = I1 for all j = 1, . . . , J, and some size-⌈αn⌉ index set I1. (ii) (reshuffled holdout) Let M = 1 and I1,1, . . . ,I1,J be independently drawn from the uniform distribution over all size-⌈αn⌉ subsets from {1, . . . , n}. (iii) (M-fold CV) Let α = 1/M and I1, . . . ,IM be a disjoint partition of {1, . . . , n}, and Im,j = Im for all j = 1, . . . , J. (iv) (reshuffled M-fold CV) Let α = 1/M and (I1,j, . . . ,IM,j ), j= 1, . . . , J, be independently drawn from the uniform distribution over disjoint partitions of {1, . . . , n}. (v) (M-fold holdout) LetIm, m= 1, . . . , M, be independently drawn from the uniform distribution over size-⌈αn⌉ subsets of {1, . . . , n} and set Im,j = Im for all m = 1, . . . , M, j= 1, . . . , J. (vi) (reshuffled M-fold holdout) Let Im,j, m= 1, . . . , M, j= 1, . . . , J, be independently drawn from the uniform distribution over size-⌈αn⌉ subsets of {1, . . . , n}. The value of τi,j,M for each example is computed explicitly in Appendix E. In all these examples, we in fact have τi,j,M = \u001aσ2, i = j τ2σ2, i ̸= j. , (1) for some method-dependent parameters σ, τshown in Table 1. The parameter σ2 captures any increase in variance caused by omitting an observation from the validation sets. The parameter τ quantifies a potential decrease in correlation in the loss surface due to reshuffling. More precisely, 3Table 1: Exemplary parametrizations in Equation (1) for resamplings; see Appendix E for details. Method σ2 τ2 holdout (HO) 1/α 1 reshuffled HO 1/α α M-fold CV 1 1 reshuffled M-fold CV 1 1 M-fold HO (subsampling / Monte Carlo CV) 1 + (1− α)/Mα 1 reshuffled M-fold HO 1 + (1− α)/Mα 1/(1 + (1− α)/Mα) the observed losses bµ(λi), bµ(λj) at distinct HPCs λi ̸= λj become less correlated when τ is small. Generally, an increase in variance leads to worse generalization performance. The effect of a correlation decrease is less obvious and is studied in detail in the following section. We make the following observations about the differences between methods in Table 1: • M-fold CV incurs no increase in variance (σ2 = 1) and — because every HPC uses the same folds — no decrease in correlation. Interestingly, the correlation does not even decrease when reshuffling the folds. In any case, all samples are used exactly once as validation and training instance. At least asymptotically, this leads to the same behavior, and reshuffling should have almost no effect on M-fold CV . • The two (1-fold) holdout methods bear the same 1/α increase in variance. This is caused by only using a fraction α of the data as validation samples. Reshuffled holdout also decreases the correlation parameter τ2. In fact, if HPCs λi ̸= λj are evaluated on largely distinct samples, the validation losses bµ(λi) and bµ(λj) become almost independent. • M-fold holdout also increases the variance, because some samples may still be omitted from validation sets. This increase is much smaller for large M. Accordingly, the correlation is also decreased by less in the reshuffled variant. 2.3 How Reshuffling Affects HPO Performance In practice, we are mainly interested in the performance of a model trained with the optimal HPC bλ. To simplify the analysis, we explore this in the large-sample regime derived in the previous section. Assume bµ(λj) = µ(λj) + ϵ(λj) (2) where ϵ(λ) is a zero-mean Gaussian process with covariance kernel Cov(ϵ(λ), ϵ(λ′)) = \u001aK(λ, λ) if λ = λ′, τ2K(λ, λ′) else. (3) Let Λ ⊆ {λ ∈ Rd : ∥λ∥ ≤1} with |Λ| = J <∞ be the set of hyperparameters. Theorem 2.2 ahead gives a bound on the expected regretE[µ(bλ)−µ(λ∗)]. It depends on several quantities characterizing the difficulty of the HPO problem. The constant κ = sup ∥λ∥,∥λ′∥≤1 |K(λ, λ) − K(λ, λ′)| K(λ, λ)∥λ − λ′∥2 . can be interpreted as a measure of correlation of the process ϵ. In particular, Corr(ϵ(λ), ϵ(λ′)) ≥ 1 − κ∥λ − λ′∥2. The constant is small when ϵ is strongly correlated, and large otherwise. Further, define η as the minimal number such that any η-ball contained in {∥λ∥ ≤1} contains at least one element of Λ. It measures how densely the set of candidate HPCsΛ covers set of all possible HPCs. If Λ is a deterministic uniform grid, we have about η ≈ J−1/d. Similarly, Lemma D.1 in the Appendix shows that η ≲ J−1/2d when randomly sampling HPCs. Finally, the constant m = sup λ∈Λ |µ(λ) − µ(λ∗)| ∥λ − λ∗∥2 , 4−2 −1 0 1 2 3 0.00 0.25 0.50 0.75 1.00 λ Loss Surface T ype T rue Empirical ( τ = 1 ) Empirical ( τ = 0.3 ) (a) High signal-to-noise ratio −2 0 2 0.00 0.25 0.50 0.75 1.00 λ Loss Surface T ype T rue Empirical ( τ = 1 ) Empirical ( τ = 0.3 ) (b) Low signal-to-noise ratio Figure 1: Example of reshuffled empirical loss yielding a worse (left) and better (right) minimizer. measures the local curvature at the minimum of the loss surface µ. Finding an HPC λ close to the theoretical optimum λ∗ is easier when the minimum is more pronounced (large m). On the other hand, the regret µ(λ) − µ(λ∗) is also punishing mistakes more quickly. Defining log(x)+ = max{0, log(x)}, we can now state our main result. Theorem 2.2. Let bµ follow the Gaussian process model(2). Suppose κ <∞, 0 < σ2 ≤ Var[ϵ(λ)] ≤ σ2 < ∞ for all λ ∈ Λ, and m >0. Then E[µ(bλ) − µ(λ∗)] ≤ σ √ d[8 + B(τ) − A(τ)]. where B(τ) = 48 hp 1 − τ2 p log J + τ p 1 + log(3κ)+ i , A (τ) = p 1 − τ2(σ/σ) s log \u0012 σ 2mη2 \u0013 + . The numeric constants result from several simplifications in a worst-case analysis, which lowers their practical relevance. A qualitative analysis of the bound is still insightful. The bound is increasing in σ and d, indicating that the HPO problem is harder when there is a lot of noise or there are many parameters to tune. The terms B(τ) and A(τ) have conceptual interpretations: • The term B(τ) quantifies how likely it is to pick a bad bλ because of bad luck: a λ far away from λ∗ had such a small ϵ(λ) that it outweighs the increase in µ. Such events are more likely when the process ϵ is weakly correlated. Accordingly, B(τ) is decreasing in τ and increasing in κ. • The term A(τ) quantifies how likely it is to pick a good bλ by luck: a λ close to λ∗ had such a small ϵ(λ) that it overshoots all the other fluctuations. Also such events are more likely when the process ϵ is weakly correlated. Accordingly, the term A(τ) is decreasing in τ. The B, as stated, is unbounded, but a closer inspection of the proof shows that it is upper bounded by √log J. This bound is attained only in the unrealistic scenario when the validation losses are essentially uncorrelated across all HPCs. The term A is bounded from below by zero, which is also the worst case because the term enters our regret bound with a negative sign. Both A and B are decreasing in the reshuffling parameter τ. There are two regimes. If σ/2mη2 ≤ e, then A(τ) = 0 and reshuffling cannot lead to an improvement of the bound. The term σ/mη2 can be interpreted as noise-to-signal ratio (relative to the grid density). If the signal is much stronger than the noise, the HPO problem is so easy that reshuffling will not help. This situation is illustrated in Figure 1a. If on the other hand σ/mη2 > e, the terms A(τ) and B(τ) enter the bound with opposing signs. This creates tension: reshuffling between HPCs increases B(τ), which is countered by a decrease in A(τ). So which scenarios favor reshuffling? When the process ϵ is strongly correlated, κ is small and reshuffling (decreasing τ) incurs a high cost in B(τ). This is intuitive: When there is strong 5correlation, the validation loss surface bµ is essentially just a vertical shift of µ. Finding the optimal λ is then almost as easy as if we would know µ, and decorrelating the surface through reshuffling would make it unnecessarily hard. When ϵ is less correlated (κ large) however, reshuffling does not hurt the term B(τ) as much, but we can reap all the benefits of increasing A(τ). Here, the effect of reshuffling can be interpreted as hedging against the catastrophic case where all bµ(λ) close to the optimal λ∗ are simultaneously dominated by a region of bad hyperparameters. This is illustrated in Figure 1b. 3 Simulation Study To test our theoretical understanding of the potential benefits of reshuffling resampling splits during HPO, we conduct a simulation study. This study helps us explore the effects of reshuffling in a controlled setting. 3.1 Design We construct a univariate quadratic loss surface function µ : Λ ⊂ R 7→ R, λ→ m(λ − 0.5)2/2 which we want to minimize. The global minimum is given at µ(0.5) = 0 . Combined with a kernel for the noise process ϵ as in Equation (3), this allows us to simulate an objective as ob- served during HPO by sampling bµ(λ) = µ(λ) + ϵ(λ). We use a squared exponential kernel K(λ, λ′) = σ2 K exp (−κ(λ − λ′)2/2) that is plugged into the covariance kernel of the noise process ϵ in Equation (3). The parameters m and κ in our simulation setup correspond exactly to the curva- ture and correlation constants from the previous sections. Recall that Theorem 2.2 states that the effect of reshuffling strongly depends on the curvature m of the loss surface µ (a larger m implies a stronger curvature) and the constant κ as a measure of correlation of the noise ϵ (a larger κ implies weaker correlation). Combined with the possibility to vary τ in the covariance kernel of ϵ, we can systematically investigate how curvature of the loss surface, correlation of the noise and the extent of reshuffling affect optimization performance. In each simulation run, we simulate the observed objective ˆµ(λ), identify the minimizer ˆλ = arg minλ∈Λ ˆµ(λ), and calculate its true risk, µ(ˆλ). We repeat this process 10000 times for various combinations of τ, m, and κ. 3.2 Results Figure 2 visualizes the true risk of the configuration ˆλ that minimizes the observed objective. We observe that for a loss surface with low curvature (i.e., m ≤ 2), reshuffling is beneficial (lower values of τ resulting in a better true risk of the configuration that optimizes the observed objective) as long as the noise process is not too correlated (i.e., κ ≥ 1). As soon as the noise process is more strongly correlated, even flat valleys of the true risk µ remain clearly visible in the observed risk bµ, and reshuffling starts to hurt the optimization performance. Moving to scenarios of high curvature, the general relationship of m and κ remains the same, but reshuffling starts to hurt optimization performance already with weaker correlation in the noise. In summary, the simulations show that in cases of low curvature of the loss surface, reshuffling (reducing τ) tends to improve the true risk of the optimized configuration, especially when the loss surface is flat (small m) and the noise is not strongly correlated (i.e., κ is large). This exactly confirms our theoretical predictions from the previous section. 4 Benchmark Experiments In this section, we present benchmark experiments of real-world HPO problems where we investigate the effect of reshuffling resampling splits during HPO. First, we discuss the experimental setup. Second, we present results for HPO using random search (Bergstra & Bengio, 2012). Third, we also show the effect of reshuffling when applied in BO using HEBO (Cowen-Rivers et al., 2022) and SMAC3 (Lindauer et al., 2022). Recall that our theoretical insight suggests that 1) reshuffling might be beneficial during HPO and 2) holdout should be affected the most by reshuffling and other resamplings should only be affected to a lesser extent. 6m: 20 κ: 0.04 m: 20 κ: 1 m: 20 κ: 4 m: 20 κ: 100 m: 10 κ: 0.04 m: 10 κ: 1 m: 10 κ: 4 m: 10 κ: 100 m: 2 κ: 0.04 m: 2 κ: 1 m: 2 κ: 4 m: 2 κ: 100 0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00 0.070 0.075 0.080 0.085 0.090 0.175 0.200 0.225 0.250 0.275 0.21 0.24 0.27 0.30 0.08 0.10 0.12 0.18 0.19 0.20 0.100 0.125 0.150 0.175 0.200 0.07 0.08 0.09 0.10 0.11 0.12 0.08 0.12 0.16 0.05 0.10 0.15 0.20 0.02 0.04 0.06 0.00 0.05 0.10 0.15 0.00 0.05 0.10 0.15 0.20 τ μ(λ^) Figure 2: Mean true risk (lower is better) of the configuration minimizing the observed objective systematically varied with respect to curvature m, correlation strength κ of the noise (a larger κ implying weaker correlation), and extent of reshuffling τ (lower τ increasing reshuffling). A τ of 1 indicates no reshuffling. Error bars represent standard errors. 4.1 Experimental Setup As benchmark tasks, we use a set of standard HPO problems defined on small- to medium-sized tabular datasets for binary classification. We suspect the effect of the resampling variant used and whether the resampling is reshuffled to be larger for smaller datasets, where the variance of the validation loss estimator is naturally higher. Furthermore, from a practical perspective, this also ensures computational feasibility given the large number of HPO runs in our experiments. We systematically vary the learning algorithm, optimized performance metric, resampling method, whether the resampling is reshuffled, and the size of the dataset used for training and validation during HPO. Below, we outline the general experimental design and refer to Appendix F for details. We used a subset of the datasets defined by the AutoML benchmark (Gijsbers et al., 2024), treating these as data generating processes (DGPs; Hothorn et al., 2005). We only considered datasets with less than 100 features to reduce the required computation time and required the number of observations to be between 10000 and 1000000; for further details see Appendix F.1. Our aim was to robustly measure the generalization performance when varying the size n, which, as defined in Section 2 denotes the size of the combined data for model selection, so one training and validation set combined. First, we sampled 5000 data points per dataset for robust assessment of the generalization error; these points are not used during HPO in any way. Then, from the remaining points we sampled tasks with n ∈ {500, 1000, 5000}. We selected CatBoost (Prokhorenkova et al., 2018) and XGBoost (Chen & Guestrin, 2016) for their state-of-the-art performance on tabular data (Grinsztajn et al., 2022; Borisov et al., 2022; McElfresh et al., 2023; Kohli et al., 2024). Additionally, we included an Elastic Net (Zou & Hastie, 2005) to represent a linear baseline with a smaller search space and a funnel-shaped MLP (Zimmer et al., 2021) as a cost-effective neural network baseline. We provide details regarding training pipelines and search spaces in Appendix F.2. We conduct a random search with500 HPC evaluations for every resampling strategy we described in Table 1, for both fixed and reshuffled splits. We always use 80/20 train-validation splits for holdout 7500 1000 5000 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 −0.730 −0.725 −0.720 −0.715 −0.70 −0.69 −0.68 −0.67 −0.66 −0.68 −0.67 −0.66 −0.65 −0.64 −0.63 No. HPC Evaluations Mean T est Performance Reshuffling FALSE TRUE Resampling Holdout 5−fold CV 5−fold Holdout 5x 5−fold CV Figure 3: Average test performance (negative ROC AUC) of the incumbent for XGBoost on dataset albert for increasing n (train-validation sizes, columns). Shaded areas represent standard errors. and 5-fold CVs, so that training set size (and negative estimation bias) are the same. Anytime test performance of an HPO run is assessed by re-training the current incumbent (i.e. the best HPC until the current HPO iteration based on validation performance) on all available train and validation data and evaluating its performance on the outer test set. Note we do this for scientific evaluation in this experiment; obviously, this is not possible in practice. Using random search allows us to record various metrics and afterwards simulate optimizing for different ones, specifically, we recorded accuracy, area under the ROC curve (ROC AUC) and logloss. We also investigated the effect of reshuffling on two state-of-the-art BO variants (Eggensperger et al., 2021; Turner et al., 2021), namely HEBO (Cowen-Rivers et al., 2022) and SMAC3 (Lindauer et al., 2022). The experimental design was the same as for random search, except for the budget, which we reduced from 500 HPCs to 250 HPCs, and only optimized ROC AUC. 4.2 Experimental Results In the following, we focus on the results obtained using ROC AUC. We present aggregated results over different tasks, learning algorithms and replications to get a general understanding of the effects. Unaggregated results and results involving accuracy and logloss can be found in Appendix G. Results of Reshuffling Different Resamplings For each resampling (holdout, 5-fold holdout, 5-fold CV , and 5x 5-fold CV), we empirically analyze the effect of reshuffling train and validation splits during HPO. In Figure 3 we exemplarily show how test performance develops over the course of an HPO run on a single task for different resamplings (with and without reshuffling). Naturally, test performance does not necessarily increase in a monotonic fashion, and especially holdout without reshuffling tends to be unstable. Its reshuffled version results in substantially better test performance. Next, we look at the relative improvement (compared to standard 5-fold CV , which we consider our baseline) with respect to test ROC AUC performance of the incumbent over time in Figure 4, i.e., the difference in test performance of the incumbent between standard 5-fold CV and a different resampling protocol; hence a positive difference tells us how much better in test error we are, if we would have chosen the other protocol instead 5-fold CV . We observe that reshuffling generally results in equal or better performance compared to the same resampling protocol without reshuffling. For 5-fold holdout and especially 5-fold CV and 5x 5-fold CV , reshuffling has a smaller effect on relative test performance improvement, as expected. Holdout is affected the most by reshuffling and results in substantially better relative test performance compared to standard holdout. We also observe that an HPO protocol based on reshuffled holdout results in similar final test performance as standard 5-fold CV while overall being substantially cheaper due to requiring less model fits per HPC evaluation. In Appendix G.2, we further provide an ablation study on the number of folds when using M-fold holdout, where we observed that – in line with our theory – the more folds are used, the less reshuffling affects M-fold holdout. 8500 1000 5000 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 −0.50 −0.25 0.00 0.25 −1.2 −0.8 −0.4 0.0 0.4 −1.0 −0.5 0.0 0.5 No. HPC Evaluations Mean T est Improvement Reshuffling FALSE TRUE Resampling Holdout 5−fold CV 5−fold Holdout 5x 5−fold CV Figure 4: Average improvement (compared to standard 5-fold CV) with respect to test performance (ROC AUC) of the incumbent over different tasks, learning algorithms and replications separately for increasing n (train-validation sizes, columns). Shaded areas represent standard errors. However, this general trend can vary for certain combinations of classifier and performance metric, see Appendix G. Especially for logloss, we observed that reshuffling rarely is beneficial; see the discussion in Section 5. Finally, the different resamplings generally behave as expected. The more we are willing to invest compute resources into a more intensive resampling like 5-fold CV or 5x 5-fold CV , the better the generalization performance of the final incumbent. Results for BO and Reshuffling Figure 5 shows that, generally HEBO and SMAC3 outperform random search with respect to generalization performance (i.e., comparing HEBO and SMAC3 to random search under standard holdout, or comparing under reshuffled holdout). More interestingly, HEBO, SMAC3 and random search all strongly benefit from reshuffling. Moreover, the performance gap between HEBO and random search but also SMAC3 and random search narrows when the resampling is reshuffled, which is an interesting finding of its own: As soon as we are concerned with generalization performance of HPO and not only investigate validation performance during optimization, the choice of optimizer might have less impact on final generalization performance compared to other choices such as whether the resampling is reshuffled during HPO or not. We present results for BO and reshuffling for different resamplings in Appendix G. 500 1000 5000 1 50 100 150 200 250 1 50 100 150 200 250 1 50 100 150 200 250 0.0 0.5 1.0 1.5 −0.5 0.0 0.5 1.0 −0.5 0.0 0.5 1.0 No. HPC Evaluations Mean T est Improvement Optimizer Random Search HEBO SMAC3 Reshuffling FALSE TRUE Figure 5: Average improvement (compared to random search on standard holdout) with respect to test performance (ROC AUC) of the incumbent over tasks, learning algorithms and replications for different n (train-validation sizes, columns). Shaded areas represent standard errors. 5 Discussion In the previous sections, we have shown theoretically and empirically that reshuffling can enhance generalization performance of HPO. The main purpose of this article is to draw attention to this 9surprising fact about a technique that is simple but rarely discussed. Our work goes beyond a preliminary experimental study on reshuffling (Lévesque, 2018), in that we also study the effect of reshuffling on random search, multiple metrics and learning algorithms, and most importantly, for the first time, we provide a theoretical analysis that explains why reshuffling can be beneficial. Limitations To unveil the mechanisms underlying the reshuffling procedures, our theoretical analysis relies on an asymptotic approximation of the empirical loss surface. This allows us to operate on Gaussian loss surfaces, which exhibit convenient concentration and anti-concentration properties required in our proof. The latter are lacking for general distributions, which explains our asymptotic approach. The analysis was further facilitated by a loss stability assumption regarding the learning algorithms that is generally rather mild; see the discussion in Bayle et al. (2020). However, it typically fails for highly sensitive losses, which has practical consequences. In fact, Figure 9 in Appendix G shows that reshuffling usually hurts generalization for the logloss and small sample sizes. It is still an open question whether this problem can be fixed by less naive implementations of the technique. Another limitation is our focus on generalization after search through a fixed, finite set of candidates. This largely ignores the dynamic nature of many HPO algorithms, which would greatly complicate our analysis. Finally, our experiments are limited in that we restricted ourselves to tabular data and binary classification and we avoided extremely small or large datasets. Relation to Overfitting The fact that generalization performance can decrease during HPO (or computational model selection in general) is sometimes known as oversearching, overtuning, or overfitting to the validation set (Quinlan & Cameron-Jones, 1995; Escalante et al., 2009; Koch et al., 2010; Igel, 2012; Bischl et al., 2023), but has arguably not been studied very thoroughly. Given recent theoretical (Feldman et al., 2019) and empirical (Purucker & Beel, 2023) findings, we expect less overtuning on multi-class datasets, making it interesting to see how reshuffling would affect the generalization performance. Several works suggest strategies to counteract this effect. First, LOOCVCV proposes a conservative choice of incumbents (Ng, 1997) at the cost of leave-one-out analysis or an additional hyperparameter. Second, it is possible to use an extra selection set (Igel, 2012; Lévesque, 2018; Mohr et al., 2018) at the cost of reduced training data, which was found to lead to reduced overall performance (Lévesque, 2018). Third, by using early stopping one can stop hyperparameter optimization before the generaliza- tion performance degrades again. This was so far demonstrated to be able to save compute budget at only marginally reduced performance, but also requires either a sensitivity hyperparameter or correct estimation of the variance of the generalization estimate and was only developed for cross-validation so far (Makarova et al., 2022). Reshuffling itself is orthogonal to these proposals and a combination with the above-mentioned methods might result in further improvements. Outlook Generally, the related literature detects overfitting to the validation set either visually (Ng, 1997) or by measuring it (Koch et al., 2010; Igel, 2012; Fabris & Freitas, 2019). Developing a unified formal definition of the above-mentioned terms and thoroughly analyzing the effect of decreased generalization performance after many HPO iterations and how it relates to our measurements of the validation performance is an important direction for future work. We further found, both theoretically and experimentally, that investing more resources when evaluating each HPC can result in better final HPO performance. To reduce the computational burden on HPO again, we suggest further investigating the use of adaptive CV techniques, as proposed by Auto- WEKA (Thornton et al., 2013) or under the name Lazy Paired Hyperparameter Tuning (Zheng & Bilenko, 2013). Designing more advanced HPO algorithms exploiting the reshuffling effect should be a promising avenue for further research. Acknowledgments and Disclosure of Funding We thank Martin Binder and Florian Karl for helpful discussions. Lennart Schneider is supported by the Bavarian Ministry of Economic Affairs, Regional Development and Energy through the Center for Analytics - Data - Applications (ADACenter) within the framework of BAYERN DIGITAL II (20-3410-2-9-8). Lennart Schneider acknowledges funding from the LMU Mentoring Program of the Faculty of Mathematics, Informatics and Statistics. 10References Arlot, S. and Celisse, A. A survey of cross-validation procedures for model selection. Statistics Surveys, 4:40 – 79, 2010. B Austern, M. and Zhou, W. Asymptotics of cross-validation. arXiv:2001.11111 [math.ST], 2020. C.1 Awad, N., Mallik, N., and Hutter, F. DEHB: Evolutionary hyberband for scalable, robust and efficient Hyperparameter Optimization. In Zhou, Z. (ed.), Proceedings of the 30th International Joint Conference on Artificial Intelligence (IJCAI’21), pp. 2147–2153, 2021. B Bayle, P., Bayle, A., Janson, L., and Mackey, L. Cross-validation confidence intervals for test error. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M.-F., and Lin, H. (eds.),Proceedings of the 33rd International Conference on Advances in Neural Information Processing Systems (NeurIPS’20), pp. 16339–16350. Curran Associates, 2020. 5, C.1, C.1, C.1 Bergman, E., Purucker, L., and Hutter, F. Don’t waste your time: Early stopping cross-validation. In Eggensperger, K., Garnett, R., Vanschoren, J., Lindauer, M., and Gardner, J. (eds.),Proceedings of the Third International Conference on Automated Machine Learning, volume 256 of Proceedings of Machine Learning Research, pp. 9/1–31. PMLR, 2024. B Bergstra, J. and Bengio, Y . Random search for hyper-parameter optimization.Journal of Machine Learning Research, 13:281–305, 2012. 4, B Bischl, B., Binder, M., Lang, M., Pielok, T., Richter, J., Coors, S., Thomas, J., Ullmann, T., Becker, M., Boulesteix, A., Deng, D., and Lindauer, M. Hyperparameter optimization: Foundations, algorithms, best practices, and open challenges. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, pp. e1484, 2023. 1, 5, B Blum, A., Kalai, A., and Langford, J. Beating the hold-out: Bounds for k-fold and progressive cross-validation. In Proceedings of the Twelfth Annual Conference on Computational Learning Theory, COLT ’99, pp. 203–208, 1999. B Borisov, V ., Leemann, T., Seßler, K., Haug, J., Pawelczyk, M., and Kasneci, G. Deep neural networks and tabular data: A survey. IEEE Transactions on Neural Networks and Learning Systems, pp. 1–21, 2022. 4.1 Bouckaert, Remcoand Frank, E. Evaluating the Replicability of Significance Tests for Comparing Learning Algorithms. In Dai, H., Srikant, R., and Zhang, C. (eds.), Advances in Knowledge Discovery and Data Mining, pp. 3–12. Springer, 2004. B Bousquet, O. and Zhivotovskiy, N. Fast classification rates without standard margin assumptions. Information and Inference: A Journal of the IMA, 10(4):1389–1421, 2021. C.1 Bouthillier, X., Delaunay, P., Bronzi, M., Trofimov, A., Nichyporuk, B., Szeto, J., Sepahvand, N. M., Raff, E., Madan, K., V oleti, V ., Kahou, S. E., Michalski, V ., Arbel, T., Pal, C., Varoquaux, G., and Vincent, P. Accounting for variance in machine learning benchmarks. In Smola, A., Dimakis, A., and Stoica, I. (eds.), Proceedings of Machine Learning and Systems 3, volume 3, pp. 747–769, 2021. B Buczak, P., Groll, A., Pauly, M., Rehof, J., and Horn, D. Using sequential statistical tests for efficient hyperparameter tuning. AStA Advances in Statistical Analysis, 108(2):441–460, 2024. B Cawley, G. and Talbot, N. On Overfitting in Model Selection and Subsequent Selection Bias in Performance Evaluation. Journal of Machine Learning Research, 11:2079–2107, 2010. B Chen, T. and Guestrin, C. XGBoost: A scalable tree boosting system. In Krishnapuram, B., Shah, M., Smola, A., Aggarwal, C., Shen, D., and Rastogi, R. (eds.), Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD’16) , pp. 785–794. ACM Press, 2016. 4.1 Cowen-Rivers, A., Lyu, W., Tutunov, R., Wang, Z., Grosnit, A., Griffiths, R., Maraval, A., Jianye, H., Wang, J., Peters, J., and Ammar, H. HEBO: Pushing the limits of sample-efficient hyper-parameter optimisation. Journal of Artificial Intelligence Research, 74:1269–1349, 2022. 4, 4.1, B 11Demšar, J. Statistical comparisons of classifiers over multiple data sets. Journal of Machine Learning Research, 7:1–30, 2006. 1 Dietterich, T. G. Approximate statistical tests for comparing supervised classification learning algorithms. Neural Computation, 10(7):1895–1923, 1998. 1 Dunias, Z., Van Calster, B., Timmerman, D., Boulesteix, A.-L., and van Smeden, M. A comparison of hyperparameter tuning procedures for clinical prediction models: A simulation study. Statistics in Medicine, 43(6):1119–1134, 2024. B Eggensperger, K., Lindauer, M., Hoos, H., Hutter, F., and Leyton-Brown, K. Efficient benchmarking of algorithm configurators via model-based surrogates. Machine Learning, 107(1):15–41, 2018. 5 Eggensperger, K., Lindauer, M., and Hutter, F. Pitfalls and best practices in algorithm configuration. Journal of Artificial Intelligence Research, pp. 861–893, 2019. B Eggensperger, K., Müller, P., Mallik, N., Feurer, M., Sass, R., Klein, A., Awad, N., Lindauer, M., and Hutter, F. HPOBench: A collection of reproducible multi-fidelity benchmark problems for HPO. In Vanschoren & Yeung (2021). 4.1, B Escalante, H., Montes, M., and Sucar, E. Particle Swarm Model Selection. Journal of Machine Learning Research, 10:405–440, 2009. 5 Fabris, F. and Freitas, A. Analysing the overfit of the auto-sklearn automated machine learning tool. In Nicosia, G., Pardalos, P., Umeton, R., Giuffrida, G., and Sciacca, V . (eds.), Machine Learning, Optimization, and Data Science, volume 11943 of Lecture Notes in Computer Science, pp. 508–520, 2019. 5 Falkner, S., Klein, A., and Hutter, F. BOHB: Robust and efficient Hyperparameter Optimization at scale. In Dy, J. and Krause, A. (eds.), Proceedings of the 35th International Conference on Machine Learning (ICML’18), volume 80, pp. 1437–1446. Proceedings of Machine Learning Research, 2018. B Feldman, V ., Frostig, R., and Hardt, M. The advantages of multiple classes for reducing overfitting from test set reuse. In Chaudhuri, K. and Salakhutdinov, R. (eds.), Proceedings of the 36th Interna- tional Conference on Machine Learning (ICML’19), volume 97, pp. 1892–1900. Proceedings of Machine Learning Research, 2019. 5 Feurer, M. and Hutter, F. Hyperparameter Optimization. In Hutter et al. (2019), chapter 1, pp. 3 – 38. Available for free at http://automl.org/book. 1, B Feurer, M., Eggensperger, K., Falkner, S., Lindauer, M., and Hutter, F. Auto-Sklearn 2.0: Hands-free automl via meta-learning. Journal of Machine Learning Research, 23(261):1–61, 2022. B Garnett, R. Bayesian Optimization. Cambridge University Press, 2023. 1, B Gijsbers, P., Bueno, M., Coors, S., LeDell, E., Poirier, S., Thomas, J., Bischl, B., and Vanschoren, J. AMLB: an automl benchmark. Journal of Machine Learning Research, 25(101):1–65, 2024. 4.1 Giné, E. and Nickl, R. Mathematical Foundations of Infinite-Dimensional Statistical Models, vol- ume 40. Cambridge University Press, 2016. C.2 Grinsztajn, L., Oyallon, E., and Varoquaux, G. Why do tree-based models still outperform deep learning on typical tabular data? In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks, pp. 507–520, 2022. 4.1 Guyon, I., Alamdari, A., Dror, G., and Buhmann, J. Performance prediction challenge. In The 2006 IEEE International Joint Conference on Neural Network Proceedings, 2006. 1 Guyon, I., Saffari, A., Dror, G., and Cawley, G. Model selection: Beyond the Bayesian/Frequentist divide. Journal of Machine Learning Research, 11:61–87, 2010. B 12Guyon, I., Bennett, K., Cawley, G., Escalante, H. J., Escalera, S., Ho, T. K., Macià, N., Ray, B., Saeed, M., Statnikov, A., and Viegas, E. Design of the 2015 ChaLearn AutoML challenge. In 2015 International Joint Conference on Neural Networks (IJCNN’15), pp. 1–8. International Neural Network Society and IEEE Computational Intelligence Society, IEEE, 2015. B Guyon, I., Sun-Hosoya, L., Boullé, M., Escalante, H., Escalera, S., Liu, Z., Jajetic, D., Ray, B., Saeed, M., Sebag, M., Statnikov, A., Tu, W., and Viegas, E. Analysis of the AutoML Challenge Series 2015-2018. In Hutter et al. (2019), chapter 10, pp. 177–219. Available for free at http: //automl.org/book. B Guyon, I., Lindauer, M., van der Schaar, M., Hutter, F., and Garnett, R. (eds.). Proceedings of the First International Conference on Automated Machine Learning, 2022. Proceedings of Machine Learning Research. 5 Hansen, N. and Ostermeier, A. Completely derandomized self-adaptation in evolution strategies. Evolutionary C., 9(2):159–195, 2001. 1 Hothorn, T., Leisch, F., Zeileis, A., and Hornik, K. The design and analysis of benchmark experiments. Journal of Computational and Graphical Statistics, 14(3):675–699, 2005. 4.1, F.1 Hutter, F., Kotthoff, L., and Vanschoren, J. (eds.).Automated Machine Learning: Methods, Systems, Challenges. Springer, 2019. Available for free at http://automl.org/book. 5 Igel, C. A note on generalization loss when evolving adaptive pattern recognition systems. IEEE Transactions on Evolutionary Computation, 17(3):345–352, 2012. 5, 5 Jamieson, K. and Talwalkar, A. Non-stochastic best arm identification and Hyperparameter Op- timization. In Gretton, A. and Robert, C. (eds.), Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics (AISTATS’16) , volume 51. Proceedings of Machine Learning Research, 2016. B Kadra, A., Janowski, M., Wistuba, M., and Grabocka, J. Scaling laws for hyperparameter optimization. In Oh, A., Naumann, T., Globerson, A., Saenko, K., Hardt, M., and Levine, S. (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 47527–47553, 2023. B Kallenberg, O. Foundations of modern probability, volume 2. Springer, 1997. D Klein, A., Falkner, S., Bartels, S., Hennig, P., and Hutter, F. Fast Bayesian optimization of machine learning hyperparameters on large datasets. In Singh, A. and Zhu, J. (eds.), Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics (AISTATS’17), volume 54. Proceedings of Machine Learning Research, 2017. B Koch, P., Konen, W., Flasch, O., and Bartz-Beielstein, T. Optimizing support vector machines for stormwater prediction. Technical Report TR10-2-007, Technische Universität Dortmund, 2010. Proceedings of Workshop on Experimental Methods for the Assessment of Computational Systems joint to PPSN2010. 5, 5 Kohli, R., Feurer, M., Bischl, B., Eggensperger, K., and Hutter, F. Towards quantifying the effect of datasets for benchmarking: A look at tabular machine learning. In Data-centric Machine Learning (DMLR) workshop at the International Conference on Learning Representations (ICLR), 2024. 4.1 Lang, M., Kotthaus, H., Marwedel, P., Weihs, C., Rahnenführer, J., and Bischl, B. Automatic model selection for high-dimensional survival analysis. Journal of Statistical Computation and Simulation, 85:62–76, 2015. B Larcher, C. and Barbosa, H. Evaluating models with dynamic sampling holdout in auto-ml. SN Computer Science, 3(506), 2022. 1 Li, L., Jamieson, K., DeSalvo, G., Rostamizadeh, A., and Talwalkar, A. Hyperband: A novel bandit-based approach to Hyperparameter Optimization. Journal of Machine Learning Research, 18(185):1–52, 2018. B Lindauer, M., Eggensperger, K., Feurer, M., Biedenkapp, A., Deng, D., Benjamins, C., Ruhkopf, T., Sass, R., and Hutter, F. SMAC3: A versatile bayesian optimization package for Hyperparameter Optimization. Journal of Machine Learning Research, 23(54):1–9, 2022. 4, 4.1, B 13Loshchilov, I. and Hutter, F. CMA-ES for Hyperparameter Optimization of deep neural networks. In International Conference on Learning Representations Workshop track, 2016. Published online: iclr.cc. B Lévesque, J. Bayesian Hyperparameter Optimization: Overfitting, Ensembles and Conditional Spaces. PhD thesis, Université Laval, 2018. 1, 5, 5 Makarova, A., Shen, H., Perrone, V ., Klein, A., Faddoul, J., Krause, A., Seeger, M., and Archambeau, C. Automatic termination for hyperparameter optimization. In Guyon et al. (2022). 5 Mallik, N., Bergman, E., Hvarfner, C., Stoll, D., Janowski, M., Lindauer, M., Nardi, L., and Hutter, F. PriorBand: Practical hyperparameter optimization in the age of deep learning. In Oh et al. (2023). B McElfresh, D., Khandagale, S., Valverde, J., Prasad C., V ., Ramakrishnan, G., Goldblum, M., and White, C. When do neural nets outperform boosted trees on tabular data? In Oh et al. (2023), pp. 76336–76369. 4.1, F.2 Mohr, F., Wever, M., and Hüllermeier, E. ML-Plan: Automated machine learning via hierarchical planning. Machine Learning, 107(8-10):1495–1515, 2018. 5, B Molinaro, A., Simon, R., and Pfeiffer, R. Prediction error estimation: A comparison of resampling methods. Bioinformatics, 21(15):3301–3307, 2005. B Nadeau, C. and Bengio, Y . Inference for the generalization error. In Solla, S., Leen, T., and Müller, K. (eds.), Proceedings of the 13th International Conference on Advances in Neural Information Processing Systems (NeurIPS’99). The MIT Press, 1999. 1 Nadeau, C. and Bengio, Y . Inference for the generalization error.Machine Learning, 52:239–281, 2003. 1 Ng, A. Preventing “overfitting”’ of cross-validation data. In Fisher, D. H. (ed.), Proceedings of the Fourteenth International Conference on Machine Learning (ICML’97), pp. 245–253. Morgan Kaufmann Publishers, 1997. 5, 5 Oh, A., Neumann, T., Globerson, A., Saenko, K., Hardt, M., and Levine, S. (eds.). Proceedings of the 36th International Conference on Advances in Neural Information Processing Systems (NeurIPS’23), 2023. Curran Associates. 5 Pfisterer, F., Schneider, L., Moosbauer, J., Binder, M., and Bischl, B. YAHPO Gym – an efficient multi-objective multi-fidelity benchmark for hyperparameter optimization. In Guyon et al. (2022). B, 5 Pineda Arango, S., Jomaa, H., Wistuba, M., and Grabocka, J. HPO-B: A large-scale reproducible benchmark for black-box HPO based on OpenML. In Vanschoren & Yeung (2021). B, 5 Probst, P., Boulesteix, A., and Bischl, B. Tunability: Importance of hyperparameters of machine learning algorithms. Journal of Machine Learning Research, 20(53):1–32, 2019. 1 Prokhorenkova, L., Gusev, G., V orobev, A., Dorogush, A., and Gulin, A. Catboost: Unbiased boosting with categorical features. In Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R. (eds.), Proceedings of the 31st International Conference on Advances in Neural Information Processing Systems (NeurIPS’18), pp. 6639–6649. Curran Associates, 2018. 4.1 Purucker, L. and Beel, J. CMA-ES for post hoc ensembling in automl: A great success and salvageable failure. In Faust, A., Garnett, R., White, C., Hutter, F., and Gardner, J. R. (eds.),Proceedings of the Second International Conference on Automated Machine Learning, volume 224 of Proceedings of Machine Learning Research, pp. 1/1–23. PMLR, 2023. 5 Quinlan, J. and Cameron-Jones, R. Oversearching and layered search in empirical learning. In Proceedings of the 14th International Joint Conference on Artificial Intelligence , volume 2 of IJCAI’95, pp. 1019–1024, 1995. 5 14Rao, R., Fung, G., and Rosales, R. On the dangers of cross-validation. an experimental evaluation. In Proceedings of the 2008 SIAM International Conference on Data Mining (SDM), pp. 588–596, 2008. B Salinas, D., Seeger, M., Klein, A., Perrone, V ., Wistuba, M., and Archambeau, C. Syne Tune: A library for large scale hyperparameter tuning and reproducible research. In Guyon et al. (2022), pp. 16–1. B Schaffer, C. Selecting a classification method by cross-validation. Machine Learning Journal, 13: 135–143, 1993. B Swersky, K., Snoek, J., and Adams, R. Freeze-thaw Bayesian optimization. arXiv:1406.3896 [stats.ML], 2014. B Talagrand, M. The generic chaining: upper and lower bounds of stochastic processes . Springer Science & Business Media, 2005. C.2 Thornton, C., Hutter, F., Hoos, H., and Leyton-Brown, K. Auto-WEKA: Combined selection and Hyperparameter Optimization of classification algorithms. In Dhillon, I., Koren, Y ., Ghani, R., Senator, T., Bradley, P., Parekh, R., He, J., Grossman, R., and Uthurusamy, R. (eds.), The 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD’13), pp. 847–855. ACM Press, 2013. 5, B Turner, R., Eriksson, D., McCourt, M., Kiili, J., Laaksonen, E., Xu, Z., and Guyon, I. Bayesian optimization is superior to random search for machine learning hyperparameter tuning: Analysis of the Black-Box Optimization Challenge 2020. In Escalante, H. and Hofmann, K. (eds.),Proceedings of the Neural Information Processing Systems Track Competition and Demonstration, pp. 3–26. Curran Associates, 2021. 4.1 van der Vaart, A. Asymptotic statistics, volume 3. Cambridge university press, 2000. C.1 van Erven, T., Grünwald, P., Mehta, N., Reid, M., and Williamson, R. Fast rates in statistical and online learning. Journal of Machine Learning Research, 16(54):1793–1861, 2015. C.1 van Rijn, J. and Hutter, F. Hyperparameter importance across datasets. In Guo, Y . and Farooq, F. (eds.), Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD’18), pp. 2367–2376. ACM Press, 2018. 1 Vanschoren, J. and Yeung, S. (eds.).Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks, 2021. Curran Associates. 5 Vanschoren, J., van Rijn, J., Bischl, B., and Torgo, L. OpenML: Networked science in machine learning. SIGKDD Explorations, 15(2):49–60, 2014. 4 Wainer, J. and Cawley, G. Empirical Evaluation of Resampling Procedures for Optimising SVM Hyperparameters. Journal of Machine Learning Research, 18:1–35, 2017. B Wainwright, M. High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cambridge university press, 2019. C.2 Wistuba, M., Schilling, N., and Schmidt-Thieme, L. Scalable Gaussian process-based transfer surrogates for Hyperparameter Optimization. Machine Learning, 107(1):43–78, 2018. G.1 Wu, J., Toscano-Palmerin, S., Frazier, P., and Wilson, A. Practical multi-fidelity Bayesian optimization for hyperparameter tuning. In Peters, J. and Sontag, D. (eds.), Proceedings of The 36th Uncertainty in Artificial Intelligence Conference (UAI’20), pp. 788–798. PMLR, 2020. B Zheng, A. and Bilenko, M. Lazy paired hyper-parameter tuning. In Rossi, F. (ed.), Proceedings of the 23rd International Joint Conference on Artificial Intelligence (IJCAI’13), pp. 1924–1931, 2013. 5, B Zimmer, L., Lindauer, M., and Hutter, F. Auto-Pytorch: Multi-fidelity metalearning for efficient and robust AutoDL. IEEE Transactions on Pattern Analysis and Machine Intelligence, 43:3079–3090, 2021. 4.1, F.2 Zou, H. and Hastie, T. Regularization and variable selection via the elastic net. Journal of the Royal Statistical Society Series B: Statistical Methodology, 67(2):301–320, 2005. 4.1 15A Notation Table 2: Notation table. We discuss all symbols used in the main paper. Xi Random vector, describing the features Yi Random variable, describing the target Zi = (Xi, Yi) Data point D = {Zi}n i=1 Dataset consisting of iid random variables n Number of observations g Inducer/ML algorithm h Model, created by the inducer via h = gλ(D) λ Hyperparameter configuration Λ Finite set of all hyperparameter configurations J |Λ|, i.e., the number of hyperparameter configurations gλj Hyperparameterized inducer µ(λ) Expected loss of a hyperparameterized inducer on the distribution of a dataset ℓ(Z, h) Loss of a model h on a fresh observation Z M Number of folds in M-fold cross-validation α Percentage of samples to be used for validation I1,j, . . . ,IM,j ⊂ {1, . . . , n} M sets of validation indices, to be used for evaluating λj Vm,j Validation data for fold m and configuration λj Tm,j Training data for fold m and configuration λj L(Vm,j, gλj (Tm,j)) Validation loss for fold m and configuration λj bµ(λj) M-fold validation loss σ2 Increase in variance of validation loss caused by resampling τ2 Decrease in correlation among validation losses caused by reshuffling τi,j,M Resampling-related component of validation loss covariance K(·, ·) Kernel capturing the covariance of the pointwise losses between two HPCs ϵ(λj) Zero-mean Gaussian process, see Equation (2) d Number of hyperparameters κ Curvature constant of covariance kernel η Density of hyperparameter set Λ m Local curvature at the minimum of the loss surface µ σ Lower bound on the noise level B(τ) Part of the regret bound penalizing reshuffling A(τ) Part of the regret bound rewarding reshuffling B Extended Related Work Due to the black box nature of the HPO problem (Feurer & Hutter, 2019; Bischl et al., 2023), gradient free, zeroth-order optimization algorithms such as BO (Garnett, 2023), Evolutionary Strate- gies (Loshchilov & Hutter, 2016) or a simple random search (Bergstra & Bengio, 2012) have become standard optimization algorithms to tackle vanilla HPO problems. In the last decade, most research on HPO has been concerned with constructing new algorithms that excel at finding configurations with a low estimated generalization error. Examples include BO variants such as as HEBO (Cowen-Rivers et al., 2022) or SMAC3 (Lindauer et al., 2022). Another direction of HPO research has been concerned with speeding up the HPO process to allow more efficient spending of compute resources. Multifidelity HPO, for example, turns the black box optimization problem into a gray box one by making use of lower fidelity approximations to the target function, i.e., using fewer numbers of epochs or subsets of the data for cheap low-fidelity evaluations that approximate the costly high-fidelity evaluation. Examples include bandit-based budget allocation algorithms such as Successive Halving (Jamieson & Talwalkar, 2016), Hyperband (Li et al., 2018) and their extensions that use non-random search mechanisms (Falkner et al., 2018; Awad et al., 2021; Mallik et al., 2023) or algorithms making use of multi-fidelity information in the context of BO (Swersky et al., 2014; Klein et al., 2017; Wu et al., 2020; Kadra et al., 2023). Several works address the problem of speeding up cross-validation techniques and use techniques that could be described as grey box optimization techniques. Besides the ones mentioned in the main paper (Thornton et al., 2013; Zheng & Bilenko, 2013), it is possible to employ racing techniques for model selection in machine learning as demonstrated by Lang et al. (2015), and there has been a recent interest in methods that adapt the cost of running full cross-validation procedures (Bergman et al., 2024; Buczak et al., 2024). When addressing the problem of HPO, we must acknowledge an inherent mismatch between the explicit objective we optimize – namely, the estimated generalization performance of a model – and the actual implicit optimization goal, which is to identify a configuration that yields the best 16generalization performance on new, unseen data. Typically, evaluations and comparisons of different HPO algorithms focus exclusively on the final best validation performance (i.e., the objective that is directly optimized), even though an unbiased estimate of performance on an external unseen test set might be available. While this approach is logical for assessing the efficacy of an optimization algorithm based on the metric it seeks to improve, relying solely on finding an optimal validation configuration is beneficial only if there is reason to assume a strong correlation between the optimized validation performance and true generalization ability on new, unseen test data. This discrepancy can be found deeply within the HPO community, where the evaluation of HPO algorithms on standard benchmark libraries is usually done solely with respect to the validation performance (Eggensperger et al., 2021; Pineda Arango et al., 2021; Salinas et al., 2022; Pfisterer et al., 2022).5 This relationship between validation performance (i.e., the estimated generalization error derived from resampling) and true generalization performance (e.g., assessed through an outer holdout test set or additional resampling) of an optimal validation configuration found during HPO remains a largely unexplored area of research. In general, little research has focused on the selection of resampling types, let alone the automated selection of resampling types (Guyon et al., 2010; Feurer et al., 2022). While we usually expect that a more intensive resampling will reduce the variance of the estimated generalization error and thereby improve the (rank) correlation between optimized validation and unbiased outer test performance within HPCs, this benefit is naturally offset by a higher computational expense. Overall, there is little research on which resampling method to use in practice for model selection, and we only know of a study for support vector machines (Wainer & Cawley, 2017), a simulation study for clinical prediction models (Dunias et al., 2024), a study on feature selection (Molinaro et al., 2005) and a study on fast CV (Bergman et al., 2024). In addition, ML-Plan (Mohr et al., 2018) proposed a two-stage procedure. In a first stage (search), the tool uses planning on hierarchical task networks to find promising machine learning pipelines on 70% of the training data. In a second step (selection), it uses 100% of the training data and retrains the most promising candidates from the search step. Finally, it uses a combination of the internal generalization error estimation that was used during search and the 0.75 percentile of the generalization error estimation from the selection step to make a more unbiased selection of the final model. The paper found that this improves performance over using only regular cross-validation for search and selection. The general consensus, that is in agreement with our findings, is that CV or repeated CV generally leads to better generalization performance. In addition, while there are theoretical works that compare the accuracy of estimating the generalization error of holdout and CV (Blum et al., 1999), our goals is to correctly identify a single solution, which generalizes well, see the excellent survey by Arlot & Celisse (2010) for a discussion on this topic. Bouthillier et al. (2021) studied the sources of variance in machine learning experiments, and find that the split into training and test data has the largest impact. Consequently, they suggest to reshuffle the data prior to splitting it into the training, which is then used for HPO, and the test set. We followed their suggestion when designing our experiments and draw a new test sample for every replication, see Section 4.1 and Appendix F. This dependence on the exact split was further already discussed in the context of how much the outcome of a statistical test on results of machine learning experiments depended on the exact train-test split (Bouckaert, 2004). Finally, the first warning against comparing too many hypothesis using cross-validation was raised by Schaffer (1993), and in addition to the works discussed in Section 5 in the main paper, also picked up by Rao et al. (2008); Cawley & Talbot (2010). Moreover, the problem of finding a correct \"upper objective\" in a bilevel optimization problem has been noted (Guyon et al., 2010, 2015, 2019). Also, in the related field of algorithm configuration the problem has been identified (Eggensperger et al., 2019). B.1 Current Treatment of Resamplings in HPO Libraries and Software In Table 3, we provide a brief summary of how resampling is handled in popular HPO libraries and software.6 For each library, we checked whether the core functionality, examples, or tutorials mention 5We admit that these benchmark libraries implement efficient benchmarking methods such as surro- gate (Eggensperger et al., 2018; Pfisterer et al., 2022) or tabular benchmarks (Pineda Arango et al., 2021). It would be possible to adapt them to return the test performance, however, changes in the HPO evaluation protocol, such as the one we propose, would not be feasible. 6This summary is not exhaustive but reflects the general consensus observed in widely-used software. 17the possibility of reshuffling the resampling during HPO or if the resampling is considered fixed. If reshuffling is used in an example, mentioned, or if core functionality uses it, we mark it with a ✓. If it is unclear or inconsistent across examples and core functionality, we mark it with a ?. Otherwise, we use a ✗. Our conclusion is that the concept of reshuffling resampling generally receives little attention. Table 3: Exemplary Treatment of Resamplings in HPO Libraries and Software Software Reshuffled? Reference(s) sklearn ✗ GridSearchCV1/ RandomizedSearchCV2 HEBO ✗ sklearn_tuner3 optuna ? Inconsistency between examples 4,5,6 bayesian-optimization ✗ sklearn Example7,8 ax ✗ CNN Example9 spearmint ✗ No official HPO Examples scikit-optimize ✗ BO for GBT Example7,10 SMAC3 ✗ SVM Example7,11 dragonfly ✗ Tree Based Ensemble Example12 aws sagemaker ✗ Blog Post13 raytune ? Inconsistency between examples 14, 15 hyperopt(-sklearn) ? Cost Function Logic 16 ✗: no reshuffling, ?: both reshuffling and no reshuffling or unclear, ✓: reshuffling 1 https://github.com/scikit-learn/scikit-learn/blob/8721245511de2f225ff5f9aa5f5fadce663cd4a3/sklearn/m odel_selection/_search.py#L1263 2 https://github.com/scikit-learn/scikit-learn/blob/8721245511de2f225ff5f9aa5f5fadce663cd4a3/sklearn/m odel_selection/_search.py#L1644 3 https://github.com/huawei-noah/HEBO/blob/b60f41aa862b4c5148e31ab4981890da6d41f2b1/HEBO/hebo/sklearn_t uner.py#L73 4 https://github.com/optuna/optuna-integration/blob/15e6b0ec6d9a0d7f572ad387be8478c56257bef7/optuna_in tegration/sklearn/sklearn.py#L223 here sklearn’s cross_validate is used which by default does not reshuffle the resampling https://github.com/scikit-learn/scikit-learn/blob/8721245511de2f225ff5f9aa5f5fadce663cd4a3/sklearn/m odel_selection/_validation.py#L186 5 https://github.com/optuna/optuna-examples/blob/dd56b9692e6d1f4fa839332edbcdd93fd48c16d8/pytorch/py torch_simple.py#L79 here, data loaders for train and valid are instantiated within the objective of the trial but the data within the loaders is fixed 6 https://github.com/optuna/optuna-examples/blob/dd56b9692e6d1f4fa839332edbcdd93fd48c16d8/xgboost/xgbo ost_simple.py#L22 here, the train validation split is performed within the objective of the trial and no seed is set which results in reshuffling https://github.com/scikit-learn/scikit-learn/blob/8721245511de2f225ff5f9aa5f5fadce663cd4a3/s klearn/model_selection/_split.py#L2597 7 functionality relies on sklearn’s cross_val_score which by default does not reshuffle the resampling https://github.com/sciki t-learn/scikit-learn/blob/8721245511de2f225ff5f9aa5f5fadce663cd4a3/sklearn/model_selection/_validati on.py#L631 8 https://github.com/bayesian-optimization/BayesianOptimization/blob/c7e5c3926944fc6011ae7ace29f7b5ed0f 9c983b/examples/sklearn_example.py#L32 9 https://github.com/facebook/Ax/blob/ac44a6661f535dd3046954f8fd8701327f4a53e2/tutorials/tune_cnn_serv ice.ipynb#L39 and https://github.com/facebook/Ax/blob/ac44a6661f535dd3046954f8fd8701327f4a53e2/ax/util s/tutorials/cnn_utils.py#L154 10 https://github.com/scikit-optimize/scikit-optimize/blob/a2369ddbc332d16d8ff173b12404b03fea472492/ex amples/hyperparameter-optimization.py#L82C21-L82C36 11 https://github.com/automl/SMAC3/blob/9aaa8e94a5b3a9657737a87b903ee96c683cc42c/examples/1_basics/2_sv m_cv.py#L63 12 https://github.com/dragonfly/dragonfly/blob/3eef7d30bcc2e56f2221a624bd8ec7f933f81e40/examples/tree_r eg/skltree.py#L111 13 https://aws.amazon.com/blogs/architecture/field-notes-build-a-cross-validation-machine-learning-mod el-pipeline-at-scale-with-amazon-sagemaker/ 14 https://github.com/ray-project/ray/blob/3f5aa5c4642eeb12447d9de5dce22085512312f3/doc/source/tune/exa mples/tune-pytorch-cifar.ipynb#L120 here, data loaders for train and valid are instantiated within the objective but the data within the loaders are fixed 15 https://github.com/ray-project/ray/blob/3f5aa5c4642eeb12447d9de5dce22085512312f3/doc/source/tune/exa mples/tune-xgboost.ipynb#L335 here, the train validation split is performed within the objective and no seed is set which results in reshuffling https://github.com/scikit-learn/scikit-learn/blob/8721245511de2f225ff5f9aa5f5fadce663cd4a3 /sklearn/model_selection/_split.py#L2597 16 https://github.com/hyperopt/hyperopt-sklearn/blob/4bc286479677a0bfd2178dac4546ea268b3f3b77/hpsklearn /estimator/_cost_fn.py#L144 dependence on random seed which by default is not set and there is no discussion of reshuffling and behavior is somewhat unclear 18C Proofs of the Main Results C.1 Proof of Theorem 2.1 We impose stability assumptions on the learning algorithm similar to Bayle et al. (2020); Austern & Zhou (2020). Let Z, Z1, . . . ,Zn, Z′ 1, be iid random variables. Define T = {Zi}n i=1, and T ′ as T but with Zn replaced by the independent copy Z′ n. Define eℓn(z, λ) = ℓ(z, gλ(T )) − E[ℓ(Z, gλ(T )) | T], assume that each gλ(T ) is invariant to the ordering in T , ℓ is bounded, and max λ∈Λ E{[eℓ(Z, gλ(T )) − eℓ(Z, gλ(T ′))]2} = o(1/n). (4) This loss stability assumption is rather mild, see Bayle et al. (2020) for an extensive discussion. Further, define the risk R(g) = E[ℓ(Z, g)] and assume that for every λ ∈ Λ, there is a prediction rule g∗ λ such that max λ∈Λ E[|R(gλ(T )) − R(g∗ λ)|] = o(1/√n). (5) This assumption requires gλ(T ) to converge to some fixed prediction rule sufficiently fast and serves as a reasonable working condition for our purposes. It is satisfied, for example, when ℓ is the square loss and gλ is an empirical risk minimizer over a hypothesis class Gλ with finite VC-dimension. For further examples, see, e.g., Bousquet & Zhivotovskiy (2021), van Erven et al. (2015), and references therein. The assumption could be relaxed, but this would lead to a more complicated limiting distribution but with the same essential interpretation. Theorem C.1. Under assumptions (4) and (5), it holds √n (bµ(λj) − µ(λj))J j=1 →d N(0, Σ), where Σj,j′ = τi,j,M lim n→∞ Cov[¯ℓn(Z, λj), ¯ℓn(Z, λj′ )], τj,j′,M = lim n→∞ 1 nM2α2 nX i=1 MX m=1 MX m′=1 Pr(i ∈ Im,j ∩ Im′,j′ ). Proof. Define eµ(λj) = 1 M MX m=1 E[L(Vm,j, gλj (Tm,j)) | Tm,j]. By the triangle inequality (first and second step), Jensen’s inequality (third step), and (5) (last step), E[|eµ(λj) − µ(λj)|] ≤ max 1≤m≤M E \u0002\f\fE[L(Vm,j, gλj (Tm,j)) | Tm,j] − E[L(Vm,j, gλj (Tm,j))] \f\f\u0003 ≤ max 1≤m≤M E h\f\f\fE[L(Vm,j, gλj (Tm,j)) | Tm,j] − E[L(Vm,j, g∗ λj )] \f\f\f i + max 1≤m≤M E h\f\f\fE[L(Vm,j, gλj (Tm,j))] − E[L(Vm,j, g∗ λj )] \f\f\f i ≤ 2 max 1≤m≤M E h\f\f\fE[L(Vm,j, gλj (Tm,j)) | Tm,j] − E[L(Vm,j, g∗ λj )] \f\f\f i = 2 max 1≤m≤M E h\f\f\fR(gλj (Tm,j)) − R(g∗ λj ) \f\f\f i = o(1/√n). Next, assumption (4) together with Theorem 2 and Proposition 3 of Bayle et al. (2020) yield √n (bµ(λj) − eµ(λj)) − 1 M MX m=1 1 α√n X i∈Im,j ¯ℓn(Zi, λj) →p 0. 19Now rewrite 1 Mα√n MX m=1 X i∈Im,j ¯ℓn(Zi, λj) = 1 Mα√n nX i=1 MX m=1 1(i ∈ Im,j)¯ℓn(Zi, λj) | {z } :=ξ(j) i,n . The sequence (ξi,n)n i=1 = (ξ(j) i,n, . . . , ξ(j) i,n)n i=1 is a triangular array of independent, centered, and bounded random vectors. Because 1(Zi ∈ Vm,j) and Zi are independent, it holds Cov(ξ(j) i,n, ξ(j′) i,n ) = MX m=1 MX m′=1 E[1(i ∈ Im,j ∩ Im′,j′ )]E[¯ℓn(Zi, λj)¯ℓn(Zi, λj′ )], so lim n→∞ Cov \" 1 Mα√n nX i=1 ξ(j) i,n, 1 Mα√n nX i=1 ξ(j′) i,n # = lim n→∞ 1 nM2α2 nX i=1 Cov h ξ(j) i,n, ξ(j′) i,n i = Σj,j′ . Now the result follows from Lindeberg’s central limit theorem for triangular arrays (e.g., van der Vaart, 2000, Proposition 2.27). C.2 Proof of Theorem 2.2 We want to bound the probability thatµ(ˆλ) −µ(λ∗) is large. For some δ >0, define the set of ‘good’ hyperparameters Λδ = {λj : µ(λj) − µ(λ∗) ≤ δ}. Now Pr \u0010 µ(bλ) − µ(λ∗) > δ \u0011 = Pr \u0010 bλ /∈ Λδ \u0011 = Pr \u0012 min λ/∈Λδ bµ(λ) < min λ∈Λδ bµ(λ) \u0013 ≤ Pr \u0012 min λ/∈Λδ bµ(λ) < min λ∈Λδ/2 bµ(λ) \u0013 = Pr \u0012 min λ/∈Λδ µ(λ) + ϵ(λ) < min λ∈Λδ/2 µ(λ) + ϵ(λ) \u0013 ≤ Pr \u0012 δ + min λ/∈Λδ ϵ(λ) < δ/2 + min λ∈Λδ/2 ϵ(λ) \u0013 = Pr \u0012 min λ/∈Λδ ϵ(λ) − min λ∈Λδ/2 ϵ(λ) < −δ/2 \u0013 = Pr \u0012 max λ/∈Λδ ϵ(λ) − max λ∈Λδ/2 ϵ(λ) > δ/2 \u0013 . (ϵ d = −ϵ) There is a tension between the two maxima. The more λ’s there are in Λδ/2 and the less they are correlated, the more likely it is to find one ϵ(λ) that is large. This makes the probability small. However, the less ϵ is correlated, the larger is maxλ/∈Λδ ϵ(λ), making the probability large. To formalize this, use the Gaussian concentration inequality (Talagrand, 2005, Lemma 2.1.3): Pr \u0012 max λ/∈Λδ ϵ(λ) − max λ∈Λδ/2 ϵ(λ) > δ/2 \u0013 ≤ Pr \u0012 2 \f\f\f\fmax λ∈Λ ϵ(λ) − E \u0014 max λ∈Λ ϵ(λ) \u0015\f\f\f\f > δ/2 − E \u0014 max λ∈Λδ/2 ϵ(λ) \u0015 + E \u0014 max λ/∈Λδ ϵ(λ) \u0015\u0013 ≤ 2 exp ( − \u0000 δ/2 − E \u0002 maxλ∈Λδ/2 ϵ(λ) \u0003 + E[maxλ/∈Λδ ϵ(λ)] \u00012 8σ2 ) , provided δ/2−E \u0002 maxλ∈Λδ/2 ϵ(λ) \u0003 +E[maxλ/∈Λδ ϵ(λ)] ≥ 0. We bound the two maxima separately. 20Lower Bound for Maximum over the Good Set Recall the definition of m right before Theorem 2.2 and observe Λδ/2 = {λ: µ(λ) − µ(λ∗) ≤ δ/2} ⊃ {λ: m∥λ − λ∗∥2 ≤ δ/2} = {λ: ∥λ − λ∗∥ ≤(δ/2m)1/2} = B(λ∗, (δ/2m)1/2). Pack the ball B(λ∗, (δ/2m)1/2) with smaller balls with radius η. We can always construct such a packing with at least (δ/2mη2)d/2 elements. By assumption, each small ball contains at least one element of Λ. Pick one element from each small ball and collect them into the set Λ′ δ/2. By construction, |Λ′ δ/2| ≥(δ/2mη2)d/2 and min λ̸=λ′∈Λ′ δ/2| ∥λ − λ′∥ ≥η. Sudakov’s minoration principle (e.g., Wainwright, 2019, Theorem 5.30) gives E \u0014 max λ∈Λδ/2 ϵ(λ) \u0015 ≥ 1 2 q log |Λ′ δ/2| min {λ̸=λ′}∩Λ′ δ/2 p Var[ϵ(λ) − ϵ(λ′)] ≥ 1 2 q log |Λ′ δ/2| min ∥λ−λ′∥≥η p Var[ϵ(λ) − ϵ(λ′)]. In general, Var[ϵ(λ) − ϵ(λ′)] = K(λ, λ) + K(λ′, λ′) − 2τ2K(λ, λ′) = (1 − τ2)[K(λ, λ) + K(λ′, λ′)] + τ2[K(λ, λ) − K(λ, λ′)] + τ2[K(λ′, λ′) − K(λ, λ′)] ≥ 2σ2(1 − τ2). Hence, we have min ∥λ−λ′∥≥η Var[ϵ(λ) − ϵ(λ′)] ≥ 2σ2(1 − τ2), which implies E \u0014 max λ∈Λδ/2 ϵ(λ) \u0015 ≥ 1 2σ √ d p 1 − τ2 p log(δ/2mη2) =: σ √ dA(τ, δ)/2. Upper Bound for Maximum over the Bad Set Dudley’s entropy bound (e.g., Giné & Nickl, 2016, Theorem 2.3.6) gives E \u0014 max λ/∈Λδ ϵ(λ) \u0015 ≤ 12 Z ∞ 0 p log N(s)ds, where N(s) is the minimum number of points λ1, . . . ,λN(s) such that sup λ∈Λ min 1≤k≤N(s) p Var[ϵ(λ) − ϵ(λk)] ≤ s. Note that sup λ,λ′∈Λ p Var[ϵ(λ) − ϵ(λ′)] ≤ 2σ, so N(s) = 1 for all s ≥ 2σ. For s2 ≤ 4σ2(1 − τ2), we can use the trivial bound N(s) ≤ J. For s2 > 4σ2(1 − τ2), cover Λ with ℓ2-balls of size (s/2στκ ). We can do this with less than N(s) ≤ (6σκ/s)d ∨ 1 such balls. Let λ1, . . . ,λN be the centers of these balls. In general, it holds Var[ϵ(λ) − ϵ(λ′)] = K(λ, λ) + K(λ′, λ′) − 2τ2K(λ, λ′) = (1 − τ2)[K(λ, λ) + K(λ′, λ′)] + τ2[K(λ, λ) − K(λ, λ′)] + τ2[K(λ′, λ′) − K(λ, λ′)] ≤ 2(1 − τ2)σ2 + 2τ2σ2κ2∥λ − λ′∥2. 21For s2 > 4σ2(1 − τ2), we thus have sup λ∈Λ min 1≤k≤N(s) Var[ϵ(λ) − ϵ(λk)] ≤ sup ∥λ−λ′∥2≤(s/2τσκ)2 Var[ϵ(λ) − ϵ(λ′)] ≤ 2(1 − τ2)σ2 + 2τ2σ2κ2(s/2τσκ )2 ≤ s2, as desired. Now decompose the integral Z ∞ 0 p log N(s)ds = Z 2σ √ 1−τ2 0 p log N(s)ds + Z 2σ 2σ √ 1−τ2 p log N(s)ds ≤ 2σ √ d p 1 − τ2 p log J + Z 2σ 2σ √ 1−τ2 p log N(s)ds. For the second term, compute Z 2σ σ √ 1−τ2 p log N(s)ds ≤ √ d Z 2σ 2σ √ 1−τ2 p log(6σκ/s)+ ds = σ √ d Z 2 2 √ 1−τ2 p log(6κ/s)+ ds ≤ σ √ d \u0012Z 2 0 log(6κ/s)+ ds \u00131/2 \u0010 2(1 − p 1 − τ2) \u00111/2 = σ √ d p 2 + 2 log(3κ)+ \u0010 2(1 − p 1 − τ2) \u00111/2 = 2σ √ d p 1 + log(3κ)+ τ (1 + √ 1 − τ2)1/2 ≤ 2σ √ dτ p 1 + log(3κ)+. We have shown that E \u0014 max λ/∈Λδ ϵ(λ) \u0015 ≤ 24σ √ d hp 1 − τ2 p log J + τ p 1 + log(3κ)+ i =: σ √ dB(τ)/4. Integrating Probabilities Summarizing the two previous steps, we have Pr \u0010 µ(bλ) − µ(λ∗) > δ \u0011 ≤ 2 exp    − \u0010 δ − σ √ d[B(τ) − A(τ, δ)] \u00112 36σ2    , provided t ≥ σ √ d[B(τ) − A(τ, δ)]. Now for any s ≥ 0 and t ≥ 2es2 mη2, it holds A(τ, s) ≥ (σ/σ) p 1 − τ2s =: A(τ)s. In particular, if t ≥ 2es2 mη2 + σ √ d[B(τ) − A(τ)s] =: C, we have Pr \u0010 µ(bλ) − µ(λ∗) > δ \u0011 ≤ 4 exp    − \u0010 δ − σ √ d[B(τ) − A(τ)s] \u00112 36σ2    . 22Integrating the probability gives E[µ(bλ) − µ(λ∗)] = Z ∞ 0 Pr \u0010 µ(bλ) − µ(λ∗) > δ \u0011 dδ = Z C 0 Pr \u0010 µ(bλ) − µ(λ∗) > δ \u0011 dδ + Z ∞ C Pr \u0010 µ(bλ) − µ(λ∗) > δ \u0011 dδ ≤ C + Z ∞ C exp    − \u0010 δ − σ √ d[B(τ) − A(τ)s] \u00112 36σ2    dδ ≤ C + √ 36σ = 2es2 mη2 + σ √ d[B(τ) − A(τ)s] + 6σ. Simplifying The bound can be optimized with respect to s, but the solution involves the Lambert W-function, which has no analytical expression. Instead choose s for simplicity as s = s log \u0012 σ 2mη2 \u0013 + . which gives E[µ(bλ) − µ(λ∗)] ≤ σ √ d \" 8 + B(τ) − A(τ) s log \u0012 σ 2mη2 \u0013# . D Additional Results on the Density of Random HPC Grids Lemma D.1. Suppose that the J elements in Λ are drawn independently from a continuous density p with c := min∥λ∥≤1 p(λ) > 0. Then with probability at least 1 − δ, η ≲ \u0010p log(1/δ)/J \u00111/d , and with probability 1, η ≲ \u0010p log(J)/J \u00111/d , for all J sufficiently large. Proof. We want to bound the probability that there is a λ such that |B(λ, η) ∩ Λ| = 0. In what follows λ is silently understood to have norm bounded by 1. Let eλ1, . . . ,eλN the centers of η/2-balls covering {∥λ∥ ≤1}, for which we may assume N ≤ (6/η)d. For eλk the closest center to λ, it holds ∥λ′ − λ∥ ≤ ∥λ′ − eλk∥ + ∥eλk − λ∥ ≤ ∥λ′ − eλk∥ + η/2, so ∥λ′ − eλk∥ ≤η/2 implies ∥λ′ − λ∥ ≤η. We thus have Pr(∃λ: |B(λ, η) ∩ Λ| = 0) = Pr   inf λ JX i=1 1{∥λi − λ∥ ≤η} ≤0 ! ≤ Pr   min 1≤k≤N JX i=1 1{∥λi − eλk∥ ≤η/2} ≤0 ! . 23Further Pr   min 1≤k≤N JX i=1 1{∥λi − eλk∥ ≤η/2} ≤0 ! = Pr   max 1≤k≤N JX i=1 −1{∥λi − eλk∥ ≤η/2} ≥0 ! ≤ Pr   max 1≤k≤N JX i=1 E h 1{∥λi − eλk∥ ≤η/2} i − 1{∥λi − eλk∥ ≤η/2} ≥J inf λ E[1{∥λi − λ∥ ≤η/2}] ! . It holds E[1{∥λi − λ∥ ≤η/2}] = Pr (∥λi − λ∥ ≤η/2) = Z ∥λ′−λ∥≤η/2 p(λ′)dλ′ ≥ c vol(B(0, η/2)) = cvd(η/2)d, where vd = vol(B(0, 1)). Now the union bound and Hoeffding’s inequality give Pr   min 1≤k≤N JX i=1 1{∥λi − eλk∥ ≤η/2} ≤0 ! ≤ N exp \u0012 −Jc2v2 d(η/2)2d 2 \u0013 ≤ (6/η)d exp \u0012 −Jc2v2 d(η/2)2d 2 \u0013 . Choosing η = 2 \u0012q 2 log(3d √ Jcvd/δ)/ √ Jcvd \u00131/d gives Pr(∃λ: |B(λ, η) ∩ Λ| = 0) ≤ δ/ q 2 log(3d √ Jcvd), which is bounded by δ when √ J ≥ e1/2/3dcvd. Further, setting η = 2( p 6 log(J)/ √ Jcvd)1/d gives Pr   min 1≤k≤N JX i=1 1{∥λi − eλk∥ ≤η/2} ≤0 ! ≲ J−5/2, so that ∞X J=1 Pr   min 1≤j≤J min 1≤k≤N jX i=1 1{∥λi − eλk∥ ≤η/2} ≤0 ! ≤ ∞X J=1 J Pr   min 1≤k≤N JX i=1 1{∥λi − eλk∥ ≤η/2} ≤0 ! ≲ ∞X J=1 1 J3/2 < ∞. Now the Borel-Cantelli lemma (e.g., Kallenberg, 1997, Theorem 4.18) implies that, with probability 1, |B(λ, η) ∩ Λ| ≥1, for all J sufficiently large. 24E Selected Validation Schemes E.1 Definition of Index Sets Recall: (i) (holdout) Let M = 1 and I1,j = I1 for all j = 1, . . . , J, and some size-⌈αn⌉ index set I1. (ii) (reshuffled holdout) Let M = 1 and I1,1, . . . ,I1,J be independently drawn from the uniform distribution over all size-⌈αn⌉ subsets from {1, . . . , n}. (iii) (M-fold CV) Let α = 1/M and I1, . . . ,IM be a disjoint partition of {1, . . . , n}, and Im,j = Im for all j = 1, . . . , J. (iv) (reshuffled M-fold CV) Let α = 1/M and (I1,j, . . . ,IM,j ), j= 1, . . . , J, be independently drawn from the uniform distribution over disjoint partitions of {1, . . . , n}. (v) (M-fold holdout) LetIm, m= 1, . . . , M, be independently drawn from the uniform distribution over size-⌈αn⌉ subsets of {1, . . . , n} and set Im,j = Im for all m = 1, . . . , M, j= 1, . . . , J. (vi) (reshuffled M-fold holdout) Let Im,j, m= 1, . . . , M, j= 1, . . . , J, be independently drawn from the uniform distribution over size-⌈αn⌉ subsets of {1, . . . , n}. E.2 Derivation of Reshuffling Parameters in Limiting Distribution Recall τi,j,M = 1 nM2α2 nX s=1 MX m=1 MX m′=1 Pr(s ∈ Im,i ∩ Im′,j). For all schemes in the proposition, the probabilities are independent of the index s, so the average over s = 1, . . . , ncan be omitted. We now verify the constants σ, τfrom Table 1. (i) It holds Pr(s ∈ I1,i ∩ I1,j) = Pr(s ∈ I1) = α. Hence, τi,j,1 = 1/α = 1/α × 1 = σ2 × τ2. (ii) (reshuffled holdout) This is a special case of part (vi) with M = 1. (iii) ( M-fold CV) It holds Pr(s ∈ Im,i ∩ Im′,j) = Pr(s ∈ Im ∩ Im′ ) = \u001a1/M, m = m′, 0, m ̸= m′. Only M probabilities in the double sum are non-zero, whence τi,j,M = 1 M2α2 × M/M = 1/α2M2 = 1 × 1 = σ2 × τ2, where we used α = 1/M. (iv) (reshuffled M-fold CV) It holds Pr(s ∈ Im,i ∩ Im′,j) =    1/M, m = m′, i= j 0, m ̸= m′, i= j 1/M2, m = m′, i̸= j 1/M2, m ̸= m′, i̸= j. For i = j, only M probabilities in the double sum are non-zero. Also using α = 1/M, we get τi,j,M = 1 M2α2 × M × 1/M = 1 = σ2. For i ̸= j, τi,j,M = 1 M2α2 × M2 × 1/M2 = 1 × 1 = σ2 × τ2. 25(v) ( M-fold holdout) It holds Pr(s ∈ Im,i ∩ Im′,j) = Pr(s ∈ Im ∩ Im′ ) = \u001aα, m = m′, α2, else. This gives τi,j,M = 1 M2α2 × [M × α + (M − 1)M × α2] = [1/αM + (M − 1)/M] × 1 = σ2 × τ2. for all i, j. (vi) (reshuffled M-fold holdout) It holds Pr(s ∈ Im,i ∩ Im′,j) = \u001aα, m = m′, i= j α2, else. For i = j, this gives τi,j,M = 1 M2α2 × [M × α + (M − 1)M × α2] = 1/αM + (M − 1)/M. For i ̸= j, τi,j,M = 1 M2α2 × (M2 × α2) = 1. This implies that (1) holds with σ2 = 1/Mα + (M − 1)/M, τ2 = 1/(1/Mα + (M − 1)/M). Remark E.1. Although not technically covered by Theorem 2.1, performing independent bootstraps for each λj correspond to reshuffled n-fold holdout with α = 1 /n. Accordingly, σ ≈ √ 2 and τ ≈ p 1/2. F Details Regarding Benchmark Experiments F.1 Datasets We list all datasets used in the benchmark experiments in Table 4. Table 4: List of datasets used in benchmark experiments. All information can be found on OpenML (Vanschoren et al., 2014). OpenML Dataset ID Dataset Name Size ( n × p) 23517 numerai28.6 96320 × 21 1169 airlines 539383 × 7 41147 albert 425240 × 78 4135 Amazon_employee_access 32769 × 9 1461 bank-marketing 45211 × 16 1590 adult 48842 × 14 41150 MiniBooNE 130064 × 50 41162 kick 72983 × 32 42733 Click_prediction_small 39948 × 11 42742 porto-seguro 595212 × 57 Note that datasets serve as data generating processes (DGPs; Hothorn et al., 2005). As we are mostly concerned with the actual generalization performance of the final best HPC found during HPO based on validation performance we rely on a comparably large held out test set that is not used during HPO. We therefore use 5000 data points sampled from a DGP as an outer test set. To further be able to measure the generalization performance robustly for varying data sizes available during HPO, we construct concrete tasks based on the DGPs by sampling subsets of (train_valid; n) size 500, 1000 and 5000 from the DGPs. This results in 30 tasks in total (10 DGPS × 3 train_valid sizes). For more details and the concrete implementation of this procedure, see Appendix F.3. We also collected another 5000 data points as an external validation set, but did not use it. Therefore, we had to tighten the restriction to 10000 data points mentioned in the main paper to 15000 data points as the lower bound on data points. To allow for stronger variation over different replications, we decided to use 20000 as the final lower bound. 26F.2 Learning Algorithms Here we briefly present training pipeline details and search spaces of the learning algorithms used in our benchmark experiments. The funnel-shaped MLP is based on sklearn’s MLP Classifier and is constructed in the following way: The hidden layer size for each layer is determined by num_layers and max_units. We start with max_units and half the number of units for every subsequent layer to create a funnel. max_batch_size is the largest power of 2 that is smaller than the number of training samples available. We use ReLU as activation function and train the network optimizing logloss as a loss function via SGD using a constant learning rate and Nesterov momentum for 100 epochs. Table 5 lists the search space (inspired from Zimmer et al. (2021)) used during HPO. The Elastic Net is based on sklearn’s Logistic Regression Classifier. We train it for a maximum of 1000 iterations using the \"saga\" solver. Table 6 lists the search space used during HPO. The XGBoost and CatBoost search spaces are listed in Table 7 and Table 8, both inspired from their search spaces used in McElfresh et al. (2023). For both the Elastic Net and Funnel MLP, missing values are imputed in the preprocessing pipeline (mean imputation for numerical features and adding a new level for categorical features). Categorical features are target encoded in a cross-validated manner using a 5-fold CV . Features are then scaled to zero mean and unit variance via a standard scaler. For XGBoost, we impute missing values for categorical features (adding a new level) and target encode them in a cross-validated manner using a 5-fold CV . For CatBoost, no preprocessing is performed. XGBoost and CatBoost models are trained for 2000 iterations and stop early if the validation loss (using the default internal loss function used during training, i.e., logloss) does not improve over a horizon of 20 iterations. For retraining the best configuration on the whole train and validation data, the number of boosting iterations is set to the number of iterations used to find the best validation performance prior to the stopping mechanism taking action.7 F.3 Exact Implementation In the following, we outline the exact implementation of performing one HPO run for a given learning algorithm on a concrete task (dataset × train_valid size) and a given resampling. We release all code to replicate benchmark results and reproduce our analyses via https://github.com/slds-l mu/paper_2024_reshuffling. For a given replication (in total 10): 1. We sample (without replacement) train_valid size (500, 1000 or 5000 points) and test size (always 5000) points from the DGP (i.e. a concrete dataset in Table 4). These are shared for every learning algorithm (i.e. all learning algorithms are evaluated on the same data). 2. A given HPC is evaluated in the following way: • The resampling operates on the train validation 8 set of size train_valid. • The learning algorithm is configured by the HPC. • The learning algorithm is trained on training splits and evaluated on validation splits according to the resampling strategy. In case reshuffling is turned on, the training and validation splits are recreated for every HPO. We compute the Accuracy, ROC AUC and logloss when using a random search and compute ROC AUC when using HEBO or SMAC3 and average performance over all folds for resamplings involving multiple folds. • For each HPC we then always re-train the model on all train_valid data being available and evaluate the model on the held-out test set to compute an outer estimate of generalization performance for each HPC (regardless of whether it is the incumbent for a given iteration or not). 7For CV and repeated holdout we take the average number of boosting iterations over the models trained on the different folds. 8With train validation we refer to all data being available during HPO which is then further split by a resampling into train and validation sets. 27Table 5: Search Space for Funnel-Shaped MLP Classifier. Parameter Type Range Log num_layers Int. 1 to 5 No max_units Int. 64, 128, 256, 512 No learning_rate Num. 1 × 10−4 to 1 × 10−1 Yes batch_size Int. 16, 32, ..., max_batch_size No momentum Num. 0.1 to 0.99 No alpha Num. 1 × 10−6 to 1 × 10−1 Yes Table 6: Search Space for Elastic Net Classifier. Parameter Type Range Log C Num. 1 × 10−6 to 1 × 104 Yes l1_ratio Num. 0.0 to 1.0 No Table 7: Search Space for XGBoost Classifier. Parameter Type Range Log max_depth Int. 2 to 12 Yes alpha Num. 1 × 10−8 to 1.0 Yes lambda Num. 1 × 10−8 to 1.0 Yes eta Num. 0.01 to 0.3 Yes Table 8: Search Space for CatBoost Classifier. Parameter Type Range Log learning_rate Num. 0.01 to 0.3 Yes depth Int. 2 to 12 Yes l2_leaf_reg Num. 0.5 to 30 Yes 3. We evaluate 500 HPCs when using random search and 250 HPC when using HEBO or SMAC3 (SMAC4HPO facade). As resamplings, we use holdout with a 80/20 train-validation split and 5 folds for CV , so that the holdout strategy is just one fold of the CV and the fraction of data points being used for training and respectively validation are the same across different resampling strategies. 5-fold holdout simply repeats the holdout procedure five times and 5x 5-fold CV repeats the 5-fold CV five times. Each of the four resamplings can be reshuffled or not (standard). As mentioned above, the test set is only varied for each of the 10 replica (repetitions with different seeds), but consistent for different tasks (i.e. the different learning algorithms are evaluated on the same test set, similarly, also the different dataset subsets all share the same test set). This allows for fair comparisons of different resamplings on a concrete problem (i.e. a given dataset, train_valid size and learning algorithm). Additionally, for the random search, the 500 HPCs evaluated for a given learning algorithm are also fixed over different dataset and train_valid size combinations. This is done to allow for an isolation of the effect, the concrete resampling (and whether it is reshuffled or not) has on generalization performance, reducing noise arising due to different HPCs. Learning algorithms themselves are not explicitly seeded to allow for variation during model training over different replications. Resamplings and partitioning of data are always performed in a stratified manner with respect to the target variable. For the random search, we only ran (standard and reshuffled) holdout and (standard and reshuffled) 5x 5-fold CV experiments (because we can simulate 5-fold CV and 5-fold holdout experiments based 28on the results obtained from the 5x 5-fold CV (by only considering the first repeat or the first fold for each of the five repeats).9 For running HEBO or SMAC3, each resampling (standard and reshuffled for holdout, 5-fold holdout, 5-fold CV , 5x 5-fold CV) has to be actually run due to the adaptive nature of BO. For the random search experiments, this results in 10 (DGPs)× 3 (train_valid sizes) × 4 (learning algorithms) × 2 (holdout or 5x 5-fold CV) × 2 (standard or reshuffled) × 10 (replications) = 4800 HPO runs,10 each involving the evaluation of 500 HPCs and each evaluation of an HPC involving either 2 (for holdout; due to retraining on train validation data) or 26 (for 5x 5-fold CV; due to retraining on train validation data) model fits. In summary, the random search experiments involve the evaluation of 2.4 Million HPCs with in total 33.6 Million model fits. Similarly, for the HEBO and SMAC3 experiments, this each results in 10 (DGPs)× 3 (train_valid sizes) × 4 (learning algorithms) × 4 (holdout, 5-fold CV , 5x 5-fold CV or 5-fold holdout) × 2 (standard or reshuffled) × 10 (replications) = 9600 HPO runs 11, each involving the evaluation of 250 HPCs and each evaluation of an HPC involving either 2 (for holdout; due to retraining on train validation data), 6 (for 5-fold CV or 5-fold holdout; due to retraining on train validation data) or 26 (for 5x 5-fold CV; due to retraining on train validation data) model fits. In summary, the HEBO and SMAC3 experiments each involve the evaluation of 2.4 Million HPCs with in total 24 Million model fits. F.4 Compute Resources We estimate our total compute time for the random search, HEBO and SMAC3 experiments to be roughly 11.86 CPU years. Benchmark experiments were run on an internal HPC cluster equipped with a mix of Intel Xeon E5-2670, Intel Xeon E5-2683 and Intel Xeon Gold 6330 instances. Jobs were scheduled to use a single CPU core and were allowed to use up to 16GB RAM. Total emissions are estimated to be an equivalent of roughly 6508.67 kg CO2. G Additional Benchmark Results Visualizations G.1 Main Experiments In this section, we provide additional visualizations of the results of our benchmark experiments. Figure 6 illustrates the trade-off between the final number of model fits required by different resam- plings and the final average normalized test performance (AUC ROC) after running random search for a budget of 500 hyperparameter configurations. We can see that the reshuffled holdout on average comes close to the final test performance of the overall more expensive 5-fold CV . Below, we give an overview of the different types of additional analyses and visualizations we provide. Normalized metrics, i.e., normalized validation or test performance refer to the measure being scaled to [0, 1] based on the empirical observed minimum and maximum values obtained on the raw results level (ADTM; see Wistuba et al., 2018). More concretely, for each scenario consisting of a learning algorithm that is run on a given task (dataset × train_valid size) given a certain performance metric, the performance values (validation or test) for all resamplings and optimizers are normalized on the replication level to [0, 1] by subtracting the empirical best value and dividing by the range of performance values. Therefore a normalized performance value of 0 is best and 1 is worst. Note that we additionally provide further aggregated results on the learning algorithm level and raw results of validation and test performance via https://github.com/slds-lmu/paper_2024_reshuffl ing. • Random search – Normalized validation performance in Figure 7. 9We even could have simulated the vanilla holdout from the 5x 5-fold CV experiments by choosing an arbitrary fold and repeat but choose not to do so, to have some sanity checks regarding our implementation by being able to compare the \"true\" holdout with a the simulated holdout. 10Note that we do not have to take the 3 different metrics into account because random search allows us to simulate runs for different metric post hoc. 11Note that HEBO and SMAC3 were only run for ROC AUC as the performance metric. 29500 1000 5000 300 1000 3000 10000 300 1000 3000 10000 300 1000 3000 10000 0.20 0.25 0.30 0.35 0.40 0.25 0.30 0.35 0.40 0.45 0.30 0.35 0.40 0.45 No. Final Model Fits Mean Normalized T est Performance Reshuffling FALSE TRUE Resampling Holdout 5−fold CV 5−fold Holdout 5x 5−fold CV Figure 6: Trade-off between the final number of model fits required by different resamplings and the final average normalized test performance (AUC ROC) after running random search for a budget of 500 hyperparameter configurations. Averaged over different tasks, learning algorithms and replications separately for increasing n (train-validation sizes, columns). Shaded areas represent standard errors. – Normalized test performance in Figure 8. – Improvement in test performance over 5-fold CV in Figure 9. – Rank w.r.t. test performance in Figure 10. • HEBO and SMAC3 vs. random search holdout – Normalized validation performance in Figure 11. – Normalized test performance in Figure 12. – Improvement in test performance over standard holdout in Figure 13. – Rank w.r.t. test performance in Figure 14. • HEBO and SMAC3 vs. random search 5-fold holdout – Normalized validation performance in Figure 15. – Normalized test performance in Figure 16. – Improvement in test performance over standard 5-fold holdout in Figure 17. – Rank w.r.t. test performance in Figure 18. • HEBO and SMAC3 vs. random search 5-fold CV – Normalized validation performance in Figure 19. – Normalized test performance in Figure 20. – Improvement in test performance over 5-fold CV in Figure 21. – Rank w.r.t. test performance in Figure 22. • HEBO and SMAC3 vs. random search 5x 5-fold CV – Normalized validation performance in Figure 23. – Normalized test performance in Figure 24. – Improvement in test performance over 5x 5-fold CV in Figure 25. – Rank w.r.t. test performance in Figure 26. 30Logloss, 500 Logloss, 1000 Logloss, 5000 ROC AUC, 500 ROC AUC, 1000 ROC AUC, 5000 Accuracy , 500 Accuracy , 1000 Accuracy , 5000 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 0.00 0.25 0.50 0.75 0.00 0.25 0.50 0.75 0.00 0.25 0.50 0.75 0.00 0.25 0.50 0.75 0.00 0.25 0.50 0.75 0.00 0.25 0.50 0.75 0.25 0.50 0.75 0.00 0.25 0.50 0.75 0.00 0.25 0.50 0.75 No. HPC Evaluations Mean Normalized Validation Performance Reshuffling FALSE TRUE Resampling Holdout 5−fold CV 5−fold Holdout 5x 5−fold CV Figure 7: Random search. Average normalized performance over tasks, learners and replications for different n (train-validation sizes, columns). Shaded areas represent standard errors. Logloss, 500 Logloss, 1000 Logloss, 5000 ROC AUC, 500 ROC AUC, 1000 ROC AUC, 5000 Accuracy , 500 Accuracy , 1000 Accuracy , 5000 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 0.3 0.4 0.5 0.2 0.3 0.4 0.5 0.6 0.2 0.3 0.4 0.5 0.6 0.2 0.3 0.4 0.5 0.3 0.4 0.5 0.1 0.2 0.3 0.4 0.5 0.3 0.4 0.25 0.30 0.35 0.40 0.45 0.50 0.1 0.2 0.3 0.4 0.5 No. HPC Evaluations Mean Normalized T est Performance Reshuffling FALSE TRUE Resampling Holdout 5−fold CV 5−fold Holdout 5x 5−fold CV Figure 8: Random search. Average normalized test performance over tasks, learners and replications for different n (train-validation sizes, columns). Shaded areas represent standard errors. 31Logloss, 500 Logloss, 1000 Logloss, 5000 ROC AUC, 500 ROC AUC, 1000 ROC AUC, 5000 Accuracy , 500 Accuracy , 1000 Accuracy , 5000 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 −0.4 −0.3 −0.2 −0.1 0.0 0.1 −0.50 −0.25 0.00 0.25 −0.3 −0.2 −0.1 0.0 0.1 −0.4 −0.2 0.0 −1.2 −0.8 −0.4 0.0 0.4 −1.5 −1.0 −0.5 0.0 −0.75 −0.50 −0.25 0.00 0.25 −1.0 −0.5 0.0 0.5 −2.0 −1.5 −1.0 −0.5 0.0 No. HPC Evaluations Mean T est Improvement Reshuffling FALSE TRUE Resampling Holdout 5−fold CV 5−fold Holdout 5x 5−fold CV Figure 9: Random search. Average improvement (compared to standard 5-fold CV) with respect to test performance of the incumbent over tasks, learners and replications for differentn (train-validation sizes, columns). Shaded areas represent standard errors. Logloss, 500 Logloss, 1000 Logloss, 5000 ROC AUC, 500 ROC AUC, 1000 ROC AUC, 5000 Accuracy , 500 Accuracy , 1000 Accuracy , 5000 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 4.0 4.5 5.0 5.5 3.5 4.0 4.5 5.0 5.5 6.0 4 5 4.0 4.5 5.0 5.5 3.5 4.0 4.5 5.0 5.5 6.0 4 5 6 4.0 4.5 5.0 4.0 4.5 5.0 5.5 4 5 6 No. HPC Evaluations Mean Rank (T est Performance) Reshuffling FALSE TRUE Resampling Holdout 5−fold CV 5−fold Holdout 5x 5−fold CV Figure 10: Random search. Average ranks (lower is better) with respect to test performance over tasks, learners and replications for different n (train-validation sizes, columns). Shaded areas represent standard errors. 32500 1000 5000 1 50 100 150 200 250 1 50 100 150 200 250 1 50 100 150 200 250 0.0 0.2 0.4 0.6 0.8 0.2 0.4 0.6 0.8 0.2 0.4 0.6 0.8 No. HPC Evaluations Mean Normalized Validation Performance Optimizer Random Search HEBO SMAC3 Reshuffling FALSE TRUE Figure 11: HEBO and SMAC3 vs. random search for holdout. Average normalized validation performance (ROC AUC) over tasks, learners and replications for differentn (train-validation sizes, columns). Shaded areas represent standard errors. 500 1000 5000 1 50 100 150 200 250 1 50 100 150 200 250 1 50 100 150 200 250 0.2 0.3 0.4 0.5 0.3 0.4 0.5 0.30 0.35 0.40 0.45 0.50 No. HPC Evaluations Mean Normalized T est Performance Optimizer Random Search HEBO SMAC3 Reshuffling FALSE TRUE Figure 12: HEBO and SMAC3 vs. random search for holdout. Average normalized test performance (ROC AUC) over tasks, learners and replications for different n (train-validation sizes, columns). Shaded areas represent standard errors. 500 1000 5000 1 50 100 150 200 250 1 50 100 150 200 250 1 50 100 150 200 250 0.0 0.5 1.0 1.5 −0.5 0.0 0.5 1.0 −0.5 0.0 0.5 1.0 No. HPC Evaluations Mean T est Improvement Optimizer Random Search HEBO SMAC3 Reshuffling FALSE TRUE Figure 13: HEBO and SMAC3 vs. random search for holdout. Average improvement (compared to standard holdout) with respect to test performance (ROC AUC) of the incumbent over tasks, learners and replications for different n (train-validation sizes, columns). Shaded areas represent standard errors. 33500 1000 5000 1 50 100 150 200 250 1 50 100 150 200 250 1 50 100 150 200 250 3.25 3.50 3.75 4.00 3.00 3.25 3.50 3.75 4.00 3.00 3.25 3.50 3.75 4.00 No. HPC Evaluations Mean Rank (T est Performance) Optimizer Random Search HEBO SMAC3 Reshuffling FALSE TRUE Figure 14: HEBO and SMAC3 vs. random search for holdout. Average ranks (lower is better) with respect to test performance (ROC AUC) of the incumbent over tasks, learners and replications for different n (train-validation sizes, columns). Shaded areas represent standard errors. 500 1000 5000 1 50 100 150 200 250 1 50 100 150 200 250 1 50 100 150 200 250 0.2 0.4 0.6 0.2 0.4 0.6 0.8 0.2 0.4 0.6 0.8 No. HPC Evaluations Mean Normalized Validation Performance Optimizer Random Search HEBO SMAC3 Reshuffling FALSE TRUE Figure 15: HEBO and SMAC3 vs. random search for 5-fold holdout. Average normalized validation performance (ROC AUC) over tasks, learners and replications for differentn (train-validation sizes, columns). Shaded areas represent standard errors. 500 1000 5000 1 50 100 150 200 250 1 50 100 150 200 250 1 50 100 150 200 250 0.25 0.35 0.45 0.55 0.3 0.4 0.5 0.25 0.30 0.35 0.40 0.45 0.50 No. HPC Evaluations Mean Normalized T est Performance Optimizer Random Search HEBO SMAC3 Reshuffling FALSE TRUE Figure 16: HEBO and SMAC3 vs. random search for 5-fold holdout. Average normalized test performance (ROC AUC) over tasks, learners and replications for differentn (train-validation sizes, columns). Shaded areas represent standard errors. 34500 1000 5000 1 50 100 150 200 250 1 50 100 150 200 250 1 50 100 150 200 250 0.0 0.5 1.0 1.5 −0.5 0.0 0.5 1.0 1.5 −0.5 0.0 0.5 No. HPC Evaluations Mean T est Improvement Optimizer Random Search HEBO SMAC3 Reshuffling FALSE TRUE Figure 17: HEBO and SMAC3 vs. random search for 5-fold holdout. Average improvement (compared to standard 5-fold holdout) with respect to test performance (ROC AUC) of the incumbent over tasks, learners and replications for different n (train-validation sizes, columns). Shaded areas represent standard errors. 500 1000 5000 1 50 100 150 200 250 1 50 100 150 200 250 1 50 100 150 200 250 3.00 3.25 3.50 3.75 4.00 3.25 3.50 3.75 3.25 3.50 3.75 No. HPC Evaluations Mean Rank (T est Performance) Optimizer Random Search HEBO SMAC3 Reshuffling FALSE TRUE Figure 18: HEBO and SMAC3 vs. random search for 5-fold holdout. Average ranks (lower is better) with respect to test performance (ROC AUC) of the incumbent tasks, learners and replications for different n (train-validation sizes, columns). Shaded areas represent standard errors. 500 1000 5000 1 50 100 150 200 250 1 50 100 150 200 250 1 50 100 150 200 250 0.2 0.4 0.6 0.2 0.4 0.6 0.8 0.2 0.4 0.6 0.8 No. HPC Evaluations Mean Normalized Validation Performance Optimizer Random Search HEBO SMAC3 Reshuffling FALSE TRUE Figure 19: HEBO and SMAC3 vs. random search for 5-fold CV . Average normalized validation performance (ROC AUC) over tasks, learners and replications for differentn (train-validation sizes, columns). Shaded areas represent standard errors. 35500 1000 5000 1 50 100 150 200 250 1 50 100 150 200 250 1 50 100 150 200 250 0.2 0.3 0.4 0.5 0.3 0.4 0.5 0.30 0.35 0.40 0.45 0.50 No. HPC Evaluations Mean Normalized T est Performance Optimizer Random Search HEBO SMAC3 Reshuffling FALSE TRUE Figure 20: HEBO and SMAC3 vs. random search for 5-fold CV . Average normalized test performance (ROC AUC) over tasks, learners and replications for different n (train-validation sizes, columns). Shaded areas represent standard errors. 500 1000 5000 1 50 100 150 200 250 1 50 100 150 200 250 1 50 100 150 200 250 0.0 0.5 1.0 1.5 0.0 0.5 1.0 1.5 −0.5 0.0 0.5 1.0 No. HPC Evaluations Mean T est Improvement Optimizer Random Search HEBO SMAC3 Reshuffling FALSE TRUE Figure 21: HEBO and SMAC3 vs. random search for 5-fold CV . Average improvement (compared to standard 5-fold CV) with respect to test performance (ROC AUC) of the incumbent over tasks, learners and replications for different n (train-validation sizes, columns). Shaded areas represent standard errors. 500 1000 5000 1 50 100 150 200 250 1 50 100 150 200 250 1 50 100 150 200 250 3.25 3.50 3.75 3.2 3.4 3.6 3.8 3.3 3.5 3.7 No. HPC Evaluations Mean Rank (T est Performance) Optimizer Random Search HEBO SMAC3 Reshuffling FALSE TRUE Figure 22: HEBO and SMAC3 vs. random search for 5-fold CV . Average ranks (lower is better) with respect to test performance (ROC AUC) of the incumbent over tasks, learners and replications for different n (train-validation sizes, columns). Shaded areas represent standard errors. 36500 1000 5000 1 50 100 150 200 250 1 50 100 150 200 250 1 50 100 150 200 250 0.0 0.2 0.4 0.6 0.2 0.4 0.6 0.2 0.4 0.6 0.8 No. HPC Evaluations Mean Normalized Validation Performance Optimizer Random Search HEBO SMAC3 Reshuffling FALSE TRUE Figure 23: HEBO and SMAC3 vs. random search for 5x 5-fold CV . Average normalized validation performance (ROC AUC) over tasks, learners and replications for differentn (train-validation sizes, columns). Shaded areas represent standard errors. 500 1000 5000 1 50 100 150 200 250 1 50 100 150 200 250 1 50 100 150 200 250 0.2 0.3 0.4 0.5 0.3 0.4 0.5 0.3 0.4 0.5 No. HPC Evaluations Mean Normalized T est Performance Optimizer Random Search HEBO SMAC3 Reshuffling FALSE TRUE Figure 24: HEBO and SMAC3 vs. random search for 5x 5-fold CV . Average normalized test performance (ROC AUC) over tasks, learners and replications for differentn (train-validation sizes, columns). Shaded areas represent standard errors. 500 1000 5000 1 50 100 150 200 250 1 50 100 150 200 250 1 50 100 150 200 250 0.0 0.5 1.0 1.5 0.0 0.5 1.0 1.5 −0.4 0.0 0.4 0.8 1.2 No. HPC Evaluations Mean T est Improvement Optimizer Random Search HEBO SMAC3 Reshuffling FALSE TRUE Figure 25: HEBO and SMAC3 vs. random search for 5x 5-fold CV . Average improvement (compared to standard 5x 5-fold CV) with respect to test performance (ROC AUC) of the incumbent over tasks, learners and replications for different n (train-validation sizes, columns). Shaded areas represent standard errors. 37500 1000 5000 1 50 100 150 200 250 1 50 100 150 200 250 1 50 100 150 200 250 3.00 3.25 3.50 3.75 3.25 3.50 3.75 4.00 3.2 3.4 3.6 3.8 No. HPC Evaluations Mean Rank (T est Performance) Optimizer Random Search HEBO SMAC3 Reshuffling FALSE TRUE Figure 26: HEBO and SMAC3 vs. random search for 5x 5-fold CV . Average ranks (lower is better) with respect to test performance (ROC AUC) of the incumbent over tasks, learners and replications for different n (train-validation sizes, columns). Shaded areas represent standard errors. 38G.2 Ablation on M-fold holdout Based on the 5x 5-fold CV results we further simulated different M-fold holdout resamplings (standard and reshuffled) by taking M repeats from the first fold of the 5x 5-fold CV . This allows us to get an understanding of the effect more folds have on M-fold holdout, especially in the context of reshuffling. Regarding normalized validation performance we observe that more folds generally result in a less optimistically biased validation performance (see Figure 27). Looking at normalized test performance (Figure 28) we observe the general trend that more folds result in better test performance – which is expected. Reshuffling generally results in better test performance compared to the standard resampling (with the exception of logloss where especially in the case of a single holdout, reshuffling can hurt generalization performance). This effect is smaller, the more folds are used, which is in line with our theoretical results presented in Table 1. Looking at improvement compared to standard 5-fold holdout with respect to test performance and ranks with respect to test performance, we observe that often reshuffled 2-fold holdout results that are highly competitive with standard 3, 4 or 5-fold holdout. Logloss, 500 Logloss, 1000 Logloss, 5000 ROC AUC, 500 ROC AUC, 1000 ROC AUC, 5000 Accuracy , 500 Accuracy , 1000 Accuracy , 5000 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 0.00 0.25 0.50 0.75 0.00 0.25 0.50 0.75 0.00 0.25 0.50 0.75 0.25 0.50 0.75 0.00 0.25 0.50 0.75 0.00 0.25 0.50 0.75 0.25 0.50 0.75 0.0 0.2 0.4 0.6 0.8 0.25 0.50 0.75 No. HPC Evaluations Mean Normalized Validation Performance Holdout 1−fold 2−fold 3−fold 4−fold 5−fold Reshuffling FALSE TRUE Figure 27: Random search. Average normalized validation performance over tasks, learners and replications for different n (train-validation sizes, columns). Shaded areas represent standard errors. 39Logloss, 500 Logloss, 1000 Logloss, 5000 ROC AUC, 500 ROC AUC, 1000 ROC AUC, 5000 Accuracy , 500 Accuracy , 1000 Accuracy , 5000 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 0.25 0.30 0.35 0.40 0.45 0.50 0.2 0.3 0.4 0.5 0.6 0.2 0.3 0.4 0.5 0.6 0.2 0.3 0.4 0.5 0.3 0.4 0.5 0.1 0.2 0.3 0.4 0.5 0.25 0.30 0.35 0.40 0.45 0.30 0.35 0.40 0.45 0.50 0.1 0.2 0.3 0.4 0.5 No. HPC Evaluations Mean Normalized T est Performance Holdout 1−fold 2−fold 3−fold 4−fold 5−fold Reshuffling FALSE TRUE Figure 28: Random search. Average normalized test performance over tasks, learners and replications for different n (train-validation sizes, columns). Shaded areas represent standard errors. Logloss, 500 Logloss, 1000 Logloss, 5000 ROC AUC, 500 ROC AUC, 1000 ROC AUC, 5000 Accuracy , 500 Accuracy , 1000 Accuracy , 5000 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 −0.4 −0.3 −0.2 −0.1 0.0 0.1 −0.6 −0.4 −0.2 0.0 0.2 −0.3 −0.2 −0.1 0.0 0.1 −0.4 −0.3 −0.2 −0.1 0.0 0.1 −1.0 −0.5 0.0 −1.5 −1.0 −0.5 0.0 −0.50 −0.25 0.00 0.25 0.50 −1.0 −0.5 0.0 −2.0 −1.5 −1.0 −0.5 0.0 No. HPC Evaluations Mean T est Improvement Holdout 1−fold 2−fold 3−fold 4−fold 5−fold Reshuffling FALSE TRUE Figure 29: Random search. Average improvement (compared to standard 5-fold holdout) with respect to test performance of the incumbent over tasks, learners and replications for different n (train-validation sizes, columns). Shaded areas represent standard errors. 40Logloss, 500 Logloss, 1000 Logloss, 5000 ROC AUC, 500 ROC AUC, 1000 ROC AUC, 5000 Accuracy , 500 Accuracy , 1000 Accuracy , 5000 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 1 100 200 300 400 500 5.0 5.5 6.0 6.5 5.0 5.5 6.0 6.5 7.0 4.5 5.0 5.5 6.0 6.5 5.0 5.5 6.0 6.5 4.5 5.0 5.5 6.0 6.5 7.0 5 6 7 4.8 5.2 5.6 6.0 5.0 5.5 6.0 6.5 5 6 7 No. HPC Evaluations Mean Rank (T est Performance) Holdout 1−fold 2−fold 3−fold 4−fold 5−fold Reshuffling FALSE TRUE Figure 30: Random search. Average ranks (lower is better) with respect to test performance of the incumbent over tasks, learners and replications for different n (train-validation sizes, columns). Shaded areas represent standard errors. 41NeurIPS Paper Checklist 1. Claims Question: Do the main claims made in the abstract and introduction accurately reflect the paper’s contributions and scope? Answer: [Yes] Justification: We outline our three main contributions in the introduction (Section 1). We do not discuss generalization in the introduction, but rather in the discussion in Section 5. Guidelines: • The answer NA means that the abstract and introduction do not include the claims made in the paper. • The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. • The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. • It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. 2. Limitations Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: The paper provides an analysis of reshuffling data in the context of estimating the generalization error for hyperparameter optimization. Our theoretical analysis explains why reshuffling works, and we experimentally verify the theoretical analysis. We discuss the limitations of our work in Section 5. Guidelines: • The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. • The authors are encouraged to create a separate \"Limitations\" section in their paper. • The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. • The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. • The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. • The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. • If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. • While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren’t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an impor- tant role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. 3. Theory Assumptions and Proofs 42Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: Full assumptions and proofs for our main results (Theorem 2.1 and Theo- rem 2.2) are given in Appendix C.1 and Appendix C.2, respectively. Derivations for the parameters in Table 1 are provided in Appendix E. The additional results for the grid density are stated and proven directly in Appendix D. Guidelines: • The answer NA means that the paper does not include theoretical results. • All the theorems, formulas, and proofs in the paper should be numbered and cross- referenced. • All assumptions should be clearly stated or referenced in the statement of any theorems. • The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. • Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. • Theorems and Lemmas that the proof relies upon should be properly referenced. 4. Experimental Result Reproducibility Question: Does the paper fully disclose all the information needed to reproduce the main ex- perimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provide thorough details on the experimental setup in Section 4.1 and Appendix F. Moreover, we provide code to reproduce our results under an open source license at https://github.com/slds-lmu/paper_2024_reshuffling. Guidelines: • The answer NA means that the paper does not include experiments. • If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. • If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. • Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. • While NeurIPS does not require releasing code, the conference does require all submis- sions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 43(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. 5. Open access to data and code Question: Does the paper provide open access to the data and code, with sufficient instruc- tions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: Regarding datasets, we rely on OpenML.org. We provide thorough details on the experimental setup in Section 4.1 and Appendix F. Moreover, we provide code to reproduce our results under an open source license at https://github.com/slds-lmu /paper_2024_reshuffling. Guidelines: • The answer NA means that paper does not include experiments requiring code. • Please see the NeurIPS code and data submission guidelines ( https://nips.cc/pu blic/guides/CodeSubmissionPolicy) for more details. • While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). • The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. • The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. • The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. • At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). • Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. 6. Experimental Setting/Details Question: Does the paper specify all the training and test details (e.g., data splits, hyper- parameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We provide thorough details on the experimental setup in Section 4.1 and Appendix F. Moreover, we provide code to reproduce our results under an open source license at https://github.com/slds-lmu/paper_2024_reshuffling. Guidelines: • The answer NA means that the paper does not include experiments. • The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. • The full details can be provided either with the code, in appendix, or as supplemental material. 7. Experiment Statistical Significance Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] 44Justification: We report the standard error in every analysis. Guidelines: • The answer NA means that the paper does not include experiments. • The authors should answer \"Yes\" if the results are accompanied by error bars, confi- dence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. • The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). • The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) • The assumptions made should be given (e.g., Normally distributed errors). • It should be clear whether the error bar is the standard deviation or the standard error of the mean. • It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. • For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). • If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. 8. Experiments Compute Resources Question: For each experiment, does the paper provide sufficient information on the com- puter resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We provide details in Appendix F.4. Guidelines: • The answer NA means that the paper does not include experiments. • The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. • The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. • The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn’t make it into the paper). 9. Code Of Ethics Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Our work provides a study on reshuffling data when estimating the generaliza- tion error in hyperparameter tuning. Therefore, our work is applicable wherever standard machine learning is applicable, and we do not see any ethical concerns in our method. Guidelines: • The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. • If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. • The authors should make sure to preserve anonymity (e.g., if there is a special consid- eration due to laws or regulations in their jurisdiction). 10. Broader Impacts 45Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: The paper conducts fundamental research that is not tied to particular applica- tions, let alone deployment. Guidelines: • The answer NA means that there is no societal impact of the work performed. • If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. • Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. • The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. • The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. • If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). 11. Safeguards Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper conducts fundamental research that is not tied to particular applica- tions, let alone deployment. The paper does not develop models that have a high risk for misuse. Guidelines: • The answer NA means that the paper poses no such risks. • Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. • Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. • We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. 12. Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] 46Justification: We used datasets from OpenML.org and reference the dataset pages. Further information of the datasets, including their licenses, are available at OpenML.org. Guidelines: • The answer NA means that the paper does not use existing assets. • The authors should cite the original paper that produced the code package or dataset. • The authors should state which version of the asset is used and, if possible, include a URL. • The name of the license (e.g., CC-BY 4.0) should be included for each asset. • For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. • If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. • For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. • If this information is not available online, the authors are encouraged to reach out to the asset’s creators. 13. New Assets Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We provide code as a new asset and describe how we make our code available in Point 5 of the NeurIPS Paper Checklist. Guidelines: • The answer NA means that the paper does not release new assets. • Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. • The paper should discuss whether and how consent was obtained from people whose asset is used. • At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. 14. Crowdsourcing and Research with Human Subjects Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does neither involve crowdsourcing nor research with human subjects. Guidelines: • The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Including this information in the supplemental material is fine, but if the main contribu- tion of the paper involves human subjects, then as much detail as possible should be included in the main paper. • According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects 47Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does neither involve crowdsourcing nor research with human subjects. Guidelines: • The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. • We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. • For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. 48",
      "meta_data": {
        "arxiv_id": "2405.15393v2",
        "authors": [
          "Thomas Nagler",
          "Lennart Schneider",
          "Bernd Bischl",
          "Matthias Feurer"
        ],
        "published_date": "2024-05-24T09:48:18Z",
        "pdf_url": "https://arxiv.org/pdf/2405.15393v2.pdf",
        "github_url": "https://github.com/slds-lmu/paper_2024_reshuffling"
      },
      "llm_extracted_info": {
        "main_contributions": "The research addresses the problem of hyperparameter optimization (HPO) where fixed resampling splits are commonly used, potentially leading to overfitting to the validation set and reduced generalization performance. The main contribution is demonstrating, theoretically and empirically, that reshuffling resampling splits for every hyperparameter configuration (HPC) often improves the final model's generalization performance on unseen data. Specifically, it provides a theoretical explanation for how reshuffling affects the asymptotic behavior of the validation loss surface and offers a bound on the expected regret. This is supported by controlled simulation studies and large-scale, realistic HPO benchmark experiments showing practical benefits, especially for holdout protocols, making them competitive with standard cross-validation.",
        "methodology": "The methodology involves a theoretical analysis and an empirical simulation and benchmark study. Theoretically, the paper investigates how reshuffling affects the empirical loss surface by deriving the limiting distribution of the validation loss error, characterizing it with parameters for variance (σ²) and correlation (τ²) changes due to reshuffling. It then provides a bound on the expected regret based on a Gaussian process model for the empirical loss, relating reshuffling's benefits to the signal-to-noise ratio, loss surface curvature (m), and noise correlation (κ). For the simulation study, a univariate quadratic loss surface combined with a squared exponential kernel for the noise process (ϵ) is used to systematically vary m, κ, and τ. The benchmark experiments employ various machine learning algorithms (CatBoost, XGBoost, Elastic Net, Funnel MLP) and HPO optimizers (Random Search, HEBO, SMAC3) across different resampling strategies (holdout, M-fold CV, M-fold holdout, M-times M-fold CV), comparing fixed versus reshuffled splits.",
        "experimental_setup": "The experimental setup utilized a subset of 10 tabular datasets from the AutoML benchmark for binary classification, serving as data generating processes (DGPs). For each DGP, 5000 data points were reserved as an independent outer test set for robust generalization error assessment. HPO was performed on subsets of varying sizes: 500, 1000, and 5000 observations for combined training and validation data. Learning algorithms included CatBoost, XGBoost, Elastic Net, and a funnel-shaped MLP, each with defined search spaces and preprocessing pipelines. Resampling strategies included holdout (80/20 split), 5-fold CV, 5-fold holdout, and 5x 5-fold CV, all evaluated with both fixed and reshuffled splits. HPO optimizers were Random Search (500 HPC evaluations) and Bayesian Optimization variants HEBO and SMAC3 (250 HPC evaluations). Performance was measured using ROC AUC, Accuracy, and Logloss (for Random Search) or only ROC AUC (for BO). Anytime test performance was assessed by retraining the current incumbent on all available train+validation data and evaluating on the outer test set. Each scenario was run for 10 replications. Total computational resources were estimated at 11.86 CPU years, with total emissions of approximately 6508.67 kg CO2.",
        "limitations": "The theoretical analysis relies on an asymptotic approximation of the empirical loss surface, assuming Gaussian loss surfaces for mathematical tractability. It also uses a loss stability assumption for learning algorithms which is generally mild but may fail for highly sensitive losses like logloss, empirically observed to sometimes hurt generalization. The study's scope is limited to generalization after searching through a fixed, finite set of candidates, not fully capturing the dynamic nature of many HPO algorithms. Furthermore, experiments were restricted to tabular data and binary classification, avoiding extremely small or large datasets, which may limit the generalizability of empirical findings.",
        "future_research_directions": "Future research directions include developing a unified formal definition for 'oversearching,' 'overtuning,' or 'overfitting to the validation set' and thoroughly analyzing its relationship to validation performance measurements. It is suggested to further investigate adaptive cross-validation techniques to mitigate computational burden while still allowing more intensive evaluation of HPCs. Designing more advanced HPO algorithms that explicitly exploit the reshuffling effect is a promising avenue. Additionally, exploring the combination of reshuffling with existing overfitting mitigation strategies (e.g., LOOCVCV, extra selection sets, early stopping) could yield further improvements. Investigating reshuffling's impact on multi-class datasets and developing less naive implementations to address its negative effects on metrics like logloss are also recommended."
      }
    },
    {
      "title": "DP-HyPO: An Adaptive Private Framework for Hyperparameter Optimization",
      "abstract": "Hyperparameter optimization, also known as hyperparameter tuning, is a widely\nrecognized technique for improving model performance. Regrettably, when\ntraining private ML models, many practitioners often overlook the privacy risks\nassociated with hyperparameter optimization, which could potentially expose\nsensitive information about the underlying dataset. Currently, the sole\nexisting approach to allow privacy-preserving hyperparameter optimization is to\nuniformly and randomly select hyperparameters for a number of runs,\nsubsequently reporting the best-performing hyperparameter. In contrast, in\nnon-private settings, practitioners commonly utilize ``adaptive''\nhyperparameter optimization methods such as Gaussian process-based\noptimization, which select the next candidate based on information gathered\nfrom previous outputs. This substantial contrast between private and\nnon-private hyperparameter optimization underscores a critical concern. In our\npaper, we introduce DP-HyPO, a pioneering framework for ``adaptive'' private\nhyperparameter optimization, aiming to bridge the gap between private and\nnon-private hyperparameter optimization. To accomplish this, we provide a\ncomprehensive differential privacy analysis of our framework. Furthermore, we\nempirically demonstrate the effectiveness of DP-HyPO on a diverse set of\nreal-world datasets.",
      "full_text": "DP-HyPO: An Adaptive Private Hyperparameter Optimization Framework Hua Wang∗ Sheng Gao† Huanyu Zhang‡ Weijie J. Su§ Milan Shen¶ November 28, 2023 Abstract Hyperparameter optimization, also known as hyperparameter tuning, is a widely recognized technique for improving model performance. Regrettably, when training private ML models, many practitioners often overlook the privacy risks associated with hyperparameter optimization, which could potentially expose sensitive information about the underlying dataset. Currently, the sole existing approach to allow privacy-preserving hyperparameter optimization is to uniformly and randomly select hyperparameters for a number of runs, subsequently reporting the best- performing hyperparameter. In contrast, in non-private settings, practitioners commonly utilize “adaptive” hyperparameter optimization methods such as Gaussian process-based optimization, which select the next candidate based on information gathered from previous outputs. This substantial contrast between private and non-private hyperparameter optimization underscores a critical concern. In our paper, we introduce DP-HyPO, a pioneering framework for “adaptive” private hyperparameter optimization, aiming to bridge the gap between private and non-private hyperparameter optimization. To accomplish this, we provide a comprehensive differential privacy analysis of our framework. Furthermore, we empirically demonstrate the effectiveness of DP-HyPO on a diverse set of real-world datasets. 1 Introduction In recent decades, modern deep learning has demonstrated remarkable advancements in various applications. Nonetheless, numerous training tasks involve the utilization of sensitive information pertaining to individuals, giving rise to substantial concerns regarding privacy [31, 7]. To address these concerns, the concept of differential privacy (DP) was introduced by [13, 14]. DP provides a mathematically rigorous framework for quantifying privacy leakage, and it has gained widespread acceptance as the most reliable approach for formally evaluating the privacy guarantees of machine learning algorithms. ∗Department of Statistics and Data Science, University of Pennsylvania, Philadelphia, PA 19104, USA. Email: wanghua@wharton.upenn.edu. †Department of Statistics and Data Science, University of Pennsylvania, Philadelphia, PA 19104, USA. Email: shenggao@wharton.upenn.edu. ‡Meta Platforms, Inc., New York, NY 10003, USA. Email:huanyuzhang@meta.com. §Department of Statistics and Data Science, University of Pennsylvania, Philadelphia, PA 19104, USA. Email: suw@wharton.upenn.edu. ¶Meta Platforms, Inc., Menlo Park, CA 94025, USA. Email:milanshen@gmail.com. 1 arXiv:2306.05734v2  [cs.LG]  27 Nov 2023When training deep learning models, the most popular method to ensure privacy is noisy (stochastic) gradient descent (DP-SGD) [4, 37]. DP-SGD typically resembles non-private gradient- based methods; however, it incorporates gradient clipping and noise injection. More specifically, each individual gradient is clipped to ensure a boundedℓ2 norm. Gaussian noise is then added to the average gradient which is utilized to update the model parameters. These adjustments guarantee a bounded sensitivity of each update, thereby enforcing DP through the introduction of additional noise. In both non-private and private settings, hyperparameter optimization (HPO) plays a crucial role in achieving optimal model performance. Commonly used methods for HPO include grid search (GS), random search (RS), and Bayesian optimization (BO). GS and RS approaches are typically non-adaptive, as they select the best hyperparameter from a predetermined or randomly selected set. While these methods are straightforward to implement, they can be computationally expensive and inefficient when dealing with large search spaces. As the dimensionality of hyperparameters increases, the number of potential trials may grow exponentially. To address this challenge, adaptive HPO methods such as Bayesian optimization have been introduced [36, 15, 42]. BO leverages a probabilistic model that maps hyperparameters to objective metrics, striking a balance between exploration and exploitation. BO quickly emerged as the default method for complex HPO tasks, offering improved efficiency and effectiveness compared to non-adaptive methods. While HPO is a well-studied problem, the integration of a DP constraint into HPO has received little attention. Previous works on DP machine learning often neglect to account for the privacy cost associated with HPO [1, 41, 44, 44]. These works either assume that the best parameters are known in advance or rely on a supplementary public dataset that closely resembles the private dataset distribution, which is not feasible in most real-world scenarios. Only recently have researchers turned to the important concept of honest HPO [30], where the privacy cost during HPO cannot be overlooked. Private HPO poses greater challenges compared to the non-private case for two primary reasons. First, learning with DP-SGD introduces additional hyperparameters (e.g., clipping norm, the noise scale, and stopping time), which hugely adds complexity to the search for optimal hyperparameters. Second, DP-SGD is more sensitive to the selection of hyperparameter combinations, with its performance largely influenced by this choice [30, 11, 33]. To tackle this challenging question, previous studies such as [26, 34] propose running the base algorithm with different hyperparameters a random number of times. They demonstrate that this approach significantly benefits privacy accounting, contrary to the traditional scaling of privacy guarantees with the square root of the number of runs (based on the composition properties from [21]). While these papers make valuable contributions, their approaches only allow for uniformly random subsampling from a finite and pre-fixed set of candidate hyperparameters at each run. As a result, any advanced technique from HPO literature that requires adaptivity is either prohibited or necessitates a considerable privacy cost (polynomially dependent on the number of runs), creating a substantial gap between non-private and private HPO methods. Given these considerations, a natural question arises:Can private hyperparameter optimization be adaptive, without a huge privacy cost?In this paper, we provide an affirmative answer to this question. 1.1 Our Contributions • We introduce the pioneering adaptive private hyperparameter optimization frame- work, DP-HyPO, which enables practitioners to adapt to previous runs and focus on 2potentially superior hyperparameters. DP-HyPO permits the flexible use of non-DP adaptive hyperparameter optimization methods, such as Gaussian process, for enhanced efficiency, while avoiding the substantial privacy costs due to composition. In contrast to the non-adaptive methods presented in [34, 26], our adaptive framework, DP-HyPO, effectively bridges the gap between private and non-private hyperparameter optimization. Importantly, our framework not only encompasses the aforementioned non-adaptive methods as special cases, but also seamlessly integrates virtually all conceivable adaptive methods into the framework. • We provide sharp DP guarantees for the adaptive private hyperparameter optimiza- tion. Specifically, when the training procedure is executed multiple times, with each iteration being DP on its own, outputting the best repetition is DP ensured by the composition property. However, applying composition results in excessively loose privacy guarantees. Prior work in [26, 34] presents bounds that are either independent of the number of repetitions or depend logarithmically on it. Nevertheless, these results require that the hyperparameter selection for each iteration follows a uniform sampling distribution. In contrast, DP-HyPO allows arbitrary adaptive sampling distributions based on previous runs. Utilizing the Rényi DP framework, we offer a strict generalization of those uniform results by providing an accurate characterization of the Rényi divergence between the adaptive sampling distributions of neighboring datasets, without any stability assumptions. • Empirically, we observe that the Gaussian process-based DP-HyPO algorithm outperforms its uniform counterpart across several practical scenarios.Gener- ally, practitioners can integrate any non-private adaptive HPO methods into the DP-HyPO framework, opening up a vast range of adaptive private HPO algorithm possibilities. Further- more, DP-HyPO grants practitioners the flexibility to determine the privacy budget allocation for adaptivity, empowering them to balance between the adaptivity and privacy loss when confronting various hyperparameter optimization challenges. 1.2 Our Contributions • We introduce the pioneering adaptive private hyperparameter optimization frame- work, DP-HyPO, which enables practitioners to adapt to previous runs and focus on potentially superior hyperparameters. DP-HyPO permits the flexible use of non-DP adaptive hyperparameter optimization methods, such as Gaussian process, for enhanced efficiency, while avoiding the substantial privacy costs due to composition. In contrast to the non-adaptive methods presented in [34, 26], our adaptive framework, DP-HyPO, effectively bridges the gap between private and non-private hyperparameter optimization. Importantly, our framework not only encompasses the aforementioned non-adaptive methods as special cases, but also seamlessly integrates virtually all conceivable adaptive methods into the framework. • We provide sharp DP guarantees for the adaptive private hyperparameter optimiza- tion. Specifically, when the training procedure is executed multiple times, with each iteration being DP on its own, outputting the best repetition is DP ensured by the composition property. However, applying composition results in excessively loose privacy guarantees. Prior work in [26, 34] presents bounds that are either independent of the number of repetitions or depend logarithmically on it. Nevertheless, these results require that the hyperparameter selection for each iteration follows a uniform sampling distribution. In contrast, DP-HyPO allows arbitrary 3adaptive sampling distributions based on previous runs. Utilizing the Rényi DP framework, we offer a strict generalization of those uniform results by providing an accurate characterization of the Rényi divergence between the adaptive sampling distributions of neighboring datasets, without any stability assumptions. • Empirically, we observe that the Gaussian process-based DP-HyPO algorithm out- performs its uniform counterpartacross several practical scenarios. Generally, practitioners can integrate any non-private adaptive HPO methods into the DP-HyPO framework, opening up a vast range of adaptive private HPO algorithm possibilities. Furthermore, DP-HyPO grants practitioners the flexibility to determine the privacy budget allocation for adaptivity, empowering them to balance between the adaptivity and privacy loss when confronting various hyperparameter optimization challenges. 2 Preliminaries 2.1 Differential Privacy and Hyperparameter Optimization Differential Privacy is a mathematically rigorous framework for quantifying privacy leakage. A DP algorithm promises that an adversary with perfect information about the entire private dataset in use – except for a single individual – would find it hard to distinguish between its presence or absence based on the output of the algorithm [13]. Formally, forε >0, and0 ≤ δ <1, we consider a (randomized) algorithmM : Zn → Ythat takes as input a dataset. Definition 2.1(Differential privacy). A randomized algorithmM is (ε, δ)-DP if for any neighboring dataset D, D′ ∈ Zn differing by an arbitrary sample, and for any eventE, we have P[M(D) ∈ E] ⩽ eε · P \u0002 M \u0000 D′\u0001 ∈ E \u0003 + δ. Here, ε and δ are privacy parameters that characterize the privacy guarantee of algorithmM. One of the fundamental properties of DP is composition. When multiple DP algorithms are sequentially composed, the resulting algorithm remains private. The total privacy cost of the composition scales approximately with the square root of the number of compositions [21]. We now formalize the problem of hyperparameter optimization with DP guarantees, which builds upon the finite-candidate framework presented in [26, 34]. Specifically, we consider a set of base DP algorithms Mλ : Zn → Y, whereλ ∈ Λ represents a set of hyperparameters of interest,Zn is the domain of datasets, andY denotes the range of the algorithms. This setΛ may be any infinite set, e.g., the cross product of the learning rateη and clipping normR in DP-SGD. We require that the set Λ is a measure space with an associated measureµ. Common choices forµ include the counting measure or Lebesgue measure. We make a mild assumption thatµ(Λ) < ∞. Based on the previous research [34], we make two simplifying assumptions. First, we assume that there is a total ordering on the rangeY, which allows us to compare two selected models based on their “performance measure”, denoted byq. Second, we assume that, for hyperparameter optimization purposes, we output the trained model, the hyperparameter, and the performance measure. Specifically, for any input datasetD and hyperparameterλ, the return value ofMλ is (x, q) ∼ Mλ(D), wherex represents the combination of the model parameters and the hyperparameter λ, andq is the (noisy) performance measure of the model. 42.2 Related Work In this section, we focus on related work concerning private HPO, while deferring the discussion on non-private HPO to Appendix F. Historically, research in DP machine learning has neglected the privacy cost associated with HPO [1, 41, 44]. It is only recently that researchers have begun to consider the honest HPO setting [30], in which the cost is taken into account. A direct approach to addressing this issue involves composition-based analysis. If each training run of a hyperparameter satisfies DP, the entire HPO procedure also complies with DP through composition across all attempted hyperparameter values. However, the challenge with this method is that the privacy guarantee derived from accounting can be excessively loose, scaling polynomially with the number of runs. Chaudhuri et al. [8] were the first to enhance the DP bounds for HPO by introducing additional stability assumptions on the learning algorithms. [26] made significant progress in enhancing DP bounds for HPO without relying on any stability properties of the learning algorithms. They proposed a simple procedure where a hyperparameter was randomly selected from a uniform distribution for each training run. This selection process was repeated a random number of times according to a geometric distribution, and the best model obtained from these runs was outputted. They showed that this procedure satisfied(3ε, 0)-DP as long as each training run of a hyperparameter was (ε, 0)-DP. Building upon this, [34] extended the procedure to accommodate negative binomial or Poisson distributions for the repeated uniform selection. They also offered more precise Rényi DP guarantees for this extended procedure. Furthermore, [9] explored a generalization of the procedure for top-k selection, considering (ε, δ)-DP guarantees. In a related context, [30] explored a setting that appeared superficially similar to ours, as their title mentioned “adaptivity.” However, their primary focus was on improving adaptive optimizers such as DP-Adam, which aimed to reduce the necessity of hyperparameter tuning, rather than the adaptive HPO discussed in this paper. Notably, in terms of privacy accounting, their approach only involved composing the privacy cost of each run without proposing any new method. Another relevant area of research is DP selection, which encompasses well-known methods such as the exponential mechanism [27] and the sparse vector technique [14], along with subsequent studies (e.g., [6] and [17]). However, this line of research always assumes the existence of a low- sensitivity score function for each candidate, which is an unrealistic assumption for hyperparameter optimization. 3 DP-HyPO: General Framework for Private Hyperparameter Op- timization The obvious approach to the problem of differentially private hyperparameter optimization would be to run each base algorithm and simply return the best one. However, running such an algorithm on large hyperparameter space is not feasible due to the privacy cost growing linearly in the worst case. While [26, 34] have successfully reduced the privacy cost for hyperparameter optimization from linear to constant, there are still two major drawbacks. First, none of the previous methods considers the case when the potential number of hyperparameter candidates is infinite, which is common in most hyperparameter optimization scenarios. In fact, we typically start with a range of hyperparameters that we are interested in, rather than a discrete set of candidates. Furthermore, prior methods are 5limited to the uniform sampling scheme over the hyperparameter domainΛ. In practice, this setting is unrealistic since we want to “adapt” the selection based on previous results. For instance, one could use Gaussian process to adaptively choose the next hyperparameter for evaluation, based on all the previous outputs. However, no adaptive hyperparameter optimization method has been proposed or analyzed under the DP constraint. In this paper, we bridge this gap by introducing the first DP adaptive hyperparameter optimization framework. 3.1 DP-HyPO Framework To achieve adaptive hyperparameter optimization with differential privacy, we propose the DP-HyPO framework. Our approach keeps an adaptive sampling distributionπ at each iteration that reflects accumulated information. Let Q(D, π) be the procedure that randomly draws a hyperparameterλ from the distribution1 π ∈ D(Λ) , and then returns the output fromMλ(D). We allow the sampling distribution to depend on both the dataset and previous outputs, and we denote asπ(j) the sampling distribution at thej-th iteration on datasetD. Similarly, the sampling distribution at thej-th iteration on the neighborhood dataset D′ is denoted asπ′(j). We now present the DP-HyPO framework, denoted asA(D, π(0), T , C, c), in Framework 1. The algorithm takes a prior distributionπ(0) ∈ D(Λ) as input, which reflects arbitrary prior knowledge about the hyperparameter space. Another input is the distributionT of the total repetitions of training runs. Importantly, we require it to be a random variable rather than a fixed number to preserve privacy. The last two inputs areC and c, which are upper and lower bounds of the density of any posterior sampling distributions. A finiteC and a positivec are required to bound the privacy cost of the entire framework. Framework 1DP-HyPO A(D, π(0), T , C, c) Initialize π(0), a prior distribution overΛ. Initialize the result setA = {} Draw T ∼ T for j = 0 to T − 1 do (x, q) ∼ Q(D, π(j)) A = A ∪ {(x, q)} Update π(j+1) based onA according to any adaptive algorithm such that for allλ ∈ Λ, c ≤ π(j+1)(λ) π(0)(λ) ≤ C Output (x, q) from A with the highestq Note that we intentionally leave the update rule forπ(j+1) unspecified in Framework 1 to reflect the fact that any adaptive update rule that leverages information from previous runs can be used. However, for a non-private adaptive HPO update rule, the requirement of bounded adaptive density c ≤ π(j+1)(λ) π(0)(λ) ≤ C may be easily violated. In Section 3.2, We provide a simple projection technique 1Here, D(Λ) represents the space of probability densities onΛ. 6to privatize any non-private update rules. In Section 4, we provide an instantiation of DP-HyPO using Gaussian process. We now state our main privacy results for this framework in terms of Rényi Differential Privacy (RDP) [29]. RDP is a privacy measure that is more general than the commonly used(ε, δ)-DP and provides tighter privacy bounds for composition. We defer its exact definition to Definition A.2 in the appendix. We note that different distributions of the number of selections (iterations),T , result in very different privacy guarantees. Here, we showcase the key idea for deriving the privacy guarantee of DP-HyPO framework by considering a special case whenT follows a truncated negative binomial distribution2 NegBin(θ, γ) (the same assumption as in [34]). In fact, as we show in the proof of Theorem 1 in Appendix A, the privacy bounds only depend onT directly through its probability generating function, and therefore one can adapt the proof to obtain the corresponding privacy guarantees for other probability families, for example, the Possion distribution considered in [34]. From here and on, unless otherwise specified, we will stick withT = NegBin(θ, γ) for simplicity. We also assume for simplicity that the prior distributionπ(0) is a uniform distribution overΛ. We provide more detailed discussion of handling informed prior other than uniform distribution in Appendix D. Theorem 1. Suppose thatT follows truncated negative Binomial distributionT ∼ NegBin(θ, γ). Let θ ∈ (−1, ∞), γ ∈ (0, 1), and 0 < c≤ C. Suppose for allMλ : Zn → Yover λ ∈ Λ, the base algorithms satisfy(α, ε)-RDP and(ˆα, ˆε)-RDP for someε, ˆε ≥ 0, α∈ (1, ∞), and ˆα ∈ [1, ∞). Then the DP-HyPO algorithmA(D, π(0), NegBin(θ, γ), C, c) satisfies (α, ε′)-RDP where ε′ = ε + (1 +θ) · \u0012 1 − 1 ˆα \u0013 ˆε + \u0012 α α − 1 + 1 +θ \u0013 log C c + (1 + θ) · log(1/γ) ˆα + log E[T] α − 1 . To prove Theorem 1, one of our main technical contributions is Lemma A.4, which quantifies the Rényi divergence of the sampling distribution at each iteration between the neighboring datasets. We then leverage this crucial result and the probability generating function ofT to bound the Rényi divergence in the output ofA. We defer the detailed proof to Appendix A. Next, we present the case with pure DP guarantees. Recall the fact that(ε, 0)-DP is equivalent to (∞, ε)-RDP [29]. When bothα and ˆα tend towards infinity, we easily obtain the following theorem in terms of(ε, 0)-DP. Theorem 2. Suppose thatT follows truncated negative Binomial distributionT ∼ NegBin(θ, γ). Let θ ∈ (−1, ∞) and γ ∈ (0, 1). If all the base algorithmsMλ satisfies (ε, 0)-DP, then the DP-HyPO algorithm A(D, π(0), NegBin(θ, γ), C, c) satisfies \u0000 (2 + θ) \u0000 ε + log C c \u0001 , 0 \u0001 -DP. Theorem 1 and Theorem 2 provide practitioners the freedom to trade off between allocating more DP budget to enhance the base algorithm or to improve adaptivity. In particular, a higher value ofC c signifies greater adaptivity, while a largerε improves the performance of base algorithms. 3.1.1 Uniform Optimization Method as a Special Case We present the uniform hyperparameter optimization method [34, 25] in Algorithm 2, which is a special case of our general DP-HyPO Framework withC = c = 1. Essentially, this algorithm never updates the sampling distributionπ. 2Truncated negative binomial distribution is a direct generalization of the geometric distribution. See Appendix B for its definition. 7Algorithm 2Uniform Hyperparameter OptimizationU(D, θ, γ,Λ) Let π = Unif({1, ...,|Λ|}), andA = {} Draw T ∼ NegBin(θ, γ) for j = 0 to T − 1 do (x, q) ∼ Q(D, π) A = A ∪ {(x, q)} Output (x, q) from A with the highestq Our results in Theorem 1 and Theorem 2 generalize the main technical results of [34, 26]. Specifically, whenC = c = 1 and Λ is a finite discrete set, our Theorem 1 precisely recovers Theorem 2 in [34]. Furthermore, when we setθ = 1, the truncated negative binomial distribution reduces to the geometric distribution, and our Theorem 2 recovers Theorem 3.2 in [26] . 3.2 Practical Recipe to Privatize HPO Algorithms In the DP-HyPO framework, we begin with a prior and adaptively update it based on the accumulated information. However, for privacy purposes, we require the densityπ(j) to be bounded by some constants c and C, which is due to the potential privacy leakage when updatingπ(j) based on the history. It is crucial to note that this distributionπ(j) can be significantly different from the distribution π′(j) if we were given a different input datasetD′. Therefore, we require the probability mass/density function to satisfy c µ(Λ) ≤ π(j)(λ) ≤ C µ(Λ) for allλ ∈ Λ to control the privacy loss due to adaptivity. This requirement is not automatically satisfied and typically necessitates modifications to current non-private HPO methods. To address this challenge, we propose a general recipe to modify any non-private method. The idea is quite straightforward: throughout the algorithm, we maintain a non-private version of the distribution densityπ(j). When sampling from the spaceΛ, we perform a projection from π(j) to the space consisting of bounded densities. Specifically, we define the space of essentially bounded density functions bySC,c = {f ∈ ΛR+ : ess supf ≤ C µ(Λ), ess inff ≥ c µ(Λ), R α∈Λ f(α)dα = 1}. For such a space to be non-empty, we require thatc ≤ 1 ≤ C, where µ is the measure onΛ. This condition is well-defined as we assumeµ(Λ) < ∞. To privatizeπ(j) at thej-th iteration, we project it into the spaceSC,c, by solving the following convex functional programming problem: min f ∥f − π(j)∥2, s.t. f ∈ SC,c. (3.1) Note that this is a convex program sinceSC,c is convex and closed. We denote the output from this optimization problem byPSC,c(π(j)). Theoretically, problem(3.1) allows the hyperparameter space Λ to be general measurable space with arbitrary topological structure. However, empirically, practitioners need to discretizeΛ to some extent to make the convex optimization computationally feasible. Compared to the previous work, our formulation provides the most general characterization of the problem and allows pratitioners toadaptively and iteratively choose a proper discretization as needed. Framework 1 tolerates a much finer level of discretization than the previous method, as the performance of latter degrades fast when the number of candidates increases. We also provide 8examples using CVX to solve this problem in Section 4.2. In Appendix C, we discuss about its practical implementation, and the connection to information projection. 4 Application: DP-HyPO with Gaussian Process In this section, we provide an instantiation of DP-HyPO using Gaussian process (GP) [40]. GPs are popular non-parametric Bayesian models frequently employed for hyperparameter optimization. At the meta-level, GPs are trained to generate surrogate models by establishing a probability distribution over the performance measureq. While traditional GP implementations are not private, we leverage the approach introduced in Section 3.2 to design a private version that adheres to the bounded density contraint. We provide the algorithmic description in Section 4.1 and the empircal evaluation in Section 4.2. 4.1 Algorithm Description The following Algorithm (AGP) is a private version of Gaussian process for hyperparameter tuning. In Algorithm 3, we utilize GP to construct a surrogate model that generates probability distributions Algorithm 3DP-HyPO with Gaussian processAGP(D, θ, γ, τ, β,Λ, C, c) Initialize π(0) = Unif(Λ), andA = {} Draw T ∼ NegBin(θ, γ) for t = 0 to T − 1 do Truncate the density of currentπ(t) to be bounded into the range of[c, C] by projecting to SC,c. ˜π(t) = PSC,c(π(t)). Sample (x, q) ∼ Q(D, ˜π(j)), and updateA = A ∪ {(x, q)} Update mean estimation and variance estimation of the Gaussian processµλ, σ2 λ, and get the score assλ = µλ + τσλ. Update true (untruncated) posteriorπ(t+1) with softmax, byπ(t+1)(λ) = exp(β·sλ)R λ′∈Λ exp(β·s′ λ). Output (x, q) from A with the highestq for the performance measureq. By estimating the mean and variance, we assign a “score” to each hyperparameter λ, known as the estimated upper confidence bound (UCB). The weight factorτ controls the balance between exploration and exploitation, where larger weights prioritize exploration by assigning higher scores to hyperparameters with greater uncertainty. To transform these scores into a sampling distribution, we apply the softmax function across all hyperparameters, incorporating the parameterβ as the inverse temperature. A higher value ofβ signifies increased confidence in the learned scores for each hyperparameter. 4.2 Empirical Evaluations We now evaluate the performance of our GP-based DP-HyPO (referred to as “GP”) in various settings. Since DP-HyPO is the first adaptive private hyperparameter optimization method of its kind, we compare it to the special case of Uniform DP-HyPO (Algorithm 2), referred to as 9“Uniform”, as proposed in [26, 34]. In this demonstration, we consider two pragmatic privacy configurations: the white-box setting and the black-box setting, contingent on whether adaptive HPO algorithms incur extra privacy cost. In the white-box scenario (Section 4.2.1 and 4.2.2), we conduct experiments involving training deep learning models on both the MNIST dataset and CIFAR-10 dataset. Conversely, when considering the black-box setting (Section 4.2.3), our attention shifts to a real-world Federated Learning (FL) task from the industry. These scenarios provide meaningful insights into the effectiveness and applicability of our GP-based DP-HyPO approach. 4.2.1 MNIST Simulation We begin with the white-box scenario, in which the data curator aims to provide overall protection to the published model. In this context, to accommodate adaptive HPO algorithms, it becomes necessary to reduce the budget allocated to the base algorithm. In this section, we consider the MNIST dataset, where we employ DP-SGD to train a standard CNN. The base algorithms in this case are different DP-SGD models with varying hyperparameters, and we evaluate each base algorithm based on its accuracy. Our objective is to identify the best hyperparameters that produce the most optimal model within a given total privacy budget. Specifically, we consider two variable hyperparameters: the learning rateη and clipping normR, while keeping the other parameters fixed. We ensure that both the GP algorithm and the Uniform algorithm operate under the same total privacy budget, guaranteeing a fair comparison. Due to constraints on computational resources, we conduct a semi-real simulation using the MNIST dataset. For both base algorithms (with different noise multipliers), we cache the mean accuracy of5 independently trained models for each discretized hyperparameter and treat that as a proxy for the “actual accuracy” of the hyperparameter. Each time we sample the accuracy of a hyperparameter, we add a Gaussian noise with a standard deviation of0.1 to the cached mean. We evaluate the performance of the output model based on the “actual accuracy” corresponding to the selected hyperparameter. Further details on the simulation and parameter configuration can be found in Appendix E.1. In the left panel of Figure 1, we demonstrated the comparison of performance of the Uniform and GP methods with total privacy budgetε = 153 and δ = 1e − 5. The accuracy reported is the actual accuracy of the output hyperparameter. From the figure, we see that whenT is very small(T <8), GP method is slightly worse than Uniform method as GP spendslog(C/c) budget less than Uniform method for each base algorithm (the cost of adaptivity). However, we see that after a short period of exploration, GP consistently outperform Uniform, mostly due to the power of being adaptive. The superiority of GP is further demonstrated in Table 1, aggregating over geometric distribution. 4.2.2 CIFAR-10 Simulation When examining the results from MNIST, a legitimate critique arises: our DP-Hypo exhibits only marginal superiority over its uniform counterpart, which questions the assertion that adaptivity holds significant value. Our conjecture is that the hyperparameter landscape of MNIST is relatively uncomplicated, which limits the potential benefits of adaptive algorithms. 3The ε values are seemingly very large. Nonetheless, the reported privacy budget encompasses the overall cost of the entire HPO, which is typically overlooked in the existing literature. Given that HPO roughly incurs three times the privacy cost of the base algorithm, anε as high as15 could be reported as only5 in many other works. 10Figure 1: Left: The accuracy of the output hyperparameter in MNIST semi-real simulation, with ε = 15, δ = 0.00001. Middle: The accuracy of the output hyperparameter in CIFAR-10, with ε = 12, δ = 0.00001. Right: The loss of the output hyperparameter in FL. Error bars stands for95% confidence. Curves for GP are calculated by averaging400 independent runs, and curves for Uniform are calculated by averaging10000 independent runs. For a clearer demonstration, we compare the performance for each fixed value ofT, and recognize that the actual performance is a weighted average across different values ofT. To test the hypothesis, we conduct experiments on the CIFAR-10 dataset, with a setup closely mirroring the previous experiment: we employ the same CNN model for training, and optimize the same set of hyperparameters, which are the learning rateη and clipping normR. The primary difference lies in how we generate the hyperparameter landscape. Given that a single run on CIFAR-10 is considerably more time-consuming than on MNIST, conducting multiple runs for every hyperparameter combination is unfeasible. To address this challenge, we leverage BoTorch [3], an open-sourced library for HPO, to generate the landscape. Since we operate in the white-box setting, where the base algorithms have distinct privacy budgets for the uniform and adaptive scenarios, we execute 50 runs and generate the landscape for each case, including the mean (µλ) and standard error (σλ) of accuracy for each hyperparameter combinationλ. When the algorithm (GP or Uniform) visits a specificλ, our oracle returns a noisy scoreq(λ) drawn from a normal distribution of N(µλ, σλ). A more detailed description of our landscapes and parameter configuration can be found in Appendix E.2. In the middle of Figure 1, we showcase a performance comparison between the Uniform and GP methods with a total privacy budget ofε = 12 and δ = 1e − 5. Clearly, GP consistently outperforms the Uniform method, with the largest performance gap occurring when the number of runs is around 10. 4.2.3 Federated Learning In this section, we move to the black-box setting, where the privacy budget allocated to the base algorithm remains fixed, while we allow extra privacy budget for HPO. That being said, the adaptivity can be achieved without compromising the utility of the base algorithm. We explore another real-world scenario: a Federated Learning (FL) task conducted on a propri- etary dataset4 from industry. Our aim is to determine the optimal learning rates for the central server (using AdaGrad) and the individual users (using SGD). To simulate this scenario, we once again rely on the landscape generated by BoTorch [3], as shown in Figure 3 in Appendix E.3. 4We have to respect confidentiality constraints that limit our ability to provide extensive details about this dataset. 11Under the assumption that base algorithms are black-box models with fixed privacy costs, we proceed with HPO while varying the degree of adaptivity. The experiment results are visualized in the right panel of Figure 1, and Table 2 presents the aggregated performance data. We consistently observe that GP outperforms Uniform in the black-box setting. Furthermore, our findings suggest that allocating a larger privacy budget to the GP method facilitates the acquisition of adaptive information, resulting in improved performance in HPO. This highlights the flexibility of GP in utilizing privacy resources effectively. Geometric(γ) 0.001 0.002 0.003 0.005 0.01 0.02 0.025 0.03 GP 0.946 0.948 0.948 0.947 0.943 0.937 0.934 0.932 Uniform 0.943 0.945 0.945 0.944 0.940 0.935 0.932 0.929 Table 1:Accuracy of MNIST using Geometric Distribution with various different values ofγ for Uniform and GP methods. Each number is the mean of200 runs. Geometric(γ) 0.001 0.002 0.003 0.005 0.01 0.02 0.025 0.03 GP (C = 1.25) 0.00853 0.0088 0.00906 0.00958 0.0108 0.0129 0.0138 0.0146 GP (C = 1.33) 0.00821 0.00847 0.00872 0.00921 0.0104 0.0123 0.0132 0.0140 GP (C = 1.5) 0.00822 0.00848 0.00872 0.00920 0.0103 0.0123 0.0131 0.0130 Uniform 0.0104 0.0106 0.0109 0.0113 0.0123 0.0141 0.0149 0.0156 Table 2:Loss of FL using Geometric Distribution with various different values ofγ for Uniform and GP methods with different choice ofC and c = 1/C. Each number is the mean of200 runs. 5 Conclusion In conclusion, this paper presents a novel framework, DP-HyPO. As the first adaptive HPO framework with sharp DP guarantees, DP-HyPO effectively bridges the gap between private and non-private HPO. Our work encompasses the random search method by [26, 34] as a special case, while also granting practitioners the ability to adaptively learn better sampling distributions based on previous runs. Importantly, DP-HyPO enables the conversion of any non-private adaptive HPO algorithm into a private one. Our framework proves to be a powerful tool for professionals seeking optimal model performance and robust DP guarantees. The DP-HyPO framework presents two interesting future directions. One prospect involves an alternative HPO specification which is practically more favorable. Considering the extensive literature on HPO, there is a significant potential to improve the empirical performance by leveraging more advanced HPO methods. Secondly, there is an interest in establishing a theoretical utility guarantee for DP-HyPO. By leveraging similar proof methodologies to those in Theorem 3.3 in [26], it is feasible to provide basic utility guarantees for the general DP-HyPO, or for some specific configurations within DP-HyPO. 126 Acknowledgements The authors would like to thank Max Balandat for his thoughtful comments and insights that helped us improve the paper. References [1] Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep learning with differential privacy. InProceedings of the 2016 ACM SIGSAC conference on computer and communications security, pages 308–318, 2016. [2] Martin S Andersen, Joachim Dahl, Lieven Vandenberghe, et al. Cvxopt: A python package for convex optimization.Available at cvxopt. org, 54, 2013. [3] Maximilian Balandat, Brian Karrer, Daniel Jiang, Samuel Daulton, Ben Letham, Andrew G Wilson, and Eytan Bakshy. Botorch: A framework for efficient monte-carlo bayesian optimization. Advances in neural information processing systems, 33:21524–21538, 2020. [4] Raef Bassily, Adam Smith, and Abhradeep Thakurta. Private empirical risk minimization: Efficient algorithms and tight error bounds. In2014 IEEE 55th annual symposium on foundations of computer science, pages 464–473. IEEE, 2014. [5] James Bergstra, Rémi Bardenet, Yoshua Bengio, and Balázs Kégl. Algorithms for hyper- parameter optimization. Advances in neural information processing systems, 24, 2011. [6] Mark Bun, Gautam Kamath, Thomas Steinke, and Steven Z Wu. Private hypothesis selection. Advances in Neural Information Processing Systems, 32, 2019. [7] Nicholas Carlini, Chang Liu, Úlfar Erlingsson, Jernej Kos, and Dawn Song. The secret sharer: Evaluating and testing unintended memorization in neural networks. InUSENIX Security Symposium, volume 267, 2019. [8] Kamalika Chaudhuri and Staal A Vinterbo. A stability-based validation procedure for differ- entially private machine learning.Advances in Neural Information Processing Systems, 26, 2013. [9] Edith Cohen, Xin Lyu, Jelani Nelson, Tamás Sarlós, and Uri Stemmer. Generalized private selection and testing with high confidence.arXiv preprint arXiv:2211.12063, 2022. [10] Imre Csiszár and Frantisek Matus. Information projections revisited.IEEE Transactions on Information Theory, 49(6):1474–1490, 2003. [11] Soham De, Leonard Berrada, Jamie Hayes, Samuel L Smith, and Borja Balle. Unlock- ing high-accuracy differentially private image classification through scale. arXiv preprint arXiv:2204.13650, 2022. [12] Jinshuo Dong, Aaron Roth, and Weijie J Su. Gaussian differential privacy.Journal of the Royal Statistical Society Series B: Statistical Methodology, 84(1):3–37, 2022. 13[13] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. InTheory of Cryptography: Third Theory of Cryptography Conference, TCC 2006, New York, NY, USA, March 4-7, 2006. Proceedings 3, pages 265–284. Springer, 2006. [14] Cynthia Dwork, Moni Naor, Omer Reingold, Guy N Rothblum, and Salil Vadhan. On the complexity of differentially private data release: efficient algorithms and hardness results. In Proceedings of the forty-first annual ACM symposium on Theory of computing, pages 381–390, 2009. [15] Matthias Feurer and Frank Hutter. Hyperparameter optimization.Automated machine learning: Methods, systems, challenges, pages 3–33, 2019. [16] Yonatan Geifman and Ran El-Yaniv. Deep active learning with a neural architecture search. Advances in Neural Information Processing Systems, 32, 2019. [17] Sivakanth Gopi, Gautam Kamath, Janardhan Kulkarni, Aleksandar Nikolov, Zhiwei Steven Wu, and Huanyu Zhang. Locally private hypothesis selection. InConference on Learning Theory, pages 1785–1816. PMLR, 2020. [18] Xin He, Kaiyong Zhao, and Xiaowen Chu. Automl: A survey of the state-of-the-art.Knowledge- Based Systems, 212:106622, 2021. [19] Andrew Hundt, Varun Jain, and Gregory D Hager. sharpdarts: Faster and more accurate differentiable architecture search.arXiv preprint arXiv:1903.09900, 2019. [20] Frank Hutter, Holger H Hoos, and Kevin Leyton-Brown. Sequential model-based optimization for general algorithm configuration. InLearning and Intelligent Optimization: 5th International Conference, LION 5, Rome, Italy, January 17-21, 2011. Selected Papers 5, pages 507–523. Springer, 2011. [21] Peter Kairouz, Sewoong Oh, and Pramod Viswanath. The composition theorem for differential privacy. InInternational conference on machine learning, pages 1376–1385. PMLR, 2015. [22] Kirthevasan Kandasamy, Willie Neiswanger, Jeff Schneider, Barnabas Poczos, and Eric P Xing. Neural architecture search with bayesian optimisation and optimal transport.Advances in neural information processing systems, 31, 2018. [23] Rajiv Khanna, Joydeep Ghosh, Rusell Poldrack, and Oluwasanmi Koyejo. Information projection and approximate inference for structured sparse variables. InArtificial Intelligence and Statistics, pages 1358–1366. PMLR, 2017. [24] Liam Li and Ameet Talwalkar. Random search and reproducibility for neural architecture search. InUncertainty in artificial intelligence, pages 367–377. PMLR, 2020. [25] Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. Hyper- band: A novel bandit-based approach to hyperparameter optimization.The Journal of Machine Learning Research, 18(1):6765–6816, 2017. [26] Jingcheng Liu and Kunal Talwar. Private selection from private candidates. InProceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing, pages 298–309, 2019. 14[27] Frank McSherry and Kunal Talwar. Mechanism design via differential privacy. In48th Annual IEEE Symposium on Foundations of Computer Science (FOCS’07), pages 94–103. IEEE, 2007. [28] Hector Mendoza, Aaron Klein, Matthias Feurer, Jost Tobias Springenberg, and Frank Hutter. Towards automatically-tuned neural networks. InWorkshop on automatic machine learning, pages 58–65. PMLR, 2016. [29] Ilya Mironov. Rényi differential privacy. In2017 IEEE 30th computer security foundations symposium (CSF), pages 263–275. IEEE, 2017. [30] Shubhankar Mohapatra, Sajin Sasy, Xi He, Gautam Kamath, and Om Thakkar. The role of adaptive optimizers for honest private hyperparameter selection. InProceedings of the aaai conference on artificial intelligence, volume 36, pages 7806–7813, 2022. [31] Milad Nasr, Reza Shokri, and Amir Houmansadr. Comprehensive privacy analysis of deep learning: Passive and active white-box inference attacks against centralized and federated learning. In 2019 IEEE symposium on security and privacy (SP), pages 739–753. IEEE, 2019. [32] Renato Negrinho, Matthew Gormley, Geoffrey J Gordon, Darshan Patil, Nghia Le, and Daniel Ferreira. Towards modular and programmable architecture search.Advances in neural informa- tion processing systems, 32, 2019. [33] Ashwinee Panda, Xinyu Tang, Vikash Sehwag, Saeed Mahloujifar, and Prateek Mittal. Dp-raft: A differentially private recipe for accelerated fine-tuning.arXiv preprint arXiv:2212.04486, 2022. [34] Nicolas Papernot and Thomas Steinke. Hyperparameter tuning with renyi differential privacy. In International Conference on Learning Representations, 2021. [35] Carl Edward Rasmussen. Gaussian processes in machine learning. In Advanced Lectures on Machine Learning: ML Summer Schools 2003, Canberra, Australia, February 2-14, 2003, Tübingen, Germany, August 4-16, 2003, Revised Lectures, pages 63–71. Springer, 2004. [36] Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P Adams, and Nando De Freitas. Taking the human out of the loop: A review of bayesian optimization.Proceedings of the IEEE, 104(1):148–175, 2015. [37] Shuang Song, Kamalika Chaudhuri, and Anand D Sarwate. Stochastic gradient descent with differentially private updates. In 2013 IEEE global conference on signal and information processing, pages 245–248. IEEE, 2013. [38] Salil Vadhan. The complexity of differential privacy.Tutorials on the Foundations of Cryptogra- phy: Dedicated to Oded Goldreich, pages 347–450, 2017. [39] Hua Wang, Sheng Gao, Huanyu Zhang, Milan Shen, and Weijie J Su. Analytical composition of differential privacy via the edgeworth accountant.arXiv preprint arXiv:2206.04236, 2022. [40] Christopher KI Williams and Carl Edward Rasmussen.Gaussian processes for machine learning, volume 2. MIT press Cambridge, MA, 2006. 15[41] Da Yu, Huishuai Zhang, Wei Chen, Jian Yin, and Tie-Yan Liu. Large scale private learning via low-rank reparametrization. In International Conference on Machine Learning, pages 12208–12218. PMLR, 2021. [42] Tong Yu and Hong Zhu. Hyper-parameter optimization: A review of algorithms and applications. arXiv preprint arXiv:2003.05689, 2020. [43] Arber Zela, Aaron Klein, Stefan Falkner, and Frank Hutter. Towards automated deep learning: Efficient joint neural architecture and hyperparameter search.arXiv preprint arXiv:1807.06906, 2018. [44] Huanyu Zhang, Ilya Mironov, and Meisam Hejazinia. Wide network learning with differential privacy. arXiv preprint arXiv:2103.01294, 2021. 16A Proofs of the technical results A.1 Proof of Main Results First, we define Rényi divergence as follows. Definition A.1(Rényi Divergences). Let P and Q be probability distributions on a common space Ω. Assume thatP is absolutely continuous with respect toQ - i.e., for all measurableE ⊂ Ω, if Q(E) = 0, thenP(E) = 0. Let P(x) and Q(x) denote the densities ofP and Q respectively. The KL divergence fromP to Q is defined as D1(P∥Q) := E X←P \u0014 log \u0012P(X) Q(X) \u0013\u0015 = Z Ω P(x) log \u0012P(x) Q(x) \u0013 dx. The max divergence fromP to Q is defined as D∞(P∥Q) := sup \u001a log \u0012P(E) Q(E) \u0013 : P(E) > 0 \u001b . For α ∈ (1, ∞), the Rényi divergence fromP to Q of orderα is defined as Dα(P∥Q) := 1 α − 1 log   E X←P \"\u0012P(X) Q(X) \u0013α−1#! = 1 α − 1 log \u0012 E X←Q \u0014\u0012P(X) Q(X) \u0013α\u0015\u0013 = 1 α − 1 log \u0012Z Q P(x)αQ(x)1−αdx \u0013 . We now present the definition of Rényi DP (RDP) in [29]. Definition A.2(Rényi Differential Privacy). A randomized algorithmM : Xn → Yis (α, ε)-Rényi differentially private if, for all neighbouring pairs of inputsD, D′ ∈ Xn, Dα (M(x)∥M (x′)) ≤ ε. We define some additional notations for the sake of the proofs. In algorithm 1, for any1 ≤ j ≤ T, and neighboring datasetD and D′, we define the following notations for anyy = (x, q) ∈ Y, the totally ordered range set. Pj(y) = P˜y∼Q(D,π(j))(˜y = y) and P′ j(y) = P˜y∼Q(D′,π′(j))(˜y = y) Pj(≤ y) = P˜y∼Q(D,π(j))(˜y ≤ y) and P′ j(≤ y) = P˜y∼Q(D′,π′(j))(˜y ≤ y) Pj(< y) = P˜y∼Q(D,π(j))(˜y < y) and P′ j(< y) = P˜y∼Q(D′,π′(j))(˜y < y). By these definitions, we havePj(≤ y) = Pj(< y) + Pj(y), andP′ j(≤ y) = P′ j(< y) + P′ j(y). And additionally, we have Pj(y) P′ j(y) = R λ∈Λ P(Mλ(D) = y)π(j)(λ)dλR λ∈Λ P(Mλ(D′) = y)π′(j)(λ)dλ ≤ sup λ∈Λ P(Mλ(D) = y)π(j)(λ) P(Mλ(D′) = y)π′(j)(λ) ≤ C c · sup λ∈Λ P(Mλ(D) = y) P(Mλ(D′) = y). (A.1) 17Here, the first inequality follows from the simple property of integration, and the second inequality follows from the fact thatπ(j) has bounded density betweenc and C. Similarly, we have Pj(≤ y) P′ j(≤ y) ≤ C c · sup λ∈Λ P(Mλ(D) ≤ y) P(Mλ(D′) ≤ y), (A.2) and Pj(< y) P′ j(< y) ≤ C c · sup λ∈Λ P(Mλ(D) < y) P(Mλ(D′) < y). (A.3) Note thatD and D′ are neighboring datasets, andMλ satisfies some DP guarantees. So the ratio P(Mλ(D)∈E) P(Mλ(D′)∈E) for any eventE can be bounded. For simplicity, we define the inner product of a distribution π with the vector M(D) = (P(Mλ(D) = y) : λ ∈ Λ) as π · M(D) := Z λ∈Λ P(Mλ(D) = y)π(λ)dλ. (A.4) Now, we define additional notations to bound the probabilities. RecallSC,s is given by{f ∈ ΛR+ : ess supf ≤ C, ess inff ≥ c, R α∈Λ f(α)dα = 1.}. It is straightforward to see this is a compact set as it is the intersection of three compact sets. We define P+(y) := sup π∈SC,c Z λ∈Λ P(Mλ(D) = y)π(j)(λ)dλ = π+ · M(D), (A.5) where π+ is the distribution that achieves the supreme in the compact setSC,c. Similarly, we define P′−(y) for D′ as given by P′−(y) := inf π∈SC,c Z λ∈Λ P(Mλ(D′) = y) · π′(j)(λ)dλ = π′− · M. (A.6) Similarly, we can defineP′+(y) and P−(y) accordingly. From the definition, we know that P−(y) ≤ Pj(y) ≤ P+(y) and P′−(y) ≤ P′ j(y) ≤ P′+(y). (A.7) We also have P+(y) P′−(y) = π∗ · M(D) π′− · M(D′) ≤ sup λ P(Mλ(D) = y) P(Mλ(D′) = y) · C c . (A.8) It is similar to define P+(≤ y) := sup π∈SC,c Z λ∈Λ P(Mλ(D) ≤ y) and P′+(≤ y) := sup π∈SC,c Z λ∈Λ P(Mλ(D′) ≤ y) P−(≤ y) := inf π∈SC,c Z λ∈Λ P(Mλ(D) ≤ y) and P′−(≤ y) := inf π∈SC,c Z λ∈Λ P(Mλ(D′) ≤ y) P+(< y) := sup π∈SC,c Z λ∈Λ P(Mλ(D) < y) and P′+(< y) := sup π∈SC,c Z λ∈Λ P(Mλ(D′) < y) 18P−(< y) := inf π∈SC,c Z λ∈Λ P(Mλ(D) < y) and P′−(< y) := inf π∈SC,c Z λ∈Λ P(Mλ(D′) < y). Following the exact same proof, we have P−(≤ y) ≤ Pj(≤ y) ≤ P+(≤ y) and P′−(≤ y) ≤ P′ j(≤ y) ≤ P′+(≤ y) (A.9) P−(< y) ≤ Pj(< y) ≤ P+(< y) and P′−(< y) ≤ P′ j(< y) ≤ P′+(< y) (A.10) P+(≤ y) P′−(≤ y) ≤ sup λ P(Mλ(D) ≤ y) P(Mλ(D′) ≤ y) · C c and P+(< y) P′−(< y) ≤ sup λ P(Mλ(D) < y) P(Mλ(D′) < y) · C c . (A.11) It is also straightforward to verify from the definition that P+(≤ y) = P+(< y) + P+(y) and P′+(≤ y) = P′+(< y) + P′+(y) (A.12) P+ − (≤ y) = P−(< y) + P−(y) and P′−(≤ y) = P′−(< y) + P′−(y). (A.13) Lemma A.3.Suppose ifaλ, bλ are non-negative andcλ, c′ λ are positive for allλ. Then we have P λ aλcλP λ bλc′ λ ≤ P λ aλP λ bλ · sup λ,λ′ \f\f\f\f cλ c′ λ \f\f\f\f. Proof of Lemma A.3.This lemma is pretty straight forward by comparing the coefficient for each term in the full expansion. Specifically, we re-write the inequality as X λ aλcλ X λ′ b′ λ ≤ X λ aλ X λ′ b′ λc′ λ · sup λ,λ′ \f\f\f\f cλ c′ λ \f\f\f\f. (A.14) For each termaλb′ λ, its coefficient on the left hand side of(A.14) is cλ, but its coefficient on the right hand side of(A.14) is c′ λ · supλ,λ′ \f\f\fcλ c′ λ \f\f\f. Since we always havec′ λ · supλ,λ′ \f\f\fcλ c′ λ \f\f\f ≥ cλ, andaλb′ λ ≥ 0, we know the inequality (A.14) holds. Next, in order to present our results in terms of RDP guarantees, we prove the following lemma. Lemma A.4.The Rényi divergence betweenP+ and P− is be bounded as follows: Dα(P+∥P′−) ≤ α α − 1 log C c + sup λ∈Λ Dα \u0000 Mλ(D)∥Mλ(D′) \u0001 Proof of Lemma A.4.We write that e(α−1)Dα(P+∥P′−) = X y∈Y P+(y)α · P′−(y)1−α = X y∈Y (P λ π+(λ)P(Mλ(D) = y))α (P λ π′−(λ)P(Mλ(D′) = y))α−1 (A.15) Here, π+ and π′− are defined in(A.5) and (A.6), so they are essentiallyπ+ y and π′− y as they depend on the value ofy. Therefore, we need to “remove” this dependence ony to leverage the RDP guarantees for each base algorithmMλ. We accomplish this task by bridging viaπ, the uniform 19density onΛ (that isπ(λ) = π(λ′) for anyλ, λ′ ∈ Λ). Specifically, we defineaλ = π(λ)P(Mλ(D) = y), bλ = π(λ)P(Mλ(D′) = y), cλ = π+ y (λ) π(λ) , andc′ λ = π′− y (λ) π(λ) . We see that sup λ,λ′ \f\f\f\f cλ c′ λ \f\f\f\f = sup λ,λ′ \f\f\f\f\f π+ y (λ)/π(λ) π′−y (λ′)/π(λ′) \f\f\f\f\f = sup λ,λ′ \f\f\f\f\f π+ y (λ)) π′−y (λ′) \f\f\f\f\f ≤ C/c, (A.16) since π is the uniform, andπ+ y and π′− y belongs toSC,c. We now apply Lemma A.3 with the above notations for eachy to (A.15), and we have X y∈Y (P λ π+(λ)P(Mλ(D) = y))α (P λ π′−(λ)P(Mλ(D′) = y))α−1 = X y∈Y \u0010P λ π(λ)P(Mλ(D) = y) · π+(λ) π(λ) \u0011α−1 \u0010P λ π(λ)P(Mλ(D) = y) · π+(λ) π(λ) \u0011 \u0010P λ π(λ)P(Mλ(D′) = y) · π′−(λ) π(λ) \u0011α−1 = X y∈Y (P λ aλ · cλ)α−1 \u0010P λ π(λ)P(Mλ(D) = y) · π+(λ) π(λ) \u0011 (P λ bλ · c′ λ)α−1 ≤ X y∈Y sup λ,λ′ \f\f\f\f cλ c′ λ \f\f\f\f α−1 (P λ aλ)α−1 \u0010P λ π(λ)P(Mλ(D) = y) · π+(λ) π(λ) \u0011 (P λ bλ)α−1 = X y∈Y sup λ,λ′ \f\f\f\f cλ c′ λ \f\f\f\f α−1 (P λ aλ)α−1 (P λ aλ · cλ) (P λ bλ)α−1 ≤ X y∈Y sup λ,λ′ \f\f\f\f cλ c′ λ \f\f\f\f α−1 (P λ aλ)α−1 (P λ aλ) · supλ cλ (P λ bλ)α−1 ≤ X y∈Y \u0012C c \u0013α−1 (P λ aλ)α−1 (P λ aλ) · \u0000C c \u0001 (P λ bλ)α−1 = X y∈Y \u0012C c \u0013α · (P λ π(λ)P(Mλ(D) = y))α (P λ π(λ)P(Mλ(D′) = y))α−1 The first inequality is due to Lemma A.3, the second inequality is becauseaλ are non-negative, and the last inequality is because of(A.16) and the fact that bothπ+(λ) and π(λ) are defined inSC,c, and thus their ratio is upper bounded byC c for anyλ. Now we only need to prove that for any fixed distributionπ that doesn’t depend on valuey, we have X y∈Y (P λ π(λ)P(Mλ(D) = y))α (P λ π(λ)P(Mλ(D′) = y))α−1 ≤ sup λ∈Λ e(α−1)Dα(Mλ(D)∥Mλ(D′)). (A.17) With this result, we immediately know the result holds for uniform distributionπ as a special case. To prove this result, we first observe that the functionf(u, v) = uαv1−α is a convex function. This 20is because the Hessian off is \u0012α(α − 1)uα−2v1−α −α(α − 1)uα−1v−α −α(α − 1)uα−1v−α α(α − 1)uαv−α−1 \u0013 , which is easy to see to be positive semi-definite. And now, consider any distributionπ, denote u(λ) = P(Mλ(D) = y) and v(λ) = P(Mλ(D′) = y) by Jensen’s inequality, we have f( X λ π(λ)u(λ), X λ π(λ)v(λ)) ≤ X λ π(λ)f(u(λ), v(λ)). By adding the summation overy on both side of the above inequality, we have X y∈Y (P λ π(λ)P(Mλ(D) = y))α (P λ π(λ)P(Mλ(D′) = y))α−1 ≤ X y∈Y X λ π(λ) P(Mλ(D) = y)α P(Mλ(D′) = y)α−1 = X λ X y∈Y π(λ) P(Mλ(D) = y)α P(Mλ(D′) = y)α−1 ≤ sup λ X y∈Y P(Mλ(D) = y)α P(Mλ(D′) = y)α−1 . The first equality is due to Fubini’s theorem. And the second inequality is straight forward as one observe π(λ) only depends onλ. This concludes the proof as we know that e(α−1)Dα(P+∥P′−) ≤ \u0012C c \u0013α sup λ X y∈Y P(Mλ(D) = y)α P(Mλ(D′) = y)α−1 = \u0012C c \u0013α sup λ e(α−1)Dα(Mλ(D)∥Mλ(D′) or equivalently, Dα(P+∥P′−) ≤ α α − 1 log C c + sup λ∈Λ Dα \u0000 Mλ(D)∥Mλ(D′) \u0001 . We now present our crucial technical lemma for adaptive hyperparameter tuing with any distribution on the number of repetitionsT. This is a generalization from [34]. Lemma A.5.Fix α >1. LetT be a random variable supported onN≥0. Letf : [0, 1] → R be the probability generating function ofK, that is,f(x) = P∞ k=0 P[T = k]xk. Let Mλ and M′ λ be the base algorithm forλ ∈ Λ on Y on D and D′ respectively. Define A1 := A(D, π(0), T , C, c), andA2 := A(D′, π(0), T , C, c). Then Dα (A1∥A2) ≤ sup λ Dα \u0000 Mλ∥M′ λ \u0001 + α α − 1 log C c + 1 α − 1 log \u0010 f′(q)α · f′ \u0000 q′\u00011−α\u0011 , where applying the same postprocessing to the bounding probabilitiesP+ and P′− gives probabilitiesq and q′ respectively. This means that, there exist a function setg : Y →[0, 1] such thatq = E X←P+ [g(X)] and q′ = E X′←P′− [g (X′)]. 21Proof of Lemma A.5.We consider the event thatA1 outputs y. By definition, we have A1(y) = ∞X k=1 P(T = k)[ kY j=1 Pj(≤ y) − kY i=1 Pj(< y)] = ∞X k=1 P(T = k)[ kX i=1 Pi(y) i−1Y j=1 Pj(< y) · kY j=i+1 Pj(≤ y)] ≤ ∞X k=1 P(T = k)[ kX i=1 P+(y) i−1Y j=1 P+(< y) · kY j=i+1 P+(≤ y)] = ∞X k=1 P(T = k)[ kX i=1 P+(y) · P+(< y)i−1 · P+(≤ y)k−i] = ∞X k=1 P(T = k)[P+(≤ y)k − P+(< y)k] = f(P+(≤ y)) − f(P+(< y)) = P+(y) · E X←Uniform([P+(<y),P+(≤y)]) [f′(X)]. The second equality is by partitioning on the events of the first time of gettingy, we usei to index such a time. The third inequality is using(A.7), (A.9), and(A.10). The third to last equality is by (A.12) and algebra. The second to last equality is by definition of the probability generating function f. The last equality follows from definition of integral. Similarly, we have A2(y) ≥ ∞X k=1 P(T = k)[P′−(≤ y)k − P′−(< y)k] = P′−(y) · E X←Uniform([P′−(<y),P′−(≤y)]) [f′(X)]. The rest part of the proof is standard and follows similarly as in [34]. Specifically, we have e(α−1)Dα(A1∥A2) = X y∈Y A1(y)α · A2(y)1−α ≤ X y∈Y P+(y)α · P′−(y)1−α · E X←[P+(<y),P+(≤y)] \u0002 f′(X) \u0003α · E X′←[P′−(<y),P′−(≤y)] \u0002 f′ \u0000 X′\u0001\u00031−α ≤ X y∈Y P+(y)α · P′−(y)1−α · E X←[P+(<y),P+(≤y)] X′←[P′−(<y),P′−(≤y)] h f′(X)α · f′ \u0000 X′\u00011−αi ≤ \u0012C c \u0013α sup λ e(α−1)Dα(Mλ(D)∥Mλ(D′)) · max y∈Y E X←[P+(<y),P+(≤y)] X′←[P′−(<y),P′−(≤y)] h f′(X)α · f′ \u0000 X′\u00011−αi . The last inequality follows from Lemma A.4. The second inequality follows from the fact that, for any α ∈ R, the functionh : (0, ∞)2 → (0, ∞) given byh(u, v) = uα · v1−α is convex. Therefore, E[U]αE[V ]1−α = h(E[(U, V)]) ≤ E[h(U, V)] = E \u0002 Uα · V 1−α\u0003 all positive random variables(U, V). Note that X and X′ are required to be uniform separately, but their joint distribution can be 22arbitrary. As in [34], we will couple them so thatX−P+(<y) P+(y) = X′−P′−(<y) P′−(y) . In particular, this implies that, for eachy ∈ Y, there exists somet ∈ [0, 1] such that E X←[P+(<y),P+(≤y)] X′←[P′−(<y),P′−(≤y)] h f′(X)α · f′ \u0000 X′\u00011−αi ≤ f′(P+(< y)+t·P+(y))α ·f′ \u0000 P′−(< y) + t · P′−(y) \u00011−α Therefore, we have Dα (A1∥A2) ≤sup λ Dα \u0000 Mλ∥M′ λ \u0001 + α α − 1 log C c + 1 α − 1 log  max y∈Y t∈[0,1] f′(P+(< y) + t · P+(y))α · f′ \u0000 P′−(< y) + t · P′−(y) \u00011−α  . To prove the result, we simply fixy∗ ∈ Yand t∗ ∈ [0, 1] achieving the maximum above and define g(y) :=    1 if y < y∗ t∗ if y = y∗ 0 if y > y∗ The result directly follows by settingq = E X←P+ [g(X)] and q′ = E X′←P′− [g (X′)]. Now we can prove Theorem 1, given the previous technical lemma. The proof share similarity to the proof of Theorem 2 in [34] with the key difference from the different form in Lemma A.5. We demonstrate this proof as follows for completeness. Proof of Theorem 1.We first specify the probability generating function of the truncated negative binomial distribution f(x) = E T∼NegBin(θ,γ) \u0002 xT \u0003 = ((1−(1−γ)x)−θ−1 γ−θ−1 if θ ̸= 0 log(1−1−γ)x) log(γ) if θ = 0 Therefore, f′(x) = (1 − (1 − γ)x)−θ−1 · (θ·(1−γ) γ−θ−1 if θ ̸= 0 1−γ log(1/γ) if θ = 0 = (1 − (1 − γ)x)−θ−1 · γθ+1 · E[T]. By Lemma A.5, for appropriate valuesq, q′ ∈ [0, 1] and for allα >1 and all ˆα >1, we have Dα (A1∥A2) ≤ sup λ Dα \u0000 Mλ∥M′ λ \u0001 + α α − 1 log C c + 1 α − 1 log \u0010 f′(q)α · f′ \u0000 q′\u00011−α\u0011 ≤ ε + α α − 1 log C c + 1 α − 1 log \u0010 γθ+1 · E[T] · (1 − (1 − γ)q)−α(θ+1) · \u0000 1 − (1 − γ)q′\u0001−(1−α)(θ+1)\u0011 = ε + α α − 1 log C c 23+ 1 α − 1 log \u0010 γθ+1 · E[T] · \u0010 (γ + (1 − γ)(1 − q))1−ˆα · \u0000 γ + (1 − γ) \u0000 1 − q′\u0001\u0001ˆα\u0011ν · (γ + (1 − γ)(1 − q))u \u0011 (Here, we letˆαν = (α − 1)(1 + θ) and (1 − ˆα)ν + u = −α(θ + 1)) ≤ ε + α α − 1 log C c + 1 α − 1 log \u0010 γθ+1 · E[T] · \u0010 γ + (1 − γ) · e(ˆα−1)Dˆα(P+∥P−)\u0011ν · (γ + (1 − γ)(1 − q))u \u0011 (Here,1 − q and 1 − q′ are postprocessings of someP+ and P′− respectively ande(ˆα−1)Dˆα(·∥·) is convex) ≤ ε + α α − 1 log C c + 1 α − 1 log \u0010 γθ+1 · E[T] · \u0010 γ + (1 − γ) · e(ˆα−1) supλ Dˆα(Mλ∥M′ λ)+ˆα log C c \u0011ν · (γ + (1 − γ)(1 − q))u \u0011 (By Lemma A.4) ≤ ε + α α − 1 log C c + 1 α − 1 log \u0010 γθ+1 · E[T] · \u0010 γ + (1 − γ) · e(ˆα−1) supλ Dˆα(Mλ∥M′ λ)+ˆα log C c \u0011ν · (γ + (1 − γ)(1 − q))u \u0011 ≤ ε + α α − 1 log C c + 1 α − 1 log \u0010 γθ+1 · E[T] · \u0010 γ + (1 − γ) · e(ˆα−1) supλ Dˆα(Mλ∥M′ λ)+ˆα log C c \u0011ν · γu \u0011 (Here γ ≤ γ + (1 − γ)(1 − q) and u ≤ 0) = ε + α α − 1 log C c + ν α − 1 log \u0010 γ + (1 − γ) · e(ˆα−1) supλ Dˆα(Mλ∥M′ λ)+ˆα log C c \u0011 + 1 α − 1 log \u0010 γθ+1 · E[T] · γu \u0011 = ε + α α − 1 log C c + ν α − 1 \u0012 (ˆα − 1) sup λ Dˆα \u0000 Mλ∥M′ λ \u0001 + ˆα log C c + log \u0010 1 − γ · \u0010 1 − e−(ˆα−1) supλ Dˆα(Mλ∥M′ λ)+ˆα log C c \u0011\u0011\u0011 + 1 α − 1 log \u0010 γu+θ+1 · E[T] \u0011 = ε + α α − 1 log C c + (1 +θ) \u0012 1 − 1 ˆα \u0013 sup λ Dˆα \u0000 Mλ∥M′ λ \u0001 + (1 +θ) log C c + 1 + θ ˆα log \u0010 1 − γ · \u0010 1 − e−(ˆα−1) supλ Dˆα(Mλ∥M′ λ)+ˆα log C c \u0011\u0011 + log(E[T]) α − 1 + 1 + θ ˆα log(1/γ) (Here we haveν = (α − 1)(1 + θ) ˆα and u = −(1 + θ) \u0012α − 1 ˆα + 1 \u0013 ) = ε + α α − 1 log C c + (1 +θ) \u0012 1 − 1 ˆα \u0013 sup λ Dˆα \u0000 Mλ∥M′ λ \u0001 + (1 +θ) log C c + 1 + θ ˆα log \u00121 γ − 1 + e−(ˆα−1) supλ Dˆα(Mλ∥M′ λ)−ˆα log C c \u0013 + log(E[T]) α − 1 ≤ ε + α α − 1 log C c + (1 +θ) \u0012 1 − 1 ˆα \u0013 ˆε + (1 +θ) log C c + 1 + θ ˆα log \u00121 γ \u0013 + log(E[T]) α − 1 , which completes the proof. B Truncated Negative Binomial Distribution We introduce the definition of truncated negative binomial distribution [34] in this section. Definition B.1.(Truncated Negative Binomial Distribution [34]). Let γ ∈ (0, 1) and θ ∈ (−1, ∞). Define a distribution NegBin(θ, γ) on N+ as follows: 24• If θ ̸= 0 and T is drawn from NegBin(θ, γ), then ∀k ∈ N P [T = k] = (1 − γ)k γ−θ − 1 · k−1Y ℓ=0 \u0012ℓ + θ ℓ + 1 \u0013 and E[T] = θ·(1−γ) γ·(1−γθ). Note that whenθ = 1, it reduces to the geometric distribution with parameter γ. • If θ = 0 and T is drawn from NegBin(0, γ), then P[T = k] = (1 − γ)k k · log(1/γ) and E[T] = 1/γ−1 log(1/γ). C Privatization of Sampling Distribution C.1 General Functional Projection Framework In section 3.2, we define the projection onto a convex setSC,c as an optimization in terms ofℓ2 loss. More generally, we can perform the following general projection at thej-th iteration by considering an additional penalty term, with a constantν: min f ∥f − π(j)∥2 + νKL(π(j), f) (C.1) s.t. f ∈ SC,c. When ν = 0, we recover the originalℓ2 projection. Moreover, it’s worth noting that our formulation has implications for the information projection literature [10, 23]. Specifically, as the penalty term parameterν approaches infinity, the optimization problem evolves into a minimization of KL divergence, recovering the objective function of information projection (in this instance, moment projection). However, the constraint sets in the literature of information projection are generally much simpler than our setSC,c, making it infeasible to directly borrow methods from its field. To the best of our knowledge, our framework is the first to address this specific problem in functional projection and establish a connection to information projection in the DP community. C.2 Practical Implementation of Functional Projection Optimization program (3.1) is essentially a functional programming sincef is a function onΛ. However, whenΛ represents a non-discrete parameter space, such functional minimization is typically difficulttosolveanalytically. Evenwithintheliteratureofinformationprojection, noneofthemethods considers our constraint setSC,c, which can be viewed as the intersections of uncountable single-point constraints onf. To obtain a feasible solution to the optimization problem, we leverage the idea of discretization. Instead of viewing(3.1) as a functional projection problem, we manually discretize Λ and solve(3.1) as a minimization problem over a discrete set. Note that such approximation is unavoidable in numerical computations since computers can only manage discrete functions, even when we solve the functional projection analytically. Moreover, we also have the freedom of choosing 25the discretization grid without incurring extra privacy loss since the privacy cost is independent of the size of parameter space. By convertingSC,c into a set of finite constraints, we are able to solve the discrete optimization problem efficiently using CVXOPT [2]. D DP-HyPO with General Prior Distribution In the main manuscript, we assumeπ(0) follows a uniform distribution over the parameter spaceΛ for simplicity. In practice, informed priors can be used when we want to integrate knowledge about the parameter space into sampling distribution, which is common in the Bayesian optimization framework. We now present the general DP-HyPO framework under the informed prior distribution. To begin with, we define the space of essentially bounded density functions with respect toπ(0) as SC,c(π(0)) = {f ∈ ΛR+ : ess supf/π(0) ≤ C, ess inff/π(0) ≥ c, Z α∈Λ f(α)dα = 1, f≪ π(0)}. When π(0) = 1 µ(λ), we recover the original definition ofSC,c. Note that heref ≪ π(0) means thatf is absolute continuous with respect to the prior distributionπ(0) and this ensures thatSC,c(π(0)) is non-empty. Note that such condition is automatically satisfied whenπ(0) is the uniform prior over the entire parameter space. To define the projection of a density at thej-th iteration, π(j), into the spaceSC,c(π(0)), we consider the following functional programming problem: min f ∥f − π(j)∥2 s.t. f ∈ SC,c(π(0)), which is a direct generalization of Equation (3.1). As before,SC,c(π(0)) is also convex and closed and the optimization program can be solved efficiently via discretization onΛ. E Experiment Details E.1 MNIST Simulation We now provide the detailed description of the experiment in Section 4.2.1. As specified therein, we consider two variable hyperparameters: the learning rateη and clipping normR, while keeping all the other hyperparameters fixed. We set the training batch size to be256, and the total number of epoch to be10. The value ofσ is determined based on the allocatedε budget for each base algorithm. Specifically,σ = 0.71 for GP andσ = 0.64 for Uniform. For demonstration purposes, we set C to 2 andc to 0.75 in the GP method, so each base algorithm of Uniform haslog C/c more privacy budget than base algorithms in GP method. In Algorithm 3, we setτ to 0.1 andβ to 1. To facilitate the implementation of both methods, we discretize the learning rates and clipping norms as specified in the following setting to allow simple implementation of sampling and projection for Uniform and GP methods. Setting E.1.we set a log-spaced grid discretization onη in the range[0.0001, 10] with a multiplicative factor of 3√ 10, resulting in16 observations forη. We also set a linear-spaced grid discretization onR 26in the range[0.3, 6] with an increment of0.3, resulting in20 observations forR. This gives a total of 320 hyperparameters over the search region. We specify the network structure we used in the simulation as below. It is the standard CNN in Tensorflow Privacy and Opacus. class ConvNet(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 16, 8, 2, padding=3) self.conv2 = nn.Conv2d(16, 32, 4, 2) self.fc1 = nn.Linear(32 * 4 * 4, 32) self.fc2 = nn.Linear(32, 10) def forward(self, x): x = F.relu(self.conv1(x)) x = F.max_pool2d(x, 2, 1) x = F.relu(self.conv2(x)) x = F.max_pool2d(x, 2, 1) x = x.view(-1, 32 * 4 * 4) x = F.relu(self.fc1(x)) x = self.fc2(x) return x Despite the simple nature of MNIST, the simulation of training CNN with the two methods over each different fixedT still take significant computation resources. Due to the constraints on computational resources, we conduct a semi-real simulation using the MNIST dataset. We cache the mean accuracy of5 independently trained models for each discretized hyperparameter and treat that as a proxy for the “actual accuracy” of the hyperparameter. Each time we sample the accuracy of a hyperparameter, we add Gaussian noise with a standard deviation of0.1 to the cached mean. We evaluate the performance of the output model based on the “actual accuracy” corresponding to the selected hyperparameter. E.2 CIFAR-10 Simulation We also provide a description of the experiment in Section 4.2.2. We set the training batch size to be 256, and the total number of epoch to be10. The value ofσ is determined based on the allocatedε budget for each base algorithm. Specifically,σ = 0.65 for GP andσ = 0.6 for Uniform. Regarding our GP method, we adopt the same set of hyperparameters as used in our MNIST experiments, which includeC = 2, c = 0.75, τ = 0.1, andβ = 1. As usual, we discretize the learning rates and clipping norms as specified in the following Setting. Setting E.2.we set a log-spaced grid discretization onη in the range[0.0001, 1] with a multiplicative factor of100.1, resulting in50 observations forη. We also set a linear-spaced grid discretization on R in the range[0, 100] with an increment of2, resulting in50 observations forR. This gives a total of 2500 hyperparameter combinations over the search region. We follow the same CNN model architecture with our MNIST experiments. 27In Figure 2, we provide the hyperparameter landscape forσ = 0.65, as generated by BoTorch [3]. Figure 2: Mean and standard error of the accuracy of DP-SGD over the two hyperparameters for σ = 0.65. The learning rate (log-scale) ranges from0.00001 (left) to 1 (right) while the clipping norm ranges from 0 (top) to 100 (bottom). The landscape forσ = 0.6 is similar, with a better accuracy. E.3 Federated Learning Simulation Figure 3: Mean and Standard Error of the loss of the FL over the two hyperparameters. We now provide the detailed description of the experiment in Section 4.2.3. As specified therein, we considered a FL task on a proprietary dataset5. Our objective is to determine the optimal learning rates for the central server (using AdaGrad) and the individual users (using SGD). To simulate this scenario, we utilize the landscape generated by BoTorch [3], as illustrated in Figure 3, and consider it as our reference landscape for both mean and standard deviation of the loss for each hyperparameter. When the algorithm (GP or Uniform) visits a specific hyperparameterλ, our oracle returns a noisy scoreq(λ) drawn from a normal distributionN(µλ, σλ). Figure 3 displays a heatmap that presents the mean (µλ) and standard error (σλ) structure of the loss over these two hyperparameters, providing insights into the landscape’s characteristics. 5We are unable to report a lot of detail about the proprietary dataset due to confidentiality. 28F Additional Related Work In this section, we delve into a more detailed review of the pertinent literature. We begin with non-private Hyperparameter Optimization, a critical topic in the realm of Auto- mated Machine Learning (AutoML) [18]. The fundamental inquiry revolves around the generation of high-performing models within a specific search space. In historical context, two types of optimiza- tions have proven significant in addressing this inquiry: architecture optimization and hyperparameter optimization. Architecture optimization pertains to model-specific parameters such as the number of neural network layers and their interconnectivity, while hyperparameter optimization concerns training-specific parameters, including the learning rate and minibatch size. In our paper, we incorpo- rate both types of optimizations within our HPO framework. Practically speaking,Λ can encompass various learning rates and network architectures for selection. For HPO, elementary methods include grid search and random search [24, 19, 16]. Progressing beyond non-adaptive random approaches, surrogate model-based optimization presents an adaptive method, leveraging information from preceding results to construct a surrogate model of the objective function [28, 43, 22, 32]. These methods predominantly employs Bayesian optimization techniques, including Gaussian process [35], Random Forest [20], and tree-structured Parzen estimator [5]. Another important topic in this paper is Differential Privacy (DP). DP offers a mathematically robust framework for measuring privacy leakage. A DP algorithm promises that an adversary with perfect information about the entire private dataset in use – except for a single individual – would find it hard to distinguish between its presence or absence based on the output of the algorithm [13]. Historically, DP machine learning research has overlooked the privacy cost associated with HPO [1, 41, 44]. The focus has only recently shifted to the “honest HPO” setting, where this cost is factored in [30]. Addressing this issue directly involves employing a composition-based analysis. If each training run of a hyperparameter upholds DP, then the overall HPO procedure adheres to DP through composition across all attempted hyperparameter values. A plethora of literature on the composition of DP mechanisms attempts to quantify a better DP guarantee of the composition. Vadhan et al. [38] demonstrated that though(ε, δ)-DP possesses a simple mathematical form, deriving the precise privacy parameters of a composition is #-P hard. Despite this obstacle, numerous advanced techniques are available to calculate a reasonably accurate approximation of the privacy parameters, such as Moments Accountant [1], GDP Accountant [12], and Edgeworth Accountant [39]. The efficacy of these accountants is attributed to the fact that it is easier to reason about the privacy guarantees of compositions within the framework of Rényi differential privacy [29] or f-differential privacy [12]. These methods have found widespread application in DP machine learning. For instance, when training deep learning models, one of the most commonly adopted methods to ensure DP is via noisy stochastic gradient descent (noisy SGD) [4, 37], which uses Moments Accountant to better quantify the privacy guarantee. Although using composition for HPO is a simple and straightforward approach, it carries with it a significant challenge. The privacy guarantee derived from composition accounting can be excessively loose, scaling polynomially with the number of runs. Chaudhuri et al. [8] were the first to enhance the DP bounds for HPO by introducing additional stability assumptions on the learning algorithms. [26] made significant progress in enhancing DP bounds for HPO without relying on any stability properties of the learning algorithms. They proposed a simple procedure where a hyperparameter was randomly selected from a uniform distribution for each training run. This selection process was repeated a random number of times according to a geometric distribution, and the best model obtained from these runs was outputted. They showed that this procedure satisfied 29(3ε, 0)-DP as long as each training run of a hyperparameter was(ε, 0)-DP. Building upon this, [34] extended the procedure to accommodate negative binomial or Poisson distributions for the repeated uniform selection. They also offered more precise Rényi DP guarantees for this extended procedure. Furthermore, [9] explored a generalization of the procedure for top-k selection, considering (ε, δ)-DP guarantees. In a related context, [30] explored a setting that appeared superficially similar to ours, as their title mentioned “adaptivity.” However, their primary focus was on improving adaptive optimizers such as DP-Adam, which aimed to reduce the necessity of hyperparameter tuning, rather than the adaptive HPO discussed in this paper. Notably, in terms of privacy accounting, their approach only involved composing the privacy cost of each run without proposing any new method. Another relevant area of research is DP selection, which encompasses well-known methods such as the exponential mechanism [27] and the sparse vector technique [14], along with subsequent studies (e.g., [6] and [17]). However, this line of research always assumes the existence of a low- sensitivity score function for each candidate, which is an unrealistic assumption for hyperparameter optimization. 30",
      "meta_data": {
        "arxiv_id": "2306.05734v2",
        "authors": [
          "Hua Wang",
          "Sheng Gao",
          "Huanyu Zhang",
          "Weijie J. Su",
          "Milan Shen"
        ],
        "published_date": "2023-06-09T07:55:46Z",
        "pdf_url": "https://arxiv.org/pdf/2306.05734v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "Introduces DP-HyPO, a pioneering framework for adaptive private hyperparameter optimization, addressing the privacy risks often overlooked in HPO for private ML models. It bridges the gap between non-adaptive private HPO methods and adaptive non-private HPO methods by allowing flexible, adaptive selection of hyperparameters while providing sharp Rényi Differential Privacy (RDP) guarantees. The framework can convert virtually any non-private adaptive HPO algorithm into a private one and empirically demonstrates that its Gaussian process-based instantiation outperforms non-adaptive uniform counterparts.",
        "methodology": "The DP-HyPO framework maintains an adaptive sampling distribution (π) for hyperparameters at each iteration, which is updated based on accumulated information from previous runs. To ensure privacy, the total number of repetitions (T) for training runs is a random variable (e.g., truncated negative binomial distribution). A crucial aspect is enforcing bounded adaptive density, where the ratio of any posterior sampling distribution to the prior (π(j+1)(λ)/π(0)(λ)) must be within a range [c, C]. Non-private update rules are privatized by projecting their resulting distribution onto this space of bounded densities, solving a convex functional programming problem. The privacy guarantees are derived using the Rényi DP framework, providing a strict generalization of previous uniform sampling results. An instantiation with Gaussian process (GP-based DP-HyPO) uses GP to build a surrogate model, assigns scores (e.g., UCB) to hyperparameters, and transforms them into a sampling distribution via a softmax function before applying the density projection.",
        "experimental_setup": "The performance of GP-based DP-HyPO (referred to as \"GP\") is empirically evaluated against a Uniform DP-HyPO baseline (\"Uniform\"), representing prior non-adaptive methods. Experiments are conducted under two privacy configurations: a white-box setting (adaptive HPO incurs extra privacy cost, reducing base algorithm budget) and a black-box setting (base algorithm privacy budget fixed, extra budget for HPO adaptivity). Datasets include MNIST (white-box) using a standard CNN with DP-SGD, CIFAR-10 (white-box) with the same CNN, and a real-world Federated Learning task on a proprietary industrial dataset (black-box). For MNIST, a semi-real simulation caches mean accuracies of independently trained models, adding Gaussian noise for sampling. For CIFAR-10 and Federated Learning, hyperparameter landscapes (mean and standard error of accuracy/loss) are generated using BoTorch, with an oracle returning noisy scores. Hyperparameters optimized include learning rate and clipping norm, with specific discretization grids (e.g., 320 for MNIST, 2500 for CIFAR-10) and total privacy budgets (e.g., ε=15 for MNIST, ε=12 for CIFAR-10). GP-specific parameters like exploration-exploitation factor (τ=0.1) and inverse temperature (β=1) are also set, with bounds C and c for adaptivity varying by scenario.",
        "limitations": "The framework requires the total number of repetitions for training runs (T) to be a random variable, not a fixed number, to preserve privacy. A key constraint is the necessity for adaptive sampling distributions to maintain a bounded density ratio (c ≤ π(j+1)(λ)/π(0)(λ) ≤ C), which may not be naturally satisfied by non-private adaptive HPO methods and requires a specific projection technique. For practical computational feasibility, especially with non-discrete hyperparameter spaces, practitioners must discretize the search space. Furthermore, the empirical evaluations on MNIST and CIFAR-10 rely on 'semi-real simulations' or pre-generated hyperparameter landscapes rather than full, independent training runs for every single hyperparameter combination, due to computational resource constraints. The framework also assumes a total ordering on the performance measure and that the base algorithm outputs the model, hyperparameter, and performance measure.",
        "future_research_directions": "Two main future research directions are suggested: 1) exploring alternative and more practically favorable HPO specifications, leveraging more advanced HPO methods to improve empirical performance; and 2) establishing theoretical utility guarantees for the general DP-HyPO framework, or for specific configurations within it, by employing proof methodologies similar to those found in related prior work."
      }
    },
    {
      "title": "Learning to Mutate with Hypergradient Guided Population"
    },
    {
      "title": "Gradient Descent: The Ultimate Optimizer",
      "abstract": "Working with any gradient-based machine learning algorithm involves the\ntedious task of tuning the optimizer's hyperparameters, such as its step size.\nRecent work has shown how the step size can itself be optimized alongside the\nmodel parameters by manually deriving expressions for \"hypergradients\" ahead of\ntime.\n  We show how to automatically compute hypergradients with a simple and elegant\nmodification to backpropagation. This allows us to easily apply the method to\nother optimizers and hyperparameters (e.g. momentum coefficients). We can even\nrecursively apply the method to its own hyper-hyperparameters, and so on ad\ninfinitum. As these towers of optimizers grow taller, they become less\nsensitive to the initial choice of hyperparameters. We present experiments\nvalidating this for MLPs, CNNs, and RNNs. Finally, we provide a simple PyTorch\nimplementation of this algorithm (see\npeople.csail.mit.edu/kach/gradient-descent-the-ultimate-optimizer).",
      "full_text": "Gradient Descent: The Ultimate Optimizer Kartik Chandra∗ MIT CSAIL† Cambridge, MA kach@csail.mit.edu Audrey Xie∗ MIT CSAIL Cambridge, MA ahx@mit.edu Jonathan Ragan-Kelley MIT CSAIL Cambridge, MA jrk@csail.mit.edu Erik Meijer Meta, Inc. Menlo Park, CA erikm@fb.com Abstract Working with any gradient-based machine learning algorithm involves the tedious task of tuning the optimizer’s hyperparameters, such as its step size. Recent work has shown how the step size can itself be optimized alongside the model parameters by manually deriving expressions for “hypergradients” ahead of time. We show how toautomatically compute hypergradients with a simple and elegant modiﬁcation to backpropagation. This allows us to easily apply the method to other optimizers and hyperparameters (e.g. momentum coefﬁcients). We can even recursively apply the method to its own hyper-hyperparameters, and so on ad in- ﬁnitum. As these towers of optimizers grow taller, they become less sensitive to the initial choice of hyperparameters. We present experiments validating this for MLPs, CNNs, and RNNs. Finally, we provide a simple PyTorch implementation of this algorithm (see people.csail.mit.edu/kach/gradient-descent-the-ultimate-optimizer). 1 Introduction When we train deep neural networks by gradient descent, we have to select a step size αfor our optimizer. If αis too small, the optimizer runs very slowly, whereas ifαis too large, the optimizer fails to converge. Choosing an appropriate αis thus itself an optimization task that machine learning practitioners face every day. Why not apply gradient descent here, too? To do so, we need to compute the derivative of the loss function not only with respect to the neural network’s weights, but also with respect to α. Baydin et al. (2018), applying an insight from Almeida et al. (1999), describe how to efﬁciently compute such “hypergradients” by manually differentiating standard optimizer update rules with respect to the step size hyperparameter. This allows for on-line learning rate adaptation, which generally improves convergence, especially when the initial αis sub-optimal. However, the above method has three limitations: (1) manually differentiating optimizer update rules is tedious and error-prone, and must be re-done for each optimizer variant; (2) the method only tunes the step size hyperparameter, not other hyperparameters such as the momentum coefﬁcient; and (3) the method introduces a new hyperparameter, the hyper-step-size, which must also be tuned. In this paper, we address all three limitations by replacing manual differentiation with automatic differentiation (AD), which (1) automatically computes correct derivatives without any additional human effort, and (2) naturally generalizes to other hyperparameters (e.g. momentum coefﬁcient) for free. As for (3), we observe that AD can be applied to optimize not only the hyperparameters, but also the hyper-hyperparameters, and the hyper-hyper-hyperparameters, and so on. In fact, we can implement arbitrarily tall towers of recursive optimizers, which are increasingly robust to the choice of initial hyperparameter. These “hyperoptimizers” therefore reduce the burden on humans responsible for tuning the hyperparameters. (Such an effect was hypothesized by Baydin et al., but not tested because manual differentiation of complex sequences of nested optimizers is impractical.) ∗Equal contribution. †Work done in part at Meta, Inc. and in part at Stanford University. 36th Conference on Neural Information Processing Systems (NeurIPS 2022). arXiv:1909.13371v2  [cs.LG]  14 Oct 2022Although “just apply AD” is a seemingly straightforward recipe, an efﬁcient implementation that properly allows for recursive self-application requires some care. To close the loop, we take inspiration from the study of recursion and combinators in programming language theory (and the long tradition of programming language papers named “Lambda: The Ultimate X”). We spell out the details in Section 2, and evaluate our method in Section 3. We ﬁnd that across a variety of architectures (MLPs, CNNs, and RNNs) our hyperoptimizers are robust to suboptimal choices of initial hyperparameters, and that this robustness increases as we grow the stacks of optimizers taller. 2 Implementing hyperoptimizers Consider some loss function f that we want to minimize using gradient descent, and let wi be the weights at the beginning of step i(we will omit subscripts on f, even though it varies at each step due to the stochasticity of minibatches). First, recall the standard weight update rule at step ifor SGD, using some ﬁxed step size α: wi+1 = wi −α∂f(wi) ∂wi We would like to also update αat each step, so we will index it as well with the step number; that is, let αi be the step size at the beginning of step i. At each step, we will ﬁrst update αi to αi+1 using some update rule yet to be derived, and then use the updated step size αi+1 to update the weights from wi to wi+1. αi+1 = αi − adjustment for αi wi+1 = wi −αi+1 ∂f(wi) ∂wi What should the adjustment for αi be? By analogy to w, we want to adjust αi in the direction of the gradient of the loss function with respect to αi, scaled by some hyper-step size κ. In other words, the adjustment should be κ(∂f(wi)/∂αi). Our modiﬁed update rule is therefore: αi+1 = αi −κ∂f(wi) ∂αi (1) wi+1 = wi −αi+1 ∂f(wi) ∂wi (2) All that remains is to compute ∂f(wi)/∂αi in equation (1). Below, we ﬁrst review how Baydin et al. (2018) take this derivative by hand. Then, we show how to obtain the same result automatically and efﬁciently using AD. Finally, we discuss how this automation allows us to generalize the method. 2.1 Computing the step-size update rule by hand One option to compute ∂f(wi)/∂αi, explored by Baydin et al. (2018), is to proceed by direct manual computation of the partial derivative. Applying the chain rule to (1), we have ∂f(wi) ∂αi = ∂f(wi) ∂wi ·∂wi ∂αi = ∂f(wi) ∂wi · ∂ ( wi−1 −αi ∂f(wi−1) ∂wi−1 ) ∂αi (3) = ∂f(wi) ∂wi · ( −∂f(wi−1) ∂wi−1 ) (4) where (3) is obtained by substituting the update rule in (2) for wi and (4) is obtained by observing that wi−1 and f(wi−1) do not depend on αi. As Baydin et al. note, this expression lends itself to a simple and efﬁcient implementation: simply remember the past two gradients from backpropagation, and take their dot product to obtain the hypergradient with respect to the step size. We were able to take this derivative by hand because the update rule for SGD is simply a multiplication by a constant, whose derivative is trivial. What about other optimizers? Consider the Adam optimizer (Kingma and Ba, 2014), which has a much more sophisticated update rule involving the four hyperparameters α,β1,β2,ϵ (though ϵis typically not tuned). Differentiating the update rule by hand, 2we obtain signiﬁcantly more complex expressions for the hypergradients: ∂wi ∂αi = − ˆmi( ϵi + √ˆvi ) ∂wi ∂β1i = − αi ( −∂f(wi−1) ∂wi−1 + mi−1 + iβ1 (i−1) i ˆmi ) ( 1 −β1 i i ) ( ϵi + √ˆvi ) ∂wi ∂ϵi = αiˆmi ( ϵi + √ˆvi )2 ∂wi ∂β2i = αiˆmi √ˆvi ( − ( ∂f(wi−1) ∂wi−1 )2 + vi−1 + iβ2 (i−1) i ˆvi ) 2vi ( ϵi + √ˆvi )2 This manual approach to derive hypergradients simply does not scale: it is tedious and error-prone, and must be repeated for every optimizer variant. However, with a little bit of care, we can compute hypergradients automatically and efﬁciently alongside the regular gradients. 2.2 Computing the step-size update rule automatically In order to compute hypergradients automatically, let us ﬁrst brieﬂy review the mechanics of reverse- mode AD. Differentiable programming systems that provide reverse-mode AD typically build up a computation graph as the function is computed forwardly. For example, when a user computes the function f(wi), the system internally stores a DAG whose leaves are the weights wi, whose internal nodes are intermediate computations, and whose root is the ﬁnal loss. It can then backpropagate through the computation graph starting at this root node, depositing gradients in each internal node as it descends, until the weights wi at the leaf nodes have accumulated their gradients ∂f(wi)/∂wi. Once the gradient ∂f(wi)/∂wi is computed by the backwards pass, we update the weights wi+1 = wi −α·∂f(wi)/∂wi, and repeat the cycle for the next step of gradient descent. An important consideration at this point is for the weights to be “detached” from the computation graph before the next iteration of this algorithm — that is, for the weights to be forcibly converted to leaves of the graph by removing any inbound edges. The effect of the “detach” operation is depicted in Figure 1a. If this step were skipped, backpropagation at the next iteration would continue beyond the current weights and into the previous iteration’s computation graph. Over time the computation graph would grow taller linearly in the number of steps taken; because backpropagation is linear in the size of the graph, the overall training would become quadratic-time and intractable. Let us take PyTorch as an example. In the built-in SGD optimizer (Paszke et al., 2017, optim/sgd.py, commit ff94c9d), this is implemented by wrapping the update in the @torch.no_grad() context manager. Here, we need ﬁner grained control over gradient ﬂow, so will make the .detach() operations explicit. Below is pseudocode for an SGD optimizer that uses .detach() as we have discussed. The highlighted calls to .detach() correspond to detaching the weights and their gradients. def SGD.__init__(self, alpha): self.alpha = alpha def SGD.step(w): d_w = w.grad.detach() w = w.detach() - self.alpha.detach() * d_w Now, in order to have backpropagation deposit the gradient with respect to αi as well as wi, we can simply refrain from detaching αi from the graph, detaching instead its parents. This is depicted in Figure 1b. Because we want to compute ∂f(wi)/∂αi, the edge from αi to wi needs to remain intact. To implement this, instead of calling .detach() on alpha directly, we detach its parents when applying equation (1). This yields the following fully-automated hyperoptimization algorithm: def HyperSGD.step(w): # update alpha using equation (1) d_alpha = self.alpha.grad.detach() self.alpha = self.alpha.detach() - kappa.detach() * d_alpha # update w using equation (2) d_w = w.grad.detach() w = w.detach() - self.alpha.detach() * d_w 3(a) Computation graph of SGD with a single ﬁxed hy- perparameter α. (b) Computation graph of SGD with a continuously- updated hyperparameter αi. Figure 1: Visualizing the computation graphs of SGD and HyperSGD. Since we only extend the computation graph by a little extra amount, corresponding to evaluating the optimizer, the hyperoptimizer’s computational overhead is negligible (see Figure 4f). 2.3 Extending to other optimizers As suggested by Maclaurin et al. (2015), it should be possible to apply gradient-based methods to tune hyperparameters of common variations on SGD such as AdaGrad (Duchi et al., 2011), AdaDelta (Zeiler, 2012), or Adam (Kingma and Ba, 2014). The above implementation of HyperSGD generalizes quite easily to these optimizers — we simply replace the last line with the new update rule. Unlike previous work, our method also allows for simultaneously optimizing all hyperparameters of these optimizers (e.g. all of α, β1, and β2 for Adam) “for free.” We simply treat them just likealpha in the implementation. Our evaluation in Section 3.2 demonstrates that this indeed advantageous to do. There are, however, two important subtleties: First, because hyperparameters like β1 and β2 must be strictly in the domain (0,1), we clamp the “raw” values to this domain using a scaled sigmoid. Without this step, we might accidentally adjust these values outside their domains. Second, the Adam optimizer in particular involves the term √ˆvi, which is continuous but not differentiable at ˆvi = 0. Because Adam normally initializes ˆv0 = 0, backpropagation fails on the ﬁrst step due to division by zero. We ﬁx this problem by initializing ˆv0 to ϵrather than 0. 2.4 Stacking hyperoptimizers recursively At this point it is natural to ask whether the hyperoptimizer can itself be optimized; that is, whether the hyper-hyperparameters can be adjusted by a hyper-hyperoptimizer. The possibility of doing so recursively ad inﬁnitum to obtain an optimization algorithm that is highly robust to the human- chosen hyperparameter was hypothesized by Baydin et al. (2018). Computing the gradients of these higher-order hyperparameters by hand is impossible without knowing the exact sequence of stacked optimizers in advance, and, as discussed above, would be extremely tedious and error-prone. However, the ability to compute these gradients automatically by AD makes it possible to realize this vision. To do so, let us revisit our previous implementation of HyperSGD. Notice that there is an opportunity for recursion lurking here: the adjustment to alpha can be factored out with a call to SGD.step, where SGD’s hyperparameter iskappa. def HyperSGD.step(w): # update alpha using Equation (1) SGD(kappa).step(self.alpha) # update w using Equation (2) d_w = w.grad.detach() w = w.detach() - self.alpha * d_w 4Because SGD is already careful to properly detach its parameter (typically w, but in this case α), this implementation is functionally identical to the one above. Indeed, any optimizer that observes this protocol would sufﬁce, so let us abstract out the optimizer as a parameter to HyperSGD: def HyperSGD.__init__(self, alpha, opt): self.alpha = alpha self.optimizer = opt def HyperSGD.step(w): self.optimizer.step(self.alpha) d_w = w.grad.detach() w = w.detach() - self.alpha * d_w opt = HyperSGD(0.01, opt=SGD(kappa)) Finally, after this refactoring, we can recursively feedHyperSGD itself as the optimizer, obtaining a level-2 hyperoptimizer HyperSGD(0.01, HyperSGD(0.01, SGD(0.01))). Similarly, we can imagine taller towers, or towers that mix and match multiple different kinds of optimizers, such as Adam- optimized-by-SGD-optimized-by-Adam. A natural concern is whether this process actually exacerbates the hyperparameter optimization problem by introducing even more hyperparameters. Baydin et al. (2018) predicted that as the towers of hyperoptimizers grew taller, the resulting algorithms would become less sensitive to the human-chosen hyperparameters. This is indeed the case; Section 3.4 presents an empirical evaluation. 3 Experiments In this section we evaluate the hyperoptimizers made possible by our system, exploring in particular the beneﬁts of optimizing hyperparameters beyond just the step size, and of stacking hyperoptimizers to multiple levels. Each of these experiments was conducted on a single NVIDIA TITAN Xp GPU. 3.1 Hyperoptimization for SGD First, we establish some basic properties about hyperoptimizers: (1) whether an SGD hyperoptimizer performs better than a regular SGD optimizer, and (2) whether the ﬁnal learned step size is better than the initial human-chosen step size. We test the latter property by running a fresh regular SGD optimizer with the ﬁnal learned step size of the hyperoptimizer. Following authors of prior work (Maclaurin et al., 2015; Baydin et al., 2018), we conducted initial experiments on the MNIST dataset (Lecun et al., 1998) using a neural network with one fully-connected hidden layer of size 128, tanh activations, and a batch size of 256. We trained all networks for 30 epochs, reporting statistics over 3 runs. As a baseline we used SGD with α= 0.01. Table 1a summarizes the results of our experiments. We ﬁnd that hyperoptimized SGD outper- forms the baseline by a signiﬁcant margin. This holds even if we use other optimizers (e.g. Adam) to adjust the step size of the SGD optimizer. Furthermore, when we re-ran the regular optimizers with the new learned hyperparameters, we found that they performed better than the initial hyperparameter. 3.2 Hyperoptimization for Adam, AdaGrad and RMSProp In Section 2.3, we described how to apply our system to the Adam optimizer, simultaneously optimizing not only the learning rate α, but also the momentum coefﬁcients β1,2. Here, we ask three questions: (1) whether hyperoptimized Adam optimizers perform better than regular Adam optimizers, (2) whether the learned hyperparameters outperform the baseline, and (3) whether there is a beneﬁt to optimizing all the hyperparameters, as opposed to only optimizing the learning rate as Baydin et al. (2018) do. Because Adam has signiﬁcantly faster convergence than SGD, we only run these experiments for 5 epochs to avoid overﬁtting. Table 1b summarizes the results of our experiments. We ﬁnd that indeed the hyperoptimized Adam optimizer outperforms the regular Adam optimizer on its “default” settings. As with SGD, the learned hyperparameters perform better than the initial hyperparameters when re-run with the regular optimizer. Inspecting the learned hyperparameters, we ﬁnd that the algorithm raises the learning rate 5Optimizer Test error SGD 8.99±0.05% SGD / SGD 4.81±0.10% SGD(0.0769) 5.44±0.10% SGD / Adam(0.1) 4.86±0.06% SGD(0.4538) 2.80±0.09% SGD / AdaGrad 4.85±0.21% SGD(0.0836) 5.17±0.03% SGD / RMSprop(0.1) 4.52±0.02% SGD(0.5920) 2.52±0.07% (a) Experiments with SGD (Section 3.1) Optimizer Test error Adam 4.67±0.06% Adam / SGD(10−5) 3.03±0.02% Adam(0.0040, 0.899, 0.999) 3.11±0.06% Adamα / SGD(10−5) 3.12±0.04% Adamα(0.0021) 3.47±0.02% Adam / Adam 3.05±0.09% Adam(0.0038, 0.870, 0.999) 3.24±0.13% Adamα / Adam 3.04±0.08% Adamα(0.0036) 3.08±0.12% (b) Experiments with Adam (Section 3.2) Optimizer Test error AdaGrad 7.40±0.08% AdaGrad / SGD 6.90±0.16% AdaGrad(0.0080) 7.75±0.02% AdaGrad / AdaGrad 5.03±0.23% AdaGrad(0.0151) 6.67±0.08% (c) Experiments with AdaGrad (Section 3.2) Optimizer Test error RMSProp 4.19±0.47% RMSPropα / SGD(10−4) 3.55±0.23% RMSProp(0.0030) 3.93±0.70% RMSpropα,γ / SGD(10−4) 3.33±0.07% RMSProp(0.0032, 0.9899) 3.25±0.09% RMSPropα / RMSProp(10−4) 3.42±0.45% RMSProp(0.0021) 3.60±0.04% RMSPropα,γ / RMSProp(10−4) 2.96±0.11% RMSProp(0.0020, 0.9962) 3.65±0.36% (d) Experiments with RMSProp (Section 3.2) Table 1: Hyperoptimization experiments with MNIST. We denote hyperoptimizers by their constituent optimizers separated by slashes (the leftmost item adjusts the model’s weights). Adamα is an Adam optimizer where only α is optimized as by Baydin et al. (2018); RMSProp α is similar. If not speciﬁed, initial hyperparameters are PyTorch defaults ( 10−2 for learning rates except 10−3 for Adam; β1 = 0.9,β2 = 0.99 for Adam and γ = 0.99 for RMSProp). Each hyperoptimizer experiment is repeated using the ﬁnal hyperparameters (typeset in pink) learned by the algorithm. αand slightly lowers β1, but does not signiﬁcantly affect β2. Nevertheless, learning β1 does help slightly, though not when the top-level optimizer is itself another Adam optimizer. Similarly, we can add any other optimizer to our system with just a few straightforward lines of code. Here, we show results for AdaGrad (Table 1c) and RMSProp (Table 1d; also run to 5 epochs). These experiments took less than an hour each to implement from scratch, and show that every hyperoptimizer stack outperforms the non-hyperoptimized baseline.We remark that AdaGrad is known to “stall” over time as the effective step size goes to zero; inspecting the learned αover time, we ﬁnd that the AdaGrad/AdaGrad hyperoptimizer increases αto make up for this effect. Additionally, we tried to hyperoptimize RMSProp’s newγparameter, which modulates the accumulation of gradient RMS terms. This yielded even better results (compare α to α,γ trials), and required only a 1-line change in our code. 3.3 Hyperoptimization at scale Next, we evaluate our hyperoptimizers on two different real-world neural network architectures. 3.3.1 Convolutional neural networks for computer vision We train a ResNet-20 (He et al., 2016) with and without hyperoptimization on the CIFAR-10 dataset (Krizhevsky, 2012). As a baseline, we replicate the training procedure of He et al. (2016): we 6(a) For a wide range of “bad” initial hyperparameter conﬁgurations, the hyperoptimizer improves on (or at least matches) ﬁnal test accuracy, and often matches or even outperforms the “good” initial hyperparameters. (b) The hyperoptimizer matches performance of the hand-engineered learning rate decay schedule by He et al. (2016), learning a strikingly similar decay schedule (right plot). Figure 2: Training ResNets on CIFAR-10 with hyperoptimizers (Section 3.3.1). use the same network architecture, optimizer (SGD), step size (0.1), momentum (0.9), and weight decay (10−4), though without their custom learning rate decay schedule (which we will address later). Experiments were run for 200 epochs, which takes around 3 hours on our hardware. First, we test how robust the hyperoptimizer is to “bad” initial choices of step size and momentum. We vary the initial step size and the momentum among “small,” “good,” and “large” values (that is, α ∈{0.01,0.1,1.0}and µ ∈{0.09,0.9,0.99}), and add a hyperoptimizer ( αα = α2 ·10−3, αµ = 1/(1−µ)·10−6). The results of this experiment are shown in Figure 2a. In every conﬁguration, the hyperoptimizer matches or outperforms the regular optimizer in ﬁnal test accuracy. Furthermore, in nearly all of the conﬁgurations, the hyperoptimizer matches or exceeds the “good” hyperparameters’ ﬁnal test accuracy. Only when both hyperparameters are bad in the same direction (too small or too large) is it unable to manage this, and even then for the too-large case it dramatically lowers the loss compared to no hyperoptimizer. We conclude that hyperoptimizers are indeed beneﬁcial for tuning both step size and momentum in this real-world setting. Next, we add in the learning rate decay schedule hand-engineered by He et al. (2016): the step size is divided by 10 at epochs 100 and 150. We compare this with a hyperoptimizer initialized with the same starting hyperparameters, training both variants for 500 epochs. Our results are shown in Figure 2b. The hyperoptimizer not only matches the ﬁnal test loss of the hand-engineered learning rate decay schedule, but also learns a decay schedule strikingly similar to one hand-engineered by He et al. Of course, both networks signiﬁcantly outperform the baseline trained with a ﬁxed step size. 3.3.2 Recurrent neural networks for language modeling We train a character-level RNN (“Char-RNN”) on the Tolstoy dataset, as proposed by Karpathy et al. (2015) as a convenient testbed for language models, which is now often used to benchmark optimizers (Schneider et al., 2018; Schmidt et al., 2021). We took the architecture (2-layer LSTM with 128 hidden nodes) and “expert” optimizer (Adam optimizer with α= 2×10−3, run for 50,000 gradient 7Figure 3: Training RNNs with hyperoptimizers (Section 3.3.2). As the initial learning rate is lowered, the regular Adam optimizer’s convergence slows, but the hyperoptimizer is able to accelerate it. The hyperoptimizer also slightly improves convergence when the initial learning rate is too high. descent steps) directly from Johnson (2017) as recommended by Karpathy et al. We compare against our HyperAdam optimizer on a wide range of initial learning rates α ∈{10−4,2 ×10−3,10−2}, with αα = α·10−2. We do not vary initial β1,2 because in our experience these hyperparameters are typically left at their default values. However, we do allow the hyperoptimizer to vary β1,2 over the course of training (with αβ1 = 10−4 and αβ2 = 2×10−4). All runs took around 1 hour to train. The results of this experiment are shown in Figure 3. We ﬁnd that the hyperoptimizer performs comparably to the expert-chosen ﬁxed step size (perplexity 5.41 ±0.26 with hyperoptimizer vs 5.27 ±0.31 without), and improves upon “bad” initial step sizes in both directions (5.45 ±0.76 vs 5.77 ±0.34 when too high; 6.51 ±0.88 vs 8.71 ±0.91 when too low). 3.4 Higher-order hyperoptimization In Section 2.4 we developed an interface for building arbitrarily tall towers of optimizers. Baydin et al. (2018) hypothesized that taller towers would yield hyperoptimizers that were increasingly robust to the initial human-chosen hyperparameters. To validate this behavior of higher-order hyperoptimizers, we ran each of our benchmarks from above (MLP on MNIST, CNN on CIFAR-10, RNN on Tolstoy) with towers of hyperoptimizers of increasing heights, and with bottom-level step sizes αinitialized across many orders of magnitude. In practice we ﬁnd that if the initial hyper-step sizes are too large, the computation diverges for networks larger than the MNIST MLP. So, we initialize each level’s hyperparameter to be smaller than that of the previous level. Speciﬁcally, we use the following scheme: from α= 10−8 to 10−4 the higher layers’ step sizes were initialized to [α·102,α ·100,α ·10−2] respectively, while for α≥10−3 they were initialized to [α·10−3,α ·10−4,10−8] respectively. Figure 4 shows our results. It is indeed the case across these different benchmarks (each of which has a different dataset, architecture, and optimizer type) that the taller the hyperoptimizer stack, the less sensitive the results become to the human-chosen hyperparameters. With a three-level optimizer stack, a single hyperoptimizer design obtains reasonable results in all of our bench- marks across several orders of magnitude of base-level step size. Further tests of scalability To test if our hyperoptimizers continue to work in even larger regimes, we ﬁne-tuned a ResNet-152 (pretrained on ImageNet) to the Caltech-256 dataset Grifﬁn et al. (2007). Figure 4e shows the results: a height-1 hyperoptimizer recovers ≈11% error for both α= 10−6 and α= 10−4 (without a hyperoptimizer, α= 10−6 gives 91.5% error). A height-2 hyperoptimizer is additionally able to make signiﬁcant progress when α= 10−2. 8(a) Results on an MLP (Sec 3.1), where all layers are initialized with the same step size. (b) Results on an MLP (Sec 3.1), where all layers are initialized as in Sec 3.4. (c) Results on a ResNet (Sec 3.3.1)  (d) Results on a Char-RNN (Sec 3.3.2) (e) Results on ﬁne-tuning a pretrained ResNet-152 to the Caltech-256 dataset (Sec 3.4) (f) Our hyperoptimizers have minimal impact on runtime, which scales linearly in height (Sec 3.4) Figure 4: Evaluating higher-order hyperoptimization across a variety of benchmarks (Section 3.4). As we stack more layers of optimizers, the resulting hyperoptimizer is less sensitive to the initial choice of hyperparameters, but costs only 1-2% more in runtime. We stress how lightweight and practical this method is. Figure 4f shows how runtime scales as a function of hyperoptimizer stack height for the above benchmarks. The scaling is linear: each additional level costs only 1-2% in additional runtime above the non-hyperoptimized baseline. 4 Related work Hyperparameter optimization has a long history, and we refer readers to a recent survey by Feurer and Hutter (2019) for the full story. Most existing work on gradient-based hyperparameter optimiza- tion (Bengio, 2000; Domke, 2012; Maclaurin et al., 2015; Pedregosa, 2016; Franceschi et al., 2017) has focused on computing hyperparameter gradients after several iterations of training, which is 9computationally expensive. Baydin et al. (2018), building on a technique ﬁrst published by Almeida et al. (1999), propose instead updating hyperparameters at each step, and Rubio (2017) provides a convergence analysis. Luketina et al. (2016) apply a similar technique to regularization hyperpa- rameters, though they note that their proposed method could work in principle for any continuous hyperparameter. As discussed above, we expand upon this line of work in three directions: (1) by fully automating this process, rather than requiring manual derivative computations; (2) by optimizing hyperparameters beyond just the learning rate; and (3) by realizing the vision of recursive higher-order hyperoptimizers and evaluating the resulting algorithms. We ﬁnd that they are indeed more robust to the initial human-chosen hyperparameter, which relates our work to other learning algorithms that minimize sensitivity to learning rates (Orabona and Tommasi, 2017; Vaswani et al., 2019). 5 Limitations and future work As discussed in Section 3.4, one limitation of hyperoptimizers is that they cannot yet handle initial hyperparameters that are set far too high, because the system is unstable and diverges before the hyperoptimizer can have an effect. Designing hyperoptimizers robust in this regime requires further research, such as a deeper theoretical analysis of convergence. Our implementation also requires some care in avoiding certain bugs related to computation graph management. For example, loggers must detach what is logged to avoid memory leaks because tensors are not garbage collected unless all children are detached. Similarly, certain PyTorch modules (e.g. the built-in LSTM) cannot be used because they silently modify the computation graph, which may lead to incorrect gradients with our system. Further research is needed to design differentiable programming languages where methods like ours can be expressed in a modular and composable manner that minimizes the risk of such bugs. Broader impact Training a modern deep learning system consumes a tremendous amount of energy, and hyperparameter searches can multiply that energy impact by many orders of magnitude (Strubell et al., 2019). We hope that advances in on-line hyperparameter tuning can reduce this impact. 6 Conclusion We presented a technique that enables gradient descent optimizers like SGD and Adam to tune their own hyperparameters. Unlike prior work, our proposed hyperoptimizers require no manual differentiation, learn hyperparameters beyond just learning rates, and can be stacked recursively to many levels. We described an elegant recursive implementation of hyperoptimizers in a reverse-mode AD system and evaluated it on a variety of benchmarks, showing that as the stacks grow taller, they become less sensitive to the initial human-chosen hyperparameter. Acknowledgments and Disclosure of Funding We thank Samantha Andow, Emilio Arroyo-Fang, Irene Dea, Johann George, Melissa Grueter, Basil Hosmer, Stefﬁ Stumpos, Alanna Tempest, and Shannon Yang for early discussions, Krishna Murthy Jatavallabhula and Josh Tenenbaum for their advice when preparing this paper, and the anonymous reviewers for their thoughtful feedback. KC and JRK were supported by NSF Grants #2105806, #CCF-1231216, #CCF-1723445 and #CCF-1846502, and ONR Grant #00010803 at MIT. Additionally, KC was supported by a Hertz Foundation Fellowship, the Paul and Daisy Soros Fellowship for New Americans, and an NSF Graduate Research Fellowship under Grant #2141064, and AX was supported by the MIT Undergraduate Research Opportunities Program (UROP). References L. E. Almeida, T. Langlois, J. F. M. do Amaral, and A. Plakhov. Parameter adaptation in stochastic optimization. In On-Line Learning in Neural Networks, 1999. A. G. Baydin, R. Cornish, D. M. Rubio, M. Schmidt, and F. Wood. Online learning rate adaptation with hypergradient descent. In Sixth International Conference on Learning Representations (ICLR), Vancouver, Canada, April 30 – May 3, 2018, 2018. 10Y . Bengio. Gradient-based optimization of hyperparameters. Neural Computation , 12(8): 1889–1900, 2000. doi: 10.1162/089976600300015187. URL https://doi.org/10.1162/ 089976600300015187. J. Domke. Generic methods for optimization-based modeling. In N. D. Lawrence and M. Girolami, editors, Proceedings of the Fifteenth International Conference on Artiﬁcial Intelligence and Statis- tics, volume 22 of Proceedings of Machine Learning Research, pages 318–326, La Palma, Canary Islands, 21–23 Apr 2012. PMLR. URL http://proceedings.mlr.press/v22/domke12. html. J. Duchi, E. Hazan, and Y . Singer. Adaptive subgradient methods for online learning and stochastic optimization. J. Mach. Learn. Res., 12:2121–2159, July 2011. ISSN 1532-4435. URL http: //dl.acm.org/citation.cfm?id=1953048.2021068. M. Feurer and F. Hutter.Hyperparameter Optimization, pages 3–33. Springer International Publishing, Cham, 2019. ISBN 978-3-030-05318-5. doi: 10.1007/978-3-030-05318-5_1. URL https: //doi.org/10.1007/978-3-030-05318-5 _1. L. Franceschi, M. Donini, P. Frasconi, and M. Pontil. Forward and reverse gradient-based hyperpa- rameter optimization. In D. Precup and Y . W. Teh, editors,Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 1165–1173, International Convention Centre, Sydney, Australia, 06–11 Aug 2017. PMLR. URL http://proceedings.mlr.press/v70/franceschi17a.html. G. Grifﬁn, A. Holub, and P. Perona. Caltech-256 object category dataset. 2007. URL http: //authors.library.caltech.edu/7694/. K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV , USA, June 27-30, 2016, pages 770–778. IEEE Computer Society, 2016. doi: 10.1109/CVPR.2016.90. URL https://doi.org/10.1109/CVPR.2016.90. J. Johnson. torch-rnn. Github repository, 2017. URL https://github.com/jcjohnson/ torch-rnn. A. Karpathy, J. Johnson, and L. Fei-Fei. Visualizing and understanding recurrent networks. arXiv preprint arXiv:1506.02078, 2015. URL https://arxiv.org/pdf/1506.02078.pdf. D. Kingma and J. Ba. Adam: A method for stochastic optimization. International Conference on Learning Representations, 12 2014. A. Krizhevsky. Learning multiple layers of features from tiny images. University of Toronto, 05 2012. Y . Lecun, L. Bottou, Y . Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE , 86(11):2278–2324, Nov 1998. ISSN 0018-9219. doi: 10.1109/5.726791. J. Luketina, M. Berglund, K. Greff, and T. Raiko. Scalable gradient-based tuning of continuous regu- larization hyperparameters. In Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48, ICML’16, pages 2952–2960. JMLR.org, 2016. URL http://dl.acm.org/citation.cfm?id=3045390.3045701. D. Maclaurin, D. Duvenaud, and R. P. Adams. Gradient-based hyperparameter optimization through reversible learning. In Proceedings of the 32Nd International Conference on International Con- ference on Machine Learning - Volume 37, ICML’15, pages 2113–2122. JMLR.org, 2015. URL http://dl.acm.org/citation.cfm?id=3045118.3045343. F. Orabona and T. Tommasi. Training deep networks without learning rates through coin betting. Advances in Neural Information Processing Systems, 30, 2017. A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer. Automatic differentiation in PyTorch. In NIPS Autodiff Workshop, 2017. 11F. Pedregosa. Hyperparameter optimization with approximate gradient. In Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48, ICML’16, pages 737–746. JMLR.org, 2016. URL http://dl.acm.org/citation.cfm?id=3045390. 3045469. D. M. Rubio. Convergence analysis of an adaptive method of gradient descent. University of Oxford, Oxford, M. Sc. thesis, 2017. URL https://damaru2.github.io/convergence_analysis_ hypergradient_descent/dissertation_hypergradients.pdf. R. M. Schmidt, F. Schneider, and P. Hennig. Descending through a crowded valley-benchmarking deep learning optimizers. In International Conference on Machine Learning, pages 9367–9376. PMLR, 2021. F. Schneider, L. Balles, and P. Hennig. Deepobs: A deep learning optimizer benchmark suite. In International Conference on Learning Representations, 2018. E. Strubell, A. Ganesh, and A. McCallum. Energy and policy considerations for deep learning in NLP. arXiv preprint arXiv:1906.02243, 2019. URL https://arxiv.org/pdf/1906.02243.pdf. S. Vaswani, A. Mishkin, I. Laradji, M. Schmidt, G. Gidel, and S. Lacoste-Julien. Painless stochastic gradient: Interpolation, line-search, and convergence rates. Advances in neural information processing systems, 32, 2019. M. D. Zeiler. ADADELTA: An adaptive learning rate method.CoRR, abs/1212.5701, 2012. URL http://dblp.uni-trier.de/db/journals/corr/corr1212.html#abs-1212-5701 . 12",
      "meta_data": {
        "arxiv_id": "1909.13371v2",
        "authors": [
          "Kartik Chandra",
          "Audrey Xie",
          "Jonathan Ragan-Kelley",
          "Erik Meijer"
        ],
        "published_date": "2019-09-29T21:41:49Z",
        "pdf_url": "https://arxiv.org/pdf/1909.13371v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the tedious and error-prone task of manually tuning optimizer hyperparameters by proposing an automated method to compute hypergradients. It introduces a simple, elegant modification to backpropagation that leverages automatic differentiation (AD) to optimize not only the learning rate but also other hyperparameters (e.g., momentum coefficients) for various optimizers like SGD, Adam, AdaGrad, and RMSProp. A key contribution is the ability to recursively apply this method, creating arbitrarily tall 'towers of optimizers' that become increasingly robust to the initial choice of hyperparameters, thus reducing the burden on human practitioners. The method is shown to be efficient, with negligible computational overhead.",
        "methodology": "The core methodology involves modifying the standard backpropagation mechanism within a differentiable programming system to automatically compute 'hypergradients.' Instead of manually deriving partial derivatives for hyperparameters, the system refrains from detaching the hyperparameters from the computation graph during the weight update step, allowing backpropagation to compute gradients with respect to them. For recursive stacking, the hyperoptimizer itself is refactored to take another optimizer as a parameter, allowing it to optimize its own hyperparameters. Specific considerations for optimizers include clamping hyperparameters like Adam's beta values within their valid domain (0,1) using a scaled sigmoid and initializing Adam's ˆv0 to epsilon instead of zero to prevent division by zero.",
        "experimental_setup": "Experiments were conducted on various neural network architectures and datasets using a single NVIDIA TITAN Xp GPU. For basic properties and robustness, an MLP with one hidden layer (128 units, tanh activation, batch size 256) was trained on MNIST for 30 epochs. Adam, AdaGrad, and RMSProp optimizers were tested for 5 epochs. For hyperoptimization at scale, a ResNet-20 was trained on CIFAR-10 for 200-500 epochs, comparing with hand-engineered learning rate schedules. A character-level RNN (2-layer LSTM with 128 hidden nodes) was trained on the Tolstoy dataset for 50,000 gradient steps. Higher-order hyperoptimization was validated across all benchmarks (MLP on MNIST, CNN on CIFAR-10, RNN on Tolstoy) by varying initial base-level step sizes and increasing optimizer stack heights. Further scalability tests involved fine-tuning a pretrained ResNet-152 on the Caltech-256 dataset. Performance was measured by test error/accuracy and perplexity for RNNs.",
        "limitations": "One limitation is that hyperoptimizers currently cannot handle initial hyperparameters that are set \"far too high,\" leading to system instability and divergence before the hyperoptimizer can take effect. The implementation also requires careful management of the computation graph to avoid bugs like memory leaks (e.g., loggers must detach logged tensors) and incorrect gradients due to silently modified computation graphs by certain PyTorch modules (e.g., built-in LSTM).",
        "future_research_directions": "Future research directions include designing hyperoptimizers that are robust to initial hyperparameters set far too high, potentially requiring a deeper theoretical analysis of convergence. Another direction is to design differentiable programming languages where such methods can be expressed in a modular and composable manner, minimizing the risk of computation graph-related bugs. The paper also broadly implies the potential for reducing the energy consumption of hyperparameter searches through advances in on-line hyperparameter tuning."
      }
    },
    {
      "title": "Implicit differentiation of Lasso-type models for hyperparameter optimization",
      "abstract": "Setting regularization parameters for Lasso-type estimators is notoriously\ndifficult, though crucial in practice. The most popular hyperparameter\noptimization approach is grid-search using held-out validation data.\nGrid-search however requires to choose a predefined grid for each parameter,\nwhich scales exponentially in the number of parameters. Another approach is to\ncast hyperparameter optimization as a bi-level optimization problem, one can\nsolve by gradient descent. The key challenge for these methods is the\nestimation of the gradient with respect to the hyperparameters. Computing this\ngradient via forward or backward automatic differentiation is possible yet\nusually suffers from high memory consumption. Alternatively implicit\ndifferentiation typically involves solving a linear system which can be\nprohibitive and numerically unstable in high dimension. In addition, implicit\ndifferentiation usually assumes smooth loss functions, which is not the case\nfor Lasso-type problems. This work introduces an efficient implicit\ndifferentiation algorithm, without matrix inversion, tailored for Lasso-type\nproblems. Our approach scales to high-dimensional data by leveraging the\nsparsity of the solutions. Experiments demonstrate that the proposed method\noutperforms a large number of standard methods to optimize the error on\nheld-out data, or the Stein Unbiased Risk Estimator (SURE).",
      "full_text": "Implicit differentiation of Lasso-type models for hyperparameter optimization Quentin Bertrand* 1 Quentin Klopfenstein* 2 Mathieu Blondel3 Samuel Vaiter4 Alexandre Gramfort1 Joseph Salmon5 Abstract Setting regularization parameters for Lasso-type estimators is notoriously difﬁcult, though cru- cial in practice. The most popular hyperparam- eter optimization approach is grid-search using held-out validation data. Grid-search however re- quires to choose a predeﬁned grid for each pa- rameter, which scales exponentially in the num- ber of parameters. Another approach is to cast hyperparameter optimization as a bi-level opti- mization problem, one can solve by gradient de- scent. The key challenge for these methods is the estimation of the gradient w.r.t.the hyperpa- rameters. Computing this gradient via forward or backward automatic differentiation is possible yet usually suffers from high memory consump- tion. Alternatively implicit differentiation typi- cally involves solving a linear system which can be prohibitive and numerically unstable in high dimension. In addition, implicit differentiation usually assumes smooth loss functions, which is not the case for Lasso-type problems. This work introduces an efﬁcient implicit differentia- tion algorithm, without matrix inversion, tailored for Lasso-type problems. Our approach scales to high-dimensional data by leveraging the sparsity of the solutions. Experiments demonstrate that the proposed method outperforms a large num- ber of standard methods to optimize the error on held-out data, or the Stein Unbiased Risk Esti- mator (SURE). *Equal contribution 1Université Paris-Saclay, Inria, CEA, Palaiseau, France 2Institut Mathématique de Bourgogne, Univer- sité de Bourgogne, Dijon, France 3Google Research, Brain team, Paris, France 4CNRS and Institut Mathématique de Bourgogne, Université de Bourgogne, Dijon, France 5IMAG, Université de Montpellier, CNRS, Montpellier, France. Correspondence to: Quentin Bertrand <quentin.bertrand@inria.fr>, Quentin Klopfen- stein <quentin.klopfenstein@u-bourgogne.fr>. Proceedings of the 37 th International Conference on Machine Learning, Online, PMLR 119, 2020. Copyright 2020 by the au- thor(s). 1. Introduction In many statistical applications, the number of parame- ters p is much larger than the number of observations n. In such scenarios, a popular approach to tackle linear re- gression problems is to consider convex ℓ1-type penalties, used in Lasso (Tibshirani, 1996), Group-Lasso (Yuan and Lin, 2006), Elastic-Net (Zou and Hastie, 2005) or adap- tive Lasso (Zou, 2006). These Lasso-type estimators rely on regularization hyperparameters, trading data ﬁdelity against sparsity. Unfortunately, setting these hyperparame- ters is hard in practice: estimators based on ℓ1-type penal- ties are indeed more sensitive to the choice of hyperparam- eters than ℓ2 regularized estimators. To control for overﬁtting, it is customary to use different datasets for model training ( i.e., computing the regression coefﬁcients) and hyperparameter selection ( i.e., choosing the best regularization parameters). A metric, e.g., hold- out loss , is optimized on a validation dataset (Stone and Ramer, 1965). Alternatively one can rely on a statistical criteria that penalizes complex models such as AIC/BIC (Liu et al., 2011) or SURE (Stein Unbiased Risk Estima- tor, Stein 1981). In all cases, hyperparameters are tuned to optimize a chosen metric. The canonical hyperparameter optimization method is grid-search. It consists in ﬁtting and selecting the best model over a predeﬁned grid of parameter values. The complexity of grid-search is exponential with the number of hyperparameters, making it only competitive when the number of hyperparameters is small. Other hyperparameter selection strategies include random search (Bergstra and Bengio, 2012) and Bayesian optimization (Brochu et al., 2010; Snoek et al., 2012) that aims to learn an approxima- tion of the metric over the parameter space and rely on an exploration policy to ﬁnd the optimum. Another line of work for hyperparameter optimization (HO) relies on gradient descent in the hyperparameter space. This strategy has been widely explored for smooth objective functions (Larsen et al., 1996; Bengio, 2000; Larsen et al., 2012). The main challenge for this class of methods is estimating the gradient w.r.t.the hyperparame- ters. Gradient estimation techniques are mostly divided in two categories. Implicit differentiation requires the exact arXiv:2002.08943v3  [stat.ML]  3 Sep 2020Implicit differentiation of Lasso-type models for hyperparameter optimization solution of the optimization problem and involves the res- olution of a linear system (Bengio, 2000). This can be ex- pensive to compute and lead to numerical instabilities, es- pecially when the system is ill-conditioned (Lorraine et al., 2019). Alternatively, iterative differentiation computes the gradient using the iterates of an optimization algorithm. Backward iterative differentiation (Domke, 2012) is com- putationally efﬁcient when the number of hyperparameters is large. However it is memory consuming since it requires storing all intermediate iterates. In contrast, forward itera- tive differentiation (Deledalle et al., 2014; Franceschi et al., 2017) does not require storing the iterates but can be com- putationally expensive with a large number of hyperparam- eters; see Baydin et al. (2018) for a survey. This article proposes to investigate the use of these meth- ods to set the regularization hyperparameters in an auto- matic fashion for Lasso-type problems. To cover the cases of both low and high number of hyperparameters, two esti- mators are investigated, namely the Lasso and the weighted Lasso which have respectively one or as many parameters as features. Our contributions are as follows: • We show that forward iterative differentiation of block coordinate descent (BCD), a state-of-the-art solver for Lasso-type problems, converges towards the true gra- dient. Crucially, we show that this scheme converges linearly once the support is identiﬁed and that its limit does not depend of the initial starting point. • These results lead to the proposed algorithm (Algo- rithm 2) where the computation of the Jacobian is de- coupled from the computation of the regression co- efﬁcients. The later can be done with state-of-the-art convex solvers, and interestingly, it does not require solving a linear system, potentially ill-conditioned. • We show through an extensive benchmark on simu- lated and real high dimensional data that the proposed method outperforms state-of-the-art HO methods. Our work is somewhat similar to Gregor and LeCun (2010); Xin et al. (2016); Borgerding et al. (2017); Liu et al. (2018); Wu et al. (2019), where the solution is differenti- ated w.r.t. optimization parameters instead of the regular- ization parameter. However the goal is very different as they want to accelerate the optimization algorithm whereas we provide an efﬁcient algorithm to compute the gradient. Notation The design matrix is X ∈Rn×p (corresponding to nsamples and pfeatures) and the observation vector is y ∈Rn. The regularization parameter, possibly multivari- ate, is denoted by λ = (λ1,...,λ r)⊤ ∈Rr. We denote ˆβ(λ) ∈Rp the regression coefﬁcients associated to λ. We denote ˆJ(λ) ≜ (∇λˆβ(λ) 1 ,..., ∇λˆβ(λ) p )⊤∈Rp×r the weak Jacobian (Evans and Gariepy, 1992) of ˆβ(λ) w.r.t.λ. For a function ψ : Rp ×Rr →R with weak derivatives of order two, we denote by∇βψ(β,λ) ∈Rp(resp. ∇λ(β,λ) ∈Rr) its weak gradient w.r.t.the ﬁrst parameter (resp. the second parameter). The weak Hessian ∇2ψ(β,λ) is a matrix in R(p+r)×(p+r) which has a block structure ∇2ψ(β,λ) = (∇2 βψ(β,λ) ∇2 β,λψ(β,λ) ∇2 λ,βψ(β,λ) ∇2 λψ(β,λ) ) . The support of ˆβ(λ) (the indices of non-zero coefﬁcients) is denoted by ˆS(λ), and ˆs(λ) represents its cardinality (i.e., the number of non-zero coefﬁcients). The sign vec- tor sign ˆβ(λ) ∈Rp is the vector of component-wise signs (with the convention thatsign(0) = 0) of ˆβ(λ). Note that to ease the reading, we drop λin the notation when it is clear from the context and use ˆβ, ˆJ, ˆS and ˆs. The Mahalanobis distance of a vector x ∈Rp and a matrix A ≻0 is noted ∥x∥A ≜ √ x⊤A−1x. 2. Background 2.1. Problem setting To favor sparse coefﬁcients, we consider Lasso-type es- timators based on non-smooth regularization functions. Such problems consist in ﬁnding: ˆβ(λ) ∈arg min β∈Rp ψ(β,λ) . (1) The Lasso (Tibshirani, 1996) is recovered, with the number of hyperparameters set to r= 1: ψ(β,λ) = 1 2n∥y−Xβ∥2 2 + eλ∥β∥1 , (2) while the weighted Lasso (wLasso, Zou 2006, introduced to reduce the bias of the Lasso) has r= phyperparameters and reads: ψ(β,λ) = 1 2n∥y−Xβ∥2 2 + p∑ j=1 eλj|βj|. (3) Note that we adopt the hyperparameter parametrization of Pedregosa (2016), i.e., we write the regularization parame- ter as eλ. This avoids working with a positivity constraint in the optimization process and ﬁxes scaling issues in the line search. It is also coherent with the usual choice of a geometric grid for grid-search (Friedman et al., 2010). Remark 1. Other formulations could be investigated like Elastic-Net or non-convex formulation, e.g., MCP (Zhang, 2010). Our theory does not cover non-convex cases, though we illustrate that it behaves properly numerically. Handling such non-convex cases is left as a question for future work. The HO problem can be expressed as a nested bi-level op- timization problem. For a given differentiable criterion C: Rp ↦→R (e.g., hold-out loss or SURE), it reads:Implicit differentiation of Lasso-type models for hyperparameter optimization arg min λ∈Rr { L(λ) ≜ C ( ˆβ(λ) )} s.t. ˆβ(λ) ∈arg min β∈Rp ψ(β,λ) . (4) Note that SURE itself is not necessarily weakly differen- tiable w.r.t. ˆβ(λ). However a weakly differentiable approx- imation can be constructed (Ramani et al., 2008; Deledalle et al., 2014). Under the hypothesis that Problem (1) has a unique solution for every λ∈Rr, the function λ↦→ˆβ(λ) is weakly differentiable (Vaiter et al., 2013). Using the chain rule, the gradient of Lw.r.t.λthen writes: ∇λL(λ) = ˆJ⊤ (λ)∇C ( ˆβ(λ) ) . (5) Computing the weak Jacobian ˆJ(λ) of the inner problem is the main challenge, as once the hypergradient ∇λL(λ) has been computed, one can use usual gradient descent, λ(t+1) = λ(t) −ρ∇λL(λ(t)), for a step size ρ > 0. Note however that Lis usually non-convex and conver- gence towards a global minimum is not guaranteed. In this work, we propose an efﬁcient algorithm to compute ˆJ(λ) for Lasso-type problems, relying on improved forward dif- ferentiation. 2.2. Implicit differentiation (smooth case) Implicit differentiation, which can be traced back to Larsen et al. (1996), is based on the knowledge of ˆβ and requires solving a p×plinear system (Bengio, 2000, Sec. 4). Since then, it has been extensively applied in various contexts. Chapelle et al. (2002); Seeger (2008) used implicit differ- entiation to select hyperparameters of kernel-based mod- els. Kunisch and Pock (2013) applied it to image restora- tion. Pedregosa (2016) showed that each inner optimiza- tion problem could be solved only approximately, leverag- ing noisy gradients. Related to our work, Foo et al. (2008) applied implicit differentiation on a “weighted” Ridge-type estimator (i.e., a Ridge penalty with one λj per feature). Yet, all the aforementioned methods have a common draw- back : they are limited to the smooth setting, since they rely on optimality conditions for smooth optimization. They proceed as follows: if β ↦→ψ(β,λ) is a smooth convex function (for any ﬁxed λ) in Problem (1), then for all λ, the solution ˆβ(λ) satisﬁes the following ﬁxed point equation: ∇βψ ( ˆβ(λ),λ ) = 0 . (6) Then, this equation can be differentiated w.r.t.λ: ∇2 β,λψ( ˆβ(λ),λ) + ˆJ⊤ (λ)∇2 βψ( ˆβ(λ),λ) = 0. (7) Assuming that ∇2 βψ( ˆβ(λ),λ) is invertible this leads to a closed form solution for the weak Jacobian ˆJ(λ): ˆJ⊤ (λ) = −∇2 β,λψ ( ˆβ(λ),λ )( ∇2 βψ(β(λ),λ) )    p×p −1 , (8) which in practice is computed by solving a linear system. Unfortunately this approach cannot be generalized for non- smooth problems since Equation (6) no longer holds. 2.3. Implicit differentiation (non-smooth case) Related to our work Mairal et al. (2012) used implicit dif- ferentiation with respect to the dictionary ( X ∈Rn×p) on Elastic-Net models to perform dictionary learning. Regard- ing Lasso problems, the literature is quite scarce, see (Dos- sal et al., 2013; Zou et al., 2007) and (Vaiter et al., 2013; Tibshirani and Taylor, 2011) for a more generic setting encompassing weighted Lasso. General methods for gra- dient estimation of non-smooth optimization schemes ex- ist (Vaiter et al., 2017) but are not practical since they de- pend on a possibly ill-posed linear system to invert. Amos and Kolter (2017) have applied implicit differentiation on estimators based on quadratic objective function with lin- ear constraints, whereas Niculae and Blondel (2017) have used implicit differentiation on a smooth objective func- tion with simplex constraints. However none of these ap- proaches leverages the sparsity of Lasso-type estimators. 3. Hypergradients for Lasso-type problems To tackle hyperparameter optimization of non-smooth Lasso-type problems, we propose in this section an efﬁcient algorithm for hypergradient estimation. Our algorithm re- lies on implicit differentiation, thus enjoying low-memory cost, yet does not require to naively solve a (potentially ill-conditioned) linear system of equations. In the sequel, we assume access to a (weighted) Lasso solver, such as ISTA (Daubechies et al., 2004) or Block Coordinate De- scent (BCD, Tseng and Yun 2009, see also Algorithm 5). 3.1. Implicit differentiation Our starting point is the key observation that Lasso-type solvers induce a ﬁxed point iteration that we can leverage to compute a Jacobian. Indeed, proximal BCD algorithms (Tseng and Yun, 2009), consist in a local gradient step com- posed with a soft-thresholding step (ST),e.g., for the Lasso, for j ∈1,...,p : βj ←ST ( βj −X⊤ :,j(Xβ −y) ∥X:,j∥2 , neλ ∥X:,j∥2 ) (9) where ST(t,τ) = sign(t)·(|t|−τ)+ for any t∈R and τ ≥ 0 (extended for vectors component-wise). The solution ofImplicit differentiation of Lasso-type models for hyperparameter optimization the optimization problem satisﬁes, for anyα> 0, the ﬁxed- point equation (Combettes and Wajs, 2005, Prop. 3.1), for j ∈1,...,p : ˆβ(λ) j = ST ( ˆβ(λ) j −1 αX⊤ j,:(Xˆβ(λ) −y),neλ α ) . (10) The former can be differentiated w.r.t. λ, see Lemma A.1 in Appendix, leading to a closed form solution for the Ja- cobian J(λ) of the Lasso and the weighted Lasso. Proposition 1(Adapting Vaiter et al. 2013, Thm. 1) . Let ˆSbe the support of the vector ˆβ(λ). Suppose that X⊤ ˆSXˆS ≻0 , then a weak Jacobian ˆJ = ˆJ(λ) of the Lasso writes: ˆJˆS = −neλ( X⊤ ˆSXˆS )−1 sign ˆβˆS, (11) ˆJˆSc = 0 , (12) and for the weighted Lasso: ˆJˆS,ˆS = − ( X⊤ ˆSXˆS )−1 diag ( neλˆS ⊙sign ˆβˆS ) (13) ˆJj1,j2 = 0 if j1 /∈ˆSor if j2 /∈ˆS . (14) The proof of Proposition 1 can be found in Appendix A.1. Note that the positivity condition in Proposition 1 is satis- ﬁed if the (weighted) Lasso has a unique solution. More- over, even for multiple solutions cases, there exists at least one satisfying the positivity condition (Vaiter et al., 2013). Proposition 1 shows that the Jacobian of the weighted Lasso ˆJ(λ) ∈ Rp×p is row and column sparse. This is key for algorithmic efﬁciency. Indeed, a priori, one has to store a possibly dense p×p matrix, which is prohibitive when pis large. Proposition 1 leads to a simple algorithm (see Algorithm 1) to compute the Jacobian in a cheap way, as it only requires storing and inverting an ˆs×ˆs matrix. Even if the linear system to solve is of size ˆs×ˆs, instead of p×pfor smooth objective function, the system to invert can be ill-conditioned, especially when a large support size ˆsis encountered. This leads to numerical instabilities and slows down the resolution (see an illustration in Figure 2). Forward (Algorithm 3 in Appendix) and backward (Algo- rithm 4 in Appendix) iterative differentiation, which do not require solving linear systems, can overcome these issues. 3.2. Link with iterative differentiation Iterative differentiation in the ﬁeld of hyperparameter set- ting can be traced back to Domke (2012) who derived a backward differentiation algorithm for gradient descent, heavy ball and L-BFGS algorithms applied to smooth loss functions. Agrawal et al. (2019) generalized it to a spe- ciﬁc subset of convex programs. Maclaurin et al. (2015) derived a backward differentiation for stochastic gradient Algorithm 1IMPLICIT DIFFERENTIATION input : X ∈Rn×p,y ∈Rn,λ ∈R,niter ∈N // jointly compute coef. and Jacobian if Lasso then Get ˆβ = Lasso(X,y,λ,n iter) and its support ˆS. ˆJ = 0p ˆJˆS = −neλ(X⊤ ˆSXˆS)−1 sign ˆβˆS if wLasso then Get ˆβ = wLasso(X,y,λ,n iter) and its support ˆS. ˆJ= 0p×p ˆJˆS,ˆS = −(X⊤ ˆSXˆS)−1 diag(neλˆS ⊙sign ˆβˆS) return ˆβ, ˆJ descent. On the other hand Deledalle et al. (2014) used forward differentiation of (accelerated) proximal gradient descent for hyperparameter optimization with non-smooth penalties. Franceschi et al. (2017) proposed a benchmark of forward mode versus backward mode, varying the num- ber of hyperparameters to learn. Frecon et al. (2018) cast the problem of inferring the groups in a group-Lasso model as a bi-level optimization problem and solved it using back- ward differentiation. Forward differentiation consists in differentiating each step of the algorithm (w.r.t.λin our case). For the Lasso solved with BCD it amounts differentiating Equation (9), and leads to the following recursive equation for the Jacobian, for j ∈1,...p , with zj = βj −X⊤ :,j(Xβ −y)/∥X:,j∥2: Jj ←∂1 ST ( zj, neλ ∥X:,j∥2 )( Jj − 1 ∥X:,j∥2 X⊤ :,jXJ ) + ∂2 ST ( zj, neλ ∥X:,j∥2 ) neλ ∥X:,j∥2 , (15) see Algorithm 3 (in Appendix) for full details. Our proposed algorithm uses the fact that after a ﬁ- nite number of epochs ∂1 ST(zj,neλ/∥X:,j∥2) and ∂2 ST(zj,neλ/∥X:,j∥2) are constant (they no no longer depends on the current β). Indeed, the sign of ˆβ is iden- tiﬁed after a ﬁnite number of iterations thus the partial derivatives are constant. It is then possible to decouple the computation of the Jacobian by only solving Problem (1) in a ﬁrst step and then apply the forward differentiation recur- sion steps, see Algorithm 2. This can be seen as the forward counterpart in a non-smooth case of the recent paper Lor- raine et al. (2019). An additional beneﬁt of such updates is that they can be restricted to the (current) support, which leads to faster Jacobian computation. We now show that the Jacobian computed using forward differentiation and our method, Algorithm 2, converges to- ward the true Jacobian.Implicit differentiation of Lasso-type models for hyperparameter optimization Proposition 2. Assuming the Lasso solution (Prob- lem (2)) (or weighted Lasso Problem (3)) is unique, then Algorithms 2 and 3 converge toward the Jaco- bian ˆJ deﬁned in Proposition 1. Algorithm 3 com- putes the Jacobian along with the regression coefﬁ- cients, once the support has been identiﬁed, the Jaco- bian converges linearly. Algorithm 2 computes ﬁrst the coefﬁcients ˆβ and then the Jacobian ˆJ, provided that the support has been identiﬁed in the ﬁrst step, the convergence is linear in the second, with the same rate as Algorithm 3: ∥J(k+1) ˆS −ˆJ∥(X⊤ :,ˆSX:,ˆS)−1 ≤Ck∥J(k) ˆS −ˆJ∥(X⊤ :,ˆSX:,ˆS)−1 where C = ∥A(jˆs) ...A (j1)∥2 <1, j1,...,j ˆs are the indices of the support of ˆβin increasing order and A(js) = Idˆs− ( X⊤ :,ˆSX:,ˆS )1/2 :,js ∥X:,js∥ ( X⊤ :,ˆSX:,ˆS )1/2 js,: ∥X:,js∥ ∈Rˆs×ˆs. Proof of Proposition 2 can be found in Appendix A.2 and A.3. Remark 3. Uniqueness. As proved in Tibshirani (2013, Lem. 3 and 4) the set of (pathological) lambdas where the Lasso solution is not unique is typically empty. More- over if the Lasso solution is not unique, there could be a non-continuous solution path λ↦→ˆβ(λ), leaving only non- gradient based methods available. Even if Proposition 2 does not provide theoretical guarantees in such a patholog- ical setting, one can still apply Algorithms 2 and 3, see Appendix E.1 for experiments in this settings. Remark 4. Rate for the backward differentiation. The backward and forward differentiation compute the same quantity: ∇λL(λ), but the backward differentiation di- rectly computes the product given in Equation (5) leading to the gradient ofL(λ). Proposition 2 provides rates for the convergence of the Jacobian Jwhich leads to rates for the gradient i.e., for the backward algorithm as well. As an illustration, Figure 1 shows the times of computa- tion of a single gradient ∇λL(λ) and the distance to “op- timum” of this gradient as a function of the number of it- erations in the inner optimization problem for the forward iterative differentiation (Algorithm 3), the backward iter- ative differentiation (Algorithm 4), and the proposed algo- rithm (Algorithm 2). The backward iterative differentiation is several order of magnitude slower than the forward and our implicit forward method. Moreover, once the support has been identiﬁed (after 20 iterations) the proposed im- plicit forward method converges faster than other methods. Note also that in Propositions 1 and 2 the Jacobian for the Imp. F. Iterdiﬀ. (ours)F. Iterdiﬀ.B. Iterdiﬀ. 20 40 60 Number of iterations 10−1 100 101 Times (s) 20 40 60 Number of iterations 10−7 10−5 Objective minus optimum Figure 1.Time to compute a single gradient(Synthetic data, Lasso, n,p = 1000,2000). Inﬂuence on the number of iterations of BCD (in the inner optimization problem of Problem (4)) on the computation time (left) and the distance to “optimum” of the gra- dient ∇λL(λ)(right) for the Lasso estimator. The “optimum” is here the gradient given by implicit differentiation (Algorithm 1). Lasso only depends on the support (i.e., the indices of the non-zero coefﬁcients) of the regression coefﬁcients ˆβ(λ). In other words, once the support of ˆβ(λ) is correctly identi- ﬁed, even if the value of the non-zeros coefﬁcients are not correctly estimated, the Jacobian is exact, see Sun et al. (2019) for support identiﬁcation guarantees. 4. Experiments Our Python code is released as an open source package: https://github.com/QB3/sparse-ho. All the experiments are written in Python using Numba (Lam et al., 2015) for the critical parts such as the BCD loop. We com- pare our gradient computation technique against other com- petitors (see the competitors section) on the HO problem (Problem (4)). Solving the inner optimization problem.Note that our proposed method, implicit forward differentiation, has the appealing property that it can be used with any solver. For instance for the Lasso one can combine the proposed al- gorithm with state of the art solver such as Massias et al. (2018) which would be tedious to combine with iterative differentiation methods. However for the comparison to be fair, for all methods we have used the same vanilla BCD algorithm (recalled in Algorithm 5). We stop the Lasso- types solver when f(β(k+1))−f(β(k)) f(0) <ϵtol ,where f is the cost function of the Lasso or wLasso and ϵtol a given toler- ance. The tolerance is ﬁxed at ϵtol = 10−5 for all methods throughout the different benchmarks. Line search. For each hypergradient-based method, the gradient step is combined with a line-search strategy fol- lowing the work of Pedregosa (2016)1. Initialization. Since the function to optimize Lis not con- 1see https://github.com/fabianp/hoag for detailsImplicit differentiation of Lasso-type models for hyperparameter optimization Table 1.Summary of cost in time and space for each method Mode Computed Space Time Space Time quantity (Lasso) (Lasso) (wLasso) (wLasso) F. Iterdiff. J O(p) O(2npniter) O(p2) O(np2niter) B. Iterdiff. J⊤v O(2pniter) O(npniter + np2niter) O(p2niter) O(npniter + np2niter) Implicit J⊤v O(p) O(npniter + ˆs3) O(p+ ˆs2) O(npniter + ˆs3) Imp. F. Iterdiff. J O(p) O(npniter + nˆsniter_jac) O(p+ ˆs2) O(npniter + nˆs2nit_jac) Algorithm 2IMP. F. I TERDIFF . (proposed) input : X ∈Rn×p,y ∈Rn,λ ∈R,niter,niter_jac ∈N init : J= 0 // sequentially compute coef. & Jacobian if Lasso then Get ˆβ = Lasso(X,y,λ,n iter) and its support ˆS. dr= −X:,ˆSJˆS // trick for cheap updates if wLasso then Get ˆβ = wLasso(X,y,λ,n iter) and its support ˆS. dr= −X:,ˆSJˆS,ˆS for k= 0,...,n iter_jac −1 do for j ∈ˆSdo if Lasso then Jold = Jj // trick for cheap update // diff. Equation (9) w.r.t. λ Jj += X⊤ :,jdr ∥X:,j∥2 − neλ ∥X:,j∥2 sign ˆβj // O(n) dr−= X:,j(Jj,: −Jold) // O(n) if wLasso then Jold = Jj,: // trick for cheap update // diff. Equation (9) w.r.t. λ Jj,ˆS += 1 ∥X:,j∥2 X⊤ :,jdr // O(n×ˆs) Jj,j −= neλj ∥X:,j∥2 sign ˆβj // O(1) dr−= X:,j ⊗(Jj,: −Jold) // O(n×ˆs) return ˆβ,J vex, initialization plays a crucial role in the ﬁnal solution as well as the convergence of the algorithm. For instance, initializing λ = λinit in a ﬂat zone of L(λ) could lead to slow convergence. In the numerical experiments, the Lasso is initialized with λinit = λmax −log(10), where λmax is the smallest λsuch that 0 is a solution of Problem (2). Competitors. In this section we compare the empirical performance of implicit forward differentiation algorithm to different competitors. Competitors are divided in two categories. Firstly, the ones relying on hyperparameter gra- dient: • Imp. F. Iterdiff. : implicit forward differentiation (proposed) described in Algorithm 2. • Implicit: implicit differentiation, which requires solv- ing a ˆs×ˆslinear system as described in Algorithm 1. • F. Iterdiff.: forward differentiation (Deledalle et al., 2014; Franceschi et al., 2017) which jointly computes the regression coefﬁcients ˆβas well as the Jacobian ˆJ as shown in Algorithm 3. Secondly, the ones not based on hyperparameter gradient: • Grid-search: as recommended by Friedman et al. (2010), we use 100 values on a uniformly-spaced grid from λmax to λmax −4 log(10). • Random-search: we sample uniformly at random 100 values taken on the same interval as for the Grid-search [λmax −4 log(10);λmax], as suggested by Bergstra et al. (2013). • Lattice Hyp.: lattice hypercube sampling (Bousquet et al., 2017), combines the idea of grid-search and random-search. We used the sampling scheme of Bouhlel et al. (2019) and their code 2 to sample the points to evaluate the function on. • Bayesian: sequential model based optimization (SMBO) using a Gaussian process to model the objec- tive function. We used the implementation of Bergstra et al. (2013).3 The constraints space for the hyperpa- rameter search was set in[λmax −4 log(10);λmax], and the expected improvement (EI) was used as aquisition function. The cost and the quantity computed by each algorithm can be found in Table 1. The backward differentiation (Domke, 2012) is not included in the benchmark in Figure 2 since it was several orders of magnitude slower than the other techniques (see Figure 1). This is due to the high cost of the BCD algorithm in backward mode, see Table 1. 4.1. Application to held-out loss When using the held-out loss, each dataset(X,y) is split in 3 equal parts: the training set (Xtrain,ytrain), the validation set (Xval,yval) and the test set (Xtest,ytest). 2https://github.com/SMTorg/smt 3https://github.com/hyperopt/hyperoptImplicit differentiation of Lasso-type models for hyperparameter optimization (Lasso, held-out criterion). For the Lasso and the held-out loss, the bilevel optimization Problem (4) reads: arg min λ∈R ∥yval −Xval ˆβ(λ)∥2 (16) s.t. ˆβ(λ) ∈arg min β∈Rp 1 2n∥ytrain −Xtrainβ∥2 2 + eλ∥β∥1 . Figure 2 (top) shows on 3 datasets (see Appendix D for dataset details) the distance to the “optimum” of ∥yval − Xval ˆβ(λ)∥2 as a function of time. Here the goal is to ﬁnd λ solution of Problem (16). The “optimum” is chosen as the minimum of ∥yval −Xval ˆβ(λ)∥2 among all the meth- ods. Figure 2 (bottom) shows the loss ∥ytest −Xtest ˆβ(λ)∥2 on the test set (independent from the training set and the validation set). This illustrates how well the estimator gen- eralizes. Firstly, it can be seen that on all datasets the pro- posed implicit forward differentiation outperforms forward differentiation which illustrates Proposition 2 and corrobo- rates the cost of each algorithm in Table 1. Secondly, it can be seen that on the 20news dataset (Figure 2, top) the im- plicit differentiation (Algorithm 1) convergence is slower than implicit forward differentiation, forward differentia- tion, and even slower than the grid-search. In this case, this is due to the very slow convergence of the conjugate gra- dient algorithm (Nocedal and Wright, 2006) when solving the ill-conditioned linear system in Algorithm 1. (MCP , held-out criterion). We also applied our algorithm on an estimator based on a non-convex penalty: the MCP (Zhang, 2010) with 2 hyperparameters. Since the penalty is non-convex the estimator may not be continuous w.r.t.hy- perparameters and the theory developed above does not hold. However experimentally implicit forward differen- tiation outperforms forward differentiation for the HO, see Appendix C for full details. 4.2. Application to another criterion: SURE Evaluating models on held-out data makes sense if the de- sign is formed from random samples as it is often consid- ered in supervised learning. However, this assumption does not hold for certain kinds of applications in signal or image processing. For these applications, the held-out loss cannot be used as the criterion for optimizing the hyperparame- ters of a given model. In this case, one may use a proxy of the prediction risk, like the Stein Unbiased Risk Estimation (SURE, Stein (1981)). The SURE is an unbiased estimator of the prediction risk under weak differentiable conditions. The drawback of this criterion is that it requires the knowl- edge of the variance of the noise. The SURE is deﬁned as follows: SURE(λ) =∥y−Xˆβ(λ)∥2−nσ2+2σ2dof( ˆβ(λ)) , where the degrees of freedom (dof Efron 1986) is deﬁned as dof( ˆβ(λ)) =∑n i=1 cov(yi,(Xˆβ(λ))i)/σ2 .The dof can be seen a measure of the complexity of the model, for in- stance for the Lasso dof ( ˆβ(λ)) = ˆs, see Zou et al. (2007). The SURE can thus be seen as a criterion trading data- ﬁdelity against model complexity. However, the dof is not differentiable (not even continuous in the Lasso case), yet it is possible to construct a weakly differentiable ap- proximation of it based on Finite Differences Monte-Carlo (see Deledalle et al. 2014 for full details), with ϵ >0 and δ∼N(0,Idn): dofFDMC(y,λ,δ,ϵ ) =1 ϵ⟨Xˆβ(λ)(y+ ϵδ) −Xˆβ(λ)(y),δ⟩. We use this smooth approximation in the bi-level optimiza- tion problem to ﬁnd the best hyperparameter. The bi-level optimization problem then reads: arg min λ∈R ∥y−Xˆβ(λ)∥2 + 2σ2dofFDMC(y,λ,δ,ϵ ) (17) s.t. ˆβ(λ)(y) ∈arg min β∈Rp 1 2n∥y−Xβ∥2 2 + eλ∥β∥1 ˆβ(λ)(y+ ϵδ) ∈arg min β∈Rp 1 2n∥y+ ϵδ−Xβ∥2 2 + eλ∥β∥1 Note that solving this problem requires the computation of two (instead of one for the held-out loss) Jacobians w.r.t.λ of the solution ˆβ(λ) at the points yand y+ ϵδ. (Lasso, SURE criterion). To investigate the estimation per- formance of the implicit forward differentiation in com- parison to the competitors described above, we used as metric the (normalized) Mean Squared Error (MSE) de- ﬁned as MSE ≜ ∥ˆβ−β∗∥2/∥β∗∥2. The entries of the design matrix X ∈Rn×p are i.i.d. random Gaussian vari- ables N(0,1). The number of rows is ﬁxed to n = 100. Then, we generated β∗with 5 non-zero coefﬁcients equals to 1. The vector y was computed by adding to Xβ∗addi- tive Gaussian noise controlled by the Signal-to-Noise Ra- tio: SNR ≜ ∥Xβ∗∥/∥y−Xβ∗∥(here SNR = 3). Fol- lowing Deledalle et al. (2014), we set ϵ = 2σ/n0.3. We varied the number of featurespbetween 200 and 10,000 on a linear grid of size 10. For a ﬁxed number of features, we performed 50 repetitions and each point of the curves rep- resents the mean of these repetitions. Comparing efﬁciency in time between methods is difﬁcult since they are not di- rectly comparable. Indeed, grid-search and random-search discretize the HO space whereas others methods work in the continuous space which is already an advantage. How- ever, to be able to compare the hypergradient methods and possibly compare them to the others, we computed the to- tal amount of time for a method to return its optimal value of λ. In order to have a fair comparison, we compared 50 evaluations of the line-search for each hypergradient meth- ods, 50 evaluations of the Bayesian methods and ﬁnally 50 evaluations on ﬁxed or random grid. We are aware that the cost of each of these evaluations is not the same but it al- lows to see that our method stays competitive in time with optimizing one parameter. Moreover we will also see that our method scales better with a large number of hyperpa- rameters to optimize.Implicit differentiation of Lasso-type models for hyperparameter optimization Imp. F. Iterdiﬀ. (ours) Implicit F. Iterdiﬀ. Grid-search Bayesian Random-search Lattice Hyp. 0.0 0.5 1.0 10−5 10−4 10−3 10−2 10−1 100 Objective minus optimum rcv1 (p = 19, 959) 0 5 10 15 10−3 10−2 10−1 100 101 102 20news (p = 130, 107) 0 100 200 300 10−4 10−3 10−2 10−1 100 101 ﬁnance (p = 1, 668, 737) 0.0 0.5 1.0 Time (s) 10−1 100 Loss on test set 0 5 10 15 Time (s) 101 102 0 100 200 300 Time (s) 10−1 100 101 Figure 2.Computation time for the HO of the Lasso on real data.Distance to “optimum” (top) and performance (bottom) on the test set for the Lasso for 3 different datasets: rcv1, 20news and ﬁnance. Imp. F. Iterdiﬀ. (ours) Implicit F. Iterdiﬀ. Grid-search Bayesian Random-search 200 2500 5000 7500 10000 Number of features (p) 0.000 0.001 0.002 0.003 0.004 relative MSE 200 2500 5000 7500 10000 Number of features (p) 10−1 100 101 102 Time (s) Figure 3.Lasso: estimation performance. Estimation relative Mean Squared Error (left) and running time (right) as a function of the number of features for the Lasso model. Figure 3 shows the inﬂuence of the number of features on the relative MSE (ie. MSE of a method minus the MSE of our implicit forward method) and the computation time. First, MSE of all gradient based methods is lower than the other methods which means that ˆβ(λ) leads to a better es- timation when λ is chosen via the gradient based meth- ods. This illustrates that continuous optimization for hy- perparameter selection leads to better estimation perfor- mance than discrete or Bayesian optimization. Yet, the running time of our proposed method is the lowest of all hypergradient-based strategies and competes with the grid- search and the random-search. (Weighted Lasso vs Lasso, SURE criterion). As our method leverages the sparsity of the solution, it can be used for HO with a large number of hyperparameters, contrary to classi- cal forward differentiation. The weighted Lasso (wLasso, Zou 2006) has p hyperparameters and was introduced to reduce the bias of the Lasso. However setting the phyper- parameters is impossible with grid-search. Figure 4 shows the estimation MSE and the running time of the different methods to obtain the hyperparameter val- ues as a function of the number of features used to simu- late the data. The simulation setting is here the same as for the Lasso problems investigated in Figure 3 ( n = 100, SNR = 3). We compared the classical Lasso estimator and the weighted Lasso estimator where the regularization hy- perparameter was chosen using implicit forward differenti- ation and the forward iterative differentiation as described in Algorithm 3. Problem (4) is not convex for the weighted Lasso and a descent algorithm like ours can be trapped in local minima, crucially depending on the starting point λinit. To alleviate this problem, we introduced a regular- ized version of Problem (4): arg min λ∈R C ( ˆβ(λ) ) + γ p∑ j λ2 j s.t. ˆβ(λ) ∈arg min β∈Rp ≜ ψ(β,λ) . (18) The solution obtained by solving Equation (18) is then used as the initialization λ(0) for our algorithm. In this experiment the regularization term is constant γ =Implicit differentiation of Lasso-type models for hyperparameter optimization Lasso F. Iterdiﬀ. Lasso Implicit Lasso Backward Lasso Imp. F. Iterdiﬀ. (ours) wLasso F. Iterdiﬀ. wLasso Implicit wLasso Backward wLasso Imp. F. Iterdiﬀ. (ours) 200 2500 5000 7500 10000 Number of features (p) 0.00 0.05 0.10 0.15 MSE 200 2500 5000 7500 10000 Number of features (p) 10−1 100 101 102 103 Time (s) Figure 4.Lasso vs wLasso.Estimation Mean Squared Error (left) and running (right) of competitors as a function of the number of features for the weighted Lasso and Lasso models. C(β(λmax))/10. We see in Figure 4 that the weighted Lasso gives a lower MSE than the Lasso and allows for a better recovery of β∗. This experiment shows that the amount of time needed to obtain the vector of hyperparameters of the weighted Lasso via our algorithm is in the same range as for obtaining the unique hyperparameter of the Lasso prob- lem. It also shows that our proposed method is much faster than the naive way of computing the Jacobian using for- ward or backward iterative differentiation. The implicit dif- ferentiation method stays competitive for the wLasso due to the small support of the solution and hence a small ma- trix to inverse. A maximum running time threshold was used for this experiment checking the running time at each line-search iteration, explaining why the forward differen- tiation and backward differentiation of the wLasso does not explode in time on Figure 4. Conclusion In this work we studied the performance of several methods to select hyperparameters of Lasso-type estimators show- ing results for the Lasso and the weighted Lasso, which have respectively one or phyperparameters. We exploited the sparsity of the solutions and the speciﬁc structure of the iterates of forward differentiation, leading to our im- plicit forward differentiation algorithm that computes efﬁ- ciently the full Jacobian of these estimatorsw.r.t.the hyper- parameters. This allowed us to select them through a stan- dard gradient descent and have an approach that scales to a high number of hyperparameters. Importantly, contrary to a classical implicit differentiation approach, the proposed algorithm does not require solving a linear system. Fi- nally, thanks to its two steps nature, it is possible to lever- age in the ﬁrst step the availability of state-of-the-art Lasso solvers that make use of techniques such as active sets or screening rules. Such algorithms, that involve calls to in- ner solvers run on subsets of features, are discontinuous w.r.t.hyperparameters which would signiﬁcantly challenge a single step approach based on automatic differentiation. Acknowledgments This work was funded by ERC Start- ing Grant SLAB ERC-StG-676943 and ANR GraVa ANR- 18-CE40-0005.Implicit differentiation of Lasso-type models for hyperparameter optimization References A. Agrawal, B. Amos, S. Barratt, S. Boyd, S. Diamond, and J. Z. Kolter. Differentiable convex optimization layers. In Advances in neural information processing systems , pages 9558–9570, 2019. B. Amos and J. Z. Kolter. Optnet: Differentiable optimiza- tion as a layer in neural networks. In ICML, volume 70, pages 136–145, 2017. A. G. Baydin, B. A. Pearlmutter, A. A. Radul, and J. M. Siskind. Automatic differentiation in machine learning: a survey. J. Mach. Learn. Res., 18(153):1–43, 2018. Y . Bengio. Gradient-based optimization of hyperparame- ters. Neural computation, 12(8):1889–1900, 2000. J. Bergstra and Y . Bengio. Random search for hyper- parameter optimization. J. Mach. Learn. Res., 2012. J. Bergstra, D. Yamins, and D. D. Cox. Hyperopt: A python library for optimizing the hyperparameters of machine learning algorithms. In Proceedings of the 12th Python in science conference, pages 13–20, 2013. M. Borgerding, P. Schniter, and S. Rangan. Amp-inspired deep networks for sparse linear inverse problems. IEEE Transactions on Signal Processing , 65(16):4293–4308, 2017. M. A. Bouhlel, J. T. Hwang, N. Bartoli, R. Lafage, J. Mor- lier, and J. R. R. A. Martins. A python surrogate model- ing framework with derivatives. Advances in Engineer- ing Software, page 102662, 2019. ISSN 0965-9978. doi: https://doi.org/10.1016/j.advengsoft.2019.03.005. O. Bousquet, S. Gelly, K. Kurach, O. Teytaud, and D. Vin- cent. Critical hyper-parameters: No random, no cry. arXiv preprint arXiv:1706.03200, 2017. P. Breheny and J. Huang. Coordinate descent algorithms for nonconvex penalized regression, with applications to biological feature selection. Ann. Appl. Stat. , 5(1):232, 2011. E. Brochu, V . M. Cora, and N. De Freitas. A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical re- inforcement learning. 2010. O. Chapelle, V . Vapnik, O. Bousquet, and S. Mukherjee. Choosing multiple parameters for support vector ma- chines. Machine learning, 46(1-3):131–159, 2002. P. L. Combettes and V . R. Wajs. Signal recovery by proxi- mal forward-backward splitting. Multiscale Modeling & Simulation, 4(4):1168–1200, 2005. I. Daubechies, M. Defrise, and C. De Mol. An iterative thresholding algorithm for linear inverse problems with a sparsity constraint. Comm. Pure Appl. Math., 57(11): 1413–1457, 2004. C.-A. Deledalle, S. Vaiter, J. Fadili, and G. Peyré. Stein Unbiased GrAdient estimator of the Risk (SUGAR) for multiple parameter selection. SIAM J. Imaging Sci. , 7 (4):2448–2487, 2014. J. Domke. Generic methods for optimization-based model- ing. In AISTATS, volume 22, pages 318–326, 2012. C. Dossal, M. Kachour, M.J. Fadili, G. Peyré, and C. Ches- neau. The degrees of freedom of the lasso for general design matrix. Statistica Sinica, 23(2):809–828, 2013. B. Efron. How biased is the apparent error rate of a pre- diction rule? J. Amer. Statist. Assoc., 81(394):461–470, 1986. L. C. Evans and R. F. Gariepy. Measure theory and ﬁne properties of functions. CRC Press, 1992. C. S. Foo, C. B. Do, and A. Y . Ng. Efﬁcient multiple hyper- parameter learning for log-linear models. InAdvances in neural information processing systems, pages 377–384, 2008. L. Franceschi, M. Donini, P. Frasconi, and M. Pontil. For- ward and reverse gradient-based hyperparameter opti- mization. In ICML, pages 1165–1173, 2017. J. Frecon, S. Salzo, and M. Pontil. Bilevel learning of the group lasso structure. InAdvances in Neural Information Processing Systems, pages 8301–8311, 2018. J. Friedman, T. J. Hastie, and R. Tibshirani. Regulariza- tion paths for generalized linear models via coordinate descent. J. Stat. Softw., 33(1):1–22, 2010. K. Gregor and Y . LeCun. Learning fast approximations of sparse coding. In ICML, pages 399–406, 2010. E. Hale, W. Yin, and Y . Zhang. Fixed-point continuation for ℓ1-minimization: Methodology and convergence. SIAM J. Optim., 19(3):1107–1130, 2008. K. Kunisch and T. Pock. A bilevel optimization approach for parameter learning in variational models. SIAM J. Imaging Sci., 6(2):938–983, 2013. S. K. Lam, A. Pitrou, and S. Seibert. Numba: A LLVM- based Python JIT Compiler. In Proceedings of the Sec- ond Workshop on the LLVM Compiler Infrastructure in HPC, pages 1–6. ACM, 2015.Implicit differentiation of Lasso-type models for hyperparameter optimization J. Larsen, L. K. Hansen, C. Svarer, and M. Ohlsson. Design and regularization of neural networks: the optimal use of a validation set. In Neural Networks for Signal Process- ing VI. Proceedings of the 1996 IEEE Signal Processing Society Workshop, 1996. J. Larsen, C. Svarer, L. N. Andersen, and L. K. Hansen. Adaptive regularization in neural network modeling. In Neural Networks: Tricks of the Trade - Second Edition , pages 111–130. Springer, 2012. J. Liu, X. Chen, Z. Wang, and W. Yin. Alista: Analytic weights are as good as learned weights in lista. In Inter- national Conference on Learning Representations, 2018. W. Liu, Y . Yang, et al. Parametric or nonparametric? a parametricness index for model selection. Ann. Statist., 39(4):2074–2102, 2011. J. Lorraine, P. Vicol, and D. Duvenaud. Optimizing millions of hyperparameters by implicit differentiation. arXiv preprint arXiv:1911.02590, 2019. D. Maclaurin, D. Duvenaud, and Ryan Adams. Gradient- based hyperparameter optimization through reversible learning. In ICML, volume 37, pages 2113–2122, 2015. J. Mairal, F. Bach, and J. Ponce. Task-driven dictionary learning. IEEE Trans. Pattern Anal. Mach. Intell., 34(4): 791–804, 2012. M. Massias, A. Gramfort, and J. Salmon. Celer: a Fast Solver for the Lasso with Dual Extrapolation. In ICML, volume 80, pages 3315–3324, 2018. M. Massias, S. Vaiter, A. Gramfort, and J. Salmon. Dual extrapolation for sparse generalized linear models.arXiv preprint arXiv:1907.05830, 2019. V . Niculae and M. Blondel. A regularized framework for sparse and structured neural attention. In Advances in neural information processing systems , pages 3338– 3348, 2017. J. Nocedal and S. J. Wright. Numerical optimization . Springer Series in Operations Research and Financial Engineering. Springer, New York, second edition, 2006. F. Pedregosa. Hyperparameter optimization with approxi- mate gradient. In ICML, 2016. S. Ramani, T. Blu, and M. Unser. Monte-Carlo SURE: a black-box optimization of regularization parameters for general denoising algorithms. IEEE Trans. Image Pro- cess., 17(9):1540–1554, 2008. M. W. Seeger. Cross-validation optimization for large scale structured classiﬁcation kernel methods. J. Mach. Learn. Res., 9:1147–1178, 2008. J. Snoek, H. Larochelle, and R. P. Adams. Practical bayesian optimization of machine learning algorithms. In Advances in neural information processing systems , 2012. E. Soubies, L. Blanc-Féraud, and G. Aubert. A uniﬁed view of exact continuous penalties for ℓ2-ℓ0 minimiza- tion. SIAM J. Optim., 27(3):2034–2060, 2017. C. M. Stein. Estimation of the mean of a multivariate nor- mal distribution. Ann. Statist., 9(6):1135–1151, 1981. L. R. A. Stone and J.C. Ramer. Estimating W AIS IQ from Shipley Scale scores: Another cross-validation. Journal of clinical psychology, 21(3):297–297, 1965. Y . Sun, H. Jeong, J. Nutini, and M. Schmidt. Are we there yet? manifold identiﬁcation of gradient-related proxi- mal methods. In AISTATS, volume 89, pages 1110–1119, 2019. R. Tibshirani. Regression shrinkage and selection via the lasso. J. R. Stat. Soc. Ser. B Stat. Methodol., 58(1):267– 288, 1996. R. J. Tibshirani. The lasso problem and uniqueness. Elec- tron. J. Stat., 7:1456–1490, 2013. R. J. Tibshirani and J. Taylor. The solution path of the generalized lasso. Ann. Statist., 39(3):1335–1371, 2011. P. Tseng and S. Yun. Block-coordinate gradient descent method for linearly constrained nonsmooth separable optimization. J. Optim. Theory Appl., 140(3):513, 2009. S. Vaiter, C.-A. Deledalle, G. Peyré, C. Dossal, and J. Fadili. Local behavior of sparse analysis regulariza- tion: Applications to risk estimation. Appl. Comput. Harmon. Anal., 35(3):433–451, 2013. S. Vaiter, C.-A. Deledalle, G. Peyré, J. M. Fadili, and C. Dossal. The degrees of freedom of partly smooth reg- ularizers. Ann. Inst. Stat. Math., 69(4):791–832, 2017. K. Wu, Y . Guo, Z. Li, and C. Zhang. Sparse coding with gated learned ista. In International Conference on Learning Representations, 2019. B. Xin, Y . Wang, W. Gao, D. Wipf, and B. Wang. Maximal sparsity with deep networks? In Advances in Neural In- formation Processing Systems, pages 4340–4348, 2016. M. Yuan and Y . Lin. Model selection and estimation in regression with grouped variables. J. R. Stat. Soc. Ser. B Stat. Methodol., 68(1):49–67, 2006. C.-H. Zhang. Nearly unbiased variable selection under minimax concave penalty. Ann. Statist., 38(2):894–942, 2010.Implicit differentiation of Lasso-type models for hyperparameter optimization H. Zou. The adaptive lasso and its oracle properties. J. Amer. Statist. Assoc., 101(476):1418–1429, 2006. H. Zou and T. J. Hastie. Regularization and variable se- lection via the elastic net. J. R. Stat. Soc. Ser. B Stat. Methodol., 67(2):301–320, 2005. H. Zou, T. J. Hastie, and R. Tibshirani. On the “degrees of freedom” of the lasso. Ann. Statist., 35(5):2173–2192, 2007.Implicit differentiation of Lasso-type models for hyperparameter optimization A. Proofs A.1. Proof of Proposition 1 We start by a lemma on the weak derivative of the soft-thresholding. Lemma A.1. The soft-thresholding ST :R×R+ ↦→R deﬁned by ST(t,τ) = sign(t) ·(|t|−τ)+ is weakly differentiable with weak derivatives ∂1 ST(t,τ) =1{|t|>τ} , (19) and ∂2 ST(t,τ) =−sign(t) ·1{|t|>τ} , (20) where 1{|t|>τ}= { 1, if |t|>τ, 0, otherwise. (21) Proof. See (Deledalle et al., 2014, Proposition 1) Proof. (Proposition 1, Lasso ISTA) The soft-thresholding is differentiable almost everywhere (a.e.), thus Equation (10) can be differentiated a.e. thanks to the previous lemma, and for any α> 0 ˆJ=   1{|ˆβ1|>0} ... 1{|ˆβp|>0}  ⊙ ( Idp−1 αX⊤X ) ˆJ− neλ α   sign( ˆβ1)1{|ˆβ1|>0} ... sign( ˆβp)1{|ˆβp|>0}   . Inspecting coordinates inside and outside the support of ˆβleads to: { ˆJˆSc = 0 ˆJˆS = ˆJˆS −1 αX⊤ :,ˆSX:,ˆS ˆJˆS −neλ α sign ˆβˆS . (22) Rearranging the term of Equation (22) it yields: X⊤ :,ˆSX:,ˆS ˆJˆS = −neλsign ˆβˆS (23) ˆJˆS = −neλ ( X⊤ :,ˆSX:,ˆS )−1 sign ˆβˆS . (24) (Proposition 1, Lasso BCD) The ﬁxed point equations for the BCD case is ˆβj = ST ( ˆβj − 1 ∥X:j∥2 2 X⊤ :j(Xˆβj −y), neλ ∥X:j∥2 2 ) . (25) As before we can differentiate this ﬁxed point equation Equation (25) ˆJj = 1{|ˆβj|>τ}· ( ˆJj − 1 ∥X:j∥2 2 X⊤ :jX ˆJ ) − neλ ∥X:j∥2 2 sign (ˆβj)1{|ˆβj|>τ} , (26) leading to the same result.Implicit differentiation of Lasso-type models for hyperparameter optimization A.2. Proof of Proposition 2 in the ISTA case Proof. (Lasso case, ISTA) In Algorithm 3, β(k) follows ISTA steps, thus (β(k))l∈N converges toward the solution of the Lasso ˆβ. Let ˆS be the support of the Lasso estimator ˆβ, and ν( ˆS) > 0 the smallest eigenvalue of X⊤ :,ˆSX:,ˆS. Under uniqueness assumption proximal gradient descent ( a.k.a. ISTA) achieves sign identiﬁcation (Hale et al., 2008), i.e., there exists k0 ∈N such that for all k≥k0 −1: sign β(k+1) = signˆβ . (27) Recalling the update of the Jacobian Jfor the Lasso solved with ISTA is the following: J(k+1) = ⏐⏐⏐sign β(k+1) ⏐⏐⏐⊙ ( Id − 1 ∥X∥2 2 X⊤X ) J(k) − neλ ∥X∥2 2 sign β(k+1) , it is clear that J(k) is sparse with the sparsity pattern β(k) for all k≥k0. Thus we have that for all k≥k0: J(k+1) ˆS = J(k) ˆS − 1 ∥X∥2 2 X⊤ :,ˆSXJ(k) − neλ ∥X∥2 2 sign ˆβˆS = J(k) ˆS − 1 ∥X∥2 2 X⊤ :,ˆSX:,ˆSJ(k) ˆS − neλ ∥X∥2 2 sign ˆβˆS = ( IdˆS− 1 ∥X∥2 2 X⊤ :,ˆSX:,ˆS ) J(k) ˆS − neλ ∥X∥2 2 sign ˆβˆS. (28) One can remark that ˆJdeﬁned in Equation (11), satisﬁes the following: ˆJˆS = ( IdˆS− 1 ∥X∥2 2 X⊤ :,ˆSX:,ˆS ) ˆJˆS − neλ ∥X∥2 2 sign ˆβˆS . (29) Combining Equations (28) and (29) and denoting ν( ˆS) >0 the smallest eigenvalue of X⊤ ˆSXˆS, we have for all k≥k0: J(k+1) ˆS − ˆJˆS = ( IdˆS− 1 ∥X∥2 2 X⊤ :,ˆSX:,ˆS )( J(k) ˆS − ˆJˆS ) ∥J(k+1) ˆS − ˆJˆS∥2 ≤ ( 1 − ν( ˆS) ∥X∥2 2 ) ∥J(k) ˆS − ˆJˆS∥2 ∥J(k) ˆS − ˆJˆS∥2 ≤ ( 1 − ν( ˆS) ∥X∥2 2 )k−k0 ∥J(k0) ˆS − ˆJˆS∥2 . Thus the sequence of Jacobian ( J(k)) k∈N converges linearly to ˆJonce the support is identiﬁed. Proof. (wLasso case, ISTA) Recalling the update of the Jacobian J ∈Rp×p for the wLasso solved with ISTA is the following: J(k+1) = ⏐⏐⏐sign β(k+1) ⏐⏐⏐⊙ ( Id − 1 ∥X∥2 2 X⊤X ) J(k) − neλ ∥X∥2 2 diag ( sign β(k+1) ) , (30) The proof follows exactly the same steps as the ISTA Lasso case to show convergence in spectral norm of the sequence (J(k))k∈N toward ˆJ.Implicit differentiation of Lasso-type models for hyperparameter optimization A.3. Proof of Proposition 2 in the BCD case The goal of the proof is to show that iterations of the Jacobian sequence (J(k))k∈N generated by the Block Coordinate Descent algorithm (Algorithm 3) converges toward the true Jacobian ˆJ. The main difﬁculty of the proof is to show that the Jacobian sequence follows a Vector AutoRegressive (V AR, see Massias et al. (2019, Thm. 10) for more detail),i.e., the main difﬁculty is to show that there exists k0 such that for all k≥k0: J(k+1) = AJ(k) + B , (31) with A∈Rp×p a contracting operator and B ∈Rp. We follow exactly the proof of Massias et al. (2019, Thm. 10). Proof. (Lasso, BCD, forward differentiation (Algorithm 3)) Let j1,...,j S be the indices of the support of ˆβ, in increasing order. As the sign is identiﬁed, coefﬁcients outside the support are 0 and remain 0. We decompose the k-th epoch of coordinate descent into individual coordinate updates: Let ˜β(0) ∈Rp denote the initialization (i.e., the beginning of the epoch, ), ˜β(1) = β(k) the iterate after coordinate j1 has been updated, etc., up to ˜β(S) after coordinate jS has been updated, i.e., at the end of the epoch ( ˜β(S) = β(k+1)). Let s ∈S, then ˜β(s) and ˜β(s−1) are equal everywhere, except at coordinate js: ˜J(s) js = ˜J(s−1) js − 1 ∥X:,js∥2 X⊤ :,jsX ˜J(s−1) − 1 ∥Xjs∥2 sign βjs after sign identiﬁcation we have: = ˜J(s−1) js − 1 ∥X:,js∥2 X⊤ :,jsX:,ˆS ˜J(s−1) ˆS − 1 ∥X:,js∥2 sign ˆβjs ˜J(s) ˆS = ( Idˆs− 1 ∥X:,js∥2 ejse⊤ jsX⊤ :,ˆSX:,ˆS )    As ˜J(s−1) ˆS − 1 ∥X:,js∥2 sign ˆβjs ( X⊤ :,ˆSX:,ˆS )1/2 ˜J(s) ˆS =  Idˆs− ( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥ ejse⊤ js ( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥      A(s) ( X⊤ :,ˆSX:,ˆS )1/2 ˜J(s−1) ˆS − ( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥2 sign ˆβjs    b(s) We thus have: ( X⊤ :,ˆSX:,ˆS )1/2 ˜J(ˆs) ˆS = A(ˆs) ...A (1)    A∈Rˆs×ˆs ( X⊤ :,ˆSX:,ˆS )1/2 J(1) ˆS + AS...A 2b1 + ··· + ASbS−1 + bS   b∈Rˆs . After sign identiﬁcation and a full update of coordinate descent we thus have: ( X⊤ :,ˆSX:,ˆS )1/2 J(t+1) ˆS = A ( X⊤ :,ˆSX:,ˆS )1/2 J(t) ˆS + b . (32) Lemma A.2. ∥As∥2 ≤1 , Moreover if A(s)x = ∥x∥then x∈vect   ( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥ ejs   ⊤ (33) Proof. ( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥ ejse⊤ js ( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥Implicit differentiation of Lasso-type models for hyperparameter optimization is a symmetric rank 1 matrix, its non-zero eigenvalue is e⊤ js ( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥ ( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥ ejs = e⊤ js X⊤ :,ˆSX:,ˆS ∥X:,js∥2 ejs = 1 . An eigenvector associated to this non-zeros eigenvalue is ( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥ ejs . Asis symmetric and real, is diagonalisable in an orthogonal basis, it has eigenvalue1 with multiplicity ˆs−1 and eigenvalue 0 with multiplicity 1. Moreover if ∥Ax∥= ∥x∥, then x∈vect (( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥ ejs )⊤ . Lemma A.3. ∥A∥2 <1 . Proof. A= A(ˆs) ...A (1) We have ∥A∥≤∥ A(ˆs)∥   ≤1 ... ∥A(1)∥   ≤1 ≤1 . Let x∈Rˆs such that ∥Ax∥= ∥x∥, we thus have for all s∈1,..., ˆs, A(s)x = ∥x∥. Using Lemma A.3 we have that for all s∈1,..., ˆsx ∈vect (( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥ ejs )⊤ , i.e., x∈vect (( X⊤ :,ˆSX:,ˆS )1/2)⊤ = {0}because X⊤ :,ˆSX:,ˆS ≻0 Using Equation (32) we have: ∥J(t+1) ˆS − ˆJ∥(X⊤ :,ˆSX:,ˆS)−1 ≤∥A∥2∥J(t) ˆS − ˆJ∥(X⊤ :,ˆSX:,ˆS)−1 , (34) with ∥A∥2 < 1, which leads to the desire result. Since the recursion of the Jacobian sequences of Algorithm 2 and Algorithm 2 are the same once the support is identiﬁed, the proof of convergence of Algorithm 2 is the same (provided that support identiﬁcation has been achieved). Proof. (wLasso case, BCD) As for the Lasso case: ˜J(s) js,: = ˜J(s−1) js,: − 1 ∥X:,js∥2 X⊤ :,jsX ˜J(s−1) − 1 ∥Xjs∥2 sign βjsejse⊤ js after sign identiﬁcation we have: ˜J(s) js,ˆS = ˜J(s−1) js,ˆS − 1 ∥X:,js∥2 X⊤ :,jsX:,ˆS ˜J(s−1) ˆS,ˆS − 1 ∥X:,js∥2 sign ˆβjsejse⊤ js (X⊤ :,ˆSX:,ˆS)1/2 ˜J(s) ˆS,ˆS = ( Idn− (X⊤ :,ˆSX:,ˆS)1/2ejse⊤ js(X⊤ :,ˆSX:,ˆS)1/2 ∥X:,js∥2 )    A(s) (X⊤ :,ˆSX:,ˆS)1/2 ˜J(s−1) ˆS,ˆS −sign ˆβjs ∥X:,js∥2 (X⊤ :,ˆSX:,ˆS)1/2    B(s) ejse⊤ js (X⊤ :,ˆSX:,ˆS)1/2 ˜J(ˆs) ˆS,ˆS = A(ˆs) ...A (1)    A∈Rˆs×ˆs (X⊤ :,ˆSX:,ˆS)1/2 ˜J(0) ˆS,ˆS + A(ˆs) ...A (2)B(1)ej1 e⊤ j1 + ··· + B(ˆs)ejˆse⊤ jˆs    D∈Rˆs×ˆs . (35) As in the Lasso case, Equation (35) leads to linear convergence once the support is identiﬁed for Algorithms 2 and 3.Implicit differentiation of Lasso-type models for hyperparameter optimization B. Block coordinate descent algorithms Algorithm 3 presents the forward iteration scheme which computes iteratively the solution of the Lasso or wLasso jointly with the Jacobian computation. This is the naive way of computing the Jacobian without taking advantage of its sparsity. Eventually, it requires to differentiate every lines of code w.r.t. to λ and take advantage of the BCD updates for cheap updates on the Jacobian as well. Algorithm 3FORWARD ITERDIFF (Deledalle et al., 2014; Franceschi et al., 2017) input : X ∈Rn×p,y ∈Rn,λ ∈R,niter ∈N // jointly compute coef. & Jacobian β = 0 // potentially warm started J= 0 // potentially warm started r= y−Xβ dr= −XJ for k= 0,...,n iter −1 do for j = 0,...,p −1 do // update the regression coefficients βold = βj zj = βj + 1 ∥X:,j∥2 X⊤ :,jr // gradient step βj = ST(zj,neλ/∥X:,j∥2) // proximal step r−= X:,j(βj −βold) // update the Jacobian if Lasso then Jold = Jj Jj = |sign βj| ( Jj + 1 ∥X:,j∥2 X⊤ :,jdr ) // diff. w.r.t. λ Jj −= neλ ∥X:,j∥2 sign βj // diff. w.r.t. λ drj −= X:,j(Jj −Jold) if wLasso then Jold = Jj,: Jj,: = |sign βj| ( Jj,: + 1 ∥X:,j∥2 X⊤ :,jdr ) // diff. w.r.t. λ1,...,λ p Jj,j −= neλj ∥X:,j∥2 sign βj // diff. w.r.t. λ1,...,λ p dr−= X:,j(Jj −Jold) return βniter ,Jniter (λ) Algorithm 4 describes the backward iterative differentiation algorithm used for benchmark. Backward differentiation requires the storage of every updates on β. As Figure 1 shows, this algorithm is not efﬁcient for our case because the function to differentiate f : R →Rp ( f : Rp →Rp, for the wLasso) has a higher dimension output space than the input space. The storage is also an issue mainly for the wLasso case which makes this algorithm difﬁcult to use in practice in our context. Algorithm 5 presents the classical BCD iterative scheme for solving the Lasso problem using the composition of a gradient step with the soft-thresholding operator.Implicit differentiation of Lasso-type models for hyperparameter optimization Algorithm 4BACKWARD ITERDIFF (Domke, 2012) input : X ∈Rn×p,y ∈Rn,λ ∈R,niter ∈N // backward computation of ˆβ and ˆJ⊤ (λ)α β = 0 // potentially warm started // compute the regression coefficients and store the iterates for k= 0,...,n iter −1 do for j = 0,...,p −1 do βold = βj zj = βj + 1 ∥X:,j∥2 X⊤ :,jr // gradient step βj = ST(zj,neλ/∥X:,j∥2) // proximal step r−= X:,j(βj −βold) // Init. backward differentiation g= 0 // g stores ˆJ⊤ λ α // compute the Jacobian for k= niter down to 1 do for j = 0,...,p −1 do if Lasso then g−= neλ ∥X:,j∥2 αjsign β(k) j αj ∗= |sign β(k) j | α−= 1 ∥X:,j∥2 αjX⊤ :,jX // O(np) if wLasso then gj −= neλj ∥X:,j∥2 αjsign β(k) j αj ∗= |sign β(k) j | α−= 1 ∥X:,j∥2 αjX⊤ :,jX return βniter ,g(1) Algorithm 5BCD FOR THE LASSO (Friedman et al., 2010) input : X ∈Rn×p,y ∈Rn,λ ∈R,β(0) ∈Rp,niter ∈N β = β(0) // warm start for k= 0,...,n iter −1 do for j = 0,...,p −1 do βold = βj zj = βj + 1 ∥X:,j∥2 X⊤ :,jr // gradient step βj = ST(zj,neλ/∥X:,j∥2) // proximal step r−= X:,j(βj −βold) return βniterImplicit differentiation of Lasso-type models for hyperparameter optimization C. Derivations for MCP Let us remind the deﬁnition of the Minimax Concave Penalty (MCP) estimator introduced by Zhang (2010), also analyzed under the name CELE0 by Soubies et al. (2017). First of all, for any t∈R: pMCP λ,γ (t) = { λ|t|− t2 2γ, if |t|≤ γλ 1 2 γλ2, if |t|>γλ . (36) The proximity operator of pλ,γ for parameters λ >0 and γ >1 is deﬁned as follow (see Breheny and Huang 2011, Sec. 2.1): proxMCP λ,γ (t) = {ST(t,λ) 1−1 γ if |t|≤ γλ t if |t|>γλ . (37) For ourselves we choose as for the Lasso an exponential parametrization of the coefﬁcients, for λ∈R and γ >0: ˆβ(λ,γ)(y) ≜ arg min β∈Rp 1 2n∥y−Xβ∥2 2 + p∑ j=1 pMCP eλ,eγ (|βj|) . (38) Update rule for Coordinate Descent Below, we provide equation to update the coefﬁcient in the coordinate descent algorithm of the MCP: βj ←arg min βj∈R 1 2n∥y−βjX:,j − ∑ j′̸=j βj′X:,j′∥2 2 + p∑ j′̸=j pMCP eλ,eγ(βj′) +pMCP eλ,eγ(βj) = arg min βj∈R 1 2n∥y−βjX:,j − ∑ j′̸=j βj′X:,j′∥2 2 + pMCP eλ,eγ(βj) = arg min βj∈R ∥X:,j∥2 2   1 2n  βj − 1 ∥X:,j∥2 2 ⟨ y− ∑ j′̸=j βj′X:,j′,X:,j ⟩  2 + 1 ∥X:,j∥2 2 pMCP eλ,eγ(βj)   = arg min βj∈R   1 2n  βj − 1 ∥X:,j∥2 2 ⟨ y− ∑ j′̸=j βj′X:,j′,X:,j ⟩  2 + 1 ∥X:,j∥2 2 pMCP eλ,eγ(βj)   = arg min βj∈R   1 2Lj  βj − 1 ∥X:,j∥2 2 ⟨ y− ∑ j′̸=j βj′X:,j′,X:,j ⟩  2 + pMCP eλ,eγ(βj)  ,with Lj ≜ n ∥X:,j∥2 2 = proxMCP eλ/Lj,eγLj ( βj − 1X2 :,j X⊤ :,j(Xβ −y),λ ) . (39) One can write the following ﬁxed point equation satisﬁed by the estimator ˆβ, with Lj = ∥X:,j∥2 /n: ˆβj = proxMCP eλ/Lj,eγLj   ⟨ y− ∑ k̸=j ˆβkX:,k, X:,j ∥X:,j∥2 ⟩  = proxMCP eλ/Lj,eγLj ( ˆβj − 1 ∥X:,j∥2 X⊤ :,j ( Xˆβ−y )) . (40) Since the MCP penalty is non-convex, the estimator may not be continuous w.r.t. hyperparameters and gradient based hyperparameter optimization may not be theoretically justiﬁed. However we can differentiate the ﬁxed point equationImplicit differentiation of Lasso-type models for hyperparameter optimization Imp. F. iterdiﬀ. (ours) F. iterdiﬀ. Grid-search 0 2 4 10−4 10−3 10−2 10−1 100 Objective minus optimum rcv1 (p=19,959) 0 10 20 30 10−2 10−1 100 101 102  20news (p=130,107) 0 2 4 Time (s) 10−1 100 Loss on test set 0 10 20 30 Time (s) 101 102 Figure 5.Computation time for the HO of the MCP on real dataDistance to “optimum” (top) and performance (bottom) on the test set for the MCP. Equation (40) almost everywhere: ˆJj = ( ˆJj − 1 ∥X:j∥2 2 X⊤ :jX ˆJ ) · ∂proxMCP eλ/Lj,eγLj ∂t ( ˆβj − 1X2 :,j X⊤ :,j(Xβ −y) ) + eλ Lj ∂proxMCP eλ/Lj,eγLj ∂λ ( ˆβj − 1X2 :,j X⊤ :,j(Xβ −y) ) + eγLj ∂proxMCP eλ/Lj,eγLj ∂γ ( ˆβj − 1X2 :,j X⊤ :,j(Xβ −y) ) . (41) where ∂proxMCP λ,γ ∂t (t) = { |sign t| 1−1 γ , if |t|≤ λγ 1, otherwise , (42) ∂proxMCP λ,γ ∂λ (t) =    0, if |t|≤ λ −sign t 1−1 γ , if λ≤|t|≤ λγ 0, if |t|>λγ , (43) ∂proxMCP λ,γ ∂γ (t) = { −ST(t,λ) (γ−1)2 if |t|≤ λγ 0 if |t|>λγ . (44) Contrary to other methods, HO based algorithms do not scale exponentially in the number of hyperparameters. Here we propose experiments on the held-out loss with the MCP estimator (Zhang, 2010), which has 2 hyperparameters λand γ. Our algorithm can generalize to such non-smooth proximity-based estimator. Comments on Figure 5 (MCP , held-out criterion). Figure 5 (top) shows the convergence of the optimum on 2 datasets (rcv1 and 20news) for the MCP estimator. As before implicit forward differentiation outperforms forward differentiation illustrating Proposition 2 and Table 1.Implicit differentiation of Lasso-type models for hyperparameter optimization D. Datasets and implementation details The code used to produce all the ﬁgures as well as the implementation details can be found in the supplementary material in the forward_implicit/expesfolder. In particular in all experiments, for our algorithm, implicit forward differentiation, the size of the loop computing the Jacobian is ﬁxed: n_iter_jac = 100. Reminding that the goal is to compute the gradient: ˆJ⊤ (λ)∇C ( ˆβ(λ) ) , (45) we break the loop if ∥(J(k+1) −J(k))∇C( ˆβ(λ))∥≤∥∇C ( ˆβ(λ))∥×ϵjac , (46) with ϵjac = 10−3. All methods beneﬁt from warm start. D.1. Details on Figure 1 Figure 1 is done using synthetic data. As described in Section 4.2, X ∈ Rn×p is a Toeplitz correlated ma- trix, with correlation coefﬁcient ρ = 0 .9, (n,p) = (1000 ,2000). β ∈ Rp is chosen with 5 non-zero coefﬁ- cients chosen at random. Then y ∈ Rn is chosen to be equal to Xβ contaminated by some i.i.d. random Gaus- sian noise, we chose SNR = 3. For Figure 1 all the implementation details can be found in the joint code in the forward_implicit/examples/plot_time_to_compute_single_gradient.py ﬁle. Figure 1 shows the time of compu- tation of one gradient and the distance to ”optimum”. For this ﬁgure we evaluated the gradient in λ= λmax −ln(10). The ”optimum” is the gradient obtained using the implicit differentiation method. D.2. Details on Figure 2 Let us ﬁrst begin by a description of all the datasets and where they can be downloaded. rcv1. The rcv1 dataset can be downloaded here: https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/ datasets/multilabel.html#rcv1v2%20(topics;%20subsets). The dataset contains n = 20,242 sam- ples and p= 19,959 features. 20news. The 20news dataset can be downloaded here https://www.csie.ntu.edu.tw/~cjlin/ libsvmtools/datasets/multiclass.html#news20. The dataset contains n = 11 ,314 samples and p= 130,107 features. ﬁnance. The ﬁnance (E2006-log1p on libsvm) dataset can be downloaded here: https://www.csie.ntu.edu. tw/~cjlin/libsvmtools/datasets/regression.html#E2006-log1p. The dataset contains n= 16,087 samples and p= 1,668,737 features. All the implementation details can be found in the code: forward_implicit/expes/main_lasso_pred.py. D.3. Details on Figure 3 Figure 3 was performed using simulated data. The matrix X ∈Rn×p was obtained by simulated n×pi.i.d. Gaussian variables N(0,1). The number of rows was ﬁxed atn= 100and we changed the number of columnspfrom 200 to 10,000 on a linear grid of size 10. Then , we generated β∗with 5 coefﬁcients equal to 1 and the rest equals to 0. The vector y is equal to Xβ∗contaminated by some i.i.d. random Gaussian noise controlled by a SNR value of 3. We performed 50 repetitions for each value of pand computed the average MSE on these repetitions. The initial value for the line-search algorithm was set at λmax + ln(0.7) and the number of iterations for the Jacobian at 500 for the whole experiment. All the implementation details can be found in the code : forward_implicit/expes/main_lasso_est.py. D.4. Details on Figure 4 Figure 4 was performed using the same simulating process as described above only this time we performed only 25 repeti- tions for each value of p. We had to deal with the fact that Problem (4) is not convex for the weighted Lasso which means that our line-search algorithm could get stuck in local minima. In order to alleviate this problem, we introduced Equa- tion (18) to obtain an initial point for the line-search algorithm. We chose the regularization term to be constant and equalsImplicit differentiation of Lasso-type models for hyperparameter optimization to C(β(λmax))/10. We used a time treshold of 500 seconds which was hit only by the forward differentiation algorithm for the wLasso. The details about this experiment can be found in the code : forward_implicit/expes/main_wLasso.py.Implicit differentiation of Lasso-type models for hyperparameter optimization E. Supplementary experiments E.1. Experiments with a non-unique solution to the inner problem We recall here that the bi-level optimization Problem (4) is solved using gradient descent. We recall also that gradient descent may not converge toward a global minima since the optimized function λ↦→L(λ) may not be convex. It may be even worse: if the inner optimization problem has not a unique solution, the function λ ↦→L(λ) may not be continuous. However our algorithm can still be applied to compute the hypergradient. Figure 6 shows the time to compute a single (hyper)gradient when the solution to the inner problem is not unique. As proved for instance in Tibshirani (2013, Lemma 3 and 4), the set of parameters where the Lasso solution is not unique is typically ∅or a set whose Lebesgue measure is zero. Moreover, there exist settings such that the solution path (as a multivalued mapping) could be non-continuous, which leaves only non-gradient based methods available. Thus, we decided to not investigate the theory in such pathological settings. The authors are not aware of a classical dataset where non-uniqueness arises. Nevertheless, in the case where there existsλsuch that the solution set is not reduced to a singleton, our proposed algorithm can still be applied to any solution without theoretical guarantees. Experimental setting for non-uniqueness.For completeness, we run our methods on the following toy example Tibshi- rani (2013): we consider a design Xsuch that n= 100, p= 10000and X1,X2,X3 are generated iid following a standard normal distribution, X4 = (X2 +X3)/2 and X5,...,X p are generated i.i.d. following a standard normal distribution, then orthogonalized X5,...,X p w.r.t.Span(X1,...,X 4). We let y= −X1 +X2 +X3. We let the reader observe that when λ = 1/n, the solution set is not reduced to a singleton. In this case, similar conclusions are observed in Figure 6 as for other datasets: Imp. F. Iterdiff (ours) still outperforms its competitors. Imp. F. Iterdiﬀ. (ours) F. Iterdiﬀ. B. Iterdiﬀ. 6 8 10 Number of iterations 100 102 Times (s) 6 8 10 Number of iterations 10−11 10−8 10−5 Objective minus optimum Figure 6.Time to compute a single gradient with non-unique solution(Synthetic data, Lasso, n,p = 1000,10000). Inﬂuence on the number of iterations of BCD (in the inner optimization problem of Problem (4)) on the computation time (left) and the distance to “optimum” of the gradient∇λL(λ)(right) for the Lasso estimator.",
      "meta_data": {
        "arxiv_id": "2002.08943v3",
        "authors": [
          "Quentin Bertrand",
          "Quentin Klopfenstein",
          "Mathieu Blondel",
          "Samuel Vaiter",
          "Alexandre Gramfort",
          "Joseph Salmon"
        ],
        "published_date": "2020-02-20T18:43:42Z",
        "pdf_url": "https://arxiv.org/pdf/2002.08943v3.pdf",
        "github_url": "https://github.com/QB3/sparse-ho"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces an efficient implicit differentiation algorithm, without explicit matrix inversion, specifically designed for hyperparameter optimization of Lasso-type models. It addresses the challenges of setting regularization parameters for non-smooth Lasso-type estimators, which are notoriously difficult due to issues like exponential complexity of grid-search, high memory consumption of iterative differentiation, or numerical instability and computational cost of classical implicit differentiation for smooth problems. The key contributions include demonstrating that forward iterative differentiation of Block Coordinate Descent (BCD) converges linearly to the true gradient once the support is identified, proposing a decoupled algorithm (Implicit Forward Iterative Differentiation) for Jacobian computation that avoids solving ill-conditioned linear systems, and experimentally showing that the proposed method outperforms numerous state-of-the-art hyperparameter optimization techniques in terms of optimizing held-out error or the Stein Unbiased Risk Estimator (SURE), particularly scaling well to high-dimensional data by leveraging solution sparsity.",
        "methodology": "The methodology frames hyperparameter optimization as a bi-level optimization problem for Lasso-type estimators (Lasso and weighted Lasso) with a differentiable criterion. It leverages implicit differentiation, starting from the fixed-point iteration property of proximal BCD algorithms for Lasso-type problems. The core contribution is deriving a weak Jacobian ˆJ(λ) that is sparse and can be computed efficiently. The proposed 'Implicit Forward Iterative Differentiation' algorithm (Algorithm 2) works in two main steps: first, it solves the inner optimization problem to compute the regression coefficients ˆβ and identify its support, which can be done with any state-of-the-art Lasso solver. Second, it computes the Jacobian by applying forward differentiation recursion steps restricted to the identified support, without explicitly solving a linear system. This approach is proven to converge linearly to the true Jacobian. For experimental comparison, a vanilla BCD algorithm is used for the inner problem across all methods, along with a line-search strategy for gradient-based optimizers. Hyperparameters are parametrized exponentially to avoid positivity constraints and scale issues.",
        "experimental_setup": "The Python code for all experiments is open-sourced as the 'sparse-ho' package, utilizing Numba for critical sections. The inner Lasso-type solvers are stopped when the relative function decrease is below ϵ_tol = 10^-5. All hypergradient-based methods use a line-search strategy for gradient steps and warm starts. Initial values for Lasso hyperparameters are set at λ_max - log(10). Competitors include implicit differentiation (solving an ˆs×ˆs linear system), forward iterative differentiation, and non-gradient methods such as grid-search, random-search, lattice hypercube sampling, and Bayesian optimization. Experiments are conducted using two main criteria: held-out loss (on three real-world datasets: rcv1 (p=19,959), 20news (p=130,107), and finance (p=1,668,737), split into training, validation, and test sets) and Stein Unbiased Risk Estimator (SURE) with a Finite Differences Monte-Carlo approximation (on synthetic data with n=100, p varying from 200 to 10,000, 50 repetitions per p value). A specific setup for non-unique solution scenarios is also explored. The proposed implicit forward differentiation algorithm uses n_iter_jac = 100 for Jacobian computation, with an early stopping condition based on Jacobian convergence.",
        "limitations": "The theoretical guarantees for the proposed method do not cover non-convex penalty functions (e.g., MCP), although experimental results suggest it performs numerically well in such cases. The convergence proofs rely on the assumption of a unique solution to the inner Lasso problem; in pathological settings where the solution is not unique or the solution path is non-continuous, the theoretical justification for gradient-based methods is challenged. Classical implicit differentiation (Algorithm 1) can suffer from slow convergence due to ill-conditioned linear systems, especially with large support sizes. Backward iterative differentiation (Algorithm 4) is found to be significantly slower and more memory-consuming than other approaches for Lasso-type problems, especially for the weighted Lasso, making it impractical for the scale of problems considered.",
        "future_research_directions": "Future work could involve extending the theoretical framework to cover non-convex Lasso-type formulations, such as those employing the Minimax Concave Penalty (MCP). Another promising direction is to further explore the integration of the proposed algorithm with state-of-the-art Lasso solvers that utilize advanced techniques like active sets or screening rules. These solvers, while highly efficient, introduce discontinuities that pose challenges for single-step automatic differentiation approaches, suggesting a need for dedicated research to effectively combine these powerful tools. Further investigation into the algorithm's behavior and theoretical guarantees in 'pathological' settings where the inner optimization problem may have non-unique solutions or non-continuous solution paths is also a relevant area for future exploration.",
        "experimental_code": "from .ho import grad_search, hyperopt_wrapper\n\nfrom .algo import Backward\nfrom .algo import Forward\nfrom .algo import ImplicitForward\nfrom .algo import Implicit\n\n\n__version__ = '0.1.dev'\n\nfrom sparse_ho.algo.backward import Backward\nfrom sparse_ho.algo.forward import Forward\nfrom sparse_ho.algo.implicit import Implicit\nfrom sparse_ho.algo.implicit_forward import ImplicitForward\n\n__all__ = ['Backward',\n           'Forward',\n           'Implicit',\n           'ImplicitForward']\n\nimport numpy as np\nfrom scipy.sparse import issparse\n\n\nclass Forward():\n    \"\"\"Algorithm to compute the hypergradient using forward differentiation of\n    proximal coordinate descent.\n\n    The algorithm jointly and iteratively computes the regression coefficients\n    and the Jacobian using forward differentiation of proximal\n    coordinate descent.\n\n    Parameters\n    ----------\n    use_stop_crit: bool, optional (default=True)\n        Use stopping criterion in hypergradient computation. If False,\n        run to maximum number of iterations.\n    verbose: bool, optional (default=False)\n        Verbosity of the algorithm.\n    \"\"\"\n\n    def __init__(self, use_stop_crit=True, verbose=False):\n        self.use_stop_crit = use_stop_crit\n        self.verbose = verbose\n\n    def compute_beta_grad(\n            self, X, y, log_alpha, model, get_grad_outer, mask0=None,\n            dense0=None, quantity_to_warm_start=None, max_iter=1000, tol=1e-3,\n            full_jac_v=False):\n        \"\"\"Compute beta and hypergradient, with forward differentiation of\n        proximal coordinate descent.\n\n        Parameters\n        ----------\n        X: array-like, shape (n_samples, n_features)\n            Design matrix.\n        y: ndarray, shape (n_samples,)\n            Observation vector.\n        log_alpha: float or np.array, shape (n_features,)\n            Logarithm of hyperparameter.\n        model:  instance of ``sparse_ho.base.BaseModel``\n            A model that follows the sparse_ho API.\n        get_grad_outer: callable\n            Function which returns the gradient of the outer criterion.\n        mask0: ndarray, shape (n_features,)\n            Boolean of active feature of the previous regression coefficients\n            beta for warm start.\n        dense0: ndarray, shape (mask.sum(),)\n            Initial value of the previous regression coefficients\n            beta for warm start.\n        quantity_to_warm_start: ndarray\n            Previous Jacobian of the inner optimization problem.\n        max_iter: int\n            Maximum number of iteration for the inner solver.\n        tol: float\n            The tolerance for the inner optimization problem.\n        full_jac_v: bool\n            TODO\n        \"\"\"\n        # jointly compute the regression coefficients beta and the Jacobian\n        mask, dense, jac = compute_beta(\n            X, y, log_alpha, model, mask0=mask0, dense0=dense0,\n            jac0=quantity_to_warm_start, max_iter=max_iter, tol=tol,\n            compute_jac=True, verbose=self.verbose,\n            use_stop_crit=self.use_stop_crit)\n        if jac is not None:\n            jac_v = model.get_jac_v(X, y, mask, dense, jac, get_grad_outer)\n            if full_jac_v:\n                jac_v = model.get_full_jac_v(mask, jac_v, X.shape[1])\n        else:\n            jac_v = None\n\n        return mask, dense, jac_v, jac\n\n\ndef compute_beta(\n        X, y, log_alpha, model, mask0=None, dense0=None, jac0=None,\n        max_iter=1000, tol=1e-3, compute_jac=True, return_all=False,\n        save_iterates=False, verbose=False, use_stop_crit=True, gap_freq=10):\n    \"\"\"\n    Parameters\n    --------------\n    X: array-like, shape (n_samples, n_features)\n        Design matrix.\n    y: ndarray, shape (n_samples,)\n        Observation vector.\n    log_alpha: float or np.array, shape (n_features,)\n        Logarithm of hyperparameter.\n    beta0: ndarray, shape (n_features,)\n        initial value of the regression coefficients\n        beta for warm start\n    dbeta0: ndarray, shape (n_features,)\n        initial value of the jacobian dbeta for warm start\n    max_iter: int\n        number of iterations of the algorithm\n    tol: float\n        The tolerance for the optimization: if the updates are\n        smaller than ``tol``, the optimization code checks the\n        primal decrease for optimality and continues until it\n        is smaller than ``tol``\n    compute_jac: bool\n        to compute or not the Jacobian along with the regression\n        coefficients\n    model:  instance of ``sparse_ho.base.BaseModel``\n        A model that follows the sparse_ho API.\n    return_all: bool\n        to store the iterates or not in order to compute the Jacobian in a\n        backward way\n    use_stop_crit: bool\n        use a stopping criterion or do all the iterations\n    gap_freq : int\n        After how many passes on the data the dual gap should be computed\n        to stop the iterations.\n\n    Returns\n    -------\n    mask : ndarray, shape (n_features,)\n        The mask of non-zero coefficients in beta.\n    dense : ndarray, shape (n_nonzeros,)\n        The beta coefficients on the support\n    jac : ndarray, shape (n_nonzeros,) or (n_nonzeros, q)\n        The jacobian restricted to the support. If there are more than\n        one hyperparameter then it has two dimensions.\n    \"\"\"\n    n_samples, n_features = X.shape\n    is_sparse = issparse(X)\n    if not is_sparse and not np.isfortran(X):\n        X = np.asfortranarray(X)\n    L = model.get_L(X)\n\n    ############################################\n    alpha = np.exp(log_alpha)\n\n    if hasattr(model, 'estimator') and model.estimator is not None:\n        return model._use_estimator(X, y, alpha, tol)\n\n    try:\n        alpha.shape[0]\n        alphas = alpha.copy()\n    except Exception:\n        alphas = np.ones(n_features) * alpha\n    ############################################\n    # warm start for beta\n    beta, dual_var = model._init_beta_dual_var(X, y, mask0, dense0)\n    ############################################\n    # warm start for dbeta\n    dbeta, ddual_var = model._init_dbeta_ddual_var(\n        X, y, mask0=mask0, dense0=dense0, jac0=jac0, compute_jac=compute_jac)\n\n    # store the values of the objective\n    pobj0 = model._get_pobj0(dual_var, np.zeros(X.shape[1]), alphas, y)\n    pobj = []\n\n    ############################################\n    # store the iterates if needed\n    if return_all:\n        list_beta = []\n    if save_iterates:\n        list_beta = []\n        list_jac = []\n\n    for i in range(max_iter):\n        if verbose:\n            print(\"%i -st iteration over %i\" % (i, max_iter))\n        if is_sparse:\n            model._update_beta_jac_bcd_sparse(\n                X.data, X.indptr, X.indices, y, n_samples, n_features, beta,\n                dbeta, dual_var, ddual_var, alphas, L,\n                compute_jac=compute_jac)\n        else:\n            model._update_beta_jac_bcd(\n                X, y, beta, dbeta, dual_var, ddual_var, alphas,\n                L, compute_jac=compute_jac)\n\n        pobj.append(model._get_pobj(dual_var, X, beta, alphas, y))\n\n        if i > 1:\n            if verbose:\n                print(\"relative decrease = \", (pobj[-2] - pobj[-1]) / pobj0)\n\n        if use_stop_crit and i % gap_freq == 0 and i > 0:\n            if hasattr(model, \"_get_dobj\"):\n                dobj = model._get_dobj(dual_var, X, beta, alpha, y)\n                dual_gap = pobj[-1] - dobj\n                if verbose:\n                    print(\"dual gap %.2e\" % dual_gap)\n                if verbose:\n                    print(\"gap %.2e\" % dual_gap)\n                if dual_gap < pobj0 * tol:\n                    break\n            else:\n                if (pobj[-2] - pobj[-1] <= pobj0 * tol):\n                    break\n        if return_all:\n            list_beta.append(beta.copy())\n        if save_iterates:\n            list_beta.append(beta.copy())\n            list_jac.append(dbeta.copy())\n    else:\n        if verbose:\n            print('did not converge !')\n\n    mask = beta != 0\n    dense = beta[mask]\n    jac = model._get_jac(dbeta, mask)\n    if hasattr(model, 'dual'):\n        model.dual_var = dual_var\n        if compute_jac:\n            model.ddual_var = ddual_var\n    if save_iterates:\n        return np.array(list_beta), np.array(list_jac)\n    if return_all:\n        return mask, dense, list_beta\n    else:\n        if compute_jac:\n            return mask, dense, jac\n        else:\n            return mask, dense, None\n\nclass ImplicitForward():\n    \"\"\"Algorithm to compute the hypergradient using implicit forward\n    differentiation.\n\n    First the algorithm computes the regression coefficients.\n    Then the iterations of the forward differentiation are applied to compute\n    the Jacobian.\n\n    Parameters\n    ----------\n    tol_jac: float\n        Tolerance for the Jacobian computation.\n    max_iter: int\n        Maximum number of iterations for the inner solver.\n    n_iter_jac: int\n        Maximum number of iterations for the Jacobian computation.\n    use_stop_crit: bool, optional (default=True)\n        Use stopping criterion in hypergradient computation. If False,\n        run to maximum number of iterations.\n    verbose: bool, optional (default=False)\n        Verbosity of the algorithm.\n    \"\"\"\n\n    def __init__(\n            self, tol_jac=1e-3, max_iter=100, n_iter_jac=100,\n            use_stop_crit=True, verbose=False):\n        self.max_iter = max_iter\n        self.tol_jac = tol_jac\n        self.n_iter_jac = n_iter_jac\n        self.use_stop_crit = use_stop_crit\n        self.verbose = verbose\n\n    def get_beta_jac(\n            self, X, y, log_alpha, model, get_grad_outer, mask0=None,\n            dense0=None, quantity_to_warm_start=None, max_iter=1000, tol=1e-3,\n            full_jac_v=False):\n        \"\"\"Compute beta and hypergradient using implicit forward\n        differentiation.\n\n        Parameters\n        ----------\n        X: array-like, shape (n_samples, n_features)\n            Design matrix.\n        y: ndarray, shape (n_samples,)\n            Observation vector.\n        log_alpha: float or np.array, shape (n_features,)\n            Logarithm of hyperparameter.\n        model:  instance of ``sparse_ho.base.BaseModel``\n            A model that follows the sparse_ho API.\n        get_grad_outer: callable\n            Function which returns the gradient of the outer criterion.\n        mask0: ndarray, shape (n_features,)\n            Boolean of active feature of the previous regression coefficients\n            beta for warm start.\n        dense0: ndarray, shape (mask.sum(),)\n            Initial value of the previous regression coefficients\n            beta for warm start.\n        quantity_to_warm_start: ndarray\n            Previous Jacobian of the inner optimization problem.\n        max_iter: int\n            Maximum number of iteration for the inner solver.\n        tol: float\n            The tolerance for the inner optimization problem.\n        full_jac_v: bool\n            TODO\n        \"\"\"\n\n        mask, dense, jac = get_bet_jac_implicit_forward(\n            X, y, log_alpha, mask0=mask0, dense0=dense0,\n            jac0=quantity_to_warm_start,\n            tol_jac=tol, tol=tol, niter_jac=self.n_iter_jac, model=model,\n            max_iter=self.max_iter, verbose=self.verbose)\n        return mask, dense, jac\n\n    def compute_beta_grad(\n            self, X, y, log_alpha, model, get_grad_outer, mask0=None,\n            dense0=None, quantity_to_warm_start=None, max_iter=1000, tol=1e-3,\n            full_jac_v=False):\n        mask, dense, jac = get_bet_jac_implicit_forward(\n            X, y, log_alpha, mask0=mask0, dense0=dense0,\n            jac0=quantity_to_warm_start,\n            tol_jac=self.tol_jac, tol=tol, niter_jac=self.n_iter_jac,\n            model=model, max_iter=self.max_iter, verbose=self.verbose,\n            use_stop_crit=self.use_stop_crit)\n        jac_v = model.get_jac_v(X, y, mask, dense, jac, get_grad_outer)\n        if full_jac_v:\n            jac_v = model.get_full_jac_v(mask, jac_v, X.shape[1])\n\n        return mask, dense, jac_v, jac\n\n\ndef get_bet_jac_implicit_forward(\n        X, y, log_alpha, model, mask0=None, dense0=None, jac0=None,\n        tol=1e-3, max_iter=1000, niter_jac=1000, tol_jac=1e-6, verbose=False,\n        use_stop_crit=True):\n\n    mask, dense, _ = compute_beta(\n        X, y, log_alpha, mask0=mask0, dense0=dense0, jac0=jac0, tol=tol,\n        max_iter=max_iter, compute_jac=False, model=model, verbose=verbose,\n        use_stop_crit=use_stop_crit)\n    dbeta0_new = model._init_dbeta0(mask, mask0, jac0)\n    reduce_alpha = model._reduce_alpha(np.exp(log_alpha), mask)\n\n    _, dual_var = model._init_beta_dual_var(X, y, mask, dense)\n    jac = get_only_jac(\n        model.reduce_X(X, mask), model.reduce_y(y, mask), dual_var,\n        reduce_alpha, model.sign(dense, log_alpha), dbeta=dbeta0_new,\n        niter_jac=niter_jac, tol_jac=tol_jac, model=model, mask=mask,\n        dense=dense, verbose=verbose, use_stop_crit=use_stop_crit)\n\n    return mask, dense, jac\n\n\ndef get_only_jac(\n        Xs, y, dual_var, alpha, sign_beta, dbeta=None, niter_jac=100,\n        tol_jac=1e-4, model=\"lasso\", mask=None, dense=None, verbose=False,\n        use_stop_crit=True):\n    n_samples, n_features = Xs.shape\n\n    L = model.get_L(Xs)\n\n    residual_norm = []\n\n    if hasattr(model, 'dual'):\n        ddual_var = model._init_ddual_var(dbeta, Xs, y, sign_beta, alpha)\n        dbeta = model.dbeta\n    else:\n        if dbeta is None:\n            dbeta = model._init_dbeta(n_features)\n        ddual_var = model._init_ddual_var(dbeta, Xs, y, sign_beta, alpha)\n\n    for i in range(niter_jac):\n        if verbose:\n            print(\"%i -st iterations over %i\" % (i, niter_jac))\n        if issparse(Xs):\n            model._update_only_jac_sparse(\n                Xs.data, Xs.indptr, Xs.indices, y, n_samples,\n                n_features, dbeta, dual_var, ddual_var, L, alpha, sign_beta)\n        else:\n            model._update_only_jac(\n                Xs, y, dual_var, dbeta, ddual_var, L, alpha, sign_beta)\n        residual_norm.append(\n            model.get_jac_residual_norm(\n                Xs, y, n_samples, sign_beta, dbeta, dual_var,\n                ddual_var, alpha))\n        if use_stop_crit and i > 1:\n            # relative stopping criterion for the computation of the jacobian\n            # and absolute stopping criterion to handle warm start\n            rel_tol = np.abs(residual_norm[-2] - residual_norm[-1])\n            if (rel_tol < np.abs(residual_norm[-1]) * tol_jac\n                    or residual_norm[-1] < 1e-10):\n                break\n    # HACK we only need this for one test, do not rely on it\n    get_only_jac.n_iter = i\n\n    return dbeta\n\nimport numpy as np\nfrom numpy.linalg import norm\nfrom scipy.sparse import issparse\nimport scipy.sparse.linalg as slinalg\nfrom numba import njit\nfrom scipy.sparse.linalg import LinearOperator\n\nfrom sparse_ho.utils import init_dbeta0_new, ST\nfrom sparse_ho.utils import sparse_scalar_product\nfrom sparse_ho.models.base import BaseModel\n\n\nclass Lasso(BaseModel):\n    \"\"\"Linear Model trained with L1 prior as regularizer (aka the Lasso).\n\n    The optimization objective for Lasso is:\n    (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n\n    Parameters\n    ----------\n    estimator: sklearn estimator\n        Estimator used to solve the optimization problem. Must follow the\n        scikit-learn API.\n    \"\"\"\n\n    def __init__(self, estimator=None):\n        self.estimator = estimator\n\n    def _init_dbeta_ddual_var(self, X, y, mask0=None, jac0=None,\n                              dense0=None, compute_jac=True):\n        n_samples, n_features = X.shape\n        dbeta = np.zeros(n_features)\n        if jac0 is None or not compute_jac:\n            ddual_var = np.zeros(n_samples)\n        else:\n            dbeta[mask0] = jac0.copy()\n            ddual_var = - X[:, mask0] @ jac0.copy()\n        return dbeta, ddual_var\n\n    def _init_beta_dual_var(self, X, y, mask0=None, dense0=None):\n        beta = np.zeros(X.shape[1])\n        if dense0 is None or len(dense0) == 0:\n            dual_var = y.copy()\n            dual_var = dual_var.astype(np.float)\n        else:\n            beta[mask0] = dense0.copy()\n            dual_var = y - X[:, mask0] @ dense0\n        return beta, dual_var\n\n    @staticmethod\n    @njit\n    def _update_beta_jac_bcd(\n            X, y, beta, dbeta, dual_var, ddual_var,\n            alpha, L, compute_jac=True):\n        n_samples, n_features = X.shape\n        non_zeros = np.where(L != 0)[0]\n\n        for j in non_zeros:\n            beta_old = beta[j]\n            if compute_jac:\n                dbeta_old = dbeta[j]\n                # compute derivatives\n            zj = beta[j] + dual_var @ X[:, j] / (L[j] * n_samples)\n            beta[j] = ST(zj, alpha[j] / L[j])\n            # beta[j:j+1] = ST(zj, alpha[j] / L[j])\n            if compute_jac:\n                dzj = dbeta[j] + X[:, j] @ ddual_var / (L[j] * n_samples)\n                dbeta[j:j+1] = np.abs(np.sign(beta[j])) * dzj\n                dbeta[j:j+1] -= alpha[j] * np.sign(beta[j]) / L[j]\n                # update residuals\n                ddual_var -= X[:, j] * (dbeta[j] - dbeta_old)\n            dual_var -= X[:, j] * (beta[j] - beta_old)\n\n    @staticmethod\n    @njit\n    def _update_beta_jac_bcd_sparse(\n            data, indptr, indices, y, n_samples, n_features, beta,\n            dbeta, dual_var, ddual_var, alphas, L, compute_jac=True):\n\n        non_zeros = np.where(L != 0)[0]\n\n        for j in non_zeros:\n            # get the j-st column of X in sparse format\n            Xjs = data[indptr[j]:indptr[j+1]]\n            # get the non zero indices\n            idx_nz = indices[indptr[j]:indptr[j+1]]\n            beta_old = beta[j]\n            if compute_jac:\n                dbeta_old = dbeta[j]\n            zj = beta[j] + dual_var[idx_nz] @ Xjs / (L[j] * n_samples)\n            beta[j:j+1] = ST(zj, alphas[j] / L[j])\n            if compute_jac:\n                dzj = dbeta[j] + Xjs @ ddual_var[idx_nz] / (L[j] * n_samples)\n                dbeta[j:j+1] = np.abs(np.sign(beta[j])) * dzj\n                dbeta[j:j+1] -= alphas[j] * np.sign(beta[j]) / L[j]\n                # update residuals\n                ddual_var[idx_nz] -= Xjs * (dbeta[j] - dbeta_old)\n            dual_var[idx_nz] -= Xjs * (beta[j] - beta_old)\n\n    @staticmethod\n    @njit\n    def _update_bcd_jac_backward(X, alpha, grad, beta, v_t_jac, L):\n        sign_beta = np.sign(beta)\n        n_samples, n_features = X.shape\n        for j in (np.arange(sign_beta.shape[0] - 1, -1, -1)):\n            grad -= (v_t_jac[j]) * alpha * sign_beta[j] / L[j]\n            v_t_jac[j] *= np.abs(sign_beta[j])\n            v_t_jac -= v_t_jac[j] / (L[j] * n_samples) * X[:, j] @ X\n\n        return grad\n\n    @staticmethod\n    @njit\n    def _update_bcd_jac_backward_sparse(\n            data, indptr, indices, n_samples, n_features,\n            alpha, grad, beta, v_t_jac, L):\n        sign_beta = np.sign(beta)\n        for j in (np.arange(sign_beta.shape[0] - 1, -1, -1)):\n            if L[j] != 0:\n                Xjs = data[indptr[j]:indptr[j+1]]\n                idx_nz = indices[indptr[j]:indptr[j+1]]\n                grad -= (v_t_jac[j]) * alpha * sign_beta[j] / L[j]\n                v_t_jac[j] *= np.abs(sign_beta[j])\n                cste = v_t_jac[j] / (L[j] * n_samples)\n                for i in (np.arange(sign_beta.shape[0] - 1, -1, -1)):\n                    Xis = data[indptr[i]:indptr[i+1]]\n                    idx = indices[indptr[i]:indptr[i+1]]\n                    product = sparse_scalar_product(Xjs, idx_nz, Xis, idx)\n                    v_t_jac[i] -= cste * product\n\n        return grad\n\n    @staticmethod\n    def _get_pobj0(dual_var, beta, alphas, y=None):\n        n_samples = dual_var.shape[0]\n        return norm(y) ** 2 / (2 * n_samples)\n\n    @staticmethod\n    def _get_pobj(dual_var, X, beta, alphas, y=None):\n        n_samples = dual_var.shape[0]\n        return (\n            norm(dual_var) ** 2 / (2 * n_samples) +\n            np.abs(alphas * beta).sum())\n\n    @staticmethod\n    def _get_dobj(dual_var, X, beta, alpha, y=None):\n        # the dual variable is theta = (y - X beta) / (alpha n_samples)\n        n_samples = X.shape[0]\n        theta = dual_var / (alpha * n_samples)\n        norm_inf_XTtheta = np.max(np.abs(X.T @ theta))\n        if norm_inf_XTtheta > 1:\n            theta /= norm_inf_XTtheta\n        dobj = alpha * y @ theta\n        dobj -= alpha ** 2 * n_samples / 2 * (theta ** 2).sum()\n        return dobj\n\n    @staticmethod\n    def _get_jac(dbeta, mask):\n        return dbeta[mask]\n\n    @staticmethod\n    def get_full_jac_v(mask, jac_v, n_features):\n        \"\"\"TODO\n\n        Parameters\n        ----------\n        mask: TODO\n        jac_v: TODO\n        n_features: int\n            Number of features.\n        \"\"\"\n        # MM sorry I don't get what this does\n        return jac_v\n\n    @staticmethod\n    def get_mask_jac_v(mask, jac_v):\n        \"\"\"TODO\n\n        Parameters\n        ----------\n        mask: TODO\n        jac_v: TODO\n        \"\"\"\n        return jac_v\n\n    @staticmethod\n    def _init_dbeta0(mask, mask0, jac0):\n        size_mat = mask.sum()\n        if jac0 is not None:\n            dbeta0_new = init_dbeta0_new(jac0, mask, mask0)\n        else:\n            dbeta0_new = np.zeros(size_mat)\n        return dbeta0_new\n\n    @staticmethod\n    def _init_dbeta(n_features):\n        dbeta = np.zeros(n_features)\n        return dbeta\n\n    @staticmethod\n    def _init_ddual_var(dbeta, X, y, sign_beta, alpha):\n        return - X @ dbeta\n\n    @staticmethod\n    def _init_g_backward(jac_v0, n_features):\n        if jac_v0 is None:\n            return 0.0\n        else:\n            return jac_v0\n\n    @staticmethod\n    @njit\n    def _update_only_jac(Xs, y, dual_var, dbeta, ddual_var,\n                         L, alpha, sign_beta):\n        n_samples, n_features = Xs.shape\n        for j in range(n_features):\n            # dbeta_old = dbeta[j].copy()\n            dbeta_old = dbeta[j]\n            dbeta[j] += Xs[:, j].T @ ddual_var / (L[j] * n_samples)\n            dbeta[j] -= alpha * sign_beta[j] / L[j]\n            ddual_var -= Xs[:, j] * (dbeta[j] - dbeta_old)\n\n    @staticmethod\n    @njit\n    def _update_only_jac_sparse(\n            data, indptr, indices, y, n_samples, n_features,\n            dbeta, dual_var, ddual_var, L, alpha, sign_beta):\n        for j in range(n_features):\n            # get the j-st column of X in sparse format\n            Xjs = data[indptr[j]:indptr[j+1]]\n            # get the non zero idices\n            idx_nz = indices[indptr[j]:indptr[j+1]]\n            # store old beta j for fast update\n            dbeta_old = dbeta[j]\n            # update of the Jacobian dbeta\n            dbeta[j] += Xjs @ ddual_var[idx_nz] / (L[j] * n_samples)\n            dbeta[j] -= alpha * sign_beta[j] / L[j]\n            ddual_var[idx_nz] -= Xjs * (dbeta[j] - dbeta_old)\n\n    @staticmethod\n    @njit\n    def _reduce_alpha(alpha, mask):\n        return alpha\n\n    @staticmethod\n    def _get_grad(X, y, jac, mask, dense, alphas, v):\n        return alphas[mask] * np.sign(dense) @ jac\n\n    def proj_hyperparam(self, X, y, log_alpha):\n        \"\"\"Project hyperparameter on an admissible range of values.\n\n        Parameters\n        ----------\n        X: array-like, shape (n_samples, n_features)\n            Design matrix.\n        y: ndarray, shape (n_samples,)\n            Observation vector.\n        log_alpha: float\n            Logarithm of hyperparameter.\n\n        Returns\n        -------\n        log_alpha: float\n            Logarithm of projected hyperparameter.\n        \"\"\"\n        if not hasattr(self, \"log_alpha_max\"):\n            alpha_max = np.max(np.abs(X.T @ y))\n            alpha_max /= X.shape[0]\n            self.log_alpha_max = np.log(alpha_max)\n        return np.clip(log_alpha, self.log_alpha_max - 12,\n                       self.log_alpha_max + np.log(0.9))\n\n    @staticmethod\n    def get_L(X):\n        \"\"\"Compute Lipschitz constant of datafit.\n\n        Parameters\n        ----------\n        X: array-like, shape (n_samples, n_features)\n            Design matrix.\n\n        Returns\n        -------\n        L: float\n            The Lipschitz constant.\n        \"\"\"\n        if issparse(X):\n            return slinalg.norm(X, axis=0) ** 2 / (X.shape[0])\n        else:\n            return norm(X, axis=0) ** 2 / (X.shape[0])\n\n    def _use_estimator(self, X, y, alpha, tol):\n        if self.estimator is None:\n            raise ValueError(\"You did not pass a solver with sklearn API\")\n        self.estimator.set_params(tol=tol, alpha=alpha)\n        self.estimator.fit(X, y)\n        mask = self.estimator.coef_ != 0\n        dense = self.estimator.coef_[mask]\n        return mask, dense, None\n\n    @staticmethod\n    def reduce_X(X, mask):\n        \"\"\"Reduce design matrix to generalized support.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Design matrix.\n        mask : ndarray, shape (n_features,)\n            Generalized support.\n        \"\"\"\n        return X[:, mask]\n\n    @staticmethod\n    def reduce_y(y, mask):\n        \"\"\"Reduce observation vector to generalized support.\n\n        Parameters\n        ----------\n        y : ndarray, shape (n_samples,)\n            Observation vector.\n        mask : ndarray, shape (n_features,)  TODO shape n_samples right?\n            Generalized support.\n        \"\"\"\n        return y\n\n    def sign(self, x, log_alpha):\n        \"\"\"Get sign of iterate.\n\n        Parameters\n        ----------\n        x : ndarray, shape TODO\n        log_alpha : ndarray, shape TODO\n            Logarithm of hyperparameter.\n        \"\"\"\n        return np.sign(x)\n\n    def get_beta(self, X, y, mask, dense):\n        \"\"\"Return primal iterate.\n\n        Parameters\n        ----------\n        X: array-like, shape (n_samples, n_features)\n            Design matrix.\n        y: ndarray, shape (n_samples,)\n            Observation vector.\n        mask: ndarray, shape (n_features,)\n            Mask corresponding to non zero entries of beta.\n        dense: ndarray, shape (mask.sum(),)\n            Non zero entries of beta.\n        \"\"\"\n        return mask, dense\n\n    def get_jac_v(self, X, y, mask, dense, jac, v):\n        \"\"\"Compute hypergradient.\n\n        Parameters\n        ----------\n        X: array-like, shape (n_samples, n_features)\n            Design matrix.\n        y: ndarray, shape (n_samples,)\n            Observation vector.\n        mask: ndarray, shape (n_features,)\n            Mask corresponding to non zero entries of beta.\n        dense: ndarray, shape (mask.sum(),)\n            Non zero entries of beta.\n        jac: TODO\n        v: TODO\n        \"\"\"\n        return jac.T @ v(mask, dense)\n\n    @staticmethod\n    def get_mat_vec(X, y, mask, dense, log_alpha):\n        \"\"\"Returns a LinearOperator computing the matrix vector product\n        with the Hessian of datafit. It is necessary to avoid storing a\n        potentially large matrix, and keep advantage of the sparsity of X.\n\n        Parameters\n        ----------\n        X: array-like, shape (n_samples, n_features)\n            Design matrix.\n        y: ndarray, shape (n_samples,)\n            Observation vector.\n        mask: ndarray, shape (n_features,)\n            Mask corresponding to non zero entries of beta.\n        dense: ndarray, shape (mask.sum(),)\n            Non zero entries of beta.\n        log_alpha: ndarray\n            Logarithm of hyperparameter.\n        \"\"\"\n        X_m = X[:, mask]\n        n_samples, size_supp = X_m.shape\n\n        def mv(v):\n            return X_m.T @ (X_m @ v) / n_samples\n        return LinearOperator((size_supp, size_supp), matvec=mv)\n\n    def generalized_supp(self, X, v, log_alpha):\n        \"\"\"Generalized support of iterate.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Design matrix.\n        v : TODO\n        log_alpha : float\n            Log of hyperparameter.\n\n        Returns\n        -------\n        TODO\n        \"\"\"\n        return v\n\n    def get_jac_residual_norm(self, Xs, ys, n_samples, sign_beta, dbeta,\n                              dual_var, ddual_var, alpha):\n        return norm(ddual_var.T @ ddual_var +\n                    n_samples * alpha * sign_beta @ dbeta)\n\nimport numpy as np\nfrom numba import njit\nfrom numpy.linalg import norm\nfrom scipy.sparse import issparse\nimport scipy.sparse.linalg as slinalg\nfrom scipy.sparse.linalg import LinearOperator\n\nfrom sparse_ho.models.base import BaseModel\nfrom sparse_ho.utils import ST, init_dbeta0_new_p\n\n\nclass WeightedLasso(BaseModel):\n    r\"\"\"Linear Model trained with weighted L1 regularizer (aka weighted Lasso).\n\n    The optimization objective for weighted Lasso is:\n\n    ..math::\n\n        ||y - Xw||^2_2 / (2 * n_samples) + \\sum_i^{n_features} \\alpha_i |wi|\n\n    Parameters\n    ----------\n    estimator: instance of ``sklearn.base.BaseEstimator``\n        An estimator that follows the scikit-learn API.\n    \"\"\"\n\n    def __init__(self, estimator=None):\n        self.estimator = estimator\n\n    def _init_dbeta_ddual_var(self, X, y, mask0=None, jac0=None,\n                              dense0=None, compute_jac=True):\n        n_samples, n_features = X.shape\n        dbeta = np.zeros((n_features, n_features))\n        ddual_var = np.zeros((n_samples, n_features))\n        if jac0 is not None:\n            dbeta[np.ix_(mask0, mask0)] = jac0.copy()\n            ddual_var[:, mask0] = - X[:, mask0] @ jac0\n        return dbeta, ddual_var\n\n    def _init_beta_dual_var(self, X, y, mask0=None, dense0=None):\n        beta = np.zeros(X.shape[1])\n        if dense0 is None or len(dense0) == 0:\n            dual_var = y.copy()\n            dual_var = dual_var.astype(np.float)\n        else:\n            beta[mask0] = dense0.copy()\n            dual_var = y - X[:, mask0] @ dense0\n        return beta, dual_var\n\n    @staticmethod\n    @njit\n    def _update_beta_jac_bcd(\n            X, y, beta, dbeta, dual_var, ddual_var,\n            alpha, L, compute_jac=True):\n        n_samples, n_features = X.shape\n        non_zeros = np.where(L != 0)[0]\n\n        for j in non_zeros:\n            beta_old = beta[j]\n            if compute_jac:\n                dbeta_old = dbeta[j, :].copy()\n            zj = beta[j] + dual_var @ X[:, j] / (L[j] * n_samples)\n            beta[j:j+1] = ST(zj, alpha[j] / L[j])\n            if compute_jac:\n                dzj = dbeta[j, :] + X[:, j] @ ddual_var / (L[j] * n_samples)\n                dbeta[j:j+1, :] = np.abs(np.sign(beta[j])) * dzj\n                dbeta[j:j+1, j] -= alpha[j] * np.sign(beta[j]) / L[j]\n                # update residuals\n                ddual_var -= np.outer(X[:, j], (dbeta[j, :] - dbeta_old))\n            dual_var -= X[:, j] * (beta[j] - beta_old)\n\n    @staticmethod\n    @njit\n    def _update_beta_jac_bcd_sparse(\n            data, indptr, indices, y, n_samples, n_features, beta,\n            dbeta, dual_var, ddual_var, alphas, L, compute_jac=True):\n        non_zeros = np.where(L != 0)[0]\n\n        for j in non_zeros:\n            # get the j-st column of X in sparse format\n            Xjs = data[indptr[j]:indptr[j+1]]\n            # get non zero idices\n            idx_nz = indices[indptr[j]:indptr[j+1]]\n            ###########################################\n            beta_old = beta[j]\n            if compute_jac:\n                dbeta_old = dbeta[j, :].copy()\n            zj = beta[j] + dual_var[idx_nz] @ Xjs / (L[j] * n_samples)\n            beta[j:j+1] = ST(zj, alphas[j] / L[j])\n            if compute_jac:\n                dzj = dbeta[j, :] + Xjs @ ddual_var[idx_nz, :] / \\\n                    (L[j] * n_samples)\n                dbeta[j:j+1, :] = np.abs(np.sign(beta[j])) * dzj\n                dbeta[j:j+1, j] -= alphas[j] * np.sign(beta[j]) / L[j]\n                # update residuals\n                ddual_var[idx_nz, :] -= np.outer(\n                    Xjs, (dbeta[j, :] - dbeta_old))\n            dual_var[idx_nz] -= Xjs * (beta[j] - beta_old)\n\n    @staticmethod\n    @njit\n    def _update_bcd_jac_backward(\n            X, alpha, jac_t_v, beta, v_, L):\n        n_samples, n_features = X.shape\n        sign_beta = np.sign(beta)\n        for j in (np.arange(sign_beta.shape[0] - 1, -1, -1)):\n            jac_t_v[j] = jac_t_v[j] - (v_[j]) * alpha[j] * sign_beta[j] / L[j]\n            v_[j] *= np.abs(sign_beta[j])\n            v_ -= v_[j] / (L[j] * n_samples) * X[:, j] @ X\n        return jac_t_v\n\n    @staticmethod\n    def _get_pobj(dual_var, X, beta, alphas, y=None):\n        n_samples = dual_var.shape[0]\n        return (\n            norm(dual_var) ** 2 / (2 * n_samples) + norm(alphas * beta, 1))\n\n    @staticmethod\n    def _get_pobj0(dual_var, beta, alphas, y=None):\n        n_samples = dual_var.shape[0]\n        return norm(y) ** 2 / (2 * n_samples)\n\n    @staticmethod\n    def _get_jac(dbeta, mask):\n        return dbeta[np.ix_(mask, mask)]\n\n    @staticmethod\n    def _init_dbeta0(mask, mask0, jac0):\n        size_mat = mask.sum()\n        if jac0 is None:\n            dbeta0_new = np.zeros((size_mat, size_mat))\n        else:\n            dbeta0_new = init_dbeta0_new_p(jac0, mask, mask0)\n        return dbeta0_new\n\n    @staticmethod\n    def _init_dbeta(n_features):\n        dbeta = np.zeros((n_features, n_features))\n        return dbeta\n\n    @staticmethod\n    def _init_ddual_var(dbeta, X, y, sign_beta, alpha):\n        return - X @ dbeta\n\n    @staticmethod\n    def _init_g_backward(jac_v0, n_features):\n        if jac_v0 is None:\n            return np.zeros(n_features)\n        else:\n            return jac_v0\n\n    @staticmethod\n    @njit\n    def _update_only_jac(Xs, y, dual_var, dbeta, ddual_var, L,\n                         alpha, sign_beta):\n        n_samples, n_features = Xs.shape\n        for j in range(n_features):\n            dbeta_old = dbeta[j, :].copy()\n            dbeta[j:j+1, :] = dbeta[j, :] + Xs[:, j] @ ddual_var / \\\n                (L[j] * n_samples)\n            dbeta[j:j+1, j] -= alpha[j] * np.sign(sign_beta[j]) / L[j]\n            # update residuals\n            ddual_var -= np.outer(Xs[:, j], (dbeta[j, :] - dbeta_old))\n\n    @staticmethod\n    @njit\n    def _update_only_jac_sparse(\n            data, indptr, indices, y, n_samples, n_features, dbeta, dual_var,\n            ddual_var, L, alpha, sign_beta):\n        for j in range(n_features):\n            # get the j-st column of X in sparse format\n            Xjs = data[indptr[j]:indptr[j+1]]\n            # get the non zero idices\n            idx_nz = indices[indptr[j]:indptr[j+1]]\n            # store old beta j for fast update\n            dbeta_old = dbeta[j, :].copy()\n\n            dbeta[j:j+1, :] += Xjs @ ddual_var[idx_nz] / (L[j] * n_samples)\n            dbeta[j, j] -= alpha[j] * np.sign(sign_beta[j]) / L[j]\n            ddual_var[idx_nz] -= np.outer(Xjs, (dbeta[j] - dbeta_old))\n\n    # @njit\n    @staticmethod\n    def _reduce_alpha(alpha, mask):\n        return alpha[mask]\n\n    @staticmethod\n    def get_full_jac_v(mask, jac_v, n_features):\n        \"\"\"TODO\n\n        Parameters\n        ----------\n        mask: TODO\n        jac_v: TODO\n        n_features: int\n            Number of features.\n        \"\"\"\n        # MM sorry I don't get what this does\n        # TODO n_features should be n_hyperparams, right ?\n        res = np.zeros(n_features)\n        res[mask] = jac_v\n        return res\n\n    @staticmethod\n    def get_mask_jac_v(mask, jac_v):\n        \"\"\"TODO\n\n        Parameters\n        ----------\n        mask: TODO\n        jac_v: TODO\n        \"\"\"\n        return jac_v[mask]\n\n    @staticmethod\n    def _get_grad(X, y, jac, mask, dense, alphas, v):\n        size_supp = mask.sum()\n        jac_t_v = np.zeros(size_supp)\n        jac_t_v = alphas[mask] * np.sign(dense) * jac\n        return jac_t_v\n\n    def proj_hyperparam(self, X, y, log_alpha):\n        \"\"\"Project hyperparameter on an admissible range of values.\n\n        Parameters\n        ----------\n        X: array-like, shape (n_samples, n_features)\n            Design matrix.\n        y: ndarray, shape (n_samples,)\n            Observation vector.\n        log_alpha: ndarray, shape (n_features,)\n            Logarithm of hyperparameter.\n\n        Returns\n        -------\n        log_alpha: ndarray, shape (n_features,)\n            Logarithm of projected hyperparameter.\n        \"\"\"\n        if not hasattr(self, \"log_alpha_max\"):\n            alpha_max = np.max(np.abs(X.T @ y)) / X.shape[0]\n            self.log_alpha_max = np.log(alpha_max)\n        log_alpha = np.clip(log_alpha, self.log_alpha_max - 5,\n                            self.log_alpha_max + np.log(0.9))\n        return log_alpha\n\n    @staticmethod\n    def get_L(X):\n        \"\"\"Compute Lipschitz constant of datafit.\n\n        Parameters\n        ----------\n        X: array-like, shape (n_samples, n_features)\n            Design matrix.\n\n        Returns\n        -------\n        L: float\n            The Lipschitz constant.\n        \"\"\"\n        if issparse(X):\n            return slinalg.norm(X, axis=0) ** 2 / (X.shape[0])\n        else:\n            return norm(X, axis=0) ** 2 / (X.shape[0])\n\n    @staticmethod\n    def get_mat_vec(X, y, mask, dense, log_alpha):\n        \"\"\"Returns a LinearOperator computing the matrix vector product\n        with the Hessian of datafit. It is necessary to avoid storing a\n        potentially large matrix, and keep advantage of the sparsity of X.\n\n        Parameters\n        ----------\n        X: array-like, shape (n_samples, n_features)\n            Design matrix.\n        y: ndarray, shape (n_samples,)\n            Observation vector.\n        mask: ndarray, shape (n_features,)\n            Mask corresponding to non zero entries of beta.\n        dense: ndarray, shape (mask.sum(),)\n            Non zero entries of beta.\n        log_alpha: ndarray\n            Logarithm of hyperparameter.\n        \"\"\"\n        X_m = X[:, mask]\n        n_samples, size_supp = X_m.shape\n\n        def mv(v):\n            return X_m.T @ (X_m @ v) / n_samples\n        return LinearOperator((size_supp, size_supp), matvec=mv)\n\n    def _use_estimator(self, X, y, alpha, tol):\n        self.estimator.set_params(tol=tol)\n        self.estimator.weights = alpha\n        self.estimator.fit(X, y)\n        mask = self.estimator.coef_ != 0\n        dense = (self.estimator.coef_)[mask]\n        return mask, dense, None\n\n    @staticmethod\n    def reduce_X(X, mask):\n        \"\"\"Reduce design matrix to generalized support.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Design matrix.\n        mask : ndarray, shape (n_features,)\n            Generalized support.\n        \"\"\"\n        return X[:, mask]\n\n    @staticmethod\n    def reduce_y(y, mask):\n        \"\"\"Reduce observation vector to generalized support.\n\n        Parameters\n        ----------\n        y : ndarray, shape (n_samples,)\n            Observation vector.\n        mask : ndarray, shape (n_features,)  TODO shape n_samples right?\n            Generalized support.\n        \"\"\"\n        return y\n\n    def sign(self, x, log_alpha):\n        \"\"\"Get sign of iterate.\n\n        Parameters\n        ----------\n        x : ndarray, shape TODO\n        log_alpha : ndarray, shape TODO\n            Logarithm of hyperparameter.\n        \"\"\"\n        return np.sign(x)\n\n    def get_beta(self, X, y, mask, dense):\n        \"\"\"Return primal iterate.\n\n        Parameters\n        ----------\n        X: array-like, shape (n_samples, n_features)\n            Design matrix.\n        y: ndarray, shape (n_samples,)\n            Observation vector.\n        mask: ndarray, shape (n_features,)\n            Mask corresponding to non zero entries of beta.\n        dense: ndarray, shape (mask.sum(),)\n            Non zero entries of beta.\n        \"\"\"\n        # TODO what's the use of this function? it does nothing for all models\n        return mask, dense\n\n    def get_jac_v(self, X, y, mask, dense, jac, v):\n        \"\"\"Compute hypergradient.\n\n        Parameters\n        ----------\n        X: array-like, shape (n_samples, n_features)\n            Design matrix.\n        y: ndarray, shape (n_samples,)\n            Observation vector.\n        mask: ndarray, shape (n_features,)\n            Mask corresponding to non zero entries of beta.\n        dense: ndarray, shape (mask.sum(),)\n            Non zero entries of beta.\n        jac: TODO\n        v: TODO\n        \"\"\"\n        # TODO this is the same for Lasso, Enet, Wlasso. Maybe inherit from\n        # a common class, LinearModelPrimal or something?\n        return jac.T @ v(mask, dense)\n\n    def generalized_supp(self, X, v, log_alpha):\n        \"\"\"Generalized support of iterate.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Design matrix.\n        v : TODO\n        log_alpha : float\n            Log of hyperparameter.\n\n        Returns\n        -------\n        TODO\n        \"\"\"\n        return v\n\n    def get_jac_residual_norm(self, Xs, ys, n_samples, sign_beta,\n                              dbeta, dual_var, ddual_var, alpha):\n        return(\n            norm(ddual_var.T @ ddual_var +\n                 n_samples * alpha * sign_beta @ dbeta))\n\n\n\n# Authors: Quentin Bertrand <quentin.bertrand@inria.fr>\n#          Quentin Klopfenstein <quentin.klopfenstein@u-bourgogne.fr>\n#\n# License: BSD (3-clause)\n\n# This files contains the functions to perform first order descent for HO\n# hyperparameter setting\n\n\nimport numpy as np\nfrom hyperopt import hp\nfrom hyperopt import fmin, tpe, rand\nfrom functools import partial\nfrom sklearn.utils import check_random_state\n\n\ndef grad_search(\n        algo, criterion, model, optimizer, X, y, alpha0, monitor):\n    \"\"\"\n    Parameters\n    ----------\n    algo: instance of BaseAlgo\n        algorithm used to compute hypergradient.\n    criterion:  instance of BaseCriterion\n        criterion to optimize during hyperparameter optimization\n        (outer optimization problem).\n    model: instance of BaseModel\n        model on which hyperparameter has to be selected\n        (inner optimization problem).\n    optimizer: instance of Optimizer\n        optimizer used to minimize the criterion (outer optimization)\n    X: array like of shape (n_samples, n_features)\n        Design matrix.\n    y: array like of shape (n_samples,)\n        Target.\n    alpha0: float\n        initial value of the hyperparameter alpha.\n    monitor: instance of Monitor\n        used to store the value of the cross-validation function.\n\n\n    Returns\n    -------\n    XXX missing\n    \"\"\"\n\n    def _get_val_grad(log_alpha, tol, monitor):\n        return criterion.get_val_grad(\n            model, X, y, log_alpha, algo.compute_beta_grad, tol=tol,\n            monitor=monitor)\n\n    def _proj_hyperparam(log_alpha):\n        return criterion.proj_hyperparam(model, X, y, log_alpha)\n\n    return optimizer._grad_search(\n        _get_val_grad, _proj_hyperparam, np.log(alpha0), monitor)\n\n\ndef hyperopt_wrapper(\n        algo, criterion, model, X, y, alpha_min, alpha_max, monitor,\n        max_evals=50, tol=1e-5, random_state=42, t_max=100_000,\n        method='bayesian', size_space=1):\n    \"\"\"\n    Parameters\n    ----------\n    algo: instance of BaseAlgo\n        algorithm used to compute hypergradient.\n    criterion:  instance of BaseCriterion\n        criterion to optimize during hyperparameter optimization\n        (outer optimization problem).\n    model:  instance of BaseModel\n        model on which hyperparameter has to be selected\n        (inner optimization problem).\n    X: array like of shape (n_samples, n_features)\n        Design matrix.\n    y: array like of shape (n_samples,)\n        Target.\n    alpha_min: float\n        minimum value for the regularization coefficient alpha.\n    alpha_max: float\n        maximum value for the regularization coefficient alpha.\n    monitor: instance of Monitor\n        used to store the value of the cross-validation function.\n    max_evals: int (default=50)\n        maximum number of evaluation of the function\n    tol: float (default=1e-5)\n        tolerance for TODO\n    random_state: int or instance of RandomState\n        Random number generator used for reproducibility.\n    t_max: int, optional (default=100_000)\n        TODO\n    method: 'random' | 'bayesian' (default='bayesian')\n        method for hyperopt\n    size_space: int (default=1)\n        size of the hyperparameter space\n\n    Returns\n    -------\n    monitor:\n        The instance of Monitor used during iterations.\n    \"\"\"\n\n    def objective(log_alpha):\n        log_alpha = np.array(log_alpha)\n        val_func = criterion.get_val(\n            model, X, y, log_alpha, monitor, tol=tol)\n        return val_func\n\n    # TODO, also size_space = n_hyperparam ?\n    space = [\n        hp.uniform(str(dim), np.log(alpha_min), np.log(alpha_max)) for\n        dim in range(size_space)]\n\n    rng = check_random_state(random_state)\n\n    if method == \"bayesian\":\n        algo = partial(tpe.suggest, n_startup_jobs=5)\n        fmin(\n            objective, space, algo=algo, max_evals=max_evals,\n            timeout=t_max, rstate=rng)\n    elif method == \"random\":\n        fmin(\n            objective, space, algo=rand.suggest, max_evals=max_evals,\n            timeout=t_max, rstate=rng)\n    return monitor\n\nimport numpy as np\nfrom numpy.linalg import norm\n\nfrom sparse_ho.optimizers.base import BaseOptimizer\n\n\nclass LineSearch(BaseOptimizer):\n    \"\"\"Gradient descent with line search for the outer problem.\n\n    The code is taken from here:\n    https://github.com/fabianp/hoag/blob/master/hoag/hoag.py\n\n    Parameters\n    ----------\n    n_outer: int, optional (default=100).\n        number of maximum updates of alpha.\n    verbose: bool, optional (default=False)\n        Verbosity.\n    tolerance_decrease: string, optional (default=\"constant\")\n        Tolerance decrease strategy for approximate gradient.\n    tol : float, optional (default=1e-5)\n        Tolerance for the inner optimization solver.\n    t_max: float, optional (default=10000)\n        Maximum running time threshold in seconds.\n    \"\"\"\n\n    def __init__(\n            self, n_outer=100, verbose=False, tolerance_decrease='constant',\n            tol=1e-5, t_max=10000):\n        self.n_outer = n_outer\n        self.verbose = verbose\n        self.tolerance_decrease = tolerance_decrease\n        self.tol = tol\n        self.t_max = t_max\n\n    def _grad_search(\n            self, _get_val_grad, proj_hyperparam, log_alpha0, monitor):\n\n        is_multiparam = isinstance(log_alpha0, np.ndarray)\n        if is_multiparam:\n            log_alphak = log_alpha0.copy()\n            old_log_alphak = log_alphak.copy()\n        else:\n            log_alphak = log_alpha0\n            old_log_alphak = log_alphak\n\n        grad_norms = []\n\n        L_log_alpha = None\n        value_outer_old = np.inf\n\n        if self.tolerance_decrease == 'exponential':\n            seq_tol = np.geomspace(1e-2, self.tol, self.n_outer)\n        else:\n            seq_tol = self.tol * np.ones(self.n_outer)\n\n        for i in range(self.n_outer):\n            tol = seq_tol[i]\n            try:\n                old_tol = seq_tol[i - 1]\n            except Exception:\n                old_tol = seq_tol[0]\n            value_outer, grad_outer = _get_val_grad(\n                log_alphak, tol=tol, monitor=monitor)\n\n            grad_norms.append(norm(grad_outer))\n            if np.isnan(grad_norms[-1]):\n                print(\"Nan present in gradient\")\n                break\n\n            if L_log_alpha is None:\n                if grad_norms[-1] > 1e-3:\n                    # make sure we are not selecting a step size\n                    # that is too small\n                    if is_multiparam:\n                        L_log_alpha = grad_norms[-1] / np.sqrt(len(log_alphak))\n                    else:\n                        L_log_alpha = grad_norms[-1]\n                else:\n                    L_log_alpha = 1\n            step_size = (1. / L_log_alpha)\n            try:\n                old_log_alphak = log_alphak.copy()\n            except Exception:\n                old_log_alphak = log_alphak\n            log_alphak -= step_size * grad_outer\n\n            incr = norm(step_size * grad_outer)\n            C = 0.25\n            factor_L_log_alpha = 1.0\n            if value_outer <= value_outer_old + C * tol + \\\n                    old_tol * (C + factor_L_log_alpha) * incr - \\\n                    factor_L_log_alpha * (L_log_alpha) * incr * incr:\n                L_log_alpha *= 0.95\n                if self.verbose > 1:\n                    print('increased step size')\n                log_alphak -= step_size * grad_outer\n\n            elif value_outer >= 1.2 * value_outer_old:\n                if self.verbose > 1:\n                    print('decrease step size')\n                # decrease step size\n                L_log_alpha *= 2\n                if is_multiparam:\n                    log_alphak = old_log_alphak.copy()\n                else:\n                    log_alphak = old_log_alphak\n                print('!!step size rejected!!', value_outer, value_outer_old)\n                value_outer, grad_outer = _get_val_grad(\n                    log_alphak, tol=tol, monitor=monitor)\n\n                tol *= 0.5\n            else:\n                old_log_alphak = log_alphak.copy()\n                log_alphak -= step_size * grad_outer\n\n            log_alphak = proj_hyperparam(log_alphak)\n            value_outer_old = value_outer\n\n            if self.verbose:\n                print(\n                    \"Iteration %i/%i || \" % (i+1, self.n_outer) +\n                    \"Value outer criterion: %.2e || \" % value_outer +\n                    \"norm grad %.2e\" % norm(grad_outer))\n            if monitor.times[-1] > self.t_max:\n                break\n        return log_alphak, value_outer, grad_outer",
        "experimental_info": "The methodology is evaluated using various Lasso-type estimators, including Lasso, ElasticNet, Weighted Lasso, and Sparse Logistic Regression.\n\n**Datasets and Data Splitting:**\n- 'rcv1.binary', 'simu' (synthetic data generated by `make_classification` or `make_correlated_data`), and MEG data (from `mne.datasets.sample`) are used.\n- Data is commonly split into training and validation sets (e.g., 50/50 split), with `idx_train = np.arange(0, n_samples // 2)` and `idx_val = np.arange(n_samples // 2, n_samples)`.\n- K-Fold cross-validation (`sklearn.model_selection.KFold`) is also employed, for example, with `n_splits=5` and `shuffle=True`.\n\n**Hyperparameter Setup:**\n- The maximum regularization parameter (`alpha_max`) is typically computed as `np.max(np.abs(X[idx_train, :].T @ y[idx_train])) / len(idx_train)` (adjusted for specific models like Logistic Regression or Elastic Net).\n- Hyperparameters are parametrized exponentially to avoid positivity constraints and scaling issues. For instance, `alphas = alpha_max * np.geomspace(1, 0.0001, n_alphas)` creates a logarithmically spaced grid of 30 values (e.g., `n_alphas=30`).\n- Starting points for gradient-based optimizers (`alpha0`) are set as a fraction of `alpha_max`, such as `alpha_max / 10` or `np.array([alpha_max * 0.9, alpha_max * 0.9])` for 2D hyperparameters.\n\n**Inner Optimization (Lasso-type Solvers):**\n- State-of-the-art Lasso solvers are used, including:\n  - `celer.Lasso(penalty='l1', fit_intercept=False, max_iter=50, warm_start=True, tol=...)\n  - `celer.ElasticNet(fit_intercept=False, max_iter=50, warm_start=True, tol=...)\n  - `sklearn.linear_model.LogisticRegression(penalty='l1', fit_intercept=False, solver='saga', max_iter=100, tol=...)\n- Inner solver tolerance (`tol`) typically ranges from `1e-8` to `1e-3`.\n- Maximum iterations (`max_iter`) for the inner solver vary, e.g., `50` to `100` for Celer models, or `1000` to `100000` for scikit-learn models.\n- `warm_start=True` is generally enabled.\n\n**Implicit Forward Iterative Differentiation Algorithm:**\n- The `sparse_ho.ImplicitForward` algorithm is central to computing the hypergradient.\n- It is configured with `tol_jac` (tolerance for Jacobian computation), typically `1e-8` or `1e-3`.\n- `n_iter_jac` (maximum iterations for Jacobian computation) is set, e.g., `100` or `1000`.\n\n**Outer Optimization (Gradient-based Optimizers):**\n- `grad_search` is used to optimize the hyperparameters.\n- Various gradient-based optimizers are employed:\n  - `LineSearch(n_outer=10, tol=...)`\n  - `GradientDescent(n_outer=10, step_size=100, p_grad_norm=1.5, verbose=True, tol=...)`\n  - `Adam(n_outer=10, lr=0.11, beta_1=0.9, beta_2=0.999)`\n- `n_outer` (number of outer iterations) is typically `10` or `25` (up to `100` in some `expes`).\n\n**Overall Approach:**\n- Hyperparameters are optimized as a bi-level problem using implicit differentiation, leveraging the fixed-point iteration property of proximal BCD algorithms.\n- The Jacobian is computed by applying forward differentiation recursion steps restricted to the identified support, without explicitly solving a linear system."
      }
    },
    {
      "title": "Stochastic Marginal Likelihood Gradients using Neural Tangent Kernels",
      "abstract": "Selecting hyperparameters in deep learning greatly impacts its effectiveness\nbut requires manual effort and expertise. Recent works show that Bayesian model\nselection with Laplace approximations can allow to optimize such\nhyperparameters just like standard neural network parameters using gradients\nand on the training data. However, estimating a single hyperparameter gradient\nrequires a pass through the entire dataset, limiting the scalability of such\nalgorithms. In this work, we overcome this issue by introducing lower bounds to\nthe linearized Laplace approximation of the marginal likelihood. In contrast to\nprevious estimators, these bounds are amenable to stochastic-gradient-based\noptimization and allow to trade off estimation accuracy against computational\ncomplexity. We derive them using the function-space form of the linearized\nLaplace, which can be estimated using the neural tangent kernel.\nExperimentally, we show that the estimators can significantly accelerate\ngradient-based hyperparameter optimization.",
      "full_text": "Stochastic Marginal Likelihood Gradients using Neural Tangent Kernels Alexander Immer 1 2 Tycho F.A. van der Ouderaa3 Mark van der Wilk 3 Gunnar R¨atsch 1 Bernhard Sch¨olkopf 2 Abstract Selecting hyperparameters in deep learning greatly impacts its effectiveness but requires man- ual effort and expertise. Recent works show that Bayesian model selection with Laplace approxi- mations can allow to optimize such hyperparame- ters just like standard neural network parameters using gradients and on the training data. How- ever, estimating a single hyperparameter gradi- ent requires a pass through the entire dataset, limiting the scalability of such algorithms. In this work, we overcome this issue by introducing lower bounds to the linearized Laplace approxi- mation of the marginal likelihood. In contrast to previous estimators, these bounds are amenable to stochastic-gradient-based optimization and allow to trade off estimation accuracy against compu- tational complexity. We derive them using the function-space form of the linearized Laplace, which can be estimated using the neural tangent kernel. Experimentally, we show that the estima- tors can significantly accelerate gradient-based hyperparameter optimization. 1. Introduction Deep learning models have many hyperparameters that need to be chosen properly to achieve good performance. For example, making the right architecture, regularization, or data augmentation choices remains a problem that requires significant human effort and computation. This is even apparent within the field of deep learning itself, where the state-of-the-art settings change frequently. Such manual trial-and-error procedure is inefficient and can hinder the application of deep learning to new problems. A more automatic procedure could be much more sustainable. 1Department of Computer Science, ETH Zurich, Switzerland 2Max Planck Institute for Intelligent Systems, T¨ubingen, Germany 3Imperial College London, UK. Correspondence to: Alexander Immer <alexander.immer@inf.ethz.ch>. Proceedings of the 40 th International Conference on Machine Learning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright 2023 by the author(s). 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Runtime (s) 0.8 1.0 1.2 1.4 1.6 1.8 2.0 2.2 2.4 Negative Log Marginal Likelihood Pareto Full GGN NTK KFAC Diag GGN 10 20 50 100 250 500 1000 2500 5000 10000 batch size Figure 1.Pareto frontier between marginal likelihood estimator tightness and runtime for invariance learning on rotated MNIST. Our proposed estimators enable stochastic approximation with varying batch size in contrast to the four previously used full-batch estimators (batch size of 10 000in black). NTK -based estimators on subsets of data perform well at more than 10× speed-up. Automated machine learing (AutoML) methods have the potential to greatly reduce the cost of deploying deep learn- ing for new problems (Hutter et al., 2019). Commonly, black-box algorithms are applied in this setting to optimize hyperparameters towards the best possible validation perfor- mance. For example, neural architectures can be optimized by training each option to convergence and assessing the val- idation performance repeatedly. However, such iterative pro- cedures require training many models to convergence and suffer from high-dimensional hyperparameters due to their black-box nature. We posit that (stochastic-)gradient-based optimization of hyperparameters would be more desirable. Bayesian model selection, where we consider hyperparame- ters and neural network weights jointly as part of a proba- bilistic model, is amenable to gradient-based optimization and also does not require any validation data (MacKay, 2003). Gradient-based optimization of hyperparameters tends to suffer less from high dimensionality than iterative black-box methods (Lorraine et al., 2020). Further, in such integrated procedure, the hyperparameters can be jointly optimized with the neural network weights (Foresee & Ha- gan, 1997; Immer et al., 2021a). While this is theoretically appealing, estimating the hyperparameter gradient is costly as it requires differentiating the marginal likelihood, i.e., the normalization constant of the posterior. 1 arXiv:2306.03968v1  [stat.ML]  6 Jun 2023Stochastic Marginal Likelihood Gradients using Neural Tangent Kernels NC NC M M NC NC N N P P 2 2 P P 2 2P P P P P P NC NC N N Theorem 1. (Structured Parametric Bounds) Theorem 2. (Subset-of-Data Bounds) Theorem 3. (Subset-of-Data Parametric Bounds) number of data pointsN number of parametersP number of outputs (e.g. classes)C batch sizeM number of parameters in  ’th layersP P P s P P Eq. (6) Figure 2.Schematic overview of the derived lower bounds to the linearized Laplace marginal likelihood. The parametric bounds on the top left justify commonly used block-diagonal and diagonal Hessian and Gauss-Newton approximations. The bounds on the right are derived from the dual NTK perspective and give a novel family of lower bounds. These lower bounds enable stochastic marginal likelihood estimation and gradients with both NTK -based estimators and parametric variants, e.g., Gauss-Newton, by translating them back (bottom). To enable Bayesian model selection of hyperparameters in deep learning with marginal likelihood gradients, Immer et al. (2021a) recently revisited the use of the Laplace ap- proximation (MacKay, 1992a) with modern Gauss-Newton approximations. While these structured linearized Laplace approximations can be used for architecture comparison and gradient-based learning of data augmentations and reg- ularization strength (Immer et al., 2021a; 2022a;b; Antor´an et al., 2022b; Daxberger et al., 2021; Hataya & Nakayama, 2021), each hyperparameter gradient requires a pass through the entire training dataset. In particular, the objective does not allow (unbiased) stochastic gradient estimation. In the present work, we derive lower bounds to the Laplace approximation of the marginal likelihood that are amenable to stochastic-gradient-based optimization and show that currently used structured approximations are in fact lower bounds (overview in Figure 2). Instead of a whole pass through the training data, our estimators require only batches of data to estimate hyperparameter gradients, which can greatly increase the scalability of such algorithms. The size of the batches can further be chosen to achieve a trade-off between estimation quality and runtime complex- ity (cf. Figure 1). We derive the lower bounds from the dual form of the linearized Laplace (Khan et al., 2019; Im- mer et al., 2021b), which relies on the (empirical) neural tangent kernel ( NTK , Jacot et al., 2018). By partitioning the NTK using batches, we obtain a lower bound to the linearized Laplace marginal likelihood, which permits un- biased stochastic estimates and gradients. Empirically, our estimators enable up to 25-fold acceleration of hyperparam- eter optimization and application to larger datasets. 2. Background We consider supervised learning tasks with inputsxn ∈ RD and targets yn ∈ RC giving dataset D = {(xn, yn)}N n=1 with N pairs. We model the targets given inputs with a neu- ral network. We parameterize the neural networkf, as usual, with weights w ∈ RP , but additionally make the depen- dence on eventual hyperparameters h, such as architecture, explicit, and have f(x; w, h) ∈ RC. Assuming the data are i.i.d., we can define a probabilistic model with likelihood p(D|w, h) = QN n=1 p(yn|xn, w, h) that depends on the neural network. Further, we have a prior p(w|h). 2.1. Bayesian Model Selection In principle, Bayesian inference makes no distinction be- tween hyperparameters h and weights w and simply in- fers both jointly. With an additional prior p(h), this means that we obtain the joint posterior p(w, h|D) ∝ p(D|w, h)p(w|h)p(h). While there exist methods to ap- proximate the posterior p(w|D, h) efficiently in deep learn- ing, Bayesian inference of the hyperparameters h is compli- cated due to their heterogeneous and complicated structure, e.g., it could require a distribution over neural architectures. Instead, it is common to perform empirical Bayes to select hyperparameters h by optimizing the marginal likelihood, p(D|h) = Z p(D|w, h)p(w|h) dw , (1) which implements Occam’s razor, performing a trade-off between model complexity and accuracy (Rasmussen & Ghahramani, 2001; Bishop, 2006). This point approxima- 2Stochastic Marginal Likelihood Gradients using Neural Tangent Kernels tion can work well unless the number of hyperparameters is too large, which leads to overfitting (Ober et al., 2021). We can optimize the marginal likelihood by gradient ascent on the differentiable hyperparameters, ht+1←ht + ∇h log p(D|h)|h=ht, (2) similarly to how we differentiate the log joint, or regularized log likelihood, to optimize neural network weights w. 2.2. Linearized Laplace Approximation The linearized Laplace ( LA) approximation (MacKay, 1992a) provides an approximation to the log marginal likeli- hood and its scalable variants are currently among the few approximations that have proven effective for deep learning hyperparameter optimization. First, the neural network is linearized at a mode of the posterior ˜w, flin ˜w (x; w, h) def = f(x; ˜w, h) +J˜w(x; h)(w − ˜w), (3) where J denotes the Jacobian of the neural network w.r.t. weights with entries [J˜w]cp = ∂fc(x;w,h) ∂wp |w= ˜w. Further, we have the Hessian of the negative log likelihood w.r.t.f, Λ˜w(x; h) def = −∇2 f log p(y|x, ˜w, h) ∈ RC×C, (4) which is positive semidefinite for common likelihoods used in classification and regression (Murphy, 2012). The linearized Laplace approximation log q(D|h) to the log marginal likelihood log p(D|h) is then given by log q˜w(D|h) = logp(D| ˜w, h) + logp( ˜w|h) − 1 2 log |JT ˜wΛ˜wJ˜w + P0| + P 2 log 2π, (5) where J˜w ∈ RNC ×P denotes stacked Jacobians J˜w(xn; h), and Λ˜w ∈ RNC ×NC is a block-diagonal matrix with blocks Λ˜w(xn; h) for n = 1, .., N, respectively, and we have sup- pressed the dependency on hyperparameters h for nota- tional brevity. The matrix P0 ∈ RP×P is the Hessian w.r.t. weights w of the log prior log p(w|h) and is diagonal here. Because the first two terms of the approximation constitute the log joint, i.e., the training loss, all terms are simple to estimate and differentiate except for the (log-)determinant, which can be written equivalently (Immer et al., 2021a) as |JT ˜wΛ˜wJ˜w + P0| = |J˜wP−1 0 JT ˜wΛ˜w + I||P0|, (6) and allows us to either compute the log-determinant of aP × P or NC ×NC matrix. Unfortunately, the log-determinant prohibits unbiased stochastic gradients on batches of data and is therefore not amenable to optimization with SGD. 2.3. Generalized Gauss-Newton and Neural Tangents The two alternative ways to compute the determinant in Equation 6 use either the generalized Gauss-Newton (GGN ) form or the neural tangent kernel (NTK ) form. In particular, H def = JT ˜wΛ˜wJ˜w and K def = J˜wP−1 0 JT ˜wΛ˜w (7) are the GGN approximation to the Hessian and the NTK with additional scaling by P−1 0 and Λ˜w, respectively.1 To obtain marginal likelihood gradients, we need to estimate and differentiate these two quantities. However, they are intractable for deep neural networks due to large numbers of parameters P and many data points and outputs NC . The GGN can be made tractable by structured block-diagonal and diagonal approximations (Martens & Grosse, 2015). Without loss of generality, we assume ordered parameter in- dices {1, .., P}. With a contiguous partition2 P = {Ps}S s=1 of parameter indices, we have H ≈ HP =   HP1 ··· 0 ... ... ... 0 ··· HPS   = HP1 ⊕...⊕| {z } block-diagonal stacking HPS , (8) a block-diagonal approximation where each block corre- sponds to a subset Ps. For example, if each Ps corresponds to the parameters of one particular layer, we recover a block- diagonal layer-wise approximation like KFAC (Martens & Grosse, 2015). If each set only contains a single parameter index, Ps = {s}, we recover a diagonal approximation. 2.4. Hyperparameter Optimization for Deep Learning To optimize general hyperparameters, we estimate and dif- ferentiate Laplace approximations during neural network training using scalable approximations (Immer et al., 2021a). The form of hyperparameters h governs the complexity of obtaining their gradients. For example, the typical problem of optimizing the prior precision, P0, is relatively simple because it acts linearly on the GGN and NTK . However, gen- eral hyperparameters, such as invariances, affect the forward pass of the neural network and act non-linearly on GGN and NTK . Therefore, they require higher-order differentiation. In the following, we illustrate the derived bounds for both settings, i.e., selecting a scalar or layer-wise prior precision (weight decay) on MNIST, and rotational invariance on rotated MNIST. We use a subset of 1000 random digits and apply a small 3-layer convolutional neural network with linear output layer. This setting ensures that we can compute the exact linearized Laplace in Equation 5 as a reference. 1We recover the standard finite width NTK for a Gaussian like- lihood and prior with unit variance, i.e., K = J˜wJT ˜w. 2A partition of a set S is a set P of disjoint subsets of S whose union is equal to S. It is contiguous if the elements in P are ordered and the order remains preserved in S. 3Stochastic Marginal Likelihood Gradients using Neural Tangent Kernels 3. Structured Parametric Laplace Approximations are Lower Bounds Practical linearized Laplace approximations of deep neu- ral networks use structured GGN approximations, such as diagonal and block-diagonal variants, to make the log- determinant computation tractable (Immer et al., 2021a; Daxberger et al., 2021; Antor ´an et al., 2022b). However, it is unclear to what extent these approximations are re- lated to the exact linearized LA. The following Theorem shows that such approximations are in fact lower bounds and therefore justifies their use better. This is similar to vari- ational inference (Blei et al., 2017), where more restrictive approximations, such as a diagonal instead of a multivariate Gaussian, lead to more slack in the evidence lower bound. Theorem 1 (Parametric structured lower bound). For any partitioning P of the parameter indices {1, .., P}, the block- diagonal approximation of HP to H results in a lower bound to the LA log marginal likelihood (Equation 5), i.e., log q˜w(D|h) ≥ log p(D, ˜w|h)− 1 2 log |HP +P0|+c, (9) where c = P 2 log 2π. Further, a refinement3 of the partition P to P′ will result in a lower bound with even more slack. Example 1.1. The block-diagonal Laplace approximation that only considers correlations within layers, similar to the popular KFAC4-Laplace (Ritter et al., 2018; Daxberger et al., 2021), is a lower bound to the linearized Laplace approximation. A diagonal Laplace approximation that only considers marginal variances of parameters is a further lower bound of such block-diagonal approximation. For a proof, refer to App. A. The bound holds for commonly used block-diagonal approximations and is further agnostic to the type of Hessian approximation. That is, it holds for the Hessian, GGN , or (empirical) Fisher variants of the LA alike. In Figure 3, we show the bound for different hyperparameter values h of the prior precision and rotational invariance in the illustrative setting described in Sec. 2.4. For the prior precision, only the diagonal approximation has a large slack and significantly different optimum, which is in line with previous empirical observations (MacKay, 1995; Daxberger et al., 2021). Perhaps surprisingly, it can still be useful to select the right rotational invariance because its shape and thus optimum is in line with the better approximations. While the above bounds justify existing Laplace approxi- mations, they do not improve scalability with dataset size, because we have partitioned the parameters rather than the dataset. To do so, we employ the dual NTK representation. 3A refinement P′ of P is a partition such that for all P′ s ∈ P′ there exists Ps ∈ Psuch that P′ s ⊆ Ps. 4Note that KFAC is a further approximation to the block- diagonal and a bound is not theoretically guaranteed. Nonetheless, it does seem to hold in practice. 10−2 100 102 prior precision −6 −4 −2 log q˜w(D|h) full blockdiag kron diag −3 −2 0 π/2 π rotational invariance −13 −12 Figure 3.Illustration of the parametric bound (Theorem 1) with commonly used approximation structures for prior precision and rotational invariance as hyperparameters h on MNIST with CNN. 4. Stochastic Gradients using the NTK While structured parametric lower bounds to the linearized Laplace marginal likelihood give good performance on model selection, a single hyperparameter gradient still re- quires an entire pass through the training data. This is because the log-determinant is in general not separable across data points and computing it on a subset of data would lead to an uncontrolled upper bound to the marginal likelihood. To overcome this limitation, we use the log- determinant in its dual NTK form in Equation 6 to devise lower bounds that enable batched computation on subsets of data and thus stochastic-gradient-based optimization. Our bounds work for NTK (Sec. 4.1) or parametric and structured GGN (Sec. 4.2) estimators alike and enable tightening the lower bound using larger batches of data or improving the partitioning of data points into batches (Sec. 4.3). 4.1. Subset-of-Data Kernel Bound Using the NTK form of the log-determinant, we construct lower bounds for subsets of data that enable stochastic marginal likelihood gradients and can therefore greatly im- prove the efficiency of hyperparameter optimization. Due to the cubic scaling with the number of data points, the NTK form has so far been only used in special cases (Immer et al., 2021a; Antor´an et al., 2022a). Using our bounds, how- ever, the NTK form on subsets of data becomes a tractable alternative for parametric variants like KFAC. Further, com- puting the NTK can be more architecture-agnostic (Novak et al., 2019; 2022) because it relies on plain Jacobians while structured approximations like KFAC are often non-trivial to compute or extend to custom architectures (Dangel et al., 2019; Osawa, 2021; Botev & Martens, 2022). Instead of partitioning the parameters as in Theorem 1, we partition the N inputs and C outputs and make use of the NTK form, which naively requires computing a NC × NC matrix, to obtain a lower bound for subsets of data. In particular, we have the set of indicesI = {(n, c) | 1 ≤ n ≤ N, 1 ≤ c ≤ C} corresponding to inputs xn and outputs fc of the neural network. The index order is fixed w.r.t. K but arbitrary. With partitioning B = {Bm}M m=1 of I, we have K ≈ KB = KB1 ⊕ ··· ⊕KBM , (10) 4Stochastic Marginal Likelihood Gradients using Neural Tangent Kernels 10−2 100 102 prior precision −2 −4 −6 log q˜w(D|h) 0 π/2 π rotational invariance −2 −4 −6 100 200 500 1000 2500 5000 10000 indices |Bm| Figure 4.Illustration of the subset-of-data NTK lower bound (The- orem 2) with varying subset sizes for prior precision and rotational invariance as hyperparameters h on the two MNIST variants. where each KBm is a NTK on a subset of data and out- puts. Mathematically, we have KBm = JBmP−1 0 JT BmΛBm where JBm ∈ R|Bm|×P are the Jacobians for the input- output pairs and ΛBm ∈ R|Bm|×|Bm| the corresponding log-likelihood Hessian terms. Theorem 2 (Data subset lower bound). For any partitioning B of inputs and outputsI, the corresponding block-diagonal NTK approximation KB to K results in a lower bound to the linearized Laplace marginal likelihood, i.e., log q˜w(D|h) ≥ log p(D, ˜w|h) − 1 2 log |KB + I||P0| + c ∝ log p(D, ˜w|h) − 1 2 log |P0| −1 2 PM m=1 log |KBm + I|, (11) where c = P 2 log 2π is left out in the second line indicated by the proportionality. Further, a refinement of partitionB to B′ will result in a lower bound with more slack. We give a proof in App. A. Theorem 2 shows that we can use the NTK on subsets of data and outputs (for multi-output cases like classification) and obtain a valid lower bound. Further, the form of Equation 11 allows unbiased stochastic estimation, gradients, and therefore SGD-based optimiza- tion of hyperparameters using (mini-)batches of data. That is, by uniformly sampling index sets Bm, we have PM m=1 log |KBm +I| = MEm∼U[M][log |KBm +I|]. (12) In Figure 4, we show the tightness of the bound for different subset sizes |Bm| on the two illustrative problems. Despite small subset sizes, the bounds are tighter than the crude diagonal approximation and already around 2% of the input- output pairs can suffice to select good hyperparameters. 4.2. Subset-of-Data Parametric Bounds Computing the NTK on subsets of data can be efficient but is non-trivial to parallelize, in which case parametric approxi- mations like KFAC might be preferable (Osawa et al., 2019). Using the NTK lower bound in Theorem 2, we can devise an equivalent parametric estimator, which gives a lower bound on subsets of data (and parameters). This enables the use of approximations like KFAC with Laplace on batches of data and thus for SGD-based hyperparameter optimization. 10−2 100 102 prior precision −2 −4 −6 log q˜w(D|h) 0 π/2 π rotational invariance −2 −4 −6 100 200 500 1000 2500 5000 10000 indices |Bm| Figure 5.Illustration of the parametric doubly lower bound using the block-diagonal GGN (Corollary 3.1) with varying subset sizes for prior and invariance hyperparameters. Setup as in Figure 4. Defining HBm def = HBm = JT BmΛBmJBm as a form of GGN on the subset of inputs and outputsBm, we have equivalence to the NTK bound, which we show in App. A: Theorem 3 (Parametric data subset bound). The NTK -based lower bound in Equation 11 to the linearized Laplace marginal likelihood is equivalent to a parametric variant, log p(D, ˜w|h)−1 2 log |P0|− 1 2 PM m=1 log |KBm+I| (13) =log p(D, ˜w|h)+ M-1 2 log |P0|-1 2 PM m=1 log |HBm+P0|, where HBm = JT BmΛBmJBm is estimated on a data subset. This Theorem is useful because it shows how to do stochas- tic estimation of the marginal likelihood on a subset of data using a parametric form, which resembles the GGN , instead of the NTK . However, the full GGN is quadratic in the num- ber of parameters and cannot be estimated in deep learning settings. To overcome this, we can further combine it with the parametric lower bound (Theorem 1) to justify struc- tured parametric approximations like the block-diagonal and its scalable approximation, KFAC, on subsets of data. Corollary 3.1 (Parametric doubly lower bound). For any partitioning B of input-output pairs I and P of parameter indices {1, .., P}, respectively, the following lower bound to the linearized Laplace marginal likelihood holds: log q˜w(D|h) ≥log p(D, ˜w|h) +M−1 2 log |P0| (14) − 1 2 PM m=1 log |HBm,P + P0| + P 2 log 2π, where HBm,P is a block-structured GGN approximation on subsets of input-output pairs. This doubly lower bound shows how parametric estimators, the most frequently used ones, can also be used for stochas- tic estimation and optimization of the log marginal likeli- hood. In Figure 5, we show the tightness of such bounds for the block-diagonal GGN . Surprisingly, even with just 5% of the input-output pairs, the bound is tighter then the diagonal approximation. Further, it only incurs a slight increase in slack compared to its NTK -based upper bound in Figure 4. In our experiments, we find that Corollary 3.1 successfully enables stochastic-gradient-based hyperparameter optimiza- tion with KFAC, which previously could only be used in the full-batch setting. 5Stochastic Marginal Likelihood Gradients using Neural Tangent Kernels 10−2 100 102 prior precision −2 −4 −6 log q˜w(D|h) 0 π/2 π rotational invariance −2 −4 −6 10 20 50 100 250 500 1000 indices |Bm| Figure 6.NTK Laplace bound using output-wise partitioning leads to tighter bounds and more efficiency. In comparison to Figure 4, the index set sizes are C = 10times smaller corresponding to an improved complexity by a factor of C3 at similar tightness. 4.3. Tighter Bounds with Better Partitions The tightness of the lower bounds derived from the NTK variant can be controlled with the partitioning B into input- output pairs. Bm can be understood as batches like in pa- rameter optimization using SGD. The bounds derived from the NTK motivate various choices of partitioning input data points and output dimensions so as to make the bounds as tight as possible. Mathematically, we want to find the parti- tion B that maximizes the lower bound in Equation 11. We discuss two simple partitioning cases B that can make the bounds derived from the NTK potentially tighter: partition- ing output dimensions and grouping inputs by labels. Partitioning the NTK by outputs means that each partition Bm contains only one particular output and corresponds to an independent kernel as, for example, used in Gaussian processes (Rasmussen & Williams, 2006). Each KBm is then an output-wise NTK , which can be much more efficient to compute than a full NTK . In the parametric space, this corresponds to a GGN for a single output and also greatly im- proves efficiency as it makes the GGN C times cheaper and is theoretically justified through a bound. Figure 6 shows that the output-wise NTK -based marginal likelihood bound is almost as tight as the full one at more thanC times smaller cost. Already 0.2% of the input-output pairs, or equivalently 2% of the data, suffice to learn invariances. Across the entire design space of estimators, it is this class-wise partitioning that provides many Pareto-optimal estimators in Figure 1. Alternatively, we can partition the index setI using the label information we have for each xn. In classification, we have access to labels yn for each xn. Assuming that the correla- tion of the NTK intra-class is greater than the anticorrelation inter-class, it is reasonable to partition I in such a way that data points xn with the same labels are in the same subset(s). In practice, this means that batches are made up from data points within the same class. We find that this approach can perform well but incurs a larger variance (cf. Figure 12 in App. C.1). In future work, it could be interesting to develop methods that track (anti-)correlation between inputs in the NTK to group them optimally during training. This is similar to inducing point optimization (Titsias, 2009). Algorithm 1 Stochastic Marginal Likelihood Estimate 1: Input: dataset D, likelihood, prior, random batch Bm ∈ B(partition of input-output indices; |B| = M), structure (e.g. NTK or GGN ), param partition P (opt.). 2: Let Dm denote data points of input indices in Bm 3: L ←|D| |Dm| log p(Dm|˜w, h) + logp( ˜w|h) − 1 2 log |P0| 4: if structure = NTK then 5: log q˜w(D|h) ← L −M 2 log |KBm +I| 6: else 7: log q˜w(D|h) ← L −M 2 log |HBm,PP−1 0 + I| 8: end if 9: Return log marginal likelihood estimate log ˜q ˜w(D|h). 4.4. Family of Estimators and Algorithm Through the NTK -based lower bound in Theorem 2, we derived estimators that enable stochastic marginal likeli- hood estimates and gradients. This allows us to optimize hyperparameters with SGD, just like neural network param- eters. In particular, Equation 12 shows that we can obtain a stochastic unbiased estimator of the lower bounds on the log-determinants derived from the NTK . The remaining terms are the log likelihood, which allows for an unbiased stochastic estimate naively, and simple terms like the log determinant of the prior and constants. In practice, we use our estimators for interleaved optimiza- tion of neural network weights and hyperparameters as in (Immer et al., 2021a). That is, we take gradient-steps on the hyperparameters every kth epoch after an initial burnin of b epochs. To obtain hyperparameter gradients, we choose a partitioning of the input-output indices I into B, which contains M such batches, and then sample from these uni- formly at random. Further, we resample the partition every epoch. In App. C.1, we show trajectories of this algorithm corresponding to the setting used for illustrating the bounds. To keep the bounds tight, we recommend output-wise parti- tioning since it eliminates complexity scaling in the number of outputs C while empirically maintaining a relatively tight lower bound. In practice, the most scalable bounds for hyperparameter gradients are the NTK -based bounds and doubly bounds using an efficient parametric approximation like KFAC, which even work well with relatively small sub- set sizes. Interestingly, the output-wise approximation can also be applied to KFAC-GGN giving a differentiable and scalable alternative for general hyperparameter learning. Alg. 1 shows how to compute stochastic marginal likelihood estimates. We use automatic differentiation to obtain gradi- ents w.r.t. h to train them with SGD. To make the bound as tight as possible and thus improve the approximations, we choose the partitioning and the size of its batches so as to fully utilize the available memory in practice. 6Stochastic Marginal Likelihood Gradients using Neural Tangent Kernels Dataset CIFAR-10 CIFAR-100 h Estimator log q˜w(D|h) log lik. acc. [%] time log q˜w(D|h) log lik. acc. [%] time prior precision BASELINE - -1.22 ± 0.04 84.6 ± 0.3 35% - -3.35 ± 0.14 56.7 ± 1.1 8% NTK -500-1 -0.87 ± 0.02 -0.53 ± 0.01 86.8 ± 0.1 60% -1.98 ± 0.08 -2.12 ± 0.04 61.7 + 0.6 16% KFAC-500-1 -1.41 ± 0.02 -0.38 ± 0.00 88.2 ± 0.1 65% -4.73 ± 0.11 -1.31 ± 0.04 63.9 ± 0.9 24% KFAC-500-10 -1.31 ± 0.01 -0.38 ± 0.00 88.6 ± 0.1 81% -3.95 ± 0.04 -1.26 ± 0.01 67.9 ± 0.2 70% KFAC-N-1 -1.10 ± 0.01 -0.38 ± 0.00 88.8 ± 0.1 52% -4.31 ± 0.09 -1.22 ± 0.02 66.8 ± 0.5 15% KFAC-N-C -0.79 ± 0.01 -0.39 ± 0.00 89.2 ± 0.1 100% -2.38 ± 0.03 -1.65 ± 0.07 64.6 ± 0.4 100% invariance NTK -150-1 -0.56 ± 0.05 -0.42 ± 0.01 90.3 ± 0.2 9% -1.19 ± 0.07 -1.45 ± 0.03 69.5 ± 0.8 4% KFAC-150-1 -0.59 ± 0.08 -0.29 ± 0.00 92.5 ± 0.2 10% -2.22 ± 0.37 -1.40 ± 0.05 71.8 ± 0.4 5% KFAC-N-1 -0.44 ± 0.07 -0.31 ± 0.01 93.0 ± 0.3 23% -3.43 ± 0.56 -1.20 ± 0.03 71.7 ± 0.7 10% KFAC-N-C -0.24 ± 0.02 -0.33 ± 0.01 93.2 ± 0.0 100% - - - ∼100% Table 1.Benchmark of stochastic marginal likelihood estimators on CIFAR classification tasks with a Wide ResNet (16-4) learning layerwise prior precisions (top) and affine invariances (bottom). “500-1” refers to a batch size of 500 and single-output approximation with N and C corresponding to the full-batch setting. The timing is relative to the full-batch estimator, KFAC-N-C, which was previously used for these settings. Our stochastic estimators can perform on par at up to 25-fold speed-up. Due to the number of classes, KFAC-N-C runs out of memory for CIFAR-100. Performance bold per category if standard errors with best overlap from both sides. 5. Experiments We experimentally validate the proposed estimators on vari- ous settings of marginal-likelihood-based hyperparameter optimization for deep learning. Overall, we find that the lower bounds using subsets of data or outputs often provide a better trade-off between performance and computational or memory complexity. In particular, they remain relatively tight even when applied only on small subsets of data and outputs. Therefore, they can greatly accelerate hyperpa- rameter optimization with Laplace approximations, making marginal-likelihood optimization possible at larger scale.5 In our experiments, we optimize prior precision parame- ters, P0, equivalent to weight-decay, per layer of neural networks (Immer et al., 2021a; Antor´an et al., 2022a) and learn invariances from data (van der Wilk et al., 2018; Immer et al., 2022b). Learning invariances requires differentiating the NTK or GGN and therefore acts as a realistic example of gradient-based optimization for general hyperparame- ters, which should be the long-term goal of Bayesian model selection for neural networks. In the following, we first discuss the illustrative example used throughout the theoretical development. Further, we compare our estimators to the full dataset KFAC-Laplace approximation, which previously provided the best results for hyperparameter optimization with Laplace approxima- tions (Immer et al., 2021a; Daxberger et al., 2021; Immer et al., 2022b). In this case, we find that estimators based on subsets of data are significantly more efficient and can perform on par even when the dataset is large. Lastly, we benchmark the two most scalable estimators on TinyIma- genet, a setting, where full dataset Laplace approximations failed due to computational costs (Mlodozeniec et al., 2023). 5Code: github.com/AlexImmer/ntk-marglik 5.1. Tightness of Bounds, Performance, and Runtime Throughout the theoretical development, we illustrate the tightness of the derived lower bounds to the linearized Laplace log marginal likelihood (cf. Figure 3, 4, 5, and 6). We use a small 3-layer convolutional network with linear output that has P ≈ 16 000parameters on a random subset of 1000 MNIST (LeCun & Cortes, 2010) digits, which al- lows analytical computation of both the full GGN and NTK . For the illustration using the rotational invariance, we addi- tionally rotate digits by a random angle θ ∼ U[−π, π]. We compare the prior precision and rotational invariance hyper- parameters with the log marginal likelihood estimate for the same trained neural network. In App. C.1, we additionally show the bound and corresponding test performance when optimizing it during training in both settings. Parametric estimators. Figure 3 indicates that block- diagonal and KFAC variant attain almost the same marginal likelihood as a full GGN and have the same test performance. It also shows that the diagonal approximation can fail catas- trophically due to its high slack in the bound (cf. Theorem 1) as it is the most refined partitioning P′ of parameter indices. Stochastic estimators. Figure 4 illustrates the proposed subset-of-data marginal likelihood estimators showing that already a small fraction of the input-output pairs leads to tight lower bounds. However, too small subset sizes can lead to failure when optimizing them, similar to the di- agonal approximation. The doubly lower bound, which allows using structured parametric estimators on subsets of data (cf. Corollary 3.1), is displayed in Figure 5 and is similarly tight. Further, handling the C = 10 outputs in- dependently for NTK -based estimators, which corresponds to a partitioning by class, almost performs as well as a full NTK in Figure 6 and is often significantly cheaper. 7Stochastic Marginal Likelihood Gradients using Neural Tangent Kernels 1k 5k 10k 20k full subset size −2.0 −1.5 −1.0 −0.5 test log likelihood original CIFAR-10 KFAC-full KFAC-400-1 (ours) NTK -400-1 (ours) 1k 5k 10k 20k full subset size −2.0 −1.5 −1.0 −0.5 fully-rotated CIFAR-10 1k 5k 10k 20k full subset size −2.0 −1.5 −1.0 −0.5 partially-rotated CIFAR-10 1k 5k 10k 20k full subset size −2.0 −1.5 −1.0 −0.5 translated CIFAR-10 Figure 7.Proposed stochastic marginal likelihood estimators usingNTK and KFAC with a fixed batch size of400 and class-wise partitioning perform well on subsets and modifications of CIFAR-10 with a ResNet. Our estimators can take more stochastic gradient steps than the full KFAC estimator at a faster runtime and thus greatly improve over it in terms of test log likelihood. This suggests that using many stochastic gradients can be more effective for hyperparameter learning than taking few exact gradients. More results in App. C.4. Pareto-efficiency. Figure 1 shows the obtained marginal likelihood estimates versus the runtime of a single estimate (and its gradient w.r.t. hyperparameters) indicating a Pareto- frontier between the two. Previously, only the full-batch estimators (black markers) were known, of which onlyKFAC is both tractable and performant. Our derived estimators greatly increase the design space and provide many Pareto- optimal estimators, especially for lower runtime budgets. Partitioning by class label. In Sec. 4.3, we hypothesized that grouping input-output sets by label information could improve bounds due to higher intra-class correlation than inter-class anti-correlation. Across all runs, this did not make a significant difference for learning invariances. How- ever, for optimizing solely the prior precision (cf. App. C.1) it improves the bound and test log likelihood by 10% on average. This suggests that improved partitioning can help. 5.2. Benchmark of Proposed Estimators We compare the proposed marginal likelihood estimators to the KFAC-Laplace, the state-of-the-art among Laplace ap- proximations for hyperparameter optimization (Immer et al., 2021a; Daxberger et al., 2021; Antor´an et al., 2022a). In the first setting, we optimize the layer-wise prior precision on a Wide ResNet 16-4 (Zagoruyko & Komodakis, 2016) on CIFAR-10 (Krizhevsky, 2009) and CIFAR-100 (Krizhevsky et al.). In the second setting, we additionally optimize learn- able invariances, similar to data augmentation, where the Laplace-based method by Immer et al. (2022b) is extremely costly since it requires more than one pass through the dataset per gradient. The results in Table 1 suggest that stochastic marginal likelihood estimators are particularly useful for learning invariances, where they accelerate the runtime up to 25-fold at similar performance. In addition to our stochastic estimators based on subsets of data, i.e., NTK - 150-1 and KFAC-150-1 with a batch size of 150 on single outputs, we show a full-dataset stochastic KFAC estimator that obtains a bound by sampling single random outputs (cf. remark in App. A). 5.3. Behavior with Varying Dataset Size Since larger datasets theoretically lead to looser bounds when using constant subsets of data, we investigate how this impacts performance in Figure 7. In particular, we follow Immer et al. (2022b) and learn invariances present in transformed versions of CIFAR-10 (Krizhevsky, 2009) on random subsets ranging from 1 000to all 50 000data points. We follow prior work by considering distribution over affine invariances (Benton et al., 2020; van der Ouderaa & van der Wilk, 2021) detailed in App. C.4. We evaluate the test log likelihood of KFAC and NTK -based estimators on subsets of 400 data points and single independent outputs, KFAC-400-1 and NTK -400-1. We compare the approach with the method by Immer et al. (2022b), KFAC-full (equal to KFAC-N-C). The cheaper marginal likelihood estimates and gradients allow us to do more hyperparameter updates per epoch given equivalent computational constraints. Compared to KFAC-full, we can therefore increase the number of hyper- parameter steps for KFAC-400-1 and NTK -400-1 from 10 to 100, while still reducing overall training time. Figure 7 shows that our estimators perform on par or better than the full KFAC variant in terms of test log likelihood, especially, but not only, for small dataset sizes. We hypothesize for invariance learning that the ability to do more gradient up- dates is more beneficial than having tighter lower bounds. Further details on invariance learning, experimental details, and results can be found in App. C.4. 5.4. Scaling to Larger Datasets and Models For invariance learning, the results on CIFAR-100 in Ta- ble 1 already indicated that KFAC-N-C is not scalable enough to enable gradient-based hyperparameter optimiza- tion. Mlodozeniec et al. (2023) also found that it is not pos- sible to run it on the even larger TinyImagenet dataset (Le & Yang, 2015), which has N = 100 000data points and C = 200classes, using a ResNet-50 with roughly 23 million parameters. Using the two fastest estimators for invariance learning, we show in Table 2 that our subset-of-data estima- tors enable optimizing hyperparameters in this setting. 8Stochastic Marginal Likelihood Gradients using Neural Tangent Kernels h Estimator log likelihood accuracy [%] prior BASELINE -4.42 ± 0.08 45.6 ± 0.3 NTK -400-1 -4.16 ± 0.25 49.8 ± 0.2 KFAC-400-1 -2.17 ± 0.02 53.0 ± 0.1 invariance AUGERINO - 41.1 ± 0.2 PARTITIONED - 48.6 ± 0.0 NTK -70-1 -3.15 ± 0.08 56.3 ± 0.1 KFAC-60-1 -2.02 ± 0.10 58.4 ± 0.3 Table 2.Hyperparameter learning on TinyImagenet with ResNet- 50 in comparison to Augerino (Benton et al., 2020) and neural network partitioning taken from Mlodozeniec et al. (2023). While previous Laplace approximations are intractable in this setting, KFAC with a subset size of 60 data points on single outputs excels. We compare to the results provided by Mlodozeniec et al. (2023) and use the same architecture but with Fixup (Zhang et al., 2019b) instead of normalization layers, which would be incompatible with weight decay (Antor´an et al., 2022b). Optimizing layer-wise prior precisions can greatly improve over the baseline with default settings and no data aug- mentation. Further, our estimators improve over invariance learning using neural network partitioning (Mlodozeniec et al., 2023) and Augerino (Benton et al., 2020). 5.5. Practical Considerations In our experiments, the proposed estimators using subset-of- data lower bounds using a single output often perform best, in particular using the parametric KFAC variant. Although, the NTK -based variant often yields a tighter bound, KFAC- based bounds seem particularly well-suited for marginal- likelihood optimization and are therefore preferable in prac- tice. As illustrated through invariance learning, our stochas- tic estimators are efficient for general differentiable hyperpa- rameter optimization, which is an exciting future direction. 6. Related Work Marginal-likelihood optimization, also referred to as em- pirical Bayes, is the de-facto standard for hyperparame- ter optimization of Gaussian process models (Rasmussen & Williams, 2006) without validation data, and was used in the early days of Bayesian neural networks (MacKay, 1992b; Foresee & Hagan, 1997). Use in modern larger neural networks dwindled, with Blundell et al. (2015) re- porting failure using mean-field variational approximations, although the approach continued to work in Deep Gaussian Processes (Damianou & Lawrence, 2013; Dutordoir et al., 2020), and newer work demonstrates the feasibility in mod- ern neural networks (Ober & Aitchison, 2021; Immer et al., 2021a). Antor ´an et al. (2022c) recently proposed an in- teresting alternative based on sampling, which works for prior precisions but not general hyperparameters. In the context of deep learning, the marginal likelihood has so far been used to select regularization strength, invariances, architectures, and representations. Our derived estimators further extend the family of Laplace approximations and provide alternatives to their parametric approximations. The partitioning of the kernel in our estimators is similar to the Bayesian committee machine (Tresp, 2000; Deisenroth & Ng, 2015), which distributes Gaussian process inference into smaller kernels but does not give a valid lower bound on the marginal likelihood. For a detailed discussion on the benefits, but also issues, of the marginal likelihood, we refer to Gelman et al. (1995, Sec. 7). NTK and GGN Jacot et al. (2018) introduced the neural tangent kernel to characterize training dynamics of neural networks under squared loss. The training dynamics are even available in a closed-form when considering infinite width. However, computing kernels in this case is not triv- ial for common architectures (Novak et al., 2019). Our estimators use the NTK of neural networks at finite width, sometimes referred to as empirical NTK (Novak et al., 2022). Interestingly, the NTK and Gauss-Newton approximation to the Hessian are dual to each other as shown by (Khan et al., 2019), and Immer et al. (2021b) for general likelihoods. Our work builds on this duality and derives novel estimators from the NTK viewpoint that allow stochastic estimation. The NTK is also used to estimate generalization without the marginal likelihood, e.g., in the context of neural architec- ture search (Park et al., 2020; Chen et al., 2021). Further, the linearized Laplace with NTK been used to improve posterior predictives (Deng et al., 2022; Kim et al., 2023). 7. Conclusion In this paper, we have derived stochastic estimators for the linearized Laplace approximation to the marginal likelihood that are suitable objectives for stochastic-gradient based op- timization of hyperparameters. Our estimators are derived from a functional view of the Laplace using the neural tan- gent kernel and allow to trade off estimation accuracy and speed. Our experiments show that the (mini-)batch estima- tors perform on par with previous full-batch estimators but are many times faster. This suggests that they could be use- ful for learning more complex hyperparameters with SGD. Future research could further find ways to make bounds tighter, for example, by improving partitioning of the NTK . Further, it could be interesting to apply the fast estimators to large-scale Bayesian linear models. Acknowledgements A.I. gratefully acknowledges funding by the Max Planck ETH Center for Learning Systems (CLS). The authors thank the reviewers for their constructive feedback and comments, in particular R2, who suggested Figures 4, 5, and 6. 9Stochastic Marginal Likelihood Gradients using Neural Tangent Kernels References Antor´an, J., Barbano, R., Leuschner, J., Hern ´andez- Lobato, J. M., and Jin, B. A probabilistic deep im- age prior for computational tomography. arXiv preprint arXiv:2203.00479, 2022a. Antor´an, J., Janz, D., Allingham, J. U., Daxberger, E., Bar- bano, R. R., Nalisnick, E., and Hern´andez-Lobato, J. M. Adapting the linearised laplace model evidence for mod- ern deep learning. In International Conference on Ma- chine Learning, pp. 796–821. PMLR, 2022b. Antor´an, J., Padhy, S., Barbano, R., Nalisnick, E., Janz, D., and Hern´andez-Lobato, J. M. Sampling-based inference for large linear models, with application to linearised laplace. arXiv preprint arXiv:2210.04994, 2022c. Benton, G., Finzi, M., Izmailov, P., and Wilson, A. G. Learning invariances in neural networks. arXiv preprint arXiv:2010.11882, 2020. Bishop, C. M. Pattern recognition and machine learning. Information Science and Statistics. Springer, 2006. Blei, D. M., Kucukelbir, A., and McAuliffe, J. D. Varia- tional inference: A review for statisticians. Journal of the American statistical Association, 112(518):859–877, 2017. Blundell, C., Cornebise, J., Kavukcuoglu, K., and Wierstra, D. Weight uncertainty in neural networks. In Proceed- ings of the 32nd International Conference on Machine Learning, pp. 1613–1622, 2015. Botev, A. and Martens, J. KFAC-JAX, 2022. URL http: //github.com/deepmind/kfac-jax. Botev, A., Ritter, H., and Barber, D. Practical Gauss-Newton optimisation for deep learning. In International Con- ference on Machine Learning, International Convention Centre, Sydney, Australia, 2017. PMLR. Chen, W., Gong, X., and Wang, Z. Neural architecture search on imagenet in four gpu hours: A theoretically inspired perspective. arXiv preprint arXiv:2102.11535, 2021. Cohen, T. and Welling, M. Group equivariant convolutional networks. In International conference on machine learn- ing, pp. 2990–2999. PMLR, 2016. Damianou, A. and Lawrence, N. D. Deep Gaussian pro- cesses. In International Conference on Artificial Intelli- gence and Statistics, volume 31, pp. 207–215, Scottsdale, Arizona, USA, 29 Apr–01 May 2013. PMLR. Dangel, F., Kunstner, F., and Hennig, P. Backpack: Packing more into backprop. In Proceedings of 7th International Conference on Learning Representations, 2019. Daxberger, E., Kristiadi, A., Immer, A., Eschenhagen, R., Bauer, M., and Hennig, P. Laplace redux-effortless bayesian deep learning. Advances in Neural Informa- tion Processing Systems, 34, 2021. Deisenroth, M. and Ng, J. W. Distributed gaussian processes. In International Conference on Machine Learning , pp. 1481–1490. PMLR, 2015. Deng, Z., Zhou, F., and Zhu, J. Accelerated linearized laplace approximation for bayesian deep learning. In Ad- vances in Neural Information Processing Systems, 2022. Dutordoir, V ., van der Wilk, M., Artemev, A., and Hensman, J. Bayesian image classification with deep convolutional gaussian processes. In Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics (AISTATS), Aug 2020. Fischer, E. ¨Uber den hadamardschen determinantensatz. Arch. Math. U. Phys. (3), 13:32–40, 1908. Foresee, F. D. and Hagan, M. T. Gauss-newton approxima- tion to bayesian learning. In International Conference on Neural Networks (ICNN’97), volume 3, pp. 1930–1935. IEEE, 1997. Gelman, A., Carlin, J. B., Stern, H. S., and Rubin, D. B. Bayesian data analysis. Chapman and Hall/CRC, 1995. Hataya, R. and Nakayama, H. Gradient-based hyperparame- ter optimization without validation data for learning fom limited labels. 2021. Horn, R. A. and Johnson, C. R. Matrix analysis. Cambridge university press, 2012. Hutter, F., Kotthoff, L., and Vanschoren, J. (eds.). Auto- mated Machine Learning - Methods, Systems, Challenges. Springer, 2019. Immer, A., Bauer, M., Fortuin, V ., R¨atsch, G., and Emtiyaz, K. M. Scalable marginal likelihood estimation for model selection in deep learning. In International Conference on Machine Learning, pp. 4563–4573. PMLR, 2021a. Immer, A., Korzepa, M., and Bauer, M. Improving pre- dictions of bayesian neural nets via local linearization. In Proceedings of The 24th International Conference on Artificial Intelligence and Statistics, pp. 703–711, 2021b. Immer, A., Torroba Hennigen, L., Fortuin, V ., and Cotterell, R. Probing as quantifying inductive bias. In Proceed- ings of the 60th Annual Meeting of the Association for Computational Linguistics, pp. 1839–1851, 2022a. Immer, A., van der Ouderaa, T. F., Ratsch, G., Fortuin, V ., and van der Wilk, M. Invariance learning in deep neu- ral networks with differentiable laplace approximations. 10Stochastic Marginal Likelihood Gradients using Neural Tangent Kernels In Advances in Neural Information Processing Systems, 2022b. Jacot, A., Gabriel, F., and Hongler, C. Neural tangent kernel: Convergence and generalization in neural networks. In Advances in neural information processing systems, pp. 8571–8580, 2018. Khan, M. E. E., Immer, A., Abedi, E., and Korzepa, M. Approximate inference turns deep networks into gaussian processes. In Advances in Neural Information Processing Systems, pp. 3088–3098, 2019. Kim, S., Park, S., Kim, K., and Yang, E. Scale-invariant bayesian neural networks with connectivity tangent ker- nel. 2023. Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. In International Conference on Learning Representations, 2015. Kingma, D. P. and Welling, M. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. Krizhevsky, A. Learning multiple layers of features from tiny images. Technical report, 2009. Krizhevsky, A., Nair, V ., and Hinton, G. Cifar-100 (canadian institute for advanced research). URL http://www. cs.toronto.edu/˜kriz/cifar.html. Le, Y . and Yang, X. Tiny imagenet visual recognition chal- lenge. CS 231N, 7(7):3, 2015. LeCun, Y . and Cortes, C. MNIST handwritten digit database. http://yann.lecun.com/exdb/mnist/, 2010. URL http: //yann.lecun.com/exdb/mnist/. Lorraine, J., Vicol, P., and Duvenaud, D. Optimizing mil- lions of hyperparameters by implicit differentiation. In International Conference on Artificial Intelligence and Statistics, pp. 1540–1552. PMLR, 2020. MacKay, D. J. Bayesian model comparison and backprop nets. In Advances in neural information processing sys- tems, pp. 839–846, 1992a. MacKay, D. J. The evidence framework applied to classi- fication networks. Neural computation, 4(5):720–736, 1992b. MacKay, D. J. Probable networks and plausible predic- tions—a review of practical bayesian methods for super- vised neural networks. Network: computation in neural systems, 6(3):469–505, 1995. MacKay, D. J. Information theory, inference and learning algorithms. Cambridge university press, 2003. Martens, J. New insights and perspectives on the natural gradient method. Journal of Machine Learning Research, 21(146):1–76, 2020. Martens, J. and Grosse, R. Optimizing neural networks with kronecker-factored approximate curvature. In Interna- tional conference on machine learning, pp. 2408–2417, 2015. Mlodozeniec, B., Reisser, M., and Louizos, C. Hyperpa- rameter optimization through neural network partitioning. arXiv preprint arXiv:2304.14766, 2023. Murphy, K. P.Machine learning: a probabilistic perspective. MIT press, 2012. Novak, R., Xiao, L., Hron, J., Lee, J., Alemi, A. A., Sohl- Dickstein, J., and Schoenholz, S. S. Neural tangents: Fast and easy infinite neural networks in python. arXiv preprint arXiv:1912.02803, 2019. Novak, R., Sohl-Dickstein, J., and Schoenholz, S. S. Fast finite width neural tangent kernel. In International Con- ference on Machine Learning, pp. 17018–17044. PMLR, 2022. Ober, S. W. and Aitchison, L. Global inducing point varia- tional posteriors for bayesian neural networks and deep gaussian processes. In International Conference on Ma- chine Learning, pp. 8248–8259. PMLR, 2021. Ober, S. W., Rasmussen, C. E., and van der Wilk, M. The promises and pitfalls of deep kernel learning. In Uncer- tainty in Artificial Intelligence (UAI) , volume 161, pp. 1206–1216. PMLR, 2021. Osawa, K. Automatic Second-Order Differentiation Li- brary (ASDL), 2021. URL http://github.com/ kazukiosawa/asdl. Osawa, K., Tsuji, Y ., Ueno, Y ., Naruse, A., Yokota, R., and Matsuoka, S. Large-scale distributed second-order opti- mization using kronecker-factored approximate curvature for deep convolutional neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12359–12367, 2019. Park, D. S., Lee, J., Peng, D., Cao, Y ., and Sohl-Dickstein, J. Towards nngp-guided neural architecture search. arXiv preprint arXiv:2011.06006, 2020. Rasmussen, C. E. and Ghahramani, Z. Occam’s razor. In Advances in neural information processing systems, pp. 294–300, 2001. Rasmussen, C. E. and Williams, C. K. Gaussian processes for machine learning. MIT press Cambridge, MA, 2006. 11Stochastic Marginal Likelihood Gradients using Neural Tangent Kernels Ritter, H., Botev, A., and Barber, D. A scalable laplace approximation for neural networks. In International Con- ference on Learning Representations, 2018. Schw¨obel, P., Jørgensen, M., Ober, S. W., and van der Wilk, M. Last layer marginal likelihood for invariance learn- ing. In Proceedings of the Twenty Fifth International Conference on Artificial Intelligence and Statistics, 2022. Titsias, M. Variational learning of inducing variables in sparse gaussian processes. In International Conference on Artificial Intelligence and Statistics , volume 5, pp. 567–574. PMLR, 16–18 Apr 2009. Tresp, V . A bayesian committee machine.Neural computa- tion, 12(11):2719–2741, 2000. van der Ouderaa, T. F. and van der Wilk, M. Learning invariant weights in neural networks. In Workshop in Un- certainty & Robustness in Deep Learning, ICML, 2021. van der Wilk, M., Bauer, M., John, S., and Hensman, J. Learning invariances using the marginal likelihood. In Advances in Neural Information Processing Systems, pp. 9938–9948, 2018. Wang, R., Walters, R., and Yu, R. Approximately equivari- ant networks for imperfectly symmetric dynamics. In In- ternational Conference on Machine Learning, pp. 23078– 23091. PMLR, 2022. Zagoruyko, S. and Komodakis, N. Wide residual networks. In BMVC. BMV A Press, 2016. Zhang, G., Wang, C., Xu, B., and Grosse, R. B. Three mechanisms of weight decay regularization. In ICLR (Poster). OpenReview.net, 2019a. Zhang, H., Dauphin, Y . N., and Ma, T. Fixup initialization: Residual learning without normalization. In International Conference on Learning Representations, 2019b. 12Stochastic Marginal Likelihood Gradients using Neural Tangent Kernels A. Theoretical Results and Proofs The following Theorem and corollary are required to bound the parametric and NTK -based marginal likelihood approxi- mations as they allow to bound the determinant, or the log-determinant, respectively by ignoring off-diagonal blocks of a matrix. Theorem 4 (Fischer’s inequality (1908)). For a positive definite matrix M, as defined hereafter, it holds that det M = det \u0014A B BT C \u0015 ≤ det \u0014A 0 0 C \u0015 = det(A ⊕ C) = detA det C. (15) This Theorem is from Fischer (1908). We refer to Horn & Johnson (2012) for a proof. We immediately have the following useful corollary negative log-determinants, which show up in the Laplace approximation to the log marginal likelihood. The result simply follows from the fact that the log is monotonically increasing. Corollary 4.1. For a positive definite matrix M as defined in Theorem 4, the following inequality holds −log detM = −log det \u0014A B B C \u0015 ≥ −log detA − log detC. (16) For our bounds, a more general partitioned form of M is required, which corresponds to repeated bounding with Theorem 4 through partitions. In particular, we deal with a matrix M ∈ RM×M and use indices 1 through M, i.e., [M] ={1, . . . , M} that, for example, allow to denote the mth diagonal element as Mm,m. Further, we introduced a notation based on index sets in Sec. 2 that allows to denote blocks or off-diagonal elements of M. In particular, let A ⊂[M] and A′ ⊂ [M] disjoint subsets, A ∩ A′ = ∅ of the indices denoting dimensions in M, then we write MA,A = MA ∈ R|A|×|A| for the square block matrix with entries Mi,j such that i ∈ Aand j ∈ A. In line with this, off-diagonal blocks are MA,A′ ∈ R|A|×|A′|. Partitioning the dimension indices [M] of M into two disjoint subsets A and A′, we have three blocks, MA, MA′ , and MA,A′ . Since we can re-order the matrix to conform to such blocks, because it relies on simultaneous permutation of rows and columns, we can apply Theorem 4 to M and have det M = det \u0014 MA MA,A′ MA′,A MA′ \u0015 ≤ det \u0014MA 0 0 M A′ \u0015 = det(MA ⊕ MA′ ) = detMA + detMA′ . (17) Note that the first equality here only holds for the determinant and not for its matrix argument since the partitioning by indices permutes the rows and columns simultaneously, i.e., an even amount of times. With this, we have the following Lemma Lemma 1. For any partitioning P = {Ps}S s=1 of dimension indices [M] of the positive definite matrix M, we have det M = det   MP1 MP1,P2 ··· MP1,PS MP2,P1 MP2,P2 ··· ... ... ... ... ... MPS,P1 ··· ··· MPS   ≤ det(MP1 ⊕ ··· ⊕MPS ) = SY s=1 det MPs, (18) and further refining the partitioning P into P′, such that each element in P′ is a subset of P′ is a futher upper bound. Proof. To obtain this bound, we iteratively apply Theorem 4. In particular, let Rs be the complement of the first s subsets, i.e., Rs = {P1, . . . ,Ps}∁, for example, R1 = P∁ 1 and RS = ∅.6 The complement (·)∁ here is with respect to the full set of indices [M]. Using Theorem 4 S times on these complementing sets, we have det M = det \u0014 MP1 MP1,R1 MR1,P1 MR1 \u0015 ≤ det MP1 det MR1 = detMP1 det \u0014 MP2 MP2,R2 MR2,P2 MR2 \u0015 ≤ det MP1 det MP2 det MR2 = ··· ≤ SY s=1 det MPs. This proves the main statement of the Lemma. By refining the partition, we simply extend the upper bounds since a refinement splits up each element in P into at least one subset, which again enables application of Theorem 4. 6The order 1, . . . , Sis arbitrary. 13Stochastic Marginal Likelihood Gradients using Neural Tangent Kernels Theorem 1 (Parametric structured lower bound). For any partitioning P of the parameter indices {1, .., P}, the block- diagonal approximation of HP to H results in a lower bound to the LA log marginal likelihood (Equation 5), i.e., log q˜w(D|h) ≥ log p(D, ˜w|h) − 1 2 log |HP + P0| + c, (9) where c = P 2 log 2π. Further, a refinement7 of the partition P to P′ will result in a lower bound with even more slack. Proof. Since the log-likelihood and log-prior terms are identical, we only need to inspect the relationship between−log |H+ P0| and −log |HP + P0|. Due to the assumption that P0 is diagonal, it can be added to the individual blocks ofHP since a diagonal is the most refined partition possible on the indices {1, . . . , P}. 8 The lower bound on the negative log determinant and the increased slack when refining then follows from Lemma 1, which upper bounds the product of determinants, and thus lower bounds the negative log determinant (Corollary 4.1). Theorem 2 (Data subset lower bound). For any partitioning B of inputs and outputs I, the corresponding block-diagonal NTK approximation KB to K results in a lower bound to the linearized Laplace marginal likelihood, i.e., log q˜w(D|h) ≥ log p(D, ˜w|h) − 1 2 log |KB + I||P0| + c ∝ log p(D, ˜w|h) − 1 2 log |P0| −1 2 PM m=1 log |KBm + I|, (11) where c = P 2 log 2π is left out in the second line indicated by the proportionality. Further, a refinement of partitionB to B′ will result in a lower bound with more slack. Proof. As shown by Immer et al. (2021a) and in Sec. 2, the linearized Laplace approximation can be written in NTK -form using the matrix determinant Lemma giving log q˜w(D|h) = logp(D, ˜w|h) − 1 2 log |J˜wP−1 0 JT ˜wΛ˜w + I||P0| + P 2 log 2π = logq(D, ˜w|h) − 1 2 log |P0| + P 2 log 2π − 1 2 log |K + I| ≥ log q(D, ˜w|h) − 1 2 log |P0| + P 2 log 2π − 1 2 log |KB + I| = logq(D, ˜w|h) − 1 2 log |P0| + P 2 log 2π − 1 2 PM m=1 log |KBm + I|, where we first re-order and use our definition of the scaled NTK (cf. Sec. 2). Then, we use the lower bound according to Lemma 1 further giving us the statement that refinement leads to more slack. We again note that adding a diagonal, in this case I to K, can simply be absorbed into the block-matrices and extracted afterwards when applying it. In particular, define ˆK = K + I, apply the bound, and we obtain ˆKBm = KBm + I for all m and thus ˆKB = KB + I. Theorem 3 (Parametric data subset bound). The NTK -based lower bound in Equation 11 to the linearized Laplace marginal likelihood is equivalent to a parametric variant, log p(D, ˜w|h)−1 2 log |P0|− 1 2 PM m=1 log |KBm+I| (13) =log p(D, ˜w|h)+ M-1 2 log |P0|-1 2 PM m=1 log |HBm+P0|, where HBm = JT BmΛBmJBm is estimated on a data subset. Proof. The idea is to apply the matrix determinant Lemma to move from the subset of data NTK bound back to a parametric variant. This gives us a log determinant that depends on the GGN defined on subsets of input-output pairs. We subtract the log joint, log p(D, ˜w|h), and add 1 2 log |P0| from to both sides of Equation 13 and multiply by 2 to abbreviate the following equations. We then have −PM m=1 log |KBm + I| = −PM m=1 log |JBmP−1 0 JT BmΛBm + I| = M log |P0| −PM m=1 log |JBmP−1 0 JT BmΛBm + I||P0| = M log |P0| −PM m=1 log |JT BmΛBmJBm + P0| = M log |P0| −PM m=1 log |HBm + P0|, 7A refinement P′ of P is a partition such that for all P′ s ∈ P′ there exists Ps ∈ Psuch that P′ s ⊆ Ps. 8In case, P0 would not be diagonal and its structure would not correspond to a refinement of P, one would have to treat it jointly with H and apply the bound to ˆH def = H + P0 and then ˆHP. 14Stochastic Marginal Likelihood Gradients using Neural Tangent Kernels where JBm corresponds to the Jacobians only for the input-output pairs in Bm and ΛBm to the log-likelihood Hessian for these outputs (cf. Sec. 2.3). subtracting and adding the removed quantities again concludes the proof. Remark. The form of Equation 13, in particular HBm is reminiscent of a GGN approximation but just on a subset of input-output pairs. In particular, it depends on the subset structure chosen for the NTK bound, and could be a class-wise GGN or simply on a random subset of data. Subsets of data do not change the shape of the parametric estimator, i.e., it still requires the log determinant of a P × P matrix but computing that matrix can be greatly sped up: apart from the requirement to compute Jacobians for all N data points, a major issue with the GGN approximation is that its cost scales linearly with the outputs C as well (Botev et al., 2017). Therefore, it is often approximated using sampling via equality to the Fisher information (Martens & Grosse, 2015) for exponential family likelihoods of natural form (Martens, 2020). By partitioning the input-output pairs [NC ] into C partitions, we can compute the GGN for a single output and use it as a proper lower-bound to the LA marginal likelihood at the same cost as the Fisher, which does not lead a lower bound of the LA due to the required sampling approximation. Further, we can apply our parametric bound in Theorem 1 to use mixed bounds on subsets of data and subsets of parameter groups enabling, for example, a lower bound to the LA marginal likelihood using KFAC on a subset of data. Corollary 3.1 (Parametric doubly lower bound). For any partitioning B of input-output pairs I and P of parameter indices {1, .., P}, respectively, the following lower bound to the linearized Laplace marginal likelihood holds: log q˜w(D|h) ≥log p(D, ˜w|h) +M−1 2 log |P0| (14) − 1 2 PM m=1 log |HBm,P + P0| + P 2 log 2π, where HBm,P is a block-structured GGN approximation on subsets of input-output pairs. Proof. This corollary simply follows from applying the parametric lower bound, Theorem 1, to the parametric estimator on a subset of data in Theorem 3. B. Computational Considerations and Complexity The computational complexity of the proposed estimators as well as the GGN , NTK , and their corresponding approximations like KFAC greatly depend on the model architecture. For simplicity, we assume a neural network with P parameters in L linear hidden layers with each width of D, inputs x ∈ RD of the same dimensionality, with output dimensionality C, i.e., we have P = DC + P l∈[L] D2, where DC are the parameters of the linear output layer. Further, we have N data points. The complexity of automatically differentiating the log determinant of the GGN or NTK is equivalent to computing the log determinant of these matrices defined in Equation 7. Computing the log determinant of the GGN first requires computing the GGN itself, which is O(NP 2C) for computing the sum of N Jacobian outer products, where each Jacobian is C × P. The log determinant is then additionally O(P3). In comparison, for naive computation of the NTK log determinant, we compute the NC × NC kernel in O(N2C2P) and its determinant in O(N3C3). The complexity for KFAC-GGN is O(NLD2C) for summing N outer products for L layers and its determinant O(D3) for each layer giving O(LD3). We have the following computational and complexities for computing and differentiating the log determinant using automatic differentiation: GGN ∈ O(NP 2C + P3) NTK ∈ O(N2C2P + N3C3) KFAC-GGN ∈ O(NLD2C + LD3). (19) While these are for naive estimators that can be improved for certain architectures, e.g., see Novak et al. (2022) for faster NTK computations, these complexities describe the worst-case setting. Our proposed lower bounds based on subsets of inputs and outputs can greatly improve these complexities. In particular, we can replace N by a subset size M ≪ N and use output-wise bounds to obtain C = 1. In our experiments, the fastest and still performant methods are NTK -M-1 and KFAC-M-1, which have a computational and memory complexity for estimation and automatic differentiation of O(M2P + M3) and O(MLD2 + LD3), respectively. Depending on the architecture, the NTK estimator can further be faster than O(M2P). Overall, the proposed methods attain an acceleration at least linear in the number of outputs C and data points N. 15Stochastic Marginal Likelihood Gradients using Neural Tangent Kernels C. Additional Experimental Results and Details We provide additional experimental details and results complementing those in the main text here. For our implemen- tation, we modify and extend the asdl library (Osawa, 2021) that offers fast computation of KFAC and NTK , as well as laplace-torch (Daxberger et al., 2021) for the marginal likelihood approximations. The code is available at https://github.com/AlexImmer/ntk-marglik. C.1. Tightness of Bounds, Performance, and Runtime The experiments illustrating the bounds in the main text described in Sec. 5.1 are conducted on randomly chosen 1000 MNIST subsets for three random seeds and further images are fully rotated at random, i.e., up to ±π. We then use the LILA method proposed by Immer et al. (2022b) to try to learn these underlying invariances, which is essential to generalize well on this task since standard convolutional and fully connected layers are not rotationally invariant. We use a simple convolutional network with three layers, max pooling, and a linear classification head that totals roughly 16 000parameters so that we can compute and differentiate the full GGN and NTK . We use 30 augmentation samples to average the outputs and optimize the network parameters, invariance parameters, and prior parameters with Adam (Kingma & Ba, 2015). For network parameters we use a learning rate of 10−3 and decay it to 10−9 using cosine decay and use a batch size of 250. The invariance and prior learning hyperparameters follow the settings of Immer et al. (2022b): 10 epochs burnin and then update hyperparameters every epoch with learning rates 0.1 and 0.05 for prior precision and invariance parameters, respectively. Both are decayed by a factor of 10 using cosine decay. We use an optimized network to assess the bounds in Figures 4 to 6. After convergence, we apply the different bounds and assess them over the entire grid of rotational invarianceη ∈ [0, π]. Here, we additionally present results for hyperparameter optimization using the same bounds in Figure 8 and Figure 9. The experiments are run on an internal compute cluster with different NVIDIA GPUs. The timing experiments are run on a single A100 sequentially to ensure comparability. In addition to the invariance learning experiments, we compare the bounds for varying values of a scalar prior precision in all illustrative figures in the main text. Here, we show results optimizing the layerwise prior precision in Figure 10, 11, and 13 again on 1000 randomly sampled MNIST digits per seed but without rotating them. When only learning prior precision parameters and not invariances, a slack in the bound translates more directly into a slack in the test performance as can be seen in the figures. However, it is worth noting that the NTK and KFAC estimators are significantly cheaper on large batches of data for just optimizing the prior precision, in comparison to learning invariances. In Figure 13, we see that stochastic NTK -based estimators are Pareto-optimal in many cases as it was the case for invariance learning. We also find that small batch sizes do not lead to problems at larger scale (cf. Sec. 5.2) where they perform on par with full dataset marginal likelihood estimators. In Figure 12, we show the effect of class-wise partitioning of batches on the bound in comparison to the random partition. We find that the bound can become tighter for both prior precision and invariance learning but incurs a slightly higher variance as indicated by the standard error displayed. 200 400 steps −4 −3 −2 −1 log q(D|h) 200 400 steps −2.0 −1.5 −1.0 −0.5 test log likelihood full blockdiag kron diag 200 400 steps −4 −3 −2 −1 log q(D|h) 200 400 steps −2.0 −1.5 −1.0 −0.5 test log likelihood 100 200 500 1000 2500 5000 10000 indices |Bm| Figure 8.Parametric and NTK lower bound when optimizing invariances on random subsets of size 1000 from rotated MNIST (LeCun & Cortes, 2010). Only small subset sizes and the diagonal approximation fail to learn the invariance and result in good test log likelihood. This figure corresponds to Figure 3 and Figure 4 in the main text, which show the bounds constructed at step500 with parameters and hyperparameters trained by the “full” variant. 16Stochastic Marginal Likelihood Gradients using Neural Tangent Kernels 200 400 steps −4 −3 −2 −1 log q(D|h) 200 400 steps −2.0 −1.5 −1.0 −0.5 test log likelihood 100 200 500 1000 2500 5000 10000 indices |Bm| 200 400 steps −4 −3 −2 −1 log q(D|h) 200 400 steps −2.0 −1.5 −1.0 −0.5 test log likelihood 10 20 50 100 250 500 1000 indices |Bm| Figure 9.Doubly parametric subset-of-data bound on the left with block-diagonal approximation and class-wiseNTK -based approximation. These figures correspond to Figure 5 and Figure 6 in the main text but use the bounds for optimization of invariances during training. In line with the bounds, already small subset sizes suffice to pick up the invariance in rotated MNIST and achieve good test log likelihood. 200 400 steps −3 −2 −1 log q(D|h) 200 400 steps −0.8 −0.6 −0.4 −0.2 test log likelihood full blockdiag kron diag 200 400 steps −3 −2 −1 log q(D|h) 200 400 steps −0.8 −0.6 −0.4 −0.2 test log likelihood 100 200 500 1000 2500 5000 10000 indices |Bm| Figure 10.Parametric and NTK lower bound when optimizing prior precision and not invariances on random subsets of size 1000 from MNIST (LeCun & Cortes, 2010). In contrast to invariance learning, lower bounds result in more reduction in test performance for the NTK -based bounds that use stochastic estimates. This figure corresponds to Figure 3 and Figure 4 in the main text. 200 400 steps −3 −2 −1 log q(D|h) 200 400 steps −0.8 −0.6 −0.4 −0.2 test log likelihood 100 200 500 1000 2500 5000 10000 indices |Bm| 200 400 steps −3 −2 −1 log q(D|h) 200 400 steps −0.8 −0.6 −0.4 −0.2 test log likelihood 10 20 50 100 250 500 1000 indices |Bm| Figure 11.Doubly parametric subset-of-data bound with block-diagonal approximation (left) and class-wise NTK -based approximation (right) for prior precision optimization corresponding to Figure 5 and Figure 6 in the main text. 10− 2 100 102 prior precision − 2 − 4 − 6 log q˜w (D|h) 0 π/2 π rotational invariance − 2 − 4 − 6 100 200 500 1000 2500 5000 10000 indices|Bm| 10− 2 100 102 prior precision − 2 − 4 − 6 log q˜w (D|h) 0 π/2 π rotational invariance − 2 − 4 − 6 100 200 500 1000 2500 5000 10000 indices|Bm| Figure 12.Effect of partitioning data by class labels (right) in comparison to random partitioning (left; same figure as Figure 4) as proposed in Sec. 4.3. Using the class labels can slighty redue the slack of the bound but tends to incur a larger variance in the estimated bound as indicated by the shaded regions denoting one standard error. C.2. Additional Details for Estimator Benchmark on CIFAR Marginal likelihood estimation with the linearized Laplace in previous works was commonly applied to ResNets on CIFAR- 10 and CIFAR-100, where it gives remarkable performance improvements over unregularized networks in the case where we have no data augmentation or prior information. For example Immer et al. (2021a) and Daxberger et al. (2021) show that it improves the performance by 4% points over the baseline on CIFAR-10. Here, we reproduce this experiment and compare the performance of the previously used full-batch KFAC estimator, which produced the best results, to the proposed stochastic estimators. The results are given in Table 1 in the top block. Indeed, all estimators improve over the baseline that uses default settings of a Wide ResNet (Zagoruyko & Komodakis, 2016) but without data augmentation and Fixup initialization instead of normalization layers (Zhang et al., 2019b). We use the Wide ResNet 16-4 with Fixup (Zhang et al., 2019b), since a Gaussian prior conflicts with normalization layers (Zhang et al., 2019a; Antor ´an et al., 2022b). We optimize 17Stochastic Marginal Likelihood Gradients using Neural Tangent Kernels 10−2 10−1 100 Runtime (s) 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 2.50 Negative Log Marginal Likelihood Full GGN NTK KFAC Diag GGN 10 20 50 100 250 500 1000 2500 5000 10000 batch size Figure 13.Pareto-frontier for prior precision learning runs on random subsets of 1000 MNIST digits with a small CNN. This figure corresponds to Figure 1 in the main text, which displays estimators for additional invariance learning, not only learning priors. Many of the NTK -based class-wise estimators are Pareto-optimal, i.e., the achieve the lowest marginal likelihood at a given runtime budget. the network for 300 epochs with a batch size of 128 using SGD with momentum of 0.9 and optimize the prior precision every 5 epochs after a burnin phase of 10 epochs for with 50 gradient steps using a learning rate of 0.1 and Adam (Kingma & Ba, 2015) for both CIFAR-10 and CIFAR-100. In the lower block of Table 1, we use our estimators in the context of learning invariance using laplace approximations (LILA , Immer et al., 2022b). We use the default hyperparameters of their largest-scale experiment, where they learn invariances with Wide ResNets using 20 augmentation samples. Their method uses full dataset KFAC for 200 epochs and takes more than 48 hours on of training on CIFAR-10 with an NVIDIA A100 GPU because they have to compute a preconditioner on the entire dataset for each hyperparameter gradient. On CIFAR-100, due to the scaling with output classes, it is therefore intractable as it would likely take weeks. The default settings from CIFAR-10 lead to out-of-memory errors. In contrast, our stochastic estimators can be directly evaluated on a small batch with a single output and thus improve the runtime by a factor 10 while maintaining the same performance, i.e., compare KFAC-150-1 with KFAC-N-C. Also our single-output KFAC bound already increases runtime by a factor of 5 and achieves the same performance. The choice of the batch size in our bounds is such that the GPU memory is utilized as much as possible. We therefore use NVIDIA A100 GPUs with 80GB memory to make the bounds as tight as possible and improve performance. To compare the runtimes, all methods use the same GPU type and runtimes are averaged over 3 or 5 seeds for invariance and prior learning, respectively. C.3. Details on TinyImagenet Benchmark using ResNet-50 We largely follow the setup of Mlodozeniec et al. (2023) in this experiment and use their results on Augerino (Benton et al., 2020) and their own network partitioning method for hyperparameter optimization. The key difference is that we use Fixup (Zhang et al., 2019b) instead of normalization layers in the ResNet-50 architecture. Like Mlodozeniec et al. (2023), we use 16 channels in the first layer and the standard growth factor of 2 per layer. The network has roughly 23 000 000 parameters and therefore poses a complex task for differentiable hyperparameter optimization. Even with 80GB of GPU memory, we can only fit 60 to 70 data points for differentiable KFAC or NTK computations. However, the performance is still sufficient to improve over the baselines. We optimize the network weights for 100 epochs and, as in the case for CIFAR, decay the learning rate with a cosine schedule from 0.1 to 10−6. The hyperparameters are optimized for 50 steps every epoch after 10 epochs of burn-in with a learning rate of 0.01 decayed to 0.001 with a cosine schedule. The learning rate for prior parameters is constant at 0.1. For only prior precision optimization, we use the default setting by Immer et al. (2021a) and update the prior parameters for 50 steps every 5 epochs after 10 epochs of burn-in. 18Stochastic Marginal Likelihood Gradients using Neural Tangent Kernels C.4. Additional and Detailed Results for Invariance Learning on CIFAR-10 Subsets. It is well known that soft or hard symmetry constraints are beneficial to machine learning methods and neural networks in particular, equivariant symmetries of (group-)convolutional layers being a canonical example (Cohen & Welling, 2016). Although explicitly embedding symmetry in architectures can be very effective, they need to be known and specified in advance and can not be adapted. The right symmetry can be difficult to choose and misspecified symmetries have the risk of restricting the capacity of a model to fit data (Wang et al., 2022). Symmetry discovery methods aim to automatically learn symmetries from available training data, which is a difficult task because symmetries provide constraints and are therefore not necessarily favorable in terms of typical training losses. In invariance learning, we parameterise a learnable distribution g(x; η) over possible transformations on inputs, defined by invariance parameters η. By averaging outcomes of a regular non-invariant neural network f over inputs transformed by g, we obtain a predictor ˜f, which is (soft-) invariant to the transformations specified by η. We might take S Monte Carlo samples ϵ1, . . . ,ϵS iid ∼ p(ϵ) (van der Wilk et al., 2018; Benton et al., 2020) to obtain an unbiased estimate of the invariant function ˆf: ˆf(x; w, η) =Ep(x′|x,η)[f(x′; w)] =Ep(ϵ)[f(g(x, ϵ; η); w)] ≈ 1 S P sf(g(x, ϵs; η); w) , (20) where ˆf is differentiable in both model parameters w as well as invariance parameters η through the reparameterisation trick (Kingma & Welling, 2013). In our experiments, we consider a combination of uniform distributions on affine generator matrices resulting in a 6-dimensional invariance parameter η ∈ R6 that defines a density over the group of affine transformations, individually controlling x-translation, y-translation, rotation, x-scaling, y-scaling and shearing. Details on the used parameterisation can be found in Benton et al. (2020) and App. B of Immer et al. (2022b). Jointly learning model parameters w and invariance parameters η is not a trivial exercise, as typical maximum likelihood solutions will always select the least restrictive no invariance in order to fit training data best, given a sufficiently flexible model. To overcome this issue, some people have considered the use of validation data, such as cross-validation or more sophisticated approaches, which may require expensive retraining or involved outer loops. Alternatively, Augerino (Benton et al., 2020) learns invariance solely on training data, but relies on explicit regularisation in order to do so, which requires knowledge of the used invariance parameterisation and additional tuning. For a discussion on particular failure cases of this approach we refer to Sec. 2. and App. C. of Immer et al. (2022b), which proposes to use scalable marginal likelihood estimates to learn invariance parameters. The marginal likelihood offers a principled way to learn invariances through Bayesian model selection, balancing data fit and model complexity through an Occam’s razor effect. Unlike prior works that rely on the true marginal likelihood (van der Wilk et al., 2018) or variational lower bounds thereof (van der Wilk et al., 2018; van der Ouderaa & van der Wilk, 2021), modern differentiable Laplace approximations of the marginal likelihood can scale to large datasets and deep neural networks, such as ResNets. We consider this as a baseline, and denote it by KFAC-full as it uses the full dataset to estimate the marginal likelihood, which leads to a significant slowdown in training compared to classical MAP training. To improve estimates, we have derived lower bounds to the marginal likelihood that can be computed much more cheaply on subsets of the training data. This allows us to take more hyperparameter gradient steps per epoch at equivalent or faster training time. We hypothesize that taking more gradient steps can be beneficial, even though individual marginal likelihood estimates provide looser bounds. We empirically evaluate performance of our method in conjunction with invariance learning on subsets of the same transformed versions of CIFAR-10 as used in Immer et al. (2022b). The use of reduced dataset sizes is a common data efficiency benchmark for invariance learning methods (Schw¨obel et al., 2022). We measure performance for varying total dataset sizes where we estimate marginal likelihoods on fixed subsets of 400 datapoints and use independent outputs, KFAC-400-1 and NTK -400-1. We compare to KFAC-full which always uses the full set of available training data points for its estimates. We use a ResNet architecture with Fixup (Zhang et al., 2019b), to prevent Gaussian prior conflicts with normalization layers (Zhang et al., 2019a; Antor´an et al., 2022b), and take S=20 Monte Carlo of invariance transformations. We optimize the network for 200 epochs with a batch size of 128 using Adam (Kingma & Ba, 2015) with cosine annealed learning rates starting at 0.1 for parameters and prior variances and invariance parameter learning rates starting at 0.05 for KFAC-full and 0.01 for KFAC-400-1 and KFAC-400-1, which use more gradient steps. In the figures below, we show the marginal likelihood estimate (Figure 14), test log-likelihood (Figure 15) and test accuracy (Figure 16). We find that marginal likelihood estimates of KFAC-full are higher, which is not surprising since KFAC-400-1 and KFAC-400-1 use looser bounds. In terms of test performance, we find that using more approximate gradients can at the same time improve runtime as well as test accuracy and test log-likelihood. 19Stochastic Marginal Likelihood Gradients using Neural Tangent Kernels 1k 5k 10k 20k full subset size −3 −2 −1 0 q(D |h) original CIFAR-10 KFAC-full KFAC-400-1 (ours) NTK -400-1 (ours) 1k 5k 10k 20k full subset size −3 −2 −1 0 fully-rotated CIFAR-10 1k 5k 10k 20k full subset size −3 −2 −1 0 partially-rotated CIFAR-10 1k 5k 10k 20k full subset size −3 −2 −1 0 translated CIFAR-10 Figure 14.Comparison of marginal likelihoods on different subsets of CIFAR-10 datasets. 1k 5k 10k 20k full subset size −2.0 −1.5 −1.0 −0.5 test log likelihood original CIFAR-10 KFAC-full KFAC-400-1 (ours) NTK -400-1 (ours) 1k 5k 10k 20k full subset size −2.0 −1.5 −1.0 −0.5 fully-rotated CIFAR-10 1k 5k 10k 20k full subset size −2.0 −1.5 −1.0 −0.5 partially-rotated CIFAR-10 1k 5k 10k 20k full subset size −2.0 −1.5 −1.0 −0.5 translated CIFAR-10 Figure 15.Comparison of test log-likelihood on different subsets of CIFAR-10 datasets. 1k 5k 10k 20k full subset size 0 .2 0 .4 0 .6 0 .8 1.0 test accuracy original CIFAR-10 KFAC-full KFAC-400-1 (ours) NTK -400-1 (ours) 1k 5k 10k 20k full subset size 0 .2 0 .4 0 .6 0 .8 1.0 fully-rotated CIFAR-10 1k 5k 10k 20k full subset size 0 .2 0 .4 0 .6 0 .8 1.0 partially-rotated CIFAR-10 1k 5k 10k 20k full subset size 0 .2 0 .4 0 .6 0 .8 1.0 translated CIFAR-10 Figure 16.Comparison of test accuracy on different subsets of CIFAR-10 datasets. 20",
      "meta_data": {
        "arxiv_id": "2306.03968v1",
        "authors": [
          "Alexander Immer",
          "Tycho F. A. van der Ouderaa",
          "Mark van der Wilk",
          "Gunnar Rätsch",
          "Bernhard Schölkopf"
        ],
        "published_date": "2023-06-06T19:02:57Z",
        "pdf_url": "https://arxiv.org/pdf/2306.03968v1.pdf",
        "github_url": "https://github.com/AlexImmer/ntk-marglik"
      },
      "llm_extracted_info": {
        "main_contributions": "This research addresses the challenge of scaling hyperparameter optimization in deep learning, which traditionally requires full-dataset passes for each gradient estimate of the marginal likelihood. The main contribution is the introduction of novel lower bounds to the linearized Laplace approximation of the marginal likelihood. These bounds are amenable to stochastic-gradient-based optimization, allowing for efficient hyperparameter learning using mini-batches of data. The paper derives these bounds from the function-space form of the linearized Laplace, estimated using the Neural Tangent Kernel (NTK), and shows that existing structured approximations are also lower bounds. Experimentally, these estimators significantly accelerate gradient-based hyperparameter optimization, enabling up to 25-fold speed-ups and application to larger datasets.",
        "methodology": "The methodology is based on Bayesian model selection, optimizing hyperparameters by maximizing the marginal likelihood using linearized Laplace approximation. The core involves deriving lower bounds to the log marginal likelihood. Two main types of bounds are introduced: Parametric structured lower bounds (Theorem 1), which justify block-diagonal and diagonal approximations of the Generalized Gauss-Newton (GGN) Hessian, and Subset-of-Data Kernel Bounds (Theorem 2), derived from the dual NTK form of the log-determinant. These NTK-based bounds allow partitioning the data and outputs into batches for stochastic estimation. A further Parametric data subset bound (Theorem 3) and a Parametric doubly lower bound (Corollary 3.1) combine both concepts, enabling structured parametric approximations like KFAC on data subsets. The method allows trade-offs between estimation accuracy and computational complexity by varying batch sizes and partitioning strategies (e.g., output-wise or class-wise partitioning). Unbiased stochastic estimates and gradients are obtained by uniformly sampling index sets for batches.",
        "experimental_setup": "The estimators were validated on several datasets and tasks: MNIST (1000 random digits, rotated and original) with a small 3-layer convolutional neural network (approx. 16,000 parameters) for illustrative purposes. CIFAR-10 and CIFAR-100 with a Wide ResNet 16-4 (using Fixup initialization) for learning layer-wise prior precisions (weight decay) and affine invariances. TinyImagenet (100,000 data points, 200 classes) with a ResNet-50 (approx. 23 million parameters) for scaling to larger datasets. Validation involved comparing the tightness of derived bounds against the exact linearized Laplace approximation (where computable), benchmark against state-of-the-art full-batch KFAC-Laplace, and evaluation of test log likelihood and accuracy. Runtimes were compared on NVIDIA A100 GPUs.",
        "limitations": "One limitation is that the derived lower bounds, while enabling stochastic optimization, introduce 'slack' compared to the exact marginal likelihood. Very small subset sizes can lead to failures during optimization, similar to the diagonal approximation. For certain tasks like prior precision optimization, stochastic NTK-based bounds can result in a more significant reduction in test performance compared to full-batch methods. Additionally, full-batch Laplace approximations, such as KFAC-N-C, proved intractable and ran out of memory for larger datasets and models like CIFAR-100 and TinyImagenet, highlighting a scalability constraint for previous methods. The assumption of a diagonal prior P0 is made in some proofs.",
        "future_research_directions": "Future research could focus on finding ways to make the bounds tighter, for example, by improving the partitioning strategies of the Neural Tangent Kernel (NTK). It would also be interesting to develop methods that track (anti-)correlation between inputs in the NTK to group them optimally during training, similar to inducing point optimization. Another direction is to apply these fast stochastic estimators to large-scale Bayesian linear models. The efficiency of stochastic estimators for general differentiable hyperparameter optimization is identified as an exciting future area.",
        "experimental_code": "def marglik_optimization(model,train_loader,marglik_loader=None,valid_loader=None,partial_loader=None,likelihood='classification',prior_structure='layerwise',prior_prec_init=1.,sigma_noise_init=1.,temperature=1.,n_epochs=500,lr=1e-1,lr_min=None,optimizer='SGD',scheduler='cos',n_epochs_burnin=0,n_hypersteps=100,n_hypersteps_prior=1,marglik_frequency=1,lr_hyp=1e-1,lr_hyp_min=1e-1,lr_aug=1e-2,lr_aug_min=1e-2,laplace=KronLaplace,backend=AsdlGGN,independent=False,single_output=False,single_output_iid=False,kron_jac=True,method='baseline',augmenter=None,stochastic_grad=False,use_wandb=False):\"\"\"Runs marglik optimization training for a given model and training dataloader.Parameters----------model : torch.nn.Moduletorch modeltrain_loader : DataLoaderpytorch training dataset loadermarglik_loader : DataLoaderpytorch data loader for fitting Laplacemarglik_loader is used for fitting the Laplace approximation, typically a subset or the full training data.valid_loader : DataLoaderpytorch data loader for validationpartial_loader : DataLoaderpytorch data loader for partial fitting for lila's grad accumulationlikelihood : str'classification' or 'regression'prior_structure : str'scalar', 'layerwise', 'diagonal'prior_prec_init : floatinitial prior precisionsigma_noise_init : floatinitial observation noise (for regression only)temperature : factortemperature of the likelihood; lower temperature leads to moreconcentrated posterior and vice versa.n_epochs : intlr : floatlearning rate for model optimizerlr_min : floatminimum learning rate, defaults to lr and hence no decayto have the learning rate decay from 1e-3 to 1e-6, setlr=1e-3 and lr_min=1e-6.optimizer : streither 'Adam' or 'SGD'scheduler : streither 'exp' for exponential and 'cos' for cosine decay towards lr_minn_epochs_burnin : int default=0how many epochs to train without estimating and differentiating marglikn_hypersteps : inthow many steps to take on diff. hyperparameters when marglik is estimatedn_hypersteps_prior : inthow many steps to take on the prior when marglik is estimatedmarglik_frequency : inthow often to estimate (and differentiate) the marginal likelihoodlr_hyp : floatlearning rate for hyperparameters (should be between 1e-3 and 1)lr_hyp_min : floatminimum learning rate, decayed to using cosine schedulelr_aug : floatlearning rate for augmentation parameterslr_aug_min : floatminimum learning rate, decayed to using cosine schedulelaplace : Laplacetype of Laplace approximation (Kron/Diag/Full)backend : BackendAsdlGGN/AsdlEF or BackPackGGN/BackPackEFindependent : boolwhether to use independent functional laplacesingle_output : boolwhether to use single random output for functional laplacesingle_output_iid : boolwhether to sample single output per sample iid (otherwise per batch)kron_jac : boolwhether to use kron_jac in the backendmethod : augmentation strategy, one of ['baseline'] -> no changeor ['lila'] -> change in protocol.augmenter : torch.nn.Module with differentiable parameterstochastic_grad : boolwhether to use stochastic gradients of marginal likelihoodusually would correspond to lower bound (unless small data)Returns-------lap : Laplacelapalce approximationmodel : torch.nn.Moduledemodel : torch.nn.Modulemargliks : listvalid_perfs : listaug_history: listNone for method == 'baseline'\"\"\"if lr_min is None: # don't decay lrlr_min = lrif marglik_loader is None:marglik_loader = train_loaderif partial_loader is None:partial_loader = marglik_loaderdevice = parameters_to_vector(model.parameters()).deviceN = len(train_loader.dataset)H = len(list(model.parameters()))P = len(parameters_to_vector(model.parameters()))optimize_aug = augmenter is not None and parameters_to_vector(augmenter.parameters()).requires_gradbackend_kwargs = dict(differentiable=(stochastic_grad and optimize_aug) or laplace is FunctionalLaplace,kron_jac=kron_jac)la_kwargs = dict(sod=stochastic_grad, single_output=single_output, single_output_iid=single_output_iid)if laplace is FunctionalLaplace:la_kwargs['independent'] = independentif use_wandb:wandb.config.update(dict(n_params=P, n_param_groups=H, n_data=N))# differentiable hyperparametershyperparameters = list()# prior precisionlog_prior_prec = get_prior_hyperparams(prior_prec_init, prior_structure, H, P, device)hyperparameters.append(log_prior_prec)# set up loss (and observation noise hyperparam)if likelihood == 'classification':criterion = CrossEntropyLoss(reduction='mean')sigma_noise = 1elif likelihood == 'regression':criterion = MSELoss(reduction='mean')log_sigma_noise_init = np.log(sigma_noise_init)log_sigma_noise = log_sigma_noise_init * torch.ones(1, device=device)log_sigma_noise.requires_grad = Truehyperparameters.append(log_sigma_noise)# set up model optimizer and scheduleroptimizer = get_model_optimizer(optimizer, model, lr)scheduler = get_scheduler(scheduler, optimizer, train_loader, n_epochs, lr, lr_min)n_steps = ((n_epochs - n_epochs_burnin) // marglik_frequency) * n_hypersteps_priorhyper_optimizer = Adam(hyperparameters, lr=lr_hyp)hyper_scheduler = CosineAnnealingLR(hyper_optimizer, n_steps, eta_min=lr_hyp_min)if optimize_aug:logging.info('MARGLIK: optimize augmentation.')aug_optimizer = Adam(augmenter.parameters(), lr=lr_aug)n_steps = ((n_epochs - n_epochs_burnin) // marglik_frequency) * (n_hypersteps if stochastic_grad else 1)aug_scheduler = CosineAnnealingLR(aug_optimizer, n_steps, eta_min=lr_aug_min)aug_history = [parameters_to_vector(augmenter.parameters()).squeeze().detach().cpu().numpy()]losses = list()valid_perfs = list()margliks = list()for epoch in range(1, n_epochs + 1):epoch_time_fwd = 0.0epoch_time_fit = 0.0epoch_loss = 0epoch_perf = 0epoch_nll = 0epoch_log = dict(epoch=epoch)# standard NN training per batchtorch.cuda.empty_cache()for X, y in train_loader:X, y = X.detach().to(device), y.to(device)optimizer.zero_grad()if likelihood == 'regression':sigma_noise = torch.exp(log_sigma_noise).detach()crit_factor = 1 / temperature / (2 * sigma_noise.square())else:crit_factor = 1 / temperatureprior_prec = torch.exp(log_prior_prec).detach()delta = expand_prior_precision(prior_prec, model)# fit datatime_fwd = time()if method == 'lila':f = model(X).mean(dim=1)else:f = model(X)epoch_time_fwd += time() - time_fwd # log total time fwd fit in epochtime_fit = time()theta = parameters_to_vector(model.parameters())loss = criterion(f, y) + (0.5 * (delta * theta) @ theta) / N / crit_factorloss.backward()optimizer.step()epoch_time_fit += time() - time_fit # log total time bwd fit in epochepoch_loss += loss.cpu().item() / len(train_loader)epoch_nll += criterion(f.detach(), y).item() / len(train_loader)if likelihood == 'regression':epoch_perf += (f.detach() - y).square().sum() / Nelse:epoch_perf += torch.sum(torch.argmax(f.detach(), dim=-1) == y).item() / Nscheduler.step()losses.append(epoch_loss)logging.info('MAP memory allocated: ' + str(torch.cuda.max_memory_allocated(loss.device)/GB_FACTOR) + ' Gb.')logging.info(f'MARGLIK[epoch={epoch}]: train. perf={epoch_perf*100:.2f}%; loss={epoch_loss:.5f}; nll={epoch_nll:.5f}')optimizer.zero_grad(set_to_none=True)llr = scheduler.get_last_lr()[0]epoch_log.update({'train/loss': epoch_loss, 'train/nll': epoch_nll, 'train/perf': epoch_perf, 'train/lr': llr,'train/time_fwd': epoch_time_fwd, 'train/time_fit': epoch_time_fit})if use_wandb and ((epoch % 5) == 0):wandb_log_parameter_norm(model)# compute validation error to report during trainingif valid_loader is not None:with torch.no_grad():val_perf, val_nll = valid_performance(model, valid_loader, likelihood, criterion, method, device)valid_perfs.append(val_perf)logging.info(f'MARGLIK[epoch={epoch}]: valid. perf={val_perf*100:.2f}%; nll={val_nll:.5f}.')epoch_log.update({'valid/perf': val_perf, 'valid/nll': val_nll})# only update hyperparameters every \"Frequency\" steps after \"burnin\"if (epoch % marglik_frequency) != 0 or epoch < n_epochs_burnin:if use_wandb:wandb.log(epoch_log, step=epoch, commit=((epoch % 10) == 0))continue# optimizer hyperparameters by differentiating margliktime_hyper = time()# 1. fit laplace approximationtorch.cuda.empty_cache()if optimize_aug:if stochastic_grad: # differentiablemarglik_loader.attach()else: # jvpmarglik_loader.detach()# first optimize prior precision jointly with direct marglik grads.margliks_local = list()n_hyper = max(n_hypersteps_prior, n_hypersteps) if stochastic_grad else n_hypersteps_priorfor i in range(n_hyper):if i == 0 or stochastic_grad:sigma_noise = 1 if likelihood == 'classification' else torch.exp(log_sigma_noise)prior_prec = torch.exp(log_prior_prec)lap = laplace(model, likelihood, sigma_noise=sigma_noise, prior_precision=prior_prec,temperature=temperature, backend=backend, backend_kwargs=backend_kwargs,**la_kwargs)lap.fit(marglik_loader)if i < n_hypersteps and optimize_aug and stochastic_grad:aug_optimizer.zero_grad()if i < n_hypersteps_prior:hyper_optimizer.zero_grad()if i < n_hypersteps_prior and not stochastic_grad: # does not fit every itsigma_noise = None if likelihood == 'classification' else torch.exp(log_sigma_noise)prior_prec = torch.exp(log_prior_prec)marglik = -lap.log_marginal_likelihood(prior_prec, sigma_noise) / Nelse: # fit with updated hparamsmarglik = -lap.log_marginal_likelihood() / Nmarglik.backward()margliks_local.append(marglik.item())if i < n_hypersteps_prior:hyper_optimizer.step()hyper_scheduler.step()if i < n_hypersteps and optimize_aug and stochastic_grad:aug_optimizer.step()aug_scheduler.step()if stochastic_grad:marglik = np.mean(margliks_local)else:marglik = margliks_local[-1]if use_wandb:wandb_log_prior(torch.exp(log_prior_prec.detach()), prior_structure, model)if likelihood == 'regression':epoch_log['hyperparams/sigma_noise'] = torch.exp(log_sigma_noise.detach()).cpu().item()epoch_log['train/marglik'] = margliklogging.info('LA memory allocated: ' + str(torch.cuda.max_memory_allocated(loss.device)/GB_FACTOR) + ' Gb.')# option 2: jvp (not direct_grad)torch.cuda.empty_cache()if optimize_aug and not stochastic_grad: # accumulate gradient with JVPpartial_loader.attach()aug_grad = torch.zeros_like(parameters_to_vector(augmenter.parameters()))lap.backend.differentiable = Trueif isinstance(lap, KronLaplace):# does the inversion internallyhess_inv = lap.posterior_precision.jvp_logdet()else:hess_inv = lap.posterior_covariance.flatten()for i, (X, y) in zip(range(n_hypersteps), partial_loader):lap.loss, H_batch = lap._curv_closure(X, y, N)# curv closure creates gradient already, need to zeroaug_optimizer.zero_grad()# compute grad wrt. neg. log-lik(- lap.log_likelihood).backward(inputs=list(augmenter.parameters()), retain_graph=True)# compute grad wrt. log det = 0.5 vec(P_inv) @ (grad-vec H)(0.5 * H_batch.flatten()).backward(gradient=hess_inv, inputs=list(augmenter.parameters()))aug_grad = (aug_grad + gradient_to_vector(augmenter.parameters()).data.clone())lap.backend.differentiable = Falsevector_to_gradient(aug_grad, augmenter.parameters())aug_optimizer.step()aug_scheduler.step()epoch_time_hyper = time() - time_hyperepoch_log.update({'train/time_hyper': epoch_time_hyper})if optimize_aug:aug_history.append(parameters_to_vector(augmenter.parameters()).squeeze().detach().cpu().numpy())logging.info(f'Augmentation params epoch {epoch}: {aug_history[-1]}')if use_wandb:wandb_log_invariance(augmenter)logging.info('LA memory allocated: ' + str(torch.cuda.max_memory_allocated(loss.device)/GB_FACTOR) + ' Gb.')margliks.append(marglik)del lapif use_wandb:if optimize_aug:epoch_log['train/lr_aug'] = aug_scheduler.get_last_lr()[0]epoch_log['train/lr_hyp'] = hyper_scheduler.get_last_lr()[0]wandb.log(epoch_log, step=epoch, commit=((epoch % 10) == 0))# early stopping on marginal likelihoodlogging.info(f'MARGLIK[epoch={epoch}]: marglik optimization. MargLik={margliks[-1]:.5f}, prec: {prior_prec.detach().mean().item():.2f}.')sigma_noise = 1 if sigma_noise is None else sigma_noiselap = laplace(model, likelihood, sigma_noise=sigma_noise, prior_precision=prior_prec,temperature=temperature, backend=backend, backend_kwargs=backend_kwargs,**la_kwargs)lap.fit(marglik_loader.detach())if optimize_aug:return lap, model, margliks, valid_perfs, aug_historyreturn lap, model, margliks, valid_perfs, None",
        "experimental_info": "Laplace Approximation Types: FunctionalLaplace (for NTK/Kernel bounds), FullLaplace, KronLaplace (for KFAC-like approximations), DiagLaplace, BlockDiagLaplace.Curvature Backends: AsdlGGN, AugAsdlGGN (for augmented models/invariances), AsdlEF (Empirical Fisher), AugAsdlEF (augmented Empirical Fisher).Likelihoods: 'classification', 'regression', 'heteroscedastic_regression'.Hyperparameter Optimization (maximizing marginal likelihood):Prior Precision (`prior_prec_init`): Initialized to 1.0 (or other values, e.g., 0.089), optimized across 'scalar', 'layerwise', or 'diagonal' structures.Observation Noise (`sigma_noise_init`): Initialized to 1.0 (for regression, or 0.27).Augmentation Parameters (e.g., `AffineLayer2d`'s `rot_factor` for `method='lila'`): Initialized to 0.0, differentiable, optimized (includes rotation, translation, scaling, shearing factors).Optimization Steps: `n_hypersteps` (e.g., 1 to 100), `n_hypersteps_prior` (e.g., 1 to 10).Learning Rates: `lr_hyp` (e.g., 0.1, decayed to 0.01), `lr_aug` (e.g., 0.05, decayed to 0.005).Optimization Frequency: `marglik_frequency` (e.g., 1 or 5 epochs).Data Handling and Bounds:Subset-of-Data (Sod): `sod=True` enables stochastic estimation using subsets for the Laplace fit.Batch Sizes: `marglik_batch_size` (e.g., 10, 20, 50, 100, 250, 500, 1000) for Laplace fitting.Data Loaders: `SubsetTensorDataLoader` (uniformly sampled random subsets), `GroupedSubsetTensorDataLoader` (class-wise partitioning).Partitioning Strategies: `single_output=True` (output-wise partitioning), `single_output_iid=True` (independent single output samples), `independent_outputs=True` (for independent kernel bounds).Stochastic Gradients: `stochastic_grad=True` when using lower bound estimators (e.g., for data subsets or augmented models).Model Training:Models: MiniNet, MLP, LeNet, ResNet, WideResNet.Datasets: MNIST, FashionMNIST, CIFAR10, CIFAR100, TinyImageNet, including rotated (`mnist_r180`), translated, and scaled versions.Epochs: `n_epochs` (e.g., 100 to 500), `n_epochs_burnin` (e.g., 0 to 10).Optimizer: Adam, SGD.Learning Rate: `lr` (e.g., 1e-3 to 0.1), with cosine annealing scheduler (`scheduler='cos'`).Miscellaneous: `temperature` (1.0), `kron_jac=True` (use Kronecker approximation for Jacobians where applicable).Evaluation Metrics: Log marginal likelihood, test log likelihood, accuracy/MSE, runtime."
      }
    },
    {
      "title": "AUTOMATA: Gradient Based Data Subset Selection for Compute-Efficient Hyper-parameter Tuning",
      "abstract": "Deep neural networks have seen great success in recent years; however,\ntraining a deep model is often challenging as its performance heavily depends\non the hyper-parameters used. In addition, finding the optimal hyper-parameter\nconfiguration, even with state-of-the-art (SOTA) hyper-parameter optimization\n(HPO) algorithms, can be time-consuming, requiring multiple training runs over\nthe entire dataset for different possible sets of hyper-parameters. Our central\ninsight is that using an informative subset of the dataset for model training\nruns involved in hyper-parameter optimization, allows us to find the optimal\nhyper-parameter configuration significantly faster. In this work, we propose\nAUTOMATA, a gradient-based subset selection framework for hyper-parameter\ntuning. We empirically evaluate the effectiveness of AUTOMATA in\nhyper-parameter tuning through several experiments on real-world datasets in\nthe text, vision, and tabular domains. Our experiments show that using\ngradient-based data subsets for hyper-parameter tuning achieves significantly\nfaster turnaround times and speedups of 3$\\times$-30$\\times$ while achieving\ncomparable performance to the hyper-parameters found using the entire dataset.",
      "full_text": "AUTOMATA : G RADIENT BASED DATA SUBSET SELECTION FOR COMPUTE -EFFICIENT HYPER -PARAMETER TUNING Krishnateja Killamsetty1, Guttu Sai Abhishek 2, Aakriti 2, Alexandre V . Evﬁmievski 3 Lucian Popa3, Ganesh Ramakrishnan 2, Rishabh Iyer 1 1 The University of Texas at Dallas 2Indian Institute of Technology Bombay, India 3 IBM Research {krishnateja.killamsetty, rishabh.iyer}@utdallas.edu {gsaiabhishek, aakriti, ganesh}@cse.iitb.ac.in {evfimi, lpopa}@us.ibm.com March 17, 2022 ABSTRACT Deep neural networks have seen great success in recent years; however, training a deep model is often challenging as its performance heavily depends on the hyper-parameters used. In addition, ﬁnding the optimal hyper-parameter conﬁguration, even with state-of-the-art (SOTA) hyper-parameter optimization (HPO) algorithms, can be time-consuming, requiring multiple training runs over the entire dataset for different possible sets of hyper-parameters. Our central insight is that using an informative subset of the dataset for model training runs involved in hyper-parameter optimization, allows us to ﬁnd the optimal hyper-parameter conﬁguration signiﬁcantly faster. In this work, we propose AUTOMATA , a gradient-based subset selection framework for hyper-parameter tuning. We empirically evaluate the effectiveness of AUTOMATA in hyper-parameter tuning through several experiments on real-world datasets in the text, vision, and tabular domains. Our experiments show that using gradient-based data subsets for hyper-parameter tuning achieves signiﬁcantly faster turnaround times and speedups of 3×-30×while achieving comparable performance to the hyper-parameters found using the entire dataset. 1 Introduction In recent years, deep learning systems have found great success in a wide range of tasks, such as object recognition [14], speech recognition [16], and machine translation [1], making people’s lives easier on a daily basis. However, in the quest for near-human performance, more complex and deeper machine learning models trained on increasingly large datasets are being used at the expense of substantial computational costs. Furthermore, deep learning is associated with a signiﬁcantly large number of hyper-parameters such as the learning algorithm, batch size, learning rate, and model conﬁguration parameters (e.g., depth, number of hidden layers, etc.) that need to be tuned. Hence, running extensive hyper-parameter tuning and auto-ml pipelines is becoming increasingly necessary to achieve state-of-the-art models. However, tuning the hyper-parameters requires multiple training runs over the entire datasets (which are signiﬁcantly large nowadays), resulting in staggering compute costs, running times, and, more importantly, CO2 emissions. To give an idea of staggering compute costs, we consider an image classiﬁcation task on a relatively simple CIFAR-10 dataset where a single training run using a relatively simple model class of Residual Networks [15] on a V100 GPU takes around 6 hours. If we perform 1000 training runs (which is not uncommon today) naively using grid search for hyper-parameter tuning, it will take 6000 GPU hours. The resulting CO2 emissions would be between 640 to 1400 arXiv:2203.08212v1  [cs.LG]  15 Mar 2022A PREPRINT - MARCH 17, 2022 kg of CO2 emitted1, which is equivalent to 1600 to 3500 miles of car travel in the US. Similarly, the costs of training state-of-the-art NLP models and vision models on larger datasets like ImageNet are even more staggering [47]2. Figure 1: AUTOMATA ’s performance summary showing speedups, relative test errors, and tuning times on SST2, glue-SST2, CIFAR10, CIFAR100, and CONNECT-4 datasets. We observe that AUTOMATA achieves speedups(and similar energy savings) of 10x - 30x with around 2% performance loss using Hyperband as a sched- uler. Similarly, even when using a more efﬁcient ASHA sched- uler, AUTOMATA achieves a speedup of around 2x-3x with a per- formance loss of 0%-2%. Naive hyper-parameter tuning methods like grid search [ 4] often fail to scale up with the dimensionality of the search space and are computationally expensive. Hence, more efﬁcient and sophisticated Bayesian optimization methods [ 2, 18, 5, 45, 26] have dominated the ﬁeld of hyper-parameter optimization in recent years. Bayesian optimization methods aim to identify good hyper-parameter conﬁg- urations quickly by building a posterior distribution over the search space and by adaptively selecting conﬁgurations based on the proba- bility distribution. More recent methods [48, 49, 10, 26] try to speed up conﬁguration evaluations for efﬁcient hyper-parameter search; these approaches speed up the conﬁguration evaluation by adaptively allocating more resources to promising hyper-parameter conﬁgura- tions while eliminating poor ones quickly. Existing SOTA methods like SHA [19], Hyperband [ 31], ASHA [ 32] use aggressive early- stopping strategies to stop not-so-promising conﬁgurations quickly while allocating more resources to the promising ones. Generally, these resources can be the size of the training set, number of gra- dient descent iterations, training time, etc. Other approaches like [39, 28] try to quickly evaluate a conﬁguration’s performance on a large dataset by evaluating the training runs on small, random subsets; they empirically show that small data subsets could sufﬁce to estimate a conﬁguration’s quality. The past works [39, 28] show that very small data subsets can be effectively used to ﬁnd the best hyper-parameters quickly. However, all these approaches have naively used random training data subsets and did not place much focus on selecting informative subsets instead. Our central insight is that using small informative data subsets allows us to ﬁnd good hyper-parameter conﬁgurations more effectively than random data subsets. In this work, we study the application of the gradient-based subset selection approach for the task of hyper-parameter tuning and automatic machine learning. On that note, the use of gradient-based data subset selection approach in supervised learning setting was explored earlier in GRAD -MATCH [22] where the authors showed that training on gradient based data subsets allows the models to achieve comparable accuracy to full data training while being signiﬁcantly faster. In this work, we empirically study the advantage of using informative gradient-based subset selection algorithms for the hyper-parameter tuning task and study its accuracy when compared to using random subsets and a full dataset. So essentially, we use subsets of data to tune the hyper-parameters. Once we obtain the tuned hyper-parameters, we then train the model (with the obtained hyper-parameters) on the full dataset. The smaller the data subset we use, the more the speed up and energy savings (and hence the decrease in CO2 emissions). In light of all these insights, we propose AUTOMATA , an efﬁcient hyper-parameter tuning framework that combines existing hyper-parameter search and scheduling algorithms with intelligent subset selection. We further empirically show the effectiveness and efﬁciency of AUTOMATA for hyper-parameter tuning, when used with existing hyper-parameter search approaches (more speciﬁcally, TPE [2], Random search [42]), and hyper-parameter tuning schedulers (more speciﬁcally, Hyperband [31], ASHA [32]) on datasets spanning text, image, and tabular domains. 1.1 Related Work Hyper-parameter tuning and auto-ml approaches: A number of algorithms have been proposed for hyper-parameter tuning including grid search3, bayesian algorithms [3], random search [42], etc. Furthermore, a number of scalable toolkits and platforms for hyper-parameter tuning exist like Ray-tune [35]4, H2O automl [30], etc. See [44, 53] for a survey of current approaches and also tricks for hyper-parameter tuning for deep models. The biggest challenges of existing hyper-parameter tuning approaches are a) the large search space and high dimensionality of hyper-parameters and b) the increased training times of training models. Recent work [ 33] has proposed an efﬁcient approach for parallelizing hyper-parameter tuning using Asynchronous Successive Halving Algorithm (ASHA). AUTOMATA is complementary to such approaches and we show that our work can be be combined effectively with them. 1https://mlco2.github.io/impact/#compute 2https://tinyurl.com/a66fexc7 3https://tinyurl.com/3hb2hans 4https://docs.ray.io/en/master/tune/index.html 2A PREPRINT - MARCH 17, 2022 Configurations  comparison Choice of    Hyper-parameters Repeat until all possible sets of hyper- parameters are exhausted DSS based model training loop using the selected configuration DSS based model training loop using the selected configuration DSS based model training loop using the selected configuration Component-1  Hyper-parameter search algorithm Model's Validation set performance  Model's Validation set performance  Model's Validation set performance  Component-3  Hyper-parameter scheduler Initial  parameters     Mini-batch SGD on  for  epochs  Gradient based    DSS Mini-batch SGD on   for  epochs Gradient based    DSS Repeat   times    DSS based model training loop Promoted Configurations Component-2 Figure 2: Diagram of AUTOMATA , including hyper-parameter search, subset based conﬁguration evaluation (where models are trained on subsets of data), and hyper-parameter scheduler. Data Subset Selection: Several recent papers have used submodular functions 5 for data subset selection towards various applications like speech recognition [52, 51], machine translation [25] and computer vision [20]. Other common approaches for subset selection include the usage of coresets. Coresets are weighted subsets of the data, which approximate certain desirable characteristics of the full data (, e.g., the loss function) [11]. Coreset algorithms have been used for several problems including k-means and k-median clustering [13], SVMs [8] and Bayesian inference [6]. Recent coreset selection-based methods [37, 23, 22, 24] have shown great promise for efﬁcient and robust training of deep models. CRAIG [37] tries to select a coreset summary of the training data that estimate the full training gradient closely. Whereas GLISTER [ 23] poses the coreset selection problem as a discrete-continuous bilevel optimization problem that minimizes the validation set loss. Similarly, RETRIEVE [24] also uses a discrete bilevel coreset selection problem to select unlabeled data subsets for efﬁcient semi-supervised learning. Another approach GRAD -MATCH [22] selects coreset summary that approximately matches the full training loss gradient using orthogonal matching pursuit. 1.2 Contributions of the Work The contributions of our work can be summarized as follows: AUTOMATA Framework: We propose AUTOMATA a framework that combines intelligent subset selection with hyper-parameter search and scheduling algorithms to enable faster hyper-parameter tuning. To our knowledge, ours is the ﬁrst work that studies the role of intelligent data subset selection for hyper-parameter tuning. In particular, we seek to answer the following question: Is it possible to use small informative data subsets between 1% to 30% for faster conﬁguration evaluations in hyper-parameter tuning, thereby enabling faster tuning times while maintaining comparable accuracy to tuning hyper-parameters on the full dataset? Effectiveness of A UTOMATA : We empirically demonstrate the effectiveness of AUTOMATA framework used in conjunction with existing hyper-parameter search algorithms like TPE, Random Search, and hyper-parameter scheduling algorithms like Hyperband, ASHA through a set of extensive experiments on multiple real-world datasets. We give a summary of the speedup vs. relative performance achieved by AUTOMATA compared to full data training in Figure 1. More speciﬁcally, AUTOMATA achieves a speedup of 3x - 30x with minimal performance loss for hyper-parameter tuning. Further, in Section 3, we show that the gradient-based subset selection approach of AUTOMATA outperforms the previously considered random subset selection for hyper-parameter tuning. 5Let V = {1, 2, ··· , n}denote a ground set of items. A set function f : 2V →R is a submodular [ 12] if it satisﬁes the diminishing returns property: for subsets S ⊆T ⊆V and j ∈V \\T, f(j|S) ≜ f(S ∪j) −f(S) ≥f(j|T). 3A PREPRINT - MARCH 17, 2022 2 A UTOMATA Framework In this section, we present AUTOMATA , a hyper-parameter tuning framework, and discuss its different components shown in Figure 2. The AUTOMATA framework consists of three components: a hyper-parameter search algorithm that identiﬁes which conﬁguration sets need to be evaluated, a gradient-based subset selection algorithm for training and evaluating each conﬁguration efﬁciently, and a hyper-parameter scheduling algorithm that provides early stopping by eliminating the poor conﬁgurations quickly. With AUTOMATA framework, one can use any of the existing hyper- parameter search and hyper-parameter scheduling algorithms and still achieve signiﬁcant speedups with minimal performance degradation due to faster conﬁguration evaluation using gradient-based subset training. 2.1 Notation Denote by H, the set of conﬁgurations selected by the hyper-parameter search algorithm. Let D= {(xi,yi)}N i=1, denote the set of training examples, and V= {(xj,yj)}M j=1 the validation set. Let θi denote the classiﬁer model parameters trained using the conﬁguration i∈H. Let Si be the subset used for training the ith conﬁguration model θi and wi be its associated weight vector i.e., each data sample in the subset has an associated weight that is used for computing the weighted loss. We superscript the changing variables like model parameters θ, subset Swith the timestep tto denote their speciﬁc values at that timestep. Next, denote by Lj T(θi) =LT(xj,yj,θi), the training loss of the jth data sample in the dataset for ith classiﬁer model, and let LT(θi) =∑ k∈DLk T(θi) be the loss over the entire training set for ith conﬁguration model. Let, Lj T(S,θi) =∑ k∈XLT(xk,yk,θi) be the loss on a subset S⊆V of the training examples at timestep j. Let the validation loss be denoted by LV. 2.2 Component-1: Hyper-parameter Search Algorithm Given a hyper-parameter search space, hyper-parameter search algorithms provide a set of conﬁgurations that need to be evaluated. A naive way of performing the hyper-parameter search is Grid-Search, which deﬁnes the search space as a grid and exhaustively evaluates each grid conﬁguration. However, Grid-Search is a time-consuming process, meaning that thousands to millions of conﬁgurations would need to be evaluated if the hyper-parameter space is large. In order to ﬁnd optimal hyper-parameter settings quickly, Bayesian optimization-based hyper-parameter search algorithms have been developed. To investigate the effectiveness ofAUTOMATA across the spectrum of search algorithms, we used the Random Search method and the Bayesian optimization-based TPE method as representative hyper-parameter search algorithms. We provide more details on Random Search and TPE in Appendix D. 2.3 Component-2: Subset based Conﬁguration Evaluation Earlier, we discussed how a hyper-parameter search algorithm presents a set of potential hyper-parameter conﬁgurations that need to be evaluated when tuning hyper-parameters. Every time a conﬁguration needs to be evaluated, prior work trained the model on the entire dataset until the resource allocated by the hyper-parameter scheduler is exhausted. Rather than using the entire dataset for training, we propose using subsets of informative data selected based on gradients instead. As a result, given any hyper-parameter search algorithm, we can use the data subset selection to speed up each training epoch by a signiﬁcant factor (say 10x), thus improving the overall turnaround time of the hyper-parameter tuning. However, the critical advantage of AUTOMATA is that we can achieve speedups while still retaining the hyper-parameter tuning algorithm’s performance in ﬁnding the best hyper-parameters. The fundamental feature of AUTOMATA is that the subset selected by AUTOMATA changes adaptively over time, based on the classiﬁer model training. Thus, instead of selecting a common subset among all conﬁgurations,AUTOMATA selects the subset that best suits each conﬁguration. We give a detailed overview of the gradient-based subset selection process of AUTOMATA below. 2.3.1 Gradient Based Subset Selection (GSS) The key idea of gradient-based subset selection of AUTOMATA is to select a subset Sand its associated weight vector w such that the weighted subset loss gradient best approximates the entire training loss gradient. The subset selection of AUTOMATA for ith conﬁguration at time step tis as follows: wt i,St i = argmin wt i,St i:|St i|≤k,wt i≥0 ∥ ∑ l∈St i wt il∇θLl T(θt i) −∇θLT(θt i)∥+ λ wt i 2 (1) 4A PREPRINT - MARCH 17, 2022 The additional regularization term prevents assignment of very large weight values to data samples, thereby reducing the possibility of overﬁtting on a few data samples. A similar formulation for subset selection in the context of efﬁcient supervised learning was employed in a recent work called GRAD -MATCH [22]. The authors of the work [22] proved that the optimization problem given in Equation (1) is approximately submodular. Therefore, the above optimization problem can be solved using greedy algorithms with approximation guarantees [9, 36]. Similar to GRAD -MATCH, we use a greedy algorithm called orthogonal matching pursuit (OMP) to solve the above optimization problem. The goal of AUTOMATA is to accelerate the hyper-parameter tuning algorithm while preserving its original performance. Efﬁciency is, therefore, an essential factor that AUTOMATA considers even when selecting subsets. Due to this, we employ a faster per-batch subset selection introduced in the work [22] in our experiments, which is described in the following section. Per-Batch Subset Selection: Instead of selecting a subset of data points, one selects a subset of mini-batches by matching the weighted sum of mini-batch training gradients to the full training loss gradients. Therefore, one will have a subset of slected mini-batches and the associated mini-batch weights. One trains the model on the selected mini-batches by performing mini-batch gradient descent using the weighted mini-batch loss. Let us denote the batch size as B, and the total number of mini-batches as bN = N B, and the training set of mini-batches as DB. Let us denote the number of mini-batches that needs to be selected as bk = k B. Let us denote the subset of mini-batches that needs to be selected as SBi and denote the weights associated with mini-batches as wBi = {wBi1,wBi2 ···wBik}for the ith model conﬁguration. Let us denote the mini-batch gradients as ∇θLBi T (θi),··· ,∇θL BbN T (θi) be the mini-batch gradients for the ith model conﬁguration. Let us denote LB T(θi) =∑ i∈[1,bN] LBk T (θi) be the loss over the entire training set. The subset selection problem of mini-batches at time step tcan be written as follows: wt Bi,St Bi = argmin wt Bi,St Bi:|St Bi|≤bk,wt Bi≥0 ∥ ∑ l∈St Bi wt Bil∇θLBl T (θt i) −∇θLB T(θt i)∥+ λ wt Bi 2 (2) In the per-batch version, because the number of samples required for selection is bk is less than k, the number of greedy iterations required for data subset selection in OMP is reduced, resulting in a speedup of B×. A critical trade-off in using larger batch sizes is that in order to get better speedups, we must also sacriﬁce data subset selection performance. Therefore, it is recommended to use smaller batch sizes for subset selection to get a optimal trade-off between speedups and performance. In our experiments on Image datasets, we use a batch size of B = 20, and on text datasets, we use the batch size as a hyper-parameter with B ∈[16,32,64]. Apart from per-batch selection, we use model warm-starting to get more informative data subsets. Further, in our experiments, we use a regularization coefﬁcient of λ= 0. We give more details on warm-starting below. Warm-starting data selection: We warm-start each conﬁguration model by training on the entire training dataset for a few epochs similar to [22]. The warm-starting process enables the model to have informative loss gradients used for subset selection. To be more speciﬁc, the classiﬁer model is trained on the entire training data for Tw = κTk N epochs, where kis the coreset size, T is the total number of epochs, κis the fraction of warm start, and N is the size of the training dataset. We use a κvalue of 0 (i.e., no warm start) for experiments using Hyperband as scheduling algorithm, and a κvalue of 0.35 for experiments using ASHA. 2.4 Component-3: Hyper-parameter Scheduling Algorithm Hyper-parameter scheduling algorithms improve the overall efﬁciency of the hyper-parameter tuning by terminating some of the poor conﬁgurations runs early. In our experiments, we consider Hyperband [31], and ASHA [32], which are extensions of the Sequential Halving algorithm (SHA) [19] that uses aggressive early stopping to terminate poor conﬁguration runs and allocates an increasingly exponential amount of resources to the better performing conﬁgurations. SHA starts with nnumber of initial conﬁgurations, each assigned with a minimum resource amount r. The SHA algorithm uses a reduction factor η to reduce the number of conﬁgurations in each round by selecting the top 1 η th fraction of conﬁgurations while also increasing the resources allocated to these conﬁgurations by ηtimes each round. We discuss Hyperband and ASHA and the issues within SHA that each of them addresses in more detail in Appendix E. Detailed pseudocode of the AUTOMATA algorithm is provided in Appendix C due to space constraints in the main paper. We use the popular deep learning framework [40] for implementation of AUTOMATA framework, Ray-tune[35] for hyper-parameter search and scheduling algorithms, and CORDS [21] for subset selection strategies. 5A PREPRINT - MARCH 17, 2022 Full Random Automata Craig a) SST5(Random,HB)  b) SST5(TPE,HB)  c) SST5(Random,ASHA)  d) SST5(TPE,ASHA) e) TREC6(Random,HB)  f) TREC6(TPE,HB)  g) TREC6(Random,ASHA)  h) TREC6(TPE,ASHA) i) CIFAR10(Random,HB)  j) CIFAR10(TPE,HB)  k) CIFAR10(Random,ASHA)  l) CIFAR10(TPE,ASHA) m) CIFAR100(Random,HB)  n) CIFAR100(TPE,HB)  o) CIFAR100(Random,ASHA)  p) CIFAR100(TPE,ASHA) q) CONNECT-4(Random,HB)  r) CONNECT-4(TPE,HB)  s) CONNECT-4(Rand,ASHA)  t) CONNECT-4(TPE,ASHA) Figure 3: Comparison of performance of AUTOMATA with baselines(RANDOM , CRAIG , FULL ) for Hyper-parameter tuning. In sub-ﬁgures (a-t), we present speedup vs. relative test error (in %), compared to Full data tuning for different methods. On each scatter plot, smaller subsets appear on the right, and larger ones appear on the left. Results are shown for (a-d) SST5, (e-h) TREC6, (i-l) CIFAR10, (m-p) CIFAR100, and (q-t) CONNECT-4 datasets with different combinations of hyper-parameter search and scheduling algorithms. The scatter plots show that AUTOMATA achieves the best speedup-accuracy tradeoff in almost every case (bottom-right corner of each plot indicates the best speedup-accuracy tradeoff region). 6A PREPRINT - MARCH 17, 2022 3 Experiments In this section, we present the effectiveness and the efﬁciency of AUTOMATA framework for hyper-parameter tuning by evaluating AUTOMATA on datasets spanning text, image, and tabular domains. Further, to assess AUTOMATA ’s effectiveness across the spectrum of existing hyper-parameter search and scheduling algorithms, we conduct experiments using combinations of different search and scheduling algorithms. As discussed earlier, we employ Random Search [42], TPE [2] as representative hyper-parameter search algorithms, and Hyperband [ 31], ASHA [ 32] as representative hyper-parameter scheduling algorithms. However, we believe the takeaways would remain the same even with other approaches. We repeat each experiment ﬁve times on the text and tabular datasets, thrice on the image datasets, and report the mean accuracy and speedups in the plots. Below, we provide further details on datasets, baselines, models, and the hyper-parameter search space used for experiments. 3.1 Baselines Our experiments aim to demonstrate the consistency and efﬁciency of AUTOMATA more speciﬁcally, the effectiveness of AUTOMATA ’s gradient-based subset selection (GSS) for hyper-parameter tuning. As baselines, we replace the GSS subset selection strategy in AUTOMATA with different subset selection strategies, namely RANDOM (randomly sample a same sized subset as AUTOMATA from the training data), CRAIG [37] (a gradient-based subset selection proposed for efﬁcient supervised learning), and FULL (using the entire training data for model training during conﬁguration evaluation). For ease of notation, we refer to baselines by the names of corresponding subset selection strategies. Note that by CRAIG baseline, we mean the faster per-batch version of CRAIG [37] for subset selection shown [22] to be more efﬁcient than the original. In addition, for all methods, we do not use any warm-start for experiments with Hyperband and use a warm start of κ= 0.35 for experiments with ASHA. We give more details on the reason for using warm-start with ASHA and no warm-start with Hyperband in Appendix F.3. We perform experiments with different subset size fractions of 1%, 5%, 10%, and 30%. In our experiments, we compare our approach’s accuracy and efﬁciency (time/energy) with Full training, Per Batch CRAIG selection, and Random selection. 3.2 Datasets, Model Architecture, and Experimental Setup To demonstrate the effectiveness ofAUTOMATA for hyper-parameter tuning, we performed experiments on datasets spanning text, image and tabular domains. Text datasets include SST2 [46], SST5 [46], glue-SST2 [50], and TREC6 [34, 17]. Image datasets include CIFAR10 [27], CIFAR100 [27], and Street View House Numbers (SVHN) [38]. Tabular datasets include DNA, SATIMAGE, LETTER, and CONNECT-4 fromLIBSVM (a library for Support Vector Machines (SVMs)) [7]. We give more details on dataset sizes and splits in Appendix F.1. For the Text datasets, we use the LSTM model (from PyTorch) with trainable GloVe [41] embeddings of 300 dimension as input. For Image datasets, we use the ResNet18 [15] and ResNet50 [15] models. For Tabular datasets, we use a multi-layer perceptron with 2 hidden layers. Once the best hyper-parameter conﬁguration is found, we perform one more ﬁnal training of the model using the best conﬁguration on the entire dataset and report the achieved test accuracy. We use ﬁnal training for all methods except FULL since the models trained on small data subsets (especially with small subset fractions of 1%, 5%) during tuning do not achieve high test accuracies. We also include the ﬁnal training times while calculating the tuning times for a more fair comparison6 For text datasets, we train the LSTM model for 20 epochs while choosing subsets (except for FULL ) every 5 epochs. The hyper-parameter space includes learning rate, hidden size & number of layers of LSTM, batch size of training. Some experiments (with TPE as the search algorithm) use 27 conﬁgurations in the hyper-parameter space, while others use 54. More details on hyper-parameter search space for text datasets are given in Appendix F.2.1. For image datasets, we train the ResNet [15] model for 300 epochs while choosing subsets (except for FULL ) every 20 epochs. We use a Stochastic Gradient Descent (SGD) optimizer with momentum set to 0.9 and weight decay factor set to 0.0005. The hyper-parameter search space consists of a choice between the Momentum method and Nesterov Accelerated Gradient method, choice of learning rate scheduler and their corresponding parameters, and four different group-wise learning rates. We use 27 conﬁgurations in the hyper-parameter space for Image datasets. More details on hyper-parameter search space for image datasets are given in Appendix F.2.2. For tabular datasets, we train a multi-layer perceptron with 2 hidden layers for 200 epochs while choosing subsets every 10 epochs. The hyper-parameter search space consists of a choice between the SGD optimizer or Adam optimizer, choice of learning rate, choice of learning rate scheduler, the sizes of the two hidden layers and batch size for training. We use 27 conﬁgurations in the hyper-parameter space for Tabular datasets. More details on hyper-parameter search space for tabular datasets are provided in Appendix F.2.3. 6Note that with a 30% subset, ﬁnal training is not required as the models trained with 30% subsets achieve similar accuracy to full data training. However, for the sake of consistency, we useﬁnal training with 30% subsets as well. 7A PREPRINT - MARCH 17, 2022 3.3 Hyper-parameter Tuning Results Results comparing the accuracy vs. efﬁciency tradeoff of different subset selection strategies for hyper-parameter tuning are shown in Figure 3. Performance is compared for different sizes of subsets of training data: 1%, 5%, 10%, and 30% along with four possible combinations of search algorithm (Random or TPE) and scheduling algorithm (ASHA or Hyperband). Text datasets results: Sub-ﬁgures(3a, 3b, 3c, 3d) show the plots of relative test error vs. speed ups, both w.r.tfull data tuning for SST5 dataset with different combinations of search and scheduling methods. Similarly, in sub-ﬁgures(3e, 3f, 3g, 3h) we present the plots of relative test error vs. speed ups for TREC6 dataset. From the results, we observe that AUTOMATA achieves best speed up vs. accuracy tradeoff and consistently gives better performance even with small subset sizes unlike other baselines like RANDOM , CRAIG . In particular, AUTOMATA achieves a speedup of 9.8×and 7.35×with a performance loss of 2.8% and a performance gain of 0.9% respectively on the SST5 dataset with TPE and Hyperband. Additionally, AUTOMATA achieves a speedup of around 3.15×, 2.68×with a performance gain of 3.4%, 4.6% respectively for the TREC6 dataset with TPE and ASHA. Image datasets results: Sub-ﬁgures(3i, 3j, 3k, 3l) show the plots of relative test error vs. speed ups, both w.r.tfull data tuning for CIFAR10 dataset with different combinations of search and scheduling methods. Similarly, sub-ﬁgures (3m, 3n, 3o, 3p) show the plots of relative test error vs. speed ups on CIFAR100. The results show that AUTOMATA achieves the best speed up vs. accuracy tradeoff consistently compared to other baselines. More speciﬁcally, AUTOMATA achieves a speedup of around 15×, 8.7× with a performance loss of 0.65%, 0.14% respectively on the CIFAR10 dataset with Random and Hyperband. Further, AUTOMATA achieves a speedup of around 3.7×, 2.3×with a performance gain of 1%, 2% for CIFAR100 dataset with TPE and ASHA. Tabular datasets results: Sub-ﬁgures(3q, 3r, 3s, 3t) show the plots of relative test errorvs. speed ups for the CONNECT-4 dataset. AUTOMATA consistently achieved better speedup vs. accuracy tradeoff compared to other baselines on CONNECT-4 as well. Owing to space constraints, we provide additional results showing the accuracy vs. efﬁciency tradeoff on additional text, image, and tabular datasets in the Appendix F.4. It is important to note that AUTOMATA obtains better speedups when used for hyper-parameter tuning on larger datasets and larger models (in terms of parameters). Apart from the speedups achieved by AUTOMATA , we show in Appendix F.5 that it also achieves similar reductions of energy consumption and CO2 emissions, thereby making it more environmentally friendly. 4 Conclusion, Limitations, and Broader Impact We introduce AUTOMATA , an efﬁcient hyper-parameter tuning framework that uses intelligent subset selection for model training for faster conﬁguration evaluations. Further, we perform extensive experiments showing the effectiveness of AUTOMATA for Hyper-parameter tuning. In particular, it achieves speedups of around 10×- 15×using Hyperband as scheduler and speedups of around 3×even with a more efﬁcient ASHA scheduler. AUTOMATA signiﬁcantly decreases CO2 emissions by making hyper-parameter tuning fast and energy-efﬁcient, in turn reducing environmental impact of such hyper-parameter tuning on society at large. We hope that the AUTOMATA framework will popularize the trend of using subset selection for hyper-parameter tuning and encourage further research on efﬁcient subset selection approaches for faster hyper-parameter tuning, helping us move closer to the goal of Green AI [43]. One of the limitations of AUTOMATA is that in scenarios in which no performance loss is desired, we do not know the minimum subset size to improve speed and, therefore, rely on larger subset sizes such as 10%, 30%. In the future, we consider adaptively changing subset sizes based on model performance for each conﬁguration to remove the dependency on subset size. References [1] L. Barrault, O. Bojar, M. R. Costa-jussà, C. Federmann, M. Fishel, Y . Graham, B. Haddow, M. Huck, P. Koehn, S. Malmasi, C. Monz, M. Müller, S. Pal, M. Post, and M. Zampieri. Findings of the 2019 conference on machine translation (WMT19). In Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1), pages 1–61, Florence, Italy, Aug. 2019. Association for Computational Linguistics. [2] J. Bergstra, R. Bardenet, Y . Bengio, and B. Kégl. Algorithms for hyper-parameter optimization. In J. Shawe-Taylor, R. Zemel, P. Bartlett, F. Pereira, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems, volume 24. Curran Associates, Inc., 2011. [3] J. Bergstra, R. Bardenet, Y . Bengio, and B. Kégl. Algorithms for hyper-parameter optimization. In25th annual conference on neural information processing systems (NIPS 2011), volume 24. Neural Information Processing Systems Foundation, 2011. [4] J. Bergstra and Y . Bengio. Random search for hyper-parameter optimization. Journal of Machine Learning Research, 13(10):281–305, 2012. [5] J. Bergstra, D. Yamins, and D. Cox. Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures. In S. Dasgupta and D. McAllester, editors, Proceedings of the 30th 8A PREPRINT - MARCH 17, 2022 International Conference on Machine Learning, volume 28 of Proceedings of Machine Learning Research, pages 115–123, Atlanta, Georgia, USA, 17–19 Jun 2013. PMLR. [6] T. Campbell and T. Broderick. Bayesian coreset construction via greedy iterative geodesic ascent. InInternational Conference on Machine Learning, pages 698–706, 2018. [7] C.-C. Chang and C.-J. Lin. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2:27:1–27:27, 2011. Software available at http://www.csie.ntu.edu.tw/~cjlin/ libsvm. [8] K. L. Clarkson. Coresets, sparse greedy approximation, and the frank-wolfe algorithm. ACM Transactions on Algorithms (TALG), 6(4):1–30, 2010. [9] A. Das and D. Kempe. Submodular meets spectral: Greedy algorithms for subset selection, sparse approximation and dictionary selection. arXiv preprint arXiv:1102.3975, 2011. [10] T. Domhan, J. T. Springenberg, and F. Hutter. Speeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves. In Proceedings of the 24th International Conference on Artiﬁcial Intelligence, IJCAI’15, page 3460–3468. AAAI Press, 2015. [11] D. Feldman. Core-sets: Updated survey. In Sampling Techniques for Supervised or Unsupervised Tasks, pages 23–44. Springer, 2020. [12] S. Fujishige. Submodular functions and optimization. Elsevier, 2005. [13] S. Har-Peled and S. Mazumdar. On coresets for k-means and k-median clustering. InProceedings of the thirty-sixth annual ACM symposium on Theory of computing, pages 291–300, 2004. [14] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectiﬁers: Surpassing human-level performance on imagenet classiﬁcation. 2015 IEEE International Conference on Computer Vision (ICCV), pages 1026–1034, 2015. [15] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770–778, 2016. [16] J. R. Hershey, S. J. Rennie, P. A. Olsen, and T. T. Kristjansson. Super-human multi-talker speech recognition: A graphical modeling approach. Comput. Speech Lang., 24(1):45–66, Jan. 2010. [17] E. Hovy, L. Gerber, U. Hermjakob, C.-Y . Lin, and D. Ravichandran. Toward semantics-based answer pinpointing. In Proceedings of the First International Conference on Human Language Technology Research, 2001. [18] F. Hutter, H. H. Hoos, and K. Leyton-Brown. Sequential model-based optimization for general algorithm conﬁguration. In C. A. C. Coello, editor,Learning and Intelligent Optimization, pages 507–523, Berlin, Heidelberg, 2011. Springer Berlin Heidelberg. [19] K. Jamieson and A. Talwalkar. Non-stochastic best arm identiﬁcation and hyperparameter optimization. In A. Gretton and C. C. Robert, editors, Proceedings of the 19th International Conference on Artiﬁcial Intelligence and Statistics, volume 51 of Proceedings of Machine Learning Research, pages 240–248, Cadiz, Spain, 09–11 May 2016. PMLR. [20] V . Kaushal, R. Iyer, S. Kothawade, R. Mahadev, K. Doctor, and G. Ramakrishnan. Learning from less data: A uniﬁed data subset selection and active learning framework for computer vision. In 2019 IEEE Winter Conference on Applications of Computer Vision (WACV), pages 1289–1299. IEEE, 2019. [21] K. Killamsetty, D. Bhat, G. Ramakrishnan, and R. Iyer. CORDS: COResets and Data Subset selection for Efﬁcient Learning, March 2021. [22] K. Killamsetty, D. Sivasubramanian, G. Ramakrishnan, A. De, and R. Iyer. Grad-match: Gradient matching based data subset selection for efﬁcient deep model training. In M. Meila and T. Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 5464–5474. PMLR, 18–24 Jul 2021. [23] K. Killamsetty, D. Sivasubramanian, G. Ramakrishnan, and R. Iyer. Glister: Generalization based data subset selection for efﬁcient and robust learning. Proceedings of the AAAI Conference on Artiﬁcial Intelligence , 35(9):8110–8118, May 2021. [24] K. Killamsetty, X. Zhao, F. Chen, and R. K. Iyer. RETRIEVE: Coreset selection for efﬁcient and robust semi- supervised learning. In A. Beygelzimer, Y . Dauphin, P. Liang, and J. W. Vaughan, editors,Advances in Neural Information Processing Systems, 2021. [25] K. Kirchhoff and J. Bilmes. Submodularity for data selection in machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 131–141, 2014. 9A PREPRINT - MARCH 17, 2022 [26] A. Klein, S. Falkner, S. Bartels, P. Hennig, and F. Hutter. Fast Bayesian Optimization of Machine Learning Hyperparameters on Large Datasets. In A. Singh and J. Zhu, editors, Proceedings of the 20th International Conference on Artiﬁcial Intelligence and Statistics, volume 54 of Proceedings of Machine Learning Research, pages 528–536. PMLR, 20–22 Apr 2017. [27] A. Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009. [28] T. Krueger, D. Panknin, and M. Braun. Fast cross-validation via sequential testing. J. Mach. Learn. Res. , 16(1):1103–1155, jan 2015. [29] A. Lacoste, A. Luccioni, V . Schmidt, and T. Dandres. Quantifying the carbon emissions of machine learning. arXiv preprint arXiv:1910.09700, 2019. [30] E. LeDell and S. Poirier. H2o automl: Scalable automatic machine learning. In Proceedings of the AutoML Workshop at ICML, volume 2020, 2020. [31] L. Li, K. Jamieson, G. DeSalvo, A. Rostamizadeh, and A. Talwalkar. Hyperband: A novel bandit-based approach to hyperparameter optimization. J. Mach. Learn. Res., 18(1):6765–6816, jan 2017. [32] L. Li, K. Jamieson, A. Rostamizadeh, E. Gonina, J. Ben-tzur, M. Hardt, B. Recht, and A. Talwalkar. A system for massively parallel hyperparameter tuning. In I. Dhillon, D. Papailiopoulos, and V . Sze, editors,Proceedings of Machine Learning and Systems, volume 2, pages 230–246, 2020. [33] L. Li, K. Jamieson, A. Rostamizadeh, E. Gonina, M. Hardt, B. Recht, and A. Talwalkar. A system for massively parallel hyperparameter tuning. arXiv preprint arXiv:1810.05934, 2018. [34] X. Li and D. Roth. Learning question classiﬁers. In COLING 2002: The 19th International Conference on Computational Linguistics, 2002. [35] R. Liaw, E. Liang, R. Nishihara, P. Moritz, J. E. Gonzalez, and I. Stoica. Tune: A research platform for distributed model selection and training. arXiv preprint arXiv:1807.05118, 2018. [36] M. Minoux. Accelerated greedy algorithms for maximizing submodular set functions. In Optimization techniques, pages 234–243. Springer, 1978. [37] B. Mirzasoleiman, J. Bilmes, and J. Leskovec. Coresets for data-efﬁcient training of machine learning models, 2020. [38] Y . Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y . Ng. Reading digits in natural images with unsupervised feature learning. 2011. [39] T. Nickson, M. A. Osborne, S. Reece, and S. J. Roberts. Automated machine learning on big data using stochastic algorithm tuning, 2014. [40] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. Pytorch: An imperative style, high-performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 8024–8035. Curran Associates, Inc., 2019. [41] J. Pennington, R. Socher, and C. Manning. GloVe: Global vectors for word representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543, Doha, Qatar, Oct. 2014. Association for Computational Linguistics. [42] N. Pinto, D. Doukhan, J. J. DiCarlo, and D. D. Cox. A high-throughput screening approach to discovering good forms of biologically inspired visual representation. PLoS Computational Biology, 5, 2009. [43] R. Schwartz, J. Dodge, N. Smith, and O. Etzioni. Green ai. Communications of the ACM, 63:54 – 63, 2020. [44] L. N. Smith. A disciplined approach to neural network hyper-parameters: Part 1–learning rate, batch size, momentum, and weight decay. arXiv preprint arXiv:1803.09820, 2018. [45] J. Snoek, O. Rippel, K. Swersky, R. Kiros, N. Satish, N. Sundaram, M. M. A. Patwary, P. Prabhat, and R. P. Adams. Scalable bayesian optimization using deep neural networks. In Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37, ICML’15, page 2171–2180. JMLR.org, 2015. [46] R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Ng, and C. Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631–1642, Seattle, Washington, USA, Oct. 2013. Association for Computational Linguistics. [47] E. Strubell, A. Ganesh, and A. McCallum. Energy and policy considerations for deep learning in nlp. arXiv preprint arXiv:1906.02243, 2019. 10A PREPRINT - MARCH 17, 2022 [48] K. Swersky, J. Snoek, and R. P. Adams. Multi-task bayesian optimization. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, editors,Advances in Neural Information Processing Systems, volume 26. Curran Associates, Inc., 2013. [49] K. Swersky, J. Snoek, and R. P. Adams. Freeze-thaw bayesian optimization. CoRR, abs/1406.3896, 2014. [50] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. 2019. In the Proceedings of ICLR. [51] K. Wei, Y . Liu, K. Kirchhoff, C. Bartels, and J. Bilmes. Submodular subset selection for large-scale speech training data. In 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 3311–3315. IEEE, 2014. [52] K. Wei, Y . Liu, K. Kirchhoff, and J. Bilmes. Unsupervised submodular subset selection for speech data. In2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 4107–4111. IEEE, 2014. [53] T. Yu and H. Zhu. Hyper-parameter optimization: A review of algorithms and applications. arXiv preprint arXiv:2003.05689, 2020. 11A PREPRINT - MARCH 17, 2022 Supplementary Material A Code The code of AUTOMATA is available at the following link: https://github.com/decile-team/cords. B Licenses We release the code repository ofAUTOMATA with MIT license, and it is available for everybody to use freely. We use the popular deep learning framework [40] for implementation of AUTOMATA framework, Ray-tune[35] for hyper-parameter search and scheduling algorithms, and CORDS [21] for subset selection strategies. As far as the datasets are considered, we use SST2 [46], SST5 [46], glue-SST2 [50], TREC6 [34, 17], CIFAR10 [27], SVHN [38], CIFAR100 [27], and DNA, SATIMAGE, LETTER, CONNECT-4 fromLIBSVM (a library for Support Vector Machines (SVMs)) [7] datasets. CIFAR10, CIFAR100 datasets are released with an MIT license. SVHN dataset is released with a CC0:Public Domain license. Furthermore, all the datasets used in this work are publicly available. In addition, the datasets used do not contain any personally identiﬁable information. 12A PREPRINT - MARCH 17, 2022 C A UTOMATA Algorithm Pseudocode We give the pseudo code of AUTOMATA algorithm in Algorithm 1. Algorithm 1: AUTOMATA Algorithm Input: Hyper-parameter scheduler Algorithm: scheduler , Hyper-parameter search Algorithm: search , No. of conﬁguration evaluations: n, Hyper-parameter search space: H, Training dataset: D, Validation dataset: V, Total no of epochs: T, Epoch interval for subset selection: R, Size of the coreset: k, Reg. Coefﬁcient: λ, Learning rates: {αt}t=T−1 t=0 , Tolerance: ϵ Generate nconﬁgurations by calling the search algorithm H = {h1,h2,··· ,hn}= search (H,n) Randomly initialize each conﬁguration model parameters h1.θ= h2.θ= ··· = hn.θ= θ Set h1.t= h2.t= ··· = hn.t= 0; Set h1.eval= h2.eval= ··· = hn.eval= 0; Assign the initial resources(i.e., in our case training epochs) using the scheduler for all initialized conﬁgurations {hi.r}i=n i=1 = scheduler (H,T) repeat ***Evaluate all remaining conﬁgurations*** for each conﬁguration numbered iin Hdo ***Train conﬁguration hiusing informative data subsets for hi.repochs and evaluate on validation set*** hi.eval,hi.theta= subset-conﬁg-evaluation (D,V,hi.theta,hi.r,R,k,λ, {αt} t=hi.r t=0 ,ϵ) hi.t= hi.t+ hi.r ***Assign resources again based on evaluation performance*** {hi.r}i=n i=1 = scheduler (H,T) until until h1.r== 0 & h2.r== 0 & ··· hn.r== 0 ***Get the best performing hyper-parameters based on ﬁnal conﬁguration evaluations*** finalconfig = argmax hi.config [hi.eval]n i=1 ***Perform ﬁnal training using the best hyper-parameter conﬁgurations*** θfinal = ﬁnaltrain (θ,D,finalconfig,T ) return θfinal Algorithm 2: subset-conﬁg-evaluation Input: Training dataset: D, Validation dataset: V, Initial model parameters: θ0, Total no of epochs: T, Epoch interval for subset selection: R, Size of the coreset: k, Reg. Coefﬁcient: λ, Learning rates: {αt}t=T−1 t=0 , Tolerance: ϵ Set t= 0; Randomly initialize coreset S0 ⊆D : |S0|= k; repeat if (t%R== 0) ∧(t> 0) then St= OMP(D,θt,λ,α t,k,ϵ ) else St = St−1 Compute batches Db = ((xb,yb); b∈(1 ··· B)) from D Compute batches Stb = ((xb); b∈(1 ··· B)) from S *** Mini-batch SGD *** Set θt0 = θt for b= 1 to Bdo Compute mask mton Stbfrom current model parameters θt(b−1) θtb = θt(b−1) −αt∇θLS(Db,θt) −αtλt ∑ j∈Stb mjt∇θlu(xj,θt(b−1)) Set θt+1 = θtB t= t+ 1 until until t≥T *** Evaluate trained model on validation set *** eval= evaluate (θT,V) return eval,θT Algorithm 3: OMP Input: Training loss LT, current parameters: θ, regularization coefﬁcient: λ, subset size: k, tolerance: ϵ Initialize S= ∅ r←∇w(∥∑ l∈Sw∇θLl T(θ) −∇θLT(θ)∥+ λ∥w∥2 |)w=0 repeat e= argmaxj|rj| S←S∪{ e} w ←argminw(∥∑ l∈Sw∇θLl T(θ) −∇θLT(θ)∥+ λ∥w∥2) r←∇w(∥∑ l∈Sw∇θLl T(θ) −∇θLT(θ)∥+ λ∥w∥2 |) until until |S|≤ kand ∥∑ l∈Sw∇θLl T(θ) −∇θLT(θ)∥+ λ∥w∥2 ≥ϵ return S,w D More details on Hyper-parameter Search Algorithms We give a brief overview of few representative hyper-parameter search algorithms, such as TPE [ 2] and Random Search [42] which we used in our experiments. As discussed earlier, given a hyper-parameter search space, hyper- parameter search algorithms provide a set of conﬁgurations that need to be evaluated. A naive way of performing the 13A PREPRINT - MARCH 17, 2022 hyper-parameter search is Grid Search, which deﬁnes the search space as a grid and exhaustively evaluates each grid conﬁguration. However, Grid Search is a time-consuming process, meaning that thousands to millions of conﬁgurations would need to be evaluated if the hyper-parameter space is large. In order to ﬁnd optimal hyper-parameter settings quickly, Bayesian optimization-based hyper-parameter search algorithms have been developed. To investigate the effectiveness of AUTOMATA across the spectrum of search algorithms, we used the Random Search method and the Bayesian optimization-based TPE method (described below) as representative hyper-parameter search algorithms. D.1 Random Search In random search [42], hyper-parameter conﬁgurations are selected at random and evaluated to discover the optimal conﬁguration among those chosen. As well as being more efﬁcient than a grid search since it does not evaluate all possible conﬁgurations exhaustively, random search also reduces overﬁtting [4]. D.2 Tree Parzen Structured Estimator (TPE) TPE [2] is a sequential model-based optimization (SMBO) approach that sequentially constructs a probability model to approximate the performance of hyper-parameters based on historical conﬁguration evaluations and then subsequently uses the model to select new conﬁgurations. TPE models the likelihood function P(D|f) and the prior over the function space P(f) using the kernel density estimation. TPE algorithm sorts the collected observations by the function evaluation value, typically validation set performance, and divides them into two groups based on some quantile. The ﬁrst group x1 contains best-performing observations, and the second group x2 contains all other observations. Then TPE models two different densities i(x1) and g(x2) based on the observations from the respective groups using kernel density estimation. Finally, TPE selects the subset observations that need to be evaluated by sampling from the distribution that models the maximum expected improvement, i.e., E[i(x)/g(x)]. E More details on Hyper-parameter Scheduling Algorithms We give a brief overview of some representative hyper-parameter scheduling algorithms, such as HyperBand [31] and ASHA [32] which we used in our experiments. As discussed earlier, hyper-parameter scheduling algorithms improve the overall efﬁciency of the hyper-parameter tuning by terminating some of the poor conﬁgurations runs early. In our experiments, we consider Hyperband, and ASHA, which are extensions of the Sequential Halving algorithm (SHA) [19] that uses aggressive early stopping to terminate poor conﬁguration runs and allocates an increasingly exponential amount of resources to the better performing conﬁgurations. SHA starts with nnumber of initial conﬁgurations, each assigned with a minimum resource amount r. The SHA algorithm uses a reduction factor η to reduce the number of conﬁgurations each round by selecting the top 1 η th fraction of conﬁgurations while also increasing the resources allocated to these conﬁgurations by ηtimes each round. Following, we will discuss Hyperband and ASHA and the issues within SHA that each of them addresses. E.1 HyperBand One of the issues with SHA is that its performance largely depends on the initial number nof conﬁgurations. Hyper- band [31] addresses this issue by performing a grid search over various feasible values of n. Further, each value of nis associated with a minimum resource rallocated to all conﬁgurations before some are terminated; larger values of nare assigned smaller rand hence more aggressive early-stopping. On the whole, in Hyperband [31] for different values of nand r, the SHA algorithm is run until completion. E.2 ASHA: One of the other issues with SHA is that the algorithm is sequential and has to wait for all the processes (assigned with an equal amount of resources) at a particular bracket to be completed before choosing the conﬁgurations to be selected for subsequent runs. Hence, due to the sequential nature of SHA, some GPU/CPU resources (with no processes running) cannot be effectively utilized in the distributed training setting, thereby taking more time for tuning. By contrast, ASHA [32] is an asynchronous variant of SHA and addresses the sequential issue of SHA by promoting a conﬁguration to the next rung as long as there are GPU or CPU resources available. If no resources appear to be promotable, it randomly adds a new conﬁguration to the base rung. 14A PREPRINT - MARCH 17, 2022 F More Experimental Details and Additional Results We performed experiments on a mix of RTX 1080, RTX 2080, and V100 GPU servers containing 2-8 GPUs. To be fair in timing computation, we ran AUTOMATA and all other baselines for a particular setting on the same GPU server. F.1 Additional Datasets Details F.1.1 Text Datasets We performed experiments on SST2 [46], SST5 [46], glue-SST2 [50], and TREC6 [34, 17] text datasets. SST2 [46] and glue-SST2 [50] dataset classify the sentiment of the sentence (movie reviews) as negative or positive. Whereas SST5 [46] classify sentiment of sentence as negative, somewhat negative, neutral, somewhat positive or positive. TREC6 [34, 17] is a dataset for question classiﬁcation consisting of open-domain, fact-based questions divided into broad semantic categories(ABBR - Abbreviation, DESC - Description and abstract concepts, ENTY - Entities, HUM - Human beings, LOC - Locations, NYM - Numeric values). The train, text and validation splits for SST2 [ 46] and SST5 [46] are used from the source itself while the validation data for TREC6 [34, 17] is obtained using 10% of the train data. The train and validation data for glue-SST2 [50] is used from source itself. In Table 1, we summarize the number classes, and number of instances in each split in the text datasets. Dataset #Classes #Train #Validation #Test SST2 2 8544 1101 2210 SST5 5 8544 1101 2210 glue-SST2 2 63982 872 3367 TREC6 6 4907 545 500 Table 1: Number of classes, Number of instances in Train, Validation and Test split in Text datasets F.1.2 Vision Datasets We performed experiments on CIFAR10 [27], CIFAR100 [27], and SVHN [38] vision datasets. The CIFAR-10 [27] dataset contains 60,000 colored images of size 32×32 divided into ten classes, each with 6000 images. CIFAR100 [27] is also similar but that it has 600 images per class and 100 classes. Both CIFAR10 [ 27] and CIFAR100 [27] have 50,000 training samples and 10,000 test samples distributed equally across all classes. SVHN [38] is obtained from house numbers in Google Street View images and has 10 classes, one for each digit. The colored images of size 32×32 are centered around a single digit with some distracting characters on the side. SVHN [38] has 73,257 training digits, 26,032 testing digits. For all 3 datasets, 10% of the training data is used for validation. In Table 2, we summarize the number classes, and number of instances in each split in the image datasets. Dataset #Classes #Train #Validation #Test CIFAR10 10 45000 5000 10000 CIFAR100 100 45000 5000 10000 SVHN 10 65932 7325 26032 Table 2: Number of classes, Number of instances in Train, Validation and Test split in Image datasets F.1.3 Tabular Datasets We performed experiments on the following tabular datasetsdna, letter, connect-4, and satimage from LIBSVM (a library for Support Vector Machines (SVMs)) [7]. Name #Classes #Train #Validation #Test #Features dna 3 1,400 600 1,186 180 satimage 6 3,104 1,331 2,000 36 letter 26 10,500 4,500 5,000 16 connect_4 3 67,557 - - 126 Table 3: Number of classes, Number of instances in Train, Validation and Test split in Tabular datasets 15A PREPRINT - MARCH 17, 2022 A brief description of the tabular datasets can be found in Table 3. For datasets without explicit validation and test datasets, 10% and 20% samples from the training set are used as validation and test datasets, respectively. F.2 Additional Experimental Details For tuning with FULL datasets, the entire dataset is used to train the model during hyper-parameter tuning. But when the AUTOMATA (or CRAIG ) is used, only a fraction of the dataset is used to train various models during tuning. Similar is the case with Random subset selection approach but the subsets are chosen at RANDOM . Note that subset selection techniques used are adaptive in nature, which mean that they chose subset every few epochs for the model to train on for coming few epochs. F.2.1 Details of Text Experiments The hyper-parameter space for experiments on text datasets include learning rate, hidden size & number of layers of LSTM and batch size of training. Some experiments (with TPE search algorithm) where the best conﬁguration among 27 conﬁgurations are found, the hyper-parameter space is learning rate: [0.001,0.1], LSTM hidden size: {64,128,256}, batch size: {16,32,64}. While the rest of the experiments where the best conﬁguration among 54 conﬁgurations are found, the hyper-parameter space is learning rate: [0.001,0.1], LSTM hidden size: {64,128,256}, number of layers in LSTM: {1, 2}, batch size: {16,32,64}. F.2.2 Details of Image Experiments The hyper-parameter search space for tuning experiments on image datasets include a choice between Momentum method and Nesterov Accelerated Gradient method, choice of learning rate scheduler and their corresponding parameters, and four different group-wise learning rates, lr1 for layers of the ﬁrst group, lr2 for layers of intermediate groups, lr3 for layers of the last group of ResNet model, and lr4 for the ﬁnal fully connected layer. For learning rate scheduler, we change the learning rates during training using either a cosine annealing schedule or decay it linearly by γafter every 20 epochs. Best conﬁguration for most experiments is selected from 27 conﬁgurations where the hyper-parameter space is lr1: [0.001, 0.01], lr2: [0.001, 0.01], lr3: [0.001, 0.01], lr4: [0.001, 0.01], Nesterov: {True, False}, learning rate scheduler: {Cosine Annealing, Linear Decay}, γ: [0.05, 0.5]. F.2.3 Details of Tabular Experiments The hyper-parameter search space consists of a choice between the Stochastic Gradient Descent(SGD) optimizer or Adam optimizer, choice of learning rate lr, choice of learning rate scheduler, the sizes of the two hidden layers h1 and h2 and batch size for training. For learning rate scheduler, we either don’t use a learning rate scheduler or change the learning rates during training using a cosine annealing schedule or decay it linearly by 0.05 after every 20 epochs. Best conﬁguration for most experiments is selected from 27 conﬁgurations where the hyper-parameter space is lr: [0.001, 0.01], Optimizer: {Adam, SGD}, learning rate scheduler: {None, Cosine Annealing, Linear Decay}, h1: {150, 200, 250, 300}, h2: {150, 200, 250, 300} and batch size: {16,32,64}. F.3 Use of Warm-start for subset selection We use warm-starting with ASHA as a scheduler because the initial bracket occurs early (i.e., att= 1) with ASHA; this implies that some of the initial conﬁguration evaluations are discarded made just after training for one epoch. The training of such conﬁgurations on small data subsets may not be sufﬁcient to make a sound decision about better- performing conﬁgurations in these scenarios. As a solution, we use warm-starting with ASHA so that all conﬁgurations are trained on the entire data for an initial few epochs. With Hyperband, the brackets do not occur very early during training, so no warm-up is necessary. F.4 More Hyper-parameter Tuning Results We present more hyper-parameter tuning results of AUTOMATA on additional text, image, and tabular datasets in Figures 4,5,6. From the results, it is evident that AUTOMATA achieves best speedup vs. accuracy tradeoff in almost all of the cases. 16A PREPRINT - MARCH 17, 2022 Full Random Automata Craig (a) SST2(Random,HB)  (b) SST2(TPE,HB)  (c) SST2(Random,ASHA)  (d) SST2(TPE,ASHA) (e) glue-SST2(Random,HB)  (f) glue-SST2(TPE,HB)  (g) glue-SST2(Random,ASHA)  (h) glue-SST2(TPE,ASHA) (i) SST5(Random,HB)  (j) SST5(TPE,HB)  (k) SST5(Random,ASHA)  (l) SST5(TPE,ASHA) (m) TREC6(Random,HB)  (n) TREC6(TPE,HB)  (o) TREC6(Random,ASHA)  (p) TREC6(TPE,ASHA) Figure 4: Tuning Results on Text Datasets: Comparison of performance of AUTOMATA with baselines(RANDOM , CRAIG , FULL ) for Hyper-parameter tuning. In sub-ﬁgures (a-p), we present speedup vs. relative test error (in %), compared to Full data tuning for different methods. On each scatter plot, smaller subsets appear on the right, and larger ones appear on the left. Results are shown for (a-d) SST2, (e-h) glue-SST2, (i-l) SST5, (m-p) TREC6 datasets with different combinations of hyper-parameter search and scheduling algorithms. The scatter plots show that AUTOMATA achieves the best speedup-accuracy tradeoff in almost every case (bottom-right corner of each plot indicates the best speedup-accuracy tradeoff region). F.5 CO2 Emissions and Energy Consumption Results Sub-ﬁgures 7a,7b,7c,7d shows the energy efﬁciency plot of AUTOMATA on CIFAR100 dataset for 1%, 5%, 10%, 30% subset fractions. For calculating the energy consumed by the GPU/CPU cores, we use pyJoules 7. From the plot, it is evident that AUTOMATA is more energy efﬁcient compared to the other baselines and full data tuning. Sub- ﬁgures 7e,7f,7g,7h shows the plot of relative error vs CO2 emissions efﬁciency, both w.r.t full training. CO2 emissions were estimated based on the total compute time using the Machine Learning Impact calculator presented in [29]. From the results, it is evident that AUTOMATA achieved the best energy vs. accuracy tradeoff and is environmentally friendly based on CO2 emissions compared to other baselines (including CRAIG and RANDOM ). 7https://pypi.org/project/pyJoules/. 17A PREPRINT - MARCH 17, 2022 Full Random Automata Craig (a) CIFAR10(Random,HB)  (b) CIFAR10(TPE,HB)  (c) CIFAR10(Random,ASHA)  (d) CIFAR10(TPE,ASHA) (e) CIFAR100(Random,HB)  (f) CIFAR100(TPE,HB)  (g) CIFAR100(Random,ASHA)  (h) CIFAR100(TPE,ASHA) (i) SVHN(Random,HB)  (j) SVHN(TPE,HB)  (k) SVHN(Random,ASHA)  (l) SVHN(TPE,ASHA) Figure 5: Tuning Results on Image Datasets: Comparison of performance of AUTOMATA with baselines(RANDOM , CRAIG , FULL ) for Hyper-parameter tuning. In sub-ﬁgures (a-l), we present speedup vs. relative test error (in %), compared to Full data tuning for different methods. On each scatter plot, smaller subsets appear on the right, and larger ones appear on the left. Results are shown for (a-d) CIFAR10, (e-h) CIFAR100, (i-l) SVHN datasets with different combinations of hyper-parameter search and scheduling algorithms. The scatter plots show that AUTOMATA achieves the best speedup-accuracy tradeoff in almost every case (bottom-right corner of each plot indicates the best speedup-accuracy tradeoff region). 18A PREPRINT - MARCH 17, 2022 Full Random Automata Craig (a) DNA(Random,HB)  (b) DNA(TPE,HB)  (c) DNA(Random,ASHA)  (d) DNA(TPE,ASHA) (e) SATIMAGE(Random,HB)  (f) SATIMAGE(TPE,HB)  (g) SATIMAGE(Random,ASHA)  (h) SATIMAGE(TPE,ASHA) (i) LETTER(Random,HB)  (j) LETTER(TPE,HB)  (k) LETTER(Random,ASHA)  (l) LETTER(TPE,ASHA) (m) CONNECT-4(Random,HB)  (n) CONNECT-4(TPE,HB)  (o) CONNECT-4(Random,ASHA)  (p) CONNECT-4(TPE,ASHA) Figure 6: Tuning Results on Tabular Datasets: Comparison of performance of AUTOMATA with baselines(RANDOM , CRAIG , FULL ) for Hyper-parameter tuning. In sub-ﬁgures (a-p), we present speedup vs. relative test error (in %), compared to Full data tuning for different methods. On each scatter plot, smaller subsets appear on the right, and larger ones appear on the left. Results are shown for (a-d) DNA, (e-h) SATIMAGE, (i-l) LETTER, (m-p) CONNECT-4 datasets with different combinations of hyper-parameter search and scheduling algorithms. The scatter plots show that AUTOMATA achieves the best speedup-accuracy tradeoff in almost every case (bottom-right corner of each plot indicates the best speedup-accuracy tradeoff region). 19A PREPRINT - MARCH 17, 2022 Full Random Automata Craig (a) CIFAR100(Random,HB)  (b) CIFAR100(TPE,HB)  (c) CIFAR100(Random,ASHA)  (d) CIFAR100(TPE,ASHA) (e) SVHN(Random,HB)  (f) SVHN(TPE,HB)  (g) SVHN(Random,ASHA)  (h) SVHN(TPE,ASHA) Figure 7: Comparison of performance of AUTOMATA with baselines(RANDOM , CRAIG , FULL ) for Hyper-parameter tuning. In sub-ﬁgures (a-d), we present energy ratio vs. relative test error (in %), compared to Full data tuning for different methods on CIFAR100 dataset. In sub-ﬁgures (e-h), we present co2 emissions ratio vs. relative test error (in %), compared to Full data tuning for different methods on SVHN dataset. On each scatter plot, smaller subsets appear on the right, and larger ones appear on the left. The scatter plots show that AUTOMATA achieves the best energy savings and CO2 reductions, thereby achieving the best efﬁciency vs. performance tradeoff in almost every case. (Bottom-right corner of each plot indicates the best efﬁciency vs. performance tradeoff region). 20",
      "meta_data": {
        "arxiv_id": "2203.08212v1",
        "authors": [
          "Krishnateja Killamsetty",
          "Guttu Sai Abhishek",
          "Aakriti",
          "Alexandre V. Evfimievski",
          "Lucian Popa",
          "Ganesh Ramakrishnan",
          "Rishabh Iyer"
        ],
        "published_date": "2022-03-15T19:25:01Z",
        "pdf_url": "https://arxiv.org/pdf/2203.08212v1.pdf",
        "github_url": "https://github.com/decile-team/cords"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the challenge of high computational cost and time consumption in hyper-parameter optimization (HPO) for deep neural networks. It proposes AUTOMATA, a novel gradient-based data subset selection framework designed for compute-efficient hyper-parameter tuning. The main contributions include demonstrating that using small, informative data subsets (between 1% and 30%) significantly accelerates HPO, achieving speedups of 3x to 30x while maintaining comparable or even superior performance compared to tuning with the entire dataset. AUTOMATA's gradient-based subset selection is shown to outperform random subset selection and other baselines like CRAIG, and it also contributes to reduced energy consumption and CO2 emissions.",
        "methodology": "AUTOMATA integrates three core components: a hyper-parameter search algorithm, a gradient-based subset selection (GSS) method, and a hyper-parameter scheduling algorithm. For hyper-parameter search, it utilizes algorithms such as Random Search and Tree Parzen Structured Estimator (TPE). The central GSS technique selects data subsets and their associated weights such that the weighted subset loss gradient closely approximates the full training loss gradient. This optimization problem, derived from GRAD-MATCH, is solved using Orthogonal Matching Pursuit (OMP). An efficient 'per-batch' variant of GSS is employed, selecting mini-batches rather than individual data points. For hyper-parameter scheduling, AUTOMATA incorporates methods like Hyperband and Asynchronous Successive Halving Algorithm (ASHA) for early termination of unpromising configurations. Additionally, a warm-starting strategy is used, where models are initially trained on the full dataset for a few epochs (e.g., κ=0.35 for ASHA) to generate more informative loss gradients for subsequent subset selection.",
        "experimental_setup": "The effectiveness of AUTOMATA was empirically evaluated through extensive experiments across various real-world datasets from text (SST2, SST5, glue-SST2, TREC6), image (CIFAR10, CIFAR100, SVHN), and tabular (DNA, SATIMAGE, LETTER, CONNECT-4 from LIBSVM) domains. Baselines included RANDOM (randomly sampled subsets), CRAIG (a gradient-based subset selection method), and FULL (using the entire dataset for training). Experiments were conducted with different subset size fractions: 1%, 5%, 10%, and 30%. Model architectures varied by domain: LSTM with GloVe embeddings for text, ResNet18 and ResNet50 for images, and a two-hidden-layer multi-layer perceptron for tabular data. Hyper-parameter search spaces included learning rates, batch sizes, optimizer choices (SGD, Adam, Momentum, Nesterov), and learning rate schedulers. For fair comparison, a final training run on the entire dataset with the best-found hyper-parameters was performed for all methods (excluding FULL). Experiments were repeated 3 to 5 times. The framework was implemented using PyTorch, Ray-tune for HPO, and CORDS for subset selection, with energy consumption and CO2 emissions calculated using pyJoules and a Machine Learning Impact calculator, respectively.",
        "limitations": "One limitation identified is that in scenarios where absolutely no performance loss is acceptable, the minimum optimal subset size for achieving speedup remains unknown. This often necessitates relying on larger subset sizes, such as 10% or 30%, which might not yield the maximum possible speedup.",
        "future_research_directions": "Future work could focus on developing mechanisms to adaptively change subset sizes based on the model's performance for each configuration, thereby removing the current dependency on fixed subset sizes. The authors also hope that the AUTOMATA framework will encourage further research into more efficient subset selection approaches to accelerate hyper-parameter tuning and contribute to the broader goal of 'Green AI' by reducing computational and environmental costs.",
        "experimental_code": "from ray import tune\n\nconfig = dict(setting= \"hyperparamtuning\",\n\n# parameter for subset selection\n# all settings for subset selection will be fetched from here\nsubset_config = \"configs/SL/config_gradmatchpb-warm_cifar10.py\",\n\n# parameters for hyper-parameter tuning\n# search space for hyper-parameter tuning\nspace = dict(learning_rate=tune.uniform(0.001, 0.01), \n        optimizer= tune.choice(['sgd', 'adam']),\n        trn_batch_size= tune.choice([20, 32, 64]),        \n        ),\n\n# tuning algorithm \nsearch_algo = \"TPE\",\n\n# number of hyper-parameter set to try\nnum_evals = 20,\n\n# metric to be optimized, for 'mean_loss' metric mode should be 'min'\nmetric = \"mean_accuracy\",\nmode = \"max\",\n\n# scheduler to be used (i.e ASHAScheduler)\n# scheduler terminates trials that perform poorly\n# learn more here: https://docs.ray.io/en/releases-0.7.1/tune-schedulers.html\nscheduler = 'hyperband',\n\n# where to store logs\nlog_dir = \"RayLogs/\",\n\n# resume hyper-parameter tuning from previous log\n# specify 'name' (i.e main_2021-03-09_18-33-56) below\nresume = False,\n\n# only required if you want to resume from previous checkpoint\n# it can also be specified if you don't want to resume\nname = None,\n\n# specify resources to be used per trial\n# i.e {'gpu':1, 'cpu':2}\nresources = {'gpu' : 0.5},\n\n# if True, trains model on Full dataset with the best parameter selected.\nfinal_train = True\n\n)\n",
        "experimental_info": "The experimental setup for AUTOMATA involves hyper-parameter tuning and gradient-based subset selection (GSS) with warm-starting. The hyper-parameter search utilizes the Tree-Parzen Structured Estimator (TPE) algorithm. The search space for hyper-parameters includes:\n-   `learning_rate`: sampled uniformly between 0.001 and 0.01.\n-   `optimizer`: a choice between 'sgd' and 'adam'.\n-   `trn_batch_size`: a choice among 20, 32, and 64.\nA total of 20 hyper-parameter sets are evaluated. The optimization metric is 'mean_accuracy', with a mode of 'max'. Hyper-parameter scheduling employs the 'Hyperband' scheduler for early termination of unpromising configurations. Each trial is allocated 0.5 GPU and 1 CPU. After tuning, the model with the best configuration is trained on the full dataset.\n\nThe subset selection component, specified as `GradMatchPB-Warm` (Gradient-based Matching Per Batch with Warm-starting), is configured to select a `fraction=0.1` of the dataset every `20` epochs. The `selection_type` is 'PerBatch', meaning mini-batches are selected. A warm-starting `kappa=0.5` is used, where the model is initially trained on the full dataset for a few epochs to generate more informative loss gradients before adaptive subset selection begins. The `lam=0` and `eps=1e-100` parameters are used for the Orthogonal Matching Pursuit (OMP) solver. The gradients from the model's last linear layer (`linear_layer=True`) are utilized for subset selection. The base optimizer for the training process is SGD with a `learning_rate` of 0.05, `momentum` of 0.9, `weight_decay` of 5e-4, and `nesterov` set to True. A cosine annealing learning rate scheduler with `T_max=300` is used. The training is performed on the CIFAR-10 dataset using a ResNet18 architecture for 300 epochs on a CUDA device."
      }
    }
  ]
}