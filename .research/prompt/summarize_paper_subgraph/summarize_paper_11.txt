
Input:

You are an expert research assistant responsible for summarizing a research paper that will serve as the foundation (Research A) for further exploration and integration.

Your task is to generate a structured summary of the given research paper with a focus on:
- **Technical Contributions**: Identify the main research problem and key findings.
- **Methodology**: Describe the techniques, models, or algorithms used.
- **Experimental Setup**: Outline the datasets, benchmarks, and validation methods.
- **Limitations**: Highlight any weaknesses, constraints, or assumptions.
- **Future Research Directions**: Suggest possible extensions or new areas for research.

Below is the full text of the research paper:

```
Multi-ﬁdelity Bayesian Optimization with Max-value Entropy Search and its parallelization Shion Takeno1, Hitoshi Fukuoka2, Yuhki Tsukada3, Toshiyuki Koyama4, Motoki Shiga5, Ichiro Takeuchi6, and Masayuki Karasuyama7 1,6,7Nagoya Institute of Technology 2,3,4Nagoya University 3,5,7Japan Science and Technology Agency 5Gifu University 6,7National Institute for Material Science 5,6RIKEN Center for Advanced Intelligence Project takeno.s.mllab.nit@gmail.com, fukuoka.hitoshi@j.mbox.nagoya-u.ac.jp, {tsukada.yuhki,koyama.toshiyuki}@material.nagoya-u.ac.jp, shiga m@gifu-u.ac.jp, {takeuchi.ichiro,karasuyama}@nitech.ac.jp Abstract In a standard setting of Bayesian optimization (BO), the objective function evaluation is assumed to be highly expensive. Multi-ﬁdelity Bayesian optimization (MFBO) accelerates BO by incorporating lower ﬁdelity observations available with a lower sampling cost. In this paper, we focus on the information- based approach, which is a popular and empirically successful approach in BO. For MFBO, however, existing information-based methods are plagued by diﬃculty in estimating the information gain. We propose an approach based on max-value entropy search (MES), which greatly facilitates computations by considering the entropy of the optimal function value instead of the optimal input point. We show that, in our multi-ﬁdelity MES (MF-MES), most of additional computations, compared with usual MES, is reduced to analytical computations. Although an additional numerical integration is necessary for the information across diﬀerent ﬁdelities, this is only in one dimensional space, which can be performed eﬃciently and accurately. Further, we also propose parallelization of MF-MES. Since there exist a variety of diﬀerent sampling costs, queries typically occur asynchronously in MFBO. We show that similar simple computations can be derived for asynchronous parallel MFBO. We demonstrate eﬀectiveness of our approach by using benchmark datasets and a real-world application to materials science data. 1 arXiv:1901.08275v2  [stat.ML]  13 Feb 20201 Introduction Bayesian optimization (BO) is a popular machine-learning technique for the black-box optimization problem. Eﬃciency of BO has been widely shown in a variety of application areas such as scientiﬁc experiments (Wigley et al., 2016), simulation calculations (Ramprasad et al., 2017), and tuning of machine-learning methods (Snoek et al., 2012). In these scenarios, observing an objective function value is usually quite expensive and thus achieving the optimal value with low querying cost is strongly demanded. Although standard BO only considers directly querying to an objective function f(x), in many practical problems, lower ﬁdelity approximations of the original objective function can be observed. For example, theoretical computations of physical processes often have multiple levels of approximations by which the trade-oﬀ between the computational cost and accuracy can be controlled. A goal of multi-ﬁdelity Bayesian optimization (MFBO) is to accelerate BO by utilizing those lower ﬁdelity observations to reduce the total cost of the optimization. In this paper, we focus on the information-based approach. For usual BO without multi-ﬁdelity, which we call single ﬁdelity BO , seminal works of this direction are entropy search (ES) and predictive entropy search (PES) proposed by Hennig & Schuler (2012) and Hern´ andez-Lobato et al. (2014), respectively. They deﬁne acquisition functions by using information gain for the optimal solution x∗:= argmaxx f(x). Unlike classical evaluation measures such as expected improvement, the information-based criterion is a measure of global utility which does not require any additional exploit-explore trade-oﬀ parameter. The superior performance of information-based methods have been shown empirically, and then, the same approach has also been extended to the multi-ﬁdelity setting (Swersky et al., 2013; Zhang et al., 2017). Even in the case of single ﬁdelity BO, however, accurately evaluating information gain is notoriously diﬃcult, which often requires complicated numerical approximations. For MFBO, evaluating information across multiple ﬁdelities is further diﬃcult. To overcome this diﬃculty, we consider a novel information-based approach to MFBO, which is based on a variant of ES called max-value entropy search (MES), proposed by Wang & Jegelka (2017). MES considers the information gain for f∗:= maxx f(x) instead of x∗. This greatly facilitates the computation of the information gain because f∗is in one dimensional space unlike x∗, and they showed superior performance of MES compared with ES/PES. Our method, called multi-ﬁdelity MES (MF-MES), can evaluate the information gain for f∗from an observation of an arbitrary ﬁdelity, and we show that additional expressions, compared with MES, can be derived analytically except for one dimensional integral, which can be calculated accurately and eﬃciently by using standard numerical integration techniques. This enables us to obtain more reliable evaluation of information gain easily unlike existing information-based MFBO methods because they contain approximations which are diﬃcult to justify. Our MF-MES is also advantageous to other measures of global utility for MFBO, such as the knowledge gradient-based method (Poloczek et al., 2017), because they are often computationally extremely complicated. Section 5 discusses related studies in more detail. Further, we also propose parallelization of MF-MES. Since objective functions have a variety of sampling 2costs, queries naturally occur asynchronously in MFBO. We extend our information gain so that points currently being queried can be taken into consideration. Similarly in the case of MF-MES, we show that a required numerical integration in addition to the sampling of f∗ is also reduced to one dimensional space through the integration by substitution. This allows us to obtain the reliable evaluation of the information gain for the parallel extension of MF-MES. Our main contributions are summarized as follows: 1. We develop an information-theoretic eﬃcient MFBO method. Na¨ ıve formulation and implementation of this problem raise computationally challenging issues that need to be addressed by carefully-tuned and time-consuming approximate computations. By using several computational tricks mainly inspired by MES (Wang & Jegelka, 2017), we show that this computational bottleneck can be nicely avoided without additional assumptions or approximations. 2. We develop an information-theoretic asynchronous parallel MFBO method. To our knowledge, there are no existing works in this topic — We believe that our method is useful in many practical experimental design and black-box optimization tasks with multiple information sources with diﬀerent ﬁdelities and its parallel evaluation. We empirically demonstrate eﬀectiveness of our approach by using benchmark functions and a real-world application to materials science data. 2 Preliminary In this section, we ﬁrst brieﬂy review a multi-ﬁdelity extension of Gaussian process regression (GPR). Suppose that y(1) x ,...,y (M) x are the observations at x ∈X ⊂Rd with M diﬀerent ﬁdelities in which y(M) x is the highest ﬁdelity and y(1) x is the lowest ﬁdelity. Each observation is modeled as y(m) x = f(m) x + ϵ in which a random noise ϵ ∼N(0,σ2 noise) is added to the underlying true function f(m) x : X→ R. The training data set Dn = {(xi,y(mi) xi ,mi)}i∈[n] contains a set of triplets consisting of an input xi, ﬁdelity mi ∈[M], and an output y(mi) xi , where [n] := {1,...,n }. Throughout the paper, we assume that a set of outputs {f(m) x }for any set of pairs ( x,m) are always modeled as the multi-variate normal distribution. Standard multi-output extensions of GPR such as multi-task GPR (Bonilla et al., 2008), co-kriging (Kennedy & O’Hagan, 2000), and semiparametric latent factor model (SLFM) (Teh et al., 2005), satisfy this condition. We call GPR ﬁtted to observations across multiple ﬁdelities multi-ﬁdelity Gaussian process regression (MF-GPR), in general. MF-GPR deﬁnes a kernel function k((xi,mi),(xj,mj)) for a pair of training instances ( xi,y(mi) xi ,mi) and (xj,y(mj) xj ,mj). An example of this kernel function in the case of SLFM is shown in appendix A.1. By deﬁning a kernel matrix K∈Rn×n in which the i,j element is deﬁned by k((xi,mi),(xj,mj)), all the ﬁdelities f(1),...,f (M) are integrated into a GPR model in which predictive mean and variance are µ(m) x = k(m) n (x)⊤C−1y, and σ2(m) x = k((x,m),(x,m)) −k(m) n (x)⊤C−1k(m) n (x), where C := K+ σ2 noiseI with the identity matrix I, y:= (y(m1) x1 ,...,y (mn) xn )⊤, and k(m) n (x) := (k((x,m),(x1,m1)),...,k ((x,m),(xn,mn)))⊤. 3For later use, we deﬁne σ2(mm′) x as the predictive covariance between ( x,m) and (x,m′), i.e., covariance for the identical xat diﬀerent ﬁdelities: σ2(mm′) x = k((x,m),(x,m′)) −k(m) n (x)⊤C−1k(m′) n (x). 3 Multi-ﬁdelity Bayesian Optimization with Max-value Entropy We consider Bayesian optimization (BO) for maximizing the highest ﬁdelity function f(M) x when M diﬀerent ﬁdelities y(m) x for m = 1,...,M are available to querying. The querying cost is assumed to be known as λ(m), where λ(1) ≤λ(2) ... ≤λ(M). Our goal is to achieve a higher value with smaller accumulated cost of the queryings. We call this problem multi-ﬁdelity Bayesian optimization (MFBO). When M = 1, MFBO is reduced to the usual black box optimization to which we refer as the single ﬁdelity setting, while we refer to the setting M ≥2 as the multi-ﬁdelity setting. We employ the information-based approach, which has been widely used in the single ﬁdelity BO. In particular, our approach is inspired by max-value entropy search (MES) proposed by Wang & Jegelka (2017), which considers information gain about the optimal value maxx∈Xf(x) obtained by a querying. In the case of MFBO, we need to consider the information gain for identifying the maximum of the highest ﬁdelity function f∗:= maxx∈Xf(M) x by observing an arbitrary ﬁdelity observation. We refer to our information-based MFBO as multi-ﬁdelity MES (MF-MES). Although information-based approaches often result in complicated computations, we show that the calculation of our information gain is reduced to simple computations by which stable information evaluation becomes possible. 3.1 Information Gain for Sequential Querying We ﬁrst consider the case that a query is sequentially issued after the previous one is observed, which we refer to as sequential querying. Suppose that we already have a training data set Dt and need to determine next xt+1 and mt+1. We deﬁne an acquisition function a(x,m) := I(f∗; f(m) x |Dt) / λ(m), (1) where I(f∗; f(m) x |Dt) is the mutual information between f∗ and f(m) x conditioned on Dt. By maximizing a(x,m), we obtain a pair of the input xand the ﬁdelity mwhich maximally gains information of the optimal value f∗of the highest ﬁdelity per unit cost. The mutual information can be written as the diﬀerence of the entropy: I(f∗; f(m) x |Dt) = H(f(m) x |Dt) −Ef∗|Dt [ H(f(m) x |f∗,Dt) ] , (2) where H(·|· ) is the conditional entropy of p(·|· ). The ﬁrst term in the right hand side can be derived analytically for any ﬁdelity m: H(f(m) x |Dt) = log ( σ(m) x √ 2πe ) , where e:= exp(1). The second term in (2) takes the expectation over the maximum f∗. Since an analytical formula is not known for this expectation, 4we employ Monte Carlo estimation by sampling f∗from the current GPR: Ef∗|Dt [ H(f(m) x |f∗,Dt) ] ≈ ∑ f∗∈F∗ H(f(m) x |f∗,Dt) |F∗| , (3) where F∗ is a set of sampled f∗. Note that since this sampling approximation is in one dimensional space, accurate approximation can be expected with a small amount of samples. In Section 4, we discuss computational procedures of this sampling. For a given sampled f∗, the entropy of p(f(m) x |f∗,Dt) is needed to calculate in (3). To make the computation tractable, we replace this conditional distribution with p(f(m) x |f(M) x ≤f∗,Dt), i.e., conditioning only on the given x rather than requiring f(M) x ≤f∗ for ∀x ∈X . Note that this simpliﬁcation has been employed by most of entropy-based BO methods (e.g., Hern´ andez-Lobato et al., 2014; Wang & Jegelka, 2017) including MES, and superior performance compared with other approaches has been shown. For any ζ ∈ R, deﬁne γ(m) ζ (x) := ( ζ −µ(m) x )/σ(m) x as a function for scaling. When m = M, the density function p(f(m) x |f(M) x ≤f∗,Dt) is truncated normal distribution. The entropy of truncated normal distribution can be represented as (Michalowicz, 2014) H(f(M) x |f(M) x ≤f∗,Dt)=log (√ 2πeσ(M) x Φ ( γ(M) f∗ (x) )) − γ(M) f∗ (x)φ ( γ(M) f∗ (x) ) 2Φ ( γ(M) f∗ (x) ) , (4) where φ and Φ are the probability density function and the cumulative distribution function of the standard normal distribution. Next, we consider the case of m̸= M. Unlike the case of m= M, the density p(f(m) x |f(M) x ≤f∗,Dt) is not the truncated normal. Since MF-GPR represents all ﬁdelities as one uniﬁed GPR, the joint marginal distribution p(f(M) x ,f(m) x |Dt) can be immediately obtained from the two dimensional predictive distribution, from which we obtain p(f(M) x |f(m) x ,Dt) as f(M) x |f(m) x ,Dt ∼N(u(x),s2(x)), (5) where u(x) = σ2(mM) x ( f(m) x −µ(m) x ) /σ2(m) x + µ(M) x , and s2(x) = σ2(M) x − ( σ2(mM) x )2 /σ2(m) x . By using this conditional distribution, the entropy of p(f(m) x |f(M) x ≤f∗,Dt) can be written as follows: Lemma 3.1. Let Z := 1/σ(m) x Φ(γ(M) f∗ (x)) and Ψ(f(m) x ) := Φ ( (f∗−u(x))/s(x) ) φ ( γ(m) f(m) x (x) ) . Then, for a given f∗, we obtain H(f(m) x |f(M) x ≤f∗,Dt) = − ∫ ZΨ(f(m) x ) log ( ZΨ(f(m) x ) ) df(m) x . (6) See Appendix B for the proof. Lemma 3.1 indicates that the entropy is represented through the one dimensional integral over f(m) x . Since the integral is only on the one dimensional space, standard numerical integration techniques (e.g., quadrature) 5TimeWorkers Query 1 Query 6 Query 3 Query 5 Query 2 Query 4 Query 8 Query 9 Query 7 Figure 1: Asynchronous parallelization in MFBO. Because of diversity of the evaluation cost of objective functions, queries typically occur asynchronously. When a worker becomes available, a next query should be determined while taking queries being evaluated in the other workers into consideration. can provide precise approximation eﬃciently. Consequently, we see that that the entropy H(f(m) x |f∗,Dt) in (3) can be obtained accurately with simple computations. 3.2 Asynchronous Parallelization We consider an extension of MF-MES for the case that multiple queries can be issued in parallel, which we refer to as parallel querying. Suppose that we have q >1 “workers” each one of which can evaluate an objective function value. In the context of parallel BO, the two settings called synchronous and asynchronous parallelizations can be considered. As shown in Figure 1, since MFBO evaluates a variety of diﬀerent costs of objective functions, queries naturally occur asynchronously. Thus, we focus on asynchronous parallelization (See Appendix D.4 for the discussion of the synchronous setting). Suppose that q−1 pairs of the input xand the ﬁdelity m, written as Q:= {(x1,m1),..., (xq−1,mq−1)}, are now being evaluated by using q−1 workers, and an additional query to an available worker needs to be determined. Let fQ:= (f(m1) x1 ,...,f (mq−1) xq−1 )⊤. Then, a natural extension of MF-MES to determine the q-th pair (xq,mq) is apara(x,m) = I(f∗; f(m) x |Dt,fQ) / λ(m). (7) The numerator is the mutual information conditioned on fQwhich is deﬁned by I(f∗; f(m) x |Dt,fQ) := EfQ|Dt [ H(f(m) x |Dt,fQ) ] −EfQ,f∗|Dt [ H(f(m) x |Dt,fQ,f(M) x ≤f∗) ] . (8) Compared with the mutual information in sequential querying (2), this equation additionally takes the expectation over fQwhich is currently under evaluation. Thus, by using (7), we can select a cost eﬀective pair of xand m while the q−1 pairs running on the other workers are taken into consideration. Although (8) contains the |Q|+ 2 dimensional integral at a glance, we show that this can be calculated by at most 2 dimensional numerical integral. Let ΣM∈R2×2 and ΣQ∈Rq−1×q−1 be the predictive covariance 6matrices for M:= {(x,m),(x,M)}and Q, respectively, and ΣQ,M(= Σ⊤ M,Q) ∈Rq−1×2 be the predictive covariance matrix of the rows Qand the columns M. For later use, we deﬁne the conditional distribution p(f(m) x ,f(M) x |Dt,fQ) as follows  f(m) x f(M) x  |Dt,fQ∼N    µ(m) x|fQ µ(M) x|fQ  ,   σ2(m) x|fQ σ2(mM) x|fQ σ2(mM) x|fQ σ2(M) x|fQ    , where  µ(m) x|fQ µ(M) x|fQ  =  µ(m) x µ(M) x  + ΣM,QΣ−1 Q (fQ−µQ), (9)   σ2(m) x|fQ σ2(mM) x|fQ σ2(mM) x|fQ σ2(M) x|fQ  = ΣM−ΣM,QΣ−1 Q ΣQ,M, (10) and µQ:= (µ(m1) x1 ,...,µ (mq−1) xq−1 )⊤. Note that (9) is a random variable vector because it depends on fQ, while all the elements of (10) are constants. By using these equations, the mutual information (8) is re-written as follows: Lemma 3.2. Let ˜f∗:= f∗−µ(M) x|fQ , (11) and ˜f(m) x := f(m) x −µ(m) x|fQ . Then, we obtain I(f∗; f(m) x |Dt,fQ) = log ( σ(m) x|fQ √ 2πe ) −E˜f∗|Dt [∫ η( ˜f∗, ˜f(m) x ) logη( ˜f∗, ˜f(m) x ) d ˜f(m) x ] (12) where η( ˜f∗, ˜f(m) x ):= Φ ( ˜f∗− ( σ2(mM) x|fQ / σ2(m) x|fQ ) ˜f(m) x σ2(M) x|fQ − ( σ2(mM) x|fQ )2 / σ2(m) x|fQ ) φ ( ˜f(m) x σ(m) x|fQ ) σm x|fQ Φ ( ˜f∗ σ(M) x|fQ ) . (13) See Appendix D.1 for the proof. It should be noted that the second term of (12) only contains the integral over two variables ( ˜f(m) x and ˜f∗) unlike the original formulation (8). The ﬁrst term of (12) can be directly calculated because σ(m) x|fQ does not depend on the random vector fQ as shown in (10). We calculate the expectation in the second term of (12) by using the Monte Carlo estimation with sampled ˜f∗: ∑ ˜f∗∈˜F∗ 1 |˜F∗| ∫ η( ˜f∗, ˜f(m) x ) logη( ˜f∗, ˜f(m) x ) d ˜f(m) x (14) where ˜F∗is a set of sampled ˜f∗. The integral in this equation can be easily evaluated by using quadrature because it is on the one dimensional space and η( ˜f∗, ˜f(m) x ) can be analytically calculated from the deﬁnition (13). Further, when m= M, this integral is also can be analytically calculated (See Appendix D.2). 74 Computations Algorithm 1 shows the procedure of MF-MES for sequential querying. As the ﬁrst step in the every iteration, a set of max values F∗ are sampled from p(f∗ |Dt). There are several approaches to sampling the max value. Wang & Jegelka (2017) showed that the eﬀective approximation is possible by using sampling through Gumbel distribution or random feature map (RFM). Gumbel distribution is widely known in extreme value theory (Gumbel, 1958) as one of generalized extreme value distributions. Although the Gumbel approximation is performed under an independent approximation of GPR, Wang & Jegelka (2017) showed the accurate approximation can be obtained. In contrast, RFM (Rahimi & Recht, 2008) can incorporate dependency in the GPR model by using a set of pre-deﬁned basis functions φ(x,m) ∈RD, and the highest ﬁdelity function is represented as f(M) x ≈w⊤φ(x,M), where w∈RD (Appendix A.2 shows an example of an RFM approximation in the case of SLFM). The max value is sampled by maximizing w⊤φ(x,M) with respect to x. For further detail of these two approaches, see (Wang & Jegelka, 2017), in which it is also shown that MES is empirically robust with respect to this sampling, and theoretically, they showed that the regret bound can be guaranteed even only for one sample of f∗. Once F∗is generated, the acquisition function calculation can be analytically performed except for one dimensional numerical integration. Although most complicated process in the algorithm is the calculation of (6) shown in line 15 of Algoirthm 1, this is also quite simple in practice as described below. For a given f∗ and the conditional distribution (5) which is constructed from the two dimension GPR predictive distribution p(f(M) x ,f(m) x |x,Dt), the integral of (6) can be computed by O(1). Further, since (5) does not depend on sampled f∗, it is not required to re-calculate (5) for each one of sampled f∗. For the acquisition function maximization ( argmax in line 4), if the candidate space Xis a discrete set, we simply calculate the acquisition values for all x∈X. For a continuous space, popular approaches such as DIRECT (Jones et al., 1993) and gradient-based optimizers are applicable. Note that our acquisition function is diﬀerentiable, and the derivative of the integral (6) can be calculated by the same one dimensional numerical integral procedure. For the case of parallel querying, the acquisition function maximization is performed when a worker becomes available. To evaluate (14), we need to sample ˜f∗, which is determined through f∗and fQas shown in (11). This can be easily performed through RFM. By calculating w⊤φ(x,m) for ( x,m) ∈Q with the sampled parameter w, we can directly obtain a sample of fQ. For f∗, we maximize w⊤φ(x,M) as in the sequential querying case. The algorithm of Parallel MF-MES is shown in Appendix D.3. Throughout the paper, we use I(f∗; f(m) x ) as the information gain for brevity. I(f∗; y(m) x ), in which noisy observation y(m) x is contained, is also possible to use with the almost same procedure (for details, see Appendix C). Although we mainly focus on the case that we only have the discrete ﬁdelity level m ∈{1,...,M }as an “ordinal scale”, several studies consider the setting in which a ﬁdelity can be deﬁned as a point z in a continuous “ﬁdelity feature” (FF) space Z(Kandasamy et al., 2017). This setting is more restrictive because 8Algorithm 1 MF-MES for sequential querying 1: function MF-MES(D0,M, X,{λ(m)}M m=1) 2: for t= 0,...,T do 3: Generate F∗from current f(M)(x) 4: (xt+1,mt+1) ←argmaxx∈X,m InfoGain(x, m, F∗, Dt) / λ(m) 5: Dt+1 ←Dt ∪(xt+1,y(mt+1)(xt+1),mt+1) 6: end for 7: end function 8: function InfoGain(x, m, F∗, Dt) 9: Calculate µ(m) x and σ(m) x 10: Set H0 ←log ( σ(m) x √ 2πe ) 11: if m= M then 12: Set H1 ←∑ f∗∈F∗ H(f(M) x |f(M) x ≤f∗,Dt) |F∗| by using (4) 13: else 14: Calculate µ(M) x and σ(M) x and σ2(mM) x 15: Set H1 ←∑ f∗∈F∗ H(f(m) x |f(M) x ≤f∗,Dt) |F∗| by using (6) 16: end if 17: Return H0 −H1 18: end function it requires additional side-information z which speciﬁes a degree of ﬁdelity, though this prior knowledge may be able to improve the accuracy. By introducing a kernel function in ﬁdelity space Z, our method can easily adapt to this setting (See appendix E). 5 Related Work Multi-ﬁdelity extension of BO has been widely studied. For example, (Huang et al., 2006; Lam et al., 2015; Picheny et al., 2013) extended the standard EI to the multi-ﬁdelity setting. As with the usual EI, these are local measures of utility unlike the information-based approaches. Gaussian process upper conﬁdence bound (GP-UCB) (Srinivas et al., 2010) is a popular approach in the single ﬁdelity setting, and some studies proposed its multi-ﬁdelity extensions. Kandasamy et al. (2016) proposed multi-ﬁdelity GP-UCB for discrete ﬁdelity m= 1,...,M , and further, Kandasamy et al. (2017) proposed a similar UCB-based approach for the setting with the continuous ﬁdelity space Z. However, the UCB criterion has a trade-oﬀ parameter which balances exploit-exploration. In practice, this parameter needs to be carefully selected to achieve good 9performance. Another approach recently proposed in (Sen et al., 2018) is a multi-ﬁdelity extension of a hierarchical space partitioning (Bubeck et al., 2011). However, this method assumes that the approximation error can be represented as a known function form of cost, and further, they associate ﬁdelity with the depth of hierarchical tree, but the appropriateness of a speciﬁc choice of a pair of a point xand ﬁdelity mis diﬃcult to interpret. Information-based BO has also been studied for the multi-ﬁdelity setting, including entropy search (ES)-based (Swersky et al., 2013; Klein et al., 2017) and predictive entropy search (PES)-based (Zhang et al., 2017; McLeod et al., 2018) methods. Although these methods can measure global utility of the query without introducing any trade-oﬀ parameter, they inherit the computational diﬃculty of the original ES and PES, which consider the entropy of p(x∗), where x∗ := argmaxx f(x) is the optimal solution. PES mitigates computational diﬃculty by using 1) the symmetric property of the mutual information, and 2) several assumptions which simplify involved densities. However, integral with respect to x∗is still necessary though the dimension of x∗can be high, and the complicated approximation procedure including expectation propagation (Minka, 2001) is required. Further, an additional assumption about inter-ﬁdelity diﬀerences are required in the case of (Zhang et al., 2017). Song et al. (2018) proposed another information-based approach, which separates phases of the low-ﬁdelity exploration and the highest ﬁdelity optimization. However, the transition of these phases are controlled by a hyper-parameter which is necessary to set appropriately beforehand. Another approach incorporating a measure of global utility is knowledge gradient (KG)-based methods (Poloczek et al., 2017; Wu & Frazier, 2017). This approach evaluates the max gain of predictive mean maxx∈Xµ(M) x . In particular, misoKG (Poloczek et al., 2017) deals with the discrete ﬁdelity case. However, the acquisition function evaluation requires the expected value of the maximum of the mean function E[maxx′∈Xµ(M) x′ ] after adding y(m) x into training set, meaning that the maximization of the acquisition function is deﬁned as a nested optimization. Although a variety of computational techniques have been studied for KG, this nested optimization process is highly cumbersome to implement and computationally expensive. In contrast, our MF-MES is based on much simpler computations compared with existing information- based methods and other measures of global utility. Original MES calculates the entropy by representing a conditional distribution of fx given f∗as a truncated normal distribution. As we saw in Section 3.1, for the information gain from a lower ﬁdelity, the truncated normal approach is not applicable anymore because lower ﬁdelity functions f(m) x for m= 1,...,M −1 are not truncated for a given f∗. We already show that equations derived in Lemma 3.1 enables us to evaluate the entropy accurately with the only one dimensional additional numerical integration. For further acceleration of MES, Ru et al. (2018) proposed approximating the density of f∗ and f given f∗ by normal distributions, but reliability of these approximations are not clearly understood, and thus we do not employ in this paper. The parallel extension of BO has been widely studied (e.g., Snoek et al., 2012; Desautels et al., 2014). 10As we described in Section 3.2, MFBO is typically asynchronous, while many of existing studies focus on the synchronous setting including PES-based parallel BO (Shah & Ghahramani, 2015). Several papers focus on the asynchronous setting (Kandasamy et al., 2018), but these methods are diﬃcult to apply to the multi-ﬁdelity setting because they do not provide any criterion to select ﬁdelity. To our knowledge, an extension of KG (Wu & Frazier, 2017) is an only parallel method proposed for MFBO. However, this method is only for the synchronous setting, and further, it is only shown for the FF-based setting which is more restrictive as we described in the end of Section 4. We also note that a parallel extension of MES has not been shown even for the single-ﬁdelity setting. About a possible sequential/parallel settings of MF-MES, a summary is shown in Appendix F. 6 Experiments We evaluate eﬀectiveness of MF-MES compared with other existing methods. To evaluate performance, we em- ployed simple regret (SR) and inference regret (IR). SR is deﬁned by maxx∈Xf(M)(x) − maxi∈{i|i∈[t],mi=M}f(M)(xi), indicating the error by the best point queried so far. IR is deﬁned by maxx∈Xf(M)(x) −f(M)(ˆxt), where ˆxt := argmaxx∈Xµ(M) x which is seen as the recommendation from the model at iteration t. If IR is larger than SR at an iteration, we employed the value of SR as IR of that iteration for stable evaluation. For MF-GPR, we used SLFM in GP-based methods, unless otherwise noted. For the kernel function, we used Gaussian kernel with automatic relevance determination (ARD). We used a synthetic function generated by GPR, two benchmark functions, and a real-world dataset from materials science. For the GP-based synthetic function, we generated d= 3 dimensional synthetic functions through an SLFM model which has two ﬁdelity levels. The benchmark functions are called Styblinski-Tang, and HartMann6, which has M = 2, and 3 ﬁdelities, respectively. The sampling cost is set ( λ(1),λ(2)) = (1,5) when M = 2, and ( λ(1),λ(2),λ(3)) = (1 ,3,5) when M = 3. As an example of practical applications, we applied our method to the parameter optimization of a simulation model in materials science. The task is to optimize two material parameters of the model (Tsukada et al., 2014) by minimizing the discrepancy between the precipitate shape predicted by the model and one measured by an electron microscope. The relative cost of the objective function evaluation is determined by the accuracy of the computational model which is speciﬁed beforehand as ( λ(1),λ(2),λ(3)) = (5,10,60). Unlike benchmark functions, the candidate xis ﬁxed beforehand in this dataset (so-called the pooled setting). Each ﬁdelity has 62,500 candidate points. The experiments on the GP-based synthetic function were performed 100 times (10 diﬀerent initialization for each one of 10 generated functions). The other benchmark functions and the material dataset were performed 10 times with diﬀerent initialization. For further detail of the settings, see Appendix G.1. 11100 200 300 400 Total cost 10 1 100 Simple Regret Synthetic Function MF-MES MES MF-PES BOCA MFSKO 50 60 70 80 90 100 Total cost 10 1 100 101 Styblinski-Tang MF-MES MES MF-PES BOCA MFSKO 150 175 200 225 250 275 300 Total cost 10 1 100 HartMann6 MF-MES MES MF-PES BOCA MFSKO 600 800 1000 1200 1400 Total cost 10 2 10 1 100 101 Material MF-MES MES MF-PES BOCA MFSKO (a) Simple regret. 100 200 300 400 Total cost 10 1 100 Inferences Regret Synthetic Function MF-MES MES MF-PES BOCA MFSKO 50 60 70 80 90 100 Total cost 10 3 10 2 10 1 100 101 Styblinski-Tang MF-MES MES MF-PES BOCA MFSKO 150 175 200 225 250 275 300 Total cost 10 1 100 HartMann6 MF-MES MES MF-PES BOCA MFSKO 600 800 1000 1200 1400 Total cost 10 2 10 1 100 Material MF-MES MES MF-PES BOCA MFSKO (b) Inference regret. Figure 2: Performance comparison on sequential querying. 100 200 300 400 Total cost 10 1 100 Simple Regret Synthetic Function Parallel MF-MES MF-MES Parallel MES MES-LP GP-UCB-PE AsyTS 50 60 70 80 90 100 Total cost 10 5 10 4 10 3 10 2 10 1 100 101 Styblinski-Tang Parallel MF-MES MF-MES Parallel MES MES-LP GP-UCB-PE AsyTS 150 175 200 225 250 275 300 Total cost 10 1 100 HartMann6 Parallel MF-MES MF-MES Parallel MES MES-LP GP-UCB-PE AsyTS 600 800 1000 1200 1400 Total cost 10 2 10 1 100 101 Material Parallel MF-MES MF-MES Parallel MES MES-LP GP-UCB-PE AsyTS (a) Simple regret. 100 200 300 400 Total cost 10 2 10 1 100 Inferences Regret Synthetic Function Parallel MF-MES MF-MES Parallel MES MES-LP GP-UCB-PE AsyTS 50 60 70 80 90 100 Total cost 10 4 10 3 10 2 10 1 100 101 Styblinski-Tang Parallel MF-MES MF-MES Parallel MES MES-LP GP-UCB-PE AsyTS 150 175 200 225 250 275 300 Total cost 10 1 100 HartMann6 Parallel MF-MES MF-MES Parallel MES MES-LP GP-UCB-PE AsyTS 600 800 1000 1200 1400 Total cost 10 2 10 1 100 Material Parallel MF-MES MF-MES Parallel MES MES-LP GP-UCB-PE AsyTS (b) Inference regret. Figure 3: Performance comparison on parallel querying. 126.1 Evaluation for Sequential Querying We ﬁrst evaluate the performance for sequential querying. For comparison, we used MF-SKO (Huang et al., 2006), Bayesian optimization with continuous approximations (BOCA) (Kandasamy et al., 2017), and multi-ﬁdelity PES (MF-PES) (Zhang et al., 2017). We also evaluated single ﬁdelity MES which applied to the highest ﬁdelity function f(M)(x). As we see in Section 5, misoKG is another measure of global utility for MFBO. However, we could not employ it as a baseline because it was not straightforward to modify the author implementation for fair comparison (e.g., changing the MF-GPR model), and creating eﬃcient implementation from scratch is also extremely complicated (na¨ ıve implementation of KG can be prohibitively slow). Only BOCA employed the multi-task GPR (MT-GPR) model because the acquisition function assumes MT-GPR. For the sampling of f∗in MES and MF-MES, we employed the RFM-based approach described in Section 4, and sampled 10 f∗s at every iteration. In MF-PES, x∗was also sampled 10 times through RFM as suggested by (Hern´ andez-Lobato et al., 2014). Figure 2 shows SR and IR. In both of SR and IR, MF-MES decreased the regret faster than or comparable with all the other methods. The single-ﬁdelity MES is relatively slow because it cannot use lower-ﬁdelity functions, and we clearly see that MF-MES successfully accelerates MES. For SR of the GP-based synthetic, HartMann6 and material functions, MF-PES was slower than the others. We empirically observed that MF-PES sometime did not aggressively select the highest ﬁdelity samples enough. A possible reason is in an approximation employed by MF-PES which assumes f(m) x ≤f(m) x∗ +cfor m<M , where c is a constant (see Zhang et al., 2017, for the detailed deﬁnition). However, even when x∗is given, this strict inequality relation does not hold obviously (note that x∗is the maximizer only when m= M), and we conjecture that the information gain from lower ﬁdelity functions can be overly estimated because of this artiﬁcial truncation. In the material data, IR was slightly unstable which was caused by noisy observations contained in this real-world dataset. In particular, MF-PES largely ﬂuctuated, and this would also be due to the lack of the highest ﬁdelity samples as we mentioned above. We also evaluate computational time of the acquisition functions in Appendix G.2. 6.2 Evaluation for Parallel Querying Next, we evaluate performance on parallel querying. For comparison, we used MES combined with local penalization (Gonzalez et al., 2016), denoted as MES-LP, Gaussian process upper conﬁdence bound with pure exploration (GP-UCB-PE) (Gonzalez et al., 2016), asynchronous parallel Thompson sampling (AsyTS) (Kandasamy et al., 2018). Here, we would like to note that no existing methods have been proposed for discrete ﬁdelity parallel MFBO, to our knowledge, and extending existing methods to this setting is not straightforward because of discreteness of ﬁdelity levels. We also compare the performance of “sequential” MF-MES (which is same as “MF-MES” in Figure 2), and a parallel extension of single-ﬁdelity MES (shown in Appendix D.4) as baselines. For the sampling of ˜f∗in Parallel MF-MES and Parallel MES, the number of 13samples are set 10 through RFM. The number of workers is set q= 4. Figure 3 shows SR and IR. We see that parallel MF-MES substantially faster than sequential MF-MES and parallel MES. This indicates that parallel MF-MES succeeded in assigning workers across multiple ﬁdelities. Compared with other methods, parallel MF-MES shows rapid or comparable convergence. 7 Conclusion We propose a novel information-based multi-ﬁdelity Bayesian optimization (MFBO). The acquisition function is deﬁned through the information gain for the optimal value f∗of the highest ﬁdelity function. We show that our method called MF-MES (multi-ﬁdelity max-value entropy search) can be reduced to simple computations, which allows reliable evaluation of the entropy. For the asynchronous setting, which naturally arises in MFBO, we further propose parallelization of MF-MES and show that it is also easy to compute. We demonstrate eﬀectiveness of MF-MES by using benchmark functions and a real-world materials science data. 14Acknowledgements This work was supported by MEXT KAKENHI to I.T. (16H06538, 17H00758), M.K. (16H06538, 17H04694) and M.S (16H02866); from JST CREST awarded to I.T. (JPMJCR1302, JPMJCR1502) and PRESTO awarded to M.K. (JPMJPR15N2), M.S (JPMJPR16N6) and Y.T (JPMJPR15NB); from the MI2I project of the Support Program for Starting Up Innovation Hub from JST awarded to I.T., and M.K.; and from RIKEN Center for AIP awarded to M.S. and I.T. 15References Bhattacharjee, T., Mendis, C., Oh-ishi, K., Ohkubo, T., and Hono, K. The eﬀect of ag and ca additions on the age hardening response of mgzn alloys. Materials Science and Engineering: A , 575:231 – 240, 2013. Bonilla, E. V., Chai, K. M., and Williams, C. Multi-task gaussian process prediction. In Advances in Neural Information Processing Systems 20 , pp. 153–160. Curran Associates, Inc., 2008. Bubeck, S., Munos, R., Stoltz, G., and Szepesv´ ari, C. X-armed bandits. Journal of Machine Learning Research, 12:1655–1695, 2011. Desautels, T., Krause, A., and Burdick, J. W. Parallelizing exploration-exploitation tradeoﬀs in Gaussian process bandit optimization. Journal of Machine Learning Research , 15:4053–4103, 2014. G, M. B. and Wilhelm, S. Moments calculation for the doubly truncated multivariate normal density, 2012. Genton, M. G., Keyes, D. E., and Turkiyyah, G. Hierarchical decompositions for the computation of high-dimensional multivariate normal probabilities. Journal of Computational and Graphical Statistics , pp. 268–277, 2017. Genz, A. Numerical computation of multivariate normal probabilities. Journal of Computational and Graphical Statistics, 1:141–150, 1992. Gonzalez, J., Dai, Z., Hennig, P., and Lawrence, N. Batch bayesian optimization via local penalization. In Proceedings of the 19th International Conference on Artiﬁcial Intelligence and Statistics , volume 51, pp. 648–657. PMLR, 2016. Gumbel, E. J. Statistics of Extremes . Columbia University Press, 1958. Hennig, P. and Schuler, C. J. Entropy search for information-eﬃcient global optimization. Journal of Machine Learning Research, 13:1809–1837, 2012. Hern´ andez-Lobato, J. M., Hoﬀman, M. W., and Ghahramani, Z. Predictive entropy search for eﬃcient global optimization of black-box functions. In Advances in Neural Information Processing Systems 27 , pp. 918–926. Curran Associates, Inc., 2014. Huang, D., Allen, T., Notz, W., and Miler, R. Sequential kriging optimization using multiple-ﬁdelity evaluations. Structural and Multidisciplinary Optimization , 32(5):369–382, 2006. Jones, D. R., Perttunen, C. D., and Stuckman, B. E. Lipschitzian optimization without the lipschitz constant. Journal of Optimization Theory and Applications , 79(1):157–181, 1993. Kandasamy, K., Dasarathy, G., Oliva, J., Schneider, J., and P´ oczos, B. Gaussian process bandit optimisation with multi-ﬁdelity evaluations. In Advances in Neural Information Processing Systems 29 , pp. 1000–1008. Curran Associates, Inc., 2016. 16Kandasamy, K., Dasarathy, G., Schneider, J., and P´ oczos, B. Multi-ﬁdelity Bayesian optimisation with continuous approximations. In Proceedings of the 34th International Conference on Machine Learning , pp. 1799–1808, 2017. Kandasamy, K., Krishnamurthy, A., Schneider, J., and Poczos, B. Parallelised bayesian optimisation via Thompson sampling. In Proceedings of the 21st International Conference on Artiﬁcial Intelligence and Statistics, volume 84, pp. 133–142. PMLR, 2018. Kennedy, M. C. and O’Hagan, A. Predicting the output from a complex computer code when fast approxi- mations are available. Biometrika, 87(1):1–13, 2000. Klein, A., Falkner, S., Bartels, S., Hennig, P., and Hutter, F. Fast Bayesian optimization of machine learning hyperparameters on large datasets. In Proceedings of the 20th International Conference on Artiﬁcial Intelligence and Statistics , volume 54, pp. 528–536. PMLR, 2017. Lam, R., Allaire, D. L., and Willcox, K. E. Multiﬁdelity optimization using statistical surrogate modeling for non-hierarchical information sources. In Proceedings of the 56th AIAA/ASCE/AHS/ASC Structures, Struc- tural Dynamics, and Materials Conference , pp. 0143. American Institute of Aeronautics and Astronautics, 2015. McLeod, M., Osborne, M. A., and Roberts, S. J. Practical Bayesian optimization for variable cost objectives. arXiv:1703.04335, 2018. Michalowicz, J. Handbook of Diﬀerential Entropy. Chapman and Hall/CRC, New York, 2014. Minka, T. P. Expectation propagation for approximate Bayesian inference. In Proceedings of the 17th Conference in Uncertainty in Artiﬁcial Intelligence , pp. 362–369. Morgan Kaufmann Publishers Inc., 2001. Picheny, V., Ginsbourger, D., Richet, Y., and Caplin, G. Quantile-based optimization of noisy computer experiments with tunable precision. Technometrics, 55(1):2–13, 2013. Poloczek, M., Wang, J., and Frazier, P. I. Multi-information source optimization. In Advances in Neural Information Processing Systems 30 , pp. 4288–4298. Curran Associates, Inc., 2017. Rahimi, A. and Recht, B. Random features for large-scale kernel machines. In Advances in Neural Information Processing Systems 20, pp. 1177–1184. Curran Associates, Inc., 2008. Ramprasad, R., Batra, R., Pilania, G., Mannodi-Kanakkithodi, A., and Kim, C. Machine learning in materials informatics: recent applications and prospects. npj Computational Materials , 3(54), 2017. Ru, B., Osborne, M. A., Mcleod, M., and Granziol, D. Fast information-theoretic Bayesian optimisation. In Proceedings of the 35th International Conference on Machine Learning , volume 80, pp. 4384–4392. PMLR, 2018. 17Sen, R., Kandasamy, K., and Shakkottai, S. Multi-ﬁdelity black-box optimization with hierarchical partitions. In Proceedings of the 35th International Conference on Machine Learning , volume 80, pp. 4538–4547. PMLR, 2018. Shah, A. and Ghahramani, Z. Parallel predictive entropy search for batch global optimization of expensive objective functions. In Advances in Neural Information Processing Systems 28 , pp. 3330–3338. Curran Associates, Inc., 2015. Snoek, J., Larochelle, H., and Adams, R. P. Practical bayesian optimization of machine learning algorithms. In Advances in Neural Information Processing Systems 25 , pp. 2951–2959. Curran Associates, Inc., 2012. Song, J., Chen, Y., and Yue, Y. A general framework for multi-ﬁdelity Bayesian optimization with gaussian processes. arXiv:1811.00755, 2018. Srinivas, N., Krause, A., Kakade, S., and Seeger, M. Gaussian process optimization in the bandit setting: No regret and experimental design. In Proceedings of the 27th International Conference on International Conference on Machine Learning, pp. 1015–1022. Omnipress, 2010. Swersky, K., Snoek, J., and Adams, R. P. Multi-task Bayesian optimization. In Advances in Neural Information Processing Systems 26 , pp. 2004–2012. Curran Associates, Inc., 2013. Teh, Y. W., Seeger, M. W., and Jordan, M. I. Semiparametric latent factor models. In Proceedings of the 8th International Conference on Artiﬁcial Intelligence and Statistics , 2005. Tsukada, Y., Beniya, Y., and Koyama, T. Equilibrium shape of isolated precipitates in the α-mg phase. Journal of Alloys and Compounds , 603:65 – 74, 2014. Wang, Z. and Jegelka, S. Max-value entropy search for eﬃcient Bayesian optimization. In Proceedings of the 34th International Conference on Machine Learning , volume 70, pp. 3627–3635. PMLR, 2017. Wigley, P. B., Everitt, P. J., van den Hengel, A., Bastian, J. W., Sooriyabandara, M. A., McDonald, G. D., Hardman, K. S., Quinlivan, C. D., Manju, P., Kuhn, C. C. N., Petersen, I. R., Luiten, A. N., Hope, J. J., Robins, N. P., and Hush, M. R. Fast machine-learning online optimization of ultra-cold-atom experiments. Scientiﬁc Reports, 6:25890, 2016. Wu, J. and Frazier, P. Continuous-ﬁdelity Bayesian optimization with knowledge gradient. In NIPS Workshop on Bayesian Optimization , 2017. Zhang, Y., Hoang, T. N., Low, B. K. H., and Kankanhalli, M. Information-based multiﬁdelity Bayesian optimization. In NIPS Workshop on Bayesian Optimization , 2017. 18A Semiparametric Latent Factor Model and its RFM approxima- tion A.1 Model Deﬁnition Semiparametric Latent Factor Model (SLFM) is a Gaussian process based multiple response model (Teh et al., 2005). SLFM represents each output as a sum of C functions having diﬀerent kernel functions k1,...,k C, where kc : x×x→R is a kernel function. Let wmc ∈R be a weight that the m-th output (ﬁdelity) assigns to the c-th function. By introducing an independent term κcm >0, the kernel function is written as k((x,m),(x′,m′)) = C∑ c=1 (wcmwcm′+ κcmδm=m′)kc(x,x′), where δm=m′ = 1 if m = m′, and 0 otherwise. The parameters wcm and κcm which control dependence between multiple outputs are regarded as hyper-parameters, and standard approaches such as marginal likelihood optimization are often used to set them. A.2 RFM for SLFM Let fx := (f(1) x ,...,f (M) x )⊤be the M-dimensional output vector, and cov(fx,fx′) :=   k((x,1),(x′,1)) ··· k((x,1),(x′,M)) ... ... k((x,M),(x′,1)) ··· k((x,M),(x′,M))   be the M ×M covariance matrix of xand x′. By deﬁning wc := (wc1,...,w cM) and κc := (κc1,...,κ cM), this covariance is written as cov(fx,fx′) = C∑ c=1 (wcw⊤ c + diag(κc))kc(x,x′). Since kc(x,x′) is assumed to be one of stationary kernel functions (e.g., Gaussian kernel), RFM can produce a feature vector representation φc which approximates the kernel function as kc(x,x′) ≈φ⊤ c (x)φc(x). To transform wcw⊤ c + diag(κc) into a form of inner product, we use the Cholesky decomposition wcw⊤ c + diag(κc) = LcL⊤ c , where Lc ∈RM×M is a lower triangular matrix. Then, we obtain cov(fx,fx′) ≈ C∑ c=1 LcL⊤ c ( φ⊤ c (x)φc(x′) ) = C∑ c=1 Ψ⊤ c (x)Ψc(x′) where Ψc(x) := L⊤ c ⊗φc(x). Here, in the last line, we use the mixed-product property of Kronecker product. Then, the m-th column of Ψc(x) is deﬁned as the feature of xfor the m-th ﬁdelity φ(x,m). 19B Proof of Lemma 3.1 Using Bayes’ theorem, we obtain p(f(m) x |f(M) x ≤f∗,Dt) = p(f(M) x ≤f∗|f(m) x ,Dt)p(f(m) x |Dt) p(f(M) x ≤f∗|Dt) . (15) The densities p(f(m) x |Dt) and p(f(M) x ≤f∗|Dt) are directly obtained from the predictive distribution: p(f(m) x |Dt) = φ(γ(m) f(m) x (x))/σ(m) x , p(f(M) x ≤f∗|Dt) = Φ(γ(M) f∗ (x)). (16) In addition, from (5), p(f(M) x ≤f∗|f(m) x ,x,Dt) is written as the cumulative distribution of this Gaussian: p(f(M) x ≤f∗|f(m) x ,Dt) = Φ((f∗−u(x))/s(x)). (17) Substituting (16) and (17) into (15), the entropy is obtained. C Information Gain with Noisy Observation Here, we describe calculation of the mutual information between f∗ and noisy observation y(m) x , where y(m) x := y(m)(x) in this section. The mutual information can be written as the diﬀerence of the entropy: I(f∗; y(m) x |x,Dt) = H(y(m) x |x,Dt) −Ep(f∗|x,Dt) [ H(y(m) x |x,f∗,Dt) ] . (18) The ﬁrst term in the right hand side is H(y(m) x |x,Dt) = log (√ 2πe(σ2(m) x + σ2 noise) ) . (19) Using the sampling approximation of f∗, the second term in (18) is Ep(f∗|x,Dt) [ H(y(m) x |x,f∗,Dt) ] ≈ ∑ f∗∈F∗ 1 |F∗|H(y(m) x |x,f∗,Dt). (20) For any ζ ∈R, deﬁne γ(m) ζ (x) := (ζ−µ(m) x )/σ(m) x , and ρ(m) ζ (x) := (ζ−µ(m) x )/ √ σ2(m) x + σ2 noise. In this case, even for the highest ﬁdelity M, the density p(y(m) x |x,f(M) x ≤f∗,Dt) is not the truncated normal because of the noise term. Using Bayes’ theorem, we decompose this density as p(y(m) x |x,f(M) x ≤f∗,Dt) = p(f(M) x ≤f∗|y(m) x ,x,Dt)p(y(m) x |x,Dt) p(f(M) x ≤f∗|x,Dt) . (21) 20The densities p(y(m) x |x,Dt) and p(f(M) x ≤f∗|x,Dt) are directly obtained from the predictive distribution: p(y(m) x |x,Dt) = 1√ σ2(m) x + σ2 noise φ(ρ(m) y(m) x (x)), p(f(M) x ≤f∗|x,Dt) = Φ(γ(M) f∗ (x)). (22) The joint marginal distribution p(f(M) x ,y(m) x |x,Dt) is written as  y(m) x f(M) x  |x,Dt ∼N    µ(m) x µ(M) x  ,  σ2(m) x + σ2 noise σ2(mM) x σ2(mM) x σ2(M) x    , From this distribution, we obtain p(f(M) x |y(m) x ,x,Dt) as f(M) x |y(m) x ,x,Dt ∼N(unoise(x),s2 noise(x)), where unoise(x) = σ2(mM) x ( y(m) x −µ(m) x ) σ2(m) x + σ2 noise + µ(M) x , s2 noise(x) = σ2(M) x − ( σ2(mM) x )2 σ2(m) x + σ2 noise . Thus, p(f(M) x ≤f∗|y(m) x ,x,Dt) is written as the cumulative distribution of this Gaussian: p(f(M) x ≤f∗|y(m) x ,x,Dt) = Φ(γ′ f∗(x)), (23) where, γ′ f∗(x) := (f∗−unoise(x))/snoise(x). Using (15), (16), and (17) in the proof of Lemma 3.1, the entropy is obtained as H(y(m) x |x,f(M) x ≤f∗,Dt) = − ∫ ZΦ ( γ′ f∗(x) ) φ ( ρ(m) y(m) x (x) ) ·log ( ZΦ ( γ′ f∗(x) ) φ ( ρ(m) y(m) x (x) )) dy(m) x , (24) where Z := 1 / √ σ2(m) x + σ2 noiseΦ(γ(M) f∗ (x)). The integral in (24) can be calculated by using numerical integration in the same way as (6). Using I(f∗; y(m) x ) instead of I(f∗; f(m) x ) would be more natural when the observations are assumed to contain the observation noise with large variance σ2 noise, but in practice, diﬀerence of these two formulations would not largely eﬀect on performance of BO when σ2 noise is small. Note that the mutual information of parallel querying I(f∗; f(m) x |Dt,fQ) can be replaced with the noisy observation I(f∗; y(m) x |Dt,fQ) by using same procedure. 21D Additional Information for Parallel Querying D.1 Proof of Lemma 3.2 The ﬁrst term of (8) is EfQ|Dt [ H(f(m) x |Dt,fQ) ] = EfQ|Dt [ log ( σ(m) x|fQ √ 2πe )] = log ( σ(m) x|fQ √ 2πe ) . The last equation holds since σ(m) x|fQ does not depend on fQ. The second term of (8) is written as EfQ,f∗|Dt [ H(f(m) x |Dt,fQ,f(M) x ≤f∗) ] = − ∫ ∫ p(fQ,f∗|Dt) ∫ p(f(m) x |Dt,fQ,f(M) x ≤f∗) logp(f(m) x |Dt,fQ,f(M) x ≤f∗)df(m) x dfQdf∗. (25) For the conditional distribution f(M) x |Dt,fQ,f(m) x ∼N(up(x),s2 p(x)), the mean and the variance function can be written as up(x) = σ2(mM) x|fQ ( f(m) x −µ(m) x|fQ ) σ2(m) x|fQ + µ(M) x|fQ , s2 p(x) = σ2(M) x|fQ − ( σ2(mM) x|fQ )2 / σ2(m) x|fQ . Then, from Bayes’ theorem, we see p(f(m) x |Dt,fQ,f(M) x ≤f∗) = p(f(M) x ≤f∗|Dt,f(m) x ,fQ)p(f(m) x |Dt,fQ) p(f(M) x ≤f∗|Dt,fQ) = Φ ( f∗−up(x) sp(x) ) φ ( f(m) x −µ(m) x|fQ σ(m) x|fQ ) σ(m) x|fQ Φ ( f∗−µ(M) x|fQ σ(M) x|fQ ) . (26) By deﬁning A:= σ2(mM) x|fQ σ2(m) x|fQ , we can re-write ˜f∗−up(x) = ˜f∗−A˜f(m) x , and then, (26) is transformed into Φ (˜f∗−A˜f(m) x sp(x) ) φ ( ˜f(m) x σ(m) x|fQ ) σ(m) x|fQ Φ ( ˜f∗ σ(M) x|fQ ) =: η( ˜f∗, ˜f(m) x ). 22By further deﬁning h( ˜f∗, ˜f(m) x ) := η( ˜f∗, ˜f(m) x ) logη( ˜f∗, ˜f(m) x ), we simplify (25) as follows − ∫ ∫ p(fQ,f∗|Dt) ∫ h( ˜f∗, ˜f(m) x )df(m) x dfQdf∗. (27) This indicates that the most inner integrand can be shown as a function which only depends two random variables ˜f∗and ˜f(m) x . We change the variables of integration from ( f∗,f(m) x ,f⊤ Q)⊤to ( ˜f∗, ˜f(m) x ,f⊤ Q)⊤. J:=   ∂˜f∗ ∂f∗ ∂˜f∗ ∂f(m) x ∂˜f∗ ∂f⊤ Q ∂˜f(m) x ∂f∗ ∂˜f(m) x ∂f(m) x ∂˜f(m) x ∂f⊤ Q ∂fQ ∂f∗ ∂fQ ∂f(m) x ∂fQ ∂f⊤ Q   =  I2 ΣM,QΣ−1 Q 0 I|Q|   where I2 and I|Q|are the identity matrices with size 2 and |Q|, respectively. Note that determinant of J is |J|= 1. Thus, by changing variables of integration and variables of the densities, (27) can be transformed into − ∫ ∫ p(fQ,f∗|Dt) ∫ h( ˜f∗, ˜f(m) x )df(m) x dfQdf∗= − ∫ ∫ p(fQ, ˜f∗|Dt) ∫ h( ˜f∗, ˜f(m) x )d ˜f(m) x dfQd ˜f∗ = − ∫ p( ˜f∗|Dt) ∫ h( ˜f∗, ˜f(m) x )d ˜f(m) x d ˜f∗ = −E˜f∗|Dt [∫ h( ˜f∗, ˜f(m) x )d ˜f(m) x ] . (28) D.2 Analytical Calculation of Entropy for m = M When m= M, the most inner integral in (25) can be further simpliﬁed because it is equal to the entropy of the truncated normal p(f(M) x |Dt,fQ,f(M) x ≤f∗), which is written as − ∫ p(f(M) x |Dt,fQ,f(M) x ≤f∗) logp(f(M) x |Dt,fQ,f(M) x ≤f∗)df(M) x = log  √ 2πeσ(M) x|fQ Φ   ˜f∗ σ(M) x|fQ    − ˜f∗ σ(M) x|fQ φ ( ˜f∗ σ(M) x|fQ ) 2Φ ( ˜f∗ σ(M) x|fQ ) =: ω( ˜f∗), By using the same change of variables as (28), we obtain − ∫ ∫ p(fQ,f∗|Dt)ω( ˜f∗)dfQdf∗= −E˜f∗|Dt [ ω( ˜f∗) ] . D.3 Algorithm As shown in Algorithm 2, the acquisition function maximization is performed when a worker becomes available. The sampling of ˜f∗∈ ˜F∗is performed through an RFM approximation of MF-GPR: w⊤φ(x,m). For the 23Algorithm 2 Parallel MF-MES 1: function Parallel MF-MES(D0,M, X,{λ(m)}M m=1) 2: for t= 0,...,T do 3: Wait for a worker to be available 4: Generate ˜F∗from RFM 5: (xt+1,mt+1) ←argmaxx∈X,m InfoGain(x, m, ˜F∗, Dt) / λ(m) 6: Dt+1 ←Dt ∪(xt+1,y(mt+1)(xt+1),mt+1) 7: end for 8: end function 9: function InfoGain(x, m, F∗, Dt) 10: Calculate µ(m) x|fQ and σ(m) x|fQ 11: Set H0 ←log ( σx|fQ √ 2πe ) 12: if m̸= M then 13: Calculate µ(M) x|fQ ,σ(m) x|fQ , and σ2(mM) x|fQ 14: end if 15: Set H1 ←(14) 16: Return H0 −H1 17: end function entropy calculation in line 19, one dimensional numerical integration is necessary for the integral in (14) when m̸= M, while the analytical formula is available when m= M as shown in Appendix D.2. D.4 Synchronous Parallelization D.4.1 Single-ﬁdelity Setting In the main text, we focus on the asynchronous setting because of the diversity of sampling costs in MFBO. On the other hand, many parallel BO studies on the single-ﬁdelity setting consider the synchronous setting (Figure 4). To our knowledge, a parallel extension of MES has not been studies even in the single-ﬁdelity setting. Our approach is actually applicable to deﬁning the single ﬁdelity acquisition function. Although our main focus is in MFBO, we here show a counterpart of our multi-ﬁdelity acquisition function in the single ﬁdelity setting. Suppose that we need to select q points written as Q= {x1,..., xq}for the single ﬁdelity parallel BO. Unlike the asynchronous setting, q points is needed to be selected simultaneously. By setting fQ:= (fx1 ,...,f xq )⊤, a natural extension of MES for synchronous single-ﬁdelity setting is written as I(f∗; fQ|Dt) := H(fQ|Dt) −EfQ|Dt [H(fQ|fQ≤f∗,Dt)] . (29) 24TimeWorkers Query 1 Query 5 Query 3 Query 4 Query 2 Query 6 Figure 4: Synchronous setting in parallel BO. Note that we impose the condition fQ≤f∗, indicating that all the elements of fQis less than or equal to f∗, instead of fx ≤f∗in the usual MES. The ﬁrst term is the entropy of the q-dimensional Gaussian distribution which can be analytically calculated. The second term is the entropy of the multi-variate truncated normal distribution, for which we show analytical and approximate approaches to the computation. First, we consider the analytical approach. The density p(fQ|Dt) is the predictive distribution of GPR, and we deﬁne µQand ΣQas the mean and covariance matrix, respectively. The truncated normal in the second term is deﬁned through this density as follows p(fQ|fQ≤f∗,Dt) =    p(fQ|Dt)/Z, if fQ≤f∗, 0, otherwise, (30) where Z := ∫ fQ≤f∗ p(fQ|Dt)dfQ. We refer to the truncated normal (30) as TN(µTN Q ,ΣTN Q ), where µTN Q and ΣTN Q are the mean and covariance matrix, respectively. Let ETN be the expectation by the density (30). Then, the entropy in the second term of (29) is re-written as H[fQ|D,fQ≤f∗] = − ∫ fQ≤f∗ p(fQ|Dt) Z log p(fQ|Dt) Z dfQ = −ETN [ log p(fQ|Dt) Z ] = −ETN [ log p(fQ|Dt) −log Z ] = −ETN [ log p(fQ|Dt) ] + logZ = −ETN [ −1 2 log |2πΣQ|− 1 2(fQ−µQ)⊤Σ−1 Q (fQ−µQ) ] + logZ = 1 2 log |2πΣQ|+ 1 2 ETN [ (fQ−µQ)⊤Σ−1 Q (fQ−µQ) ]    =:B + logZ. 25By deﬁning d= µTN Q −µQ, we see B = ETN [ Tr ( Σ−1 Q (fQ−µQ)(fQ−µQ)⊤)] = Tr ( Σ−1 Q ETN [ (fQ−µQ)(fQ−µQ)⊤]) = Tr ( Σ−1 Q ETN [ (fQ−µTN Q + d)(fQ−µTN Q + d)⊤]) = Tr ( Σ−1 Q ETN [ (fQ−µTN Q )(fQ−µTN Q )⊤+ d(fQ−µTN Q )⊤+ (fQ−µTN Q )d⊤+ dd⊤]) . Since ETN[(fQ−µQ)] = 0, we further obtain B = Tr ( Σ−1 Q ETN [ (fQ−µTN Q )(fQ−µTN Q )⊤+ dd⊤]) = Tr ( Σ−1 Q (ΣTN Q + dd⊤) ) Therefore, we obtain H[fQ |D,fQ ≤f∗] = 1 2 ( log |2πΣQ|+ Tr ( Σ−1 Q (ΣTN Q + dd⊤) )) + logZ. If Z, µTN Q , and ΣTN Q are available, the above equation is easily calculated. The normalization term Z is the q-dimensional Gaussian CDF, for which a lot of fast computation algorithms have been proposed (e.g., Genz, 1992; Genton et al., 2017). A method proposed by (Genz, 1992) has been widely used, which requires O(q2) computations. For µTN Q , and ΣTN Q , G & Wilhelm (2012) shows analytical formulas which also depend on the multivariate Gaussian CDF. This needs q times computations of the q−1 dimensional CDF, and q(q−1) times computations of the q−2 dimensional CDF. To avoid many computations of q−1 dimensional CDF, we can introduce approximation of the entropy calculation or greedy selection of Q. As a fast approximation, expectation propagation (EP) can be used to replace the truncated normal distribution with a Gaussian distribution, which makes the entropy calculation analytical. The similar technique is also used in (Hern´ andez-Lobato et al., 2014). For the greedy strategy, we can choose a next point to add Qby maximizing I(f∗; fx |Dt,f˜Q), where ˜Qis a set of ( x,m) already determined to be included in Q. This information can be evaluated by the same way as we saw in the asynchronous setting (8) because the equation has the same form of conditional mutual information. D.4.2 Multi-ﬁdelity Setting Combining the synchronous setting with multi-ﬁdelity functions m= 1,...,M results in a combinatorial selection of Q= {(x1,m1),..., (xq,mq)}because of the discreteness of the ﬁdelity level m. When a simple greedy strategy is employed to select Q, the procedure is reduced to the almost the same procedure as the synchronous single ﬁdelity case described above. This indicates that we can avoid the q dimensional integral by using the technique shown in Section 3.2. 26E Incorporating Fidelity Feature Our proposed method is applicable to the case that the ﬁdelity is deﬁned as a point of a ﬁdelity feature (FF) space Zinstead of the discrete ﬁdelity level 1 ,...,M (Kandasamy et al., 2017). Let f(z) x be the predictive distribution for the ﬁdelity z∈Z. The goal is to solve maxx∈Xf(z∗) x , where z∗∈Z is the highest ﬁdelity to be optimized. For example, in the neural network hyper-parameter optimization, Zcan be a two dimensional space deﬁned by the number of training data and the number of training iterations. In this case, our acquisition function (1) is extended to a(x,z) := I(f∗; f(z) x ) / λ(z), (31) where f∗:= maxx∈Xf(z∗) x in this case, and λ(z) is known cost for z∈Z. As with (Kandasamy et al., 2017), we represent the output f(z) x as a Gaussian process on the direct product space X×Z . Suppose that the observed training data set is written as Dn = {(xi,y(zi)(xi),zi)}n i=1, where y(zi)(xi) is an observation of xi at the ﬁdelity zi. A standard approach to deﬁning a kernel on the joint space X×Z is to use the product form k((xi,zi),(xj,zj)) = kx(xi,xj) kz(zi,zj), where kx : X×X→ R is a kernel for the input space X, and kz : Z×Z→ R is a kernel for the ﬁdelity space Z. Based on this kernel, predictive distribution of GPR can be deﬁned for any pair of ( x,z), and thus the numerator of (31) can be calculated by using the same approach as I(f∗; f(m) x ) which we describe in Section 3.1. Parallelization can also be considered in this FF-based case. For the asynchronous setting, the acquisition function is apara(x,z) = I(f∗; f(z) x |Dt,fQ)/λ(z), in which information gain is conditioned on the set of points currently under evaluation Q= {(x1,m1),..., (xq−1,mq−1)}. As in the sequential case above, the calculation of this acquisition function is almost same as the discrete case in Section 3.2. For the synchronous case, the same discussion as Appendix D.4 also holds. F Summary of Settings in Sequential/Parallel MFBO A possible combination of the single/multiple ﬁdelity and sequential/parallel querying are summarized in Table 1. Our main focus is in FF-free MFBO, and FF-free parallel MFBO with asynchronous querying. In particular, for parallel MFBO, except for the FF-based synchronous querying, no prior works exist to our knowledge. 27Table 1: Summary of possible settings. “FF-based” indicates the setting that the ﬁdelity feature z is available, while “FF-free” does not assume it. Synchronous querying is denoted as ’sync’, and asynchronous querying is denoted as ’asyn’. Fidelity (S)equential/ Our description Note (P)arallel Parallel BO Single P (sync) Appendix D.4.1 - Single P (asyn) Special case of Parallel MF-MES - MFBO Multiple (FF-based) S Appendix E - Multiple (FF-free) S MF-MES described in Section 3.1 - Parallel MFBO Multiple (FF-based) P (sync) Appendix E (Wu & Frazier, 2017) Multiple (FF-based) P (asyn) Appendix E No prior work Multiple (FF-free) P (sync) Appendix D.4.2 No prior work Multiple (FF-free) P (asyn) Parallel MF-MES described in Section 3.2 No prior work G Additional Information of Empirical Evaluation G.1 Other Experimental Settings G.1.1 Settings of Methods We trained the GPR model using normalized training observations (mean 0, and standard deviation 1), other than the GP-based synthetic function. Model hyper-parameters were optimized by marginal-likelihood at every 5 iterations. For the GP-based synthetic function, we set the GPR hyper-parameters as parameters used for sampling the function. For the initial observations, we employed the Latin hypercube approach shown by (Huang et al., 2006). The number of initial training points x∈X⊂ Rd were set as follows: • 5d and 4d for m= 1 and 2, respectively, if M = 2 • 6d, 3d and 2d for m= 1,2 and 3, respectively, if M = 3 • 10d, 7d and 3d for m= 1,2 and 3, respectively, in the material dataset We used the Gaussian kernel k(x,x′) = exp(−∑d i=1(xi −x′ i)2/(2ℓ2 i)) for all kernels. The length scale parameter ℓd was optimized through marginal-likelihood in the following interval: • ℓd ∈[Domain size/10,Domain size ×10] for the GP-based synthetic function and the benchmark functions, here Domain size is the diﬀerence between the maximum and the minimum of the input domain in each dimension. The input domain of each function is shown in Appendix G.1.2. • ℓd ∈[10−3,10−1] for the material dateset • The task kernel in BOCA: ℓd ∈[2,(M −1) ×10] for benchmark functions, and ℓd ∈[10,103] for the material dataset 28The noise parameter of GPR was ﬁxed as σ2 noise = 10−6. The number of kernels in SLFM was C = 2. The hyper-parameters in covariance among diﬀerent output dimension were also optimized through marginal- likelihood in the following interval: • wc1 ∈[ √ 0.75,1] for c= 1,2 • wc2 ∈[− √ 0.25, √ 0.25] for c= 1,2 • κcm ∈[10−3,10−1] for c= 1,2 and m= 1,...,M The number of basis D in RFM was 1000, which was used by MF-MES, MF-PES, MES-LP, and AsyTS. The number of samplings for f∗in MES and PES was 10. For all compared methods, including BOCA, MFSKO, local penalization in MES-LP, GP-UCB-PE, and AsyTS, we followed the settings of hyper-parameters in their original papers. G.1.2 Details of Benchmark Datasets GP-based Synthetic functions We used RFM for SLFM described in Appendix A.2. The input dimension is d = 3 and the domain is xi ∈[0,1]. The parameters are C = 1,w = (0.9,0.9)⊤,κ = (0.1,0.1)⊤, and ℓi = 0.1 for i= 1,2,3. Styblinski-Tang function f(1) = 1 2 2∑ i=1 (0.9x4 i −15x2 i + 6xi), f(2) = 1 2 2∑ i=1 (x4 i −16x2 i + 5xi), xi ∈[−5,5],i = 1,2 29HartMann6 function f(1) = − 4∑ i=1 (αi −0.2) exp ( − 6∑ j=1 Aij(xj −Pij)2 ) , f(2) = − 4∑ i=1 (αi −0.1) exp ( − 6∑ j=1 Aij(xj −Pij)2 ) f(3) = − 4∑ i=1 αiexp ( − 6∑ j=1 Aij(xj −Pij)2 ) α= [1.0,1.2,3.0,3.2]⊤ A=   10 3 17 3 .5 1 .7 8 0.05 10 17 0 .1 8 14 3 3 .5 1 .7 10 17 8 17 8 0 .05 10 0 .1 14   P = 10−4   1312 1696 5569 124 8283 5886 2329 4135 8307 3736 1004 9991 2348 1451 3522 2883 3047 6650 4047 8828 8732 5743 1091 381   xj ∈[0,1],j = 1,..., 6 Materials Data As an example of practical application, we applied our method to the parameter opti- mization of computational simulation model in materials science. There is a computational model (Tsukada et al., 2014) that predicts equilibrium shape of precipitates in the α-Mg phase when material parameters are given. We estimate two material parameters (lattice mismatch and interface energy between the α-Mg and precipitate phases) from experimental data on precipitate shape measured by transmission electron microscopy (TEM) (Bhattacharjee et al., 2013). The objective function is the discrepancy between precipitate shape predicted by the computational model and one measured by TEM. G.2 Measuring Computational Time of Acquisition Functions We measured the computational time for the maximization of the acquisition functions. We assume that the predictive distribution of the GPR model is already obtained, because it is almost common for all the methods. The training dataset is created by the initialization process in our experiment described in Appendix G.1. Figure 5 shows the results on three benchmark dataset, used in the main text. BOCA and MFSKO are relatively easy to compute because they are based on UCB and EI, respectively. Their acquisition function is simple, but diﬃcult to incorporate global utility of the candidate without tuning parameters as we discuss in the main text. MF-MES was much faster than MF-PES. We emphasize that MF-PES employs the approximation based on EP to accelerate the computation, unlike our MF-MES which is almost 30MF-MES MF-PES BOCA MFSKO 0 25 50 75 100 125 150Time (sec) (a) Styblinski-Tang MF-MES MF-PES BOCA MFSKO 0 100 200 300 400 500 600Time (sec) (b) HartMann6 Figure 5: Computational time for acquisition function maximization. analytical. This indicates that MF-MES provides more reliable entropy computation with smaller amount of computations than MF-PES. 31
```

## **Instructions:**
1. Analyze the paper based on the categories listed below.
2. Your response **must be a valid JSON object** that can be directly parsed using `json.loads()`.
3. Do not include any extra text, explanations, or formatting outside of the JSON object.
4. **If a field has no available information, set its value to `"Not mentioned"` instead of leaving it empty.**
5. Ensure that the JSON format is correct, including the use of **double quotes (`"`) for all keys and values.**
## **Output Format (JSON)**:
```json
{
    "main_contributions": "<Concise description of the main research problem and contributions>",
    "methodology": "<Brief explanation of the key techniques, models, or algorithms>",
    "experimental_setup": "<Description of datasets, benchmarks, and validation methods>",
    "limitations": "<Summary of weaknesses, constraints, or assumptions>",
    "future_research_directions": "<Potential areas for extending this research>"
}
```
Output:
{
    "main_contributions": "The paper introduces Multi-Fidelity Max-value Entropy Search (MF-MES), an information-theoretic efficient Multi-Fidelity Bayesian Optimization (MFBO) method. It addresses the computational difficulty of estimating information gain in existing information-based MFBO by leveraging Max-value Entropy Search (MES), which considers the entropy of the optimal function value (f*) instead of the optimal input point (x*). This approach reduces most additional computations to analytical expressions and one-dimensional numerical integrations. Furthermore, the paper proposes an information-theoretic asynchronous parallelization of MF-MES, which efficiently handles multiple queries with varying sampling costs. The effectiveness of MF-MES is demonstrated on benchmark datasets and a real-world materials science application.",
    "methodology": "The proposed MF-MES method for sequential querying defines an acquisition function as the mutual information between the optimal value of the highest fidelity function (f*) and an observation of an arbitrary fidelity (f(m)x), divided by its querying cost λ(m). The mutual information is computed as the difference of entropies: H(f(m)x | Dt) - E[H(f(m)x | f*, Dt)]. The first term is analytical. The expectation in the second term is approximated via Monte Carlo sampling of f* from the current Gaussian Process Regression (GPR). For the highest fidelity (m=M), H(f(M)x | f*, Dt) is derived from a truncated normal distribution. For lower fidelities (m≠M), H(f(m)x | f*, Dt) is obtained through an efficient and accurate one-dimensional numerical integral, facilitated by Lemma 3.1. The underlying GPR model uses a multi-fidelity extension like Semiparametric Latent Factor Model (SLFM). For asynchronous parallelization, the acquisition function extends MF-MES to condition on fQ (values of objective functions currently being evaluated by other workers), which also reduces to at most two-dimensional integrals with parts calculable via one-dimensional numerical integration (Lemma 3.2). Sampling of f* (and its parallel counterpart ~f*) is performed using Gumbel distribution or Random Feature Maps (RFM).",
    "experimental_setup": "The performance of MF-MES was evaluated using Simple Regret (SR) and Inference Regret (IR) against the total cost. The Multi-Fidelity Gaussian Process Regression (MF-GPR) model employed SLFM with a Gaussian kernel and Automatic Relevance Determination (ARD). Initial observations were generated using a Latin hypercube approach. Three types of datasets were used: a GP-based synthetic function (3D, 2 fidelities), two benchmark functions (Styblinski-Tang with 2 fidelities and HartMann6 with 3 fidelities), and a real-world materials science dataset (parameter optimization with 3 fidelities). Sampling costs (λ(m)) were defined for each fidelity level. For sequential querying, MF-MES was compared against MF-SKO, BOCA, MF-PES, and single-fidelity MES. For parallel querying, it was compared against MES with local penalization (MES-LP), GP-UCB with pure exploration (GP-UCB-PE), asynchronous parallel Thompson sampling (AsyTS), sequential MF-MES, and a parallel extension of single-fidelity MES. Experiments were repeated 100 times for synthetic data and 10 times for other datasets.",
    "limitations": "The method employs an approximation where the conditional distribution H(f(m)x | f*, Dt) is replaced by p(f(m)x | f(M)x ≤ f*, Dt), which is a common simplification in entropy-based BO methods. Monte Carlo estimation is used for the expectation over f*, though this is argued to be accurate due to its one-dimensional nature. The Gumbel approximation for f* sampling is based on an independent approximation of GPR. While the paper highlights MF-MES's computational simplicity compared to other methods, it does not explicitly discuss inherent limitations of its own approach besides these approximations. It also observes that noisy observations in real-world data can cause some instability in regret metrics, including for MF-MES.",
    "future_research_directions": "Future research could explore and incorporate other reliable approximation techniques for further acceleration of MF-MES, especially for scenarios where the reliability of existing approximations (e.g., density approximations by normal distributions) is not fully understood. Further investigation into the multi-fidelity setting with a continuous 'fidelity feature' (FF) space Z, and robust adaptation of MF-MES in this context, could be a fruitful direction. Additionally, for synchronous parallel Bayesian optimization, developing more efficient or robust strategies for handling the computational burden of multivariate Gaussian CDFs, or alternative approximation methods for entropy calculation in that setting, remains an open area."
}
