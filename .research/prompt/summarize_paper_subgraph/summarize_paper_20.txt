
Input:

You are an expert research assistant responsible for summarizing a research paper that will serve as the foundation (Research A) for further exploration and integration.

Your task is to generate a structured summary of the given research paper with a focus on:
- **Technical Contributions**: Identify the main research problem and key findings.
- **Methodology**: Describe the techniques, models, or algorithms used.
- **Experimental Setup**: Outline the datasets, benchmarks, and validation methods.
- **Limitations**: Highlight any weaknesses, constraints, or assumptions.
- **Future Research Directions**: Suggest possible extensions or new areas for research.

Below is the full text of the research paper:

```
Conformal inference is (almost) free for neural networks trained with early stopping Ziyi Liang∗, Yanfei Zhou†, Matteo Sesia† June 28, 2023 Abstract Early stopping based on hold-out data is a popular regularization technique designed to mitigate overfitting and increase the predictive accuracy of neural networks. Models trained with early stopping often provide relatively accurate predictions, but they generally still lack precise statistical guarantees unless they are further calibrated using independent hold-out data. This paper addresses the above limitation with conformalized early stopping : a novel method that combines early stopping with conformal calibration while efficiently recycling the same hold-out data. This leads to models that are both accurate and able to provide exact predictive inferences without multiple data splits nor overly conservative adjustments. Practical implementations are developed for different learning tasks—outlier detection, multi-class classification, regression— and their competitive performance is demonstrated on real data. 1 Introduction Deep neural networks can detect complex data patterns and leverage them to make accurate pre- dictions in many applications, including computer vision, natural language processing, and speech recognition, to name a few examples. These models can sometimes even outperform skilled hu- mans [1], but they still make mistakes. Unfortunately, the severity of these mistakes is compounded by the fact that the predictions computed by neural networks are often overconfident [2], partly due to overfitting [3, 4]. Several training strategies have been developed to mitigate overfitting, including dropout [5], batch normalization [6], weight normalization [7], data augmentation [8], and early stopping [9]; the latter is the focus of this paper. Early stopping consists of continuously evaluating after each batch of stochastic gradient updates (or epoch) the predictive performance of the current model on hold-out independent data. After a large number of gradient updates, only the intermediate model achieving the best performance on the hold-out data is utilized to make predictions. This strategy is often effective at mitigat- ing overfitting and can produce relatively accurate predictions compared to fully trained models, but it does not fully resolve overconfidence because it does not lead to models with finite-sample guarantees. ∗Department of Mathematics, University of Southern California, Los Angeles, CA, USA. †Department of Data Sciences and Operations, University of Southern California, Los Angeles, CA, USA. 1 arXiv:2301.11556v2  [stat.ML]  26 Jun 2023A general framework for quantifying the predictive uncertainty of anyblack-box machine learning model is that of conformal inference [10]. The idea is to apply a pre-trained model to a calibration set of hold-out observations drawn at random from the target population. If the calibration data are exchangeable with the test point of interest, the model performance on the calibration set can be translated into statistically rigorous predictive inferences. This framework is flexible and can accommodate different learning tasks, including out-of-distribution testing [11], classification [12], and regression [10]. For example, in the context of classification, conformal inference can give prediction sets that contain the correct label for a new data point with high probability. In theory, the quality of the trained model has no consequence on the average validity of conformal inferences, but it does affect their reliability and usefulness on a case-by-case level. In particular, conformal uncertainty estimates obtained after calibrating an overconfident model may be too conservative for some test cases and too optimistic for others [13]. The goal of this paper is to combine conformal calibration with standard early stopping training techniques as efficiently as possible, in order to produce more reliable predictive inferences with a finite amount of available data. Achieving high accuracy with deep learning often requires large training sets [14], and conformal inference makes the overall pipeline even more data-intensive. As high-quality observations can be expensive to collect, in some situations practitioners may naturally wonder whether the advantage of having principled uncertainty estimates is worth a possible reduction in predictive accuracy due to fewer available training samples. This concern is relevant because the size of the calibration set cannot be too small if one wants stable and reliable conformal inferences [15, 16]. In fact, very large calibration sets may be necessary to obtain stronger conformal inferences that are valid not only on average but also conditionally on some important individual features; see Vovk et al. [12], Romano et al. [17], and Barber et al. [18]. This paper resolves the above dilemma by showing that conformal inferences for deep learning models trained with early stopping can be obtained almost “for free”—without spending more precious data. More precisely, we present an innovative method that blends model training with early stopping and conformal calibration using the same hold-out samples, essentially obtaining rigorous predictive inferences at no additional data cost compared to standard early stopping. It is worth emphasizing this result is not trivial. In fact, naively applying existing conformal calibration methods using the same hold-out samples utilized for early stopping would not lead to theoretically valid inferences, at least not without resorting to very conservative corrections. The paper is organized as follows. Section 2 develops our conformalized early stopping (CES) method, starting from outlier detection and classification, then addressing regression. Section 3 demonstrates the advantages of CES through numerical experiments. Section 4 concludes with a discussion and some ideas for further research. Additional details and results, including a theoretical analysis of the naive benchmark mentioned above, can be found in the Appendices, along with all mathematical proofs. Related Work Conformal inference [10, 19, 20] has become a very rich and active area of research [21–24]. Many prior works studied the computation of efficient conformal inferences starting from pre-trainedblack- box models, including for example in the context of outlier detection [11, 25–27], classification [12, 13, 28–30], and regression [10, 22, 31]. Other works have studied the general robustness of conformal inferences to distribution shifts [32, 33] and, more broadly, to failures of the data exchangeability 2assumption [34, 35]. Our research is orthogonal, as we look inside the black-box model and develop a novel early-stopping training technique that is naturally integrated with conformal calibration. Nonetheless, the proposed method could be combined with those described in the aforementioned papers. Other recent research has explored different ways of bringing conformal inference into the learning algorithms [36–39], and some of those works apply standard early stopping techniques, but they do not address our problem. This paper is related to Yang and Kuchibhotla [40], which proposed a general theoretical ad- justment for conformal inferences computed after model selection. That method could be utilized to account for early stopping without further data splits, as explained in Section 2.2. However, we will demonstrate that even an improved version of such analysis remains overly conservative in the context of model selection via early stopping, and the alternative method developed in this paper performs much better in practice. Our solution is inspired by Mondrian conformal inference [12] as well as by the integrative conformal method of Liang et al. [26]. The latter deals with the problem of selecting the best model from an arbitrary machine learning toolbox to obtain the most pow- erful conformal p-values for outlier testing. The idea of Liang et al. [26] extends naturally to the early stopping problem in the special cases of outlier detection and classification, but the regression setting requires substantial technical innovations. The work of Liang et al. [26] is also related to Marandon et al. [41], although the latter is more distant from this paper because it focuses on theoretically controlling the false discovery rate [42] in multiple testing problems. Finally, this paper draws inspiration from Kim et al. [43], which shows that models trained with bootstrap (or bagging) techniques can also lead to valid conformal inferences essentially for free. 2 Methods 2.1 Standard Conformal Inference and Early Stopping Consider n data points, Zi for i ∈ D= [n] = {1, . . . , n}, sampled exchangeably (e.g., i.i.d.) from an unknown distribution PZ with support on some space Z. Consider also an additional test sample, Zn+1. In the context of outlier detection, one wishes to test whether Zn+1 was sampled exchangeably from PZ. In classification or regression, one can write Zi = (Xi, Yi), where Xi is a feature vector while Yi is a discrete category or a continuous response, and the goal is to predict the unobserved value of Yn+1 given Xn+1 and the data in D. The standard pipeline begins by randomly splitting the data in D into three disjoint subsets: Dtrain, Des, Dcal ⊂ [n]. The samples in Dtrain are utilized to train a model M via stochastic gradient descent, in such a way as to (approximately) minimize the desired loss L, while the observations in Des and Dcal are held out. We denote by Mt the model learnt after t epochs of stochastic gradient descent, for any t ∈ [tmax], where tmax is a pre-determined maximum number of epochs. For sim- plicity, L is assumed to be an additive loss, in the sense that its value calculated on the training data after t epochs is Ltrain(Mt) = P i∈Dtrain ℓ(Mt; Zi), for some appropriate function ℓ. For example, a typical choice for regression would be the squared-error loss: ℓ(Mt; Zi) = [Yi − ˆµ(Xi; Mt)]2, where ˆµ(Xi; Mt) indicates the value of the regression function at Xi, as estimated by Mt. Similarly, the loss evaluated onDes is denoted as Les(Mt) = P i∈Des ℓ(Mt; Zi). After training for tmax epochs, early stopping selects the model ˆMes that minimizes the loss on Des: ˆMes = arg minMt : 0≤t≤tmax Les(Mt). Conformal calibration of ˆMes is then conducted using the independent hold-out data set Dcal, as sketched in Figure 1 (a). This pipeline requires a three-way data split because: (i) Dtrain and Des 3must be disjoint to ensure the early stopping criterion is effective at mitigating overfitting; and (ii) Dcal must be disjoint from Dtrain ∪ Des to ensure the performance of the selected model ˆMes on the calibration data gives us an unbiased preview of its future performance at test time, enabling valid conformal inferences. data set D Dtrain candidate models Des Dcal selected model test point conformal inference (a) Conformal inference after early stopping. data set D Dtrain candidate models Des-cal selected model test point conformal inference (b) Conformalized early stopping (CES). Figure 1: Schematic visualization of rigorous conformal inferences for models trained with early stopping. (a) Conventional pipeline requiring a three-way sample split. (b) Conformalized early stopping, requiring only a two-way split. 2.2 The Limitations of a Naive Benchmark An intuitive alternative to the standard approach described in the previous section is to naively perform standard conformal inference using the same hold-out samples utilized for early stopping, as sketched in Figure 2. This approach, which we call the naive benchmark, may seem appealing as it avoids a three-way data split, but it does not provide rigorous inferences. In fact, the necessary exchangeability with the test point is broken if the same hold-out data are used twice—first to evaluate the early stopping criterion and then to perform conformal calibration. In principle, the issue could be corrected by applying a conservative adjustment to the nominal significance level of the conformal inferences, as studied by Yang and Kuchibhotla [40] and reviewed in Appendix A1. However, this leads to overly conservative inferences in practice when applied with the required theoretical correction, as demonstrated by the numerical experiments summarized in Figure 3, even if a tighter adjustment developed in Appendix A1 is utilized instead of that of Yang and Kuchibhotla [40]. Intuitively, the problem is that there tend to be complicated dependencies among the candidate models provided to the early stopping algorithm, but the available analyses are not equipped to handle such intricacies and must therefore take a pessimistic viewpoint of the model selection process. Thus, the naive benchmark remains unsatisfactory, although it can serve as an informative benchmark for the novel method developed in this paper. Interestingly, we will see empirically that the naive benchmark applied without the required theoretical corrections often performs similarly to the rigorous method proposed in this paper, especially for large data sets. 2.3 Preview of our Contribution This paper develops a novel conformalized early stopping (CES) method to jointly carry out both early stopping and conformal inference using a single hold-out data set, denoted in the following as Des-cal. The advantage of this approach is that it avoids a three-way data split, so that more samples can be allocated to Dtrain, without breaking the required data exchangeability. As a result, 4data set D Dtrain candidate models Des-cal selected model test point conformal inference Figure 2: Schematic visualization of heuristic conformal inferences based on a naive benchmark that utilizes the same hold-out data twice. Coverage (marginal) Coverage (conditional) Width 200 500 1000 2000 200 500 1000 2000 200 500 1000 2000 16 17 18 19 20 21 0.7 0.8 0.9 1.0 0.7 0.8 0.9 1.0 Sample size Method CES Naive + theory Data splitting Full training Figure 3: Average performance, as a function of the sample size, of conformal inferences based on neural networks trained and calibrated with different methods, on the bio regression data [44]. Ideally, the coverage of the conformal prediction intervals should be close to 90% and their width should be small. All methods shown here guarantee 90% marginal coverage. See Table A3 for more detailed results and standard errors. CES often leads to relatively more reliable and informative conformal inferences compared to other existing approaches; e.g., see for example the empirical performance preview shown in Figure 3. The CES method is based on the following idea inspired by Liang et al. [26]. Valid conformal inferences can be obtained by calibrating ˆMes using the same data set Des-cal used for model selection, as long as the test sample Zn+1 is also involved in the early stopping rule exchangeably with all other samples in Des-cal. This concept, sketched in Figure 1 (b), is not obvious to translate into a practical method, however, for two reasons. Firstly, the ground truth for the test point (i.e., its outlier status or its outcome label) is unknown. Secondly, the method may need to be repeatedly applied for a large number of distinct test points in a computationally efficient way, and one cannot re-train the model separately for each test point. In the next section, we will explain how to overcome these challenges in the special case of early stopping for outlier detection; then, the solution will be extended to the classification and regression settings. 2.4 CES for Outlier Detection Consider testing whether Zn+1, is an inlier, in the sense that it was sampled from PZ exchangeably with the data in D. Following the notation of Section 2.1, consider a partition ofD into two subsets, Dtrain and Des-cal, chosen at random independently of everything else, such thatD = Dtrain ∪Des-cal. The first step of CES consists of training a deep one-class classifier M using the data in Dtrain via 5stochastic gradient descent for tmax epochs, storing all parameters characterizing the intermediate model after each τ epochs. We refer to τ ∈ [tmax] as the storage period, a parameter pre-defined by the user. Intuitively, a smaller τ increases the memory cost of CES but may also lead to the selection of a more accurate model. While the memory cost of this approach is higher compared to that of standard early-stopping training techniques, which only require storing one model at a time, it is not prohibitively expensive. In fact, the candidate models do not need to be kept in precious RAM memory but can be stored on a relatively cheap hard drive. As reasonable choices of τ may typically be in the order of T = ⌊tmax/τ⌋ ≈100, the cost of CES is not excessive in many real-world situations. For example, it takes approximately 100 MB to store a pre-trained standard ResNet50 computer vision model, implying that CES would require approximately 10 GB of storage in such applications—today this costs less than $0.25/month in the cloud. After pre-training and storing T candidate models, namely Mt1, . . . , MtT for some sub-sequence (t1, . . . , tT ) of [ tmax], the next step is to select the appropriate early-stopped model based on the hold-out data in Des-cal as well as the test point Zn+1. Following the notation of Section 2.1, define the value of the one-class classification loss L for model Mt, for any t ∈ [T], evaluated on Des-cal as: Les-cal(Mt) = P i∈Des-cal ℓ(Mt; Zi). Further, for any z ∈ Z, define also L+1 es-cal(Mt, z) as: L+1 es-cal(Mt, z) = Les-cal(Mt) + ℓ(Mt; z). (1) Therefore, L+1 es-cal(Mt, Zn+1) can be interpreted as the cumulative value of the loss function calcu- lated on an augmented hold-out data set including alsoZn+1. Then, we select the model ˆMces(Zn+1) minimizing L+1 es-cal(Mt, Zn+1): ˆMces(Zn+1) = arg min Mtj : 1≤j≤T L+1 es-cal(Mtj , Zn+1). (2) Note that the computational cost of evaluating (2) is negligible compared to that of training the models. Next, the selected model ˆMces(Zn+1) is utilized to compute a conformal p-value [27] to test whether Zn+1 is an inlier. In particular, ˆMces(Zn+1) is utilized to compute nonconformity scores ˆSi(Zn+1) for all samples i ∈ Des-cal ∪{n+1}. These scores rank the observations in Des-cal ∪{n+1} based on how the one-class classifier ˆMces(Zn+1) perceives them to be similar to the training data; by convention, a smaller value of ˆSi(Zn+1) suggests Zi is more likely to be an outlier. Suitable scores are typically included in the output of standard one-class classification models, such as those provided by the Python library PyTorch. For simplicity, we assume all scores are almost-surely distinct; otherwise, ties can be broken at random by adding a small amount of independent noise. Then, the conformal p-value ˆu0(Zn+1) is given by the usual formula: ˆu0(Zn+1) = 1 + |i ∈ Des-cal : ˆSi ≤ ˆSn+1| 1 + |Des-cal| , (3) making the dependence of ˆSi on Zn+1 implicit in the interest of space. This method, outlined by Algorithm 1, gives p-values that are exactly valid in finite samples, in the sense that they are stochastically dominated by the uniform distribution under the null hypothesis. 6Algorithm 1 Conformalized early stopping for outlier detection 1: Input: Exchangeable data points Z1, . . . , Zn; test point Zn+1. 2: Input: Maximum number of training epochs tmax; storage period hyper-parameter τ. 3: Input: One-class classifier trainable via (stochastic) gradient descent. 4: Randomly split the exchangeable data points into Dtrain and Des-cal. 5: Train the one-class classifier for tmax epochs and save the intermediate models Mt1, . . . , MtT . 6: Pick the most promising model ˆMces(Zn+1) based on (2), using the data in Des-cal ∪ {n + 1}. 7: Compute nonconformity scores ˆSi(Zn+1) for all i ∈ Des-cal ∪ {n + 1} using model ˆMces(Zn+1). 8: Output: Conformal p-value ˆu0(Zn+1) given by (3). Theorem 1. Assume Z1, . . . , Zn, Zn+1 are exchangeable random samples, and let ˆu0(Zn+1) be the output of Algorithm 1, as given in (3). Then, P[ˆu0(Zn+1) ≤ α] ≤ α for any α ∈ (0, 1). 2.5 CES for Classification The above CES method will now be extended to deal with K-class classification problems, for any K ≥ 2. Consider n exchangeable pairs of observations ( Xi, Yi), for i ∈ D= [n], and a test point (Xn+1, Yn+1) whose label Yn+1 ∈ [K] has not yet been observed. The goal is to construct an informative prediction set for Yn+1 given the observed features Xn+1 and the rest of the data, assuming (Xn+1, Yn+1) is exchangeable with the observations indexed byD. An ideal goal would be to construct the smallest possible prediction set with guaranteedfeature-conditional coverageat level 1 − α, for any fixed α ∈ (0, 1). Formally, a prediction set ˆCα(Xn+1) ⊆ [K] has feature-conditional coverage at level 1 − α if P[Yn+1 ∈ ˆCα(Xn+1) | Xn+1 = x] ≥ 1 − α, for any x ∈ X, where X is the feature space. Unfortunately, perfect feature-conditional coverage is extremely difficult to achieve unless the feature space X is very small [18]. Therefore, in practice, one must be satisfied with obtaining relatively weaker guarantees, such as label-conditional coverage and marginal coverage. Formally, ˆCα(Xn+1) has 1−α label-conditional coverage if P[Yn+1 ∈ ˆCα(Xn+1) | Yn+1 = y] ≥ 1−α, for any y ∈ [K], while marginal coverage corresponds to P[Yn+1 ∈ ˆCα(Xn+1)] ≥ 1 − α. Label- conditional coverage is stronger than marginal coverage, but both criteria are useful because the latter is easier to achieve with smaller (and hence more informative) prediction sets. We begin by focusing on label-conditional coverage, as this follows most easily from the results of Section 2.4. This solution will be extended in Appendix A2 to target marginal coverage. The first step of CES consists of randomly splitting D into two subsets, Dtrain and Des-cal, as in Section 2.4. The samples in Des-cal are further divided into subsets Dy es-cal with homogeneous labels; that is, Dy es-cal = {i ∈ Des-cal : Yi = y} for each y ∈ [K]. The data in Dtrain are utilized to train a neural network classifier via stochastic gradient descent, storing the intermediate candidate models Mt after each τ epochs. This is essentially the same approach as in Section 2.4, with the only difference being that the neural network is now designed to perform K-class classification rather than one-class classification. Therefore, this neural network should have a soft-max layer with K nodes near its output, whose values corresponding to an input data point with features x are denoted as ˆπy(x), for all y ∈ [K]. Intuitively, we will interpret ˆπy(x) as approximating (possibly inaccurately) the true conditional data-generating distribution; i.e., ˆπy(x) ≈ P[Y = y | X = x]. For any model Mt, any x ∈ X, and any y ∈ [K], define the augmented loss L+1 es-cal(Mt, x, y) as: L+1 es-cal(Mt, x, y) = Les-cal(Mt) + ℓ(Mt; x, y). (4) 7Concretely, a typical choice for ℓ is the cross-entropy loss: ℓ(Mt; x, y) = −log ˆπt y(x), where ˆπt denotes the soft-max probability distribution estimated by model Mt. Intuitively, L+1 es-cal(Mt, x, y) is the cumulative value of the loss function calculated on an augmented hold-out data set including also the imaginary test sample ( x, y). Then, for any y ∈ [K], CES selects the model ˆMces(Xn+1, y) minimizing L+1 es-cal(Mt, Xn+1, y) among the T stored models: ˆMces(Xn+1, y) = arg min Mtj : 1≤j≤T L+1 es-cal(Mtj , Xn+1, y). (5) The selected model ˆMces(Xn+1, y) is then utilized to compute a conformal p-value for testing whether Yn+1 = y. In particular, we compute nonconformity scores ˆSy i (Xn+1) for alli ∈ Dy es-cal∪{n+ 1}, imagining that Yn+1 = y. Different types of nonconformity scores can be easily accommodated, but in this paper, we follow the adaptive strategy of Romano et al. [13]. The computation of these nonconformity scores based on the selected model ˆMces is reviewed in Appendix A3. Here, we simply note the p-value is given by: ˆuy(Xn+1) = 1 + |i ∈ Dy es-cal : ˆSy i ≤ ˆSy n+1| 1 + |Dy es-cal| , (6) again making the dependence of ˆSy i on Xn+1 implicit. Finally, the prediction set ˆCα(Xn+1) is constructed by including all possible labels for which the corresponding null hypothesis cannot be rejected at level α: ˆCα(Xn+1) = {y ∈ [K] : ˆuy(Xn+1) ≥ α}. (7) This method, outlined by Algorithm 2, guarantees label-conditional coverage at level 1 − α. Algorithm 2 Conformalized early stopping for multi-class classification 1: Input: Exchangeable data points ( X1, Y1), . . . ,(Xn, Yn) with labels Yi ∈ [K]. 2: Input: Test point with features Xn+1. Desired coverage level 1 − α. 3: Input: Maximum number of training epochs tmax; storage period hyper-parameter τ. 4: Input: K-class classifier trainable via (stochastic) gradient descent. 5: Randomly split the exchangeable data points into Dtrain and Des-cal. 6: Train the K-class classifier for tmax epochs and save the intermediate models Mt1, . . . , MtT . 7: for y ∈ [K] do 8: Define Dy es-cal = {i ∈ Des-cal : Yi = y} and imagine Yn+1 = y. 9: Pick the model ˆMces(Xn+1, y) according to (5), using the data in Des-cal ∪ {n + 1}. 10: Compute scores ˆSy i (Xn+1), ∀i ∈ Dy es-cal ∪ {n + 1}, using ˆMces(Xn+1, y); see Appendix A3. 11: Compute the conformal p-value ˆuy(Xn+1) according to (6). 12: end for 13: Output: Prediction set ˆCα(Xn+1) given by (7). Theorem 2. Assume (X1, Y1), . . . ,(Xn+1, Yn+1) are exchangeable, and let ˆCα(Xn+1) be the output of Algorithm 2, as given in (7), for any given α ∈ (0, 1). Then, P[Yn+1 ∈ ˆCα(Xn+1) | Yn+1 = y] ≥ 1 − α for any y ∈ [K]. 82.6 CES for Regression This section extends CES to regression problems with a continuous outcome. As in the previous sections, consider a data set containing n exchangeable observations (Xi, Yi), for i ∈ D= [n], and a test point (Xn+1, Yn+1) with a latent label Yn+1 ∈ R. The goal is to construct a reasonably narrow prediction interval ˆCα(Xn+1) for Yn+1 that is guaranteed to have marginal coverage above some level 1 − α, i.e., P[Yn+1 ∈ ˆCα(Xn+1)] ≥ 1 − α, and can also practically achieve reasonably high feature- conditional coverage. Developing a CES method for this problem is more difficult compared to the classification case studied in Section 2.5 due to the infinite number of possible values for Yn+1. In fact, a naive extension of Algorithm 2 would be computationally unfeasible in the regression setting, for the same reason why full-conformal prediction [10] is generally impractical. The novel solution described below is designed to leverage the particular structure of an early stopping criterion based on the squared-error loss evaluated on hold-out data. Focusing on the squared-error loss makes CES easier to implement and explain using classical absolute residual nonconformity scores [10, 45]. However, similar ideas could also be repurposed to accommodate other scores, such as those based on quantile regression [31], conditional distributions [46, 47], or conditional histograms [48]. As usual, we randomly split D into Dtrain and Des-cal. The data in Dtrain are utilized to train a neural network via stochastic gradient descent, storing the intermediate models Mt after each τ epoch. The approach is similar to those in Sections 2.4–2.5, although now the output of a model Mt applied to a sample with features x is denoted by ˆµt(x) and is designed to approximate (possibly inaccurately) the conditional mean of the unknown data-generating distribution; i.e., ˆ µt(x) ≈ E[Y | X = x]. (Note that we will omit the superscript t unless necessary to avoid ambiguity). For any model Mt and any x ∈ X, y ∈ R, define L+1 es-cal(Mt, x, y) = Les-cal(Mt) + [y − ˆµt(x)]2. (8) Consider now the following optimization problem, ˆMces(Xn+1, y) = arg min Mtj : 1≤j≤T L+1 es-cal(Mtj , Xn+1, y), (9) which can be solved simultaneously for all y ∈ R thanks to the amenable form of (8). In fact, each L+1 es-cal(Mt, x, y) is a simple quadratic function of y; see the sketch in Figure 4. This implies ˆMces(Xn+1, y) is a step function, whose parameters can be computed at cost O(T log T) with an efficient divide-and-conquer algorithm designed to find the lower envelope of a family of parabolas [49, 50]; see Appendix A4. Therefore, ˆMces(Xn+1, y) has L distinct steps, for some L = O(T log T) that may depend on Xn+1, and it can be written as a function of y as: ˆMces(Xn+1, y) = LX l=1 ml(Xn+1)1 [y ∈ (kl−1, kl]] , (10) where ml(Xn+1) ∈ [T] represents the best model selected within the interval ( kl−1, kl] such that ml(Xn+1) ̸= ml−1(Xn+1) for all l ∈ [L]. Above, k1 ≤ k2 ≤ ··· ≤kL denote the knots of ˆMces(Xn+1, y), which also depend on Xn+1 and are defined as the boundaries in the domain of y between each consecutive pair of steps, with the understanding that k0 = −∞ and kL+1 = +∞. Then, for each step l ∈ [L], let Bl indicate the interval Bl = (kl−1, kl] and, for all i ∈ Des-cal, eval- 9y L +1 es-cal ( M t ,x ,y ) M 1 M 2 M 3 𝑘!𝑘"𝑘# Figure 4: Squared-error loss on test-augmented hold-out data for three alternative regression models M1, M2 and M3, as a function of the place-holder outcome y for the test point. The CES method utilizes the best model for each possible value of y, which is identified by the lower envelope of these three parabolas. In this case, the lower envelope has two finite knots at k1 and k3. uate the nonconformity score ˆSi(Xn+1, Bl) for observation ( Xi, Yi) based on the regression model indicated by ml(Xn+1); i.e., ˆSi(Xn+1, Bl) = |Yi − ˆµml(Xn+1)(Xi)|. (11) Let ˆQ1−α(Xn+1, Bl) denote the ⌈(1 − α)(1 + |Des-cal|)⌉-th smallest value among all nonconfor- mity scores ˆSi(Xn+1, Bl), assuming for simplicity that there are no ties; otherwise, ties can be broken at random. Then, define the interval ˆCα(Xn+1, Bl) as that obtained by applying the stan- dard conformal prediction method with absolute residual scores based on the regression model ˆµml(Xn+1)(Xn+1): ˆCα(Xn+1, Bl) = ˆµml(Xn+1)(Xn+1) ± ˆQ1−α(Xn+1, Bl). (12) Finally, the prediction interval ˆCα(Xn+1) is given by: ˆCα(Xn+1) = Convex  ∪L l=1{Bl ∩ ˆCα(Xn+1, Bl)}  , (13) where Convex(·) denotes the convex hull of a set. This procedure is summarized in Algorithm 3 and it is guaranteed to produce prediction sets with valid marginal coverage. 10Algorithm 3 Conformalized early stopping for regression 1: Input: Exchangeable data points ( X1, Y1), . . . ,(Xn, Yn) with outcomes Yi ∈ R. 2: Input: Test point with features Xn+1. Desired coverage level 1 − α. 3: Input: Maximum number of training epochs tmax; storage period hyper-parameter τ. 4: Input: Regression model trainable via (stochastic) gradient descent. 5: Randomly split the exchangeable data points into Dtrain and Des-cal. 6: Train the regression model for tmax epochs and save the intermediate models Mt1, . . . , MtT . 7: Evaluate ˆMces(Xn+1, y) as in (10), using Algorithm A9. 8: Partition the domain of Y into L intervals Bl, for l ∈ [L], based on knots of ˆMces(Xn+1, y). 9: for l ∈ [L] do 10: Evaluate nonconformity scores ˆSi(Xn+1, Bl) for all i ∈ Des-cal as in (11). 11: Compute ˆQ1−α(Xn+1, Bl): the ⌈(1 −α)(1 +|Des-cal|)⌉-th smallest value among ˆSi(Xn+1, Bl). 12: Construct the interval ˆCα(Xn+1, Bl) according to (12). 13: end for 14: Output: Prediction interval ˆCα(Xn+1) given as a function of { ˆCα(Xn+1, Bl)}L l=1 by (13). Theorem 3. Assume (X1, Y1), . . . ,(Xn+1, Yn+1) are exchangeable, and let ˆCα(Xn+1) be the output of Algorithm 3, as given by (13), for any given α ∈ (0, 1). Then, P[Yn+1 ∈ ˆCα(Xn+1)] ≥ 1 − α. The intuition behind the above method is as follows. Each intermediate interval ˆCα(Xn+1, Bl), for l ∈ [L], may be thought of as being computed by applying, under the null hypothesis that Yn+1 ∈ Bl, the classification method from Section 2.5 for a discretized version of our problem based on the partition {Bl}L l=1. Then, leveraging the classical duality between confidence intervals and p-values, it becomes clear that taking the intersection of Bl and ˆCα(Xn+1, Bl) essentially amounts to including the “label” Bl in the output prediction if the null hypothesis Yn+1 ∈ Bl cannot be rejected. The purpose of the final convex hull operation is to generate a contiguous prediction interval, which is what we originally stated to seek. One may intuitively be concerned that this method may output excessively wide prediction interval if the location of {Bl ∩ ˆCα(Xn+1, Bl)} is extremely large in absolute value. However, our numerical experiments will demonstrate that, as long as the number of calibration data points is not too small, the selected models in general provide reasonably concentrated predictions around the true test response regardless of the placeholder value y. Therefore, the interval ˆCα(Xn+1, Bl) tends to be close to the true y even if Bl is far away, in which case ˆCα(Xn+1, Bl) ∩ Bl = ∅ does not expand the final prediction interval ˆCα(Xn+1). Although it is unlikely, Algorithm 3 may sometimes produce an empty set, which is an uninfor- mative and potentially confusing output. A simple solution consists of replacing any empty output with the naive conformal prediction interval computed by Algorithm A7 in Appendix A1, which leverages an early-stopped model selected by looking at the original calibration data set without the test point. This approach is outlined by Algorithm A11 in Appendix A5. As the intervals given by Algorithm A11 always contain those output by Algorithm 3, it follows that Algorithm A11 also enjoys guaranteed coverage; see Corollary A3. 2.7 CES for Quantile Regression The CES method for regression described in Section 2.6 relies on classical nonconformity scores [10, 45] that are not designed to deal efficiently with heteroscedastic data [16, 31]. However, the idea 11can be extended to accommodate other nonconformity scores, including those based on quantile regression [31], conditional distributions [46, 47], or conditional histograms [48]. The reason why we have so far focused on the classical absolute residual scores is that they are more intuitive to apply in conjunction with an early stopping criterion based on the squared-error loss. In this section, we extend CES to the conformalized quantile regression (CQR) method of Romano et al. [31]. A review of the CQR method is provided in Appendix A6. As in the previous section, consider a data set containing n exchangeable observations (Xi, Yi), for i ∈ D= [n], and a test point ( Xn+1, Yn+1) with a latent label Yn+1 ∈ R. We first randomly split D into two subsets, Dtrain and Des-cal. The data in Dtrain are utilized to train a neural network quantile regression model [51] by seeking to minimize the pinball loss instead of the squared error loss, for each target level β = βlow and β = βhigh (e.g., βlow = α/2 and βhigh = 1 − α/2). Note that the same neural network, with two separate output nodes, can be utilized to estimate conditional quantiles at two different levels; e.g., as in Romano et al. [31]. For anyt ∈ [tmax], let Mβ,t denote the intermediate neural network model stored after t epochs of stochastic gradient descent, following the same notation as in Section 2.6. For each target level β and any x ∈ X, let ˆqβ,t(x) denote the approximate β−th conditional quantile of the unknown conditional distribution P(Y | X = x) estimated by Mβ,t. Similarly to Section 2.6, for any model Mβ,t and any x ∈ X, y ∈ R, define the augmented loss evaluated on the calibration data including also a dummy test point ( x, y) as: L+1 es-cal(Mβ,t, x, y) = Les-cal(Mβ,t) + L(Mβ,t, x, y) = X i∈Des-cal ρβ(Yi, ˆqβ,t(Xi)) + ρβ(y, ˆqβ,t(x)), (14) where ρβ denotes the pinball loss function defined in (A31). For any model Mβ,t, the augmented loss is equal to a constant plus a convex function of y, namely ρβ(y, ˆqβ,t(x)). Therefore, for any fixed x, the quantity in (14) can be sketched as a function of Mβ,t and y as shown in Figure 5. This is analogous to Figure 4 from Section 2.6, with the difference that now the quadratic functions have been replaced by piece-wise linear “pinball” functions. After pre-training and storing T candidate models, namely Mβ,t1, . . . , Mβ,tT for some sub- sequence (t1, . . . , tT ) of [tmax], consider the following optimization problem, ˆMβ,ces(Xn+1, y) = arg min Mβ,tj : 1≤j≤T L+1 es-cal(Mβ,tj , Xn+1, y). (15) This problem is equivalent to identifying the lower envelope of a family of shifted pinball loss func- tions, similarly to Section 2.6; see Figure 5 for a schematic visualization. Again, this lower envelope can be found at computational cost O(T log T), with the same divide-and-conquer algorithm de- scribed in Appendix A4. In particular, ˆMβ,ces(Xn+1, y) is a step function with respect to y with L distinct steps, for some L = O(T log T), and it can be written as: ˆMβ,ces(Xn+1, y) = LX l=1 mβ,l(Xn+1)1 h y ∈ (kβ l−1, kβ l ] i , (16) where mβ,l(Xn+1) ∈ [T] represents the best model selected within the interval ( kβ l−1, kβ l ] such that mβ,l(Xn+1) ̸= mβ,l−1(Xn+1) for all l ∈ [L]. Above, kβ 1 ≤ kβ 2 ≤ ··· ≤kβ L denote the knots of 12y L +1 es-cal ( M   ,t ,x ,y ) M   , 1 M   , 2 M   , 3 !"!! Figure 5: Pinball loss functions on test-augmented hold-out data for three alternative regression models, M1, M2 and M3, as a function of the place-holder outcome y for the test point. The CES method utilizes the best model for each possible value of y, which is identified by the lower envelope of these three pinball loss functions. In this case, the lower envelope has a single finite knot at k2. ˆMβ,ces(Xn+1, y), which also depend on Xn+1 and are defined as the boundaries in the domain of y between each consecutive pair of steps, with the understanding that kβ 0 = −∞ and kβ L+1 = +∞; see Figure 5 for a schematic visualization. After computing ˆMβ,ces(Xn+1, y) in (15) for both βlow and βhigh, we concatenate the respective knots klow 1 , . . . , klow L1 , khigh 1 , . . . , khigh L2 and sort them into k1 ≤ k2 ≤ kL1+L2, so that within each interval Bl = (kl−1, kl] for step l ∈ [L1 +L2], there exist exactly one best model for βlow and exactly one best model for βhigh. Then, for each interval Bl = (kl−1, kl] associated with step l ∈ [L1 + L2], evaluate the nonconformity score ˆEi(Xn+1, Bl) for all i ∈ Des-cal, based on the regression model indicated by mβlow,l(Xn+1) and mβhigh,l(Xn+1); i.e., ˆEi(Xn+1, Bl) = max n ˆqmβlow,l(Xn+1)(Xi) − Yi, Yi − ˆqmβhigh,l(Xn+1)(Xi) o . (17) Let ˆQ1−α(Xn+1, Bl) denote the ⌈(1−α)(1+ |Des-cal|)⌉-th smallest value among all nonconformity scores ˆEi(Xn+1, Bl), assuming for simplicity that there are no ties; otherwise, ties can be broken at random. Then, define the interval ˆCα(Xn+1, Bl) as that obtained by applying the conformal prediction method of Romano et al. [31] with nonconformity scores (17) based on the estimated conditional quantiles ˆqmβlow,l(Xn+1)(Xn+1) and ˆqmβhigh,l(Xn+1)(Xn+1); that is, ˆCα(Xn+1, Bl) = [ˆqmβlow,l(Xn+1)(Xn+1) − ˆQ1−α(Xn+1, Bl), ˆqmβhigh,l(Xn+1)(Xn+1) + ˆQ1−α(Xn+1, Bl)]. (18) 13Finally, the output prediction interval ˆCα(Xn+1) is given by: ˆCα(Xn+1) = Convex  ∪L l=1{Bl ∩ ˆCα(Xn+1, Bl)}  . (19) This procedure, summarized in Algorithm 4, guarantees valid marginal coverage. Algorithm 4 Conformalized early stopping for quantile regression 1: Input: Exchangeable data points ( X1, Y1), . . . ,(Xn, Yn) with outcomes Yi ∈ R. 2: Input: Test point with features Xn+1. Desired coverage level 1 − α. 3: Input: Maximum number of training epochs tmax; storage period hyper-parameter τ. 4: Input: Trainable quantile regression model with target quantiles [ βlow, βhigh]. 5: Randomly split the exchangeable data points into Dtrain and Des-cal. 6: Train for tmax epochs and save the intermediate models Mβlow,t1, . . . , Mβlow,tT , Mβhigh,t1, . . . , Mβhigh,tT . 7: Evaluate ˆMβlow,ces(Xn+1, y) and ˆMβhigh,ces(Xn+1, y) as in (16), using Algorithm A10. 8: Partition the domain of Y into L1 + L2 intervals Bl, for l ∈ [L1 + L2], based on the knots of ˆMβlow,ces(Xn+1, y) and ˆMβhigh,ces(Xn+1, y). 9: for l ∈ [L1 + L2] do 10: Evaluate nonconformity scores ˆEi(Xn+1, Bl) for all i ∈ Des-cal as in (17). 11: Compute ˆQ1−α(Xn+1, Bl): the ⌈(1 − α)(1 +|Des-cal|)⌉-th smallest value among ˆEi(Xn+1, Bl). 12: Construct the interval ˆCα(Xn+1, Bl) according to (18). 13: end for 14: Output: Prediction interval ˆCα(Xn+1) given as a function of { ˆCα(Xn+1, Bl)}L l=1 by (19). Theorem 4. Assume (X1, Y1), . . . ,(Xn+1, Yn+1) are exchangeable, and let ˆCα(Xn+1) be the output of Algorithm 4, as given by (19), for any given α ∈ (0, 1). Then, P[Yn+1 ∈ ˆCα(Xn+1)] ≥ 1 − α. Similarly to Section 2.6, it is possible (although unlikely) that Algorithm 4 may sometimes produce an empty prediction set. Therefore, we present Algorithm A12 in Appendix A7, which extends Algorithm 4 in such a way as to explicitly avoid returning empty prediction intervals. As the intervals given by Algorithm A12 always contain those output by Algorithm 4, it follows from Theorem 4 that Algorithm A12 also enjoys guaranteed coverage; see Corollary A4. 2.8 Implementation Details and Computational Cost Beyond the cost of training the neural network (which is relatively expensive but does not need to be repeated for different test points) and the storage cost associated with saving the candidate models (which we have argued to be feasible in many applications), CES is quite computationally efficient. Firstly, CES treats all test points individually and could process them in parallel, although many operations do not need to be repeated. In particular, one can recycle the evaluation of the calibration loss across different test points; e.g., see (4). Thus, the model selection component can be easily implemented at cost O((nes-cal +ntest)·T +ntest ·nes-cal) for classification (of which outlier detection is a special case) and O((nes-cal + ntest) · T · log T + ntest · nes-cal) for regression, where nes-cal = |Des-cal| and T is the number of candidate models. Note that the T · log T dependence in the regression setting comes from the divide-and-conquer algorithm explained in Appendix A4. 14It is possible that the cost of CES may become a barrier in some applications, particularly if T is very large, despite the slightly more than linear scaling. Hence, we recommend employing moderate values of T (e.g., 100 or 1000). 3 Numerical Experiments 3.1 Outlier Detection The use of CES for outlier detection is demonstrated using the CIFAR10 data set [52], a collec- tion of 60,000 32-by-32 RGB images from 10 classes including common objects and animals. A convolutional neural network with ReLU activation functions is trained on a subset of the data to minimize the cross-entropy loss. The maximum number of epochs is set to be equal to 50. The trained classification model is then utilized to compute conformity scores for outlier detection with the convention that cats are inliers and the other classes are outliers. In particular, a nonconformity score for each Zn+1 is defined as 1 minus the output of the soft-max layer corresponding to the label “cat”. This can be interpreted as an estimated probability of Zn+1 being an outlier. After translating these scores into a conformal p-value ˆu0(Zn+1), the null hypothesis that Zn+1 is a cat is rejected if ˆu0(Zn+1) ≤ α = 0.1. The total number of samples utilized for training, early stopping, and conformal calibration is varied between 500 and 2000. In each case, CES is applied using 75% of the samples for training and 25% for early stopping and calibration. Note that the calibration step only utilizes inliers, while the other data subsets also contain outliers. The empirical performance of CES is measured in terms of the probability of falsely rejecting a true null hypothesis—the false positive rate (FPR)—and the probability of correctly rejecting a false null hypothesis—the true positive rate (TPR). The CES method is compared to three benchmarks. The first benchmark is naive early stopping with the best (hybrid) theoretical correction for the nominal coverage level described in Appendix A1.2. The second benchmark is early stopping based on data splitting, which utilizes 50% of the available samples for training, 25% for early stopping, and 25% for calibration. The third benchmark is full training without early stopping, which simply selects the model obtained after the last epoch. The test set consists of 100 independent test images, half of which are outliers. All results are averaged over 100 trials based on independent data subsets. FPR TPR 500 1000 2000 500 1000 2000 0.1 0.2 0.3 0.4 0.00 0.05 0.10 0.15 Sample size Method CES Naive + theory Data splitting Full training Figure 6: Average performance, as a function of the sample size, of conformal inferences for outlier detection based on neural networks trained and calibrated with different methods, on the CIFAR10 data [52]. Ideally, the TPR should be as large as possible while maintaining the FPR below 0.1. See Table A1 for more detailed results and standard errors. Figure 6 summarizes the performance of the four methods as a function of the total sample 15size; see Table A1 in Appendix A8 for the corresponding standard errors. All methods control the FPR below 10%, as expected, but CES achieves the highest TPR. The increased power of CES compared to data splitting is not surprising, as the latter relies on a less accurate model trained on less data. By contrast, the naive benchmark trains a model more similar to that of CES, but its TPR is not as high because the theoretical correction for the naive conformal p-values is overly pessimistic. Finally, full training is the least powerful competitor for large sample sizes because its underlying model becomes more and more overconfident as the training set grows. Note that Table A1 also includes the results obtained with the naive benchmark detailed in Appendix A1, applied without the theoretical correction necessary to guarantee marginal coverage. Remarkably, the results show that the naive benchmark performs similarly to the CES method, even though only the latter has the advantage of enjoying rigorous finite-sample guarantees. 3.2 Multi-class Classification The same CIFAR10 data [52] are utilized to demonstrate the performance of CES for a 10-class classification task. These experiments are conducted similarly to those in Section 3.1, with the difference that now the soft-max output of the convolutional neural network is translated into conformal prediction sets, as explained in Appendix A3, instead of conformal p-values. The CES method is compared to the same three benchmarks from Section 3.1. All prediction sets guarantee 90% marginal coverage, and their performances are evaluated based on average cardinality. Coverage (marginal) Cardinality 500 1000 2000 500 1000 2000 6 7 8 9 0.7 0.8 0.9 1.0 Sample size Method CES Naive + theory Data splitting Full training Figure 7: Average performance, as a function of the sample size, of conformal prediction sets for multi-class classification based on neural networks trained and calibrated with different methods, on the CIFAR10 data [52]. Ideally, the coverage should be close to 90% and the cardinality should be small. See Table A2 for more detailed results and standard errors. Figure 7 summarizes the results averaged over 100 independent realizations of these experiments, while Table A2 in Appendix A8 reports on the corresponding standard errors. While all approaches always achieve the nominal coverage level, the CES method is able to do so with the smallest, and hence most informative, prediction sets. As before, the more disappointing performance of the data splitting benchmark can be explained by the more limited amount of data available for training, that of the naive benchmark by the excessive conservativeness of its theoretical correction, and that of the full training benchmark by overfitting. Table A2 also includes the results obtained with the naive benchmark without the theoretical correction, which again performs similarly to CES. 3.3 Regression We now apply the CES method from Section 2.6 to the following 3 public-domain regression data sets from the UCI Machine Learning repository [53]: physicochemical properties of protein tertiary 16structure (bio) [44], hourly and daily counts of rental bikes ( bike) [54], and concrete compressive strength (concrete) [55]. These data sets were previously also considered by Romano et al. [31], to which we refer for further details. As in the previous sections, we compare CES to the usual three benchmarks: naive early stopping with the hybrid theoretical correction for the nominal coverage level, early stopping based on data splitting, and full model training without early stopping. All methods utilize the same neural network with two hidden layers of width 128 and ReLU activation functions, trained for up to 1000 epochs. The models are calibrated in such a way as to produce conformal prediction sets with guaranteed 90% marginal coverage for a test set of 100 independent data points. The total sample size available for training, early stopping and calibration is varied between 200 and 2000. These data are allocated for specific training, early-stopping, and calibration operations as in Sections 3.1–3.2. The performance of each method is measured in terms of marginal coverage, worst-slab conditional coverage [56]—estimated as described in Sesia and E. J. Cand` es [16]—and average width of the prediction intervals. All results are averaged over 100 independent experiments, each based on a different random sample from the original raw data sets. Figure 3 summarizes the performance of the four alternative methods on the bio data, as a function of the total sample size; see Table A12 in Appendix A8 for the corresponding standard errors. These results show that all methods reach 90% marginal coverage in practice, as anticipated by the mathematical guarantees, although the theoretical correction for the naive early stopping method appears to be overly conservative. The CES method clearly performs best, in the sense that it leads to the shortest prediction intervals while also achieving approximately valid conditional coverage. By contrast, the conformal prediction intervals obtained without early stopping have significantly lower conditional coverage, which is consistent with the prior intuition that fully trained neural networks can sometimes suffer from overfitting. More detailed results from these experiments can be found in Table A3 in Appendix A8. Analogous results corresponding to thebike and concrete data sets can be found in Figures A12–A13 and Tables A4–A5 in Appendix A8. Tables A3–A5 also include the results obtained with the naive benchmark applied without the necessary theoretical correction, which performs similarly to CES. Finally, it must be noted that the widths of the prediction intervals output by the CES method in these experiments are very similar to those of the corresponding intervals produced by naively applying early stopping without data splitting and without the theoretical correction described in Appendix A1. This naive approach was not taken as a benchmark because it does not guarantee valid coverage, unlike the other methods. Nonetheless, it is interesting to note that the rigorous theoretical properties of the CES method do not come at the expense of a significant loss of power compared to this very aggressive heuristic, and in this sense, one may say that the conformal inferences computed by CES are “almost free”. 3.4 Quantile Regression We apply the CES quantile regression method from Section 2.7 to the following publicly avail- able and commonly investigated regression data sets from the UCI Machine Learning repository [53]: medical expenditure panel survey number 21 ( MEPS 21) [57]; blog feedback ( blog data) [58]; Tennessee’s student teacher achievement ratio ( STAR) [59]; community and crimes ( community) [60]; physicochemical properties of protein tertiary structure ( bio) [44]; house sales in King County (homes) [61]; and hourly and daily counts of rental bikes (bike) [54]. These data sets were previously also considered by Romano et al. [31]. 17As in the previous sections, we compare CES to the usual three benchmarks, now implemented based on quantile regression: naive early stopping with the hybrid theoretical correction for the nominal coverage level, early stopping based on data splitting and full model training without early stopping. We follow the same model architecture and data preprocessing steps as in Romano et al. [31]. To be specific, the input features are standardized to have zero mean and unit variance, and the response values are rescaled by diving the absolute mean of the training responses. All methods utilize the same neural network with three hidden layers and ReLU activation functions between layers, trained for up to 2000 epochs. The parameters are trained minimizing the pinball loss function (see Appendix A6) with Adam optimizer [64], minibatches of size 25, 0 weight decay and dropout, and fixed learning rate (0.001 for STAR, homes, bike, and bio , 0.0001 for community, and 0.00005 for MEPS 21 and blog data). The models are calibrated in such a way as to produce conformal prediction sets with guaranteed 90% marginal coverage for a test set of 1000 independent data points. The total sample size available for training, early stopping and calibration is varied between 200 and 2000 (200 and 1000 for small data sets such as community and STAR). These data are allocated for specific training, early- stopping, and calibration operations as in Sections 3.1–3.2. Again, the performance of each method is measured in terms of marginal coverage, worst-slab conditional coverage [56], and average width of the prediction intervals. All results are averaged over 25 independent experiments, each based on a different random sample from the original raw data sets. Figure 8 summarizes the performance of the four alternative methods on the homes data, as a function of the total sample size; The error bar corresponding to standard errors are plotted around each data point. These results show that all methods reach 90% marginal coverage in practice, as anticipated by the mathematical guarantees, although the theoretical correction for the naive early stopping method appears to be overly conservative. Full training, though producing the smallest prediction bands, has very low conditional coverage, which indicates that fully trained neural network models can suffer from overfitting and therefore is not appealing. Data splitting method beats full training as it gives higher approximated conditional coverage, and CES further beats data splitting in terms of conditional coverage, meanwhile producing prediction intervals of similar length as data splitting. These patterns hold true in general for additional data sets, as illustrated by Figures A14–A19 and by Tables A6–A12. Tables A6–A12 also include the results obtained with the naive benchmark applied without the necessary theoretical correction, which performs similarly to CES. Coverage (marginal) Coverage (conditional) Width 200 500 1000 2000 200 500 1000 2000 200 500 1000 2000 1 2 3 0.7 0.8 1.0 0.7 0.8 1.0 Sample size Method CES Naive + theory Data splitting Full training Figure 8: Average performance, as a function of the sample size, of conformal prediction sets for quantile regression based on neural networks trained and calibrated with different methods, on the homes data [61]. The marginal coverage is theoretically guaranteed to be above 90%. Ideally, the conditional coverage should high and the prediction intervals should be tight. See Table A6 for additional details and standard errors. 184 Discussion This paper has focused on early stopping and conformal calibration because these are two popular techniques, respectively designed to mitigate overfitting and reduce overconfidence, that were pre- viously combined without much thought. However, the relevance of our methodology extends well beyond the problem considered in this paper. In fact, related ideas have already been utilized in the context of outlier detection to tune hyper-parameters and select the most promising candidate from an arbitrary toolbox of machine learning models [26]. The techniques developed in this paper also allow one to calibrate, without further data splits, the most promising model selected in a data- driven way from an arbitrary machine learning toolbox in the context of multi-class classification and regression. As mentioned in Section 2.2 and detailed in Appendix A1, the naive benchmark that uses the same hold-out data twice, both for standard early stopping and standard conformal calibration, is not theoretically valid without conservative corrections. Nonetheless, our numerical experiments have shown that this naive approach often performs similarly to CES in practice. Of course, the naive benchmark may sometimes fail, and thus we would advise practitioners to apply the theoretically principled CES whenever its additional memory costs are not prohibitive. However, the empirical evidence suggests the naive benchmark may not be a completely unreasonable heuristic when CES is not applicable. Software implementing the algorithms and data experiments are available online at https: //github.com/ZiyiLiang/Conformalized_early_stopping. Acknowledgements The authors thank the Center for Advanced Research Computing at the University of Southern California for providing computing resources to carry out numerical experiments. The authors are also grateful to three anonymous reviewers for their insightful comments and suggestions. M. S. and Y. Z. are supported by NSF grant DMS 2210637. M. S. is also supported by an Amazon Research Award. References [1] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, et al. “Mastering the game of Go with deep neural networks and tree search”. In: Nature 529.7587 (2016), pp. 484–489. [2] C. Guo, G. Pleiss, Y. Sun, and K. Q. Weinberger. “On calibration of modern neural networks”. In: Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org. 2017, pp. 1321–1330. [3] S. Thulasidasan, G. Chennupati, J. A. Bilmes, T. Bhattacharya, and S. Michalak. “On mixup training: Improved calibration and predictive uncertainty for deep neural networks”. In: Adv. Neural. Inf. Process. Syst. 2019, pp. 13888–13899. [4] Y. Ovadia, E. Fertig, J. Ren, Z. Nado, D. Sculley, S. Nowozin, J. Dillon, B. Lakshminarayanan, and J. Snoek. “Can you trust your model’s uncertainty? evaluating predictive uncertainty under dataset shift”. In: Advances in Neural Information Processing Systems 32 (2019). 19[5] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. “Dropout: A Simple Way to Prevent Neural Networks from Overfitting”. In: J. Mach. Learn. Res. 15 (2014), pp. 1929–1958. [6] S. Ioffe and C. Szegedy. “Batch normalization: Accelerating deep network training by reducing internal covariate shift”. In: International conference on machine learning . PMLR. 2015, pp. 448–456. [7] T. Salimans and D. P. Kingma. “Weight normalization: A simple reparameterization to ac- celerate training of deep neural networks”. In: Advances in neural information processing systems 29 (2016). [8] C. Shorten and T. M. Khoshgoftaar. “A survey on image data augmentation for deep learn- ing”. In: Journal of Big Data 6.1 (2019), pp. 1–48. [9] L. Prechelt. “Automatic early stopping using cross validation: quantifying the criteria”. In: Neural networks 11.4 (1998), pp. 761–767. [10] V. Vovk, A. Gammerman, and G. Shafer. Algorithmic learning in a random world . Springer, 2005. [11] J. Smith, I. Nouretdinov, R. Craddock, C. Offer, and A. Gammerman. “Conformal anomaly detection of trajectories with a multi-class hierarchy”. In: International symposium on sta- tistical learning and data sciences . Springer. 2015, pp. 281–290. [12] V. Vovk, D. Lindsay, I. Nouretdinov, and A. Gammerman. Mondrian Confidence Machine . Technical Report. On-line Compression Modelling project. Royal Holloway, University of London, 2003. [13] Y. Romano, M. Sesia, and E. J. Cand` es. “Classification with Valid and Adaptive Coverage”. In: Advances in Neural Information Processing Systems 33 (2020). [14] G. Marcus. “Deep learning: A critical appraisal”. In: arXiv preprint arXiv:1801.00631 (2018). [15] V. Vovk. “Conditional Validity of Inductive Conformal Predictors”. In: Proceedings of the Asian Conference on Machine Learning . Vol. 25. 2012, pp. 475–490. [16] M. Sesia and E. J. Cand` es. “A comparison of some conformal quantile regression methods”. In: Stat 9.1 (2020). [17] Y. Romano, R. F. Barber, C. Sabatti, and E. Cand` es. “With Malice Toward None: Assessing Uncertainty via Equalized Coverage”. In: Harvard Data Science Review (2020). [18] R. F. Barber, E. J. Cand` es, A. Ramdas, and R. J. Tibshirani. “The limits of distribution-free conditional predictive inference”. In: Information and Inference 10.2 (2021), pp. 455–482. [19] C. Saunders, A. Gammerman, and V. Vovk. “Transduction with confidence and credibility”. In: IJCAI. 1999. [20] V. Vovk, A. Gammerman, and C. Saunders. “Machine-learning applications of algorithmic randomness”. In: International Conference on Machine Learning . 1999, pp. 444–453. [21] J. Lei, J. Robins, and L. Wasserman. “Distribution-Free Prediction Sets”. In: J. Am. Stat. Assoc. 108.501 (2013), pp. 278–287. [22] J. Lei and L. Wasserman. “Distribution-free prediction bands for non-parametric regression”. In: J. R. Stat. Soc. (B) 76.1 (2014), pp. 71–96. [23] J. Lei, M. G’Sell, A. Rinaldo, R. J. Tibshirani, and L. Wasserman. “Distribution-free predic- tive inference for regression”. In: J. Am. Stat. Assoc. 113.523 (2018), pp. 1094–1111. 20[24] R. F. Barber, E. J. Cand` es, A. Ramdas, R. J. Tibshirani, et al. “Predictive inference with the jackknife+”. In: Ann. Stat. 49.1 (2021), pp. 486–507. [25] L. Guan and R. Tibshirani. “Prediction and outlier detection in classification problems”. In: J. R. Stat. Soc. (B) 84.2 (2022), pp. 524–546. [26] Z. Liang, M. Sesia, and W. Sun. “Integrative conformal p-values for powerful out-of-distribution testing with labeled outliers”. In: arXiv preprint arXiv:2208.11111 (2022). [27] S. Bates, E. Cand` es, L. Lei, Y. Romano, and M. Sesia. “Testing for outliers with conformal p-values”. In: Ann. Stat. 51.1 (2023), pp. 149–178. [28] Y. Hechtlinger, B. P´ oczos, and L. Wasserman. Cautious Deep Learning . arXiv:1805.09460. 2018. [29] A. N. Angelopoulos, S. Bates, M. Jordan, and J. Malik. “Uncertainty Sets for Image Classifiers using Conformal Prediction”. In:International Conference on Learning Representations. 2021. [30] S. Bates, A. Angelopoulos, L. Lei, J. Malik, and M. Jordan. “Distribution-free, risk-controlling prediction sets”. In: Journal of the ACM (JACM) 68.6 (2021), pp. 1–34. [31] Y. Romano, E. Patterson, and E. J. Cand` es. “Conformalized quantile regression”. In: Ad- vances in Neural Information Processing Systems . 2019, pp. 3538–3548. [32] R. J. Tibshirani, R. Foygel Barber, E. Candes, and A. Ramdas. “Conformal prediction under covariate shift”. In: Advances in neural information processing systems 32 (2019). [33] M. Sesia, S. Favaro, and E. Dobriban. “Conformal Frequency Estimation with Sketched Data under Relaxed Exchangeability”. In: arXiv preprint arXiv:2211.04612 (2022). [34] R. F. Barber, E. J. Cand` es, A. Ramdas, and R. J. Tibshirani. “Conformal prediction beyond exchangeability”. In: arXiv preprint arXiv:2202.13415 (2022). [35] I. Gibbs and E. Cand` es. “Conformal inference for online prediction with arbitrary distribution shifts”. In: arXiv preprint arXiv:2208.08401 (2022). [36] N. Colombo and V. Vovk. “Training conformal predictors”. In: Conformal and Probabilistic Prediction and Applications. PMLR. 2020, pp. 55–64. [37] A. Bellotti. “Optimized conformal classification using gradient descent approximation”. In: arXiv preprint arXiv:2105.11255 (2021). [38] D. Stutz, K. Dvijotham, A. T. Cemgil, and A. Doucet. “Learning Optimal Conformal Clas- sifiers”. In: arXiv preprint arXiv:2110.09192 (2021). [39] B.-S. Einbinder, Y. Romano, M. Sesia, and Y. Zhou. “Training Uncertainty-Aware Classifiers with Conformalized Deep Learning”. In: Adv. Neural Inf. Process. Syst. Vol. 35. 2022. [40] Y. Yang and A. K. Kuchibhotla. “Finite-sample efficient conformal prediction”. In: arXiv preprint arXiv:2104.13871 (2021). [41] A. Marandon, L. Lei, D. Mary, and E. Roquain. “Machine learning meets false discovery rate”. In: arXiv preprint arXiv:2208.06685 (2022). [42] Y. Benjamini and Y. Hochberg. “Controlling the false discovery rate: a practical and powerful approach to multiple testing”. In: J. R. Stat. Soc. (B) 57.1 (1995), pp. 289–300. [43] B. Kim, C. Xu, and R. Barber. “Predictive inference is free with the jackknife+-after- bootstrap”. In: Advances in Neural Information Processing Systems 33 (2020), pp. 4138– 4149. 21[44] Physicochemical properties of protein tertiary structure data set . https://archive.ics. uci.edu/ml/datasets/Physicochemical+Properties+of+Protein+Tertiary+Structure . Accessed: July, 2019. [45] J. Lei, M. G’Sell, A. Rinaldo, R. J. Tibshirani, and L. Wasserman. “Distribution-free predic- tive inference for regression”. In: J. Am. Stat. Assoc. 113.523 (2018), pp. 1094–1111. [46] V. Chernozhukov, K. W¨ uthrich, and Y. Zhu. “Distributional conformal prediction”. In: Pro- ceedings of the National Academy of Sciences 118.48 (2021), e2107794118. [47] R. Izbicki, G. Shimizu, and R. Stern. “Flexible distribution-free conditional predictive bands using density estimators”. In: International Conference on Artificial Intelligence and Statis- tics. PMLR. 2020, pp. 3068–3077. [48] M. Sesia and Y. Romano. “Conformal Prediction using Conditional Histograms”. In: Advances in Neural Information Processing Systems 34 (2021). [49] O. Devillers and M. J. Golin. “Incremental algorithms for finding the convex hulls of cir- cles and the lower envelopes of parabolas”. In: Information Processing Letters 56.3 (1995), pp. 157–164. [50] F. Nielsen and M. Yvinec. “An output-sensitive convex hull algorithm for planar objects”. In: International Journal of Computational Geometry & Applications 8.01 (1998), pp. 39–65. [51] J. W. Taylor. “A quantile regression neural network approach to estimating the conditional density of multiperiod returns”. In: Journal of Forecasting 19.4 (2000), pp. 299–311. [52] A. Krizhevsky. Learning Multiple Layers of Features from Tiny Images . 2009. [53] H. K. Pınar T¨ ufekci. UCI Machine Learning Repository. 2012. [54] Bike sharing dataset . https : / / archive . ics . uci . edu / ml / datasets / bike + sharing + dataset. Accessed: July, 2019. [55] Concrete compressive strength data set . http://archive.ics.uci.edu/ml/datasets/ concrete+compressive+strength. Accessed: July, 2019. [56] M. Cauchois, S. Gupta, and J. C. Duchi. “Knowing what You Know: valid and validated confidence sets in multiclass and multilabel prediction.” In: J. Mach. Learn. Res. 22 (2021), pp. 81–1. [57] Medical Expenditure Panel Survey, Panel 21 . https://meps.ahrq.gov/mepsweb/data_ stats/download_data_files_detail.jsp?cboPufNumber=HC- 192 . Accessed: January, 2019. [58] BlogFeedback dataset. https://github.com/xinbinhuang/feature-selection_blogfeedback. Accessed: Mar, 2023. [59] C. Achilles, H. P. Bain, F. Bellott, J. Boyd-Zaharias, J. Finn, J. Folger, J. Johnston, and E. Word. Tennessee’s Student Teacher Achievement Ratio (STAR) project. Version V1. 2008. [60] Communities and crime dataset . https://github.com/vbordalo/Communities- Crime . Accessed: Mar, 2023. [61] House prices from King County dataset. https://www.kaggle.com/datasets/shivachandel/ kc-house-data?select=kc_house_data.csv. Accessed: Mar, 2023. [62] R. Koenker and G. Bassett. “Regression Quantiles”. In: Econometrica 46.1 (1978), pp. 33–50. [63] I. Steinwart and A. Christmann. “Estimating conditional quantiles with the help of the pinball loss”. In: Bernoulli 17.1 (2011), pp. 211–225. 22[64] D. P. Kingma and J. Ba. “Adam: A method for stochastic optimization”. In: preprint at arXiv:1412.6980 (2014). [65] M. Abramowitz and I. A. Stegun. Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables. ninth Dover printing, tenth GPO printing. Dover, 1964. 23A1 Naive Early Stopping Benchmarks A1.1 Detailed Implementation of the Naive Benchmarks We detail here the implementation of the naive benchmark discussed in Section 2.2. This approach can serve as an informative benchmark and it becomes useful in Appendix A5 to extend our rigorous conformalized early stopping method for regression problems in such as way as to explicitly avoid returning empty prediction intervals. For completeness, we present the implementation of the naive benchmark separately for outlier detection, multi-class classification, and regression, respectively in Algorithms A5, A6 and A7. Note that Algorithm A6 also allows for the possibility of computing prediction sets seeking (approximate) marginal coverage instead of (approximate) label-conditional coverage for multi-class classification problems; see Appendix A2 for further details on multi-class classification with marginal coverage. Algorithm A5 Naive conformal outlier detection benchmark with greedy early stopping 1: Input: Exchangeable data points Z1, . . . , Zn; test point Zn+1. 2: Input: Maximum number of training epochs tmax; storage period hyper-parameter τ. 3: Input: One-class classifier trainable via (stochastic) gradient descent. 4: Randomly split the exchangeable data points into Dtrain and Des-cal. 5: Train the one-class classifier for tmax epochs and save the intermediate models Mt1, . . . , MtT . 6: Pick the most promising model t∗ ∈ [T] minimizing Les-cal(Mt) in (1), based on Des-cal. 7: Compute nonconformity scores ˆSi(Zn+1) for all i ∈ Des-cal ∪ {n + 1} using model t∗. 8: Output: Naive conformal p-value ˆunaive 0 (Zn+1) given by (3). 24Algorithm A6 Naive conformal multi-class classification benchmark with greedy early stopping 1: Input: Exchangeable data points ( X1, Y1), . . . ,(Xn, Yn) with labels Yi ∈ [K]. 2: Input: Test point with features Xn+1. Desired coverage level 1 − α. 3: Input: Maximum number of training epochs tmax; storage period hyper-parameter τ. 4: Input: K-class classifier trainable via (stochastic) gradient descent. 5: Randomly split the exchangeable data points into Dtrain and Des-cal. 6: Train the K-class classifier for tmax epochs and save the intermediate models Mt1, . . . , MtT . 7: Pick the most promising model t∗ ∈ [T] minimizing Les-cal(Mt) in (4), based on Des-cal. 8: for y ∈ [K] do 9: if Label-conditional coverage is desired then 10: Define Dy es-cal = {i ∈ Des-cal : Yi = y}. 11: Compute scores ˆSy i (Xn+1) for all i ∈ Dy es-cal ∪ {n + 1} using model t∗; see Appendix A3. 12: Compute the naive conformal p-value ˆunaive y (Xn+1) according to (6). 13: else 14: Compute scores ˆSy i (Xn+1) for all i ∈ Des-cal ∪ {n + 1} using model t∗; see Appendix A3. 15: Compute the naive conformal p-value ˆunaive y (Xn+1) according to ˆunaive y (Xn+1) = 1 + |i ∈ Des-cal : ˆSy i (Xn+1) ≤ ˆSy n+1(Xn+1)| 1 + |Des-cal| . 16: end if 17: end for 18: Output: Naive prediction set ˆCnaive α (Xn+1) given by (7). Algorithm A7 Naive conformal regression benchmark with greedy early stopping 1: Input: Exchangeable data points ( X1, Y1), . . . ,(Xn, Yn) with outcomes Yi ∈ R. 2: Input: Test point with features Xn+1. Desired coverage level 1 − α. 3: Input: Maximum number of training epochs tmax; storage period hyper-parameter τ. 4: Input: Regression model trainable via (stochastic) gradient descent. 5: Randomly split the exchangeable data points into Dtrain and Des-cal. 6: Train the regression model for tmax epochs and save the intermediate models Mt1, . . . , MtT . 7: Pick the most promising model t∗ ∈ [T] minimizing Les-cal(Mt) in (8). 8: Evaluate nonconformity scores ˆSi(Xn+1) = |Yi − ˆµt∗(Xi)| for all i ∈ Des-cal. 9: Compute ˆQ1−α(Xn+1) = ⌈(1 − α)(1 + |Des-cal|)⌉-th smallest value in ˆSi(Xn+1) for i ∈ Des-cal. 10: Output: Prediction interval ˆCnaive α (Xn+1) = ˆµt∗(Xn+1) ± ˆQ1−α(Xn+1). A1.2 Theoretical Analysis of the Naive Benchmark Although the naive benchmarks described above often perform similarly to CES in practice, they do not enjoy the same desirable theoretical guarantees. Nonetheless, we can study their behaviour in sufficient detail as to prove that their inferences are too far from being valid. Unfortunately, as demonstrated in Section 3, these theoretical results are still not tight enough to be very useful in practice. For simplicity, we will begin by focusing on outlier detection. Review of existing results based on the DKW inequality. Yang and Kuchibhotla [40] have recently studied the finite-sample coverage rate of a conformal 25prediction interval formed by naively calibrating a model selected among T possible candidates based on its performance on the calibration data set itself, which we denote by Des-cal. Although Yang and Kuchibhotla [40] focus on conformal prediction intervals, here we find it easier to explain their ideas in the context of conformal p-values for outlier detection. Let ˆSi(Zn+1; t), for all i ∈ Des-cal and t ∈ [T], denote the nonconformity scores corresponding to model t, and denote the ⌊α(1 +|Des-cal|)⌋-th smallest value in ˆSi(Xn+1; t) as ˆQα(Zn+1; t). Let t∗ indicate the selected model. As we are interested in constructing a conformal p-value ˆunaive 0 (Zn+1), the goal is to bound from above the tail probability P   ˆunaive 0 (Zn+1) > α  = E h P  ˆSi(Xn+1; t∗) > ˆQα(Zn+1; t∗) | Des-cal i . (A20) Intuitively, if nes-cal = |Des-cal| is sufficiently large, the conditional probability inside the expected value on the right-hand-side above can be well-approximated by the following empirical quantity: 1 n X i∈Des-cal 1 n ˆSi(Xn+1; t∗) > ˆQα(Zn+1; t∗) o = ⌈(1 + nes-cal)(1 − α)⌉ nes-cal ≥  1 + 1 nes-cal  (1 − α). The quality of this approximation in finite samples can be bound by the DKW inequality, which holds for any ε ≥ 0: P  sup s∈R  1 nes-cal X i∈Des-cal 1 n ˆSi(Xn+1; t∗) > s o − P  ˆSi(Xn+1; t∗) > s| Des-cal   > ε   ≤ 2e−2nes-calε2 . (A21) Starting from this, Theorem 1 in Yang and Kuchibhotla [40] shows that P(ˆunaive 0 (Zn+1) > α) ≥  1 + 1 nes-cal  (1 − α) − p log(2T)/2 + c(T)√nes-cal , (A22) where c(T) is a constant that can be computed explicitly and is generally smaller than 1 /3. Intu- itively, the [ p log(2T)/2 +c(T)]/√nes-cal term above can be interpreted as the worst-case approxi- mation error among all possible models t ∈ [T]. One limitation with this result is that is gives a worst-case correction that does not depend on the chosen level α, and one would intuitively expect this bound to be tighter for α = 1/2 and overly conservative for the small α values (e.g., α = 0.1) that are typically interesting in practice. (This intuition will be confirmed empirically in Figure A10.) This observation motivates the following alternative analysis, which can often give tighter results. Alternative probabilistic bound based on Markov’s inequality. Define Wt = P  ˆunaive 0 (Zn+1; t) > α| Des-cal  . Lemma 3 in Vovk [15] tells us that Wt follows a Beta distribution, assuming exchangeability among Des-cal and the test point. That is, Wt ∼ Beta(nes-cal + 1 − l, l), l = ⌊α(nes-cal + 1)⌋. In the following, we will denote the corresponding inverse Beta cumulative distribution function as I−1(x; nes-cal + 1 − l, l). This result can be used to derive an alternative upper bound for P(ˆunaive 0 (Zn+1) > α) using the Markov’s inequality. 26Proposition A1. Assume Z1, . . . , Zn, Zn+1 are exchangeable random samples, and let ˆunaive 0 (Zn+1) be the output of Algorithm A5, for any given α ∈ (0, 1). Then, for any fixed α ∈ (0, 1) and any b >1, letting l = ⌊α(nes-cal + 1)⌋, P  ˆunaive 0 (Zn+1) > α  ≥ I−1  1 bT ; nes-cal + 1 − l, l  · (1 − 1/b). Note that this bound depends on α in a more complex way compared to that of Yang and Kuchibhotla [40]. However, its asymptotic behaviour in the large-T limit remains similar, as shown below. Lemma A1. Denote I−1(x; nes-cal + 1 − l, l) as the inverse Beta cumulative distribution function. For any fixed b >1 and α ∈ (0, 1) , letting l = ⌊α(nes-cal + 1)⌋, for sufficiently large T and nes-cal, we have: I−1  1 bT ; nes-cal + 1 − l, l  = (1 − α) − s α(1 − α) nes-cal + 1 · p 2 log(bT) + O   1p nes-cal log(T) ! . In simpler terms, Lemma A1 implies that the coverage lower bound in Proposition A1 is ap- proximately equal to  (1 − α) − s α(1 − α) nes-cal + 1 · p 2 log(bT)   ·  1 − 1 b  , which displays an asymptotic behaviour similar to that of the bound from Yang and Kuchibhotla [40]. Further, the Markov bound is easy to compute numerically and often turns out to be tighter as long as b is moderately large (e.g., b = 100), as we shall see below. Naturally, the same idea can also be applied to bound the coverage of naive conformal prediction sets or intervals output by Algorithm A6 or Algorithm A7, respectively. Corollary A1. Assume (X1, Y1), . . . ,(Xn, Yn), (Xn+1, Yn+1) are exchangeable random sample, and let ˆCnaive α (Xn+1) be the output of Algorithm A6, for any given α ∈ (0, 1). Then, for any b > 1, letting l = ⌊α(nes-cal + 1)⌋, P h Yn+1 ∈ ˆCnaive α (Xn+1) i ≥ I−1  1 bT ; nes-cal + 1 − l, l  · (1 − 1/b). Corollary A2. Assume (X1, Y1), . . . ,(Xn, Yn), (Xn+1, Yn+1) are exchangeable random samples, and let ˆCnaive α (Xn+1) be the output of Algorithm A7, for any α ∈ (0, 1). Then, for any b > 1, letting l = ⌊α(nes-cal + 1)⌋, P h Yn+1 ∈ ˆCnaive α (Xn+1) i ≥ I−1  1 bT ; nes-cal + 1 − l, l  · (1 − 1/b). Hybrid probabilistic bound. Since neither the DKW nor the Markov bound described above always dominate the other for all possible combinations of T, nes-cal, and α, it makes sense to combine them to obtain a uniformly tighter hybrid bound. For any fixed b >1 and any T, nes-cal, and α, let M(T, nes-cal, α) = I−1 (1/bT; nes-cal + 1 − l, l) · (1 − 1/b) denote the Markov bound and D(T, nes-cal, α) = (1 + 1/(nes-cal)) (1− α) − ( p log(2T)/2 +c(T))/√nes-cal denote the DKW bound, 27Figure A9: Numerical comparison of different theoretical lower bounds for the marginal coverage of conformal prediction sets computed with a naive early stopping benchmark (e.g., Algorithm A6). Left: lower bounds for the marginal coverage as a function of the number of candidate models T, when α = 0.1 and nes-cal = 8000. Right: lower bounds for the marginal coverage as a function of the number of hold-out data points, nes-cal, when α = 0.1 and T = 100. Higher values correspond to tighter bounds. define H(T, nes-cal, α) as H(T, nes-cal, α) = max {M(T, nes-cal, α), D(T, nes-cal, α)}. It then follows immediately from Yang and Kuchibhotla [40] and Proposition A1 that, under the same conditions of Proposition A1, for any fixed b >1, P  ˆunaive 0 (Zn+1) > α  ≥ H(T, nes-cal, α). Of course, the same argument can also be utilized to tighten the results of Corollaries A1–A2. Numerical comparison of different probabilistic bounds. Figure A9 compares the three probabilistic bounds described above ( DKW, Markov, and hybrid) as a function of the number of candidate models T and of the number of hold-out data points nes-cal, in the case of α = 0.1. For simplicity, the Markov and hybrid bounds are evaluated by setting b = 100, which may not be the optimal choice but appears to work reasonably well. These results show that Markov bound tends to be tighter than the DKW bound for large values of T and for small values of nes-cal, while the hybrid bound generally achieves the best of both worlds. Lastly, Figure A10 demonstrates that the Markov bound tends to be tighter when α is small. The Markov and hybrid bounds here are also evaluated using b = 100. A2 Classification with Marginal Coverage The conformalized early stopping method presented in Section 2.5 can be easily modified to produce prediction sets with marginal rather than label-conditional coverage, as outlined in Algorithm A8. The difference between Algorithm 2 and Algorithm A8 is that the latter utilizes all calibration data in Des-cal to compute each conformal p-value ˆuy(Xn+1), not only the samples with true label y. An advantage of this approach is that conformal p-values based on a larger calibration samples are less aleatoric [27] and require less conservative finite-sample corrections (i.e., the “+1” term the 28Figure A10: Numerical comparison of different theoretical lower bounds for the marginal coverage of conformal prediction sets computed with a naive early stopping benchmark (e.g., Algorithm A6), as a function of the nominal significance level α. Left: lower bounds for the marginal coverage as a function of α, when T = 1000 and nes-cal = 1000. Right: theoretically corrected significance level necessary needed to achieve the marginal coverage guarantees expected at the nominal α level, as a function of α when T = 1000 and nes-cal = 1000. The dashed grey lines indicate the ideal values corresponding to standard conformal inferences based on calibration data that are independent of those used for early stopping. Higher values correspond to tighter bounds. numerator of the p-value formula becomes more negligible as the calibration set size increases). In turn, this tends to lead to smaller prediction sets with potentially more stable coverage conditional on the calibration data [16, 27] Of course, the downside of these prediction sets is that they can only be guaranteed to provide marginal coverage, although they can sometimes also perform well empirically in terms of label-conditional coverage [13]. Theorem A5. Assume (X1, Y1), . . . ,(Xn, Yn), (Xn+1, Yn+1) are exchangeable random samples, and let ˆCα(Xn+1) be the output of Algorithm A8, for any α ∈ (0, 1). Then, P[Yn+1 ∈ ˆCα(Xn+1)] ≥ 1−α. 29Algorithm A8 Conformalized early stopping for multi-class classification with marginal coverage 1: Input: Exchangeable data points ( X1, Y1), . . . ,(Xn, Yn) with labels Yi ∈ [K]. 2: Input: Test point with features Xn+1. Desired coverage level 1 − α. 3: Input: Maximum number of training epochs tmax; storage period hyper-parameter τ. 4: Input: K-class classifier trainable via (stochastic) gradient descent. 5: Randomly split the exchangeable data points into Dtrain and Des-cal. 6: Train the K-class classifier for tmax epochs and save the intermediate models Mt1, . . . , MtT . 7: for y ∈ [K] do 8: Imagine Yn+1 = y. 9: Pick the model ˆMces(Xn+1, y) according to (5), using the data in Des-cal ∪ {n + 1}. 10: Compute scores ˆSi(Xn+1, y) for all i ∈ Des-cal ∪{n+1} using ˆMces(Xn+1, y); see Appendix A3. 11: Compute the conformal p-value ˆumarg y (Xn+1) according to ˆumarg y (Xn+1) = 1 + |i ∈ Des-cal : ˆSy i (Xn+1) ≤ ˆSy n+1(Xn+1)| 1 + |Des-cal| . (A23) 12: end for 13: Output: Prediction set ˆCα(Xn+1) given by (7), with ˆumarg y (Xn+1) instead of ˆuy(Xn+1). A3 Review of Nonconformity Scores for Classification This section reviews the relevant background on the adaptive nonconformity scores for classification developed by Romano et al. [13]. For any x ∈ Xand y ∈ [K], let ˆπy(x) denote any (possibly very inaccurate) estimate of the true P[Y = y | X = x] corresponding to the unknown data-generating distribution. Concretely, a typical choice of ˆπ may be given by the output of the final softmax layer of a neural network classifier, for example. For any x ∈ Xand τ ∈ [0, 1], define the generalized conditional quantile function L, with input x, ˆπ, τ, as: L(x; ˆπ, τ) = min{k ∈ [K] : ˆ π(1)(x) + ˆπ(2)(x) + . . .+ ˆπ(k)(x) ≥ τ}, (A24) where ˆπ(1)(x) ≤ ˆπ(2)(x) ≤ . . .ˆπ(K)(x) are the order statistics of ˆπ1(x) ≤ ˆπ2(x) ≤ . . .ˆπK(x). Intu- itively, L(x; ˆπ, τ) gives the size of the smallest possible subset of labels whose cumulative probability mass according to ˆπ is at least τ. Define also a function S with input x, u ∈ (0, 1), ˆπ, and τ that computes the set of most likely labels up to (but possibly excluding) the one identified byL(x; ˆπ, τ): S(x, u; ˆπ, τ) = ( ‘y’ indices of the L(x; ˆπ, τ) − 1 largest ˆπy(x), if u ≤ V (x; ˆπ, τ), ‘y’ indices of the L(x; ˆπ, τ) largest ˆπy(x), otherwise, (A25) where V (x; ˆπ, τ) = 1 ˆπ(L(x;ˆπ,τ))(x)   L(x;ˆπ,τ)X k=1 ˆπ(k)(x) − τ  . 30Then, define the generalized inverse quantile nonconformity score function s, with input x, y, u; ˆπ, as: s(x, y, u; ˆπ) = min {τ ∈ [0, 1] : y ∈ S(x, u; ˆπ, τ)}. (A26) Intuitively, s(x, y, u; ˆπ) is the smallest value of τ for which the set S(x, u; ˆπ, τ) contains the label y. Finally, the nonconformity score for a data point ( Xi, Yi) is given by: ˆSi = s(Xi, Yi, Ui; ˆπ), (A27) where Ui is a uniform random variable independent of anything else. Note that this can also be equivalently written more explicitly as: ˆSi = ˆπ(1)(Xi) + ˆπ(2)(Xi) + . . .+ ˆπ(r(Yi,ˆπ(Xi)))(Xi) − Ui · ˆπ(r(Yi,ˆπ(Xi)))(Xi), (A28) where r(Yi, ˆπ(Xi)) is the rank of Yi among the possible labels y ∈ [K] based on ˆπy(Xi), so that r(y, ˆπ(Xi)) = 1 if ˆπy(Xi) = ˆπ(1)(Xi). The idea motivating this construction is that the nonconfor- mity score ˆSi defined above is guaranteed to be uniformly distributed on [0 , 1] conditional on X if the model ˆπ estimates the true unknown P[Y = y | X = x] accurately for all x ∈ X. This is a desirable property in conformal inference because it leads to statistically efficient prediction sets that can often achieve relatively high feature-conditional coverage in practice, even if the true data- generating distribution is such that some observations are much noisier than others; see Romano et al. [13] for further details. Finally, we conclude this appendix by noting that the nonconformity scores in Section 2.5 are written as ˆSi(Xn+1, y), instead of the more compact notation ˆSi adopted here, simply to emphasize that they are computed based on class probabilities ˆ π estimated by a data-driven model ˆM that depends on the test features Xn+1 as well as on the placeholder label y for Yn+1. A4 Efficient Computation of the Lower Envelope This section explains how to implement a computationally efficient divide-and-conquer algorithm for finding the lower envelope of a family of T parabolas or a family of shifted pinball loss functions at cost O(T log T) [49, 50]. This solution, outlined in Algorithm A9 and Algorithm A10, is useful to implement the proposed CES method for regression problems, as detailed in Algorithm 3 and Algorithm 4. 31Algorithm A9 Divide-and-conquer algorithm for finding the lower envelope of many parabolas 1: Input: A set of parabolas L = {l1, l2, . . . , lT } of forms li = aix2 + bix + ci for i = 1, . . . , T. 2: Randomly split L into two subsets. Repeat splitting until each subset only contains one parabola or is empty. 3: For each subset with only one parabola, set the parabola itself as the lower envelope and set the initial breakpoint list to [ −∞, +∞]. 4: for each interval constructed by adjacent breakpoints do 5: Within the interval, identify the two parabolas contributing to the previous lower envelopes, denoted as P1, P2. 6: Evaluate P1 and P2 at the current interval endpoints. 7: Calculate the intersection point p of P1 and P2. There exists at most one such p because ai = 1, ∀i, by (8). 8: if p not exists or p exists but lies outside the current interval then 9: Set the new lower envelope as the parabola with smaller values computed at the interval endpoints. 10: else 11: Add p as a breakpoint. 12: Within the current interval, set the new lower envelope below and above p based on eval- uations of the parabolas at the interval endpoints. 13: end if 14: Update and sort the breakpoint list and update the new lower envelope. 15: end for 16: Recursively merge two lower envelopes to form a new lower envelope by repeating Lines 4–15. 17: Output: A sorted dictionary of breakpoints and parabola indices characterizing the lower envelope of L. 32Algorithm A10 Divide-and-conquer algorithm for finding the lower envelope of many pinball loss functions 1: Input: A set of shifted pinball loss functions L = {l1, l2, . . . , lT } of forms li = ci + ρβ(y, ˆy) for i = 1, . . . , T. 2: Randomly split L into two subsets. Repeat splitting until each subset only contains one pinball loss function or is empty. 3: For each subset with only one pinball loss function, set the function itself as the lower envelope and set the initial breakpoint list to [ −∞, +∞]. 4: for each interval constructed by adjacent breakpoints do 5: Within the interval, identify the two pinball loss functions contributing to the previous lower envelopes; i.e., P1, P2. 6: Evaluate P1 and P2 at the current interval endpoints. 7: Calculate the intersection point p of P1 and P2. There exists at most one such p because β is the same ∀i, by (14). 8: if p not exists or p exists but lies outside the current interval then 9: Set the new lower envelope as the pinball loss function with smaller values computed at the interval endpoints. 10: else 11: Add p as a breakpoint. 12: Within the current interval, set the new lower envelope below and above p based on eval- uations of the pinball loss functions at the interval endpoints. 13: end if 14: Update and sort the breakpoint list and update the new lower envelope. 15: end for 16: Recursively merge two lower envelopes to form a new lower envelope by repeating Lines 4–15. 17: Output: A sorted dictionary of breakpoints and pinball loss function indices characterizing the lower envelope of L. A5 Avoiding Empty Predictions in CES for Regression This section presents Algorithm A11, which extends Algorithm 3 from Section 2.6 in such a way as to explicitly avoid returning empty prediction intervals. Algorithm A11 Conformalized early stopping for regression, avoiding empty predictions 1: Input: Exchangeable data points ( X1, Y1), . . . ,(Xn, Yn) with outcomes Yi ∈ R. 2: Input: Test point with features Xn+1. Desired coverage level 1 − α. 3: Input: Maximum number of training epochs tmax; storage period hyper-parameter τ. 4: Input: Regression model trainable via (stochastic) gradient descent. 5: Randomly split the exchangeable data points into Dtrain and Des-cal. 6: Train the regression model for tmax epochs and save the intermediate models Mt1, . . . , MtT . 7: Evaluate ˆCα(Xn+1) using Algorithm 3. 8: if ˆCα(Xn+1) = ∅ then 9: Evaluate ˆCnaive α (Xn+1) using Algorithm A7. Set ˆCα(Xn+1) = ˆCnaive α (Xn+1). 10: end if 11: Output: A non-empty prediction interval ˆCα(Xn+1). 33Corollary A3. Assume (X1, Y1), . . . ,(Xn, Yn), (Xn+1, Yn+1) are exchangeable random samples, and let ˆCα(Xn+1) be the output of Algorithm A11, for any α ∈ (0, 1). Then, P[Yn+1 ∈ ˆCα(Xn+1)] ≥ 1 − α. A6 Review of Conformalized Quantile Regression This section reviews the relevant background on conditional quantile regression [62] and conformal- ized quantile regression (CQR) [31]. In contrast to the classical regression models that estimate the conditional mean of the test response Yn+1 given the test feature Xn+1 = x, quantile regression estimates the conditional quantile qβ of Yn+1 given Xn+1 = x, which is defined as qβ(x) = inf{y ∈ R : P(Yn+1 ≤ y|Xn+1 = x) ≥ β}. (A29) This can be formulated as solving the optimization problem: ˆqβ(x) = f(x, ˆθ), ˆθ = arg min θ 1 n nX i=1 ρβ(Yi, f(Xi, θ)), (A30) where f(x, θ) represents the quantile regression function [62] and ρβ is the convex “pinball loss” function [63], illustrated in Figure A11 and mathematically defined as ρβ(y, ˆy) = ( β(y − ˆy), if y − ˆy >0, (1 − β)(ˆy − y), otherwise. (A31) y− ˆy ρβ(y,ˆy) (1 − β)(ˆy− y) β(y− ˆy) Figure A11: Visualization of the pinball loss function defined in (A31). To construct an efficient prediction interval ˆC(Xn+1) whose length is adaptive to the local variability of Xn+1, CQR operates as follows. As in split conformal prediction, firstly the available data are randomly split into a proper training set, indexed by I1, and a calibration set, indexed by I2. Given any quantile regression algorithm A, two conditional quantile functions, ˆqαlo and ˆqαhi, are fitted on I1, where αlo = α/2 and αhi = 1 − α/2: {ˆqαlo, ˆqαhi} ← A({(Xi, Yi) : i ∈ I1}). (A32) 34Then, conformity scores are computed on the calibration data set I2 as: Ei = max{ˆqαlo(Xi) − Yi, Yi − ˆqαhi(Xi)} for i ∈ I2. (A33) The conformity score in (A33) can account both for possible under-coverage and over-coverage of the quantile regression model [31]. If Yi is outside the interval [ˆqαlo(Xi), ˆqαhi(Xi)], then Ei is the (positive) distance of Yi from the closest endpoint of the interval. Otherwise, if Yi is inside the interval [ˆqαlo(Xi), ˆqαhi(Xi)], then Ei is the negative of the distance of Yi from the closest endpoint of the interval. Therefore, if the quantile regression model is well-calibrated, approximately 90% of the calibration data points should have Ei ≤ 0 [31]. Finally, CQR constructs the prediction interval for the test response value Yn+1 through ˆC(Xn+1) = [ˆqαlo(Xn+1) − ˆQ1−α(E, I2), ˆqαhi(Xn+1) + ˆQ1−α(E, I2)], (A34) where ˆQ1−α(E, I2) is the (1 − α)(1 + 1/|I2|)-th empirical quantile of {Ei : i ∈ I2}. A7 Avoiding Empty Predictions for Regression with CQR In this section, we introduce Algorithm A12 as an extension of Algorithm 4 in the main text to address the rare possibility of generating empty prediction sets. Algorithm A12 ensures that the intervals it produces are always non-empty, while still encompassing the intervals obtained from Algorithm 4. Consequently, Algorithm A12 maintains the guaranteed coverage provided by Theorem 4, as indicated in Corollary A4. Algorithm A12 Conformalized early stopping for quantile regression, avoiding empty predictions 1: Input: Exchangeable data points ( X1, Y1), . . . ,(Xn, Yn) with outcomes Yi ∈ R. 2: Input: Test point with features Xn+1. Desired coverage level 1 − α. 3: Input: Maximum number of training epochs tmax; storage period hyper-parameter τ. 4: Input: Trainable quantile regression model with target quantiles [ βlow, βhigh]. 5: Randomly split the exchangeable data points into Dtrain and Des-cal. 6: Train for tmax epochs and save the intermediate models Mβlow,t1, . . . , Mβlow,tT , Mβhigh,t1, . . . , Mβhigh,tT . 7: Evaluate ˆCα(Xn+1) using Algorithm 4. 8: if ˆCα(Xn+1) = ∅ then 9: Evaluate ˆCnaive α (Xn+1) using Algorithm A13. Set ˆCα(Xn+1) = ˆCnaive α (Xn+1). 10: end if 11: Output: A non-empty prediction interval ˆCα(Xn+1). Corollary A4. Assume (X1, Y1), . . . ,(Xn, Yn), (Xn+1, Yn+1) are exchangeable random samples, and let ˆCα(Xn+1) be the output of Algorithm A12, for any α ∈ (0, 1). Then, P[Yn+1 ∈ ˆCα(Xn+1)] ≥ 1 − α. 35A7.1 Implementation of the Naive Benchmark Algorithm A13 Naive conformal quantile regression benchmark with greedy early stopping 1: Input: Exchangeable data points ( X1, Y1), . . . ,(Xn, Yn) with outcomes Yi ∈ R. 2: Input: Test point with features Xn+1. Desired coverage level 1 − α. 3: Input: Maximum number of training epochs tmax; storage period hyper-parameter τ. 4: Input: Trainable quantile regression model with target quantiles [ βlow, βhigh]. 5: Randomly split the exchangeable data points into Dtrain and Des-cal. 6: Train for tmax epochs and save the intermediate models Mβlow,t1, . . . , MβlowtT , Mβhigh,t1, . . . , MβhightT . 7: Pick the most promising models t∗ low, t∗ high ∈ [T] minimizing Les-cal(Mt) in (14). 8: Evaluate the scores ˆEi(Xn+1) = max{ˆqt∗ low(Xi) − Yi, Yi − ˆqt∗ high(Xi)} for all i ∈ Des-cal. 9: Compute ˆQ1−α(Xn+1) = ⌈(1 − α)(1 + |Des-cal|)⌉-th smallest value in ˆEi(Xn+1) for i ∈ Des-cal. 10: Output: ˆCnaive α (Xn+1) = [ˆqt∗ low(Xn+1) − ˆQ1−α(Xn+1), ˆqt∗ high(Xn+1) + ˆQ1−α(Xn+1)]. 36A8 Additional Results from Numerical Experiments A8.1 Outlier Detection Table A1: Performance of outlier detection based on classification models trained with different methods, on the CIFAR10 data set [52]. Other details are as in Figure 6. The numbers in paren- thesis indicate standard errors. The numbers in bold highlight TPR values within 1 standard error of the best TPR across all methods, for each sample size. Sample size Method TPR FPR 500 500 CES 0.296 (0.008) 0.098 (0.003) 500 Naive 0.295 (0.008) 0.097 (0.003) 500 Naive + theory 0.114 (0.006) 0.016 (0.001) 500 Data splitting 0.234 (0.008) 0.091 (0.003) 500 Full training 0.217 (0.011) 0.072 (0.004) 1000 1000 CES 0.401 (0.007) 0.100 (0.004) 1000 Naive 0.401 (0.007) 0.100 (0.004) 1000 Naive + theory 0.237 (0.006) 0.030 (0.002) 1000 Data splitting 0.337 (0.009) 0.094 (0.003) 1000 Full training 0.189 (0.013) 0.055 (0.004) 2000 2000 CES 0.450 (0.005) 0.100 (0.003) 2000 Naive 0.450 (0.005) 0.100 (0.003) 2000 Naive + theory 0.337 (0.006) 0.048 (0.002) 2000 Data splitting 0.404 (0.007) 0.095 (0.003) 2000 Full training 0.050 (0.009) 0.017 (0.003) A8.2 Multi-class Classification 37Table A2: Performance of multi-class classification based on classification models trained with different methods, on the CIFAR10 data set [52]. Other details are as in Figure 7. The numbers in parenthesis indicate standard errors. The numbers in bold highlight cardinality values within 1 standard error of the best cardinality across all methods, for each sample size. Sample size Method Cardinality Marignal coverage 500 500 CES 6.754 (0.074) 0.908 (0.003) 500 Naive 6.735 (0.072) 0.906 (0.003) 500 Naive + theory 9.193 (0.052) 0.988 (0.001) 500 Data splitting 7.022 (0.077) 0.900 (0.004) 500 Full training 6.759 (0.091) 0.909 (0.004) 1000 1000 CES 5.902 (0.060) 0.902 (0.003) 1000 Naive 5.908 (0.059) 0.901 (0.003) 1000 Naive + theory 7.767 (0.064) 0.972 (0.002) 1000 Data splitting 6.294 (0.063) 0.900 (0.004) 1000 Full training 6.270 (0.092) 0.897 (0.004) 2000 2000 CES 5.352 (0.045) 0.903 (0.003) 2000 Naive 5.347 (0.045) 0.902 (0.003) 2000 Naive + theory 6.609 (0.049) 0.955 (0.002) 2000 Data splitting 5.674 (0.040) 0.904 (0.003) 2000 Full training 7.776 (0.194) 0.934 (0.006) A8.3 Regression Coverage (marginal) Coverage (conditional) Width 200 500 1000 2000 200 500 1000 2000 200 500 1000 2000 400 600 800 0.7 0.8 0.9 1.0 0.7 0.8 0.9 1.0 Sample size Method CES Naive + theory Data splitting Full training Figure A12: Performance of conformal prediction intervals based on regression models trained with different methods, on the bike data set [54]. The results are shown as a function of the total sample size. The nominal marginal coverage level is 90%. See Table A4 for additional details and standard errors. 38Coverage (marginal) Coverage (conditional) Width 200 500 1000 200 500 1000 200 500 1000 20 40 60 80 0.7 0.8 0.9 1.0 0.7 0.8 0.9 1.0 Sample size Method CES Naive + theory Data splitting Full training Figure A13: Performance of conformal prediction intervals based on regression models trained with different methods, on the concrete data set [55]. The results are shown as a function of the total sample size. The nominal marginal coverage level is 90%. See Table A5 for additional details and standard errors. Table A3: Performance of conformal prediction intervals based on regression models trained with different methods, on the bio data set [44]. Other details are as in Figure 3. The numbers in parenthesis indicate standard errors. The numbers in bold highlight width values within 1 standard error of the best width across all methods, for each sample size. The numbers in red highlight coverage values below 0.85. Coverage Sample size Data Method Width Marginal Conditional 200 200 bio CES 18.740 (0.123) 0.924 (0.005) 0.898 (0.014) 200 bio Naive 18.544 (0.133) 0.917 (0.005) 0.899 (0.015) 200 bio Naive + theory 20.942 (0.006) 1.000 (0.000) 1.000 (0.000) 200 bio Data splitting 19.068 (0.113) 0.925 (0.005) 0.902 (0.015) 200 bio Full training 18.673 (0.125) 0.919 (0.004) 0.890 (0.018) 500 500 bio CES 17.435 (0.125) 0.914 (0.004) 0.909 (0.013) 500 bio Naive 17.363 (0.134) 0.910 (0.004) 0.890 (0.017) 500 bio Naive + theory 20.391 (0.061) 0.993 (0.001) 0.996 (0.001) 500 bio Data splitting 18.076 (0.123) 0.911 (0.004) 0.888 (0.015) 500 bio Full training 18.245 (0.129) 0.918 (0.003) 0.890 (0.019) 1000 1000 bio CES 16.251 (0.081) 0.901 (0.003) 0.885 (0.015) 1000 bio Naive 16.219 (0.084) 0.900 (0.003) 0.890 (0.012) 1000 bio Naive + theory 19.167 (0.073) 0.977 (0.002) 0.976 (0.006) 1000 bio Data splitting 16.728 (0.089) 0.903 (0.003) 0.887 (0.016) 1000 bio Full training 17.962 (0.141) 0.902 (0.004) 0.814 (0.022) 2000 2000 bio CES 15.812 (0.042) 0.899 (0.003) 0.893 (0.015) 2000 bio Naive 15.805 (0.042) 0.899 (0.003) 0.897 (0.014) 2000 bio Naive + theory 17.691 (0.047) 0.957 (0.002) 0.963 (0.006) 2000 bio Data splitting 16.043 (0.059) 0.900 (0.003) 0.903 (0.015) 2000 bio Full training 17.014 (0.113) 0.902 (0.003) 0.829 (0.019) 39Table A4: Performance of conformal prediction intervals based on regression models trained with different methods, on the bike data set [54]. Other details are as in Figure A12. The numbers in parenthesis indicate standard errors. The numbers in bold highlight width values within 1 standard error of the best width across all methods, for each sample size. The numbers in red highlight coverage values below 0.85. Coverage Sample size Data Method Width Marginal Conditional 200 200 bike CES 412.534 (6.439) 0.922 (0.004) 0.900 (0.018) 200 bike Naive 392.474 (6.019) 0.908 (0.005) 0.878 (0.016) 200 bike Naive + theory 913.440 (3.737) 1.000 (0.000) 1.000 (0.000) 200 bike Data splitting 427.964 (7.282) 0.920 (0.004) 0.910 (0.016) 200 bike Full training 444.656 (6.760) 0.920 (0.005) 0.905 (0.013) 500 500 bike CES 354.180 (4.183) 0.913 (0.004) 0.896 (0.017) 500 bike Naive 343.641 (4.220) 0.902 (0.004) 0.889 (0.018) 500 bike Naive + theory 622.837 (8.381) 0.990 (0.001) 0.989 (0.004) 500 bike Data splitting 371.079 (3.777) 0.912 (0.004) 0.908 (0.014) 500 bike Full training 381.951 (5.175) 0.913 (0.004) 0.881 (0.018) 1000 1000 bike CES 303.516 (3.047) 0.905 (0.004) 0.876 (0.017) 1000 bike Naive 300.091 (3.127) 0.903 (0.004) 0.894 (0.015) 1000 bike Naive + theory 484.565 (4.958) 0.977 (0.002) 0.982 (0.003) 1000 bike Data splitting 333.939 (2.891) 0.905 (0.003) 0.895 (0.018) 1000 bike Full training 308.981 (3.760) 0.901 (0.004) 0.891 (0.016) 2000 2000 bike CES 234.322 (1.935) 0.902 (0.003) 0.894 (0.018) 2000 bike Naive 231.571 (1.956) 0.897 (0.003) 0.893 (0.018) 2000 bike Naive + theory 334.724 (2.988) 0.957 (0.002) 0.954 (0.012) 2000 bike Data splitting 272.589 (2.532) 0.899 (0.003) 0.880 (0.019) 2000 bike Full training 240.714 (2.389) 0.901 (0.003) 0.901 (0.015) 40Table A5: Performance of conformal prediction intervals based on regression models trained with different methods, on the concrete data set [55]. Other details are as in Figure A13. The numbers in parenthesis indicate standard errors. The numbers in bold highlight width values within 1 standard error of the best width across all methods, for each sample size. The numbers in red highlight coverage values below 0.85. Coverage Sample size Data Method Width Marginal Conditional 200 200 concrete CES 26.948 (0.515) 0.928 (0.005) 0.904 (0.016) 200 concrete Naive 24.793 (0.520) 0.903 (0.006) 0.856 (0.021) 200 concrete Naive + theory 79.089 (0.130) 1.000 (0.000) 1.000 (0.000) 200 concrete Data splitting 29.021 (0.564) 0.924 (0.004) 0.888 (0.018) 200 concrete Full training 28.676 (0.568) 0.926 (0.005) 0.878 (0.021) 500 500 concrete CES 19.232 (0.263) 0.913 (0.004) 0.867 (0.018) 500 concrete Naive 18.340 (0.276) 0.896 (0.004) 0.829 (0.022) 500 concrete Naive + theory 39.492 (0.861) 0.989 (0.002) 0.990 (0.003) 500 concrete Data splitting 22.876 (0.323) 0.919 (0.003) 0.841 (0.023) 500 concrete Full training 19.857 (0.300) 0.903 (0.004) 0.824 (0.024) 930 930 concrete CES 14.399 (0.134) 0.908 (0.002) 0.863 (0.014) 930 concrete Naive 13.738 (0.127) 0.899 (0.002) 0.863 (0.011) 930 concrete Naive + theory 26.596 (0.334) 0.978 (0.001) 0.967 (0.004) 930 concrete Data splitting 16.659 (0.122) 0.906 (0.002) 0.864 (0.014) 930 concrete Full training 14.998 (0.143) 0.908 (0.002) 0.834 (0.016) 41A8.4 Quantile Regression Coverage (marginal) Coverage (conditional) Width 200 500 1000 200 500 1000 200 500 1000 2 3 4 0.7 0.8 1.0 0.7 0.8 1.0 Sample size Method CES Naive + theory Data splitting Full training Figure A14: Performance of conformal prediction intervals based on quantile regression models trained with different methods, on the community data set [60]. The results are shown as a function of the total sample size with error bars corresponding to standard error. The nominal marginal coverage level is 90%. See Table A7 for additional details and standard errors. Coverage (marginal) Coverage (conditional) Width 200 500 1000 2000 200 500 1000 2000 200 500 1000 2000 2 3 4 0.7 0.8 1.0 0.7 0.8 1.0 Sample size Method CES Naive + theory Data splitting Full training Figure A15: Performance of conformal prediction intervals based on quantile regression models trained with different methods, on the bio data set [44]. The results are shown as a function of the total sample size with error bars corresponding to standard error. The nominal marginal coverage level is 90%. See Table A8 for additional details and standard errors. Coverage (marginal) Coverage (conditional) Width 200 500 1000 2000 200 500 1000 2000 200 500 1000 2000 3 10 30 0.7 0.8 1.0 0.7 0.8 1.0 Sample size Method CES Naive + theory Data splitting Full training Figure A16: Performance of conformal prediction intervals based on quantile regression models trained with different methods, on the meps 21 data set [57].The results are shown as a function of the total sample size with error bars corresponding to standard error. The nominal marginal coverage level is 90%. See Table A9 for additional details and standard errors. 42Coverage (marginal) Coverage (conditional) Width 200 500 1000 2000 200 500 1000 2000 200 500 1000 2000 3 10 30 0.7 0.8 1.0 0.7 0.8 1.0 Sample size Method CES Naive + theory Data splitting Full training Figure A17: Performance of conformal prediction intervals based on quantile regression models trained with different methods, on the blog data data set [58]. The results are shown as a function of the total sample size with error bars corresponding to standard error. The nominal marginal coverage level is 90%. See Table A10 for additional details and standard errors. Coverage (marginal) Coverage (conditional) Width 200 500 1000 200 500 1000 200 500 1000 0.2 0.3 0.5 0.7 0.8 1.0 0.7 0.8 1.0 Sample size Method CES Naive + theory Data splitting Full training Figure A18: Performance of conformal prediction intervals based on quantile regression models trained with different methods, on the STAR data set [59]. The results are shown as a function of the total sample size with error bars corresponding to standard error. The nominal marginal coverage level is 90%. See Table A11 for additional details and standard errors. Coverage (marginal) Coverage (conditional) Width 200 500 1000 2000 200 500 1000 2000 200 500 1000 2000 1 2 3 0.7 0.8 1.0 0.7 0.8 1.0 Sample size Method CES Naive + theory Data splitting Full training Figure A19: Performance of conformal prediction intervals based on quantile regression models trained with different methods, on the bike data set [54]. The results are shown as a function of the total sample size with error bars corresponding to standard error. The nominal marginal coverage level is 90%. See Table A12 for additional details and standard errors. 43Table A6: Performance of conformal prediction intervals based on quantile regression models trained with different methods, on the homes data set [61]. Other details are as in Figure 8. The numbers in parenthesis indicate standard errors. The numbers in bold highlight width values within 1 standard error of the best width across all methods, for each sample size. The numbers in red highlight coverage values below 0.85. Coverage Sample size Data Method Width Marginal Conditional 200 200 homes CES 1.396 (0.047) 0.924 (0.007) 0.804 (0.016) 200 homes Naive 1.171 (0.048) 0.904 (0.009) 0.761 (0.029) 200 homes Naive + theory 2.607 (0.222) 0.983 (0.004) 0.931 (0.015) 200 homes Data splitting 1.329 (0.049) 0.907 (0.006) 0.730 (0.029) 200 homes Full training 1.195 (0.053) 0.919 (0.008) 0.674 (0.030) 500 500 homes CES 0.997 (0.022) 0.909 (0.007) 0.842 (0.016) 500 homes Naive 0.940 (0.026) 0.899 (0.007) 0.798 (0.018) 500 homes Naive + theory 2.698 (0.308) 0.989 (0.003) 0.950 (0.017) 500 homes Data splitting 0.985 (0.019) 0.896 (0.006) 0.810 (0.020) 500 homes Full training 0.940 (0.043) 0.913 (0.007) 0.710 (0.019) 1000 1000 homes CES 0.858 (0.015) 0.910 (0.005) 0.827 (0.017) 1000 homes Naive 0.824 (0.017) 0.894 (0.006) 0.784 (0.015) 1000 homes Naive + theory 1.229 (0.039) 0.969 (0.004) 0.917 (0.015) 1000 homes Data splitting 0.926 (0.020) 0.905 (0.004) 0.786 (0.021) 1000 homes Full training 0.755 (0.019) 0.902 (0.005) 0.698 (0.019) 2000 2000 homes CES 0.726 (0.016) 0.902 (0.005) 0.824 (0.013) 2000 homes Naive 0.667 (0.013) 0.891 (0.003) 0.799 (0.016) 2000 homes Naive + theory 0.865 (0.013) 0.949 (0.003) 0.880 (0.013) 2000 homes Data splitting 0.779 (0.011) 0.900 (0.003) 0.804 (0.013) 2000 homes Full training 0.653 (0.013) 0.889 (0.005) 0.686 (0.019) 44Table A7: Performance of conformal prediction intervals based on quantile regression models trained with different methods, on the community data set [60]. Other details are as in Figure A14. The numbers in parenthesis indicate standard errors. The numbers in bold highlight width values within 1 standard error of the best width across all methods, for each sample size. The numbers in red highlight coverage values below 0.85. Coverage Sample size Data Method Width Marginal Conditional 200 200 community CES 2.226 (0.057) 0.905 (0.006) 0.835 (0.018) 200 community Naive 2.100 (0.065) 0.891 (0.009) 0.814 (0.018) 200 community Naive + theory 3.876 (0.193) 0.980 (0.004) 0.963 (0.008) 200 community Data splitting 2.250 (0.081) 0.902 (0.007) 0.811 (0.017) 200 community Full training 2.345 (0.070) 0.903 (0.007) 0.752 (0.023) 500 500 community CES 1.957 (0.033) 0.902 (0.006) 0.849 (0.014) 500 community Naive 1.866 (0.043) 0.889 (0.007) 0.830 (0.012) 500 community Naive + theory 4.312 (0.254) 0.990 (0.002) 0.971 (0.008) 500 community Data splitting 2.041 (0.037) 0.904 (0.006) 0.843 (0.011) 500 community Full training 2.084 (0.067) 0.898 (0.007) 0.770 (0.018) 994 994 community CES 1.717 (0.016) 0.911 (0.003) 0.850 (0.007) 994 community Naive 1.601 (0.019) 0.902 (0.003) 0.853 (0.008) 994 community Naive + theory 2.486 (0.049) 0.971 (0.002) 0.943 (0.005) 994 community Data splitting 1.818 (0.013) 0.910 (0.002) 0.865 (0.008) 994 community Full training 1.784 (0.029) 0.901 (0.003) 0.797 (0.011) 45Table A8: Performance of conformal prediction intervals based on quantile regression models trained with different methods, on the bio data set [44]. Other details are as in Figure A15. The numbers in parenthesis indicate standard errors. The numbers in bold highlight width values within 1 standard error of the best width across all methods, for each sample size. The numbers in red highlight coverage values below 0.85. Coverage Sample size Data Method Width Marginal Conditional 200 200 bio CES 2.732 (0.074) 0.926 (0.006) 0.848 (0.022) 200 bio Naive 2.018 (0.064) 0.876 (0.011) 0.805 (0.025) 200 bio Naive + theory 3.392 (0.254) 0.971 (0.005) 0.954 (0.010) 200 bio Data splitting 2.334 (0.083) 0.902 (0.010) 0.803 (0.029) 200 bio Full training 2.848 (0.124) 0.899 (0.010) 0.844 (0.016) 500 500 bio CES 2.154 (0.034) 0.921 (0.004) 0.872 (0.010) 500 bio Naive 1.900 (0.032) 0.892 (0.007) 0.839 (0.014) 500 bio Naive + theory 3.893 (0.161) 0.995 (0.001) 0.996 (0.002) 500 bio Data splitting 2.015 (0.024) 0.903 (0.004) 0.840 (0.013) 500 bio Full training 2.579 (0.057) 0.890 (0.011) 0.819 (0.032) 1000 1000 bio CES 1.997 (0.019) 0.918 (0.004) 0.890 (0.013) 1000 bio Naive 1.871 (0.018) 0.894 (0.005) 0.838 (0.018) 1000 bio Naive + theory 2.318 (0.033) 0.969 (0.002) 0.943 (0.010) 1000 bio Data splitting 1.895 (0.018) 0.898 (0.006) 0.811 (0.018) 1000 bio Full training 2.381 (0.035) 0.877 (0.010) 0.786 (0.037) 2000 2000 bio CES 1.869 (0.018) 0.906 (0.004) 0.867 (0.016) 2000 bio Naive 1.763 (0.017) 0.890 (0.004) 0.834 (0.016) 2000 bio Naive + theory 1.998 (0.017) 0.948 (0.003) 0.922 (0.014) 2000 bio Data splitting 1.830 (0.013) 0.898 (0.004) 0.852 (0.015) 2000 bio Full training 2.219 (0.029) 0.874 (0.008) 0.781 (0.035) 46Table A9: Performance of conformal prediction intervals based on quantile regression models trained with different methods, on the MEPS 21 data set [57]. Other details are as in Figure A16. The numbers in parenthesis indicate standard errors. The numbers in bold highlight width values within 1 standard error of the best width across all methods, for each sample size. The numbers in red highlight coverage values below 0.85. Coverage Sample size Data Method Width Marginal Conditional 200 200 meps 21 CES 4.442 (0.468) 0.927 (0.007) 0.844 (0.020) 200 meps 21 Naive 3.602 (0.452) 0.901 (0.010) 0.786 (0.025) 200 meps 21 Naive + theory 17.470 (2.874) 0.979 (0.003) 0.943 (0.012) 200 meps 21 Data splitting 3.239 (0.212) 0.909 (0.008) 0.780 (0.021) 200 meps 21 Full training 6.070 (0.509) 0.906 (0.009) 0.654 (0.030) 500 500 meps 21 CES 3.453 (0.074) 0.912 (0.004) 0.845 (0.015) 500 meps 21 Naive 3.077 (0.068) 0.898 (0.005) 0.802 (0.012) 500 meps 21 Naive + theory 35.882 (4.705) 0.991 (0.002) 0.974 (0.008) 500 meps 21 Data splitting 3.147 (0.109) 0.906 (0.006) 0.832 (0.012) 500 meps 21 Full training 6.038 (0.305) 0.913 (0.005) 0.709 (0.021) 1000 1000 meps 21 CES 3.447 (0.068) 0.896 (0.005) 0.833 (0.012) 1000 meps 21 Naive 3.102 (0.073) 0.888 (0.006) 0.825 (0.014) 1000 meps 21 Naive + theory 6.054 (0.443) 0.970 (0.002) 0.930 (0.009) 1000 meps 21 Data splitting 3.231 (0.093) 0.901 (0.005) 0.817 (0.016) 1000 meps 21 Full training 5.183 (0.188) 0.902 (0.004) 0.646 (0.019) 2000 2000 meps 21 CES 3.666 (0.054) 0.907 (0.004) 0.867 (0.012) 2000 meps 21 Naive 3.454 (0.047) 0.902 (0.004) 0.875 (0.013) 2000 meps 21 Naive + theory 4.242 (0.069) 0.951 (0.003) 0.925 (0.010) 2000 meps 21 Data splitting 3.485 (0.060) 0.902 (0.003) 0.847 (0.009) 2000 meps 21 Full training 4.349 (0.128) 0.897 (0.003) 0.651 (0.018) 47Table A10: Performance of conformal prediction intervals based on quantile regression models trained with different methods, on the blog data data set [58]. Other details are as in Figure A17. The numbers in parenthesis indicate standard errors. The numbers in bold highlight width values within 1 standard error of the best width across all methods, for each sample size. The numbers in red highlight coverage values below 0.85. Coverage Sample size Data Method Width Marginal Conditional 200 200 blog data CES 4.658 (0.194) 0.911 (0.007) 0.779 (0.021) 200 blog data Naive 3.427 (0.210) 0.897 (0.008) 0.766 (0.021) 200 blog data Naive + theory 22.166 (5.645) 0.973 (0.004) 0.917 (0.017) 200 blog data Data splitting 4.049 (0.359) 0.892 (0.013) 0.720 (0.034) 200 blog data Full training 7.524 (0.544) 0.910 (0.007) 0.662 (0.026) 500 500 blog data CES 4.538 (0.291) 0.924 (0.005) 0.819 (0.017) 500 blog data Naive 3.986 (0.333) 0.917 (0.005) 0.796 (0.014) 500 blog data Naive + theory 58.965 (11.745) 0.992 (0.002) 0.987 (0.004) 500 blog data Data splitting 3.443 (0.139) 0.905 (0.005) 0.751 (0.020) 500 blog data Full training 9.364 (0.535) 0.927 (0.003) 0.719 (0.019) 1000 1000 blog data CES 4.313 (0.088) 0.903 (0.004) 0.812 (0.016) 1000 blog data Naive 3.559 (0.073) 0.896 (0.004) 0.816 (0.016) 1000 blog data Naive + theory 7.336 (0.620) 0.973 (0.002) 0.951 (0.007) 1000 blog data Data splitting 3.525 (0.119) 0.896 (0.003) 0.788 (0.015) 1000 blog data Full training 7.784 (0.427) 0.919 (0.003) 0.694 (0.018) 2000 2000 blog data CES 4.169 (0.066) 0.906 (0.003) 0.808 (0.013) 2000 blog data Naive 3.671 (0.047) 0.899 (0.003) 0.783 (0.018) 2000 blog data Naive + theory 4.486 (0.074) 0.954 (0.002) 0.911 (0.009) 2000 blog data Data splitting 3.625 (0.069) 0.899 (0.003) 0.805 (0.011) 2000 blog data Full training 5.323 (0.274) 0.909 (0.003) 0.718 (0.014) 48Table A11: Performance of conformal prediction intervals based on quantile regression models trained with different methods, on the STAR data set [59]. Other details are as in Figure A18. The numbers in parenthesis indicate standard errors. The numbers in bold highlight width values within 1 standard error of the best width across all methods, for each sample size. The numbers in red highlight coverage values below 0.85. Coverage Sample size Data Method Width Marginal Conditional 200 200 star CES 0.567 (0.023) 0.933 (0.006) 0.891 (0.012) 200 star Naive 0.421 (0.031) 0.904 (0.008) 0.847 (0.012) 200 star Naive + theory 0.656 (0.038) 0.984 (0.002) 0.960 (0.006) 200 star Data splitting 0.502 (0.022) 0.911 (0.009) 0.862 (0.013) 200 star Full training 0.323 (0.008) 0.914 (0.006) 0.872 (0.012) 500 500 star CES 0.318 (0.007) 0.929 (0.004) 0.893 (0.012) 500 star Naive 0.288 (0.008) 0.902 (0.007) 0.848 (0.016) 500 star Naive + theory 0.520 (0.025) 0.992 (0.001) 0.978 (0.005) 500 star Data splitting 0.328 (0.009) 0.910 (0.006) 0.860 (0.015) 500 star Full training 0.224 (0.005) 0.909 (0.006) 0.874 (0.013) 1000 1000 star CES 0.252 (0.005) 0.914 (0.003) 0.864 (0.012) 1000 star Naive 0.236 (0.007) 0.897 (0.003) 0.862 (0.011) 1000 star Naive + theory 0.323 (0.007) 0.967 (0.003) 0.929 (0.009) 1000 star Data splitting 0.269 (0.007) 0.903 (0.003) 0.846 (0.012) 1000 star Full training 0.182 (0.002) 0.898 (0.004) 0.848 (0.012) 1161 1161 star CES 0.237 (0.005) 0.920 (0.004) 0.882 (0.012) 1161 star Naive 0.220 (0.005) 0.900 (0.005) 0.860 (0.012) 1161 star Naive + theory 0.308 (0.007) 0.969 (0.002) 0.944 (0.007) 1161 star Data splitting 0.254 (0.007) 0.903 (0.004) 0.864 (0.014) 1161 star Full training 0.174 (0.003) 0.896 (0.004) 0.863 (0.013) 49Table A12: Performance of conformal prediction intervals based on quantile regression models trained with different methods, on the bike data set [54]. Other details are as in Figure A19. The numbers in parenthesis indicate standard errors. The numbers in bold highlight width values within 1 standard error of the best width across all methods, for each sample size. The numbers in red highlight coverage values below 0.85. Coverage Sample size Data Method Width Marginal Conditional 200 200 bike CES 2.690 (0.065) 0.923 (0.006) 0.895 (0.014) 200 bike Naive 2.535 (0.053) 0.905 (0.008) 0.868 (0.015) 200 bike Naive + theory 4.106 (0.192) 0.979 (0.003) 0.958 (0.008) 200 bike Data splitting 2.663 (0.056) 0.899 (0.008) 0.832 (0.018) 200 bike Full training 2.843 (0.095) 0.910 (0.006) 0.870 (0.012) 500 500 bike CES 2.244 (0.039) 0.913 (0.004) 0.886 (0.014) 500 bike Naive 2.137 (0.047) 0.901 (0.006) 0.880 (0.012) 500 bike Naive + theory 4.284 (0.152) 0.991 (0.002) 0.984 (0.005) 500 bike Data splitting 2.275 (0.032) 0.902 (0.004) 0.859 (0.014) 500 bike Full training 2.265 (0.062) 0.908 (0.004) 0.839 (0.014) 1000 1000 bike CES 1.978 (0.036) 0.911 (0.004) 0.872 (0.013) 1000 bike Naive 1.779 (0.057) 0.893 (0.004) 0.821 (0.016) 1000 bike Naive + theory 2.590 (0.061) 0.968 (0.002) 0.955 (0.007) 1000 bike Data splitting 1.996 (0.040) 0.903 (0.004) 0.852 (0.013) 1000 bike Full training 1.625 (0.045) 0.901 (0.005) 0.858 (0.013) 2000 2000 bike CES 1.262 (0.030) 0.902 (0.003) 0.854 (0.012) 2000 bike Naive 1.113 (0.029) 0.884 (0.004) 0.847 (0.012) 2000 bike Naive + theory 1.570 (0.033) 0.948 (0.003) 0.933 (0.009) 2000 bike Data splitting 1.517 (0.033) 0.893 (0.003) 0.853 (0.011) 2000 bike Full training 1.061 (0.019) 0.887 (0.004) 0.839 (0.013) 50A9 Mathematical Proofs Proof of Theorem 1. It suffices to show that the nonconformity scores ˆSi for i ∈ {n+1}∪D es-cal are exchangeable. In fact, if the nonconformity scores are almost-surely unique, this implies the rank of ˆSn+1 is uniformly distributed over {ˆSi}i∈{n+1}∪Des-cal, and in that case the conformal p-value is uniformly distributed over {1/(1+ |Des-cal|), 2/(1+ |Des-cal|), . . . ,1}. If the nonconformity scores are not almost-surely unique and ties are not broken at random, then the distribution of the conformal p-value becomes stochastically larger than uniform, in which case the result still holds. To prove the exchangeability of the nonconformity scores, let σ be any permutation of {n + 1} ∪ Des-cal, and imagine applying Algorithm 1, with the same random seed, to the shuffled data set indexed by σ({n + 1} ∪ Des-cal), which has the same distribution as the original data set. To clarify the notation, we will refer to quantities computed under this data shuffling scenario with their usual symbol followed by an apostrophe; i.e.,M′ t1 instead of Mt1. As the gradient updates only involve the unperturbed observations in Dtrain and the maximum number of epochs tmax is fixed, the sequence of saved models remains exactly the same under this scenario: ( M′ t1, . . . , M′ tT ) = ( Mt1, . . . , MtT ). Further, the loss function in (1) is also invariant to permutations of {n + 1} ∪ Des-cal, in the sense that L+1′ es-cal = L+1 es-cal, because L is additive. Therefore, the model selected according to (2) is also invariant, ˆM′ ces = ˆMces, which implies the nonconformity scores are simply re-ordered: ˆS′ σ(i) = ˆSi. Therefore, we have: σ({ˆSi}i∈{n+1}∪Dcal) = {ˆS′ i}i∈{n+1}∪Dcal d= {ˆSi}i∈{n+1}∪Dcal, where the last equality follows from the initial data exchangeability assumption. Proof of Theorem 2. Note that, conditional on Yn+1 = y, the miscoverage event Yn+1 ̸∈ ˆCα(Xn+1) occurs if and only if ˆ uy(Xn+1) ≤ α, where ˆuy(Xn+1) is defined as in (6). Therefore, it suffices to show P[ˆuy(Xn+1) ≤ α | Yn+1 = y] ≤ α for any α ∈ (0, 1). However, this is directly implied by Theorem 1, because the ˆuy(Xn+1) calculated by Algorithm 2 is equivalent to the conformal p-value ˆu0(Zn+1) given by Algorithm 1 applied to the subset of the data in Des-cal with Yi = y, with the understanding that Zi = (Xi, Yi) for all i ∈ {n + 1} ∪ Des-cal. Proof of Theorem A5. Note that Yn+1 ̸∈ ˆCm α (Xn+1) if and only if ˆ umarg(Xn+1; Yn+1) ≤ α, where ˆumarg(Xn+1; Yn+1) is defined as in (A23). Hence it suffices to show thatP[ˆumarg(Xn+1; Yn+1) ≤ α] ≤ α for any α ∈ (0, 1). This can be established using the same approach as in the proof of Theorem 1, setting Zi = (Xi, Yi) for all i ∈ {n + 1} ∪ Des-cal. In fact, the maximum number of epochs tmax is fixed, the sequence of saved models is invariant to permutations of {n + 1} ∪ Des-cal, and the model ˆMces selected according to (5) is also invariant. Thus, it follows that the nonconformity scores ˆSi are exchangeable with one another for all i ∈ {n + 1} ∪ Des-cal. Proof of Theorem 3. Consider an imaginary oracle algorithm producing an interval ˆCoracle α (Xn+1) defined as ˆCoracle α (Xn+1) = Bl∗(Yn+1) T ˆCα(Xn+1, Bl∗(Yn+1)), where l∗(Yn+1) is the exact index of the bin Bl to which the true Yn+1 belongs. Clearly, this oracle is just a theoretical tool, not a practical method because the outcome value for the test point is unknown. However, this oracle is useful because it is easier to analyze, and it suffices to establish that P[Yn+1 ∈ ˆCoracle α (Xn+1)] ≥ 1 − α, for any α ∈ (0, 1), since ˆCα(Xn+1) ⊇ ˆCoracle α (Xn+1) almost-surely. The coverage property for the oracle can be established using an approach similar to that of the proof of Theorem 1, setting 51Zi = ( Xi, Yi) for all i ∈ {n + 1} ∪ Des-cal. In fact, the maximum number of epochs tmax is fixed, the sequence of saved models is invariant to permutations of {n + 1} ∪ Des-cal, and the model ˆMces selected by the oracle according to (10) is also invariant. Thus, it follows that the oracle nonconformity scores ˆS∗ i = ˆSi(Xn+1, Bl∗(Yn+1)) are exchangeable with one another for all i ∈ {n + 1} ∪ Des-cal. Further, by construction of the prediction intervals (12), we know that the miscoverage event Yn+1 ̸∈ ˆCoracle α (Xn+1) occurs if and only if ˆS∗ i > ˆQ∗ 1−α, where ˆQ∗ 1−α is the ⌈(1 − α)(1 +|Des-cal|)⌉-th smallest value among all nonconformity scores ˆSi(Xn+1, Bl). However, it is a well-known exchangeability result that P[ ˆS∗ i ≤ ˆQ∗ 1−α] ≥ 1 − α; see for example Lemma 1 in Romano et al. [31]. Proof of Theorem 4. Same as the proof of Theorem 3. Proof of Corollary A3. This corollary follows immediately from Theorem 3 because the prediction interval given by Algorithm A11 is always contained in that output by Algorithm 3. Proof of Corollary A4. Same as the proof of Corollary A3. Proof of Proposition A1. Note that ˆunaive 0 (Zn+1) = ˆunaive 0 (Zn+1; t∗), hence P  ˆunaive 0 (Zn+1) > α  = E  P  ˆunaive 0 (Zn+1; t∗) > α| Des-cal  ≥ E  min t∈[T] Wt  ≥ sup a∈[0,1] a · P  min t∈[T] Wt ≥ a  = sup a∈[0,1] a  1 − P  min t∈[T] Wt ≤ a  ≥ sup a∈[0,1] a (1 − T · P[Wt ≤ a]) , where the last inequality follows from a union bound. To simplify the right-hand-side term above, let a = I−1   1 bT ; nes-cal + 1 − l, l  , where b is any large constant. Hence we obtain P  ˆunaive 0 (Zn+1) > α  ≥ I−1  1 bT ; nes-cal + 1 − l, l  · (1 − 1/b). Proof of Corollary A1. Note that Yn+1 ∈ ˆCnaive α (Xn+1) if and only if ˆunaive Yn+1 (Xn+1; t∗) > α. Let Wt denote the calibration conditional coverage P h ˆunaive Yn+1 (Xn+1; t) > α| Des-cal i . Then, we have P h Yn+1 ∈ ˆCnaive α (Xn+1) i = E h P h ˆunaive Yn+1 (Xn+1; t∗) > α| Des-cal ii = E[Wt∗] ≥ E  min t∈[T] Wt  . The rest of the proof follows the same argument as in the proof of Proposition A1. 52Proof of Corollary A2. Let ˆSi(Xn+1, t) = |Yi − ˆµt(Xi)| denote the residual score calculated with model t ∈ [T], for all i ∈ Des-cal. Note that Yn+1 ∈ ˆCnaive α (Xn+1) if and only if ˆSXn+1(Xn+1, t∗) ≤ ˆQ1−α. Then, we just need to bound Wt = P h ˆSXn+1(Xn+1, t∗) ≤ ˆQ1−α | Des-cal i , and the rest of the proof follows the same steps as the proof of Proposition A1. Proof of Lemma A1. Recall that l = ⌊α(nes-cal + 1)⌋, and define the following helpful notations: Beta (nes-cal + 1 − l, l) := Beta (nes-cal · c, nes-cal · d) , where c = nes-cal + 1 − l nes-cal , d = l nes-cal . Denote Gamma(k, θ) as the gamma distribution with shape parameter k and scale parameter θ. It is a well known fact that the beta distribution can be expressed as a ratio of gamma distributions as: Beta (nes-cal · c, nes-cal · d) = Gamma(nes-cal · c, 1) Gamma(nes-cal · c, 1) + Gamma(nes-cal · d, 1). Further, Gamma(nes-cal · c, 1) can be seen as the distribution of a sum of nes-cal · c independent exponentially distributed random variables with mean equal to 1; therefore, by the central limit theorem, Gamma( nes-cal · c, 1) has an asymptotic Gaussian distribution as nes-cal → ∞. Denote Φ(x, µ, σ2) as the cumulative distribution function of a Gaussian random variable with mean µ and variance σ2. Applying the delta method, it follows that, in the limit of large nes-cal, I (x; nes-cal · c, nes-cal · d) = Φ  x; c c + d, 1 nes-cal · cd (c + d)3  + O  1 nes-cal  , for any x ∈ [0, 1]. Since I and Φ are continuous and strictly increasing over [0, 1], letting Φ−1 be the inverse Gaussian CDF, we have I−1  1 bT ; nes-cal · c, nes-cal · d  = Φ−1  1 bT ; c c + d, 1 nes-cal · cd (c + d)3  + O  1 nes-cal  = Φ−1  1 bT ; 1− α, α(1 − α) nes-cal + 1  + O  1 nes-cal  = (1 − α) + s α(1 − α) nes-cal + 1 · Φ−1  1 bT ; 0, 1  + O  1 nes-cal  = (1 − α) − s α(1 − α) nes-cal + 1 · p 2 log(bT) + O   1p nes-cal log(T) ! , where the second equality is obtained by substituting c and d with their defined values and the last inequality follows from Equation 26.2.23 in Abramowitz and Stegun [65] for sufficiently large T. 53
```

## **Instructions:**
1. Analyze the paper based on the categories listed below.
2. Your response **must be a valid JSON object** that can be directly parsed using `json.loads()`.
3. Do not include any extra text, explanations, or formatting outside of the JSON object.
4. **If a field has no available information, set its value to `"Not mentioned"` instead of leaving it empty.**
5. Ensure that the JSON format is correct, including the use of **double quotes (`"`) for all keys and values.**
## **Output Format (JSON)**:
```json
{
    "main_contributions": "<Concise description of the main research problem and contributions>",
    "methodology": "<Brief explanation of the key techniques, models, or algorithms>",
    "experimental_setup": "<Description of datasets, benchmarks, and validation methods>",
    "limitations": "<Summary of weaknesses, constraints, or assumptions>",
    "future_research_directions": "<Potential areas for extending this research>"
}
```
Output:
{
    "main_contributions": "The paper introduces Conformalized Early Stopping (CES), a novel method that efficiently combines early stopping with conformal calibration using the same hold-out data. This addresses the limitation of neural networks trained with early stopping, which often lack precise statistical guarantees, and avoids the need for multiple data splits or overly conservative adjustments typically required for rigorous predictive inferences. CES enables accurate models that provide exact predictive inferences without additional data cost, with practical implementations for outlier detection, multi-class classification, and regression (including quantile regression).",
    "methodology": "CES leverages a single hold-out dataset (Des-cal) for both early stopping and conformal inference. It involves training a neural network for a maximum number of epochs (tmax), storing intermediate models at regular intervals (τ epochs). For a test point, an 'augmented loss' function, L+1 es-cal(Mt, z), is defined, which incorporates the test point into the loss calculation over the Des-cal set. The best intermediate model is then selected by minimizing this augmented loss. Nonconformity scores are computed using this selected model for the Des-cal set and the test point. For outlier detection and classification, this leads to conformal p-values and prediction sets with finite-sample validity (label-conditional or marginal coverage). For regression tasks using squared-error loss or pinball loss (for quantile regression), the selection of the best model for continuous outcomes involves finding the lower envelope of a family of quadratic or piecewise-linear pinball loss functions, respectively, which is done using an efficient O(T log T) divide-and-conquer algorithm. This results in a step function for the best model, allowing for the construction of prediction intervals that guarantee marginal coverage.",
    "experimental_setup": "The performance of CES was evaluated across different learning tasks using various datasets. For outlier detection and multi-class classification, the CIFAR10 dataset (60,000 32x32 RGB images from 10 classes) was used, training a convolutional neural network with ReLU activations for 50 epochs. For regression with squared-error loss, three UCI datasets were used: 'bio', 'bike', and 'concrete', training neural networks with two hidden layers (width 128, ReLU) for up to 1000 epochs. For quantile regression (CQR), seven UCI datasets were used: 'MEPS 21', 'blog data', 'STAR', 'community', 'bio', 'homes', and 'bike', training neural networks with three hidden layers (ReLU) for up to 2000 epochs. Input features were standardized, and response values rescaled for CQR. Data was split into 75% for training and 25% for Des-cal in CES. Test sets contained 100 to 1000 independent data points. Metrics included False Positive Rate (FPR) and True Positive Rate (TPR) for outlier detection, average cardinality and marginal coverage for classification, and marginal coverage, worst-slab conditional coverage, and average width for regression. CES was compared against a naive early stopping benchmark with theoretical correction, early stopping with a three-way data split, and full training without early stopping. Results were averaged over 25 to 100 independent trials.",
    "limitations": "The computational cost of CES may become a barrier in some applications if the number of candidate models (T) is very large, although it scales slightly more than linearly, recommending moderate values for T (e.g., 100 or 1000). The storage cost for saving all intermediate models is higher than standard early stopping, but is argued to be feasible for many real-world situations (e.g., 10GB for 100 ResNet50 models). In some rare cases, the regression algorithms (Algorithms 3 and 4) might produce empty prediction sets, which is addressed by a fallback mechanism to a naive conformal prediction interval (Algorithms A11 and A12). While the naive benchmark performs similarly to CES in practice for large datasets, it lacks rigorous finite-sample guarantees, and its theoretical corrections are shown to be overly conservative.",
    "future_research_directions": "The methodology presented in this paper extends beyond early stopping and conformal calibration. It can be applied to hyper-parameter tuning and selecting the most promising candidate from an arbitrary machine learning toolbox for outlier detection. Furthermore, the techniques can be used to calibrate, without further data splits, models selected in a data-driven way from arbitrary machine learning toolboxes in multi-class classification and regression. The proposed method could also be combined with existing research on the robustness of conformal inferences to distribution shifts and failures of the data exchangeability assumption."
}
