
Input:
You are a cutting-edge AI researcher. Based on the new method described in # New Methods and the experimental policy outlined in # Experiment Strategy, please generate 1 distinct Experiment objects.

# Instructions
- Generate 1 major experimental lines (Experiment objects) based on the experimental strategy.
- Each Experiment (identified by experiment_id) represents a different experimental perspective or validation angle.
- Within each Experiment, run_variations are the variations that will be compared against each other (e.g., ["baseline", "proposed"], ["full-method", "ablation-A", "ablation-B"]).
- Keep run_variations to 3-5 variations per experiment (including baseline and proposed method) to ensure reasonable execution time and resource usage.
- Each Experiment should:
    - Have a unique experiment_id (e.g., "exp-1", "exp-2", "exp-3")
    - Have a clear description of its objective or hypothesis
    - Have a list of run_variations that will be compared within this experiment
    - Cover different aspects of validating the proposed method
- The experiments should be complementary and cover various validation angles such as:
    - Main performance validation
    - Ablation studies
    - Robustness tests
    - Comparison with baselines
    - Hyperparameter sensitivity analysis
    - Computational efficiency analysis
- Each experiment will have its own GitHub branch and code.
- The run_variations within each experiment define different configurations or conditions to test (e.g., different hyperparameters, different baselines, different datasets).

- Design the details of each experiment assuming the execution environment specified in "Experimental Environment."
- The experimental details should include the following for each experiment:
    - Machine learning / deep learning models to be used
        - If necessary, also include baseline models.
    - Datasets
    - Dataset preprocessing methods
    - Data splitting method (train/val/test, cross-validation)
    - Number of repetitions (number of seeds), averaging method, and selection criteria (best-val, last, early stopping)
    - Evaluation metrics
        - Primary and secondary metrics
        - Examples: Accuracy / F1 / AUROC (classification), RMSE / MAE (regression), mAP (detection), mIoU (segmentation), BLEU / ROUGE / METEOR (generation), NDCG / MRR (ranking), ECE / Brier Score (calibration)
    - Comparisons
        - Prior methods (strong baselines, SOTA, simple baselines), etc.
        - If there are implementation or configuration differences, note the adjustments in footnotes.
    - Methods for analyzing important hyperparameters (e.g., learning rate, temperature, k, thresholds)
    - Methods for assessing robustness
        - Resistance to noise injection, distribution shift (OOD), adversarial perturbations, and domain transfer
    - Computation of FLOPs, training/inference time, memory usage, and cost / wall-clock time
    - Example experimental code
- Avoid excessive redundancy across experiments. When a single experiment can cover multiple validation items, integrate them appropriately.
- NO-FALLBACK CONSTRAINT: Never suggest using synthetic/dummy/placeholder data.
- Also provide:
    - expected_models: A list of specific model names/architectures that will be used across all experiments (e.g., ["ResNet-50", "BERT-base", "GPT-3.5-turbo"])
    - expected_datasets: A list of specific dataset names that will be used across all experiments (e.g., ["CIFAR-10", "ImageNet", "IMDB Reviews"])

## Output Format
Please provide:
- experiments: A list of 1 Experiment objects, each with:
    - experiment_id: Unique identifier
    - run_variations: List of variation names/identifiers for this experiment
    - description: Detailed description including all aspects mentioned in the instructions
- expected_models: List of model names/architectures
- expected_datasets: List of dataset names

# Experimental Environment
NVIDIA Tesla T4
VRAM：16 GB
RAM：500 MB

# Current Research Method (Target for Experiment Design)
{
    "Open Problems": "1. Most BO‐based HPO algorithms either (a) train a configuration to a fixed budget (Hyperband family) or (b) embed the full learning curve in an expensive joint surrogate (e.g. BOIL, DyHPO).  Both strategies waste budget on clearly converged runs.\n2. A very cheap, task–agnostic early–stopping rule that can be fed back into a BO loop without re–designing the surrogate is still missing.",
    "Methods": "SlopeStop-BO (Slope-informed Early-Stopping Bayesian Optimisation)\n1. Online slope test  ▸  During training we keep a W–step moving window of the validation score v_t.  At step t we compute the window slope s_t = (v_t – v_{t-W})/W.\n2. Stopping rule  ▸  If s_t < ϵ·exp(–γ·t) the run is considered saturated and training is terminated immediately (two scalars ϵ,γ are global hyper-parameters chosen once per task; default ϵ=10⁻³, γ=0.01).\n3. Fast curve compression  ▸  At termination we store a two–dimensional summary per configuration x:  (μ=mean(v_{t-W: t}),  τ=t_stop / T_max) instead of the whole curve.  This removes the need for a product kernel over (x,t).\n4. Lightweight surrogate  ▸  A standard GP with ARD RBF kernel is fitted on the augmented input [x ; τ] → μ. GP hyper-parameters are updated every 2·D observations by ML; no expensive joint optimisation over logistic parameters as in BOIL.\n5. Acquisition  ▸  Expected Improvement divided by estimated cost c(x)=τ·T_max.  The next configuration is chosen with EI/c as usual.\n6. Budget bookkeeping  ▸  Because many runs stop early, the total consumed epochs fall 30-50 % below a fixed-budget scheme.",
    "Experimental Setup": "Datasets:  • LCBench (35 tabular tasks, 50 epoch learning curves)  • NAS-Bench-201 (3 image tasks, 200 epoch curves).\nBaselines: Hyperband, BOHB, DyHPO, BOIL.\nProtocol: 10 independent seeds, total wall-clock budget identical to BOIL (GPU: RTX 3090).\nHyper-parameters W=3, ϵ=1e-3, γ=0.01 fixed for all tasks.",
    "Experimental Code": "# pseudo-python\nfor conf in pool:\n    t, buf = 0, []\n    while t < T_max:\n        loss = train_one_epoch(conf)\n        buf.append(loss); t += 1\n        if len(buf) > W:\n            slope = (buf[-1]-buf[-W-1])/W\n            if slope < eps*np.exp(-gamma*t):\n                break                # ← early stop\n    mu = np.mean(buf[-W:]); tau = t / T_max\n    D.append(([conf, tau], mu, cost=t))\n    if len(D)% (2*dim)==0: refit_GP(D)\n    x_next = argmax_EI_over_cost(GP)",
    "Expected Result": "Compared with BOIL:  • ~40 % less total training epochs  • identical median simple-regret after budget normalisation.\nCompared with Hyperband/BOHB:  • ~60 % lower final regret for the same wall time.",
    "Expected Conclusion": "A purely slope-based, task-agnostic early-stopping rule plus a standard GP surrogate is enough to harvest most of the budget savings that more complex learning-curve models aim for, while keeping the optimisation loop simple and cheap.  SlopeStop-BO is therefore a practical drop-in replacement for fixed-budget BO in large-scale HPO campaigns."
}

# Experiment Strategy
Overall Experimental Strategy for Validating SlopeStop-BO

0. Guiding Principle
Evaluate whether a cheap, task–agnostic slope-based early-stopping rule coupled with a lightweight GP surrogate (a) attains the same or better optimisation quality than existing HPO methods and (b) does so with demonstrably lower computational cost, higher robustness and broader applicability.

1. Core Properties to Validate
1.1 Optimisation quality: final and anytime simple-regret / best-seen validation error.
1.2 Computational efficiency: (i) consumed training epochs, (ii) wall-clock time, (iii) GPU memory footprint.
1.3 Cost–performance trade-off: Pareto frontier of regret vs cost.
1.4 Robustness & stability: variance across random seeds and across heterogeneous tasks.
1.5 Generalisation: behaviour on unseen domains (vision vs tabular, small vs large models, shallow vs deep learning curves).
1.6 Method components: contribution of each design choice (slope rule, curve compression, EI/c acquisition).

2. Required Comparisons
2.1 External baselines: Hyperband, BOHB, DyHPO, BOIL, vanilla fixed-budget BO.
2.2 Internal ablations:
    A0  No early stop (fixed budget) + same GP + EI/c.
    A1  SlopeStop w/o cost-aware acquisition.
    A2  SlopeStop with full-curve GP (replace compression).
    A3  Vary (W, ε, γ) to test sensitivity.
2.3 Oracle reference: best possible performance given full curves (post-hoc) to bound achievable regret.

3. Experimental Angles / Evidence Streams
3.1 Quantitative benchmarks
    • Curves of mean/median regret vs wall-time and vs epochs.
    • Area-Under-Curve (AUC) of regret over time (lower is better).
    • Number of early terminations & average stopping epoch.
    • Paired significance tests (Wilcoxon signed-rank, α = 0.05) per task.
3.2 Efficiency analysis
    • GPU runtime and VRAM peak via nvidia-smi logs.
    • CPU RAM usage via psutil (critical with 500 MB cap).
    • Scalability plots: cost ∝ dataset size & T_max.
3.3 Robustness analysis
    • Standard deviation across 10 seeds.
    • Leave-one-domain-out generalisation (train hyper-params on tabular, test on vision and vice-versa).
    • Stress test on noisy tasks by injecting synthetic noise.
3.4 Qualitative inspection
    • Example learning curves showing early-stop points vs final convergence.
    • 2-D GP posterior slices on [x; τ] to illustrate surrogate smoothness.

4. Experimental Protocol Template (applies to every dataset)
P1 Budget normalisation: all methods receive identical total GPU-seconds. Budget is chosen so the largest baseline (BOIL) fits on a single NVIDIA T4 with 16 GB.
P2 10 independent random seeds.
P3 Fixed common preprocessing & data loaders to reduce variance.
P4 Unified logging format (wandb/CSV) capturing per-epoch loss, time, memory.
P5 Post-processing script computes all metrics and statistical tests in one pass.

5. Success / Validation Criteria
C1 Performance: SlopeStop-BO attains equal or lower median final regret than best baseline on ≥70 % of tasks and is not significantly worse on any task.
C2 Efficiency: achieves ≥30 % reduction in total epochs and ≥20 % wall-time savings versus BOIL while staying within 16 GB VRAM and 500 MB system RAM.
C3 Robustness: coefficient of variation (CV) across seeds ≤ baseline CV for at least 80 % of tasks.
C4 Ablations: full model outperforms every ablation in both regret and AUC on aggregate benchmarks.
C5 Generalisation: hyper-parameters (W, ε, γ) tuned on half of the tasks transfer without retuning to the remaining tasks with <5 % relative regret degradation.

6. Practical Considerations for the T4 / 16 GB / 500 MB Environment
• Use mixed-precision (FP16) training where applicable.
• Stream dataset batches from disk to respect 500 MB RAM limit; keep only last W+1 validation losses in memory.
• Store compressed curve summaries (μ, τ) to minimise RAM.
• Run methods serially rather than in parallel to avoid memory contention; accumulate wall-time budgets accordingly.

7. Reporting
All results presented as
    (a) Task-level tables of final regret, epochs, wall-time
    (b) Aggregate violin/box plots
    (c) Pareto scatter plots (regret vs cost)
    (d) Ablation bar charts
    (e) Example qualitative figures.

This comprehensive, multi-angle strategy ensures that the proposed SlopeStop-BO is assessed not only for raw performance but also for efficiency, robustness, and practicality under stringent hardware constraints, thereby providing compelling evidence for its effectiveness as a drop-in replacement for existing HPO methods.

---


# Reference Information from Previous Iteration

Output:
{
    "experiments": [
        {
            "experiment_id": "exp-1",
            "run_variations": [
                "SlopeStop-BO",
                "A0-No-EarlyStop (fixed-budget GP-EI/c)",
                "A1-SlopeStop w/o Cost-Aware EI",
                "BOIL",
                "Hyperband"
            ],
            "description": "Objective / Hypothesis:\nValidate that SlopeStop-BO achieves equal or lower simple-regret than strong Bayesian and bandit baselines while consuming significantly fewer training epochs and wall-clock seconds on both tabular and vision HPO benchmarks.\n\nModels under HPO:\n• FeedForwardNet-LC (3-layer ReLU MLP, ≈20 k params) on LCBench tasks.\n• NAS-Bench-201 search-cell CNN (≈2 M params) on CIFAR-10, CIFAR-100, ImageNet-16-120 tasks.\n(Both architectures are part of the original benchmarks and provided as reference implementations.)\n\nDatasets:\n• LCBench (35 OpenML tabular datasets, max 50 epochs).\n• NAS-Bench-201 (3 image datasets, max 200 epochs).\nAll datasets are used as-is; features are z-normalised (tabular) or per-channel mean-std normalised (vision).\n\nPre-processing:\n• Tabular: standardisation per feature, categorical one-hot (already stored in LCBench).  Batches streamed from disk.\n• Vision: random crop + horizontal flip during training, centre crop at val/test.\n\nData Splitting:\n• Each dataset keeps its official train/val/test split from the benchmark.  The HPO objective is validation metric; test split is revealed only for the final model selected by best validation configuration.\n\nBudget & Repetitions:\n• 10 independent seeds per dataset × method.\n• Global GPU-second budget per seed equals BOIL runtime on the slowest task (pre-measured ≈8 k s for NAS-Bench-201 ImageNet-16-120 on T4).\n• Early stopping inside SlopeStop-BO counts against the same budget (unused time can be spent on more BO iterations).\n\nEvaluation Metrics:\nPrimary – Final simple-regret (val error of best configuration – best known).\nSecondary – AUC of regret vs wall-time, #epochs consumed, wall-clock time, peak VRAM, CPU RAM, FLOPs*, energy (via NVIDIA SMI power logs).\n*FLOPs computed with fvcore.get_flops() once per architecture; total = FLOPs×epochs.\n\nStatistical Tests:\nWilcoxon signed-rank on task-level paired regrets (α=0.05, Holm correction).\n\nHyper-parameter Sensitivity (inside variation “SlopeStop-BO” only):\nGrid W∈{3,5,7}, ε∈{1e-3,3e-3}, γ∈{0.005,0.01,0.02}; pick default by median validation regret on 50 % of tasks, freeze for rest.\n\nRobustness Checks:\n1. Noise injection – add N(0,0.02) noise to validation loss online for 20 % of runs; measure early-stop frequency & regret delta.\n2. Domain transfer – tune (W,ε,γ) on tabular, evaluate on vision and vice-versa.\n3. Seed variance – coefficient of variation across 10 seeds.\n\nComputational-efficiency Analysis:\n• Log epoch-level timestamps; compute avg seconds/epoch and cumulative wall-time per BO iteration.\n• Memory – sample nvidia-smi every 2 s; store max.\n\nComparisons & Footnotes:\n(1) A0 shares SlopeStop GP surrogate & EI/c but disables slope rule (fixed 50/200 epochs).  (2) A1 keeps slope rule yet replaces EI/c with plain EI to isolate acquisition benefit.  (3) BOIL implementation from original authors, re-tuned GP hyper-prior to avoid out-of-memory on T4†.  (4) Hyperband implementation from hpbandster, identical training code.\n†Changed inducing-point count from 128→64.\n\nCode Skeleton (PyTorch, BoTorch):\n```\nfor seed in seeds:\n    torch.manual_seed(seed)\n    bo = init_optimizer(method)  # picks GP, acquisition, early-stopper\n    while budget_left():\n        x = bo.suggest()\n        mu, tau, cost = train_cfg(x, method)  # contains slope early-stop if enabled\n        bo.observe(x, mu, cost)\n```\ntrain_cfg implements mixed-precision, keeps only last W+1 val losses, and streams batches to stay <500 MB system RAM.\n\nSuccess Criteria:\nC1–C5 from strategy; pass/fail recorded per method.\n\nExpected Findings:\n• SlopeStop-BO reaches ≤ baseline median regret on ≥70 % tasks, with ≈40 % fewer epochs and ≥20 % wall-time reduction vs BOIL.\n• A0 shows similar regret but 40-50 % higher cost ⇒ confirms value of slope rule.\n• A1 loses ≈8 % regret, same cost ⇒ confirms EI/c benefit.\n• Hyperband fastest but worst regret; BOIL best regret but highest cost → SlopeStop Pareto-optimal.\n"
        }
    ],
    "expected_models": [
        "FeedForwardNet-LC",
        "NASBench201-CNN"
    ],
    "expected_datasets": [
        "LCBench",
        "NAS-Bench-201"
    ]
}
