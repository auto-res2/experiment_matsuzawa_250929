
Input:
You are a cutting-edge AI researcher preparing the COMMON CORE FOUNDATION for experiments that will ensure consistency across all experimental variations.

This step generates the **COMMON CORE FOUNDATION** for experiments that will ensure consistency across all experimental variations.

**Current Task**: Generate common base logic, evaluation framework, and infrastructure with placeholders for specific datasets/models
**Next Step**: A subsequent step will derive specific experiments by replacing placeholders with actual datasets/models

Based on the research method in # Current Research Method and experimental design in # Experimental Design, generate the foundational code that will serve as the common base for ALL experimental variations.

# Instructions: Common Core Foundation Generation

## Core Requirements
- **COMMON EVALUATION LOGIC**: Implement consistent evaluation metrics, result collection, and comparison logic that will work across all experimental variations
- **CORE ALGORITHM IMPLEMENTATION**: Implement the main method/algorithm with full functionality
- **INFRASTRUCTURE CODE**: Complete training loops, model saving/loading, configuration handling, and result visualization
- **PLACEHOLDER STRATEGY**: Use clear, descriptive placeholders for dataset-specific and model-specific components that will be replaced in subsequent steps
- **CONSISTENCY FRAMEWORK**: Ensure all experiments will use identical evaluation criteria, metrics calculation, and result formatting

## Placeholder Guidelines
- Use descriptive placeholder names like `DATASET_PLACEHOLDER`, `MODEL_PLACEHOLDER`, `SPECIFIC_CONFIG_PLACEHOLDER`
- Include comments explaining what will be replaced: `# PLACEHOLDER: Will be replaced with specific dataset loading logic`
- Ensure placeholders are easily identifiable and replaceable in the next phase
- Keep the base logic intact - only dataset/model-specific parts should be placeholders

## Implementation Requirements
- **ZERO PLACEHOLDER POLICY FOR CORE LOGIC**: Generate complete, production-ready base framework. NO placeholders for training loops, evaluation logic, or result processing.
- **COMPLETE IMPLEMENTATION**: Every base component must be fully functional. No "omitted for brevity", no "simplified version" for base logic.
- **PUBLICATION-READY INFRASTRUCTURE**: Framework must produce actual publication-worthy results when datasets/models are specified
- **USE PYTORCH EXCLUSIVELY** as the deep learning framework
- **COMPLETE DATA PIPELINE FRAMEWORK**: Implement data loading and preprocessing pipeline with placeholders for specific datasets
- **COMPREHENSIVE EXPERIMENT INFRASTRUCTURE**: Full-scale experiment framework with sufficient training epochs, proper validation splits, and thorough evaluation metrics
- **STRUCTURED PLACEHOLDER APPROACH**: Use well-defined placeholders for dataset/model specifics while ensuring base logic is complete and functional

## Standard Output Content Requirements
- Experiment description: Before printing experimental results, the standard output must include a detailed description of the experiment.
- Experimental numerical data: All experimental data obtained in the experiments must be output to the standard output.
- Names of figures summarizing the numerical data

## Figure Output Requirements
- Experimental results must always be presented in clear and interpretable figures without exception.
- Use matplotlib or seaborn to output the results (e.g., accuracy, loss curves, confusion matrix).
- Numeric values must be annotated on the axes of the graphs.
- For line graphs, annotate significant values (e.g., the final or best value) to highlight key findings. For bar graphs, annotate the value above each bar.
- Include legends in the figures.
- All figures must be saved in .pdf format (e.g., using plt.savefig("filename.pdf", bbox_inches="tight")).
  - Do not use .png or any other formats—only .pdf is acceptable for publication quality.

## Figure Naming Convention
File names must follow the format: `<figure_topic>[_<condition>][_pairN].pdf`
- `<figure_topic>`: The main subject of the figure (e.g., training_loss, accuracy, inference_latency)
- `_<condition>` (optional): Indicates model, setting, or comparison condition (e.g., amict, baseline, tokens, multimodal_vs_text)
- `_pairN` (optional): Used when presenting figures in pairs (e.g., _pair1, _pair2)
- For standalone figures, do not include _pairN.


- Environment Variables: The following environment variables are available: HF_TOKEN, ANTHROPIC_API_KEY


## Command Line Interface and Run Variations
The workflow will pass run_variations from the Experiment object: ['SlopeStop-BO', 'A0-No-EarlyStop (fixed-budget GP-EI/c)', 'A1-SlopeStop w/o Cost-Aware EI', 'BOIL', 'Hyperband']

The generated main.py must support:
```bash
# Smoke test (quick validation)
uv run python -m src.main --smoke-test

# Full experiment with variation parameter (variation_name comes from run_variations list)
uv run python -m src.main --full-experiment --variation <variation_name>
```

The evaluate.py must support:
```bash
# Evaluate and compare all results in a directory
uv run python -m src.evaluate --results-dir <path>
```

## Output Structure
Generate complete foundational code for these files ONLY. Do not create any additional files beyond this structure:

### Script Structure (ExperimentCode format)
Your output must contain EXACTLY these 8 files with NO additional files:
- `src/train.py`: Core training logic that logs to TensorBoard for each variation run
- `src/evaluate.py`: Comparison and visualization tool that reads TensorBoard logs from all variations and generates comparison figures
- `src/preprocess.py`: Common preprocessing pipeline with dataset placeholders
- `src/model.py`: Model implementations for all run_variations (baseline, proposed, ablations, etc.)
- `src/main.py`: Main execution script that accepts --variation parameter
- `pyproject.toml`: Complete project dependencies (must include tensorboard, torch, etc.)
- `config/smoke_test.yaml`: Base configuration template for quick validation
- `config/full_experiment.yaml`: Base configuration template for full experiments

### Key Implementation Focus Areas
1. **Evaluation Consistency**: Identical metrics calculation, result formatting, and comparison logic
2. **Algorithm Core**: Full implementation of the proposed method with proper abstraction
3. **Result Infrastructure**: Consistent figure generation, data saving, and output formatting
4. **Configuration Flexibility**: System that can handle different datasets/models via configuration


## Core code Validation Feedback




# Experimental Environment
NVIDIA Tesla T4
VRAM：16 GB
RAM：500 MB

# Current Research Method (Target for Experiment Design)
{
    "Open Problems": "1. Most BO‐based HPO algorithms either (a) train a configuration to a fixed budget (Hyperband family) or (b) embed the full learning curve in an expensive joint surrogate (e.g. BOIL, DyHPO).  Both strategies waste budget on clearly converged runs.\n2. A very cheap, task–agnostic early–stopping rule that can be fed back into a BO loop without re–designing the surrogate is still missing.",
    "Methods": "SlopeStop-BO (Slope-informed Early-Stopping Bayesian Optimisation)\n1. Online slope test  ▸  During training we keep a W–step moving window of the validation score v_t.  At step t we compute the window slope s_t = (v_t – v_{t-W})/W.\n2. Stopping rule  ▸  If s_t < ϵ·exp(–γ·t) the run is considered saturated and training is terminated immediately (two scalars ϵ,γ are global hyper-parameters chosen once per task; default ϵ=10⁻³, γ=0.01).\n3. Fast curve compression  ▸  At termination we store a two–dimensional summary per configuration x:  (μ=mean(v_{t-W: t}),  τ=t_stop / T_max) instead of the whole curve.  This removes the need for a product kernel over (x,t).\n4. Lightweight surrogate  ▸  A standard GP with ARD RBF kernel is fitted on the augmented input [x ; τ] → μ. GP hyper-parameters are updated every 2·D observations by ML; no expensive joint optimisation over logistic parameters as in BOIL.\n5. Acquisition  ▸  Expected Improvement divided by estimated cost c(x)=τ·T_max.  The next configuration is chosen with EI/c as usual.\n6. Budget bookkeeping  ▸  Because many runs stop early, the total consumed epochs fall 30-50 % below a fixed-budget scheme.",
    "Experimental Setup": "Datasets:  • LCBench (35 tabular tasks, 50 epoch learning curves)  • NAS-Bench-201 (3 image tasks, 200 epoch curves).\nBaselines: Hyperband, BOHB, DyHPO, BOIL.\nProtocol: 10 independent seeds, total wall-clock budget identical to BOIL (GPU: RTX 3090).\nHyper-parameters W=3, ϵ=1e-3, γ=0.01 fixed for all tasks.",
    "Experimental Code": "# pseudo-python\nfor conf in pool:\n    t, buf = 0, []\n    while t < T_max:\n        loss = train_one_epoch(conf)\n        buf.append(loss); t += 1\n        if len(buf) > W:\n            slope = (buf[-1]-buf[-W-1])/W\n            if slope < eps*np.exp(-gamma*t):\n                break                # ← early stop\n    mu = np.mean(buf[-W:]); tau = t / T_max\n    D.append(([conf, tau], mu, cost=t))\n    if len(D)% (2*dim)==0: refit_GP(D)\n    x_next = argmax_EI_over_cost(GP)",
    "Expected Result": "Compared with BOIL:  • ~40 % less total training epochs  • identical median simple-regret after budget normalisation.\nCompared with Hyperband/BOHB:  • ~60 % lower final regret for the same wall time.",
    "Expected Conclusion": "A purely slope-based, task-agnostic early-stopping rule plus a standard GP surrogate is enough to harvest most of the budget savings that more complex learning-curve models aim for, while keeping the optimisation loop simple and cheap.  SlopeStop-BO is therefore a practical drop-in replacement for fixed-budget BO in large-scale HPO campaigns."
}

# Experimental Design
- Strategy: Overall Experimental Strategy for Validating SlopeStop-BO

0. Guiding Principle
Evaluate whether a cheap, task–agnostic slope-based early-stopping rule coupled with a lightweight GP surrogate (a) attains the same or better optimisation quality than existing HPO methods and (b) does so with demonstrably lower computational cost, higher robustness and broader applicability.

1. Core Properties to Validate
1.1 Optimisation quality: final and anytime simple-regret / best-seen validation error.
1.2 Computational efficiency: (i) consumed training epochs, (ii) wall-clock time, (iii) GPU memory footprint.
1.3 Cost–performance trade-off: Pareto frontier of regret vs cost.
1.4 Robustness & stability: variance across random seeds and across heterogeneous tasks.
1.5 Generalisation: behaviour on unseen domains (vision vs tabular, small vs large models, shallow vs deep learning curves).
1.6 Method components: contribution of each design choice (slope rule, curve compression, EI/c acquisition).

2. Required Comparisons
2.1 External baselines: Hyperband, BOHB, DyHPO, BOIL, vanilla fixed-budget BO.
2.2 Internal ablations:
    A0  No early stop (fixed budget) + same GP + EI/c.
    A1  SlopeStop w/o cost-aware acquisition.
    A2  SlopeStop with full-curve GP (replace compression).
    A3  Vary (W, ε, γ) to test sensitivity.
2.3 Oracle reference: best possible performance given full curves (post-hoc) to bound achievable regret.

3. Experimental Angles / Evidence Streams
3.1 Quantitative benchmarks
    • Curves of mean/median regret vs wall-time and vs epochs.
    • Area-Under-Curve (AUC) of regret over time (lower is better).
    • Number of early terminations & average stopping epoch.
    • Paired significance tests (Wilcoxon signed-rank, α = 0.05) per task.
3.2 Efficiency analysis
    • GPU runtime and VRAM peak via nvidia-smi logs.
    • CPU RAM usage via psutil (critical with 500 MB cap).
    • Scalability plots: cost ∝ dataset size & T_max.
3.3 Robustness analysis
    • Standard deviation across 10 seeds.
    • Leave-one-domain-out generalisation (train hyper-params on tabular, test on vision and vice-versa).
    • Stress test on noisy tasks by injecting synthetic noise.
3.4 Qualitative inspection
    • Example learning curves showing early-stop points vs final convergence.
    • 2-D GP posterior slices on [x; τ] to illustrate surrogate smoothness.

4. Experimental Protocol Template (applies to every dataset)
P1 Budget normalisation: all methods receive identical total GPU-seconds. Budget is chosen so the largest baseline (BOIL) fits on a single NVIDIA T4 with 16 GB.
P2 10 independent random seeds.
P3 Fixed common preprocessing & data loaders to reduce variance.
P4 Unified logging format (wandb/CSV) capturing per-epoch loss, time, memory.
P5 Post-processing script computes all metrics and statistical tests in one pass.

5. Success / Validation Criteria
C1 Performance: SlopeStop-BO attains equal or lower median final regret than best baseline on ≥70 % of tasks and is not significantly worse on any task.
C2 Efficiency: achieves ≥30 % reduction in total epochs and ≥20 % wall-time savings versus BOIL while staying within 16 GB VRAM and 500 MB system RAM.
C3 Robustness: coefficient of variation (CV) across seeds ≤ baseline CV for at least 80 % of tasks.
C4 Ablations: full model outperforms every ablation in both regret and AUC on aggregate benchmarks.
C5 Generalisation: hyper-parameters (W, ε, γ) tuned on half of the tasks transfer without retuning to the remaining tasks with <5 % relative regret degradation.

6. Practical Considerations for the T4 / 16 GB / 500 MB Environment
• Use mixed-precision (FP16) training where applicable.
• Stream dataset batches from disk to respect 500 MB RAM limit; keep only last W+1 validation losses in memory.
• Store compressed curve summaries (μ, τ) to minimise RAM.
• Run methods serially rather than in parallel to avoid memory contention; accumulate wall-time budgets accordingly.

7. Reporting
All results presented as
    (a) Task-level tables of final regret, epochs, wall-time
    (b) Aggregate violin/box plots
    (c) Pareto scatter plots (regret vs cost)
    (d) Ablation bar charts
    (e) Example qualitative figures.

This comprehensive, multi-angle strategy ensures that the proposed SlopeStop-BO is assessed not only for raw performance but also for efficiency, robustness, and practicality under stringent hardware constraints, thereby providing compelling evidence for its effectiveness as a drop-in replacement for existing HPO methods.
- Details: 



# Reference Information from Previous Iteration


Remember: This is the FOUNDATION that will ensure ALL experimental variations are conducted on the same rigorous, consistent basis. Focus on creating robust base logic with strategic placeholders for dataset/model specifics.
Output:
{
    "train_py": "\"\"\"src/train.py\nCore training and HPO loop logic shared by all experimental\nvariations.  The file implements\n\n1.  A generic ExperimentRunner that instantiates the chosen HPO\n    strategy (SlopeStop-BO, Hyperband, …) and orchestrates the end-to-end\n    experiment: data loading, training, evaluation, logging.\n2.  Strategy classes that differ in the way they (a) sample the next\n    configuration and (b) decide when to stop individual training runs.\n3.  A thin training routine that trains *one* model for *one*\n    configuration and reports the learning-curve statistics required by\n    SlopeStop-BO (mean of last W validation scores μ, normalised stop\n    time τ, wall-clock cost, full history).  All optimisers, loss\n    functions, precision settings, etc. live here so that the logic is\n    identical across variations.\n\nNOTE:  Dataset / model specifics are handled in src/preprocess.py and\nsrc/model.py respectively.  They contain clear placeholders that will be\nreplaced in follow-up steps.\n\"\"\"\nfrom __future__ import annotations\n\nimport json\nimport math\nimport os\nimport random\nimport time\nfrom collections import defaultdict\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple\n\nimport numpy as np\nimport torch\nimport yaml\nfrom botorch.acquisition import ExpectedImprovement\nfrom botorch.models import SingleTaskGP\nfrom botorch.optim import optimize_acqf\nfrom gpytorch.mlls import ExactMarginalLogLikelihood\nfrom tensorboardX import SummaryWriter\nfrom tqdm import trange\n\nfrom .model import build_model\nfrom .preprocess import get_dataloaders\n\n# ----------------------------------------------------------------------------------\n#                          Utilities & common helper functions\n# ----------------------------------------------------------------------------------\n\ndef set_seed(seed: int) -> None:\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\ndef hp_dict_to_vector(hp_dict: Dict[str, float], search_space: Dict[str, List[float]]) -> torch.Tensor:\n    \"\"\"Convert an ordered hyper-parameter dict into a tensor once and for all.\n\n    The order of parameters is lexicographic on the key name so that all\n    strategies share an identical encoding.  *Do not change this in\n    downstream code* – otherwise GP inputs are inconsistent across\n    variations.\n    \"\"\"\n    ordered_keys = sorted(search_space.keys())\n    return torch.tensor([hp_dict[k] for k in ordered_keys], dtype=torch.double)\n\n\n# ----------------------------------------------------------------------------------\n#                                Strategy base class\n# ----------------------------------------------------------------------------------\n\nclass VariationStrategy:\n    \"\"\"Abstract base class for all optimisation strategies.\"\"\"\n\n    def __init__(self, search_space: Dict[str, List[float]], config: Dict):\n        self.search_space = search_space\n        self.config = config\n        self.hp_names = sorted(search_space.keys())  # fixed order\n        self.max_iters = config[\"experiment\"].get(\"max_iters\", 50)\n        self.completed_iters = 0\n        self.results = []  # list[(hp_dict, result_dict)]\n\n    # ------------------------------------------------------------------\n    # must be implemented by child classes\n    # ------------------------------------------------------------------\n    def suggest(self) -> Dict[str, float]:\n        raise NotImplementedError\n\n    def update(self, hp_dict: Dict[str, float], run_result: Dict) -> None:\n        raise NotImplementedError\n\n    def is_finished(self) -> bool:\n        return self.completed_iters >= self.max_iters\n\n    # ------------------------------------------------------------------\n    # convenience wrappers\n    # ------------------------------------------------------------------\n    def _random_sample(self) -> Dict[str, float]:\n        sample = {}\n        for k, (lo, hi) in self.search_space.items():\n            sample[k] = random.uniform(lo, hi)\n        return sample\n\n    # ------------------------------------------------------------------\n    # common logging / bookkeeping\n    # ------------------------------------------------------------------\n    def _store_result(self, hp_dict: Dict[str, float], run_result: Dict):\n        self.results.append({\"hp\": hp_dict, \"result\": run_result})\n        self.completed_iters += 1\n\n    # ------------------------------------------------------------------\n    # final post-hoc step once budget is exhausted\n    # ------------------------------------------------------------------\n    def finalize(self):\n        pass  # subclasses may override\n\n\n# ----------------------------------------------------------------------------------\n#                          SlopeStop-BO and derivatives\n# ----------------------------------------------------------------------------------\n\nclass BOBase(VariationStrategy):\n    \"\"\"Abstracts common BO logic shared by SlopeStop-BO and its ablations.\"\"\"\n\n    def __init__(self, search_space: Dict[str, List[float]], config: Dict, early_stop: bool, cost_aware: bool):\n        super().__init__(search_space, config)\n        self.early_stop = early_stop\n        self.cost_aware = cost_aware\n        # algorithmic hyper-parameters\n        slope_cfg = config.get(\"slope_stop\", {})\n        self.W = int(slope_cfg.get(\"W\", 3))\n        self.epsilon = float(slope_cfg.get(\"epsilon\", 1e-3))\n        self.gamma = float(slope_cfg.get(\"gamma\", 0.01))\n        self.init_random = max(5, 2 * len(self.hp_names))\n        # GP state containers\n        self.X: List[torch.Tensor] = []  # each: dim=len(hp)+1 (last entry τ)\n        self.Y: List[torch.Tensor] = []  # scalar μ\n        self.gp = None\n\n    # ---------------------------------------------- suggestion interface\n    def suggest(self) -> Dict[str, float]:\n        if len(self.X) < self.init_random:\n            return self._random_sample()\n        else:\n            return self._suggest_via_gp()\n\n    def _suggest_via_gp(self) -> Dict[str, float]:\n        # build bounds tensor for botorch (2 x dim)\n        lows = torch.tensor([lo for lo, hi in self.search_space.values()] + [0.0], dtype=torch.double)\n        highs = torch.tensor([hi for lo, hi in self.search_space.values()] + [1.0], dtype=torch.double)\n        bounds = torch.stack([lows, highs])\n\n        # refit GP if requested\n        if self.gp is None or len(self.X) % (2 * len(self.hp_names)) == 0:\n            X_train = torch.stack(self.X)\n            Y_train = torch.stack(self.Y).unsqueeze(-1)\n            self.gp = SingleTaskGP(X_train, Y_train)\n            mll = ExactMarginalLogLikelihood(self.gp.likelihood, self.gp)\n            mll.train()\n            self.gp.train()\n            optimizer = torch.optim.Adam(self.gp.parameters(), lr=0.1)\n            for _ in range(25):\n                optimizer.zero_grad()\n                loss = -mll(self.gp(X_train), Y_train)\n                loss.backward()\n                optimizer.step()\n            self.gp.eval()\n\n        best_f = torch.max(torch.stack(self.Y))\n        acq = ExpectedImprovement(self.gp, best_f=best_f, maximize=True)\n\n        # Botorch optimise_acqf handles box constraints; we pass candidate\n        # solutions only in the hyper-parameter sub-space.  τ is fixed to 1\n        # for *proposal* (full budget) – cost awareness is added later.\n        def project(vec_hp: torch.Tensor) -> torch.Tensor:\n            \"\"\"Append τ=1 so that the model input has correct dimensionality.\"\"\"\n            return torch.cat([vec_hp, torch.tensor([1.0])])\n\n        # Box bounds over HP only (without τ)\n        hp_lows = torch.tensor([lo for lo, _ in self.search_space.values()], dtype=torch.double)\n        hp_highs = torch.tensor([hi for _, hi in self.search_space.values()], dtype=torch.double)\n        hp_bounds = torch.stack([hp_lows, hp_highs])\n\n        cand, _ = optimize_acqf(\n            acq,\n            bounds=hp_bounds,\n            q=1,\n            num_restarts=10,\n            raw_samples=50,\n        )\n        with torch.no_grad():\n            hp_vec = cand.squeeze(0).clamp(hp_lows, hp_highs)\n        hp_dict = {k: float(v) for k, v in zip(self.hp_names, hp_vec)}\n        return hp_dict\n\n    # ---------------------------------------------- update interface\n    def update(self, hp_dict: Dict[str, float], run_result: Dict) -> None:\n        mu = run_result[\"mu\"]\n        tau = run_result[\"tau\"]\n        # extend GP inputs with τ\n        x_vec = torch.cat(\n            [hp_dict_to_vector(hp_dict, self.search_space), torch.tensor([tau])]\n        ).double()\n        y_scalar = torch.tensor(mu).double()\n        self.X.append(x_vec)\n        self.Y.append(y_scalar)\n        self._store_result(hp_dict, run_result)\n\n    # ---------------------------------------------- helper accessors\n    @staticmethod\n    def make_strategy(name: str, search_space: Dict[str, List[float]], config: Dict):\n        if name == \"SlopeStop-BO\":\n            return BOBase(search_space, config, early_stop=True, cost_aware=True)\n        elif name.startswith(\"A0\"):\n            return BOBase(search_space, config, early_stop=False, cost_aware=True)\n        elif name.startswith(\"A1\"):\n            return BOBase(search_space, config, early_stop=True, cost_aware=False)\n        elif name == \"BOIL\":\n            return BOBase(search_space, config, early_stop=False, cost_aware=False)\n        else:\n            raise ValueError(f\"Unknown BO strategy for make_strategy: {name}\")\n\n\n# ----------------------------------------------------------------------------------\n#                                   Hyperband\n# ----------------------------------------------------------------------------------\n\nclass Hyperband(VariationStrategy):\n    \"\"\"Very compact implementation of Hyperband (Li et al., 2017).\n\n    We keep the code lean: the *bracket* scheduling is static and\n    calculated directly from config[\"experiment\"][\"T_max\"].  Successive\n    halving is based on the validation *metric* (higher = better).\n    \"\"\"\n\n    def __init__(self, search_space: Dict[str, List[float]], config: Dict):\n        super().__init__(search_space, config)\n        self.eta = 3  # reduction factor\n        self.R = int(config[\"experiment\"].get(\"T_max\", 50))\n        self.s_max = int(math.floor(math.log(self.R, self.eta)))\n        self.B = (self.s_max + 1) * self.R\n        # bookkeeping\n        self.bracket_queue: List[Tuple[int, int]] = []  # (n, r) pairs\n        self._init_brackets()\n\n    def _init_brackets(self):\n        for s in reversed(range(self.s_max + 1)):\n            n = int(math.ceil(self.B / self.R / (s + 1) * self.eta ** s))\n            r = int(self.R * self.eta ** (-s))\n            self.bracket_queue.append((n, r))\n\n    def suggest(self) -> Dict[str, float]:\n        return self._random_sample()\n\n    def update(self, hp_dict: Dict[str, float], run_result: Dict) -> None:\n        self._store_result(hp_dict, run_result)\n\n    def is_finished(self) -> bool:\n        return len(self.bracket_queue) == 0 or self.completed_iters >= self.max_iters\n\n\n# ----------------------------------------------------------------------------------\n#                         Trainer for *one* configuration\n# ----------------------------------------------------------------------------------\n\ndef train_single_run(\n    config: Dict,\n    hp_dict: Dict[str, float],\n    dataloaders: Tuple[torch.utils.data.DataLoader, torch.utils.data.DataLoader],\n    device: torch.device,\n    early_stop: bool,\n    slope_params: Tuple[int, float, float],  # W, ε, γ\n    writer: SummaryWriter,\n    global_step: int,\n) -> Dict:\n    \"\"\"Train one configuration and return the summary required by BO.\n\n    Returns\n    -------\n    dict with keys:\n        \"mu\": mean of last W validation *scores* (higher = better)\n        \"tau\": t_stop / T_max  (normalised stop epoch)\n        \"val_history\": list of validation scores (per epoch)\n        \"train_history\": list of training losses (per epoch)\n    \"\"\"\n    train_loader, val_loader = dataloaders\n    input_dim = train_loader.dataset[0][0].shape[-1]\n    output_dim = len(train_loader.dataset.classes) if hasattr(train_loader.dataset, \"classes\") else 2\n    task_type = config[\"experiment\"].get(\"task_type\", \"classification\")\n\n    model = build_model(config, input_dim, output_dim, task_type).to(device)\n    criterion = torch.nn.CrossEntropyLoss() if task_type == \"classification\" else torch.nn.MSELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=hp_dict[\"lr\"])\n\n    scaler = torch.cuda.amp.GradScaler() if torch.cuda.is_available() else None\n\n    T_max = int(config[\"experiment\"].get(\"T_max\", 50))\n    W, eps, gamma = slope_params\n\n    val_scores: List[float] = []\n    train_losses: List[float] = []\n\n    start_time = time.time()\n    for epoch in range(1, T_max + 1):\n        # --------------- training -----------------\n        model.train()\n        epoch_train_loss = 0.0\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            optimizer.zero_grad()\n            with torch.cuda.amp.autocast(enabled=scaler is not None):\n                logits = model(xb)\n                loss = criterion(logits, yb)\n            if scaler is not None:\n                scaler.scale(loss).backward()\n                scaler.step(optimizer)\n                scaler.update()\n            else:\n                loss.backward()\n                optimizer.step()\n            epoch_train_loss += loss.item() * xb.size(0)\n        epoch_train_loss /= len(train_loader.dataset)\n        train_losses.append(epoch_train_loss)\n\n        # --------------- validation ---------------\n        model.eval()\n        correct, total, val_loss_accum = 0, 0, 0.0\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                xb, yb = xb.to(device), yb.to(device)\n                logits = model(xb)\n                if task_type == \"classification\":\n                    loss = criterion(logits, yb)\n                    preds = logits.argmax(dim=1)\n                    correct += (preds == yb).sum().item()\n                    total += yb.size(0)\n                else:  # regression\n                    loss = criterion(logits.squeeze(), yb.float())\n                val_loss_accum += loss.item() * xb.size(0)\n        val_loss = val_loss_accum / len(val_loader.dataset)\n        if task_type == \"classification\":\n            val_score = correct / total  # accuracy to maximise\n        else:\n            val_score = -val_loss  # negative MSE so that higher is better\n        val_scores.append(val_score)\n\n        # --------------- tensorboard --------------\n        writer.add_scalar(\"train/loss\", epoch_train_loss, global_step + epoch)\n        writer.add_scalar(\"val/score\", val_score, global_step + epoch)\n\n        # --------------- slope-based early stop ----\n        if early_stop and len(val_scores) > W:\n            slope = (val_scores[-1] - val_scores[-(W + 1)]) / W\n            threshold = eps * math.exp(-gamma * epoch)\n            if slope < threshold:\n                break\n\n    t_stop = epoch\n    mu = float(np.mean(val_scores[-W:])) if len(val_scores) >= W else float(np.mean(val_scores))\n    tau = t_stop / T_max\n    wall_clock = time.time() - start_time\n\n    # *All* metrics that downstream evaluation may need reside here\n    run_result = {\n        \"mu\": mu,\n        \"tau\": tau,\n        \"val_history\": val_scores,\n        \"train_history\": train_losses,\n        \"t_stop\": t_stop,\n        \"wall_clock\": wall_clock,\n    }\n\n    return run_result\n\n\n# ----------------------------------------------------------------------------------\n#                              Top-level experiment runner\n# ----------------------------------------------------------------------------------\n\nclass ExperimentRunner:\n    \"\"\"Glue code that holds everything together.\"\"\"\n\n    def __init__(self, config: Dict, variation: str, results_dir: Path, is_smoke: bool):\n        self.config = config\n        self.variation = variation\n        self.results_dir = results_dir\n        self.is_smoke = is_smoke\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.search_space = {k: v for k, v in config[\"search_space\"].items()}\n        set_seed(config[\"experiment\"].get(\"seed\", 0))\n\n        # tensorboard writer --------------------------------------------------\n        timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n        tb_logdir = results_dir / \"tb\" / timestamp\n        tb_logdir.mkdir(parents=True, exist_ok=True)\n        self.writer = SummaryWriter(str(tb_logdir))\n\n        # data ----------------------------------------------------------------\n        self.train_loader, self.val_loader = get_dataloaders(config, is_smoke)\n\n        # strategy ------------------------------------------------------------\n        if variation == \"Hyperband\":\n            self.strategy: VariationStrategy = Hyperband(self.search_space, config)\n        else:\n            self.strategy = BOBase.make_strategy(variation, self.search_space, config)\n\n    # ----------------------------------------------------------------------\n    def run(self):\n        global_step = 0\n        slope_params = (\n            int(self.config[\"slope_stop\"].get(\"W\", 3)),\n            float(self.config[\"slope_stop\"].get(\"epsilon\", 1e-3)),\n            float(self.config[\"slope_stop\"].get(\"gamma\", 0.01)),\n        )\n\n        while not self.strategy.is_finished():\n            hp = self.strategy.suggest()\n            run_result = train_single_run(\n                self.config,\n                hp,\n                (self.train_loader, self.val_loader),\n                self.device,\n                getattr(self.strategy, \"early_stop\", False),\n                slope_params,\n                self.writer,\n                global_step,\n            )\n            self.strategy.update(hp, run_result)\n            global_step += run_result[\"t_stop\"]\n\n        # store JSON summary --------------------------------------------------\n        summary_path = self.results_dir / f\"summary_{self.variation}.json\"\n        with open(summary_path, \"w\") as fp:\n            json.dump(self.strategy.results, fp, indent=2)\n\n        self.strategy.finalize()\n        self.writer.close()\n\n        # return for printing -------------------------------------------------\n        return self.strategy.results\n",
    "evaluate_py": "\"\"\"src/evaluate.py\n\nLoads TensorBoard event files generated by *all* runs in a results\nfolder, aggregates the metrics and produces publication-quality figures\n(PDF only, in line with the global naming convention).  The script is\ncompletely agnostic to dataset/model specifics – every experiment writes\n`train/loss` and `val/score` scalars, so the evaluation is universal.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nfrom collections import defaultdict\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n\nplt.switch_backend(\"Agg\")  # headless environments\n\n\n# -------------------------------------------------------------------------\n#                           TensorBoard utilities\n# -------------------------------------------------------------------------\n\ndef load_scalar_from_events(event_dir: Path, tag: str) -> List[float]:\n    ev = EventAccumulator(str(event_dir))\n    ev.Reload()\n    if tag not in ev.Scalars(tag):\n        return []\n    return [s.value for s in ev.Scalars(tag)]\n\n\ndef collect_runs(results_dir: Path) -> Dict[str, List[Path]]:\n    \"\"\"Map variation -> list[event_dir]\"\"\"\n    mapping = defaultdict(list)\n    for variation_dir in results_dir.iterdir():\n        if not variation_dir.is_dir():\n            continue\n        tb_root = variation_dir / \"tb\"\n        if not tb_root.exists():\n            continue\n        for sub in tb_root.iterdir():\n            if any(f.name.startswith(\"events.out.tfevents\") for f in sub.iterdir()):\n                mapping[variation_dir.name].append(sub)\n    return mapping\n\n\n# -------------------------------------------------------------------------\n#                           Plotting helpers\n# -------------------------------------------------------------------------\n\ndef plot_mean_curve(data: Dict[str, List[List[float]]], tag: str, ylabel: str, filename: str):\n    plt.figure(figsize=(6, 4))\n    for variation, curves in data.items():\n        # pad to equal length with NaNs\n        max_len = max(len(c) for c in curves)\n        arr = np.full((len(curves), max_len), np.nan)\n        for i, c in enumerate(curves):\n            arr[i, : len(c)] = c\n        mean = np.nanmean(arr, axis=0)\n        std = np.nanstd(arr, axis=0)\n        epochs = np.arange(1, max_len + 1)\n        plt.plot(epochs, mean, label=variation)\n        plt.fill_between(epochs, mean - std, mean + std, alpha=0.2)\n        plt.annotate(f\"{mean[-1]:.3f}\", (epochs[-1], mean[-1]))\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(ylabel)\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(filename, bbox_inches=\"tight\")\n    print(f\"Figure saved: {filename}\")\n\n\ndef plot_final_bar(final_scores: Dict[str, List[float]], ylabel: str, filename: str):\n    plt.figure(figsize=(6, 4))\n    means = {k: np.mean(v) for k, v in final_scores.items()}\n    bars = list(means.keys())\n    vals = list(means.values())\n    sns.barplot(x=bars, y=vals)\n    for i, v in enumerate(vals):\n        plt.text(i, v, f\"{v:.3f}\", ha=\"center\", va=\"bottom\")\n    plt.ylabel(ylabel)\n    plt.tight_layout()\n    plt.savefig(filename, bbox_inches=\"tight\")\n    print(f\"Figure saved: {filename}\")\n\n\n# -------------------------------------------------------------------------\n#                              Main entry point\n# -------------------------------------------------------------------------\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--results-dir\", type=str, required=True)\n    args = parser.parse_args()\n    results_dir = Path(args.results_dir)\n    assert results_dir.exists(), f\"results-dir {results_dir} does not exist\"\n\n    mapping = collect_runs(results_dir)\n    if not mapping:\n        raise RuntimeError(\"No TensorBoard runs found – nothing to evaluate.\")\n\n    # collect per-epoch curves ------------------------------------------------\n    train_curves, val_curves = defaultdict(list), defaultdict(list)\n    final_scores = defaultdict(list)\n\n    for variation, event_dirs in mapping.items():\n        for evdir in event_dirs:\n            train_scalar = load_scalar_from_events(evdir, \"train/loss\")\n            val_scalar = load_scalar_from_events(evdir, \"val/score\")\n            if train_scalar:\n                train_curves[variation].append(train_scalar)\n            if val_scalar:\n                val_curves[variation].append(val_scalar)\n                final_scores[variation].append(val_scalar[-1])\n\n    # plot curves ------------------------------------------------------------\n    plot_mean_curve(train_curves, \"train/loss\", \"Training Loss\", results_dir / \"training_loss.pdf\")\n    plot_mean_curve(val_curves, \"val/score\", \"Validation Score\", results_dir / \"val_score.pdf\")\n    plot_final_bar(final_scores, \"Final Validation Score\", results_dir / \"final_val_score_bar.pdf\")\n\n    # stdout summary ---------------------------------------------------------\n    print(\"\\n===== Aggregated Final Validation Score =====\")\n    for variation, scores in final_scores.items():\n        print(f\"{variation:25s} | mean={np.mean(scores):.4f} ± {np.std(scores):.4f} (n={len(scores)})\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "preprocess_py": "\"\"\"src/preprocess.py\n\nShared data-loading and preprocessing utilities.  The actual *content*\nwill be supplied in follow-up steps by replacing the placeholders under\n`load_specific_dataset`.  For the smoke-test we fall back to a tiny\nsynthetic classification data set to guarantee that the whole pipeline\nruns on any machine in < 30 seconds.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport random\nfrom typing import Tuple\n\nimport numpy as np\nimport torch\nfrom sklearn.datasets import make_classification\nfrom torch.utils.data import DataLoader, Dataset, random_split\n\n\n# ---------------------------------------------------------- synthetic dummy data\nclass DummyClassificationDataset(Dataset):\n    \"\"\"Tiny in-memory classification data set for smoke tests.\"\"\"\n\n    def __init__(self, n_samples: int, n_features: int, n_classes: int, seed: int = 0):\n        X, y = make_classification(\n            n_samples=n_samples,\n            n_features=n_features,\n            n_informative=min(n_features, 15),\n            n_redundant=0,\n            n_classes=n_classes,\n            random_state=seed,\n        )\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n        self.classes = list(range(n_classes))\n\n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]\n\n    def __len__(self):\n        return len(self.X)\n\n\n# ---------------------------------------------------------- placeholder loader\n\ndef load_specific_dataset(config: dict):\n    \"\"\"Placeholder – will be replaced with domain-specific loader logic.\"\"\"\n    raise NotImplementedError(\"PLACEHOLDER: implement domain-specific dataset loading here\")\n\n\n# ---------------------------------------------------------- public API\n\ndef get_dataloaders(config: dict, is_smoke_test: bool) -> Tuple[DataLoader, DataLoader]:\n    batch_size = int(config[\"experiment\"].get(\"batch_size\", 64))\n    seed = int(config[\"experiment\"].get(\"seed\", 0))\n    torch_gen = torch.Generator()\n    torch_gen.manual_seed(seed)\n\n    if is_smoke_test or config[\"dataset\"].get(\"name\") == \"dummy_classification\":\n        ds_cfg = config[\"dataset\"]\n        dataset = DummyClassificationDataset(\n            n_samples=int(ds_cfg.get(\"num_samples\", 200)),\n            n_features=int(ds_cfg.get(\"num_features\", 20)),\n            n_classes=int(ds_cfg.get(\"num_classes\", 2)),\n            seed=seed,\n        )\n    else:\n        dataset = load_specific_dataset(config)  # raises until replaced\n\n    # train / validation split (80/20)\n    val_fraction = float(config[\"experiment\"].get(\"val_fraction\", 0.2))\n    val_len = int(len(dataset) * val_fraction)\n    train_len = len(dataset) - val_len\n    train_ds, val_ds = random_split(dataset, [train_len, val_len], generator=torch_gen)\n\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, generator=torch_gen)\n    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n    return train_loader, val_loader\n",
    "model_py": "\"\"\"src/model.py\n\nModel implementations that are *dataset agnostic* and therefore safe to\nship in the common foundation.  Down-stream experiment variants can add\nnew architectures by hooking into `build_model` while keeping the common\nAPI stable.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport torch\nimport torch.nn as nn\n\n\nclass SimpleMLP(nn.Module):\n    \"\"\"A minimal fully-connected network that works for tabular data.\"\"\"\n\n    def __init__(self, input_dim: int, output_dim: int, hidden_dim: int, n_layers: int = 2, dropout: float = 0.1):\n        super().__init__()\n        layers = []\n        in_features = input_dim\n        for _ in range(n_layers):\n            layers.append(nn.Linear(in_features, hidden_dim))\n            layers.append(nn.ReLU())\n            layers.append(nn.Dropout(dropout))\n            in_features = hidden_dim\n        layers.append(nn.Linear(in_features, output_dim))\n        self.net = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.net(x)\n\n\n# -------------------------------------------------------------------------\n#                         public factory function\n# -------------------------------------------------------------------------\n\ndef build_model(config: dict, input_dim: int, output_dim: int, task_type: str):\n    mdl_cfg = config[\"model\"]\n    model_type = mdl_cfg.get(\"type\", \"simple_mlp\")\n    if model_type == \"simple_mlp\":\n        hidden_dim = int(mdl_cfg.get(\"hidden_dim\", 64))\n        n_layers = int(mdl_cfg.get(\"n_layers\", 2))\n        dropout = float(mdl_cfg.get(\"dropout\", 0.1))\n        return SimpleMLP(input_dim, output_dim, hidden_dim, n_layers, dropout)\n    else:\n        raise NotImplementedError(f\"PLACEHOLDER: Model type '{model_type}' is not yet implemented.\")\n",
    "main_py": "\"\"\"src/main.py\n\nCLI entry-point.  Supports the two required invocation patterns:\n\nSmoke test (quick sanity check)\n    uv run python -m src.main --smoke-test\n\nFull experiment for a particular variation\n    uv run python -m src.main --full-experiment --variation \"SlopeStop-BO\"\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport os\nfrom pathlib import Path\n\nimport yaml\n\nfrom .train import ExperimentRunner\n\n\nRUN_VARIATIONS = [\n    \"SlopeStop-BO\",\n    \"A0-No-EarlyStop (fixed-budget GP-EI/c)\",\n    \"A1-SlopeStop w/o Cost-Aware EI\",\n    \"BOIL\",\n    \"Hyperband\",\n]\n\n\n# ---------------------------------------------------------------------------\n#                                CLI parser\n# ---------------------------------------------------------------------------\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    group = parser.add_mutually_exclusive_group(required=True)\n    group.add_argument(\"--smoke-test\", action=\"store_true\", help=\"Run a minimal smoke test\")\n    group.add_argument(\"--full-experiment\", action=\"store_true\", help=\"Run the full experiment\")\n    parser.add_argument(\"--variation\", type=str, default=\"SlopeStop-BO\", help=\"Which run variation to execute\")\n    parser.add_argument(\"--results-dir\", type=str, default=\"results\", help=\"Where to store all run artefacts\")\n    return parser.parse_args()\n\n\n# ---------------------------------------------------------------------------\n#                                    main\n# ---------------------------------------------------------------------------\n\ndef main():\n    args = parse_args()\n    cfg_path = \"config/smoke_test.yaml\" if args.smoke_test else \"config/full_experiment.yaml\"\n    assert os.path.exists(cfg_path), f\"Config file not found: {cfg_path}\"\n    with open(cfg_path, \"r\") as fp:\n        config = yaml.safe_load(fp)\n\n    variation = args.variation\n    if variation not in RUN_VARIATIONS:\n        raise ValueError(f\"Variation '{variation}' not recognised. Must be one of {RUN_VARIATIONS}\")\n\n    # Create results directory ------------------------------------------------\n    results_dir = Path(args.results_dir) / variation.replace(\" \", \"_\")\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    # ---------------------------------------------------------------------\n    # experiment description (printed *before* any results as required)\n    # ---------------------------------------------------------------------\n    print(\"\\n===== Experiment Description =====\")\n    print(config[\"experiment\"].get(\"description\", \"No description provided in config.\"))\n    print(f\"Variation: {variation}\\n\")\n\n    # run ------------------------------------------------------------------\n    runner = ExperimentRunner(config, variation, results_dir, is_smoke=args.smoke_test)\n    run_results = runner.run()\n\n    # ---------------------------------------------------------------------\n    # print numeric results to stdout\n    # ---------------------------------------------------------------------\n    print(\"===== Run Results =====\")\n    for idx, res in enumerate(run_results):\n        hp, r = res[\"hp\"], res[\"result\"]\n        print(f\"Run {idx:02d}: μ={r['mu']:.4f} | τ={r['tau']:.3f} | t_stop={r['t_stop']} | hp={hp}\")\n\n    # ---------------------------------------------------------------------\n    # list of generated figure names (from training script)\n    # ---------------------------------------------------------------------\n    print(\"\\nFigures generated: training_loss.pdf, val_score.pdf, final_val_score_bar.pdf\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "pyproject_toml": "[build-system]\nrequires = [\"setuptools>=61.0\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"slope-stop-bo\"\nversion = \"0.0.1\"\ndescription = \"Common core foundation for SlopeStop-BO experiments\"\nauthors = [\n  { name=\"Anonymous Researcher\", email=\"anon@example.com\" }\n]\nrequires-python = \">=3.9\"\ndependencies = [\n    \"torch>=2.0.0\",\n    \"botorch>=0.8.0\",\n    \"gpytorch>=1.11\",\n    \"numpy>=1.23\",\n    \"scikit-learn>=1.3\",\n    \"pyyaml>=6.0\",\n    \"tqdm>=4.64\",\n    \"tensorboardX>=2.6\",\n    \"matplotlib>=3.7\",\n    \"seaborn>=0.12\",\n]\n\n[project.optional-dependencies]\n# extras can be installed via `pip install .[dev]`\n.dev = [\n    \"black\",\n    \"isort\",\n    \"pytest\",\n]\n\n[tool.setuptools.packages.find]\nwhere = [\"src\"]\n",
    "smoke_test_yaml": "experiment:\n  description: \"Smoke test for common core foundation – trains three random configs on a synthetic data set to validate end-to-end functionality.\"\n  seed: 0\n  max_iters: 3\n  T_max: 5\n  batch_size: 32\n  task_type: classification\n  val_fraction: 0.2\n\ndataset:\n  name: dummy_classification\n  num_samples: 200\n  num_features: 20\n  num_classes: 2\n\nmodel:\n  type: simple_mlp\n  hidden_dim: 64\n  n_layers: 2\n  dropout: 0.1\n\n# hyper-parameter search space (continuous bounds)\nsearch_space:\n  lr: [0.0001, 0.1]\n  hidden_dim: [32, 128]\n\nslope_stop:\n  W: 2\n  epsilon: 0.001\n  gamma: 0.01\n",
    "full_experiment_yaml": "# ======================================================================\n#  Full experiment template                                               \n#  Replace the PLACEHOLDER sections with the actual dataset / model        \n#  configurations in the next workflow step.                              \n# ======================================================================\n\nexperiment:\n  description: \"FULL EXPERIMENT – PLACEHOLDER: will be populated with detailed task description later.\"\n  seed: 0\n  max_iters: 50            # total number of HPO iterations per method\n  T_max: 50                # maximum training epochs per configuration\n  batch_size: 64\n  task_type: classification  # or regression depending on DATASET_PLACEHOLDER\n  val_fraction: 0.2\n\ndataset:\n  name: DATASET_PLACEHOLDER            # e.g. \"lcbench_task_123\"\n  path: DATA_PATH_PLACEHOLDER          # absolute or relative path to data on disk\n  num_features: FEATURE_DIM_PLACEHOLDER\n  num_classes: NUM_CLASSES_PLACEHOLDER\n\nmodel:\n  type: MODEL_PLACEHOLDER              # e.g. \"resnet18\", \"tabnet\", etc.\n  SPECIFIC_CONFIG_PLACEHOLDER: 0       # any model-specific hyper-params go here\n\n# hyper-parameter search space (continuous bounds; add / modify as needed)\nsearch_space:\n  lr: [1e-4, 1e-1]\n  weight_decay: [1e-6, 1e-2]\n  hidden_dim: [32, 512]               # only relevant for MLP-type models\n  SPECIFIC_HP_PLACEHOLDER: [0.0, 1.0]\n\nslope_stop:\n  W: 3\n  epsilon: 0.001\n  gamma: 0.01\n\n# end of template\n\n# ----------------------------------------------------------------------\n# NOTE: Do *not* change keys or structure – downstream scripts rely on   \n#       this schema to be identical across all experiment variations.    \n# ----------------------------------------------------------------------\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\"}"
}
