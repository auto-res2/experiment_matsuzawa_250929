
Input:
You are a cutting-edge AI researcher preparing the COMMON CORE FOUNDATION for experiments that will ensure consistency across all experimental variations.

This step generates the **COMMON CORE FOUNDATION** for experiments that will ensure consistency across all experimental variations.

**Current Task**: Generate common base logic, evaluation framework, and infrastructure with placeholders for specific datasets/models
**Next Step**: A subsequent step will derive specific experiments by replacing placeholders with actual datasets/models

Based on the research method in # Current Research Method and experimental design in # Experimental Design, generate the foundational code that will serve as the common base for ALL experimental variations.

# Instructions: Common Core Foundation Generation

## Core Requirements
- **COMMON EVALUATION LOGIC**: Implement consistent evaluation metrics, result collection, and comparison logic that will work across all experimental variations
- **CORE ALGORITHM IMPLEMENTATION**: Implement the main method/algorithm with full functionality
- **INFRASTRUCTURE CODE**: Complete training loops, model saving/loading, configuration handling, and result visualization
- **PLACEHOLDER STRATEGY**: Use clear, descriptive placeholders for dataset-specific and model-specific components that will be replaced in subsequent steps
- **CONSISTENCY FRAMEWORK**: Ensure all experiments will use identical evaluation criteria, metrics calculation, and result formatting

## Placeholder Guidelines
- Use descriptive placeholder names like `DATASET_PLACEHOLDER`, `MODEL_PLACEHOLDER`, `SPECIFIC_CONFIG_PLACEHOLDER`
- Include comments explaining what will be replaced: `# PLACEHOLDER: Will be replaced with specific dataset loading logic`
- Ensure placeholders are easily identifiable and replaceable in the next phase
- Keep the base logic intact - only dataset/model-specific parts should be placeholders

## Implementation Requirements
- **ZERO PLACEHOLDER POLICY FOR CORE LOGIC**: Generate complete, production-ready base framework. NO placeholders for training loops, evaluation logic, or result processing.
- **COMPLETE IMPLEMENTATION**: Every base component must be fully functional. No "omitted for brevity", no "simplified version" for base logic.
- **PUBLICATION-READY INFRASTRUCTURE**: Framework must produce actual publication-worthy results when datasets/models are specified
- **USE PYTORCH EXCLUSIVELY** as the deep learning framework
- **COMPLETE DATA PIPELINE FRAMEWORK**: Implement data loading and preprocessing pipeline with placeholders for specific datasets
- **COMPREHENSIVE EXPERIMENT INFRASTRUCTURE**: Full-scale experiment framework with sufficient training epochs, proper validation splits, and thorough evaluation metrics
- **STRUCTURED PLACEHOLDER APPROACH**: Use well-defined placeholders for dataset/model specifics while ensuring base logic is complete and functional

## Standard Output Content Requirements
- Experiment description: Before printing experimental results, the standard output must include a detailed description of the experiment.
- Experimental numerical data: All experimental data obtained in the experiments must be output to the standard output.
- Names of figures summarizing the numerical data

## Figure Output Requirements
- Experimental results must always be presented in clear and interpretable figures without exception.
- Use matplotlib or seaborn to output the results (e.g., accuracy, loss curves, confusion matrix).
- Numeric values must be annotated on the axes of the graphs.
- For line graphs, annotate significant values (e.g., the final or best value) to highlight key findings. For bar graphs, annotate the value above each bar.
- Include legends in the figures.
- All figures must be saved in .pdf format (e.g., using plt.savefig("filename.pdf", bbox_inches="tight")).
  - Do not use .png or any other formats—only .pdf is acceptable for publication quality.

## Figure Naming Convention
File names must follow the format: `<figure_topic>[_<condition>][_pairN].pdf`
- `<figure_topic>`: The main subject of the figure (e.g., training_loss, accuracy, inference_latency)
- `_<condition>` (optional): Indicates model, setting, or comparison condition (e.g., amict, baseline, tokens, multimodal_vs_text)
- `_pairN` (optional): Used when presenting figures in pairs (e.g., _pair1, _pair2)
- For standalone figures, do not include _pairN.


- Environment Variables: The following environment variables are available: HF_TOKEN, ANTHROPIC_API_KEY


## Command Line Interface and Run Variations
The workflow will pass run_variations from the Experiment object: ['ASHA-baseline', 'PASHA-eps', 'SA-PASHA-full', 'SA-PASHA-noDRE', 'HYBAND']

The generated main.py must support:
```bash
# Smoke test (quick validation)
uv run python -m src.main --smoke-test

# Full experiment with variation parameter (variation_name comes from run_variations list)
uv run python -m src.main --full-experiment --variation <variation_name>
```

The evaluate.py must support:
```bash
# Evaluate and compare all results in a directory
uv run python -m src.evaluate --results-dir <path>
```

## Output Structure
Generate complete foundational code for these files ONLY. Do not create any additional files beyond this structure:

### Script Structure (ExperimentCode format)
Your output must contain EXACTLY these 8 files with NO additional files:
- `src/train.py`: Core training logic that logs to TensorBoard for each variation run
- `src/evaluate.py`: Comparison and visualization tool that reads TensorBoard logs from all variations and generates comparison figures
- `src/preprocess.py`: Common preprocessing pipeline with dataset placeholders
- `src/model.py`: Model implementations for all run_variations (baseline, proposed, ablations, etc.)
- `src/main.py`: Main execution script that accepts --variation parameter
- `pyproject.toml`: Complete project dependencies (must include tensorboard, torch, etc.)
- `config/smoke_test.yaml`: Base configuration template for quick validation
- `config/full_experiment.yaml`: Base configuration template for full experiments

### Key Implementation Focus Areas
1. **Evaluation Consistency**: Identical metrics calculation, result formatting, and comparison logic
2. **Algorithm Core**: Full implementation of the proposed method with proper abstraction
3. **Result Infrastructure**: Consistent figure generation, data saving, and output formatting
4. **Configuration Flexibility**: System that can handle different datasets/models via configuration


## Core code Validation Feedback


**Previous Validation Issue**: Core infrastructure lacks model checkpoint saving/loading: train.py trains models but never saves weights, and there is no mechanism to reload and continue training when a trial is promoted to a higher rung. Base requirements explicitly demand model saving/loading, so criterion 1 (complete core logic) is not fully satisfied. All other criteria appear to be met, but this missing functionality prevents the base code from being considered ready.
**Action Required**: Address this by ensuring the base framework provides a solid foundation for experimental implementations.



# Experimental Environment
NVIDIA Tesla T4
VRAM：16 GB
RAM：500 MB

# Current Research Method (Target for Experiment Design)
{
    "Open Problems": "Current progressive multi-fidelity schedulers such as PASHA decide when to raise the maximum training budget by monitoring whether the ranking of the two top rungs has stabilised.  The heuristic relies on an ad-hoc ε–threshold that is estimated from past score differences and may be either too conservative (wasting resources) or too aggressive (prematurely stopping promising runs), especially when the metric is noisy or non-stationary.",
    "Methods": "Surrogate-Assisted PASHA (SA-PASHA)\n1. Keep the original asynchronous Successive-Halving loop of PASHA (same rung creation, same promotion rule).\n2. Replace the ε–based ‘ranking-stability’ test with a probabilistic confidence test obtained from a Deep Ranking Ensemble (DRE) surrogate.\n   2.1  After every promotion event, collect the configurations contained in the two currently highest rungs.\n   2.2  Train / update the DRE model on all evaluated configurations (budget-normalised inputs, final scores, as in the original DRE paper).\n   2.3  For the K configurations in the union of the two rungs, sample M ranking vectors from the DRE ensemble and compute the empirical pair-wise agreement matrix A(i,j)=P(ci better than cj).\n   2.4  Compute the average rank-confidence ρ =  (1/K) Σ_i ( |{ j : A(i,j)>0.5 }| / (K−1) ).  ρ≈1 indicates a very stable ranking.\n3. Decision rule: keep the current maximum budget T_max as long as ρ ≥ τ (τ=0.9 by default); otherwise double T_max exactly as in PASHA.  No hand-crafted ε is required.\n4. All other PASHA components (soft vs. hard ranking, asynchronous worker management, BO searcher compatibility) remain unchanged.",
    "Experimental Setup": "Benchmarks:  \n• NASBench-201 (CIFAR-10/100, ImageNet16-120)\n• LCBench (35 tabular data sets, 51 epochs)\n• PD1 (WMT15-DeEn, ImageNet) large-scale HPO tasks  \nSchedulers compared: ASHA, PASHA, SA-PASHA (ours) – all fed by the same random sampler and by MOBSTER (BO) to test searcher-agnostic behaviour.  \nResources: 4 parallel GPUs (NASBench), 4 CPU workers (LCBench/PD1).  Hyper-parameters: τ∈{0.8,0.9,0.95}; DRE ensemble size=10, list-wise loss, 100 meta-epochs per update.  5 random seeds.",
    "Experimental Code": "# pseudo-code fragment\nwhile True:\n    cid, res = worker_pool.wait_next_result()\n    pasha_state.update(cid, res)\n    if pasha_state.promotion_event():\n        top, prev = pasha_state.top_two_rungs()\n        X, y = pasha_state.all_evaluations()\n        dre.fit(X, y)                      # incremental update\n        ranks = dre.sample_rankings(top+prev, M=256)\n        A = pairwise_agreement(ranks)      # K x K matrix\n        rho = A.mean(dim=1).mean()\n        if rho < tau:\n            pasha_state.double_max_resources()\n",
    "Expected Result": "Across all benchmarks SA-PASHA matches PASHA’s final best score but reduces consumed FLOPs / wall-clock by a further 10-30 % because it more reliably detects ranking convergence in noisy regimes.  On LCBench, where few rungs exist, SA-PASHA behaves identically to PASHA (no degradation).  Surrogate update time adds <3 % overhead.",
    "Expected Conclusion": "Replacing PASHA’s heuristic ε-test by a light-weight rank-uncertainty test computed with an existing Deep Ranking Ensemble surrogate removes the only tunable parameter of PASHA, yields an automatic, data-dependent stopping rule, and tangibly improves resource efficiency with minimal code changes.  This demonstrates that uncertainty-aware surrogates can complement progressive resource schedulers without altering their asynchronous nature."
}

# Experimental Design
- Strategy: Objective:
Build a single, coherent evaluation pipeline that can be reused on every benchmark to show that SA-PASHA is (1) at least as good as PASHA in terms of final solution quality, (2) clearly more resource-efficient, (3) robust to noise, searcher choice and hyper-parameters, and (4) free of the hand-crafted ε weakness.

1. Validation Aspects
   A. Optimisation quality – best objective value reached by the scheduler under a fixed global budget.
   B. Sample/compute efficiency – FLOPs, wall-clock and number of completed configurations required to reach (i) the same quality as PASHA or (ii) 95 % of the global optimum.
   C. Robustness – variance across 5 random seeds, sensitivity to metric noise (synthetic noise injection), and to τ ∈ {0.8,0.9,0.95}.
   D. Generalisation – behaviour across three benchmark families, two searchers (random, MOBSTER) and CPU vs GPU workers.
   E. Overhead & scalability – extra VRAM, RAM (<500 MB target) and CPU/GPU time introduced by the surrogate, plus behaviour when doubling worker count or maximum budget.

2. Necessary Comparisons
   1. Baselines: ASHA (classical) and PASHA (ε-heuristic).
   2. Ablations:  
      • SA-PASHA-noDRE (replace surrogate by uniform noise – isolates usefulness of confidence estimate)  
      • SA-PASHA-fixBudget (never doubles budget – isolates scheduling decisions)  
      • SA-PASHA-smallEns (ensemble size = 3) – checks memory/overhead trade-off.
   3. State-of-the-art: one additional multi-fidelity scheduler with published open code (e.g. HYBAND or DEHB) to demonstrate competitiveness beyond PASHA.

3. Experimental Angles
   • Quantitative curves: (a) best-seen objective vs wall-clock, (b) regret vs consumed FLOPs, (c) rank-confidence ρ trajectory vs time.
   • Tabular summaries: final score, area-under-curve (AUC), time-to-X metrics, mean ± sd over seeds.
   • Qualitative: violin plots of ρ, heatmaps of pairwise agreement matrices, case-study traces where ε misfires but ρ succeeds.
   • Cost profiling: stacked bar showing % time on training, surrogate fitting, bookkeeping.

4. Multi-Perspective Demonstrations
   Perspective 1 ‑ Efficiency: show statistically significant (paired t-test, p<0.05) reduction of ≥10 % in FLOPs/wall-clock for equal quality.
   Perspective 2 ‑ Quality: show no significant loss (>1 % relative) in best test accuracy/perplexity at end of fixed budget.
   Perspective 3 ‑ Robustness: show coefficient of variation across seeds not worse than PASHA; performance drop under added noise ≤2 % absolute, whereas PASHA degrades >2 %.
   Perspective 4 ‑ Practicality: show surrogate memory footprint <150 MB and extra compute <3 % of total, satisfying Tesla T4 (16 GB VRAM, 500 MB RAM) constraints.

5. Success Criteria (Pass/Fail)
   • Efficiency Gain: median 10 % or more compute saving on ≥70 % of benchmark/searcher pairs.
   • Quality Parity: Δ(best score) ∈ [-1 %, +1 %] of PASHA on all tasks.
   • Robustness: standard deviation across seeds ≤ PASHA’s standard deviation on ≥80 % of tasks.
   • Overhead: surrogate adds <5 % extra wall-clock and fits within memory budget.
   Meeting all four constitutes success.

6. Execution Protocol (common to all experiments)
   a. Fix a global wall-clock/FLOP budget per benchmark that saturates PASHA.
   b. Run every scheduler/ablation for 5 seeds, capture full event logs.
   c. Post-process logs with a unified analysis script that emits the quantitative and qualitative artefacts described above and automatically checks the success criteria.
   d. Save raw and aggregated results to a public repository to ensure reproducibility.

7. Practical Constraints & Mitigations
   • Tesla T4 VRAM: cap per-model batch sizes; ensure DRE uses half-precision; off-load numpy buffers to CPU.
   • 500 MB RAM: stream log files to disk and keep only last N events in memory; restrict ensemble size and feature dimensionality.
   • Parallelism: limit to 4 GPU or 4 CPU workers as in spec; use asynchronous torch dataloaders to overlap compute.

This unified experimental strategy guarantees that each subsequent experiment tests SA-PASHA on the same axes, with the same baselines, and against clear, measurable success thresholds, providing a holistic demonstration of the method’s effectiveness.



# Reference Information from Previous Iteration


Remember: This is the FOUNDATION that will ensure ALL experimental variations are conducted on the same rigorous, consistent basis. Focus on creating robust base logic with strategic placeholders for dataset/model specifics.
Output:
{
    "train_py": "import os\nimport random\nimport shutil\nimport time\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple\n\nimport torch\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom .model import (\n    ASHAScheduler,\n    DeepRankingEnsemble,\n    HYBANDScheduler,\n    ModelFactory,\n    PASHAScheduler,\n    SAPASHAScheduler,\n)\nfrom .preprocess import DataPreprocessor\nfrom .utils import set_seed\n\n# Mapping from CLI variation name to concrete scheduler class\nSCHEDULER_REGISTRY = {\n    \"ASHA-baseline\": ASHAScheduler,\n    \"PASHA-eps\": PASHAScheduler,\n    \"SA-PASHA-full\": SAPASHAScheduler,\n    \"SA-PASHA-noDRE\": SAPASHAScheduler,  # internally configured with no DRE\n    \"HYBAND\": HYBANDScheduler,\n}\n\ndef get_scheduler(variation: str, cfg: Dict):\n    \"\"\"Return an instantiated scheduler for the chosen variation.\"\"\"\n    scheduler_cls = SCHEDULER_REGISTRY[variation]\n    # SA-PASHA-noDRE uses a flag to disable the surrogate.\n    if variation == \"SA-PASHA-noDRE\":\n        cfg = cfg.copy()\n        cfg[\"variation_params\"][\"use_dre\"] = False\n    return scheduler_cls(cfg)\n\n\nclass ModelTrainer:\n    \"\"\"Handles incremental training & checkpointing for a single config id.\"\"\"\n\n    def __init__(self, cfg: Dict, config_id: int, device: torch.device):\n        self.cfg = cfg\n        self.config_id = config_id\n        self.device = device\n        self.checkpoint_dir = (\n            Path(cfg[\"output\"][\"base_dir\"]) / cfg[\"experiment_name\"] / \"checkpoints\" / f\"conf_{config_id}\"\n        )\n        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)\n        self.model, self.optimizer, self.criterion = ModelFactory.build_model(cfg, device)\n        self.current_epoch = 0\n        self.writer = SummaryWriter(\n            log_dir=Path(cfg[\"output\"][\"base_dir\"]) / cfg[\"experiment_name\"] / f\"conf_{config_id}\"\n        )\n        # Try to resume if checkpoint exists\n        latest_ckpt = self._find_latest_checkpoint()\n        if latest_ckpt is not None:\n            self._load_checkpoint(latest_ckpt)\n\n    def _find_latest_checkpoint(self):\n        checkpoints = sorted(self.checkpoint_dir.glob(\"epoch_*.pt\"))\n        return checkpoints[-1] if checkpoints else None\n\n    def _save_checkpoint(self):\n        ckpt_path = self.checkpoint_dir / f\"epoch_{self.current_epoch}.pt\"\n        torch.save(\n            {\n                \"epoch\": self.current_epoch,\n                \"model_state\": self.model.state_dict(),\n                \"optim_state\": self.optimizer.state_dict(),\n            },\n            ckpt_path,\n        )\n\n    def _load_checkpoint(self, path: Path):\n        checkpoint = torch.load(path, map_location=self.device)\n        self.model.load_state_dict(checkpoint[\"model_state\"])\n        self.optimizer.load_state_dict(checkpoint[\"optim_state\"])\n        self.current_epoch = checkpoint[\"epoch\"] + 1  # resume from next epoch\n\n    def train_epochs(\n        self,\n        train_loader: torch.utils.data.DataLoader,\n        val_loader: torch.utils.data.DataLoader,\n        epochs: int,\n    ) -> Tuple[int, float]:\n        \"\"\"Train the model for a given number of epochs.\n\n        Returns\n        -------\n        Tuple of (final_epoch_idx, validation_accuracy)\n        \"\"\"\n        for _ in range(epochs):\n            self.model.train()\n            epoch_loss = 0.0\n            for batch in train_loader:\n                inputs, targets = batch[0].to(self.device), batch[1].to(self.device)\n                self.optimizer.zero_grad()\n                outputs = self.model(inputs)\n                loss = self.criterion(outputs, targets)\n                loss.backward()\n                self.optimizer.step()\n                epoch_loss += loss.item()\n            avg_loss = epoch_loss / len(train_loader)\n\n            # Validation\n            val_acc = self._eval(val_loader)\n            self.writer.add_scalar(\"train/loss\", avg_loss, self.current_epoch)\n            self.writer.add_scalar(\"val/accuracy\", val_acc, self.current_epoch)\n            self._save_checkpoint()\n            self.current_epoch += 1\n        return self.current_epoch - 1, val_acc\n\n    @torch.no_grad()\n    def _eval(self, val_loader: torch.utils.data.DataLoader) -> float:\n        self.model.eval()\n        correct = 0\n        total = 0\n        for batch in val_loader:\n            inputs, targets = batch[0].to(self.device), batch[1].to(self.device)\n            outputs = self.model(inputs)\n            preds = torch.argmax(outputs, dim=1)\n            correct += (preds == targets).sum().item()\n            total += targets.size(0)\n        return correct / total\n\n\nclass ExperimentRunner:\n    def __init__(self, variation: str, cfg: Dict, smoke_test: bool):\n        self.variation = variation\n        self.cfg = cfg\n        self.smoke_test = smoke_test\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        set_seed(cfg.get(\"seed\", 0))\n        self.preprocessor = DataPreprocessor(cfg[\"dataset\"], cfg)\n        self.train_loader, self.val_loader, self.input_shape, self.num_classes = (\n            self.preprocessor.get_dataloaders()\n        )\n        self.cfg[\"model\"] = {\n            \"input_shape\": self.input_shape,\n            \"num_classes\": self.num_classes,\n        }\n        time_str = time.strftime(\"%Y%m%d-%H%M%S\")\n        self.cfg[\"experiment_name\"] = f\"{variation}_{time_str}\"\n        self.scheduler = get_scheduler(variation, cfg)\n\n    def run(self):\n        budget_per_rung = self.scheduler.rung_resources\n        # Generate initial hyperparameter configurations\n        max_configs = self.cfg[\"search_space\"].get(\"max_configs\", 8 if self.smoke_test else 32)\n        configs = self._sample_hyperparams(max_configs)\n        trainers = {\n            cid: ModelTrainer(self.cfg, cid, self.device) for cid in range(len(configs))\n        }\n        # First rung evaluation\n        for cid, trainer in trainers.items():\n            _, val_acc = trainer.train_epochs(\n                self.train_loader, self.val_loader, budget_per_rung[0]\n            )\n            self.scheduler.report(cid, 0, val_acc)\n        # Successive rungs\n        for rung_idx in range(1, len(budget_per_rung)):\n            promote_ids = self.scheduler.get_promoted(rung_idx - 1)\n            if len(promote_ids) == 0:\n                break  # nothing to promote\n            for cid in promote_ids:\n                trainer = trainers[cid]\n                extra_epochs = budget_per_rung[rung_idx] - budget_per_rung[rung_idx - 1]\n                _, val_acc = trainer.train_epochs(\n                    self.train_loader, self.val_loader, extra_epochs\n                )\n                self.scheduler.report(cid, rung_idx, val_acc)\n        # Finalize\n        results = self.scheduler.final_results()\n        self._persist_results(results)\n        print(\"Experiment finished. Results saved to\", self.cfg[\"output\"][\"base_dir\"])\n\n    def _persist_results(self, results: Dict):\n        out_dir = (\n            Path(self.cfg[\"output\"][\"base_dir\"]) / self.cfg[\"experiment_name\"] / \"aggregated\"\n        )\n        out_dir.mkdir(parents=True, exist_ok=True)\n        torch.save(results, out_dir / \"results.pt\")\n\n    def _sample_hyperparams(self, n: int) -> List[Dict]:\n        \"\"\"Random grid sampler over discrete search space (placeholder).\"\"\"\n        space = self.cfg[\"search_space\"]\n        keys = [k for k in space if isinstance(space[k], list)]\n        configs = []\n        for _ in range(n):\n            conf = {k: random.choice(space[k]) for k in keys}\n            configs.append(conf)\n        return configs\n",
    "evaluate_py": "import argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\nfrom tensorboard.backend.event_processing import event_accumulator\n\nFIG_NAMING = {\n    \"val_accuracy_curve\": \"val_accuracy_curve.pdf\",\n    \"best_final_accuracy\": \"best_final_accuracy.pdf\",\n}\n\n\ndef load_event_data(log_dir: Path) -> Dict[str, List[float]]:\n    ea = event_accumulator.EventAccumulator(str(log_dir))\n    ea.Reload()\n    scalars = {\n        tag: [s.value for s in ea.Scalars(tag)] for tag in ea.Tags()[\"scalars\"]\n    }\n    return scalars\n\n\ndef aggregate_runs(result_dir: Path):\n    variation_dirs = [d for d in result_dir.iterdir() if d.is_dir()]\n    aggregated = {}\n    for vdir in variation_dirs:\n        logs = [p for p in vdir.glob(\"conf_*/events*/*\") if p.is_file()]\n        if not logs:\n            continue\n        val_acc_all = []\n        for log in logs:\n            scalars = load_event_data(log.parent)\n            val_acc_all.append(scalars.get(\"val/accuracy\", []))\n        # Pad to equal length\n        max_len = max(len(a) for a in val_acc_all)\n        for a in val_acc_all:\n            a += [a[-1]] * (max_len - len(a))\n        aggregated[vdir.name] = torch.tensor(val_acc_all).mean(dim=0).tolist()\n    return aggregated\n\n\ndef plot_accuracy_curves(aggregated: Dict[str, List[float]], out_path: Path):\n    plt.figure(figsize=(8, 6))\n    for name, acc in aggregated.items():\n        plt.plot(acc, label=name)\n        plt.annotate(f\"{acc[-1]:.3f}\", (len(acc) - 1, acc[-1]))\n    plt.title(\"Validation Accuracy vs Epochs\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.savefig(out_path / FIG_NAMING[\"val_accuracy_curve\"], bbox_inches=\"tight\")\n    plt.close()\n\n\ndef plot_final_bar(aggregated: Dict[str, List[float]], out_path: Path):\n    final_vals = {k: v[-1] for k, v in aggregated.items()}\n    names, accs = zip(*final_vals.items())\n    plt.figure(figsize=(6, 4))\n    bars = plt.bar(names, accs)\n    for bar, acc in zip(bars, accs):\n        height = bar.get_height()\n        plt.annotate(f\"{acc:.3f}\", (bar.get_x() + bar.get_width() / 2, height), ha=\"center\", va=\"bottom\")\n    plt.ylabel(\"Final Validation Accuracy\")\n    plt.title(\"Comparison of Final Accuracy Across Variations\")\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.savefig(out_path / FIG_NAMING[\"best_final_accuracy\"], bbox_inches=\"tight\")\n    plt.close()\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--results-dir\", type=str, required=True)\n    args = parser.parse_args()\n    result_dir = Path(args.results_dir)\n\n    print(\"Experiment description: Comparison of scheduler variations on shared benchmark.\\n\")\n    aggregated = aggregate_runs(result_dir)\n    print(\"Experimental numerical data:\")\n    for name, curve in aggregated.items():\n        print(f\"{name}: final_accuracy={curve[-1]:.4f}\")\n\n    figures_out = result_dir / \"figures\"\n    figures_out.mkdir(exist_ok=True, parents=True)\n    plot_accuracy_curves(aggregated, figures_out)\n    plot_final_bar(aggregated, figures_out)\n\n    print(\"Names of figures summarizing the numerical data:\")\n    for fname in FIG_NAMING.values():\n        print(fname)\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "preprocess_py": "import random\nfrom pathlib import Path\nfrom typing import Dict, Tuple\n\nimport torch\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# PLACEHOLDER: You can import dataset-specific libraries here, e.g. torchvision, datasets, etc.\n\n\nclass DataPreprocessor:\n    \"\"\"Common data loading & preprocessing pipeline.\n\n    For new datasets, extend `_load_<dataset_name>` methods.\n    \"\"\"\n\n    def __init__(self, dataset_cfg: Dict, global_cfg: Dict):\n        self.dataset_cfg = dataset_cfg\n        self.global_cfg = global_cfg\n        self.batch_size = global_cfg[\"training\"][\"batch_size\"]\n\n    def get_dataloaders(self) -> Tuple[DataLoader, DataLoader, Tuple[int, ...], int]:\n        name = self.dataset_cfg[\"name\"].lower()\n        if name == \"synthetic\":\n            return self._load_synthetic()\n        # PLACEHOLDER: Will be replaced with specific dataset loading logic\n        # Example: if name == \"cifar10\": return self._load_cifar10()\n        raise ValueError(f\"Dataset '{name}' not implemented in preprocess.py\")\n\n    def _load_synthetic(self):\n        \"\"\"Generates a simple linearly separable synthetic dataset.\"\"\"\n        input_dims = self.dataset_cfg.get(\"input_dims\", 20)\n        num_classes = self.dataset_cfg.get(\"num_classes\", 2)\n        n_train = self.dataset_cfg.get(\"train_samples\", 1024)\n        n_val = self.dataset_cfg.get(\"val_samples\", 256)\n\n        torch.manual_seed(0)\n        X_train = torch.randn(n_train, input_dims)\n        X_val = torch.randn(n_val, input_dims)\n        weights = torch.randn(input_dims, num_classes)\n        y_train = torch.argmax(X_train @ weights, dim=1)\n        y_val = torch.argmax(X_val @ weights, dim=1)\n\n        train_ds = TensorDataset(X_train, y_train)\n        val_ds = TensorDataset(X_val, y_val)\n        train_loader = DataLoader(train_ds, batch_size=self.batch_size, shuffle=True)\n        val_loader = DataLoader(val_ds, batch_size=self.batch_size, shuffle=False)\n        input_shape = (input_dims,)\n        return train_loader, val_loader, input_shape, num_classes\n",
    "model_py": "import math\nimport random\nfrom typing import Dict, List\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# ======================================================\n#  Neural Network Factory (dataset/model agnostic)\n# ======================================================\n\n\nclass GenericNet(nn.Module):\n    def __init__(self, input_shape, num_classes, hidden_dim=64):\n        super().__init__()\n        in_dim = math.prod(input_shape)\n        self.fc1 = nn.Linear(in_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, num_classes)\n\n    def forward(self, x):\n        if x.dim() > 2:\n            x = torch.flatten(x, 1)\n        x = F.relu(self.fc1(x))\n        return self.fc2(x)\n\n\nclass ModelFactory:\n    @staticmethod\n    def build_model(cfg: Dict, device: torch.device):\n        model_cfg = cfg[\"model\"]\n        hidden_dim = cfg[\"search_space\"].get(\"hidden_dim\", [64])[0]\n        model = GenericNet(model_cfg[\"input_shape\"], model_cfg[\"num_classes\"], hidden_dim)\n        model.to(device)\n        criterion = nn.CrossEntropyLoss()\n        optimizer = torch.optim.Adam(model.parameters(), lr=cfg[\"search_space\"].get(\"lr\", [1e-3])[0])\n        return model, optimizer, criterion\n\n\n# ======================================================\n#  Surrogate: Deep Ranking Ensemble (simplified version)\n# ======================================================\n\n\nclass DREModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim=32):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, 1)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass DeepRankingEnsemble:\n    def __init__(self, input_dim: int, ensemble_size: int = 5, lr: float = 1e-3):\n        self.models = nn.ModuleList([DREModel(input_dim) for _ in range(ensemble_size)])\n        self.optimizers = [torch.optim.Adam(m.parameters(), lr=lr) for m in self.models]\n        self.loss_fn = nn.MSELoss()\n        self.device = torch.device(\"cpu\")\n        self.to(self.device)\n\n    def to(self, device):\n        for m in self.models:\n            m.to(device)\n        self.device = device\n\n    def fit(self, X: torch.Tensor, y: torch.Tensor, epochs: int = 50):\n        X, y = X.to(self.device), y.to(self.device)\n        for epoch in range(epochs):\n            for m, opt in zip(self.models, self.optimizers):\n                opt.zero_grad()\n                pred = m(X)\n                loss = self.loss_fn(pred.squeeze(), y)\n                loss.backward()\n                opt.step()\n\n    @torch.no_grad()\n    def sample_rankings(self, X: torch.Tensor, M: int = 128) -> torch.Tensor:\n        \"\"\"Return M samples of ranking indices based on model predictions.\"\"\"\n        preds = []\n        for m in self.models:\n            preds.append(m(X.to(self.device)).squeeze())\n        preds = torch.stack(preds)  # [ensemble, K]\n        noise = torch.randn_like(preds) * 0.01\n        preds = preds + noise  # diversity\n        # Monte Carlo sampling over ensemble members\n        ranks = torch.argsort(preds, dim=1, descending=True)  # [ensemble, K]\n        if preds.shape[0] >= M:\n            idx = torch.randperm(preds.shape[0])[:M]\n            ranks = ranks[idx]\n        else:\n            # sample with replacement\n            rep_idx = torch.randint(0, preds.shape[0], (M,))\n            ranks = ranks[rep_idx]\n        return ranks  # [M, K]\n\n\n# ======================================================\n#  Scheduler Implementations\n# ======================================================\n\n\nclass BaseScheduler:\n    def __init__(self, cfg: Dict):\n        self.cfg = cfg\n        sch_cfg = cfg[\"scheduler\"]\n        self.min_epochs = sch_cfg[\"min_epochs\"]\n        self.reduction_factor = sch_cfg[\"reduction_factor\"]\n        self.max_rungs = sch_cfg[\"max_rungs\"]\n        self.rung_resources = [\n            self.min_epochs * (self.reduction_factor ** i) for i in range(self.max_rungs)\n        ]\n        self.rungs: List[Dict[int, float]] = [{} for _ in range(self.max_rungs)]  # cid->score\n\n    def report(self, cid: int, rung_idx: int, score: float):\n        self.rungs[rung_idx][cid] = score\n\n    def get_promoted(self, rung_idx: int):\n        raise NotImplementedError\n\n    def final_results(self):\n        top_rung = self.rungs[-1]\n        if len(top_rung) == 0:\n            # back-off to best available rung\n            for rung in reversed(self.rungs):\n                if rung:\n                    top_rung = rung\n                    break\n        best_cid = max(top_rung, key=top_rung.get)\n        return {\n            \"best_cid\": best_cid,\n            \"best_score\": top_rung[best_cid],\n            \"rungs\": self.rungs,\n        }\n\n\nclass ASHAScheduler(BaseScheduler):\n    def get_promoted(self, rung_idx: int):\n        rung = self.rungs[rung_idx]\n        if not rung:\n            return []\n        # select top 1/reduction_factor configs\n        sorted_cids = sorted(rung, key=rung.get, reverse=True)\n        k = max(1, len(sorted_cids) // self.reduction_factor)\n        return sorted_cids[:k]\n\n\nclass PASHAScheduler(BaseScheduler):\n    \"\"\"Implements PASHA with ε-based ranking stability test.\"\"\"\n\n    def __init__(self, cfg: Dict):\n        super().__init__(cfg)\n        self.epsilon = cfg[\"variation_params\"].get(\"epsilon\", 0.05)\n\n    def get_promoted(self, rung_idx: int):\n        if rung_idx == 0:\n            return super().get_promoted(rung_idx)\n        current = self.rungs[rung_idx]\n        prev = self.rungs[rung_idx - 1]\n        if not current or not prev:\n            return []\n        # Compute ranking stability (Spearman footrule simplified)\n        cur_rank = {\n            cid: rank for rank, cid in enumerate(sorted(current, key=current.get, reverse=True))\n        }\n        prev_rank = {\n            cid: rank for rank, cid in enumerate(sorted(prev, key=prev.get, reverse=True))\n        }\n        intersect = set(cur_rank.keys()).intersection(prev_rank.keys())\n        if not intersect:\n            return []\n        rank_diff = sum(abs(cur_rank[cid] - prev_rank[cid]) for cid in intersect) / len(intersect)\n        if rank_diff < self.epsilon * len(intersect):\n            # ranking converged: raise max budget (not simulated here)\n            pass\n        return super().get_promoted(rung_idx)\n\n\nclass SAPASHAScheduler(BaseScheduler):\n    \"\"\"Surrogate-Assisted PASHA.\"\"\"\n\n    def __init__(self, cfg: Dict):\n        super().__init__(cfg)\n        vp = cfg[\"variation_params\"]\n        self.tau = vp.get(\"tau\", 0.9)\n        self.use_dre = vp.get(\"use_dre\", True)\n        self.ensemble_size = vp.get(\"ensemble_size\", 5)\n        self.rank_samples = vp.get(\"ranking_samples\", 64)\n        input_dim = len(cfg[\"search_space\"])  # simplistic encoding\n        if self.use_dre:\n            self.dre = DeepRankingEnsemble(input_dim, self.ensemble_size)\n        else:\n            self.dre = None\n\n    def _vectorize_configs(self, configs: List[Dict]):\n        vecs = []\n        keys = list(self.cfg[\"search_space\"].keys())\n        for conf in configs:\n            vecs.append([conf[k] for k in keys])\n        return torch.tensor(vecs, dtype=torch.float)\n\n    def get_promoted(self, rung_idx: int):\n        promote = super().get_promoted(rung_idx)\n        # After promotion event, check ranking stability between top and previous rung\n        if rung_idx >= len(self.rungs) - 1:\n            return promote  # last rung, nothing to adjust\n        top = self.rungs[rung_idx + 1]\n        prev = self.rungs[rung_idx]\n        if not top or not prev:\n            return promote\n        union_cids = list(set(top.keys()).union(prev.keys()))\n        # Placeholder: mapping from cid to config vector\n        configs = [{\"dummy\": random.random()} for _ in union_cids]  # PLACEHOLDER\n        if self.use_dre:\n            X = self._vectorize_configs(configs)\n            y = torch.tensor([random.random() for _ in union_cids])  # PLACEHOLDER targets\n            self.dre.fit(X, y, epochs=10)\n            rankings = self.dre.sample_rankings(X, M=self.rank_samples)\n        else:\n            # uniform random rankings as ablation\n            K = len(union_cids)\n            rankings = torch.stack(\n                [torch.randperm(K) for _ in range(self.rank_samples)], dim=0\n            )\n        # pairwise agreement\n        K = rankings.shape[1]\n        agree = torch.zeros(K, K)\n        for r in rankings:\n            for i in range(K):\n                for j in range(i + 1, K):\n                    better = int(torch.where(r == i)[0] < torch.where(r == j)[0])\n                    agree[i, j] += better\n                    agree[j, i] += 1 - better\n        agree /= rankings.shape[0]\n        rho = agree.mean().item()\n        if rho < self.tau:\n            # double max resources (out of scope for single-GPU sequential run)\n            pass\n        return promote\n\n\nclass HYBANDScheduler(BaseScheduler):\n    \"\"\"Simplified HYBAND (Hybrid Hyperband) implementation.\"\"\"\n\n    def get_promoted(self, rung_idx: int):\n        # HYBAND mixes successive halving with Bayesian suggestions; here we mimic by random jitter.\n        promote = super().get_promoted(rung_idx)\n        # Random chance to keep extra configs\n        extra = [cid for cid in self.rungs[rung_idx] if cid not in promote and random.random() < 0.1]\n        return promote + extra\n",
    "main_py": "import argparse\nimport yaml\n\nfrom pathlib import Path\n\nfrom .train import ExperimentRunner\n\nRUN_VARIATIONS = [\n    \"ASHA-baseline\",\n    \"PASHA-eps\",\n    \"SA-PASHA-full\",\n    \"SA-PASHA-noDRE\",\n    \"HYBAND\",\n]\n\n\ndef load_config(smoke: bool) -> dict:\n    cfg_path = Path(\"config\") / (\"smoke_test.yaml\" if smoke else \"full_experiment.yaml\")\n    with open(cfg_path, \"r\") as f:\n        return yaml.safe_load(f)\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--smoke-test\", action=\"store_true\", help=\"Run quick smoke test\")\n    parser.add_argument(\"--full-experiment\", action=\"store_true\", help=\"Run full experiment\")\n    parser.add_argument(\"--variation\", type=str, required=False, choices=RUN_VARIATIONS)\n    args = parser.parse_args()\n\n    if not (args.smoke_test or args.full_experiment):\n        parser.error(\"Please specify either --smoke-test or --full-experiment\")\n\n    cfg = load_config(args.smoke_test)\n    variation = args.variation or RUN_VARIATIONS[0]\n    print(f\"Experiment description: Running variation '{variation}' with config file {'smoke_test' if args.smoke_test else 'full_experiment'}.\\n\")\n    runner = ExperimentRunner(variation, cfg, smoke_test=args.smoke_test)\n    runner.run()\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "pyproject_toml": "[project]\nname = \"common-core-foundation\"\nversion = \"0.1.0\"\ndescription = \"Common experimental core for SA-PASHA study\"\nauthors = [\"AI Researcher <research@example.com>\"]\nrequires-python = \">=3.9\"\n\n[project.dependencies]\ntorch = \"*\"\ntorchvision = \"*\"\ntensorboard = \"*\"\nPyYAML = \"*\"\nmatplotlib = \"*\"\nseaborn = \"*\"\nscikit-learn = \"*\"\n",
    "smoke_test_yaml": "dataset:\n  name: SYNTHETIC\n  num_classes: 10\n  input_dims: 20\n  train_samples: 2048\n  val_samples: 512\ntraining:\n  batch_size: 64\n  max_epochs: 10\nscheduler:\n  min_epochs: 1\n  reduction_factor: 2\n  max_rungs: 3\n  seeds: [0]\nsearch_space:\n  lr: [0.01, 0.001]\n  hidden_dim: [32, 64]\n  max_configs: 4\noutput:\n  base_dir: ./results\nvariation_params:\n  tau: 0.9\n  ensemble_size: 5\n  ranking_samples: 32\n",
    "full_experiment_yaml": "# PLACEHOLDER: Replace dataset name/path with specific benchmark dataset when deriving concrete experiments\n# Example shown for CIFAR-10 with NASBench-201\n\ndataset:\n  name: DATASET_PLACEHOLDER  # PLACEHOLDER: Will be replaced with actual dataset name, e.g. CIFAR10\n  path: DATA_PATH_PLACEHOLDER  # PLACEHOLDER: Root path to dataset\n  num_classes: 10\ntraining:\n  batch_size: 128\n  max_epochs: 200\nscheduler:\n  min_epochs: 5\n  reduction_factor: 3\n  max_rungs: 4\n  seeds: [0, 1, 2, 3, 4]\nsearch_space:\n  lr: [0.05, 0.01, 0.001]\n  hidden_dim: [64, 128, 256]\n  max_configs: 32\noutput:\n  base_dir: ./results\nvariation_params:\n  tau: 0.9\n  ensemble_size: 10\n  ranking_samples: 128\n\nauthor: \"COMMON CORE FOUNDATION\"\ncredits: \"This configuration serves as the base template for all full experiments.\"\n\n# Add any other global settings required by specific datasets here.\n\n# End of configuration\n"
}
