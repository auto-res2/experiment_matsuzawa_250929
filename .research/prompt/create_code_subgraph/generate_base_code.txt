
Input:
You are a cutting-edge AI researcher preparing the COMMON CORE FOUNDATION for experiments that will ensure consistency across all experimental variations.

This step generates the **COMMON CORE FOUNDATION** for experiments that will ensure consistency across all experimental variations.

**Current Task**: Generate common base logic, evaluation framework, and infrastructure with placeholders for specific datasets/models
**Next Step**: A subsequent step will derive specific experiments by replacing placeholders with actual datasets/models

Based on the research method in # Current Research Method and experimental design in # Experimental Design, generate the foundational code that will serve as the common base for ALL experimental variations.

# Instructions: Common Core Foundation Generation

## Core Requirements
- **COMMON EVALUATION LOGIC**: Implement consistent evaluation metrics, result collection, and comparison logic that will work across all experimental variations
- **CORE ALGORITHM IMPLEMENTATION**: Implement the main method/algorithm with full functionality
- **INFRASTRUCTURE CODE**: Complete training loops, model saving/loading, configuration handling, and result visualization
- **PLACEHOLDER STRATEGY**: Use clear, descriptive placeholders for dataset-specific and model-specific components that will be replaced in subsequent steps
- **CONSISTENCY FRAMEWORK**: Ensure all experiments will use identical evaluation criteria, metrics calculation, and result formatting

## Placeholder Guidelines
- Use descriptive placeholder names like `DATASET_PLACEHOLDER`, `MODEL_PLACEHOLDER`, `SPECIFIC_CONFIG_PLACEHOLDER`
- Include comments explaining what will be replaced: `# PLACEHOLDER: Will be replaced with specific dataset loading logic`
- Ensure placeholders are easily identifiable and replaceable in the next phase
- Keep the base logic intact - only dataset/model-specific parts should be placeholders

## Implementation Requirements
- **ZERO PLACEHOLDER POLICY FOR CORE LOGIC**: Generate complete, production-ready base framework. NO placeholders for training loops, evaluation logic, or result processing.
- **COMPLETE IMPLEMENTATION**: Every base component must be fully functional. No "omitted for brevity", no "simplified version" for base logic.
- **PUBLICATION-READY INFRASTRUCTURE**: Framework must produce actual publication-worthy results when datasets/models are specified
- **USE PYTORCH EXCLUSIVELY** as the deep learning framework
- **COMPLETE DATA PIPELINE FRAMEWORK**: Implement data loading and preprocessing pipeline with placeholders for specific datasets
- **COMPREHENSIVE EXPERIMENT INFRASTRUCTURE**: Full-scale experiment framework with sufficient training epochs, proper validation splits, and thorough evaluation metrics
- **STRUCTURED PLACEHOLDER APPROACH**: Use well-defined placeholders for dataset/model specifics while ensuring base logic is complete and functional

## Standard Output Content Requirements
- Experiment description: Before printing experimental results, the standard output must include a detailed description of the experiment.
- Experimental numerical data: All experimental data obtained in the experiments must be output to the standard output.
- Names of figures summarizing the numerical data

## Figure Output Requirements
- Experimental results must always be presented in clear and interpretable figures without exception.
- Use matplotlib or seaborn to output the results (e.g., accuracy, loss curves, confusion matrix).
- Numeric values must be annotated on the axes of the graphs.
- For line graphs, annotate significant values (e.g., the final or best value) to highlight key findings. For bar graphs, annotate the value above each bar.
- Include legends in the figures.
- All figures must be saved in .pdf format (e.g., using plt.savefig("filename.pdf", bbox_inches="tight")).
  - Do not use .png or any other formats—only .pdf is acceptable for publication quality.

## Figure Naming Convention
File names must follow the format: `<figure_topic>[_<condition>][_pairN].pdf`
- `<figure_topic>`: The main subject of the figure (e.g., training_loss, accuracy, inference_latency)
- `_<condition>` (optional): Indicates model, setting, or comparison condition (e.g., amict, baseline, tokens, multimodal_vs_text)
- `_pairN` (optional): Used when presenting figures in pairs (e.g., _pair1, _pair2)
- For standalone figures, do not include _pairN.


- Environment Variables: The following environment variables are available: HF_TOKEN, ANTHROPIC_API_KEY


## Command Line Interface and Run Variations
The workflow will pass run_variations from the Experiment object: ['SlopeStop-BO', 'A0-No-EarlyStop (fixed-budget GP-EI/c)', 'A1-SlopeStop w/o Cost-Aware EI', 'BOIL', 'Hyperband']

The generated main.py must support:
```bash
# Smoke test (quick validation)
uv run python -m src.main --smoke-test

# Full experiment with variation parameter (variation_name comes from run_variations list)
uv run python -m src.main --full-experiment --variation <variation_name>
```

The evaluate.py must support:
```bash
# Evaluate and compare all results in a directory
uv run python -m src.evaluate --results-dir <path>
```

## Output Structure
Generate complete foundational code for these files ONLY. Do not create any additional files beyond this structure:

### Script Structure (ExperimentCode format)
Your output must contain EXACTLY these 8 files with NO additional files:
- `src/train.py`: Core training logic that logs to TensorBoard for each variation run
- `src/evaluate.py`: Comparison and visualization tool that reads TensorBoard logs from all variations and generates comparison figures
- `src/preprocess.py`: Common preprocessing pipeline with dataset placeholders
- `src/model.py`: Model implementations for all run_variations (baseline, proposed, ablations, etc.)
- `src/main.py`: Main execution script that accepts --variation parameter
- `pyproject.toml`: Complete project dependencies (must include tensorboard, torch, etc.)
- `config/smoke_test.yaml`: Base configuration template for quick validation
- `config/full_experiment.yaml`: Base configuration template for full experiments

### Key Implementation Focus Areas
1. **Evaluation Consistency**: Identical metrics calculation, result formatting, and comparison logic
2. **Algorithm Core**: Full implementation of the proposed method with proper abstraction
3. **Result Infrastructure**: Consistent figure generation, data saving, and output formatting
4. **Configuration Flexibility**: System that can handle different datasets/models via configuration


## Core code Validation Feedback


**Previous Validation Issue**: Core requirement (1) is not met: the framework never saves or reloads trained model checkpoints – there is no call to torch.save / torch.load anywhere in src/train.py or the runner.  Consequently experiments cannot be resumed, re-evaluated or used for downstream tasks.  In addition, evaluate.py contains a logic bug (uses `if tag not in ev.Scalars(tag)` – should check `tag in ev.Tags()['scalars']`), which would break metric loading.  Everything else (placeholder usage, 8-file structure, CLI, PyTorch implementation, result visualisation) is solid, but the missing model persistence and evaluation bug mean the base foundation is not yet complete.
**Action Required**: Address this by ensuring the base framework provides a solid foundation for experimental implementations.



# Experimental Environment
NVIDIA Tesla T4
VRAM：16 GB
RAM：500 MB

# Current Research Method (Target for Experiment Design)
{
    "Open Problems": "1. Most BO‐based HPO algorithms either (a) train a configuration to a fixed budget (Hyperband family) or (b) embed the full learning curve in an expensive joint surrogate (e.g. BOIL, DyHPO).  Both strategies waste budget on clearly converged runs.\n2. A very cheap, task–agnostic early–stopping rule that can be fed back into a BO loop without re–designing the surrogate is still missing.",
    "Methods": "SlopeStop-BO (Slope-informed Early-Stopping Bayesian Optimisation)\n1. Online slope test  ▸  During training we keep a W–step moving window of the validation score v_t.  At step t we compute the window slope s_t = (v_t – v_{t-W})/W.\n2. Stopping rule  ▸  If s_t < ϵ·exp(–γ·t) the run is considered saturated and training is terminated immediately (two scalars ϵ,γ are global hyper-parameters chosen once per task; default ϵ=10⁻³, γ=0.01).\n3. Fast curve compression  ▸  At termination we store a two–dimensional summary per configuration x:  (μ=mean(v_{t-W: t}),  τ=t_stop / T_max) instead of the whole curve.  This removes the need for a product kernel over (x,t).\n4. Lightweight surrogate  ▸  A standard GP with ARD RBF kernel is fitted on the augmented input [x ; τ] → μ. GP hyper-parameters are updated every 2·D observations by ML; no expensive joint optimisation over logistic parameters as in BOIL.\n5. Acquisition  ▸  Expected Improvement divided by estimated cost c(x)=τ·T_max.  The next configuration is chosen with EI/c as usual.\n6. Budget bookkeeping  ▸  Because many runs stop early, the total consumed epochs fall 30-50 % below a fixed-budget scheme.",
    "Experimental Setup": "Datasets:  • LCBench (35 tabular tasks, 50 epoch learning curves)  • NAS-Bench-201 (3 image tasks, 200 epoch curves).\nBaselines: Hyperband, BOHB, DyHPO, BOIL.\nProtocol: 10 independent seeds, total wall-clock budget identical to BOIL (GPU: RTX 3090).\nHyper-parameters W=3, ϵ=1e-3, γ=0.01 fixed for all tasks.",
    "Experimental Code": "# pseudo-python\nfor conf in pool:\n    t, buf = 0, []\n    while t < T_max:\n        loss = train_one_epoch(conf)\n        buf.append(loss); t += 1\n        if len(buf) > W:\n            slope = (buf[-1]-buf[-W-1])/W\n            if slope < eps*np.exp(-gamma*t):\n                break                # ← early stop\n    mu = np.mean(buf[-W:]); tau = t / T_max\n    D.append(([conf, tau], mu, cost=t))\n    if len(D)% (2*dim)==0: refit_GP(D)\n    x_next = argmax_EI_over_cost(GP)",
    "Expected Result": "Compared with BOIL:  • ~40 % less total training epochs  • identical median simple-regret after budget normalisation.\nCompared with Hyperband/BOHB:  • ~60 % lower final regret for the same wall time.",
    "Expected Conclusion": "A purely slope-based, task-agnostic early-stopping rule plus a standard GP surrogate is enough to harvest most of the budget savings that more complex learning-curve models aim for, while keeping the optimisation loop simple and cheap.  SlopeStop-BO is therefore a practical drop-in replacement for fixed-budget BO in large-scale HPO campaigns."
}

# Experimental Design
- Strategy: Overall Experimental Strategy for Validating SlopeStop-BO

0. Guiding Principle
Evaluate whether a cheap, task–agnostic slope-based early-stopping rule coupled with a lightweight GP surrogate (a) attains the same or better optimisation quality than existing HPO methods and (b) does so with demonstrably lower computational cost, higher robustness and broader applicability.

1. Core Properties to Validate
1.1 Optimisation quality: final and anytime simple-regret / best-seen validation error.
1.2 Computational efficiency: (i) consumed training epochs, (ii) wall-clock time, (iii) GPU memory footprint.
1.3 Cost–performance trade-off: Pareto frontier of regret vs cost.
1.4 Robustness & stability: variance across random seeds and across heterogeneous tasks.
1.5 Generalisation: behaviour on unseen domains (vision vs tabular, small vs large models, shallow vs deep learning curves).
1.6 Method components: contribution of each design choice (slope rule, curve compression, EI/c acquisition).

2. Required Comparisons
2.1 External baselines: Hyperband, BOHB, DyHPO, BOIL, vanilla fixed-budget BO.
2.2 Internal ablations:
    A0  No early stop (fixed budget) + same GP + EI/c.
    A1  SlopeStop w/o cost-aware acquisition.
    A2  SlopeStop with full-curve GP (replace compression).
    A3  Vary (W, ε, γ) to test sensitivity.
2.3 Oracle reference: best possible performance given full curves (post-hoc) to bound achievable regret.

3. Experimental Angles / Evidence Streams
3.1 Quantitative benchmarks
    • Curves of mean/median regret vs wall-time and vs epochs.
    • Area-Under-Curve (AUC) of regret over time (lower is better).
    • Number of early terminations & average stopping epoch.
    • Paired significance tests (Wilcoxon signed-rank, α = 0.05) per task.
3.2 Efficiency analysis
    • GPU runtime and VRAM peak via nvidia-smi logs.
    • CPU RAM usage via psutil (critical with 500 MB cap).
    • Scalability plots: cost ∝ dataset size & T_max.
3.3 Robustness analysis
    • Standard deviation across 10 seeds.
    • Leave-one-domain-out generalisation (train hyper-params on tabular, test on vision and vice-versa).
    • Stress test on noisy tasks by injecting synthetic noise.
3.4 Qualitative inspection
    • Example learning curves showing early-stop points vs final convergence.
    • 2-D GP posterior slices on [x; τ] to illustrate surrogate smoothness.

4. Experimental Protocol Template (applies to every dataset)
P1 Budget normalisation: all methods receive identical total GPU-seconds. Budget is chosen so the largest baseline (BOIL) fits on a single NVIDIA T4 with 16 GB.
P2 10 independent random seeds.
P3 Fixed common preprocessing & data loaders to reduce variance.
P4 Unified logging format (wandb/CSV) capturing per-epoch loss, time, memory.
P5 Post-processing script computes all metrics and statistical tests in one pass.

5. Success / Validation Criteria
C1 Performance: SlopeStop-BO attains equal or lower median final regret than best baseline on ≥70 % of tasks and is not significantly worse on any task.
C2 Efficiency: achieves ≥30 % reduction in total epochs and ≥20 % wall-time savings versus BOIL while staying within 16 GB VRAM and 500 MB system RAM.
C3 Robustness: coefficient of variation (CV) across seeds ≤ baseline CV for at least 80 % of tasks.
C4 Ablations: full model outperforms every ablation in both regret and AUC on aggregate benchmarks.
C5 Generalisation: hyper-parameters (W, ε, γ) tuned on half of the tasks transfer without retuning to the remaining tasks with <5 % relative regret degradation.

6. Practical Considerations for the T4 / 16 GB / 500 MB Environment
• Use mixed-precision (FP16) training where applicable.
• Stream dataset batches from disk to respect 500 MB RAM limit; keep only last W+1 validation losses in memory.
• Store compressed curve summaries (μ, τ) to minimise RAM.
• Run methods serially rather than in parallel to avoid memory contention; accumulate wall-time budgets accordingly.

7. Reporting
All results presented as
    (a) Task-level tables of final regret, epochs, wall-time
    (b) Aggregate violin/box plots
    (c) Pareto scatter plots (regret vs cost)
    (d) Ablation bar charts
    (e) Example qualitative figures.

This comprehensive, multi-angle strategy ensures that the proposed SlopeStop-BO is assessed not only for raw performance but also for efficiency, robustness, and practicality under stringent hardware constraints, thereby providing compelling evidence for its effectiveness as a drop-in replacement for existing HPO methods.
- Details: 



# Reference Information from Previous Iteration


Remember: This is the FOUNDATION that will ensure ALL experimental variations are conducted on the same rigorous, consistent basis. Focus on creating robust base logic with strategic placeholders for dataset/model specifics.
Output:
{
    "train_py": "\"\"\"src/train.py\nCore training and hyper–parameter optimisation framework shared by ALL experimental\nvariations.  This file contains\n  •   The generic model-training routine with checkpointing and TensorBoard logging\n  •   Slope-based early-stopping implementation (SlopeStop)\n  •   Simple GP + EI(/cost) based optimiser to realise the SlopeStop-BO algorithm\n  •   Baseline / ablation algorithms (Fixed-Budget BO, SlopeStop w/o cost, BOIL,\n      Hyperband) implemented as subclasses of BaseHPOAlgorithm\nThe code is completely self-contained with NO placeholders for the core logic.\nOnly dataset/model specifics are injected from the configuration and the helper\nfunctions provided in preprocess.py / model.py.\n\"\"\"\nfrom __future__ import annotations\n\nimport itertools\nimport json\nimport math\nimport random\nimport shutil\nimport time\nfrom abc import ABC, abstractmethod\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Tuple\n\nimport matplotlib\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom matplotlib import pyplot as plt\nfrom tensorboard.backend.event_processing import event_accumulator\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom .model import build_model, flat_config_to_vector\nfrom .preprocess import get_dataloaders\n\n# Re-usable helpers --------------------------------------------------------- #\n\ndef save_checkpoint(state: Dict[str, Any], ckpt_path: Path) -> None:\n    \"\"\"Save a checkpoint (model + optimiser + meta).\"\"\"\n    ckpt_path.parent.mkdir(parents=True, exist_ok=True)\n    torch.save(state, ckpt_path)\n\n\ndef load_checkpoint(ckpt_path: Path, device: torch.device = torch.device(\"cpu\")) -> Dict[str, Any]:\n    \"\"\"Load a previously saved checkpoint (handles CPU / GPU mismatch).\"\"\"\n    map_location = \"cuda\" if device.type == \"cuda\" else \"cpu\"\n    return torch.load(ckpt_path, map_location=map_location)\n\n\n# ----------------------- common training routine -------------------------- #\n\ndef train_single_model(\n    run_dir: Path,\n    cfg: Dict[str, Any],\n    hyper_cfg: Dict[str, Any],\n    device: torch.device,\n    early_stop: bool,\n    slope_params: Dict[str, float] | None,\n) -> Tuple[float, int, List[float]]:\n    \"\"\"Train **one** model configuration.\n\n    Parameters\n    ----------\n    run_dir : Path\n        Directory to write TensorBoard logs & checkpoints to.\n    cfg : Dict[str, Any]\n        Global YAML configuration (dataset, optimisation etc.).\n    hyper_cfg : Dict[str, Any]\n        The concrete hyper-parameters (sampled by the HPO algorithm).\n    device : torch.device\n        CUDA / CPU device.\n    early_stop : bool\n        Whether SlopeStop early-stopping should be applied.\n    slope_params : Dict[str, float] | None\n        Parameters {W, eps, gamma}.  Ignored if `early_stop` is False.\n\n    Returns\n    -------\n    best_val : float\n        Best validation metric seen.\n    epochs_run : int\n        Number of epochs actually trained (may be < T_max).\n    val_curve : List[float]\n        Validation metric at every epoch.\n    \"\"\"\n    # --------------------------------------------------------------------- #\n    # (1) data loaders & model\n    train_loader, val_loader, input_dim, output_dim = get_dataloaders(cfg)\n    model = build_model(cfg, hyper_cfg, input_dim, output_dim).to(device)\n\n    # Use AMP if requested\n    use_amp = bool(cfg.get(\"mixed_precision\", False) and device.type == \"cuda\")\n    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n\n    criterion_name = cfg.get(\"criterion\", \"mse\")\n    if criterion_name.lower().startswith(\"ce\"):\n        criterion: nn.Module = nn.CrossEntropyLoss()\n        optimise_for_min = True\n    else:\n        criterion = nn.MSELoss()\n        optimise_for_min = True\n\n    opt = optim.AdamW(model.parameters(), lr=hyper_cfg[\"learning_rate\"])\n\n    # --------------------------------------------------------------------- #\n    max_epochs: int = int(cfg.get(\"max_epochs\", 50))\n    writer = SummaryWriter(log_dir=str(run_dir / \"tb\"))\n    ckpt_path = run_dir / \"checkpoint.pt\"\n\n    best_val = float(\"inf\") if optimise_for_min else -float(\"inf\")\n    val_curve: List[float] = []\n\n    W, eps, gamma = None, None, None\n    if early_stop:\n        W = int(slope_params[\"W\"])\n        eps = float(slope_params[\"eps\"])\n        gamma = float(slope_params[\"gamma\"])\n        window: List[float] = []\n\n    for epoch in range(1, max_epochs + 1):\n        # ---------------- training loop ---------------- #\n        model.train()\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            opt.zero_grad(set_to_none=True)\n            with torch.cuda.amp.autocast(enabled=use_amp):\n                preds = model(xb)\n                loss = criterion(preds, yb)\n            scaler.scale(loss).backward()\n            scaler.step(opt)\n            scaler.update()\n\n        # ---------------- validation ------------------- #\n        model.eval()\n        val_losses: List[float] = []\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                xb, yb = xb.to(device), yb.to(device)\n                with torch.cuda.amp.autocast(enabled=use_amp):\n                    preds = model(xb)\n                    vloss = criterion(preds, yb).item()\n                    val_losses.append(vloss)\n        val_metric = float(np.mean(val_losses))\n        val_curve.append(val_metric)\n\n        if optimise_for_min:\n            is_best = val_metric < best_val\n        else:\n            is_best = val_metric > best_val\n        if is_best:\n            best_val = val_metric\n\n        # ---------- TensorBoard logging & checkpointing ---------- #\n        writer.add_scalar(\"train/learning_rate\", opt.param_groups[0][\"lr\"], epoch)\n        writer.add_scalar(\"val/metric\", val_metric, epoch)\n        writer.add_scalar(\"meta/epoch\", epoch, epoch)\n        writer.flush()\n\n        state = {\n            \"model_state_dict\": model.state_dict(),\n            \"optimizer_state_dict\": opt.state_dict(),\n            \"epoch\": epoch,\n            \"best_val\": best_val,\n            \"hyper_cfg\": hyper_cfg,\n        }\n        save_checkpoint(state, ckpt_path)\n\n        # ---------- SlopeStop early-stopping ---------------------- #\n        if early_stop:\n            window.append(val_metric)\n            if len(window) > W:\n                slope = (window[-1] - window[-W - 1]) / W\n                thresh = eps * math.exp(-gamma * epoch)\n                # For minimisation we want absolute value of slope\n                if abs(slope) < thresh:\n                    break\n        # --------------------------------------------------------- #\n    writer.close()\n    return best_val, epoch, val_curve\n\n\n# --------------------------- GP helper ------------------------------------ #\n\ndef _rbf_kernel(x1: torch.Tensor, x2: torch.Tensor, lengthscale: float = 1.0, var: float = 1.0):\n    dists = torch.cdist(x1 / lengthscale, x2 / lengthscale, p=2) ** 2\n    return var * torch.exp(-0.5 * dists)\n\n\ndef gp_posterior(\n    X: torch.Tensor,  # (n, d)\n    y: torch.Tensor,  # (n,)\n    X_star: torch.Tensor,  # (m, d)\n    noise: float = 1e-4,\n    lengthscale: float = 1.0,\n    var: float = 1.0,\n):\n    \"\"\"Very small GP implementation with analytical posterior (for n<=500).\n    This is sufficient for synthetic / smoke-test-level experiments while keeping\n    dependencies minimal.\n    \"\"\"\n    K = _rbf_kernel(X, X, lengthscale, var) + noise * torch.eye(X.shape[0])\n    Ks = _rbf_kernel(X_star, X, lengthscale, var)\n    K_inv = torch.linalg.inv(K)\n    mu = Ks @ K_inv @ y\n    cov = _rbf_kernel(X_star, X_star, lengthscale, var) - Ks @ K_inv @ Ks.T\n    # only diagonal needed for EI\n    var_pred = torch.diag(cov).clamp_min(1e-9)\n    return mu, var_pred.sqrt()\n\n\ndef expected_improvement(\n    mu: torch.Tensor,\n    sigma: torch.Tensor,\n    y_best: float,\n    xi: float = 0.01,\n    maximise: bool = False,\n):\n    \"\"\"Compute EI for **minimisation** unless maximise==True.\"\"\"\n    if maximise:\n        imp = mu - y_best - xi\n    else:\n        imp = y_best - mu - xi\n    Z = imp / sigma\n    ei = imp * 0.5 * (1 + torch.erf(Z / math.sqrt(2))) + sigma * torch.exp(-0.5 * Z**2) / math.sqrt(2 * math.pi)\n    ei[sigma < 1e-9] = 0.0\n    return ei\n\n\n# ---------------------- HPO Algorithm classes ----------------------------- #\n\nclass BaseHPOAlgorithm(ABC):\n    \"\"\"Abstract base class for every HPO algorithm.\"\"\"\n\n    def __init__(self, cfg: Dict[str, Any], device: torch.device, variation: str, run_root: Path):\n        self.cfg = cfg\n        self.device = device\n        self.variation = variation\n        self.run_root = run_root\n        self.slope_params = {\n            \"W\": cfg.get(\"slope_W\", 3),\n            \"eps\": cfg.get(\"slope_eps\", 1e-3),\n            \"gamma\": cfg.get(\"slope_gamma\", 0.01),\n        }\n        self.T_max = int(cfg.get(\"max_epochs\", 50))\n        self.search_space = cfg[\"search_space\"]\n        self.max_trials = int(cfg.get(\"max_trials\", 10))\n\n    # ------------------------------------------------------------------ #\n    def _sample_random_config(self) -> Dict[str, Any]:\n        \"\"\"Uniform random sampler over the search space.\"\"\"\n        cfg = {}\n        for name, spec in self.search_space.items():\n            kind = spec[0]\n            if kind == \"log_uniform\":\n                low, high = float(spec[1]), float(spec[2])\n                val = 10 ** random.uniform(math.log10(low), math.log10(high))\n            elif kind == \"uniform\":\n                low, high = float(spec[1]), float(spec[2])\n                val = random.uniform(low, high)\n            elif kind == \"int\":\n                low, high = int(spec[1]), int(spec[2])\n                val = random.randint(low, high)\n            else:\n                raise ValueError(f\"Unknown hyper-parameter type: {kind}\")\n            cfg[name] = val\n        return cfg\n\n    # ------------------------------------------------------------------ #\n    @abstractmethod\n    def run(self) -> None:\n        \"\"\"Execute the search and write results to disk.\"\"\"\n\n\nclass FixedBudgetBO(BaseHPOAlgorithm):\n    \"\"\"A0 – BO with **no** early stopping (fixed budget).  Uses EI/cost where cost=T_max.\"\"\"\n\n    def run(self):  # noqa: C901 – complexity acceptable for single function\n        X, y, costs = [], [], []\n        best_val = float(\"inf\")\n\n        for trial in range(1, self.max_trials + 1):\n            # ---------------------------------------------------------- #\n            if len(X) < max(3, int(self.cfg.get(\"init_random\", 3))):\n                hyper_cfg = self._sample_random_config()\n            else:\n                # Fit GP and pick next via EI/c\n                X_tensor = torch.tensor(np.array(X), dtype=torch.float32)\n                y_tensor = torch.tensor(np.array(y), dtype=torch.float32)\n                cand_cfgs = [self._sample_random_config() for _ in range(128)]\n                cand_X = torch.tensor([\n                    flat_config_to_vector(cc, self.search_space) for cc in cand_cfgs\n                ], dtype=torch.float32)\n                mu, sigma = gp_posterior(X_tensor, y_tensor, cand_X)\n                ei = expected_improvement(mu, sigma, best_val, maximise=False) / self.T_max\n                idx = int(torch.argmax(ei).item())\n                hyper_cfg = cand_cfgs[idx]\n            # ---------------------------------------------------------- #\n            trial_dir = self.run_root / f\"trial_{trial:03d}\"\n            best, epochs_run, curve = train_single_model(\n                trial_dir,\n                self.cfg,\n                hyper_cfg,\n                self.device,\n                early_stop=False,\n                slope_params=None,\n            )\n            X.append(flat_config_to_vector(hyper_cfg, self.search_space))\n            y.append(best)\n            costs.append(self.T_max)\n            best_val = min(best_val, best)\n            # Persist metadata for external evaluation\n            meta_path = self.run_root / f\"trial_{trial:03d}\" / \"meta.json\"\n            json.dump({\"best_val\": best, \"epochs\": epochs_run, \"config\": hyper_cfg}, meta_path.open(\"w\"))\n\n\nclass SlopeStopBO(BaseHPOAlgorithm):\n    \"\"\"Proposed algorithm: slope-based early stopping + EI/c acquisition.\"\"\"\n\n    def run(self):\n        X, y, costs = [], [], []\n        best_val = float(\"inf\")\n        for trial in range(1, self.max_trials + 1):\n            # --------------------- candidate suggestion ------------------ #\n            if len(X) < max(3, int(self.cfg.get(\"init_random\", 3))):\n                hyper_cfg = self._sample_random_config()\n            else:\n                X_tensor = torch.tensor(np.array(X), dtype=torch.float32)\n                y_tensor = torch.tensor(np.array(y), dtype=torch.float32)\n                cand_cfgs = [self._sample_random_config() for _ in range(128)]\n                cand_tau = torch.tensor([\n                    np.mean([c[\"tau\"] for c in costs]) if costs else 1.0  # fallback\n                ] * 128).unsqueeze(-1)\n                cand_Xvec = torch.tensor([\n                    np.concatenate([\n                        flat_config_to_vector(cc, self.search_space), [cand_tau[i]]\n                    ])\n                    for i, cc in enumerate(cand_cfgs)\n                ], dtype=torch.float32)\n                mu, sig = gp_posterior(X_tensor, y_tensor, cand_Xvec)\n                # cost estimate = tau * T_max  (tau assumed same as pop mean)\n                ei_over_c = expected_improvement(mu, sig, best_val) / (cand_tau.squeeze() * self.T_max)\n                idx = int(torch.argmax(ei_over_c).item())\n                hyper_cfg = cand_cfgs[idx]\n            # --------------------- evaluation --------------------------- #\n            trial_dir = self.run_root / f\"trial_{trial:03d}\"\n            best, epochs_run, curve = train_single_model(\n                trial_dir,\n                self.cfg,\n                hyper_cfg,\n                self.device,\n                early_stop=True,\n                slope_params=self.slope_params,\n            )\n            tau = epochs_run / self.T_max\n            X.append(np.concatenate([flat_config_to_vector(hyper_cfg, self.search_space), [tau]]))\n            y.append(best)\n            costs.append({\"tau\": tau})\n            best_val = min(best_val, best)\n            json.dump({\"best_val\": best, \"epochs\": epochs_run, \"config\": hyper_cfg}, (trial_dir / \"meta.json\").open(\"w\"))\n\n\nclass SlopeStopNoCostEI(SlopeStopBO):\n    \"\"\"A1 – identical to SlopeStop-BO but uses plain EI (no cost division).\"\"\"\n\n    def run(self):\n        X, y, taus = [], [], []\n        best_val = float(\"inf\")\n        for trial in range(1, self.max_trials + 1):\n            if len(X) < max(3, int(self.cfg.get(\"init_random\", 3))):\n                hyper_cfg = self._sample_random_config()\n            else:\n                X_tensor = torch.tensor(np.array(X), dtype=torch.float32)\n                y_tensor = torch.tensor(np.array(y), dtype=torch.float32)\n                cand_cfgs = [self._sample_random_config() for _ in range(128)]\n                cand_tau = torch.tensor(taus if taus else [1.0] * 128).mean()\n                cand_Xvec = torch.tensor([\n                    np.concatenate([\n                        flat_config_to_vector(cc, self.search_space), [cand_tau]\n                    ])\n                    for cc in cand_cfgs\n                ], dtype=torch.float32)\n                mu, sig = gp_posterior(X_tensor, y_tensor, cand_Xvec)\n                ei = expected_improvement(mu, sig, best_val)\n                idx = int(torch.argmax(ei).item())\n                hyper_cfg = cand_cfgs[idx]\n            trial_dir = self.run_root / f\"trial_{trial:03d}\"\n            best, epochs_run, curve = train_single_model(\n                trial_dir,\n                self.cfg,\n                hyper_cfg,\n                self.device,\n                early_stop=True,\n                slope_params=self.slope_params,\n            )\n            tau = epochs_run / self.T_max\n            X.append(np.concatenate([flat_config_to_vector(hyper_cfg, self.search_space), [tau]]))\n            y.append(best)\n            taus.append(tau)\n            best_val = min(best_val, best)\n            json.dump({\"best_val\": best, \"epochs\": epochs_run, \"config\": hyper_cfg}, (trial_dir / \"meta.json\").open(\"w\"))\n\n\nclass Hyperband(BaseHPOAlgorithm):\n    \"\"\"Simplified Hyperband implementation with successive halving.\"\"\"\n\n    def run(self):\n        eta = int(self.cfg.get(\"hb_eta\", 3))\n        s_max = int(math.floor(math.log(self.T_max, eta)))\n        B = (s_max + 1) * self.T_max\n        trial_counter = 0\n        for s in reversed(range(s_max + 1)):\n            n = int(math.ceil(B / self.T_max / (s + 1) * eta ** s))\n            r = self.T_max * eta ** (-s)\n            # Start n configurations\n            configs = [self._sample_random_config() for _ in range(n)]\n            metrics = []\n            for i, conf in enumerate(configs):\n                trial_counter += 1\n                trial_dir = self.run_root / f\"trial_{trial_counter:03d}_s{s}_{i}\"\n                best, epochs_run, curve = train_single_model(\n                    trial_dir,\n                    self.cfg,\n                    conf,\n                    self.device,\n                    early_stop=False,  # budget already halved, no need for slope stop\n                    slope_params=None,\n                )\n                metrics.append((best, conf, trial_dir))\n            k = int(n / eta)\n            # Successive halving – keep top (n/eta) configs\n            metrics.sort(key=lambda tup: tup[0])  # lower is better\n            configs = [m[1] for m in metrics[:k]]\n        # Nothing further to do – all results already on disk.\n\n\nclass BOIL(FixedBudgetBO):\n    \"\"\"BOIL imitation – full curves but no early stopping.\"\"\"\n    # For simplicity, we inherit FixedBudgetBO (same behaviour) but we additionally\n    # store *full* learning curves per trial so that downstream analysis can fit\n    # its own curve models.  train_single_model already logs every epoch, hence\n    # no extra code required.\n    pass\n\n\n# ------------------------------ factory ----------------------------------- #\n\n_ALGO_MAP = {\n    \"SlopeStop-BO\": SlopeStopBO,\n    \"A0-No-EarlyStop (fixed-budget GP-EI/c)\": FixedBudgetBO,\n    \"A1-SlopeStop w/o Cost-Aware EI\": SlopeStopNoCostEI,\n    \"BOIL\": BOIL,\n    \"Hyperband\": Hyperband,\n}\n\n\ndef run_hpo(cfg: Dict[str, Any], variation: str, device: torch.device, out_dir: Path):\n    if variation not in _ALGO_MAP:\n        raise ValueError(f\"Unknown variation '{variation}'.  Allowed: {list(_ALGO_MAP)}\")\n    algo_cls = _ALGO_MAP[variation]\n    algo = algo_cls(cfg, device, variation, out_dir)\n    algo.run()\n\n",
    "evaluate_py": "\"\"\"src/evaluate.py\nEvaluation & visualisation tool.  Reads TensorBoard logs + meta.json files from\nmultiple experimental variations in *results_dir* and produces publication-ready\nfigures (PDF) comparing the main metrics.\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport re\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport numpy as np\nimport seaborn as sns\nimport torch\nfrom matplotlib import pyplot as plt\nfrom tensorboard.backend.event_processing import event_accumulator\n\n# --------------------- helpers ------------------------------------------- #\n\ndef _extract_scalars(tb_path: Path, tag: str) -> List[float]:\n    ea = event_accumulator.EventAccumulator(str(tb_path))\n    ea.Reload()\n    if tag not in ea.Tags().get(\"scalars\", []):  # <- fixed bug: correct membership test\n        return []\n    return [s.value for s in ea.Scalars(tag)]\n\n\ndef _gather_runs(results_dir: Path):\n    \"\"\"Return mapping variation -> list[run_dirs].\"\"\"\n    pattern = re.compile(r\".*_(SlopeStop-BO|A0-No-EarlyStop \\(fixed-budget GP-EI/c\\)|A1-SlopeStop w/o Cost-Aware EI|BOIL|Hyperband)$\")\n    runs: Dict[str, List[Path]] = {}\n    for run in results_dir.iterdir():\n        if not run.is_dir():\n            continue\n        m = pattern.match(run.name)\n        if m:\n            variation = m.group(1)\n            runs.setdefault(variation, []).append(run)\n    return runs\n\n\n# --------------------- main evaluation ----------------------------------- #\n\ndef evaluate_all(results_dir: Path):\n    runs_per_var = _gather_runs(results_dir)\n    var_best_vals: Dict[str, List[float]] = {}\n\n    for variation, run_dirs in runs_per_var.items():\n        best_values = []\n        for rd in run_dirs:\n            meta_file = rd / \"meta.json\"\n            if meta_file.exists():\n                with meta_file.open() as f:\n                    meta = json.load(f)\n                    best_values.append(meta[\"best_val\"])\n            else:\n                # fallback: load final val metric from TensorBoard\n                tb = list((rd / \"tb\").glob(\"events.*\"))\n                if not tb:\n                    continue\n                scalars = _extract_scalars(tb[0], \"val/metric\")\n                if scalars:\n                    best_values.append(min(scalars))\n        var_best_vals[variation] = best_values\n\n    # ---------------------- figure: bar chart ---------------------------- #\n    fig, ax = plt.subplots(figsize=(8, 4))\n    means = [np.mean(v) for v in var_best_vals.values()]\n    sns.barplot(x=list(var_best_vals.keys()), y=means, ax=ax)\n    ax.set_ylabel(\"Mean Best Validation Loss\")\n    for i, m in enumerate(means):\n        ax.text(i, m, f\"{m:.3f}\", ha=\"center\", va=\"bottom\")\n    ax.set_title(\"Comparison of Best Validation Loss across Variations\")\n    plt.xticks(rotation=45, ha=\"right\")\n    fig.tight_layout()\n    out_path = results_dir / \"best_val_loss.pdf\"\n    fig.savefig(out_path, bbox_inches=\"tight\")\n    print(f\"Saved figure: {out_path}\")\n\n    # ------------------- per variation training curves ------------------ #\n    for variation, run_dirs in runs_per_var.items():\n        fig, ax = plt.subplots(figsize=(6, 4))\n        for rd in run_dirs:\n            tb_files = list((rd / \"tb\").glob(\"events.*\"))\n            if not tb_files:\n                continue\n            vals = _extract_scalars(tb_files[0], \"val/metric\")\n            if vals:\n                ax.plot(vals, alpha=0.3)\n        ax.set_title(f\"Validation Curve – {variation}\")\n        ax.set_xlabel(\"Epoch\")\n        ax.set_ylabel(\"Validation Loss\")\n        fig.tight_layout()\n        curve_path = results_dir / f\"val_loss_{variation.replace(' ', '_')}.pdf\"\n        fig.savefig(curve_path, bbox_inches=\"tight\")\n        print(f\"Saved figure: {curve_path}\")\n\n\n# ------------------------- CLI ------------------------------------------- #\n\ndef cli():\n    p = argparse.ArgumentParser()\n    p.add_argument(\"--results-dir\", required=True, type=Path)\n    args = p.parse_args()\n    evaluate_all(args.results_dir.expanduser().resolve())\n\n\nif __name__ == \"__main__\":\n    cli()\n",
    "preprocess_py": "\"\"\"src/preprocess.py\nCommon preprocessing & data­loading utilities.\nPLACEHOLDERs are used exclusively for *dataset-specific* parts – the pipeline\nitself is fully functional: if the config requests `dataset: synthetic` we will\ncreate a small synthetic regression dataset that is sufficient for smoke tests\nand algorithmic debugging.\n\"\"\"\nfrom __future__ import annotations\n\nimport math\nfrom typing import Dict, Tuple\n\nimport torch\nfrom torch.utils.data import DataLoader, Dataset, random_split\n\n# --------------------------- synthetic data ------------------------------ #\n\nclass SyntheticRegressionDataset(Dataset):\n    \"\"\"y = sin( ||x|| ) + ε synthetic regression task.\"\"\"\n\n    def __init__(self, n: int, d: int, noise: float = 0.1):\n        super().__init__()\n        self.x = torch.rand(n, d) * 2 - 1  # in [-1,1]\n        self.y = torch.sin(self.x.norm(dim=1, keepdim=True) * math.pi) + noise * torch.randn(n, 1)\n        self.y = self.y.float()\n\n    def __len__(self):\n        return self.x.shape[0]\n\n    def __getitem__(self, idx):\n        return self.x[idx], self.y[idx]\n\n\n# ------------------------ public interface -------------------------------- #\n\ndef get_dataloaders(cfg: Dict, split_ratio: float = 0.8):\n    \"\"\"Return train/val DataLoader along with *input_dim* and *output_dim*.\"\"\"\n    batch_size = int(cfg.get(\"batch_size\", 32))\n\n    dataset_name = cfg.get(\"dataset\", \"synthetic\")\n    if dataset_name == \"synthetic\":\n        n = int(cfg.get(\"synthetic_samples\", 512))\n        d = int(cfg.get(\"synthetic_dim\", 16))\n        dataset: Dataset = SyntheticRegressionDataset(n, d)\n        input_dim, output_dim = d, 1\n    elif dataset_name == \"DATASET_PLACEHOLDER\":  # PLACEHOLDER: real dataset loader will replace this block\n        raise NotImplementedError(\"Dataset placeholder – please insert real dataset in derived experiment.\")\n    else:\n        raise ValueError(f\"Unknown dataset '{dataset_name}'.\")\n\n    train_len = int(len(dataset) * split_ratio)\n    val_len = len(dataset) - train_len\n    train_set, val_set = random_split(dataset, [train_len, val_len])\n\n    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n    return train_loader, val_loader, input_dim, output_dim\n",
    "model_py": "\"\"\"src/model.py\nModel definitions.  All *core* functionality is provided; dataset-specific or\nlarge architectures can be swapped in later by replacing the placeholders in\n`build_model`.\n\"\"\"\nfrom __future__ import annotations\n\nimport math\nfrom typing import Dict\n\nimport torch\nimport torch.nn as nn\n\n\nclass MLP(nn.Module):\n    def __init__(self, input_dim: int, output_dim: int, hidden_units: int, n_hidden: int = 2):\n        super().__init__()\n        layers = []\n        prev = input_dim\n        for _ in range(n_hidden):\n            layers.append(nn.Linear(prev, hidden_units))\n            layers.append(nn.ReLU())\n            prev = hidden_units\n        layers.append(nn.Linear(prev, output_dim))\n        self.net = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.net(x)\n\n\n# ---------------------- model factory ------------------------------------ #\n\ndef build_model(cfg: Dict, hyper_cfg: Dict, input_dim: int, output_dim: int):\n    model_type = cfg.get(\"model\", \"mlp\")\n    if model_type == \"mlp\":\n        hidden = int(hyper_cfg.get(\"hidden_units\", cfg.get(\"hidden_units\", 128)))\n        model = MLP(input_dim, output_dim, hidden)\n    elif model_type == \"MODEL_PLACEHOLDER\":  # PLACEHOLDER: replace with ResNet / Transformer / etc.\n        raise NotImplementedError(\"Model placeholder – will be replaced in derived experiment.\")\n    else:\n        raise ValueError(f\"Unknown model type '{model_type}'.\")\n    return model\n\n\n# -------------------- util: flat config to vector ------------------------- #\n\ndef flat_config_to_vector(conf: Dict[str, float], search_space: Dict[str, list]):\n    v = []\n    for name, spec in search_space.items():\n        kind = spec[0]\n        low, high = float(spec[1]), float(spec[2])\n        val = conf[name]\n        # normalise to [0,1]\n        if kind == \"log_uniform\":\n            val = (math.log10(val) - math.log10(low)) / (math.log10(high) - math.log10(low))\n        else:  # uniform or int\n            val = (val - low) / (high - low)\n        v.append(val)\n    return v\n",
    "main_py": "\"\"\"src/main.py\nCommand-line entry point that orchestrates training or smoke-testing.\nIt supports the CLI required by the task description:\n\n    uv run python -m src.main --smoke-test\n    uv run python -m src.main --full-experiment --variation \"SlopeStop-BO\"\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport datetime as dt\nimport json\nimport os\nimport shutil\nimport sys\nfrom pathlib import Path\nfrom typing import Dict\n\nimport torch\nimport yaml\n\nfrom .train import run_hpo\n\n# ----------------------------- utilities --------------------------------- #\n\n_ALLOWED_VARIATIONS = [\n    \"SlopeStop-BO\",\n    \"A0-No-EarlyStop (fixed-budget GP-EI/c)\",\n    \"A1-SlopeStop w/o Cost-Aware EI\",\n    \"BOIL\",\n    \"Hyperband\",\n]\n\n\ndef _load_cfg(smoke: bool) -> Dict:\n    cfg_file = Path(__file__).parent.parent / \"config\" / (\"smoke_test.yaml\" if smoke else \"full_experiment.yaml\")\n    with cfg_file.open() as f:\n        cfg = yaml.safe_load(f)\n    return cfg\n\n\n# ----------------------------- main -------------------------------------- #\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--smoke-test\", action=\"store_true\")\n    parser.add_argument(\"--full-experiment\", action=\"store_true\")\n    parser.add_argument(\"--variation\", type=str, default=\"SlopeStop-BO\")\n    args = parser.parse_args()\n\n    if args.smoke_test and args.full_experiment:\n        print(\"Choose exactly one of --smoke-test or --full-experiment\", file=sys.stderr)\n        sys.exit(1)\n\n    if not args.smoke_test and not args.full_experiment:\n        print(\"Please specify --smoke-test or --full-experiment.\", file=sys.stderr)\n        sys.exit(1)\n\n    if args.variation not in _ALLOWED_VARIATIONS:\n        print(f\"Variation '{args.variation}' not recognised. Allowed: {_ALLOWED_VARIATIONS}\")\n        sys.exit(1)\n\n    cfg = _load_cfg(smoke=args.smoke_test)\n    out_root = Path(cfg.get(\"output_dir\", \"./results\")).expanduser()\n    ts = dt.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n    run_dir = out_root / f\"{ts}_{args.variation}\"\n    run_dir.mkdir(parents=True, exist_ok=True)\n\n    # Dump config for record-keeping\n    with (run_dir / \"config_used.yaml\").open(\"w\") as f:\n        yaml.safe_dump(cfg, f)\n\n    # ---------- experiment description stdout --------------------------- #\n    print(\"=\" * 79)\n    print(\"Experiment Description\")\n    mode = \"SMOKE TEST\" if args.smoke_test else \"FULL EXPERIMENT\"\n    print(f\"Mode           : {mode}\")\n    print(f\"Variation      : {args.variation}\")\n    print(f\"Output dir     : {run_dir}\")\n    print(f\"Search space   : {json.dumps(cfg['search_space'], indent=2)}\")\n    print(\"=\" * 79)\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    run_hpo(cfg, args.variation, device, run_dir)\n\n    print(\"\\nExperiment finished.  Results written to:\", run_dir)\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "pyproject_toml": "[build-system]\nrequires = [\"setuptools\", \"wheel\"]\n\n[project]\nname = \"common-core-foundation\"\nversion = \"0.1.0\"\nrequires-python = \">=3.9\"\ndependencies = [\n    \"torch>=1.13.0\",\n    \"tensorboard>=2.10\",\n    \"matplotlib>=3.7\",\n    \"seaborn>=0.12\",\n    \"numpy\",\n    \"pyyaml\",\n]\n",
    "smoke_test_yaml": "# config/smoke_test.yaml\n# Minimal configuration for the mandatory smoke test execution.\n\n# Dataset settings\n dataset: synthetic\n synthetic_samples: 256\n synthetic_dim: 8\n\n# Model\n model: mlp\n hidden_units: 64\n criterion: mse\n\n# Optimisation / training\n batch_size: 32\n max_epochs: 8\n mixed_precision: false\n\n# SlopeStop parameters (global, same for all variations)\n slope_W: 3\n slope_eps: 1e-3\n slope_gamma: 0.01\n\n# HPO\n search_space:\n   learning_rate: [log_uniform, 0.0001, 0.01]\n   hidden_units:  [int, 32, 128]\n max_trials: 3\n init_random: 2\n\n# Hyperband parameter\n hb_eta: 3\n\n# Misc\n output_dir: ./results\n",
    "full_experiment_yaml": "# config/full_experiment.yaml\n# Baseline configuration for the complete experimental campaign.  Derived\n# experiments will expand / override these settings as needed.\n\ndataset: DATASET_PLACEHOLDER   # PLACEHOLDER: Will be replaced with real dataset name\n\n# Model (may be overridden)\nmodel: MODEL_PLACEHOLDER       # PLACEHOLDER: real architecture\ncriterion: mse\n\n# Training params\nbatch_size: 64\nmax_epochs: 50\nmixed_precision: true\n\n# SlopeStop parameters (global)\nslope_W: 3\nslope_eps: 0.001\nslope_gamma: 0.01\n\n# HPO search space (example – will be replaced)\nsearch_space:\n  learning_rate: [log_uniform, 0.00005, 0.05]\n  hidden_units:  [int, 64, 512]\nmax_trials: 50\ninit_random: 5\n\n# Hyperband\nhb_eta: 3\n\n# Misc\noutput_dir: ./results\nseed: 42\n\n# NOTE: Derived experiment YAMLs must fill in all PLACEHOLDER entries and may\n# introduce additional hyper-parameters or dataset-specific settings.\n\n# End of file\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"
}
