
Input:
You are a cutting-edge AI researcher specializing experiments from a common base foundation.

**Previous Step (Completed)**: Common base logic and evaluation framework have been generated with placeholders
**Current Task**: Replace placeholders with specific datasets, models, and experimental configurations

Your task is to take the common base foundation code and derive specific experimental variations by replacing placeholders with actual datasets, models, and configurations specified in the experimental design.

# Instructions: Experiment Specialization

## Core Task
- **PLACEHOLDER REPLACEMENT**: Replace all placeholders in the common base code with actual datasets, models, and configurations
- **SPECIALIZATION**: Adapt the generic framework to work with specific experimental requirements
- **REAL IMPLEMENTATIONS**: Convert all placeholder logic to working implementations using actual Hugging Face resources
- **CONFIGURATION COMPLETION**: Fill in specific parameters, hyperparameters, and experimental settings

## Specialization Requirements
- Implement all run_variations (e.g., baseline, proposed, ablations) in `src/model.py`
- Replace dataset placeholders with actual Hugging Face dataset loading and preprocessing
- Replace model placeholders with specific model architectures for each variation
- Fill in exact experimental parameters based on experimental design
- Ensure `src/main.py` routes `--variation` parameter to the correct model implementation
- Complete configuration files with actual experimental values from External Resources
- Ensure all external resources specified in the experimental design are properly integrated

## Key Replacement Areas
1. **Dataset Loading**: Replace `DATASET_PLACEHOLDER` with specific Hugging Face dataset code
2. **Model Architecture**: Replace `MODEL_PLACEHOLDER` with actual model definitions
3. **Configuration Values**: Replace `SPECIFIC_CONFIG_PLACEHOLDER` with real experimental parameters
4. **Preprocessing**: Adapt generic preprocessing to dataset-specific requirements
5. **Evaluation**: Customize evaluation metrics for specific datasets/tasks

## Complete Output Policy
- If a script/file has ANY changes: Output the COMPLETE, FULL script/file content
- If a script/file has NO changes needed: Output `[UNCHANGED]` placeholder only
- NEVER truncate or abbreviate changed content


# Experimental Environment
NVIDIA Tesla T4
VRAM：16 GB
RAM：500 MB

# Current Research Method
{
    "Open Problems": "1. Most BO‐based HPO algorithms either (a) train a configuration to a fixed budget (Hyperband family) or (b) embed the full learning curve in an expensive joint surrogate (e.g. BOIL, DyHPO).  Both strategies waste budget on clearly converged runs.\n2. A very cheap, task–agnostic early–stopping rule that can be fed back into a BO loop without re–designing the surrogate is still missing.",
    "Methods": "SlopeStop-BO (Slope-informed Early-Stopping Bayesian Optimisation)\n1. Online slope test  ▸  During training we keep a W–step moving window of the validation score v_t.  At step t we compute the window slope s_t = (v_t – v_{t-W})/W.\n2. Stopping rule  ▸  If s_t < ϵ·exp(–γ·t) the run is considered saturated and training is terminated immediately (two scalars ϵ,γ are global hyper-parameters chosen once per task; default ϵ=10⁻³, γ=0.01).\n3. Fast curve compression  ▸  At termination we store a two–dimensional summary per configuration x:  (μ=mean(v_{t-W: t}),  τ=t_stop / T_max) instead of the whole curve.  This removes the need for a product kernel over (x,t).\n4. Lightweight surrogate  ▸  A standard GP with ARD RBF kernel is fitted on the augmented input [x ; τ] → μ. GP hyper-parameters are updated every 2·D observations by ML; no expensive joint optimisation over logistic parameters as in BOIL.\n5. Acquisition  ▸  Expected Improvement divided by estimated cost c(x)=τ·T_max.  The next configuration is chosen with EI/c as usual.\n6. Budget bookkeeping  ▸  Because many runs stop early, the total consumed epochs fall 30-50 % below a fixed-budget scheme.",
    "Experimental Setup": "Datasets:  • LCBench (35 tabular tasks, 50 epoch learning curves)  • NAS-Bench-201 (3 image tasks, 200 epoch curves).\nBaselines: Hyperband, BOHB, DyHPO, BOIL.\nProtocol: 10 independent seeds, total wall-clock budget identical to BOIL (GPU: RTX 3090).\nHyper-parameters W=3, ϵ=1e-3, γ=0.01 fixed for all tasks.",
    "Experimental Code": "# pseudo-python\nfor conf in pool:\n    t, buf = 0, []\n    while t < T_max:\n        loss = train_one_epoch(conf)\n        buf.append(loss); t += 1\n        if len(buf) > W:\n            slope = (buf[-1]-buf[-W-1])/W\n            if slope < eps*np.exp(-gamma*t):\n                break                # ← early stop\n    mu = np.mean(buf[-W:]); tau = t / T_max\n    D.append(([conf, tau], mu, cost=t))\n    if len(D)% (2*dim)==0: refit_GP(D)\n    x_next = argmax_EI_over_cost(GP)",
    "Expected Result": "Compared with BOIL:  • ~40 % less total training epochs  • identical median simple-regret after budget normalisation.\nCompared with Hyperband/BOHB:  • ~60 % lower final regret for the same wall time.",
    "Expected Conclusion": "A purely slope-based, task-agnostic early-stopping rule plus a standard GP surrogate is enough to harvest most of the budget savings that more complex learning-curve models aim for, while keeping the optimisation loop simple and cheap.  SlopeStop-BO is therefore a practical drop-in replacement for fixed-budget BO in large-scale HPO campaigns."
}

# Experimental Design
- Strategy: Overall Experimental Strategy for Validating SlopeStop-BO

0. Guiding Principle
Evaluate whether a cheap, task–agnostic slope-based early-stopping rule coupled with a lightweight GP surrogate (a) attains the same or better optimisation quality than existing HPO methods and (b) does so with demonstrably lower computational cost, higher robustness and broader applicability.

1. Core Properties to Validate
1.1 Optimisation quality: final and anytime simple-regret / best-seen validation error.
1.2 Computational efficiency: (i) consumed training epochs, (ii) wall-clock time, (iii) GPU memory footprint.
1.3 Cost–performance trade-off: Pareto frontier of regret vs cost.
1.4 Robustness & stability: variance across random seeds and across heterogeneous tasks.
1.5 Generalisation: behaviour on unseen domains (vision vs tabular, small vs large models, shallow vs deep learning curves).
1.6 Method components: contribution of each design choice (slope rule, curve compression, EI/c acquisition).

2. Required Comparisons
2.1 External baselines: Hyperband, BOHB, DyHPO, BOIL, vanilla fixed-budget BO.
2.2 Internal ablations:
    A0  No early stop (fixed budget) + same GP + EI/c.
    A1  SlopeStop w/o cost-aware acquisition.
    A2  SlopeStop with full-curve GP (replace compression).
    A3  Vary (W, ε, γ) to test sensitivity.
2.3 Oracle reference: best possible performance given full curves (post-hoc) to bound achievable regret.

3. Experimental Angles / Evidence Streams
3.1 Quantitative benchmarks
    • Curves of mean/median regret vs wall-time and vs epochs.
    • Area-Under-Curve (AUC) of regret over time (lower is better).
    • Number of early terminations & average stopping epoch.
    • Paired significance tests (Wilcoxon signed-rank, α = 0.05) per task.
3.2 Efficiency analysis
    • GPU runtime and VRAM peak via nvidia-smi logs.
    • CPU RAM usage via psutil (critical with 500 MB cap).
    • Scalability plots: cost ∝ dataset size & T_max.
3.3 Robustness analysis
    • Standard deviation across 10 seeds.
    • Leave-one-domain-out generalisation (train hyper-params on tabular, test on vision and vice-versa).
    • Stress test on noisy tasks by injecting synthetic noise.
3.4 Qualitative inspection
    • Example learning curves showing early-stop points vs final convergence.
    • 2-D GP posterior slices on [x; τ] to illustrate surrogate smoothness.

4. Experimental Protocol Template (applies to every dataset)
P1 Budget normalisation: all methods receive identical total GPU-seconds. Budget is chosen so the largest baseline (BOIL) fits on a single NVIDIA T4 with 16 GB.
P2 10 independent random seeds.
P3 Fixed common preprocessing & data loaders to reduce variance.
P4 Unified logging format (wandb/CSV) capturing per-epoch loss, time, memory.
P5 Post-processing script computes all metrics and statistical tests in one pass.

5. Success / Validation Criteria
C1 Performance: SlopeStop-BO attains equal or lower median final regret than best baseline on ≥70 % of tasks and is not significantly worse on any task.
C2 Efficiency: achieves ≥30 % reduction in total epochs and ≥20 % wall-time savings versus BOIL while staying within 16 GB VRAM and 500 MB system RAM.
C3 Robustness: coefficient of variation (CV) across seeds ≤ baseline CV for at least 80 % of tasks.
C4 Ablations: full model outperforms every ablation in both regret and AUC on aggregate benchmarks.
C5 Generalisation: hyper-parameters (W, ε, γ) tuned on half of the tasks transfer without retuning to the remaining tasks with <5 % relative regret degradation.

6. Practical Considerations for the T4 / 16 GB / 500 MB Environment
• Use mixed-precision (FP16) training where applicable.
• Stream dataset batches from disk to respect 500 MB RAM limit; keep only last W+1 validation losses in memory.
• Store compressed curve summaries (μ, τ) to minimise RAM.
• Run methods serially rather than in parallel to avoid memory contention; accumulate wall-time budgets accordingly.

7. Reporting
All results presented as
    (a) Task-level tables of final regret, epochs, wall-time
    (b) Aggregate violin/box plots
    (c) Pareto scatter plots (regret vs cost)
    (d) Ablation bar charts
    (e) Example qualitative figures.

This comprehensive, multi-angle strategy ensures that the proposed SlopeStop-BO is assessed not only for raw performance but also for efficiency, robustness, and practicality under stringent hardware constraints, thereby providing compelling evidence for its effectiveness as a drop-in replacement for existing HPO methods.

# Current Experiment (to generate code for)
- Experiment ID: exp-1
- Description: Objective / Hypothesis:
Validate that SlopeStop-BO achieves equal or lower simple-regret than strong Bayesian and bandit baselines while consuming significantly fewer training epochs and wall-clock seconds on both tabular and vision HPO benchmarks.

Models under HPO:
• FeedForwardNet-LC (3-layer ReLU MLP, ≈20 k params) on LCBench tasks.
• NAS-Bench-201 search-cell CNN (≈2 M params) on CIFAR-10, CIFAR-100, ImageNet-16-120 tasks.
(Both architectures are part of the original benchmarks and provided as reference implementations.)

Datasets:
• LCBench (35 OpenML tabular datasets, max 50 epochs).
• NAS-Bench-201 (3 image datasets, max 200 epochs).
All datasets are used as-is; features are z-normalised (tabular) or per-channel mean-std normalised (vision).

Pre-processing:
• Tabular: standardisation per feature, categorical one-hot (already stored in LCBench).  Batches streamed from disk.
• Vision: random crop + horizontal flip during training, centre crop at val/test.

Data Splitting:
• Each dataset keeps its official train/val/test split from the benchmark.  The HPO objective is validation metric; test split is revealed only for the final model selected by best validation configuration.

Budget & Repetitions:
• 10 independent seeds per dataset × method.
• Global GPU-second budget per seed equals BOIL runtime on the slowest task (pre-measured ≈8 k s for NAS-Bench-201 ImageNet-16-120 on T4).
• Early stopping inside SlopeStop-BO counts against the same budget (unused time can be spent on more BO iterations).

Evaluation Metrics:
Primary – Final simple-regret (val error of best configuration – best known).
Secondary – AUC of regret vs wall-time, #epochs consumed, wall-clock time, peak VRAM, CPU RAM, FLOPs*, energy (via NVIDIA SMI power logs).
*FLOPs computed with fvcore.get_flops() once per architecture; total = FLOPs×epochs.

Statistical Tests:
Wilcoxon signed-rank on task-level paired regrets (α=0.05, Holm correction).

Hyper-parameter Sensitivity (inside variation “SlopeStop-BO” only):
Grid W∈{3,5,7}, ε∈{1e-3,3e-3}, γ∈{0.005,0.01,0.02}; pick default by median validation regret on 50 % of tasks, freeze for rest.

Robustness Checks:
1. Noise injection – add N(0,0.02) noise to validation loss online for 20 % of runs; measure early-stop frequency & regret delta.
2. Domain transfer – tune (W,ε,γ) on tabular, evaluate on vision and vice-versa.
3. Seed variance – coefficient of variation across 10 seeds.

Computational-efficiency Analysis:
• Log epoch-level timestamps; compute avg seconds/epoch and cumulative wall-time per BO iteration.
• Memory – sample nvidia-smi every 2 s; store max.

Comparisons & Footnotes:
(1) A0 shares SlopeStop GP surrogate & EI/c but disables slope rule (fixed 50/200 epochs).  (2) A1 keeps slope rule yet replaces EI/c with plain EI to isolate acquisition benefit.  (3) BOIL implementation from original authors, re-tuned GP hyper-prior to avoid out-of-memory on T4†.  (4) Hyperband implementation from hpbandster, identical training code.
†Changed inducing-point count from 128→64.

Code Skeleton (PyTorch, BoTorch):
```
for seed in seeds:
    torch.manual_seed(seed)
    bo = init_optimizer(method)  # picks GP, acquisition, early-stopper
    while budget_left():
        x = bo.suggest()
        mu, tau, cost = train_cfg(x, method)  # contains slope early-stop if enabled
        bo.observe(x, mu, cost)
```
train_cfg implements mixed-precision, keeps only last W+1 val losses, and streams batches to stay <500 MB system RAM.

Success Criteria:
C1–C5 from strategy; pass/fail recorded per method.

Expected Findings:
• SlopeStop-BO reaches ≤ baseline median regret on ≥70 % tasks, with ≈40 % fewer epochs and ≥20 % wall-time reduction vs BOIL.
• A0 shows similar regret but 40-50 % higher cost ⇒ confirms value of slope rule.
• A1 loses ≈8 % regret, same cost ⇒ confirms EI/c benefit.
• Hyperband fastest but worst regret; BOIL best regret but highest cost → SlopeStop Pareto-optimal.

- Run Variations: ['SlopeStop-BO', 'A0-No-EarlyStop (fixed-budget GP-EI/c)', 'A1-SlopeStop w/o Cost-Aware EI', 'BOIL', 'Hyperband']

# Base Code
{'train_py': '"""src/train.py\nCore training and HPO‐loop logic shared by all experimental variations.\nThe only dataset/model specific code lives in preprocess.py and model.py, where\nPLACEHOLDER comments mark the pieces that will be swapped out in the next\npipeline step.\n"""\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport shutil\nimport time\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Any, Dict, Tuple\n\nimport numpy as np\nimport torch\nfrom torch.utils.tensorboard import SummaryWriter\nimport yaml\n\nfrom .preprocess import get_dataloaders\nfrom .model import (\n    SlopeStopBOAlgorithm,\n    A0NoEarlyStopAlgorithm,\n    A1SlopeStopNoCostAwareAlgorithm,\n    HyperbandAlgorithm,\n    BOILAlgorithm,\n)\n\nALG_REGISTRY = {\n    "SlopeStop-BO": SlopeStopBOAlgorithm,\n    "A0-No-EarlyStop (fixed-budget GP-EI/c)": A0NoEarlyStopAlgorithm,\n    "A1-SlopeStop w/o Cost-Aware EI": A1SlopeStopNoCostAwareAlgorithm,\n    "Hyperband": HyperbandAlgorithm,\n    "BOIL": BOILAlgorithm,\n}\n\n# -----------------------------------------------------------------------------\n# Utility helpers\n# -----------------------------------------------------------------------------\n\ndef sanitize_variation(name: str) -> str:\n    """Make a file-system friendly version of *name* and guarantee uniqueness."""\n    import re\n\n    sanitized = re.sub(r"[^A-Za-z0-9_.\\-]", "_", name)\n    sanitized = re.sub(r"_+", "_", sanitized).strip("_")\n    return sanitized\n\n\ndef load_yaml(path: str | Path) -> Dict[str, Any]:\n    with open(path, "r", encoding="utf-8") as f:\n        return yaml.safe_load(f)\n\n\n# -----------------------------------------------------------------------------\n# Training entry point\n# -----------------------------------------------------------------------------\n\ndef train_variation(variation: str, cfg: Dict[str, Any], smoke: bool = False) -> None:\n    if variation not in ALG_REGISTRY:\n        raise ValueError(f"Unknown variation \'{variation}\'. Allowed: {list(ALG_REGISTRY)}")\n\n    # ------------------------------------------------------------------\n    # Directory layout & logging\n    # ------------------------------------------------------------------\n    root_out = Path(cfg["output"]["root_dir"])\n    root_out.mkdir(parents=True, exist_ok=True)\n\n    time_stamp = datetime.now().strftime("%Y%m%d_%H%M%S")\n    var_dir_name = f"{time_stamp}_{sanitize_variation(variation)}"\n    exp_dir = root_out / var_dir_name\n    exp_dir.mkdir(parents=True, exist_ok=False)\n\n    # Remember original variation string so that evaluate.py can de-sanitize\n    with open(exp_dir / "meta.yaml", "w", encoding="utf-8") as f:\n        yaml.safe_dump({"variation": variation, "smoke": smoke}, f)\n\n    writer = SummaryWriter(log_dir=str(exp_dir / "tensorboard"))\n\n    # ------------------------------------------------------------------\n    # Data pipeline\n    # ------------------------------------------------------------------\n    train_loader, val_loader, input_dim, output_dim = get_dataloaders(cfg, smoke)\n\n    # ------------------------------------------------------------------\n    # HPO algorithm logic\n    # ------------------------------------------------------------------\n    AlgorithmCls = ALG_REGISTRY[variation]\n    algo = AlgorithmCls(\n        input_dim=input_dim,\n        output_dim=output_dim,\n        train_loader=train_loader,\n        val_loader=val_loader,\n        cfg=cfg,\n        log_writer=writer,\n    )\n\n    algo.optimize()\n\n    # ------------------------------------------------------------------\n    # Save the best configuration and close the writer\n    # ------------------------------------------------------------------\n    with open(exp_dir / "best_config.json", "w", encoding="utf-8") as f:\n        json.dump(algo.best_config, f, indent=2)\n\n    writer.flush()\n    writer.close()\n\n    # For CI / smoke-test convenience we print a small summary to stdout\n    print("\\n========== EXPERIMENT SUMMARY ==========")\n    print(f"Variation                 : {variation}")\n    print(f"Best validation metric    : {algo.best_metric:.4f}")\n    print(f"Consumed epochs (overall) : {algo.total_consumed_epochs}")\n    print(f"Results directory         : {exp_dir}")\n    print("=======================================\\n")\n\n\n# -----------------------------------------------------------------------------\n# CLI wrapper\n# -----------------------------------------------------------------------------\n\ndef main() -> None:\n    parser = argparse.ArgumentParser(description="COMMON CORE training driver")\n    group = parser.add_mutually_exclusive_group(required=True)\n    group.add_argument("--smoke-test", action="store_true", help="quick 30-second run")\n    group.add_argument("--full-experiment", action="store_true", help="run full experiment")\n    parser.add_argument("--variation", type=str, default="SlopeStop-BO", help="variation name")\n    args = parser.parse_args()\n\n    cfg_path = Path("config") / ("smoke_test.yaml" if args.smoke_test else "full_experiment.yaml")\n    cfg = load_yaml(cfg_path)\n\n    # Ensure output root exists even if config forgets it\n    cfg.setdefault("output", {"root_dir": "results"})\n\n    # Re-seed everything for reproducibility\n    seed = cfg.get("seed", 0)\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n\n    train_variation(args.variation, cfg, smoke=args.smoke_test)\n\n\nif __name__ == "__main__":\n    main()\n', 'evaluate_py': '"""src/evaluate.py\nAggregate TensorBoard logs of *all* variations stored under a given directory,\ncompute identical evaluation metrics and create comparison figures.\n"""\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport re\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport yaml\nfrom tensorboard.backend.event_processing import event_accumulator as ea\n\nFIG_KW = dict(bbox_inches="tight", dpi=300, format="pdf")\n\n# -----------------------------------------------------------------------------\n# TensorBoard utility\n# -----------------------------------------------------------------------------\n\n\ndef _accumulate_scalars(tb_dir: Path, tag: str) -> Tuple[np.ndarray, np.ndarray]:\n    ev_files = list(tb_dir.glob("*tfevents.*"))\n    if not ev_files:\n        raise FileNotFoundError(f"No TensorBoard event files found in {tb_dir}")\n\n    ev = ea.EventAccumulator(str(ev_files[0]), size_guidance={ea.SCALARS: 0})\n    ev.Reload()\n    if tag not in ev.Tags()["scalars"]:\n        raise KeyError(f"Scalar tag \'{tag}\' not found in {tb_dir}")\n\n    wall_times, steps, vals = zip(*ev.Scalars(tag))\n    return np.array(steps), np.array(vals)\n\n\n# -----------------------------------------------------------------------------\n# Core evaluation logic\n# -----------------------------------------------------------------------------\n\n\ndef _collect_runs(results_root: Path) -> Dict[str, List[Path]]:\n    """Map ORIGINAL variation names -> list of run directories."""\n    mapping: Dict[str, List[Path]] = {}\n    for run_dir in sorted(results_root.iterdir()):\n        if not run_dir.is_dir():\n            continue\n        meta_file = run_dir / "meta.yaml"\n        if not meta_file.exists():\n            continue\n        with open(meta_file, "r", encoding="utf-8") as f:\n            meta = yaml.safe_load(f)\n        var_name = meta["variation"]\n        mapping.setdefault(var_name, []).append(run_dir)\n    if not mapping:\n        raise RuntimeError(f"No experiment sub-directories with meta.yaml found in {results_root}")\n    return mapping\n\n\ndef _aggregate_metric(mapping: Dict[str, List[Path]], tag: str) -> pd.DataFrame:\n    rows = []\n    for var, dirs in mapping.items():\n        for d in dirs:\n            tb_dir = d / "tensorboard"\n            steps, vals = _accumulate_scalars(tb_dir, tag)\n            best_val = float(vals.min()) if "loss" in tag else float(vals.max())\n            rows.append({"variation": var, "run_dir": str(d), "best": best_val})\n    return pd.DataFrame(rows)\n\n\n# -----------------------------------------------------------------------------\n# Plot helpers\n# -----------------------------------------------------------------------------\n\n\ndef _plot_bar(df: pd.DataFrame, metric: str, ylabel: str, fname: Path) -> None:\n    plt.figure(figsize=(6, 4))\n    sns.barplot(data=df, x="variation", y="best", ci="sd")\n    plt.ylabel(ylabel)\n    plt.xlabel("Variation")\n    plt.xticks(rotation=30, ha="right")\n    for idx, row in df.groupby("variation").mean().reset_index().iterrows():\n        plt.text(idx, row["best"], f"{row[\'best\']:.3f}", ha="center", va="bottom", fontsize=8)\n    plt.tight_layout()\n    plt.savefig(fname, **FIG_KW)\n    plt.close()\n\n\n# -----------------------------------------------------------------------------\n# Main CLI\n# -----------------------------------------------------------------------------\n\ndef main() -> None:\n    parser = argparse.ArgumentParser(description="COMMON CORE evaluator")\n    parser.add_argument("--results-dir", type=str, required=True, help="root dir with experiment runs")\n    args = parser.parse_args()\n\n    results_root = Path(args.results_dir).expanduser().resolve()\n    mapping = _collect_runs(results_root)\n\n    # ------------------------------------------------------------------\n    # Collect & compare validation accuracy\n    # ------------------------------------------------------------------\n    acc_df = _aggregate_metric(mapping, "val/accuracy")\n    loss_df = _aggregate_metric(mapping, "train/loss")\n\n    print("=== Validation accuracy (best per run) ===")\n    print(acc_df.groupby("variation")["best"].describe())\n    print("\\n=== Training loss (best per run) ===")\n    print(loss_df.groupby("variation")["best"].describe())\n\n    # ------------------------------------------------------------------\n    # Produce publication-ready figures\n    # ------------------------------------------------------------------\n    fig_dir = results_root / "figures"\n    fig_dir.mkdir(exist_ok=True)\n\n    _plot_bar(acc_df, "val/accuracy", "Best validation accuracy", fig_dir / "accuracy.pdf")\n    _plot_bar(loss_df, "train/loss", "Lowest training loss", fig_dir / "training_loss.pdf")\n\n    print("Generated figures:")\n    for f in fig_dir.glob("*.pdf"):\n        print(" -", f.name)\n\n\nif __name__ == "__main__":\n    main()\n', 'preprocess_py': '"""src/preprocess.py\nShared data-loading and preprocessing utilities.\nDataset-specific logic is isolated behind clearly marked placeholders so that\nfuture steps can simply swap-in real datasets without touching the rest of the\npipeline.\n"""\nfrom __future__ import annotations\n\nfrom typing import Tuple\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader, TensorDataset, random_split\n\n\n# -----------------------------------------------------------------------------\n# Synthetic fallback dataset (used for smoke test & CI)\n# -----------------------------------------------------------------------------\n\ndef _synthetic_classification(n_samples: int, n_features: int, n_classes: int, seed: int = 0):\n    rng = np.random.default_rng(seed)\n    X = rng.standard_normal(size=(n_samples, n_features)).astype(np.float32)\n    W = rng.standard_normal(size=(n_features, n_classes))\n    logits = X @ W\n    y = logits.argmax(axis=1).astype(np.int64)\n    return X, y\n\n\n# -----------------------------------------------------------------------------\n# Public API\n# -----------------------------------------------------------------------------\n\ndef get_dataloaders(cfg, smoke: bool = False):\n    """Return train/val DataLoaders and basic metadata.\n\n    PLACEHOLDER: Real datasets (e.g. LCBench, NAS-Bench-201) must be wired in\n    here.  Replace the synthetic stubs below with proper dataset loaders.\n    """\n\n    ds_cfg = cfg["dataset"]\n    name = ds_cfg.get("name", "synthetic_classification")\n    batch_size = ds_cfg.get("batch_size", 64)\n    seed = cfg.get("seed", 0)\n\n    if name == "synthetic_classification":\n        n = 1024 if smoke else 4096\n        n_features = ds_cfg.get("n_features", 20)\n        n_classes = ds_cfg.get("n_classes", 3)\n        X, y = _synthetic_classification(n, n_features, n_classes, seed)\n        tensor_x = torch.tensor(X)\n        tensor_y = torch.tensor(y)\n        dataset = TensorDataset(tensor_x, tensor_y)\n\n        # 80/20 split\n        val_size = int(0.2 * len(dataset))\n        train_size = len(dataset) - val_size\n        train_set, val_set = random_split(dataset, [train_size, val_size], generator=torch.Generator().manual_seed(seed))\n\n        train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n        val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n        return train_loader, val_loader, n_features, n_classes\n\n    # ------------------------------------------------------------------\n    # PLACEHOLDER hooks for real datasets\n    # ------------------------------------------------------------------\n    if name == "DATASET_PLACEHOLDER":\n        # PLACEHOLDER: Will be replaced with specific dataset loading logic\n        raise NotImplementedError("Dataset \'DATASET_PLACEHOLDER\' not yet implemented.")\n\n    raise ValueError(f"Unknown dataset \'{name}\'.")\n', 'model_py': '"""src/model.py\nAll model architectures and HPO algorithms live here so that *train.py* stays\nclean and generic.  The surrogate & acquisition sub-modules are implemented in\npure PyTorch + GPyTorch to satisfy the *PyTorch-only* requirement.\n"""\nfrom __future__ import annotations\n\nimport itertools\nimport math\nimport time\nfrom typing import Any, Dict, List, Tuple\n\nimport gpytorch\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.tensorboard import SummaryWriter\n\n# -----------------------------------------------------------------------------\n# Task model (simple MLP – can be replaced later)\n# -----------------------------------------------------------------------------\n\n\nclass MLP(nn.Module):\n    def __init__(self, input_dim: int, output_dim: int, hidden_dim: int = 64, depth: int = 2):\n        super().__init__()\n        layers: List[nn.Module] = []\n        dims = [input_dim] + [hidden_dim] * depth + [output_dim]\n        for d_in, d_out in zip(dims[:-1], dims[1:]):\n            layers.append(nn.Linear(d_in, d_out))\n            if d_out != output_dim:\n                layers.append(nn.ReLU())\n        self.net = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.net(x)\n\n\n# -----------------------------------------------------------------------------\n# Early stopping – slope-based rule (core contribution)\n# -----------------------------------------------------------------------------\n\n\ndef should_stop_slope(buffer: List[float], step: int, W: int, eps: float, gamma: float) -> bool:\n    if len(buffer) <= W:\n        return False\n    slope = (buffer[-1] - buffer[-W - 1]) / W\n    return slope < eps * math.exp(-gamma * step)\n\n\n# -----------------------------------------------------------------------------\n# Surrogate Model (GPyTorch – Exact GP)\n# -----------------------------------------------------------------------------\n\n\nclass ExactGPModel(gpytorch.models.ExactGP):\n    def __init__(self, train_x, train_y, likelihood):\n        super().__init__(train_x, train_y, likelihood)\n        self.mean_module = gpytorch.means.ConstantMean()\n        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n\n    def forward(self, x):\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n\n\nclass GaussianProcessSurrogate:\n    def __init__(self, input_dim: int):\n        self.input_dim = input_dim\n        self.train_x, self.train_y = None, None  # type: ignore\n        self.likelihood = gpytorch.likelihoods.GaussianLikelihood()\n        self.model: ExactGPModel | None = None\n\n    def update(self, X: torch.Tensor, y: torch.Tensor):\n        self.train_x = X.float()\n        self.train_y = y.float()\n        self.model = ExactGPModel(self.train_x, self.train_y, self.likelihood)\n        self.model.train()\n        self.likelihood.train()\n\n        optimizer = torch.optim.Adam(self.model.parameters(), lr=0.1)\n        mll = gpytorch.mlls.ExactMarginalLogLikelihood(self.likelihood, self.model)\n        for _ in range(25):\n            optimizer.zero_grad()\n            output = self.model(self.train_x)\n            loss = -mll(output, self.train_y)\n            loss.backward()\n            optimizer.step()\n\n        self.model.eval()\n        self.likelihood.eval()\n\n    def predict(self, X: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        if self.model is None:\n            raise RuntimeError("Surrogate has not been trained yet.")\n        with torch.no_grad(), gpytorch.settings.fast_pred_var():\n            preds = self.likelihood(self.model(X.float()))\n            mean = preds.mean\n            var = preds.variance.clamp_min(1e-9)\n        return mean, var\n\n\n# -----------------------------------------------------------------------------\n# Acquisition functions\n# -----------------------------------------------------------------------------\n\n\ndef expected_improvement(mu: torch.Tensor, sigma2: torch.Tensor, best: float, minimize: bool = True):\n    std = sigma2.sqrt()\n    if minimize:\n        improvement = best - mu\n    else:\n        improvement = mu - best\n    Z = improvement / std.clamp_min(1e-9)\n    ei = improvement * torch.distributions.Normal(0, 1).cdf(Z) + std * torch.distributions.Normal(0, 1).log_prob(Z).exp()\n    return ei\n\n\n# -----------------------------------------------------------------------------\n# HPO algorithm base class\n# -----------------------------------------------------------------------------\n\n\nclass HPOAlgorithmBase:\n    def __init__(\n        self,\n        *,\n        input_dim: int,\n        output_dim: int,\n        train_loader,\n        val_loader,\n        cfg: Dict[str, Any],\n        log_writer: SummaryWriter,\n    ) -> None:\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n        self.cfg = cfg\n        self.writer = log_writer\n        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")\n\n        self.search_space = cfg["search_space"]\n        self.max_epochs = cfg.get("epochs", 50)\n        self.seed = cfg.get("seed", 0)\n\n        self.total_consumed_epochs = 0\n        self.best_metric = float("inf")\n        self.best_config: Dict[str, Any] = {}\n\n        torch.manual_seed(self.seed)\n        np.random.seed(self.seed)\n\n    # ------------------------------------------------------------------\n    # API subclasses must implement\n    # ------------------------------------------------------------------\n    def optimize(self):\n        raise NotImplementedError\n\n    # ------------------------------------------------------------------\n    # Helper – train one config (with/without early stop)\n    # ------------------------------------------------------------------\n    def _train_model(self, params: Dict[str, Any], early_stop: bool) -> Tuple[float, int]:\n        model = MLP(self.input_dim, self.output_dim, hidden_dim=params["hidden_dim"]).to(self.device)\n        optimizer = torch.optim.Adam(model.parameters(), lr=params["lr"])\n        criterion = nn.CrossEntropyLoss()\n\n        val_buffer: List[float] = []\n        best_val = float("inf")\n        consumed = 0\n\n        for epoch in range(1, self.max_epochs + 1):\n            model.train()\n            for xb, yb in self.train_loader:\n                xb, yb = xb.to(self.device), yb.to(self.device)\n                optimizer.zero_grad()\n                out = model(xb)\n                loss = criterion(out, yb)\n                loss.backward()\n                optimizer.step()\n\n            # validation\n            model.eval()\n            correct = 0\n            total = 0\n            val_loss = 0.0\n            with torch.no_grad():\n                for xb, yb in self.val_loader:\n                    xb, yb = xb.to(self.device), yb.to(self.device)\n                    logits = model(xb)\n                    loss = criterion(logits, yb)\n                    val_loss += loss.item() * yb.size(0)\n                    pred = logits.argmax(dim=1)\n                    correct += (pred == yb).sum().item()\n                    total += yb.size(0)\n            val_loss /= total\n            acc = correct / total\n\n            # logging\n            step_global = self.total_consumed_epochs + epoch  # approximate global step\n            self.writer.add_scalar("train/loss", loss.item(), step_global)\n            self.writer.add_scalar("val/accuracy", acc, step_global)\n\n            # early stopping decision\n            val_buffer.append(acc)\n            if early_stop and should_stop_slope(val_buffer, epoch, W=3, eps=1e-3, gamma=0.01):\n                consumed = epoch\n                break\n        else:\n            consumed = self.max_epochs\n\n        self.total_consumed_epochs += consumed\n        # we *minimize* negative accuracy to align with EI\n        return -acc, consumed\n\n    # ------------------------------------------------------------------\n    # Helper – sample random configuration from search space\n    # ------------------------------------------------------------------\n    def _sample_random_config(self) -> Dict[str, Any]:\n        cfg = {\n            k: np.random.choice(v) if isinstance(v, (list, tuple)) else v for k, v in self.search_space.items()\n        }\n        return cfg\n\n\n# -----------------------------------------------------------------------------\n# SlopeStop-BO (main method)\n# -----------------------------------------------------------------------------\n\n\nclass SlopeStopBOAlgorithm(HPOAlgorithmBase):\n    def optimize(self):\n        n_init = self.cfg.get("n_init", 5)\n        n_iter = self.cfg.get("n_iter", 15)\n\n        X: List[List[float]] = []\n        y: List[float] = []\n        costs: List[float] = []\n        gp = GaussianProcessSurrogate(input_dim=len(self.search_space) + 1)  # +1 for tau\n\n        # --------------------------------------------------------------\n        # Initial random designs\n        # --------------------------------------------------------------\n        for _ in range(n_init):\n            params = self._sample_random_config()\n            metric, epochs = self._train_model(params, early_stop=True)\n            tau = epochs / self.max_epochs\n            X.append([*params.values(), tau])\n            y.append(metric)\n            costs.append(tau * self.max_epochs)\n            if metric < self.best_metric:\n                self.best_metric = metric\n                self.best_config = params\n\n        # --------------------------------------------------------------\n        # BO loop\n        # --------------------------------------------------------------\n        for _ in range(n_iter):\n            # 1) fit surrogate\n            X_t = torch.tensor(X)\n            y_t = torch.tensor(y)\n            gp.update(X_t, y_t)\n\n            # 2) sample candidates & compute EI/c\n            cand_params_list = [self._sample_random_config() for _ in range(100)]\n            cands = torch.tensor([[*p.values(), 1.0] for p in cand_params_list])  # assume full budget first\n            mu, var = gp.predict(cands)\n            ei = expected_improvement(mu, var, best=self.best_metric)\n            # cost aware – denominator = expected cost (full budget here simplified)\n            ei_div_cost = ei / self.max_epochs\n            best_idx = torch.argmax(ei_div_cost).item()\n            next_params = cand_params_list[best_idx]\n\n            # 3) evaluate with slope early stop\n            metric, epochs = self._train_model(next_params, early_stop=True)\n            tau = epochs / self.max_epochs\n            X.append([*next_params.values(), tau])\n            y.append(metric)\n            costs.append(tau * self.max_epochs)\n\n            if metric < self.best_metric:\n                self.best_metric = metric\n                self.best_config = next_params\n\n\n# -----------------------------------------------------------------------------\n# Ablation: A0 – no Early Stop, cost-aware EI/c\n# -----------------------------------------------------------------------------\n\n\nclass A0NoEarlyStopAlgorithm(SlopeStopBOAlgorithm):\n    def optimize(self):\n        n_init = self.cfg.get("n_init", 5)\n        n_iter = self.cfg.get("n_iter", 15)\n\n        X, y_, costs = [], [], []\n        gp = GaussianProcessSurrogate(input_dim=len(self.search_space) + 1)\n\n        # random designs\n        for _ in range(n_init):\n            p = self._sample_random_config()\n            metric, epochs = self._train_model(p, early_stop=False)  # full budget\n            tau = 1.0\n            X.append([*p.values(), tau])\n            y_.append(metric)\n            costs.append(self.max_epochs)\n            if metric < self.best_metric:\n                self.best_metric = metric\n                self.best_config = p\n\n        for _ in range(n_iter):\n            gp.update(torch.tensor(X), torch.tensor(y_))\n            cand_params = [self._sample_random_config() for _ in range(100)]\n            cands_t = torch.tensor([[*c.values(), 1.0] for c in cand_params])\n            mu, var = gp.predict(cands_t)\n            ei = expected_improvement(mu, var, best=self.best_metric)\n            ei_c = ei / self.max_epochs  # cost aware\n            best_idx = torch.argmax(ei_c).item()\n            p_star = cand_params[best_idx]\n            metric, epochs = self._train_model(p_star, early_stop=False)\n            X.append([*p_star.values(), 1.0])\n            y_.append(metric)\n            costs.append(self.max_epochs)\n            if metric < self.best_metric:\n                self.best_metric = metric\n                self.best_config = p_star\n\n\n# -----------------------------------------------------------------------------\n# Ablation: A1 – slope early stop but *no* cost-aware acquisition\n# -----------------------------------------------------------------------------\n\n\nclass A1SlopeStopNoCostAwareAlgorithm(SlopeStopBOAlgorithm):\n    def optimize(self):\n        n_init = self.cfg.get("n_init", 5)\n        n_iter = self.cfg.get("n_iter", 15)\n\n        X, y_, costs = [], [], []\n        gp = GaussianProcessSurrogate(input_dim=len(self.search_space) + 1)\n\n        for _ in range(n_init):\n            p = self._sample_random_config()\n            m, e = self._train_model(p, early_stop=True)\n            tau = e / self.max_epochs\n            X.append([*p.values(), tau])\n            y_.append(m)\n            costs.append(e)\n            if m < self.best_metric:\n                self.best_metric = m\n                self.best_config = p\n\n        for _ in range(n_iter):\n            gp.update(torch.tensor(X), torch.tensor(y_))\n            cand_params = [self._sample_random_config() for _ in range(100)]\n            cands_t = torch.tensor([[*c.values(), 1.0] for c in cand_params])\n            mu, var = gp.predict(cands_t)\n            ei = expected_improvement(mu, var, best=self.best_metric)\n            best_idx = torch.argmax(ei).item()  # ← no cost division\n            p_star = cand_params[best_idx]\n            m, e = self._train_model(p_star, early_stop=True)\n            tau = e / self.max_epochs\n            X.append([*p_star.values(), tau])\n            y_.append(m)\n            costs.append(e)\n            if m < self.best_metric:\n                self.best_metric = m\n                self.best_config = p_star\n\n\n# -----------------------------------------------------------------------------\n# Hyperband (simplified – successive halving)\n# -----------------------------------------------------------------------------\n\n\nclass HyperbandAlgorithm(HPOAlgorithmBase):\n    def optimize(self):\n        eta = 3  # down-sampling rate\n        R = self.max_epochs\n        s_max = int(math.log(R, eta))\n        B = (s_max + 1) * R\n\n        def run_then_return_val_loss(params, max_epochs):\n            metric, _ = self._train_model(params, early_stop=False)  # always full; but limited epochs\n            return metric\n\n        for s in reversed(range(s_max + 1)):\n            n = int(math.ceil(B / R / (s + 1) * eta ** s))\n            r = int(R * eta ** (-s))\n\n            # initial configs\n            T = [self._sample_random_config() for _ in range(n)]\n            for i in range(s + 1):\n                n_i = n * eta ** (-i)\n                r_i = r * eta ** i\n                metrics = [run_then_return_val_loss(t, r_i) for t in T]\n                idx_sorted = np.argsort(metrics)\n                n_keep = int(n_i / eta)\n                if n_keep <= 0:\n                    break\n                T = [T[i] for i in idx_sorted[:n_keep]]\n                if metrics[idx_sorted[0]] < self.best_metric:\n                    self.best_metric = metrics[idx_sorted[0]]\n                    self.best_config = T[0]\n\n\n# -----------------------------------------------------------------------------\n# BOIL (simplified – imitates full curve surrogate)\n# -----------------------------------------------------------------------------\n\n\nclass BOILAlgorithm(HPOAlgorithmBase):\n    def optimize(self):\n        # For brevity we reuse slope-stopping but fit GP on (x, t).\n        n_init = self.cfg.get("n_init", 5)\n        n_iter = self.cfg.get("n_iter", 15)\n        X, y_ = [], []\n        gp = GaussianProcessSurrogate(input_dim=len(self.search_space) + 1)  # +1 time dim\n\n        for _ in range(n_init):\n            p = self._sample_random_config()\n            m, e = self._train_model(p, early_stop=False)\n            # store full budget info (t=1)\n            X.append([*p.values(), 1.0])\n            y_.append(m)\n            if m < self.best_metric:\n                self.best_metric = m\n                self.best_config = p\n\n        for _ in range(n_iter):\n            gp.update(torch.tensor(X), torch.tensor(y_))\n            cand_params = [self._sample_random_config() for _ in range(100)]\n            cands_t = torch.tensor([[*c.values(), 1.0] for c in cand_params])\n            mu, var = gp.predict(cands_t)\n            ei = expected_improvement(mu, var, best=self.best_metric)\n            best_idx = torch.argmax(ei).item()\n            p_star = cand_params[best_idx]\n            m, e = self._train_model(p_star, early_stop=False)\n            X.append([*p_star.values(), 1.0])\n            y_.append(m)\n            if m < self.best_metric:\n                self.best_metric = m\n                self.best_config = p_star\n', 'main_py': '# src/main.py\n"""CLI façade that dispatches to *train.py* so users only ever have to call\n`python -m src.main ...`.  Keeping the entry point slim makes CI / packaging\nsimpler and ensures a single source of truth for argument parsing.\n"""\nfrom __future__ import annotations\n\nimport importlib\n\nfrom . import train as _train_mod  # re-export for mypy clarity\n\nif __name__ == "__main__":\n    _train_mod.main()\n', 'pyproject_toml': '[build-system]\nrequires = ["setuptools>=42"]\nbuild-backend = "setuptools.build_meta"\n\n[project]\nname = "common-core-foundation"\nversion = "0.1.0"\ndescription = "Foundation code for consistent HPO experiments"\nauthors = [\n  { name="Researcher", email="research@example.com" }\n]\nrequires-python = ">=3.10"\nlicense = {text = "MIT"}\n\n[project.dependencies]\ntorch = "*"\n# GPyTorch satisfies the requirement for a pure PyTorch GP implementation\n# and is lightweight enough for the 500 MB system RAM limit.\ngpytorch = "*"\ntensorboard = "*"\nmatplotlib = "*"\nseaborn = "*"\npyyaml = "*"\nscikit-learn = "*"\nnumpy = "*"\npandas = "*"\nscipy = "*"\n', 'smoke_test_yaml': '# config/smoke_test.yaml\nseed: 0\nepochs: 5\nn_init: 2\nn_iter: 2\noutput:\n  root_dir: "results"\n\ndataset:\n  name: "synthetic_classification"\n  n_features: 10\n  n_classes: 3\n  batch_size: 32\n\nsearch_space:\n  lr: [0.01, 0.001]\n  hidden_dim: [32, 64]\n', 'full_experiment_yaml': '# config/full_experiment.yaml\n# Default configuration for a *real* experimental run.  Dataset/model specific\n# placeholders must be swapped out in the next pipeline stage.\n\nseed: 0\nepochs: 50\nn_init: 10\nn_iter: 30\noutput:\n  root_dir: "results"\n\ndataset:\n  name: "DATASET_PLACEHOLDER"  # PLACEHOLDER: replace with "lcb-bench" / "nas-bench-201" etc.\n  batch_size: 128\n\n# Search space for ML hyper-parameters. Extend or edit as required by the task.\nsearch_space:\n  lr: [0.1, 0.03, 0.01, 0.003, 0.001]\n  hidden_dim: [32, 64, 128, 256]\n\n# Any additional task-specific configs go here.\nSPECIFIC_CONFIG_PLACEHOLDER: 0\n\n# End of file\n'}

# External Resources (Use these to replace placeholders)

**HuggingFace Models (Replace MODEL_PLACEHOLDER with these):**


**HuggingFace Datasets (Replace DATASET_PLACEHOLDER with these):**




# Common Base Code (Previous Step Output to be specialized)
The common base code generated in the previous step contains the following placeholder patterns that need to be replaced:
- `DATASET_PLACEHOLDER` → Replace with specific Hugging Face dataset loading
- `MODEL_PLACEHOLDER` → Replace with specific model architecture
- `SPECIFIC_CONFIG_PLACEHOLDER` → Replace with actual experimental parameters


# Validation Feedback


**Previous Validation Failed**: 
Please address the validation issues and regenerate the affected files while keeping successful files unchanged using [UNCHANGED] markers.



Take the foundation code and create complete, specialized experiments using the External Resources specified above.
Output:
{
    "train_py": "\"\"\"src/train.py\nCore training and HPO‐loop logic shared by all experimental variations.\nThe only dataset/model specific code lives in preprocess.py and model.py, where\nPLACEHOLDER comments mark the pieces that will be swapped out in the next\npipeline step.\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport shutil\nimport time\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Any, Dict, Tuple\n\nimport numpy as np\nimport torch\nfrom torch.utils.tensorboard import SummaryWriter\nimport yaml\n\nfrom .preprocess import get_dataloaders\nfrom .model import (\n    SlopeStopBOAlgorithm,\n    A0NoEarlyStopAlgorithm,\n    A1SlopeStopNoCostAwareAlgorithm,\n    HyperbandAlgorithm,\n    BOILAlgorithm,\n)\n\nALG_REGISTRY = {\n    \"SlopeStop-BO\": SlopeStopBOAlgorithm,\n    \"A0-No-EarlyStop (fixed-budget GP-EI/c)\": A0NoEarlyStopAlgorithm,\n    \"A1-SlopeStop w/o Cost-Aware EI\": A1SlopeStopNoCostAwareAlgorithm,\n    \"Hyperband\": HyperbandAlgorithm,\n    \"BOIL\": BOILAlgorithm,\n}\n\n# -----------------------------------------------------------------------------\n# Utility helpers\n# -----------------------------------------------------------------------------\n\ndef sanitize_variation(name: str) -> str:\n    \"\"\"Make a file-system friendly version of *name* and guarantee uniqueness.\"\"\"\n    import re\n\n    sanitized = re.sub(r\"[^A-Za-z0-9_.\\-]\", \"_\", name)\n    sanitized = re.sub(r\"_+\", \"_\", sanitized).strip(\"_\")\n    return sanitized\n\n\ndef load_yaml(path: str | Path) -> Dict[str, Any]:\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        return yaml.safe_load(f)\n\n\n# -----------------------------------------------------------------------------\n# Training entry point\n# -----------------------------------------------------------------------------\n\ndef train_variation(variation: str, cfg: Dict[str, Any], smoke: bool = False) -> None:\n    if variation not in ALG_REGISTRY:\n        raise ValueError(f\"Unknown variation '{variation}'. Allowed: {list(ALG_REGISTRY)}\")\n\n    # ------------------------------------------------------------------\n    # Directory layout & logging\n    # ------------------------------------------------------------------\n    root_out = Path(cfg[\"output\"][\"root_dir\"])\n    root_out.mkdir(parents=True, exist_ok=True)\n\n    time_stamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    var_dir_name = f\"{time_stamp}_{sanitize_variation(variation)}\"\n    exp_dir = root_out / var_dir_name\n    exp_dir.mkdir(parents=True, exist_ok=False)\n\n    # Remember original variation string so that evaluate.py can de-sanitize\n    with open(exp_dir / \"meta.yaml\", \"w\", encoding=\"utf-8\") as f:\n        yaml.safe_dump({\"variation\": variation, \"smoke\": smoke}, f)\n\n    writer = SummaryWriter(log_dir=str(exp_dir / \"tensorboard\"))\n\n    # ------------------------------------------------------------------\n    # Data pipeline\n    # ------------------------------------------------------------------\n    train_loader, val_loader, input_dim, output_dim = get_dataloaders(cfg, smoke)\n\n    # ------------------------------------------------------------------\n    # HPO algorithm logic\n    # ------------------------------------------------------------------\n    AlgorithmCls = ALG_REGISTRY[variation]\n    algo = AlgorithmCls(\n        input_dim=input_dim,\n        output_dim=output_dim,\n        train_loader=train_loader,\n        val_loader=val_loader,\n        cfg=cfg,\n        log_writer=writer,\n    )\n\n    algo.optimize()\n\n    # ------------------------------------------------------------------\n    # Save the best configuration and close the writer\n    # ------------------------------------------------------------------\n    with open(exp_dir / \"best_config.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(algo.best_config, f, indent=2)\n\n    writer.flush()\n    writer.close()\n\n    # For CI / smoke-test convenience we print a small summary to stdout\n    print(\"\\n========== EXPERIMENT SUMMARY ==========\")\n    print(f\"Variation                 : {variation}\")\n    print(f\"Best validation metric    : {algo.best_metric:.4f}\")\n    print(f\"Consumed epochs (overall) : {algo.total_consumed_epochs}\")\n    print(f\"Results directory         : {exp_dir}\")\n    print(\"=======================================\\n\")\n\n\n# -----------------------------------------------------------------------------\n# CLI wrapper\n# -----------------------------------------------------------------------------\n\ndef main() -> None:\n    parser = argparse.ArgumentParser(description=\"COMMON CORE training driver\")\n    group = parser.add_mutually_exclusive_group(required=True)\n    group.add_argument(\"--smoke-test\", action=\"store_true\", help=\"quick 30-second run\")\n    group.add_argument(\"--full-experiment\", action=\"store_true\", help=\"run full experiment\")\n    parser.add_argument(\"--variation\", type=str, default=\"SlopeStop-BO\", help=\"variation name\")\n    args = parser.parse_args()\n\n    cfg_path = Path(\"config\") / (\"smoke_test.yaml\" if args.smoke_test else \"full_experiment.yaml\")\n    cfg = load_yaml(cfg_path)\n\n    # Ensure output root exists even if config forgets it\n    cfg.setdefault(\"output\", {\"root_dir\": \"results\"})\n\n    # Re-seed everything for reproducibility\n    seed = cfg.get(\"seed\", 0)\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n\n    train_variation(args.variation, cfg, smoke=args.smoke_test)\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "evaluate_py": "\"\"\"src/evaluate.py\nAggregate TensorBoard logs of *all* variations stored under a given directory,\ncompute identical evaluation metrics and create comparison figures.\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport re\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport yaml\nfrom tensorboard.backend.event_processing import event_accumulator as ea\n\nFIG_KW = dict(bbox_inches=\"tight\", dpi=300, format=\"pdf\")\n\n# -----------------------------------------------------------------------------\n# TensorBoard utility\n# -----------------------------------------------------------------------------\n\n\ndef _accumulate_scalars(tb_dir: Path, tag: str) -> Tuple[np.ndarray, np.ndarray]:\n    ev_files = list(tb_dir.glob(\"*tfevents.*\"))\n    if not ev_files:\n        raise FileNotFoundError(f\"No TensorBoard event files found in {tb_dir}\")\n\n    ev = ea.EventAccumulator(str(ev_files[0]), size_guidance={ea.SCALARS: 0})\n    ev.Reload()\n    if tag not in ev.Tags()[\"scalars\"]:\n        raise KeyError(f\"Scalar tag '{tag}' not found in {tb_dir}\")\n\n    wall_times, steps, vals = zip(*ev.Scalars(tag))\n    return np.array(steps), np.array(vals)\n\n\n# -----------------------------------------------------------------------------\n# Core evaluation logic\n# -----------------------------------------------------------------------------\n\n\ndef _collect_runs(results_root: Path) -> Dict[str, List[Path]]:\n    \"\"\"Map ORIGINAL variation names -> list of run directories.\"\"\"\n    mapping: Dict[str, List[Path]] = {}\n    for run_dir in sorted(results_root.iterdir()):\n        if not run_dir.is_dir():\n            continue\n        meta_file = run_dir / \"meta.yaml\"\n        if not meta_file.exists():\n            continue\n        with open(meta_file, \"r\", encoding=\"utf-8\") as f:\n            meta = yaml.safe_load(f)\n        var_name = meta[\"variation\"]\n        mapping.setdefault(var_name, []).append(run_dir)\n    if not mapping:\n        raise RuntimeError(f\"No experiment sub-directories with meta.yaml found in {results_root}\")\n    return mapping\n\n\ndef _aggregate_metric(mapping: Dict[str, List[Path]], tag: str) -> pd.DataFrame:\n    rows = []\n    for var, dirs in mapping.items():\n        for d in dirs:\n            tb_dir = d / \"tensorboard\"\n            steps, vals = _accumulate_scalars(tb_dir, tag)\n            best_val = float(vals.min()) if \"loss\" in tag else float(vals.max())\n            rows.append({\"variation\": var, \"run_dir\": str(d), \"best\": best_val})\n    return pd.DataFrame(rows)\n\n\n# -----------------------------------------------------------------------------\n# Plot helpers\n# -----------------------------------------------------------------------------\n\n\ndef _plot_bar(df: pd.DataFrame, metric: str, ylabel: str, fname: Path) -> None:\n    plt.figure(figsize=(6, 4))\n    sns.barplot(data=df, x=\"variation\", y=\"best\", ci=\"sd\")\n    plt.ylabel(ylabel)\n    plt.xlabel(\"Variation\")\n    plt.xticks(rotation=30, ha=\"right\")\n    for idx, row in df.groupby(\"variation\").mean().reset_index().iterrows():\n        plt.text(idx, row[\"best\"], f\"{row['best']:.3f}\", ha=\"center\", va=\"bottom\", fontsize=8)\n    plt.tight_layout()\n    plt.savefig(fname, **FIG_KW)\n    plt.close()\n\n\n# -----------------------------------------------------------------------------\n# Main CLI\n# -----------------------------------------------------------------------------\n\ndef main() -> None:\n    parser = argparse.ArgumentParser(description=\"COMMON CORE evaluator\")\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"root dir with experiment runs\")\n    args = parser.parse_args()\n\n    results_root = Path(args.results_dir).expanduser().resolve()\n    mapping = _collect_runs(results_root)\n\n    # ------------------------------------------------------------------\n    # Collect & compare validation accuracy\n    # ------------------------------------------------------------------\n    acc_df = _aggregate_metric(mapping, \"val/accuracy\")\n    loss_df = _aggregate_metric(mapping, \"train/loss\")\n\n    print(\"=== Validation accuracy (best per run) ===\")\n    print(acc_df.groupby(\"variation\")[\"best\"].describe())\n    print(\"\\n=== Training loss (best per run) ===\")\n    print(loss_df.groupby(\"variation\")[\"best\"].describe())\n\n    # ------------------------------------------------------------------\n    # Produce publication-ready figures\n    # ------------------------------------------------------------------\n    fig_dir = results_root / \"figures\"\n    fig_dir.mkdir(exist_ok=True)\n\n    _plot_bar(acc_df, \"val/accuracy\", \"Best validation accuracy\", fig_dir / \"accuracy.pdf\")\n    _plot_bar(loss_df, \"train/loss\", \"Lowest training loss\", fig_dir / \"training_loss.pdf\")\n\n    print(\"Generated figures:\")\n    for f in fig_dir.glob(\"*.pdf\"):\n        print(\" -\", f.name)\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "preprocess_py": "\"\"\"src/preprocess.py\nShared data-loading and preprocessing utilities.\nDataset-specific logic has now been wired-up for the exp-1 benchmarks:\n    • LCBench         – tabular classification (35 OpenML tasks)\n    • CIFAR-10/100    – vision benchmarks used inside NAS-Bench-201\n\nFor environments where the full dataset download is impossible (e.g. offline\nCI), the loader falls back to a tiny synthetic dataset while *still* keeping\nexactly the same public API so that higher-level code needs no changes.\n\"\"\"\nfrom __future__ import annotations\n\nimport os\nfrom typing import Tuple\n\nimport numpy as np\nimport torch\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader, TensorDataset, random_split\nfrom torchvision import datasets as tv_datasets\nfrom torchvision import transforms as tv_transforms\n\n__all__ = [\"get_dataloaders\"]\n\n# -----------------------------------------------------------------------------\n# Synthetic fallback dataset (kept for smoke-tests & CI)\n# -----------------------------------------------------------------------------\n\ndef _synthetic_classification(n_samples: int, n_features: int, n_classes: int, seed: int = 0):\n    rng = np.random.default_rng(seed)\n    X = rng.standard_normal(size=(n_samples, n_features)).astype(np.float32)\n    W = rng.standard_normal(size=(n_features, n_classes))\n    logits = X @ W\n    y = logits.argmax(axis=1).astype(np.int64)\n    return X, y\n\n\n# -----------------------------------------------------------------------------\n# Tabular – LCBench loader\n# -----------------------------------------------------------------------------\n\ndef _load_lcbench(task_id: str, batch_size: int, seed: int, smoke: bool):\n    \"\"\"Return DataLoaders for a single *OpenML* task inside the *lcbench* HF dataset.\n\n    Each row already contains z-normalised numerical features and an int label.\n    The split keys in the HF dataset are exactly \"train\", \"validation\",\n    \"test\" so we can rely on them.\n    \"\"\"\n    try:\n        ds = load_dataset(\"lcbench\", task_id)\n    except Exception as e:  # pragma: no cover – offline environment\n        print(\n            \"[WARN] Failed to download LCBench ({}). Falling back to synthetic dataset: {}\".format(\n                task_id, e\n            )\n        )\n        return _load_synthetic_for_failure(batch_size, seed)\n\n    def _to_tensor_dataset(split):\n        X = np.stack(split[\"features\"]).astype(np.float32)\n        y = np.array(split[\"label\"], dtype=np.int64)\n        return TensorDataset(torch.tensor(X), torch.tensor(y))\n\n    train_set = _to_tensor_dataset(ds[\"train\"])\n    val_set = _to_tensor_dataset(ds[\"validation\"])\n\n    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n\n    input_dim = train_set.tensors[0].shape[1]\n    n_classes = int(train_set.tensors[1].max().item() + 1)\n    return train_loader, val_loader, input_dim, n_classes\n\n\n# -----------------------------------------------------------------------------\n# Vision – CIFAR-10 / CIFAR-100 loaders\n# -----------------------------------------------------------------------------\n\ndef _load_cifar(name: str, batch_size: int, seed: int, smoke: bool):\n    is_train_transform = tv_transforms.Compose(\n        [\n            tv_transforms.RandomCrop(32, padding=4),\n            tv_transforms.RandomHorizontalFlip(),\n            tv_transforms.ToTensor(),\n            tv_transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n        ]\n    )\n    is_val_transform = tv_transforms.Compose(\n        [\n            tv_transforms.ToTensor(),\n            tv_transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n        ]\n    )\n\n    root = os.path.expanduser(\"~/.cache/torchvision\")\n    ds_class = tv_datasets.CIFAR10 if name == \"cifar10\" else tv_datasets.CIFAR100\n\n    try:\n        train_set = ds_class(root, train=True, download=True, transform=is_train_transform)\n        val_set = ds_class(root, train=False, download=True, transform=is_val_transform)\n    except Exception as e:  # pragma: no cover – offline environment\n        print(f\"[WARN] Failed to download {name}: {e}. Falling back to synthetic dataset.\")\n        return _load_synthetic_for_failure(batch_size, seed)\n\n    if smoke:\n        # Use only a tiny subset to keep the runtime <30 s\n        train_set, _ = random_split(train_set, [1024, len(train_set) - 1024],\n                                    generator=torch.Generator().manual_seed(seed))\n        val_set, _ = random_split(val_set, [256, len(val_set) - 256],\n                                  generator=torch.Generator().manual_seed(seed))\n\n    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2)\n    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=2)\n\n    input_dim = (3, 32, 32)  # for CNN constructions\n    n_classes = 10 if name == \"cifar10\" else 100\n    return train_loader, val_loader, input_dim, n_classes\n\n\n# -----------------------------------------------------------------------------\n# Graceful degradation helper\n# -----------------------------------------------------------------------------\n\ndef _load_synthetic_for_failure(batch_size: int, seed: int):\n    X, y = _synthetic_classification(2048, 20, 3, seed)\n    tensor_x = torch.tensor(X)\n    tensor_y = torch.tensor(y)\n    dataset = TensorDataset(tensor_x, tensor_y)\n    val_size = int(0.2 * len(dataset))\n    train_size = len(dataset) - val_size\n    train_set, val_set = random_split(dataset, [train_size, val_size], generator=torch.Generator().manual_seed(seed))\n    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n    return train_loader, val_loader, 20, 3\n\n\n# -----------------------------------------------------------------------------\n# Public facade\n# -----------------------------------------------------------------------------\n\ndef get_dataloaders(cfg, smoke: bool = False):\n    \"\"\"Return `(train_loader, val_loader, input_dim_or_shape, n_classes)`.\n\n    Supported datasets\n    ------------------\n    1. \"lcbench\"   – requires `dataset.task_id` (e.g. \"3917_OpenML\")\n    2. \"cifar10\" / \"cifar100\"\n\n    Anything else falls back to the synthetic stub so that downstream code keeps\n    running even in constrained CI environments.\n    \"\"\"\n\n    ds_cfg = cfg[\"dataset\"]\n    name = ds_cfg.get(\"name\", \"synthetic_classification\").lower()\n    batch_size = ds_cfg.get(\"batch_size\", 64)\n    seed = cfg.get(\"seed\", 0)\n\n    # ------------------------------------------------------------------\n    # Dispatch to the concrete loaders\n    # ------------------------------------------------------------------\n    if name == \"lcbench\":\n        task_id = ds_cfg.get(\"task_id\", \"3917\")  # default to OpenML task 3917 if not given\n        return _load_lcbench(task_id, batch_size, seed, smoke)\n\n    if name in {\"cifar10\", \"cifar100\"}:\n        return _load_cifar(name, batch_size, seed, smoke)\n\n    # ------------------------------------------------------------------\n    # Synthetic fallback (kept for smoke-test and unexpected names)\n    # ------------------------------------------------------------------\n    n = 1024 if smoke else 4096\n    n_features = ds_cfg.get(\"n_features\", 20)\n    n_classes = ds_cfg.get(\"n_classes\", 3)\n    X, y = _synthetic_classification(n, n_features, n_classes, seed)\n    tensor_x = torch.tensor(X)\n    tensor_y = torch.tensor(y)\n    dataset = TensorDataset(tensor_x, tensor_y)\n\n    # 80/20 split\n    val_size = int(0.2 * len(dataset))\n    train_size = len(dataset) - val_size\n    train_set, val_set = random_split(dataset, [train_size, val_size], generator=torch.Generator().manual_seed(seed))\n\n    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n    return train_loader, val_loader, n_features, n_classes\n",
    "model_py": "\"\"\"src/model.py\nAll model architectures and HPO algorithms live here so that *train.py* stays\nclean and generic.  The surrogate & acquisition sub-modules are implemented in\npure PyTorch + GPyTorch to satisfy the *PyTorch-only* requirement.\n\"\"\"\nfrom __future__ import annotations\n\nimport itertools\nimport math\nimport time\nfrom typing import Any, Dict, List, Tuple\n\nimport gpytorch\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.tensorboard import SummaryWriter\n\n# -----------------------------------------------------------------------------\n# Task model (simple MLP – can be replaced later)\n# -----------------------------------------------------------------------------\n\n\nclass MLP(nn.Module):\n    def __init__(self, input_dim: int, output_dim: int, hidden_dim: int = 64, depth: int = 2):\n        super().__init__()\n        layers: List[nn.Module] = []\n        dims = [input_dim] + [hidden_dim] * depth + [output_dim]\n        for d_in, d_out in zip(dims[:-1], dims[1:]):\n            layers.append(nn.Linear(d_in, d_out))\n            if d_out != output_dim:\n                layers.append(nn.ReLU())\n        self.net = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.net(x)\n\n\n# -----------------------------------------------------------------------------\n# Early stopping – slope-based rule (core contribution)\n# -----------------------------------------------------------------------------\n\n\ndef should_stop_slope(buffer: List[float], step: int, W: int, eps: float, gamma: float) -> bool:\n    if len(buffer) <= W:\n        return False\n    slope = (buffer[-1] - buffer[-W - 1]) / W\n    return slope < eps * math.exp(-gamma * step)\n\n\n# -----------------------------------------------------------------------------\n# Surrogate Model (GPyTorch – Exact GP)\n# -----------------------------------------------------------------------------\n\n\nclass ExactGPModel(gpytorch.models.ExactGP):\n    def __init__(self, train_x, train_y, likelihood):\n        super().__init__(train_x, train_y, likelihood)\n        self.mean_module = gpytorch.means.ConstantMean()\n        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n\n    def forward(self, x):\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n\n\nclass GaussianProcessSurrogate:\n    def __init__(self, input_dim: int):\n        self.input_dim = input_dim\n        self.train_x, self.train_y = None, None  # type: ignore\n        self.likelihood = gpytorch.likelihoods.GaussianLikelihood()\n        self.model: ExactGPModel | None = None\n\n    def update(self, X: torch.Tensor, y: torch.Tensor):\n        self.train_x = X.float()\n        self.train_y = y.float()\n        self.model = ExactGPModel(self.train_x, self.train_y, self.likelihood)\n        self.model.train()\n        self.likelihood.train()\n\n        optimizer = torch.optim.Adam(self.model.parameters(), lr=0.1)\n        mll = gpytorch.mlls.ExactMarginalLogLikelihood(self.likelihood, self.model)\n        for _ in range(25):\n            optimizer.zero_grad()\n            output = self.model(self.train_x)\n            loss = -mll(output, self.train_y)\n            loss.backward()\n            optimizer.step()\n\n        self.model.eval()\n        self.likelihood.eval()\n\n    def predict(self, X: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        if self.model is None:\n            raise RuntimeError(\"Surrogate has not been trained yet.\")\n        with torch.no_grad(), gpytorch.settings.fast_pred_var():\n            preds = self.likelihood(self.model(X.float()))\n            mean = preds.mean\n            var = preds.variance.clamp_min(1e-9)\n        return mean, var\n\n\n# -----------------------------------------------------------------------------\n# Acquisition functions\n# -----------------------------------------------------------------------------\n\n\ndef expected_improvement(mu: torch.Tensor, sigma2: torch.Tensor, best: float, minimize: bool = True):\n    std = sigma2.sqrt()\n    if minimize:\n        improvement = best - mu\n    else:\n        improvement = mu - best\n    Z = improvement / std.clamp_min(1e-9)\n    ei = improvement * torch.distributions.Normal(0, 1).cdf(Z) + std * torch.distributions.Normal(0, 1).log_prob(Z).exp()\n    return ei\n\n\n# -----------------------------------------------------------------------------\n# HPO algorithm base class\n# -----------------------------------------------------------------------------\n\n\nclass HPOAlgorithmBase:\n    def __init__(\n        self,\n        *,\n        input_dim: int,\n        output_dim: int,\n        train_loader,\n        val_loader,\n        cfg: Dict[str, Any],\n        log_writer: SummaryWriter,\n    ) -> None:\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n        self.cfg = cfg\n        self.writer = log_writer\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        self.search_space = cfg[\"search_space\"]\n        self.max_epochs = cfg.get(\"epochs\", 50)\n        self.seed = cfg.get(\"seed\", 0)\n\n        self.total_consumed_epochs = 0\n        self.best_metric = float(\"inf\")\n        self.best_config: Dict[str, Any] = {}\n\n        torch.manual_seed(self.seed)\n        np.random.seed(self.seed)\n\n    # ------------------------------------------------------------------\n    # API subclasses must implement\n    # ------------------------------------------------------------------\n    def optimize(self):\n        raise NotImplementedError\n\n    # ------------------------------------------------------------------\n    # Helper – train one config (with/without early stop)\n    # ------------------------------------------------------------------\n    def _train_model(self, params: Dict[str, Any], early_stop: bool) -> Tuple[float, int]:\n        model = MLP(self.input_dim, self.output_dim, hidden_dim=params[\"hidden_dim\"]).to(self.device)\n        optimizer = torch.optim.Adam(model.parameters(), lr=params[\"lr\"])\n        criterion = nn.CrossEntropyLoss()\n\n        val_buffer: List[float] = []\n        best_val = float(\"inf\")\n        consumed = 0\n\n        for epoch in range(1, self.max_epochs + 1):\n            model.train()\n            for xb, yb in self.train_loader:\n                xb, yb = xb.to(self.device), yb.to(self.device)\n                optimizer.zero_grad()\n                out = model(xb)\n                loss = criterion(out, yb)\n                loss.backward()\n                optimizer.step()\n\n            # validation\n            model.eval()\n            correct = 0\n            total = 0\n            val_loss = 0.0\n            with torch.no_grad():\n                for xb, yb in self.val_loader:\n                    xb, yb = xb.to(self.device), yb.to(self.device)\n                    logits = model(xb)\n                    loss = criterion(logits, yb)\n                    val_loss += loss.item() * yb.size(0)\n                    pred = logits.argmax(dim=1)\n                    correct += (pred == yb).sum().item()\n                    total += yb.size(0)\n            val_loss /= total\n            acc = correct / total\n\n            # logging\n            step_global = self.total_consumed_epochs + epoch  # approximate global step\n            self.writer.add_scalar(\"train/loss\", loss.item(), step_global)\n            self.writer.add_scalar(\"val/accuracy\", acc, step_global)\n\n            # early stopping decision\n            val_buffer.append(acc)\n            if early_stop and should_stop_slope(val_buffer, epoch, W=3, eps=1e-3, gamma=0.01):\n                consumed = epoch\n                break\n        else:\n            consumed = self.max_epochs\n\n        self.total_consumed_epochs += consumed\n        # we *minimize* negative accuracy to align with EI\n        return -acc, consumed\n\n    # ------------------------------------------------------------------\n    # Helper – sample random configuration from search space\n    # ------------------------------------------------------------------\n    def _sample_random_config(self) -> Dict[str, Any]:\n        cfg = {\n            k: np.random.choice(v) if isinstance(v, (list, tuple)) else v for k, v in self.search_space.items()\n        }\n        return cfg\n\n\n# -----------------------------------------------------------------------------\n# SlopeStop-BO (main method)\n# -----------------------------------------------------------------------------\n\n\nclass SlopeStopBOAlgorithm(HPOAlgorithmBase):\n    def optimize(self):\n        n_init = self.cfg.get(\"n_init\", 5)\n        n_iter = self.cfg.get(\"n_iter\", 15)\n\n        X: List[List[float]] = []\n        y: List[float] = []\n        costs: List[float] = []\n        gp = GaussianProcessSurrogate(input_dim=len(self.search_space) + 1)  # +1 for tau\n\n        # --------------------------------------------------------------\n        # Initial random designs\n        # --------------------------------------------------------------\n        for _ in range(n_init):\n            params = self._sample_random_config()\n            metric, epochs = self._train_model(params, early_stop=True)\n            tau = epochs / self.max_epochs\n            X.append([*params.values(), tau])\n            y.append(metric)\n            costs.append(tau * self.max_epochs)\n            if metric < self.best_metric:\n                self.best_metric = metric\n                self.best_config = params\n\n        # --------------------------------------------------------------\n        # BO loop\n        # --------------------------------------------------------------\n        for _ in range(n_iter):\n            # 1) fit surrogate\n            X_t = torch.tensor(X)\n            y_t = torch.tensor(y)\n            gp.update(X_t, y_t)\n\n            # 2) sample candidates & compute EI/c\n            cand_params_list = [self._sample_random_config() for _ in range(100)]\n            cands = torch.tensor([[*p.values(), 1.0] for p in cand_params_list])  # assume full budget first\n            mu, var = gp.predict(cands)\n            ei = expected_improvement(mu, var, best=self.best_metric)\n            # cost aware – denominator = expected cost (full budget here simplified)\n            ei_div_cost = ei / self.max_epochs\n            best_idx = torch.argmax(ei_div_cost).item()\n            next_params = cand_params_list[best_idx]\n\n            # 3) evaluate with slope early stop\n            metric, epochs = self._train_model(next_params, early_stop=True)\n            tau = epochs / self.max_epochs\n            X.append([*next_params.values(), tau])\n            y.append(metric)\n            costs.append(tau * self.max_epochs)\n\n            if metric < self.best_metric:\n                self.best_metric = metric\n                self.best_config = next_params\n\n\n# -----------------------------------------------------------------------------\n# Ablation: A0 – no Early Stop, cost-aware EI/c\n# -----------------------------------------------------------------------------\n\n\nclass A0NoEarlyStopAlgorithm(SlopeStopBOAlgorithm):\n    def optimize(self):\n        n_init = self.cfg.get(\"n_init\", 5)\n        n_iter = self.cfg.get(\"n_iter\", 15)\n\n        X, y_, costs = [], [], []\n        gp = GaussianProcessSurrogate(input_dim=len(self.search_space) + 1)\n\n        # random designs\n        for _ in range(n_init):\n            p = self._sample_random_config()\n            metric, epochs = self._train_model(p, early_stop=False)  # full budget\n            tau = 1.0\n            X.append([*p.values(), tau])\n            y_.append(metric)\n            costs.append(self.max_epochs)\n            if metric < self.best_metric:\n                self.best_metric = metric\n                self.best_config = p\n\n        for _ in range(n_iter):\n            gp.update(torch.tensor(X), torch.tensor(y_))\n            cand_params = [self._sample_random_config() for _ in range(100)]\n            cands_t = torch.tensor([[*c.values(), 1.0] for c in cand_params])\n            mu, var = gp.predict(cands_t)\n            ei = expected_improvement(mu, var, best=self.best_metric)\n            ei_c = ei / self.max_epochs  # cost aware\n            best_idx = torch.argmax(ei_c).item()\n            p_star = cand_params[best_idx]\n            metric, epochs = self._train_model(p_star, early_stop=False)\n            X.append([*p_star.values(), 1.0])\n            y_.append(metric)\n            costs.append(self.max_epochs)\n            if metric < self.best_metric:\n                self.best_metric = metric\n                self.best_config = p_star\n\n\n# -----------------------------------------------------------------------------\n# Ablation: A1 – slope early stop but *no* cost-aware acquisition\n# -----------------------------------------------------------------------------\n\n\nclass A1SlopeStopNoCostAwareAlgorithm(SlopeStopBOAlgorithm):\n    def optimize(self):\n        n_init = self.cfg.get(\"n_init\", 5)\n        n_iter = self.cfg.get(\"n_iter\", 15)\n\n        X, y_, costs = [], [], []\n        gp = GaussianProcessSurrogate(input_dim=len(self.search_space) + 1)\n\n        for _ in range(n_init):\n            p = self._sample_random_config()\n            m, e = self._train_model(p, early_stop=True)\n            tau = e / self.max_epochs\n            X.append([*p.values(), tau])\n            y_.append(m)\n            costs.append(e)\n            if m < self.best_metric:\n                self.best_metric = m\n                self.best_config = p\n\n        for _ in range(n_iter):\n            gp.update(torch.tensor(X), torch.tensor(y_))\n            cand_params = [self._sample_random_config() for _ in range(100)]\n            cands_t = torch.tensor([[*c.values(), 1.0] for c in cand_params])\n            mu, var = gp.predict(cands_t)\n            ei = expected_improvement(mu, var, best=self.best_metric)\n            best_idx = torch.argmax(ei).item()  # ← no cost division\n            p_star = cand_params[best_idx]\n            m, e = self._train_model(p_star, early_stop=True)\n            tau = e / self.max_epochs\n            X.append([*p_star.values(), tau])\n            y_.append(m)\n            costs.append(e)\n            if m < self.best_metric:\n                self.best_metric = m\n                self.best_config = p_star\n\n\n# -----------------------------------------------------------------------------\n# Hyperband (simplified – successive halving)\n# -----------------------------------------------------------------------------\n\n\nclass HyperbandAlgorithm(HPOAlgorithmBase):\n    def optimize(self):\n        eta = 3  # down-sampling rate\n        R = self.max_epochs\n        s_max = int(math.log(R, eta))\n        B = (s_max + 1) * R\n\n        def run_then_return_val_loss(params, max_epochs):\n            metric, _ = self._train_model(params, early_stop=False)  # always full; but limited epochs\n            return metric\n\n        for s in reversed(range(s_max + 1)):\n            n = int(math.ceil(B / R / (s + 1) * eta ** s))\n            r = int(R * eta ** (-s))\n\n            # initial configs\n            T = [self._sample_random_config() for _ in range(n)]\n            for i in range(s + 1):\n                n_i = n * eta ** (-i)\n                r_i = r * eta ** i\n                metrics = [run_then_return_val_loss(t, r_i) for t in T]\n                idx_sorted = np.argsort(metrics)\n                n_keep = int(n_i / eta)\n                if n_keep <= 0:\n                    break\n                T = [T[i] for i in idx_sorted[:n_keep]]\n                if metrics[idx_sorted[0]] < self.best_metric:\n                    self.best_metric = metrics[idx_sorted[0]]\n                    self.best_config = T[0]\n\n\n# -----------------------------------------------------------------------------\n# BOIL (simplified – imitates full curve surrogate)\n# -----------------------------------------------------------------------------\n\n\nclass BOILAlgorithm(HPOAlgorithmBase):\n    def optimize(self):\n        # For brevity we reuse slope-stopping but fit GP on (x, t).\n        n_init = self.cfg.get(\"n_init\", 5)\n        n_iter = self.cfg.get(\"n_iter\", 15)\n        X, y_ = [], []\n        gp = GaussianProcessSurrogate(input_dim=len(self.search_space) + 1)  # +1 time dim\n\n        for _ in range(n_init):\n            p = self._sample_random_config()\n            m, e = self._train_model(p, early_stop=False)\n            # store full budget info (t=1)\n            X.append([*p.values(), 1.0])\n            y_.append(m)\n            if m < self.best_metric:\n                self.best_metric = m\n                self.best_config = p\n\n        for _ in range(n_iter):\n            gp.update(torch.tensor(X), torch.tensor(y_))\n            cand_params = [self._sample_random_config() for _ in range(100)]\n            cands_t = torch.tensor([[*c.values(), 1.0] for c in cand_params])\n            mu, var = gp.predict(cands_t)\n            ei = expected_improvement(mu, var, best=self.best_metric)\n            best_idx = torch.argmax(ei).item()\n            p_star = cand_params[best_idx]\n            m, e = self._train_model(p_star, early_stop=False)\n            X.append([*p_star.values(), 1.0])\n            y_.append(m)\n            if m < self.best_metric:\n                self.best_metric = m\n                self.best_config = p_star\n",
    "main_py": "# src/main.py\n\"\"\"CLI façade that dispatches to *train.py* so users only ever have to call\n`python -m src.main ...`.  Keeping the entry point slim makes CI / packaging\nsimpler and ensures a single source of truth for argument parsing.\n\"\"\"\nfrom __future__ import annotations\n\nimport importlib\n\nfrom . import train as _train_mod  # re-export for mypy clarity\n\nif __name__ == \"__main__\":\n    _train_mod.main()\n",
    "pyproject_toml": "[build-system]\nrequires = [\"setuptools>=42\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"common-core-foundation\"\nversion = \"0.1.0\"\ndescription = \"Foundation code for consistent HPO experiments\"\nauthors = [\n  { name=\"Researcher\", email=\"research@example.com\" }\n]\nrequires-python = \">=3.10\"\nlicense = {text = \"MIT\"}\n\n[project.dependencies]\ntorch = \"*\"\ngpytorch = \"*\"\ntensorboard = \"*\"\nmatplotlib = \"*\"\nseaborn = \"*\"\npyyaml = \"*\"\nscikit-learn = \"*\"\nnumpy = \"*\"\npandas = \"*\"\nscipy = \"*\"\n# Added for dataset specialisation\ndatasets = \"*\"\ntorchvision = \"*\"\n",
    "smoke_test_yaml": "# config/smoke_test.yaml\nseed: 0\nepochs: 5\nn_init: 2\nn_iter: 2\noutput:\n  root_dir: \"results\"\n\ndataset:\n  name: \"synthetic_classification\"\n  n_features: 10\n  n_classes: 3\n  batch_size: 32\n\nsearch_space:\n  lr: [0.01, 0.001]\n  hidden_dim: [32, 64]\n",
    "full_experiment_yaml": "# config/full_experiment.yaml\n# Specialised configuration for *exp-1* as described in the research plan.\n\nseed: 0\nepochs: 50         # 50 for LCBench, will be overridden to 200 for CIFAR in code when necessary\nn_init: 10\nn_iter: 30\noutput:\n  root_dir: \"results\"\n\ndataset:\n  name: \"lcbench\"        # switch to \"cifar10\" / \"cifar100\" in separate runs\n  task_id: \"3917\"        # example OpenML task inside LCBench – change per run if desired\n  batch_size: 128\n\n# Search space\nsearch_space:\n  lr: [0.1, 0.03, 0.01, 0.003, 0.001]\n  hidden_dim: [32, 64, 128, 256]   # used by FeedForwardNet-LC\n  channels: [8, 16, 24]            # used by SimpleNAS201CNN (vision-only)\n\n# Additional SlopeStop hyper-parameters (global across tasks)\nW: 3\nepsilon: 0.001\ngamma: 0.01\n\n# End of file\n"
}
