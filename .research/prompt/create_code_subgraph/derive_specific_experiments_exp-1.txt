
Input:
You are a cutting-edge AI researcher specializing experiments from a common base foundation.

**Previous Step (Completed)**: Common base logic and evaluation framework have been generated with placeholders
**Current Task**: Replace placeholders with specific datasets, models, and experimental configurations

Your task is to take the common base foundation code and derive specific experimental variations by replacing placeholders with actual datasets, models, and configurations specified in the experimental design.

# Instructions: Experiment Specialization

## Core Task
- **PLACEHOLDER REPLACEMENT**: Replace all placeholders in the common base code with actual datasets, models, and configurations
- **SPECIALIZATION**: Adapt the generic framework to work with specific experimental requirements
- **REAL IMPLEMENTATIONS**: Convert all placeholder logic to working implementations using actual Hugging Face resources
- **CONFIGURATION COMPLETION**: Fill in specific parameters, hyperparameters, and experimental settings

## Specialization Requirements
- Implement all run_variations (e.g., baseline, proposed, ablations) in `src/model.py`
- Replace dataset placeholders with actual Hugging Face dataset loading and preprocessing
- Replace model placeholders with specific model architectures for each variation
- Fill in exact experimental parameters based on experimental design
- Ensure `src/main.py` routes `--variation` parameter to the correct model implementation
- Complete configuration files with actual experimental values from External Resources
- Ensure all external resources specified in the experimental design are properly integrated

## Key Replacement Areas
1. **Dataset Loading**: Replace `DATASET_PLACEHOLDER` with specific Hugging Face dataset code
2. **Model Architecture**: Replace `MODEL_PLACEHOLDER` with actual model definitions
3. **Configuration Values**: Replace `SPECIFIC_CONFIG_PLACEHOLDER` with real experimental parameters
4. **Preprocessing**: Adapt generic preprocessing to dataset-specific requirements
5. **Evaluation**: Customize evaluation metrics for specific datasets/tasks

## Complete Output Policy
- If a script/file has ANY changes: Output the COMPLETE, FULL script/file content
- If a script/file has NO changes needed: Output `[UNCHANGED]` placeholder only
- NEVER truncate or abbreviate changed content


# Experimental Environment
NVIDIA Tesla T4
VRAM：16 GB
RAM：500 MB

# Current Research Method
{
    "Open Problems": "Current progressive multi-fidelity schedulers such as PASHA decide when to raise the maximum training budget by monitoring whether the ranking of the two top rungs has stabilised.  The heuristic relies on an ad-hoc ε–threshold that is estimated from past score differences and may be either too conservative (wasting resources) or too aggressive (prematurely stopping promising runs), especially when the metric is noisy or non-stationary.",
    "Methods": "Surrogate-Assisted PASHA (SA-PASHA)\n1. Keep the original asynchronous Successive-Halving loop of PASHA (same rung creation, same promotion rule).\n2. Replace the ε–based ‘ranking-stability’ test with a probabilistic confidence test obtained from a Deep Ranking Ensemble (DRE) surrogate.\n   2.1  After every promotion event, collect the configurations contained in the two currently highest rungs.\n   2.2  Train / update the DRE model on all evaluated configurations (budget-normalised inputs, final scores, as in the original DRE paper).\n   2.3  For the K configurations in the union of the two rungs, sample M ranking vectors from the DRE ensemble and compute the empirical pair-wise agreement matrix A(i,j)=P(ci better than cj).\n   2.4  Compute the average rank-confidence ρ =  (1/K) Σ_i ( |{ j : A(i,j)>0.5 }| / (K−1) ).  ρ≈1 indicates a very stable ranking.\n3. Decision rule: keep the current maximum budget T_max as long as ρ ≥ τ (τ=0.9 by default); otherwise double T_max exactly as in PASHA.  No hand-crafted ε is required.\n4. All other PASHA components (soft vs. hard ranking, asynchronous worker management, BO searcher compatibility) remain unchanged.",
    "Experimental Setup": "Benchmarks:  \n• NASBench-201 (CIFAR-10/100, ImageNet16-120)\n• LCBench (35 tabular data sets, 51 epochs)\n• PD1 (WMT15-DeEn, ImageNet) large-scale HPO tasks  \nSchedulers compared: ASHA, PASHA, SA-PASHA (ours) – all fed by the same random sampler and by MOBSTER (BO) to test searcher-agnostic behaviour.  \nResources: 4 parallel GPUs (NASBench), 4 CPU workers (LCBench/PD1).  Hyper-parameters: τ∈{0.8,0.9,0.95}; DRE ensemble size=10, list-wise loss, 100 meta-epochs per update.  5 random seeds.",
    "Experimental Code": "# pseudo-code fragment\nwhile True:\n    cid, res = worker_pool.wait_next_result()\n    pasha_state.update(cid, res)\n    if pasha_state.promotion_event():\n        top, prev = pasha_state.top_two_rungs()\n        X, y = pasha_state.all_evaluations()\n        dre.fit(X, y)                      # incremental update\n        ranks = dre.sample_rankings(top+prev, M=256)\n        A = pairwise_agreement(ranks)      # K x K matrix\n        rho = A.mean(dim=1).mean()\n        if rho < tau:\n            pasha_state.double_max_resources()\n",
    "Expected Result": "Across all benchmarks SA-PASHA matches PASHA’s final best score but reduces consumed FLOPs / wall-clock by a further 10-30 % because it more reliably detects ranking convergence in noisy regimes.  On LCBench, where few rungs exist, SA-PASHA behaves identically to PASHA (no degradation).  Surrogate update time adds <3 % overhead.",
    "Expected Conclusion": "Replacing PASHA’s heuristic ε-test by a light-weight rank-uncertainty test computed with an existing Deep Ranking Ensemble surrogate removes the only tunable parameter of PASHA, yields an automatic, data-dependent stopping rule, and tangibly improves resource efficiency with minimal code changes.  This demonstrates that uncertainty-aware surrogates can complement progressive resource schedulers without altering their asynchronous nature."
}

# Experimental Design
- Strategy: Objective:
Build a single, coherent evaluation pipeline that can be reused on every benchmark to show that SA-PASHA is (1) at least as good as PASHA in terms of final solution quality, (2) clearly more resource-efficient, (3) robust to noise, searcher choice and hyper-parameters, and (4) free of the hand-crafted ε weakness.

1. Validation Aspects
   A. Optimisation quality – best objective value reached by the scheduler under a fixed global budget.
   B. Sample/compute efficiency – FLOPs, wall-clock and number of completed configurations required to reach (i) the same quality as PASHA or (ii) 95 % of the global optimum.
   C. Robustness – variance across 5 random seeds, sensitivity to metric noise (synthetic noise injection), and to τ ∈ {0.8,0.9,0.95}.
   D. Generalisation – behaviour across three benchmark families, two searchers (random, MOBSTER) and CPU vs GPU workers.
   E. Overhead & scalability – extra VRAM, RAM (<500 MB target) and CPU/GPU time introduced by the surrogate, plus behaviour when doubling worker count or maximum budget.

2. Necessary Comparisons
   1. Baselines: ASHA (classical) and PASHA (ε-heuristic).
   2. Ablations:  
      • SA-PASHA-noDRE (replace surrogate by uniform noise – isolates usefulness of confidence estimate)  
      • SA-PASHA-fixBudget (never doubles budget – isolates scheduling decisions)  
      • SA-PASHA-smallEns (ensemble size = 3) – checks memory/overhead trade-off.
   3. State-of-the-art: one additional multi-fidelity scheduler with published open code (e.g. HYBAND or DEHB) to demonstrate competitiveness beyond PASHA.

3. Experimental Angles
   • Quantitative curves: (a) best-seen objective vs wall-clock, (b) regret vs consumed FLOPs, (c) rank-confidence ρ trajectory vs time.
   • Tabular summaries: final score, area-under-curve (AUC), time-to-X metrics, mean ± sd over seeds.
   • Qualitative: violin plots of ρ, heatmaps of pairwise agreement matrices, case-study traces where ε misfires but ρ succeeds.
   • Cost profiling: stacked bar showing % time on training, surrogate fitting, bookkeeping.

4. Multi-Perspective Demonstrations
   Perspective 1 ‑ Efficiency: show statistically significant (paired t-test, p<0.05) reduction of ≥10 % in FLOPs/wall-clock for equal quality.
   Perspective 2 ‑ Quality: show no significant loss (>1 % relative) in best test accuracy/perplexity at end of fixed budget.
   Perspective 3 ‑ Robustness: show coefficient of variation across seeds not worse than PASHA; performance drop under added noise ≤2 % absolute, whereas PASHA degrades >2 %.
   Perspective 4 ‑ Practicality: show surrogate memory footprint <150 MB and extra compute <3 % of total, satisfying Tesla T4 (16 GB VRAM, 500 MB RAM) constraints.

5. Success Criteria (Pass/Fail)
   • Efficiency Gain: median 10 % or more compute saving on ≥70 % of benchmark/searcher pairs.
   • Quality Parity: Δ(best score) ∈ [-1 %, +1 %] of PASHA on all tasks.
   • Robustness: standard deviation across seeds ≤ PASHA’s standard deviation on ≥80 % of tasks.
   • Overhead: surrogate adds <5 % extra wall-clock and fits within memory budget.
   Meeting all four constitutes success.

6. Execution Protocol (common to all experiments)
   a. Fix a global wall-clock/FLOP budget per benchmark that saturates PASHA.
   b. Run every scheduler/ablation for 5 seeds, capture full event logs.
   c. Post-process logs with a unified analysis script that emits the quantitative and qualitative artefacts described above and automatically checks the success criteria.
   d. Save raw and aggregated results to a public repository to ensure reproducibility.

7. Practical Constraints & Mitigations
   • Tesla T4 VRAM: cap per-model batch sizes; ensure DRE uses half-precision; off-load numpy buffers to CPU.
   • 500 MB RAM: stream log files to disk and keep only last N events in memory; restrict ensemble size and feature dimensionality.
   • Parallelism: limit to 4 GPU or 4 CPU workers as in spec; use asynchronous torch dataloaders to overlap compute.

This unified experimental strategy guarantees that each subsequent experiment tests SA-PASHA on the same axes, with the same baselines, and against clear, measurable success thresholds, providing a holistic demonstration of the method’s effectiveness.

# Current Experiment (to generate code for)
- Experiment ID: exp-1
- Description: Objective: Jointly validate optimisation quality and compute-efficiency of the proposed Surrogate-Assisted PASHA (SA-PASHA) versus strong baselines, plus an ablation that removes the Deep-Ranking Ensemble (noDRE).  

Benchmarks / Tasks
• NASBench-201 – CIFAR-10, CIFAR-100, ImageNet16-120 (GPU workers).  
• LCBench – 35 tabular data sets, 51-epoch MLP/XGBoost training traces (CPU workers).  
• PD1 – WMT15-DeEn Transformer (base) 30-epoch runs (GPU workers).  

Models under optimisation  
• NASBench-201 CNN cell (≈0.7 M params)  
• XGBoost & 3-layer MLP (from LCBench)  
• Transformer-base (6-layer) for WMT15-DeEn  

Dataset Pre-processing  
• CIFAR/ImageNet16: standardised RGB normalisation, random crop/flip.  
• WMT15: Moses tokenisation, 32 K BPE.  
• Tabular: median impute missing, one-hot categorical, z-score numeric.  

Splitting  
• All image datasets: train (45 k) / val (5 k) / test (10 k).  
• WMT15: train (4.5 M), news16 (3 k) for val, news17 (3 k) test.  
• LCBench provides fixed train/val/test splits.  

Scheduler Configuration per variation  
• ASHA-baseline: rung halving ratio η=3, max budget = full trace length.  
• PASHA-eps: ε estimated from past deltas (original paper).  
• SA-PASHA-full: τ=0.9, DRE ensemble size=10.  
• SA-PASHA-noDRE: confidence ρ computed from uniform noise → isolates surrogate benefit.  
• HYBAND: η=3, default θ parameters.  

Searcher  
• Random searcher and MOBSTER (BO) — each variation is executed twice (one per searcher).  

Repetitions & Selection  
• 5 random seeds × 2 searchers × 3 benchmarks = 30 runs per variation.  
• For each run we record full trace; report (a) final test metric of best-val configuration, (b) last-epoch metric.  

Evaluation Metrics  
Primary:  
• NASBench/Tabular – regret (lower is better) + accuracy (%)  
• WMT15 – BLEU ↑  
Secondary: FLOPs, wall-clock (s), completed configs, memory (MB).  
Robustness: coefficient of variation (CV) across seeds, performance under injected Gaussian score noise σ = {0.5 %, 1 %}.  

Hyper-parameter Sensitivity  
Grid over τ ∈ {0.8,0.9,0.95} for SA-PASHA; ensemble size ∈ {3,10}.  Analyse AUC(regret) vs τ and memory vs ensemble size.  

Computational Footprint  
• PyTorch-profile hooks compute per-step FLOPs and GPU memory.  
• Wall-clock captured via time.time(); averaged over 5 seeds.  

Robustness Tests  
1. Noise injection: add ε~N(0,σ) to validation metric before scheduler sees it.  
2. Distribution shift: retrain best architecture from CIFAR-10 on SVHN without re-search; report accuracy drop.  
3. Worker scaling: repeat NASBench-201 with 8 GPU workers on multi-node cluster (sanity check scalability).  

Success Criteria  
• Efficiency gain ≥10 % FLOPs vs PASHA on ≥70 % task/searcher pairs (paired t-test p<0.05).  
• Quality parity Δ≤1 % relative on all tasks.  
• CV not larger than PASHA on ≥80 % cases.  
• Surrogate overhead <5 % wall-clock, <150 MB GPU.  

Example Code Snippet  
```
from hpobench.schedulers import ASHA, PASHA, SAPASHA, HYBAND
from dre import DeepRankingEnsemble
from utils import profile

sched = SAPASHA(tau=0.9, dre=DeepRankingEnsemble(M=10))
for seed in range(5):
    for bench in BENCHES:
        for searcher in [Random(), MOBSTER()]:
            state = sched.init(bench, searcher, seed)
            while not state.budget_exhausted():
                cid, res = state.wait_next()
                state.update(cid, res)
            log_results(state, bench, seed)
profile.analyse('logs/*.json')
```
All experiments run on NVIDIA Tesla T4 (16 GB); DRE uses torch.float16 to stay within 150 MB VRAM.  

Expected Outcome  
SA-PASHA-full matches PASHA’s final accuracy/BLEU but reduces median FLOPs and wall-clock by 12-28 %. Ablation noDRE loses this advantage, confirming surrogate value. HYBAND is competitive in quality but 15-35 % slower.  


- Run Variations: ['ASHA-baseline', 'PASHA-eps', 'SA-PASHA-full', 'SA-PASHA-noDRE', 'HYBAND']

# Base Code
{'train_py': '"""src/train.py\nCore training logic supporting various schedulers (ASHA, PASHA, SA-PASHA, HYBAND).\nWrites TensorBoard logs for every variation run and is 100 % free of algorithmic\nplace-holders.  Dataset/model specifics are handled in preprocess.py / model.py.\n"""\nfrom __future__ import annotations\nimport argparse\nimport copy\nimport json\nimport math\nimport os\nimport pathlib\nimport random\nimport time\nfrom collections import defaultdict, deque\nfrom datetime import datetime\nfrom typing import Any, Dict, List, Tuple\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport yaml\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\nfrom .preprocess import get_dataset\nfrom .model import get_model\n\n# -----------------------------  generic helpers --------------------------------\n\ndef set_seed(seed: int) -> None:\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\ndef sample_config(search_space: Dict[str, Any]) -> Dict[str, Any]:\n    """Uniformly sample a configuration from the search-space dictionary.\n    Numerical ranges are specified as [low, high]  and sampled log-uniformly\n    if low>0 & logarithmic flag is present.  Categorical lists are sampled\n    uniformly. """\n    cfg = {}\n    for k, v in search_space.items():\n        if isinstance(v, list):\n            # numerical or categorical list\n            if all(isinstance(x, (int, float)) for x in v):\n                # treat as continuous interval [low, high]\n                low, high = v\n                if low > 0 and k.endswith("_log"):\n                    cfg[k] = float(10 ** np.random.uniform(math.log10(low), math.log10(high)))\n                else:\n                    cfg[k] = float(np.random.uniform(low, high))\n            else:\n                cfg[k] = random.choice(v)\n        else:\n            raise ValueError(f"Unsupported search space spec for key {k}: {v}")\n    return cfg\n\n\ndef encode_config(cfg: Dict[str, Any], search_space: Dict[str, Any]) -> np.ndarray:\n    """Simple numerical encoding that works generically across experiments.\n    • Continuous values – scaled to [0,1] using search-space min/max.\n    • Categoricals – one-hot encoded.\n    Returned vector is 1-D float32 numpy array. """\n    encoded: List[float] = []\n    for k, spec in search_space.items():\n        if isinstance(spec[0], (int, float)):\n            low, high = spec\n            val = float(cfg[k])\n            encoded.append((val - low) / (high - low + 1e-12))\n        else:  # categorical\n            one_hot = [0.0] * len(spec)\n            one_hot[spec.index(cfg[k])] = 1.0\n            encoded.extend(one_hot)\n    return np.asarray(encoded, dtype=np.float32)\n\n# --------------------------------- DRE surrogate --------------------------------\n\nclass RankNet(nn.Module):\n    def __init__(self, in_dim: int, hidden: int = 64):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(in_dim, hidden), nn.ReLU(), nn.Linear(hidden, hidden), nn.ReLU(), nn.Linear(hidden, 1)\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:  # (B, D)\n        return self.net(x).squeeze(-1)  # (B,)\n\n\nclass SurrogateEnsemble:\n    """Small Deep-Ranking-Ensemble (DRE) with pair-wise logistic loss."""\n\n    def __init__(self, input_dim: int, ensemble_size: int = 10, device: str = "cpu"):\n        self.device = device\n        self.models = [RankNet(input_dim).to(device) for _ in range(ensemble_size)]\n        self.optims = [optim.Adam(m.parameters(), lr=1e-3) for m in self.models]\n        self.criterion = nn.BCEWithLogitsLoss()\n\n    def _pairwise_batches(self, X: np.ndarray, y: np.ndarray, batch_size: int = 256):\n        n = len(X)\n        idx = np.arange(n)\n        # produce random pairs per epoch\n        np.random.shuffle(idx)\n        for i in range(0, n, batch_size):\n            a_idx = idx[i : i + batch_size]\n            b_idx = np.random.randint(0, n, size=len(a_idx))\n            xa, xb = X[a_idx], X[b_idx]\n            ya, yb = y[a_idx], y[b_idx]\n            target = (ya > yb).astype(np.float32)  # 1 if a better than b (assuming higher is better)\n            yield xa, xb, target\n\n    def fit(self, X: np.ndarray, y: np.ndarray, epochs: int = 1):\n        X_tensor = torch.from_numpy(X).float().to(self.device)\n        y_tensor = torch.from_numpy(y).float().to(self.device)\n        for model, opt in zip(self.models, self.optims):\n            model.train()\n            for _ in range(epochs):\n                for xa, xb, target in self._pairwise_batches(X, y):\n                    xa = torch.from_numpy(xa).float().to(self.device)\n                    xb = torch.from_numpy(xb).float().to(self.device)\n                    target = torch.from_numpy(target).float().to(self.device)\n                    opt.zero_grad()\n                    logits = model(xa) - model(xb)\n                    loss = self.criterion(logits, target)\n                    loss.backward()\n                    opt.step()\n\n    @torch.no_grad()\n    def predict(self, X: np.ndarray) -> np.ndarray:  # (E, N)\n        X_tensor = torch.from_numpy(X).float().to(self.device)\n        preds = []\n        for m in self.models:\n            preds.append(m(X_tensor).cpu().numpy())\n        return np.vstack(preds)  # (ensemble, N)\n\n    def sample_rankings(self, X: np.ndarray, M: int = 256) -> np.ndarray:\n        preds = self.predict(X)  # (E, N)\n        E = preds.shape[0]\n        rankings = []\n        for _ in range(M):\n            # pick random ensemble member and add tiny Gaussian noise for diversity\n            idx = np.random.randint(0, E)\n            scores = preds[idx] + np.random.randn(preds.shape[1]) * 1e-3\n            rankings.append(np.argsort(-scores))  # descending (higher is better)\n        return np.stack(rankings, axis=0)  # (M, N)\n\n# ------------------------------ Scheduler base-class ---------------------------\n\nclass BaseScheduler:\n    def __init__(self, search_space: Dict[str, Any], max_epochs: int, eta: int = 3, seed: int = 0):\n        self.search_space = search_space\n        self.max_epochs = max_epochs  # global maximum budget\n        self.eta = eta\n        self.rungs: Dict[int, List[Tuple[int, float]]] = defaultdict(list)  # resource -> list of (cid, metric)\n        self.configs: Dict[int, Dict[str, Any]] = {}\n        self.cid_counter = 0\n        self.hp_encodings: Dict[int, np.ndarray] = {}\n        self.seed = seed\n        self._promote_queue: deque = deque()\n        set_seed(seed)\n        self.history: List[Tuple[int, float, int]] = []  # (cid, score, resource)\n\n    # ----- user API -----\n    def suggest(self) -> Tuple[int, Dict[str, Any], int]:\n        if self._promote_queue:\n            cid, budget = self._promote_queue.popleft()\n            return cid, copy.deepcopy(self.configs[cid]), budget\n        cfg = sample_config(self.search_space)\n        cid = self.cid_counter\n        self.cid_counter += 1\n        self.configs[cid] = cfg\n        enc = encode_config(cfg, self.search_space)\n        self.hp_encodings[cid] = enc\n        return cid, cfg, self.min_budget()\n\n    def min_budget(self) -> int:  # to be overridden if needed\n        return 1\n\n    def report(self, cid: int, metric: float, budget: int):\n        self.rungs[budget].append((cid, metric))\n        self.history.append((cid, metric, budget))\n        self._maybe_promotion(budget)\n\n    def _maybe_promotion(self, budget: int):\n        raise NotImplementedError\n\n    def is_finished(self, max_trials: int) -> bool:\n        return len(self.configs) >= max_trials\n\n    def best_config(self) -> Tuple[Dict[str, Any], float]:\n        best = max(self.history, key=lambda t: t[1])  # assume higher better\n        cid, score, _ = best\n        return self.configs[cid], score\n\n# ------------------------------ ASHA ------------------------------------------\n\nclass ASHAScheduler(BaseScheduler):\n    def __init__(self, search_space: Dict[str, Any], max_epochs: int, eta: int = 3, seed: int = 0):\n        super().__init__(search_space, max_epochs, eta, seed)\n        # construct resource levels 1, eta, eta^2, ... <= max_epochs\n        self.budgets = []\n        b = 1\n        while b <= max_epochs:\n            self.budgets.append(b)\n            b *= eta\n\n    def _maybe_promotion(self, budget: int):\n        rung = self.rungs[budget]\n        k = len(rung)\n        next_idx = self.budgets.index(budget) + 1 if budget in self.budgets else None\n        if next_idx is None or next_idx >= len(self.budgets):\n            return  # top rung\n        promote_count = max(1, k // self.eta)\n        if promote_count == 0:\n            return\n        # promote top configurations\n        rung.sort(key=lambda t: t[1], reverse=True)\n        promotees = rung[:promote_count]\n        next_budget = self.budgets[next_idx]\n        for cid, _ in promotees:\n            self._promote_queue.append((cid, next_budget))\n\n# ------------------------------ PASHA (ε) -------------------------------------\n\nclass PASHAScheduler(ASHAScheduler):\n    def __init__(self, search_space: Dict[str, Any], max_epochs: int, eta: int = 3, seed: int = 0, eps: float | None = None):\n        super().__init__(search_space, max_epochs, eta, seed)\n        self.eps = eps\n        self.score_diffs: deque = deque(maxlen=50)  # keep last diffs to adapt ε\n\n    def _ranking_stable(self, top_scores: List[float], prev_scores: List[float]) -> bool:\n        diff = abs(np.mean(top_scores) - np.mean(prev_scores))\n        self.score_diffs.append(diff)\n        if self.eps is None:\n            # dynamic epsilon: median of past diffs (original PASHA heuristic)\n            eps = np.median(self.score_diffs) if self.score_diffs else 0.0\n        else:\n            eps = self.eps\n        return diff < eps\n\n    def _maybe_promotion(self, budget: int):\n        super()._maybe_promotion(budget)  # normal ASHA promotions\n        # after every promotion event, check whether budget equals top two rungs\n        rungs_sorted = sorted(self.rungs.keys())\n        if len(rungs_sorted) < 2:\n            return\n        top, prev = rungs_sorted[-1], rungs_sorted[-2]\n        top_scores = [m for (_, m) in self.rungs[top]]\n        prev_scores = [m for (_, m) in self.rungs[prev]]\n        if not self._ranking_stable(top_scores, prev_scores):\n            # double max resource\n            self._extend_budgets()\n\n    def _extend_budgets(self):\n        last = self.budgets[-1]\n        new_budget = min(self.max_epochs, last * 2)\n        if new_budget > last:\n            self.budgets.append(new_budget)\n\n# ------------------------------ SA-PASHA --------------------------------------\n\nclass SAPASHAScheduler(ASHAScheduler):\n    def __init__(\n        self,\n        search_space: Dict[str, Any],\n        max_epochs: int,\n        eta: int = 3,\n        seed: int = 0,\n        tau: float = 0.9,\n        ensemble: int = 10,\n        no_dre: bool = False,\n    ):\n        super().__init__(search_space, max_epochs, eta, seed)\n        self.tau = tau\n        self.no_dre = no_dre\n        if not no_dre:\n            # create surrogate once we know feature dimension (after first config encoded)\n            self.surrogate: SurrogateEnsemble | None = None\n        else:\n            self.surrogate = None\n\n    def _ensure_surrogate(self):\n        if self.surrogate is None:\n            any_enc = next(iter(self.hp_encodings.values()))\n            in_dim = len(any_enc)\n            device = "cuda" if torch.cuda.is_available() else "cpu"\n            self.surrogate = SurrogateEnsemble(in_dim, ensemble_size=10, device=device)\n\n    def _rho_confidence(self, configs: List[int]) -> float:\n        if self.no_dre:\n            # use random agreement (baseline)\n            K = len(configs)\n            return 1.0 / K  # intentional low confidence\n        self._ensure_surrogate()\n        X = np.stack([self.hp_encodings[c] for c in configs])\n        y = np.array([max([m for (cid, m, _) in self.history if cid == c]) for c in configs], dtype=np.float32)\n        self.surrogate.fit(X, y, epochs=1)  # incremental\n        ranks = self.surrogate.sample_rankings(X, M=128)  # (M, K)\n        K = len(configs)\n        agree = np.zeros((K, K), dtype=np.float32)\n        for r in ranks:\n            order = {cid: idx for idx, cid in enumerate(r)}\n            for i in range(K):\n                for j in range(i + 1, K):\n                    agree[i, j] += order[i] < order[j]\n                    agree[j, i] += order[j] < order[i]\n        agree /= ranks.shape[0]\n        rho = np.mean([(agree[i] > 0.5).sum() - 1 for i in range(K)]) / max(1, K - 1)\n        return rho\n\n    def _maybe_promotion(self, budget: int):\n        super()._maybe_promotion(budget)\n        rungs_sorted = sorted(self.rungs.keys())\n        if len(rungs_sorted) < 2:\n            return\n        top, prev = rungs_sorted[-1], rungs_sorted[-2]\n        configs_union = [cid for (cid, _) in self.rungs[top]] + [cid for (cid, _) in self.rungs[prev]]\n        rho = self._rho_confidence(configs_union)\n        if rho < self.tau:\n            self._extend_budgets()\n\n    def _extend_budgets(self):\n        last = self.budgets[-1]\n        new_budget = min(self.max_epochs, last * 2)\n        if new_budget > last:\n            self.budgets.append(new_budget)\n\n# ------------------------------ HYBAND (HyperBand) ----------------------------\n\nclass HYBANDScheduler(BaseScheduler):\n    """Synchronous HyperBand implementation (high-level baseline)."""\n\n    def __init__(self, search_space: Dict[str, Any], max_epochs: int, eta: int = 3, seed: int = 0):\n        super().__init__(search_space, max_epochs, eta, seed)\n        # compute n & s schedules as in HyperBand paper\n        self.s = int(math.floor(math.log(max_epochs, eta)))\n        self.brackets: List[deque] = [deque() for _ in range(self.s + 1)]\n        self.current_bracket = 0\n\n    def suggest(self):\n        # follow HyperBand bracket logic\n        if self.brackets[self.current_bracket]:\n            cid, budget = self.brackets[self.current_bracket].popleft()\n            return cid, copy.deepcopy(self.configs[cid]), budget\n        # else create new set of configs for this bracket\n        s = self.current_bracket\n        n = int(math.ceil((self.s + 1) / (s + 1) * self.eta ** s))\n        r = self.max_epochs / self.eta ** s\n        new_jobs = []\n        for _ in range(n):\n            cfg = sample_config(self.search_space)\n            cid = self.cid_counter\n            self.cid_counter += 1\n            self.configs[cid] = cfg\n            self.hp_encodings[cid] = encode_config(cfg, self.search_space)\n            new_jobs.append((cid, int(r)))\n        self.brackets[s].extend(new_jobs)\n        job = self.brackets[s].popleft()\n        return job\n\n    def report(self, cid: int, metric: float, budget: int):\n        super().report(cid, metric, budget)\n        # Not implementing intra-bracket promotions due to brevity; sufficient for baseline.\n\n    def _maybe_promotion(self, budget: int):\n        pass  # HyperBand uses scheduled promotions at bracket level (omitted for clarity)\n\n# ------------------------------ trainer ---------------------------------------\n\nclass Trainer:\n    def __init__(self, cfg: Dict[str, Any], variation: str, smoke: bool = False):\n        self.cfg = cfg\n        self.variation = variation\n        self.smoke = smoke\n        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")\n        self.output_dir = pathlib.Path("results") / variation / timestamp\n        self.output_dir.mkdir(parents=True, exist_ok=True)\n        self.writer = SummaryWriter(log_dir=str(self.output_dir / "tb"))\n        set_seed(cfg.get("seed", 0))\n        # data\n        self.train_ds, self.val_ds, input_dim, num_classes = get_dataset(cfg["dataset"], cfg)\n        self.device = "cuda" if torch.cuda.is_available() else "cpu"\n        self.search_space = cfg["search_space"]\n        self.max_epochs = cfg["max_epochs"]\n        self.max_trials = cfg["max_trials"]\n        self.scheduler = self._create_scheduler()\n        self.input_dim = input_dim\n        self.num_classes = num_classes\n\n    def _create_scheduler(self):\n        if self.variation == "ASHA-baseline":\n            return ASHAScheduler(self.search_space, self.max_epochs, eta=3, seed=self.cfg.get("seed", 0))\n        if self.variation == "PASHA-eps":\n            return PASHAScheduler(self.search_space, self.max_epochs, eta=3, seed=self.cfg.get("seed", 0))\n        if self.variation == "SA-PASHA-full":\n            return SAPASHAScheduler(self.search_space, self.max_epochs, eta=3, seed=self.cfg.get("seed", 0), tau=0.9, no_dre=False)\n        if self.variation == "SA-PASHA-noDRE":\n            return SAPASHAScheduler(self.search_space, self.max_epochs, eta=3, seed=self.cfg.get("seed", 0), tau=0.9, no_dre=True)\n        if self.variation == "HYBAND":\n            return HYBANDScheduler(self.search_space, self.max_epochs, eta=3, seed=self.cfg.get("seed", 0))\n        raise ValueError(f"Unsupported variation {self.variation}")\n\n    # -------- model training for a single configuration -----------------------\n    def _train_one(self, cfg: Dict[str, Any], budget: int) -> Tuple[float, float]:\n        model = get_model(self.cfg.get("model", "mlp"), self.input_dim, self.num_classes, cfg).to(self.device)\n        criterion = nn.CrossEntropyLoss()\n        optimizer = optim.Adam(model.parameters(), lr=cfg.get("lr", 1e-3))\n        train_loader = DataLoader(self.train_ds, batch_size=cfg.get("batch_size", 64), shuffle=True)\n        val_loader = DataLoader(self.val_ds, batch_size=256, shuffle=False)\n        model.train()\n        start = time.time()\n        for epoch in range(budget):\n            for xb, yb in train_loader:\n                xb, yb = xb.to(self.device), yb.to(self.device)\n                optimizer.zero_grad()\n                logits = model(xb)\n                loss = criterion(logits, yb)\n                loss.backward()\n                optimizer.step()\n        model.eval()\n        correct, total = 0, 0\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                xb, yb = xb.to(self.device), yb.to(self.device)\n                logits = model(xb)\n                pred = logits.argmax(dim=1)\n                correct += (pred == yb).sum().item()\n                total += yb.size(0)\n        accuracy = correct / total\n        wall_time = time.time() - start\n        return accuracy, wall_time\n\n    def _flops_estimate(self, model: nn.Module, n_samples: int, epochs: int) -> float:\n        params = sum(p.numel() for p in model.parameters())\n        return params * n_samples * epochs * 6  # rough forward+backward multiplier\n\n    # --------------------------------------------------------------------------\n    def run(self):\n        step = 0\n        while not self.scheduler.is_finished(self.max_trials):\n            cid, cfg, budget = self.scheduler.suggest()\n            acc, wall = self._train_one(cfg, budget)\n            self.scheduler.report(cid, acc, budget)\n            # logging\n            self.writer.add_scalar("acc", acc, global_step=step)\n            self.writer.add_scalar("budget", budget, global_step=step)\n            self.writer.add_scalar("best_acc", self.scheduler.best_config()[1], global_step=step)\n            step += 1\n            if self.smoke and step >= 5:\n                break\n        best_cfg, best_score = self.scheduler.best_config()\n        print("Experiment description:")\n        print(json.dumps({"variation": self.variation, "best_score": best_score, "best_cfg": best_cfg}, indent=2))\n        print(f"Best score: {best_score:.4f}")\n        print("TensorBoard logs stored in", self.output_dir)\n        self.writer.flush()\n        self.writer.close()\n        # save JSON summary\n        with open(self.output_dir / "summary.json", "w") as fp:\n            json.dump({"best_cfg": best_cfg, "best_score": best_score}, fp, indent=2)\n\n# ----------------------------------- CLI --------------------------------------\n\ndef main():\n    parser = argparse.ArgumentParser(description="Common-Core Experiment Trainer")\n    parser.add_argument("--smoke-test", action="store_true", help="run quick smoke test")\n    parser.add_argument("--full-experiment", action="store_true", help="run full experiment")\n    parser.add_argument("--variation", type=str, required=False, default="ASHA-baseline")\n    parser.add_argument("--config-path", type=str, default=None)\n    args = parser.parse_args()\n\n    if args.smoke_test and args.full_experiment:\n        raise ValueError("Choose either --smoke-test or --full-experiment")\n\n    if args.smoke_test:\n        cfg_path = args.config_path or "config/smoke_test.yaml"\n    else:\n        cfg_path = args.config_path or "config/full_experiment.yaml"\n    with open(cfg_path, "r") as fp:\n        cfg = yaml.safe_load(fp)\n    trainer = Trainer(cfg, args.variation, smoke=args.smoke_test)\n    trainer.run()\n\n\nif __name__ == "__main__":\n    main()\n', 'evaluate_py': '"""src/evaluate.py\nUnified evaluation & comparison script.  It reads TensorBoard event files from\nall variations in a directory and produces publication-quality .pdf figures that\ncompare best accuracy over wall-clock, accuracy vs. consumed epochs as well as\nfinal accuracy bar-plots.  All metrics are identical across experimental\nvariations to guarantee consistency.\n"""\nfrom __future__ import annotations\nimport argparse\nimport json\nimport pathlib\nfrom collections import defaultdict\nfrom typing import Dict, List\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom tensorboard.backend.event_processing import event_accumulator\n\nplt.rcParams.update({"pdf.fonttype": 42})  # publication friendly\n\n\nclass RunLog:\n    def __init__(self, tb_dir: pathlib.Path):\n        self.tb_dir = tb_dir\n        self.steps: List[int] = []\n        self.acc: List[float] = []\n        self.best_acc: List[float] = []\n        self._load()\n        self.final_best = self.best_acc[-1] if self.best_acc else 0.0\n\n    def _load(self):\n        ea = event_accumulator.EventAccumulator(str(self.tb_dir))\n        ea.Reload()\n        if "acc" not in ea.scalars.Keys():\n            return\n        for scalar in ea.scalars.Items("acc"):\n            self.steps.append(scalar.step)\n            self.acc.append(scalar.value)\n        if "best_acc" in ea.scalars.Keys():\n            self.best_acc = [s.value for s in ea.scalars.Items("best_acc")]\n\n\nclass Evaluator:\n    def __init__(self, results_dir: pathlib.Path):\n        self.results_dir = results_dir\n        self.runs: Dict[str, List[RunLog]] = defaultdict(list)\n        self._discover()\n\n    def _discover(self):\n        for variation_dir in self.results_dir.iterdir():\n            if not variation_dir.is_dir():\n                continue\n            # expect multiple timestamped sub-dirs\n            subdirs = [d for d in variation_dir.iterdir() if (d / "tb").exists()]\n            for sd in subdirs:\n                tb_path = sd / "tb"\n                self.runs[variation_dir.name].append(RunLog(tb_path))\n\n    def _aggregate(self):\n        summary = {}\n        for var, logs in self.runs.items():\n            best_scores = [run.final_best for run in logs]\n            summary[var] = {\n                "mean": float(np.mean(best_scores)),\n                "std": float(np.std(best_scores)),\n                "runs": best_scores,\n            }\n        return summary\n\n    # ------------------------- plotting helpers ------------------------------\n    def _plot_final_bar(self, summary: Dict[str, Dict[str, float]]):\n        fig, ax = plt.subplots(figsize=(6, 4))\n        vars_ = list(summary.keys())\n        means = [summary[v]["mean"] for v in vars_]\n        stds = [summary[v]["std"] for v in vars_]\n        ax.bar(vars_, means, yerr=stds, color=sns.color_palette("Set2", len(vars_)))\n        for idx, m in enumerate(means):\n            ax.text(idx, m + 0.005, f"{m:.3f}", ha="center", va="bottom")\n        ax.set_ylabel("Final Best Accuracy")\n        ax.set_title("Scheduler Comparison")\n        ax.set_ylim(0, 1)\n        fig.tight_layout()\n        out_path = self.results_dir / "final_accuracy.pdf"\n        fig.savefig(out_path, bbox_inches="tight")\n        print("Saved", out_path)\n\n    def _plot_learning_curves(self):\n        fig, ax = plt.subplots(figsize=(6, 4))\n        for var, logs in self.runs.items():\n            # average across runs (interpolate to common steps)\n            if not logs[0].steps:\n                continue\n            max_step = max(max(r.steps) for r in logs)\n            common = np.linspace(0, max_step, 200)\n            curves = []\n            for run in logs:\n                if not run.steps:\n                    continue\n                curves.append(np.interp(common, run.steps, run.best_acc))\n            mean = np.mean(curves, axis=0)\n            ax.plot(common, mean, label=var)\n            ax.annotate(f"{mean[-1]:.3f}", (common[-1], mean[-1]))\n        ax.set_xlabel("Scheduler step")\n        ax.set_ylabel("Best Accuracy")\n        ax.set_title("Best Accuracy vs Scheduler Step")\n        ax.legend()\n        fig.tight_layout()\n        out_path = self.results_dir / "accuracy_vs_steps.pdf"\n        fig.savefig(out_path, bbox_inches="tight")\n        print("Saved", out_path)\n\n    def run(self):\n        summary = self._aggregate()\n        # print numerical data\n        print("Experiment description:")\n        print(json.dumps(summary, indent=2))\n        self._plot_final_bar(summary)\n        self._plot_learning_curves()\n\n\n# -------------------------------- CLI ----------------------------------------\n\ndef main():\n    parser = argparse.ArgumentParser(description="Evaluate & compare results")\n    parser.add_argument("--results-dir", required=True, type=str)\n    args = parser.parse_args()\n    evaluator = Evaluator(pathlib.Path(args.results_dir))\n    evaluator.run()\n\n\nif __name__ == "__main__":\n    main()\n', 'preprocess_py': '"""src/preprocess.py\nCommon data-loading & preprocessing utilities.  Supports a synthetic\nclassification data set for smoke-tests and defines clear placeholders for\nbenchmark-specific data sets that will be plugged in during later experiment\nphases.\n"""\nfrom __future__ import annotations\nimport math\nimport random\nfrom typing import Any, Dict, Tuple\n\nimport numpy as np\nimport torch\nfrom sklearn.datasets import make_classification\nfrom torch.utils.data import Dataset\n\n# -------------------------------- synthetic data -----------------------------\n\nclass SyntheticClassificationDataset(Dataset):\n    def __init__(self, n_samples: int = 2000, n_features: int = 20, n_classes: int = 2, seed: int = 0):\n        X, y = make_classification(\n            n_samples=n_samples,\n            n_features=n_features,\n            n_informative=int(n_features * 0.6),\n            n_redundant=int(n_features * 0.2),\n            n_classes=n_classes,\n            random_state=seed,\n        )\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx: int):\n        return self.X[idx], self.y[idx]\n\n\n# ------------------------------- dataset factory -----------------------------\n\ndef get_dataset(name: str, cfg: Dict[str, Any]) -> Tuple[Dataset, Dataset, int, int]:\n    """Returns train-set, val-set, input_dim, num_classes"""\n    if name == "synthetic":\n        seed = cfg.get("seed", 0)\n        n_features = cfg.get("n_features", 20)\n        n_classes = cfg.get("n_classes", 2)\n        full_ds = SyntheticClassificationDataset(2000, n_features, n_classes, seed=seed)\n        # 80/20 split\n        n_train = int(0.8 * len(full_ds))\n        n_val = len(full_ds) - n_train\n        train_ds, val_ds = torch.utils.data.random_split(full_ds, [n_train, n_val])\n        return train_ds, val_ds, n_features, n_classes\n    # ------------------- PLACEHOLDER for benchmark-specific datasets ----------\n    # PLACEHOLDER: Will be replaced with specific dataset loading logic (e.g. NASBench-201, LCBench, etc.)\n    raise NotImplementedError(f"Dataset \'{name}\' not implemented yet.  This is the only placeholder in preprocessing.")\n', 'model_py': '"""src/model.py\nModel factory providing lightweight PyTorch networks.  Works with both the\nsynthetic data used in smoke-tests and larger data sets that will be plugged in\nlater.  Uses no placeholders in core logic.\n"""\nfrom __future__ import annotations\nimport math\nfrom typing import Any, Dict\n\nimport torch\nimport torch.nn as nn\n\n\nclass MLP(nn.Module):\n    def __init__(self, input_dim: int, num_classes: int, hidden_dim: int = 64, dropout: float = 0.0):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, num_classes),\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.net(x)\n\n\n# ----------------------- model factory ---------------------------------------\n\ndef get_model(name: str, input_dim: int, num_classes: int, cfg: Dict[str, Any]):\n    name = name.lower()\n    if name == "mlp":\n        hidden = int(cfg.get("hidden_dim", 64))\n        dropout = float(cfg.get("dropout", 0.0))\n        return MLP(input_dim, num_classes, hidden_dim=hidden, dropout=dropout)\n    # PLACEHOLDER: dataset-specific sophisticated architectures will be inserted here\n    raise NotImplementedError(f"Model \'{name}\' is not available in the common core foundation.")\n', 'main_py': '"""src/main.py\nCommand-line entry point.  Delegates to train.py (training) or evaluate.py\n(evaluation) depending on arguments.  Ensures the CLI contract required by the\nworkflow is fully respected.\n"""\nimport argparse\nimport importlib\nimport sys\n\n\ndef main():\n    parser = argparse.ArgumentParser(description="COMMON CORE FOUNDATION entry-point")\n    sub = parser.add_subparsers(dest="mode")\n\n    # training modes -----------------------------------------------------------\n    train_p = sub.add_parser("train", help="run training experiment")\n    train_p.add_argument("--smoke-test", action="store_true", help="quick validation run")\n    train_p.add_argument("--full-experiment", action="store_true", help="full benchmark run")\n    train_p.add_argument("--variation", type=str, required=False, default="ASHA-baseline")\n    train_p.add_argument("--config-path", type=str, default=None)\n\n    # evaluation ---------------------------------------------------------------\n    eval_p = sub.add_parser("evaluate", help="evaluate directory of runs")\n    eval_p.add_argument("--results-dir", type=str, required=True)\n\n    args = parser.parse_args()\n\n    if args.mode == "train":\n        module = importlib.import_module("src.train")\n        sys.argv = ["train"]  # clean argv for sub-module parser\n        if args.smoke_test:\n            sys.argv += ["--smoke-test"]\n        if args.full_experiment:\n            sys.argv += ["--full-experiment"]\n        sys.argv += ["--variation", args.variation]\n        if args.config_path:\n            sys.argv += ["--config-path", args.config_path]\n        module.main()\n    elif args.mode == "evaluate":\n        module = importlib.import_module("src.evaluate")\n        sys.argv = ["evaluate", "--results-dir", args.results_dir]\n        module.main()\n    else:\n        parser.print_help()\n\n\nif __name__ == "__main__":\n    main()\n', 'pyproject_toml': '[build-system]\nrequires = ["setuptools>=61.0"]\nbuild-backend = "setuptools.build_meta"\n\n[project]\nname = "common-core-foundation"\nversion = "0.1.0"\ndescription = "Unified experimental framework for SA-PASHA vs baselines"\nrequires-python = ">=3.10"\ndependencies = [\n    "torch>=2.0.0",\n    "torchvision>=0.15.0",\n    "numpy>=1.24",\n    "scikit-learn>=1.3",\n    "pyyaml>=6.0",\n    "tqdm>=4.66",\n    "tensorboard>=2.14",\n    "matplotlib>=3.8",\n    "seaborn>=0.13",\n    "pandas>=2.1",\n]\n\n[tool.setuptools.packages.find]\nwhere = ["src"]\n', 'smoke_test_yaml': '# config/smoke_test.yaml\nseed: 0\ndataset: synthetic\nmodel: mlp\nn_features: 20\nn_classes: 2\nmax_epochs: 16\nmax_trials: 20\nsearch_space:\n  lr: [0.0001, 0.01]\n  hidden_dim: [32, 64]\n  dropout: [0.0, 0.3]\n', 'full_experiment_yaml': '# config/full_experiment.yaml\nseed: 0\n# PLACEHOLDER: specify concrete dataset name, e.g. nasbench201_cifar10\ndataset: DATASET_PLACEHOLDER\nmodel: MODEL_PLACEHOLDER\nmax_epochs: 200\nmax_trials: 100\nsearch_space: SPECIFIC_SEARCH_SPACE_PLACEHOLDER  # to be replaced in subsequent steps\n'}

# External Resources (Use these to replace placeholders)

**HuggingFace Models (Replace MODEL_PLACEHOLDER with these):**

- ID: th1enq/xgboost_checkpoint

- Code: import joblib
from huggingface_hub import hf_hub_download

model_path = hf_hub_download(repo_id="th1enq/xgboost_checkpoint", filename="xgboost phishing detection models.joblib")

model = joblib.load(model_path)

predictions = model.predict(X_test)


- ID: rachana-rj-news-classification/xgboost-tfidf-classifier


- ID: elawrie/rnn-xgboost-classifier



**HuggingFace Datasets (Replace DATASET_PLACEHOLDER with these):**

- ID: uoft-cs/cifar10


- ID: uoft-cs/cifar100


- ID: tanganke/cifar100


- ID: tanganke/cifar10


- ID: kjj0/cifar10-multirun-logits

- Code: import torch
import torchvision
import huggingface_hub

def normalize(logits):
    logits = logits.float()
    logits = logits.log_softmax(-1)
    logits = (logits - logits.mean(0, keepdim=True)) / logits.std(0, keepdim=True)
    return logits

def compute_kernel(logits1, logits2):
    
    logits1 = normalize(logits1)
    logits2 = normalize(logits2)
    assert len(logits1) == len(logits2)
    
    kernel = torch.zeros(logits1.shape[1], logits2.shape[1]).cuda()
    for c in range(10):
        logits1_cls = logits1[..., c].cuda()
        logits2_cls = logits2[..., c].cuda()
        corr_cls = (logits1_cls.T @ logits2_cls) / len(logits1)
        kernel += corr_cls / 10
    return kernel

######################################################################################
#  Setup: Download CIFAR-10 labels and the outputs from 500 repeated training runs.  #
######################################################################################

labels_train = torch.tensor(torchvision.datasets.CIFAR10('cifar10', train=True).targets)
labels_test = torch.tensor(torchvision.datasets.CIFAR10('cifar10', train=False).targets)

api = huggingface_hub.HfApi()
fname = 'logs_saveoutputs_main/06109e85-f5d7-4ac8-b0b0-f03542f23234/log.pt'
obj_path = api.hf_hub_download('kjj0/cifar10-multirun-logits', repo_type='dataset',
                               filename=fname)
obj = torch.load(obj_path, map_location='cpu')

# print(obj['code']) # Uncomment if you want to see the training code

######################################################################################
#     Evaluate both the per-model and ensembled accuracy of the training outputs.    #
######################################################################################

each_acc = (obj['logits'].argmax(-1) == labels_test).float().mean(1)
avg_acc = each_acc.mean()
print('average single-model accuracy \t: %.2f' % (100 * avg_acc))

ens_pred = obj['logits'].mean(0).argmax(1)
ens_acc = (ens_pred == labels_test).float().mean()
print('ensemble accuracy (%d models) \t: %.2f' % (len(obj['logits']), 100 * ens_acc))
# (n.b. averaging probabilities instead of logits makes no difference)

######################################################################################
#               Evaluate the new kernel / ensemble inference method.                 #
######################################################################################

# use correlations between log_softmax outputs as a similarity metric for k-NN inference.
kernel = compute_kernel(obj['logits'], obj['logits_train'])
k = 3
nbrs = kernel.topk(k, dim=1)
nbr_labels = labels_train[nbrs.indices.cpu()]
pred = nbr_labels.mode(1).values
acc = (pred == labels_test).float().mean()
print('kernel accuracy (k-NN w/ k=%d) \t: %.2f' % (k, 100 * acc))

## average single-model accuracy   : 93.26
## ensemble accuracy (500 models)  : 94.69
## kernel accuracy (k-NN w/ k=3)   : 95.01


- ID: noahshinn/cifar100_2_to_100


- ID: uoft-cs/cifar100


- ID: tanganke/cifar100


- ID: noahshinn/cifar100_2_to_100


- ID: tomas-gajarsky/cifar100-lt


- ID: ufldl-stanford/svhn


- ID: dpdl-benchmark/svhn_cropped_balanced


- ID: dpdl-benchmark/svhn_cropped





# Common Base Code (Previous Step Output to be specialized)
The common base code generated in the previous step contains the following placeholder patterns that need to be replaced:
- `DATASET_PLACEHOLDER` → Replace with specific Hugging Face dataset loading
- `MODEL_PLACEHOLDER` → Replace with specific model architecture
- `SPECIFIC_CONFIG_PLACEHOLDER` → Replace with actual experimental parameters



Take the foundation code and create complete, specialized experiments using the External Resources specified above.
Output:
{
    "train_py": "\"\"\"src/train.py\nCore training logic supporting various schedulers (ASHA, PASHA, SA-PASHA, HYBAND).\nWrites TensorBoard logs for every variation run and is 100 % free of algorithmic\nplace-holders.  Dataset/model specifics are handled in preprocess.py / model.py.\n\"\"\"\nfrom __future__ import annotations\nimport argparse\nimport copy\nimport json\nimport math\nimport os\nimport pathlib\nimport random\nimport time\nfrom collections import defaultdict, deque\nfrom datetime import datetime\nfrom typing import Any, Dict, List, Tuple\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport yaml\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\nfrom .preprocess import get_dataset\nfrom .model import get_model\n\n# -----------------------------  generic helpers --------------------------------\n\ndef set_seed(seed: int) -> None:\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\ndef sample_config(search_space: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"Uniformly sample a configuration from the search-space dictionary.\n    Numerical ranges are specified as [low, high]  and sampled log-uniformly\n    if low>0 & logarithmic flag is present.  Categorical lists are sampled\n    uniformly. \"\"\"\n    cfg = {}\n    for k, v in search_space.items():\n        if isinstance(v, list):\n            # numerical or categorical list\n            if all(isinstance(x, (int, float)) for x in v):\n                # treat as continuous interval [low, high]\n                low, high = v\n                if low > 0 and k.endswith(\"_log\"):\n                    cfg[k] = float(10 ** np.random.uniform(math.log10(low), math.log10(high)))\n                else:\n                    cfg[k] = float(np.random.uniform(low, high))\n            else:\n                cfg[k] = random.choice(v)\n        else:\n            raise ValueError(f\"Unsupported search space spec for key {k}: {v}\")\n    return cfg\n\n\ndef encode_config(cfg: Dict[str, Any], search_space: Dict[str, Any]) -> np.ndarray:\n    \"\"\"Simple numerical encoding that works generically across experiments.\n    • Continuous values – scaled to [0,1] using search-space min/max.\n    • Categoricals – one-hot encoded.\n    Returned vector is 1-D float32 numpy array. \"\"\"\n    encoded: List[float] = []\n    for k, spec in search_space.items():\n        if isinstance(spec[0], (int, float)):\n            low, high = spec\n            val = float(cfg[k])\n            encoded.append((val - low) / (high - low + 1e-12))\n        else:  # categorical\n            one_hot = [0.0] * len(spec)\n            one_hot[spec.index(cfg[k])] = 1.0\n            encoded.extend(one_hot)\n    return np.asarray(encoded, dtype=np.float32)\n\n# --------------------------------- DRE surrogate --------------------------------\n\nclass RankNet(nn.Module):\n    def __init__(self, in_dim: int, hidden: int = 64):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(in_dim, hidden), nn.ReLU(), nn.Linear(hidden, hidden), nn.ReLU(), nn.Linear(hidden, 1)\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:  # (B, D)\n        return self.net(x).squeeze(-1)  # (B,)\n\n\nclass SurrogateEnsemble:\n    \"\"\"Small Deep-Ranking-Ensemble (DRE) with pair-wise logistic loss.\"\"\"\n\n    def __init__(self, input_dim: int, ensemble_size: int = 10, device: str = \"cpu\"):\n        self.device = device\n        self.models = [RankNet(input_dim).to(device) for _ in range(ensemble_size)]\n        self.optims = [optim.Adam(m.parameters(), lr=1e-3) for m in self.models]\n        self.criterion = nn.BCEWithLogitsLoss()\n\n    def _pairwise_batches(self, X: np.ndarray, y: np.ndarray, batch_size: int = 256):\n        n = len(X)\n        idx = np.arange(n)\n        # produce random pairs per epoch\n        np.random.shuffle(idx)\n        for i in range(0, n, batch_size):\n            a_idx = idx[i : i + batch_size]\n            b_idx = np.random.randint(0, n, size=len(a_idx))\n            xa, xb = X[a_idx], X[b_idx]\n            ya, yb = y[a_idx], y[b_idx]\n            target = (ya > yb).astype(np.float32)  # 1 if a better than b (assuming higher is better)\n            yield xa, xb, target\n\n    def fit(self, X: np.ndarray, y: np.ndarray, epochs: int = 1):\n        X_tensor = torch.from_numpy(X).float().to(self.device)\n        y_tensor = torch.from_numpy(y).float().to(self.device)\n        for model, opt in zip(self.models, self.optims):\n            model.train()\n            for _ in range(epochs):\n                for xa, xb, target in self._pairwise_batches(X, y):\n                    xa = torch.from_numpy(xa).float().to(self.device)\n                    xb = torch.from_numpy(xb).float().to(self.device)\n                    target = torch.from_numpy(target).float().to(self.device)\n                    opt.zero_grad()\n                    logits = model(xa) - model(xb)\n                    loss = self.criterion(logits, target)\n                    loss.backward()\n                    opt.step()\n\n    @torch.no_grad()\n    def predict(self, X: np.ndarray) -> np.ndarray:  # (E, N)\n        X_tensor = torch.from_numpy(X).float().to(self.device)\n        preds = []\n        for m in self.models:\n            preds.append(m(X_tensor).cpu().numpy())\n        return np.vstack(preds)  # (ensemble, N)\n\n    def sample_rankings(self, X: np.ndarray, M: int = 256) -> np.ndarray:\n        preds = self.predict(X)  # (E, N)\n        E = preds.shape[0]\n        rankings = []\n        for _ in range(M):\n            # pick random ensemble member and add tiny Gaussian noise for diversity\n            idx = np.random.randint(0, E)\n            scores = preds[idx] + np.random.randn(preds.shape[1]) * 1e-3\n            rankings.append(np.argsort(-scores))  # descending (higher is better)\n        return np.stack(rankings, axis=0)  # (M, N)\n\n# ------------------------------ Scheduler base-class ---------------------------\n\nclass BaseScheduler:\n    def __init__(self, search_space: Dict[str, Any], max_epochs: int, eta: int = 3, seed: int = 0):\n        self.search_space = search_space\n        self.max_epochs = max_epochs  # global maximum budget\n        self.eta = eta\n        self.rungs: Dict[int, List[Tuple[int, float]]] = defaultdict(list)  # resource -> list of (cid, metric)\n        self.configs: Dict[int, Dict[str, Any]] = {}\n        self.cid_counter = 0\n        self.hp_encodings: Dict[int, np.ndarray] = {}\n        self.seed = seed\n        self._promote_queue: deque = deque()\n        set_seed(seed)\n        self.history: List[Tuple[int, float, int]] = []  # (cid, score, resource)\n\n    # ----- user API -----\n    def suggest(self) -> Tuple[int, Dict[str, Any], int]:\n        if self._promote_queue:\n            cid, budget = self._promote_queue.popleft()\n            return cid, copy.deepcopy(self.configs[cid]), budget\n        cfg = sample_config(self.search_space)\n        cid = self.cid_counter\n        self.cid_counter += 1\n        self.configs[cid] = cfg\n        enc = encode_config(cfg, self.search_space)\n        self.hp_encodings[cid] = enc\n        return cid, cfg, self.min_budget()\n\n    def min_budget(self) -> int:  # to be overridden if needed\n        return 1\n\n    def report(self, cid: int, metric: float, budget: int):\n        self.rungs[budget].append((cid, metric))\n        self.history.append((cid, metric, budget))\n        self._maybe_promotion(budget)\n\n    def _maybe_promotion(self, budget: int):\n        raise NotImplementedError\n\n    def is_finished(self, max_trials: int) -> bool:\n        return len(self.configs) >= max_trials\n\n    def best_config(self) -> Tuple[Dict[str, Any], float]:\n        best = max(self.history, key=lambda t: t[1])  # assume higher better\n        cid, score, _ = best\n        return self.configs[cid], score\n\n# ------------------------------ ASHA ------------------------------------------\n\nclass ASHAScheduler(BaseScheduler):\n    def __init__(self, search_space: Dict[str, Any], max_epochs: int, eta: int = 3, seed: int = 0):\n        super().__init__(search_space, max_epochs, eta, seed)\n        # construct resource levels 1, eta, eta^2, ... <= max_epochs\n        self.budgets = []\n        b = 1\n        while b <= max_epochs:\n            self.budgets.append(b)\n            b *= eta\n\n    def _maybe_promotion(self, budget: int):\n        rung = self.rungs[budget]\n        k = len(rung)\n        next_idx = self.budgets.index(budget) + 1 if budget in self.budgets else None\n        if next_idx is None or next_idx >= len(self.budgets):\n            return  # top rung\n        promote_count = max(1, k // self.eta)\n        if promote_count == 0:\n            return\n        # promote top configurations\n        rung.sort(key=lambda t: t[1], reverse=True)\n        promotees = rung[:promote_count]\n        next_budget = self.budgets[next_idx]\n        for cid, _ in promotees:\n            self._promote_queue.append((cid, next_budget))\n\n# ------------------------------ PASHA (ε) -------------------------------------\n\nclass PASHAScheduler(ASHAScheduler):\n    def __init__(self, search_space: Dict[str, Any], max_epochs: int, eta: int = 3, seed: int = 0, eps: float | None = None):\n        super().__init__(search_space, max_epochs, eta, seed)\n        self.eps = eps\n        self.score_diffs: deque = deque(maxlen=50)  # keep last diffs to adapt ε\n\n    def _ranking_stable(self, top_scores: List[float], prev_scores: List[float]) -> bool:\n        diff = abs(np.mean(top_scores) - np.mean(prev_scores))\n        self.score_diffs.append(diff)\n        if self.eps is None:\n            # dynamic epsilon: median of past diffs (original PASHA heuristic)\n            eps = np.median(self.score_diffs) if self.score_diffs else 0.0\n        else:\n            eps = self.eps\n        return diff < eps\n\n    def _maybe_promotion(self, budget: int):\n        super()._maybe_promotion(budget)  # normal ASHA promotions\n        # after every promotion event, check whether budget equals top two rungs\n        rungs_sorted = sorted(self.rungs.keys())\n        if len(rungs_sorted) < 2:\n            return\n        top, prev = rungs_sorted[-1], rungs_sorted[-2]\n        top_scores = [m for (_, m) in self.rungs[top]]\n        prev_scores = [m for (_, m) in self.rungs[prev]]\n        if not self._ranking_stable(top_scores, prev_scores):\n            # double max resource\n            self._extend_budgets()\n\n    def _extend_budgets(self):\n        last = self.budgets[-1]\n        new_budget = min(self.max_epochs, last * 2)\n        if new_budget > last:\n            self.budgets.append(new_budget)\n\n# ------------------------------ SA-PASHA --------------------------------------\n\nclass SAPASHAScheduler(ASHAScheduler):\n    def __init__(\n        self,\n        search_space: Dict[str, Any],\n        max_epochs: int,\n        eta: int = 3,\n        seed: int = 0,\n        tau: float = 0.9,\n        ensemble: int = 10,\n        no_dre: bool = False,\n    ):\n        super().__init__(search_space, max_epochs, eta, seed)\n        self.tau = tau\n        self.no_dre = no_dre\n        if not no_dre:\n            # create surrogate once we know feature dimension (after first config encoded)\n            self.surrogate: SurrogateEnsemble | None = None\n        else:\n            self.surrogate = None\n\n    def _ensure_surrogate(self):\n        if self.surrogate is None:\n            any_enc = next(iter(self.hp_encodings.values()))\n            in_dim = len(any_enc)\n            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n            self.surrogate = SurrogateEnsemble(in_dim, ensemble_size=10, device=device)\n\n    def _rho_confidence(self, configs: List[int]) -> float:\n        if self.no_dre:\n            # use random agreement (baseline)\n            K = len(configs)\n            return 1.0 / K  # intentional low confidence\n        self._ensure_surrogate()\n        X = np.stack([self.hp_encodings[c] for c in configs])\n        y = np.array([max([m for (cid, m, _) in self.history if cid == c]) for c in configs], dtype=np.float32)\n        self.surrogate.fit(X, y, epochs=1)  # incremental\n        ranks = self.surrogate.sample_rankings(X, M=128)  # (M, K)\n        K = len(configs)\n        agree = np.zeros((K, K), dtype=np.float32)\n        for r in ranks:\n            order = {cid: idx for idx, cid in enumerate(r)}\n            for i in range(K):\n                for j in range(i + 1, K):\n                    agree[i, j] += order[i] < order[j]\n                    agree[j, i] += order[j] < order[i]\n        agree /= ranks.shape[0]\n        rho = np.mean([(agree[i] > 0.5).sum() - 1 for i in range(K)]) / max(1, K - 1)\n        return rho\n\n    def _maybe_promotion(self, budget: int):\n        super()._maybe_promotion(budget)\n        rungs_sorted = sorted(self.rungs.keys())\n        if len(rungs_sorted) < 2:\n            return\n        top, prev = rungs_sorted[-1], rungs_sorted[-2]\n        configs_union = [cid for (cid, _) in self.rungs[top]] + [cid for (cid, _) in self.rungs[prev]]\n        rho = self._rho_confidence(configs_union)\n        if rho < self.tau:\n            self._extend_budgets()\n\n    def _extend_budgets(self):\n        last = self.budgets[-1]\n        new_budget = min(self.max_epochs, last * 2)\n        if new_budget > last:\n            self.budgets.append(new_budget)\n\n# ------------------------------ HYBAND (HyperBand) ----------------------------\n\nclass HYBANDScheduler(BaseScheduler):\n    \"\"\"Synchronous HyperBand implementation (high-level baseline).\"\"\"\n\n    def __init__(self, search_space: Dict[str, Any], max_epochs: int, eta: int = 3, seed: int = 0):\n        super().__init__(search_space, max_epochs, eta, seed)\n        # compute n & s schedules as in HyperBand paper\n        self.s = int(math.floor(math.log(max_epochs, eta)))\n        self.brackets: List[deque] = [deque() for _ in range(self.s + 1)]\n        self.current_bracket = 0\n\n    def suggest(self):\n        # follow HyperBand bracket logic\n        if self.brackets[self.current_bracket]:\n            cid, budget = self.brackets[self.current_bracket].popleft()\n            return cid, copy.deepcopy(self.configs[cid]), budget\n        # else create new set of configs for this bracket\n        s = self.current_bracket\n        n = int(math.ceil((self.s + 1) / (s + 1) * self.eta ** s))\n        r = self.max_epochs / self.eta ** s\n        new_jobs = []\n        for _ in range(n):\n            cfg = sample_config(self.search_space)\n            cid = self.cid_counter\n            self.cid_counter += 1\n            self.configs[cid] = cfg\n            self.hp_encodings[cid] = encode_config(cfg, self.search_space)\n            new_jobs.append((cid, int(r)))\n        self.brackets[s].extend(new_jobs)\n        job = self.brackets[s].popleft()\n        return job\n\n    def report(self, cid: int, metric: float, budget: int):\n        super().report(cid, metric, budget)\n        # Not implementing intra-bracket promotions due to brevity; sufficient for baseline.\n\n    def _maybe_promotion(self, budget: int):\n        pass  # HyperBand uses scheduled promotions at bracket level (omitted for clarity)\n\n# ------------------------------ trainer ---------------------------------------\n\nclass Trainer:\n    def __init__(self, cfg: Dict[str, Any], variation: str, smoke: bool = False):\n        self.cfg = cfg\n        self.variation = variation\n        self.smoke = smoke\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        self.output_dir = pathlib.Path(\"results\") / variation / timestamp\n        self.output_dir.mkdir(parents=True, exist_ok=True)\n        self.writer = SummaryWriter(log_dir=str(self.output_dir / \"tb\"))\n        set_seed(cfg.get(\"seed\", 0))\n        # data\n        self.train_ds, self.val_ds, input_dim, num_classes = get_dataset(cfg[\"dataset\"], cfg)\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self.search_space = cfg[\"search_space\"]\n        self.max_epochs = cfg[\"max_epochs\"]\n        self.max_trials = cfg[\"max_trials\"]\n        self.scheduler = self._create_scheduler()\n        self.input_dim = input_dim\n        self.num_classes = num_classes\n\n    def _create_scheduler(self):\n        if self.variation == \"ASHA-baseline\":\n            return ASHAScheduler(self.search_space, self.max_epochs, eta=3, seed=self.cfg.get(\"seed\", 0))\n        if self.variation == \"PASHA-eps\":\n            return PASHAScheduler(self.search_space, self.max_epochs, eta=3, seed=self.cfg.get(\"seed\", 0))\n        if self.variation == \"SA-PASHA-full\":\n            return SAPASHAScheduler(self.search_space, self.max_epochs, eta=3, seed=self.cfg.get(\"seed\", 0), tau=0.9, no_dre=False)\n        if self.variation == \"SA-PASHA-noDRE\":\n            return SAPASHAScheduler(self.search_space, self.max_epochs, eta=3, seed=self.cfg.get(\"seed\", 0), tau=0.9, no_dre=True)\n        if self.variation == \"HYBAND\":\n            return HYBANDScheduler(self.search_space, self.max_epochs, eta=3, seed=self.cfg.get(\"seed\", 0))\n        raise ValueError(f\"Unsupported variation {self.variation}\")\n\n    # -------- model training for a single configuration -----------------------\n    def _train_one(self, cfg: Dict[str, Any], budget: int) -> Tuple[float, float]:\n        model = get_model(self.cfg.get(\"model\", \"mlp\"), self.input_dim, self.num_classes, cfg).to(self.device)\n        criterion = nn.CrossEntropyLoss()\n        optimizer = optim.Adam(model.parameters(), lr=cfg.get(\"lr\", 1e-3))\n        train_loader = DataLoader(self.train_ds, batch_size=cfg.get(\"batch_size\", 64), shuffle=True)\n        val_loader = DataLoader(self.val_ds, batch_size=256, shuffle=False)\n        model.train()\n        start = time.time()\n        for epoch in range(budget):\n            for xb, yb in train_loader:\n                xb, yb = xb.to(self.device), yb.to(self.device)\n                optimizer.zero_grad()\n                logits = model(xb)\n                loss = criterion(logits, yb)\n                loss.backward()\n                optimizer.step()\n        model.eval()\n        correct, total = 0, 0\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                xb, yb = xb.to(self.device), yb.to(self.device)\n                logits = model(xb)\n                pred = logits.argmax(dim=1)\n                correct += (pred == yb).sum().item()\n                total += yb.size(0)\n        accuracy = correct / total\n        wall_time = time.time() - start\n        return accuracy, wall_time\n\n    def _flops_estimate(self, model: nn.Module, n_samples: int, epochs: int) -> float:\n        params = sum(p.numel() for p in model.parameters())\n        return params * n_samples * epochs * 6  # rough forward+backward multiplier\n\n    # --------------------------------------------------------------------------\n    def run(self):\n        step = 0\n        while not self.scheduler.is_finished(self.max_trials):\n            cid, cfg, budget = self.scheduler.suggest()\n            acc, wall = self._train_one(cfg, budget)\n            self.scheduler.report(cid, acc, budget)\n            # logging\n            self.writer.add_scalar(\"acc\", acc, global_step=step)\n            self.writer.add_scalar(\"budget\", budget, global_step=step)\n            self.writer.add_scalar(\"best_acc\", self.scheduler.best_config()[1], global_step=step)\n            step += 1\n            if self.smoke and step >= 5:\n                break\n        best_cfg, best_score = self.scheduler.best_config()\n        print(\"Experiment description:\")\n        print(json.dumps({\"variation\": self.variation, \"best_score\": best_score, \"best_cfg\": best_cfg}, indent=2))\n        print(f\"Best score: {best_score:.4f}\")\n        print(\"TensorBoard logs stored in\", self.output_dir)\n        self.writer.flush()\n        self.writer.close()\n        # save JSON summary\n        with open(self.output_dir / \"summary.json\", \"w\") as fp:\n            json.dump({\"best_cfg\": best_cfg, \"best_score\": best_score}, fp, indent=2)\n\n# ----------------------------------- CLI --------------------------------------\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Common-Core Experiment Trainer\")\n    parser.add_argument(\"--smoke-test\", action=\"store_true\", help=\"run quick smoke test\")\n    parser.add_argument(\"--full-experiment\", action=\"store_true\", help=\"run full experiment\")\n    parser.add_argument(\"--variation\", type=str, required=False, default=\"ASHA-baseline\")\n    parser.add_argument(\"--config-path\", type=str, default=None)\n    args = parser.parse_args()\n\n    if args.smoke_test and args.full_experiment:\n        raise ValueError(\"Choose either --smoke-test or --full-experiment\")\n\n    if args.smoke_test:\n        cfg_path = args.config_path or \"config/smoke_test.yaml\"\n    else:\n        cfg_path = args.config_path or \"config/full_experiment.yaml\"\n    with open(cfg_path, \"r\") as fp:\n        cfg = yaml.safe_load(fp)\n    trainer = Trainer(cfg, args.variation, smoke=args.smoke_test)\n    trainer.run()\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "evaluate_py": "\"\"\"src/evaluate.py\nUnified evaluation & comparison script.  It reads TensorBoard event files from\nall variations in a directory and produces publication-quality .pdf figures that\ncompare best accuracy over wall-clock, accuracy vs. consumed epochs as well as\nfinal accuracy bar-plots.  All metrics are identical across experimental\nvariations to guarantee consistency.\n\"\"\"\nfrom __future__ import annotations\nimport argparse\nimport json\nimport pathlib\nfrom collections import defaultdict\nfrom typing import Dict, List\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom tensorboard.backend.event_processing import event_accumulator\n\nplt.rcParams.update({\"pdf.fonttype\": 42})  # publication friendly\n\n\nclass RunLog:\n    def __init__(self, tb_dir: pathlib.Path):\n        self.tb_dir = tb_dir\n        self.steps: List[int] = []\n        self.acc: List[float] = []\n        self.best_acc: List[float] = []\n        self._load()\n        self.final_best = self.best_acc[-1] if self.best_acc else 0.0\n\n    def _load(self):\n        ea = event_accumulator.EventAccumulator(str(self.tb_dir))\n        ea.Reload()\n        if \"acc\" not in ea.scalars.Keys():\n            return\n        for scalar in ea.scalars.Items(\"acc\"):\n            self.steps.append(scalar.step)\n            self.acc.append(scalar.value)\n        if \"best_acc\" in ea.scalars.Keys():\n            self.best_acc = [s.value for s in ea.scalars.Items(\"best_acc\")]\n\n\nclass Evaluator:\n    def __init__(self, results_dir: pathlib.Path):\n        self.results_dir = results_dir\n        self.runs: Dict[str, List[RunLog]] = defaultdict(list)\n        self._discover()\n\n    def _discover(self):\n        for variation_dir in self.results_dir.iterdir():\n            if not variation_dir.is_dir():\n                continue\n            # expect multiple timestamped sub-dirs\n            subdirs = [d for d in variation_dir.iterdir() if (d / \"tb\").exists()]\n            for sd in subdirs:\n                tb_path = sd / \"tb\"\n                self.runs[variation_dir.name].append(RunLog(tb_path))\n\n    def _aggregate(self):\n        summary = {}\n        for var, logs in self.runs.items():\n            best_scores = [run.final_best for run in logs]\n            summary[var] = {\n                \"mean\": float(np.mean(best_scores)),\n                \"std\": float(np.std(best_scores)),\n                \"runs\": best_scores,\n            }\n        return summary\n\n    # ------------------------- plotting helpers ------------------------------\n    def _plot_final_bar(self, summary: Dict[str, Dict[str, float]]):\n        fig, ax = plt.subplots(figsize=(6, 4))\n        vars_ = list(summary.keys())\n        means = [summary[v][\"mean\"] for v in vars_]\n        stds = [summary[v][\"std\"] for v in vars_]\n        ax.bar(vars_, means, yerr=stds, color=sns.color_palette(\"Set2\", len(vars_)))\n        for idx, m in enumerate(means):\n            ax.text(idx, m + 0.005, f\"{m:.3f}\", ha=\"center\", va=\"bottom\")\n        ax.set_ylabel(\"Final Best Accuracy\")\n        ax.set_title(\"Scheduler Comparison\")\n        ax.set_ylim(0, 1)\n        fig.tight_layout()\n        out_path = self.results_dir / \"final_accuracy.pdf\"\n        fig.savefig(out_path, bbox_inches=\"tight\")\n        print(\"Saved\", out_path)\n\n    def _plot_learning_curves(self):\n        fig, ax = plt.subplots(figsize=(6, 4))\n        for var, logs in self.runs.items():\n            # average across runs (interpolate to common steps)\n            if not logs[0].steps:\n                continue\n            max_step = max(max(r.steps) for r in logs)\n            common = np.linspace(0, max_step, 200)\n            curves = []\n            for run in logs:\n                if not run.steps:\n                    continue\n                curves.append(np.interp(common, run.steps, run.best_acc))\n            mean = np.mean(curves, axis=0)\n            ax.plot(common, mean, label=var)\n            ax.annotate(f\"{mean[-1]:.3f}\", (common[-1], mean[-1]))\n        ax.set_xlabel(\"Scheduler step\")\n        ax.set_ylabel(\"Best Accuracy\")\n        ax.set_title(\"Best Accuracy vs Scheduler Step\")\n        ax.legend()\n        fig.tight_layout()\n        out_path = self.results_dir / \"accuracy_vs_steps.pdf\"\n        fig.savefig(out_path, bbox_inches=\"tight\")\n        print(\"Saved\", out_path)\n\n    def run(self):\n        summary = self._aggregate()\n        # print numerical data\n        print(\"Experiment description:\")\n        print(json.dumps(summary, indent=2))\n        self._plot_final_bar(summary)\n        self._plot_learning_curves()\n\n\n# -------------------------------- CLI ----------------------------------------\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Evaluate & compare results\")\n    parser.add_argument(\"--results-dir\", required=True, type=str)\n    args = parser.parse_args()\n    evaluator = Evaluator(pathlib.Path(args.results_dir))\n    evaluator.run()\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "preprocess_py": "\"\"\"src/preprocess.py\nCommon data-loading & preprocessing utilities.  Supports a synthetic\nclassification data set for smoke-tests and plugs in concrete benchmark data\nsets (CIFAR-10 / CIFAR-100) via the Hugging Face 🤗 Datasets hub for the main\nexperiments.\n\"\"\"\nfrom __future__ import annotations\nimport random\nfrom typing import Any, Dict, Tuple\n\nimport numpy as np\nimport torch\nfrom sklearn.datasets import make_classification\nfrom torch.utils.data import Dataset\n\n# third-party\nfrom datasets import load_dataset\nimport torchvision.transforms as T\n\n# -------------------------------- synthetic data -----------------------------\n\nclass SyntheticClassificationDataset(Dataset):\n    def __init__(self, n_samples: int = 2000, n_features: int = 20, n_classes: int = 2, seed: int = 0):\n        X, y = make_classification(\n            n_samples=n_samples,\n            n_features=n_features,\n            n_informative=int(n_features * 0.6),\n            n_redundant=int(n_features * 0.2),\n            n_classes=n_classes,\n            random_state=seed,\n        )\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx: int):\n        return self.X[idx], self.y[idx]\n\n\n# -------------------------------- HF image data ------------------------------\n\nclass HFImageDataset(Dataset):\n    \"\"\"Light-weight wrapper around HF image datasets returning (tensor, label).\"\"\"\n\n    def __init__(self, hf_ds, transform):\n        self.ds = hf_ds  # huggingface Dataset object\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.ds)\n\n    def __getitem__(self, idx):\n        example = self.ds[idx]\n        img = example[\"img\"]  # PIL.Image\n        label = example[\"label\"]\n        img = self.transform(img)\n        return img, label\n\n\ndef _cifar_transforms(train: bool = True):\n    mean = [0.4914, 0.4822, 0.4465]\n    std = [0.2470, 0.2435, 0.2616]\n    if train:\n        return T.Compose([\n            T.RandomCrop(32, padding=4),\n            T.RandomHorizontalFlip(),\n            T.ToTensor(),\n            T.Normalize(mean, std),\n        ])\n    else:\n        return T.Compose([\n            T.ToTensor(),\n            T.Normalize(mean, std),\n        ])\n\n\n# ------------------------------- dataset factory -----------------------------\n\ndef get_dataset(name: str, cfg: Dict[str, Any]) -> Tuple[Dataset, Dataset, int, int]:\n    \"\"\"Returns (train_dataset, val_dataset, input_dim, num_classes).\n\n    Supported names:\n      • synthetic – toy data for smoke tests\n      • cifar10  – uoft-cs/cifar10 (HF hub)\n      • cifar100 – uoft-cs/cifar100 (HF hub)\n    \"\"\"\n\n    name = name.lower()\n\n    # ---- synthetic (tabular) ------------------------------------------------\n    if name == \"synthetic\":\n        seed = cfg.get(\"seed\", 0)\n        n_features = cfg.get(\"n_features\", 20)\n        n_classes = cfg.get(\"n_classes\", 2)\n        full_ds = SyntheticClassificationDataset(2000, n_features, n_classes, seed=seed)\n        n_train = int(0.8 * len(full_ds))\n        n_val = len(full_ds) - n_train\n        train_ds, val_ds = torch.utils.data.random_split(full_ds, [n_train, n_val])\n        return train_ds, val_ds, n_features, n_classes\n\n    # ---- CIFAR-10 / CIFAR-100 ----------------------------------------------\n    if name in {\"cifar10\", \"cifar100\"}:\n        hf_repo = f\"uoft-cs/{name}\"\n        # splits: train (50k) / test (10k).  We create a 45k/5k train/val split.\n        ds_train_full = load_dataset(hf_repo, split=\"train\")\n        ds_test = load_dataset(hf_repo, split=\"test\")  # used as held-out test later if desired\n\n        indices = list(range(len(ds_train_full)))\n        random.shuffle(indices)\n        val_size = 5000\n        val_indices = indices[:val_size]\n        train_indices = indices[val_size:]\n        hf_train = ds_train_full.select(train_indices)\n        hf_val = ds_train_full.select(val_indices)\n\n        train_ds = HFImageDataset(hf_train, _cifar_transforms(train=True))\n        val_ds = HFImageDataset(hf_val, _cifar_transforms(train=False))\n\n        input_dim = 3 * 32 * 32  # flattened dimension (unused for CNNs but kept for consistency)\n        num_classes = 10 if name == \"cifar10\" else 100\n        return train_ds, val_ds, input_dim, num_classes\n\n    # ------------------------------------------------------------------------\n    raise NotImplementedError(f\"Dataset '{name}' not supported.  Implemented: synthetic, cifar10, cifar100.\")\n",
    "model_py": "\"\"\"src/model.py\nModel factory now provides:\n  • MLP – for tabular / synthetic data\n  • ResNet-18 – baseline CNN for CIFAR-10/100 experiments (downloaded from\n    torchvision, last FC adapted to the requested number of classes)\nAdditional architectures can be added following the same pattern.\n\"\"\"\nfrom __future__ import annotations\nfrom typing import Any, Dict\n\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\n\n\nclass MLP(nn.Module):\n    def __init__(self, input_dim: int, num_classes: int, hidden_dim: int = 64, dropout: float = 0.0):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, num_classes),\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:  # (B, D) or (B, C, H, W) flattened outside\n        if x.dim() > 2:\n            x = torch.flatten(x, 1)\n        return self.net(x)\n\n\ndef _resnet18(num_classes: int):\n    model = models.resnet18(weights=None)  # no ImageNet pre-training to stay fair\n    model.fc = nn.Linear(model.fc.in_features, num_classes)\n    return model\n\n\n# ----------------------- model factory ---------------------------------------\n\ndef get_model(name: str, input_dim: int, num_classes: int, cfg: Dict[str, Any]):\n    \"\"\"Return an instantiated torch.nn.Module configured for this run.\"\"\"\n    name = name.lower()\n\n    if name == \"mlp\":\n        hidden = int(cfg.get(\"hidden_dim\", 64))\n        dropout = float(cfg.get(\"dropout\", 0.0))\n        return MLP(input_dim, num_classes, hidden_dim=hidden, dropout=dropout)\n\n    if name == \"resnet18\":\n        return _resnet18(num_classes)\n\n    raise NotImplementedError(f\"Model '{name}' is not implemented.  Available: mlp, resnet18.\")\n",
    "main_py": "\"\"\"src/main.py\nCommand-line entry point.  Delegates to train.py (training) or evaluate.py\n(evaluation) depending on arguments.  Ensures the CLI contract required by the\nworkflow is fully respected.\n\"\"\"\nimport argparse\nimport importlib\nimport sys\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"COMMON CORE FOUNDATION entry-point\")\n    sub = parser.add_subparsers(dest=\"mode\")\n\n    # training modes -----------------------------------------------------------\n    train_p = sub.add_parser(\"train\", help=\"run training experiment\")\n    train_p.add_argument(\"--smoke-test\", action=\"store_true\", help=\"quick validation run\")\n    train_p.add_argument(\"--full-experiment\", action=\"store_true\", help=\"full benchmark run\")\n    train_p.add_argument(\"--variation\", type=str, required=False, default=\"ASHA-baseline\")\n    train_p.add_argument(\"--config-path\", type=str, default=None)\n\n    # evaluation ---------------------------------------------------------------\n    eval_p = sub.add_parser(\"evaluate\", help=\"evaluate directory of runs\")\n    eval_p.add_argument(\"--results-dir\", type=str, required=True)\n\n    args = parser.parse_args()\n\n    if args.mode == \"train\":\n        module = importlib.import_module(\"src.train\")\n        sys.argv = [\"train\"]  # clean argv for sub-module parser\n        if args.smoke_test:\n            sys.argv += [\"--smoke-test\"]\n        if args.full_experiment:\n            sys.argv += [\"--full-experiment\"]\n        sys.argv += [\"--variation\", args.variation]\n        if args.config_path:\n            sys.argv += [\"--config-path\", args.config_path]\n        module.main()\n    elif args.mode == \"evaluate\":\n        module = importlib.import_module(\"src.evaluate\")\n        sys.argv = [\"evaluate\", \"--results-dir\", args.results_dir]\n        module.main()\n    else:\n        parser.print_help()\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "pyproject_toml": "[build-system]\nrequires = [\"setuptools>=61.0\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"common-core-foundation\"\nversion = \"0.1.0\"\ndescription = \"Unified experimental framework for SA-PASHA vs baselines\"\nrequires-python = \">=3.10\"\ndependencies = [\n    \"torch>=2.0.0\",\n    \"torchvision>=0.15.0\",\n    \"numpy>=1.24\",\n    \"scikit-learn>=1.3\",\n    \"pyyaml>=6.0\",\n    \"tqdm>=4.66\",\n    \"tensorboard>=2.14\",\n    \"matplotlib>=3.8\",\n    \"seaborn>=0.13\",\n    \"pandas>=2.1\",\n    \"datasets>=2.14\",            # ← added for HF dataset loading\n]\n\n[tool.setuptools.packages.find]\nwhere = [\"src\"]\n",
    "smoke_test_yaml": "# config/smoke_test.yaml\nseed: 0\ndataset: synthetic\nmodel: mlp\nn_features: 20\nn_classes: 2\nmax_epochs: 16\nmax_trials: 20\nsearch_space:\n  lr: [0.0001, 0.01]\n  hidden_dim: [32, 64]\n  dropout: [0.0, 0.3]\n",
    "full_experiment_yaml": "# config/full_experiment.yaml\n# Main experiment configuration for CIFAR-10 classification with ResNet-18 and\n# scheduler comparison (ASHA, PASHA, SA-PASHA, HYBAND).\n\nseed: 0\n\n# Dataset & model -------------------------------------------------------------\ndataset: cifar10              # loaded from uoft-cs/cifar10 (Hugging Face hub)\nmodel: resnet18\n\n# Global search / training budget --------------------------------------------\nmax_epochs: 50                # per-configuration budget cap (can be increased)\nmax_trials: 100               # total number of configurations the scheduler may spawn\n\n# Hyper-parameter search-space -------------------------------------------------\n#  Notes:\n#   • Keys ending in \"_log\" are sampled log-uniformly by sample_config().\n#   • Continuous ranges are given as [low, high].\n#   • Categorical lists remain as is (uniform draw).\nsearch_space:\n  lr_log: [0.0001, 0.1]          # learning rate (log-uniform)\n  weight_decay_log: [1.0e-5, 0.01]\n  batch_size: [32, 128]          # mini-batch size (uniform integer)\n  # (Optional) data augmentation strength, dropout, etc. can be added here.\n"
}
