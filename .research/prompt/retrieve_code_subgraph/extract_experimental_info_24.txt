
Input:
You are a researcher with expertise in engineering in the field of machine learning.

# Instructions
- The content described in “Repository Content” corresponds to the GitHub repository of the method described in “Method.”
- Please extract the following two pieces of information from “Repository Content”:
    - experimental_code：Extract the implementation sections that are directly related to the method described in “Method.”
    - experimental_info：Extract and output the experimental settings related to the method described in “Method.”

# Method
PB2 frames the online hyperparameter selection problem as batch Gaussian Process (GP) bandit optimization of a time-varying function. The core methodology involves: (1) Modeling the objective function (e.g., RL reward) with a time-varying GP, using a kernel that combines a squared exponential kernel with a time kernel (kSE ◦ ktime) to capture the non-stationary nature of neural network training. A parameter 'ω' is introduced to model how the function varies over time. (2) Employing a parallel agent selection mechanism based on the GP model. For a batch of B agents, hyperparameters are selected by sequentially maximizing an acquisition function: xb_t = arg max x∈D µt,1(x) + sqrt(βt)σt,b(x), which updates the uncertainty based on currently training agents to reduce redundancy and explore distinct regions. (3) The PB2 algorithm iteratively updates network weights, evaluates models, records data, and periodically (every 'tready' steps) performs two actions: copying weights from top-performing agents to underperforming ones, and selecting new hyperparameters for all agents by fitting a GP model to the collected data and maximizing the acquisition function. The theoretical foundation includes a convergence guarantee showing sublinear cumulative regret under correlated time-varying functions.

# Repository Content
File Path: kernel.py
Content:


from GPy.kern import Kern
from GPy.core import Param
from sklearn.metrics import pairwise_distances
from sklearn.metrics.pairwise import euclidean_distances

import numpy as np

class TV_SquaredExp(Kern):
    def __init__(self,input_dim, variance=1.,lengthscale=1.,epsilon=0.,active_dims=None):
        super().__init__(input_dim, active_dims, 'time_se')
        self.variance = Param('variance', variance)
        self.lengthscale = Param('lengthscale', lengthscale)
        self.epsilon = Param('epsilon', epsilon)
        self.link_parameters(self.variance, self.lengthscale, self.epsilon)
        
    def K(self,X,X2):
        # time must be in the far left column
        if self.epsilon > 0.5: # 0.5
            self.epsilon = 0.5
        if X2 is None: X2 = np.copy(X)
        T1 = X[:, 0].reshape(-1, 1)
        T2 = X2[:, 0].reshape(-1, 1)
        dists = pairwise_distances(T1,T2, 'cityblock')
        timekernel=(1-self.epsilon)**(0.5*dists)
        
        X = X[:, 1:]
        X2 = X2[:, 1:]

        RBF = self.variance*np.exp(-np.square(euclidean_distances(X,X2))/self.lengthscale)
        
        return RBF * timekernel
    
    def Kdiag(self,X):
        return self.variance*np.ones(X.shape[0])
    
    def update_gradients_full(self, dL_dK, X, X2):
        if X2 is None: X2 = np.copy(X)
        T1 = X[:, 0].reshape(-1, 1)
        T2 = X2[:, 0].reshape(-1, 1)
        
        X = X[:, 1:]
        X2 = X2[:, 1:]
        dist2 = np.square(euclidean_distances(X,X2))/self.lengthscale
    
        dvar = np.exp(-np.square((euclidean_distances(X,X2))/self.lengthscale))
        dl =  - (2 * euclidean_distances(X,X2)**2 * self.variance * np.exp(-dist2)) * self.lengthscale**(-2)
        n = pairwise_distances(T1,T2, 'cityblock')/2
        deps = -n * (1-self.epsilon)**(n-1)
    
        self.variance.gradient = np.sum(dvar*dL_dK)
        self.lengthscale.gradient = np.sum(dl*dL_dK)
        self.epsilon.gradient = np.sum(deps*dL_dK)
 
File Path: pb2.py
Content:
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import copy
from copy import deepcopy
import itertools
import logging
import json
import math
import os
import random
import shutil
import GPy
import numpy as np
import pandas as pd
from scipy.optimize import minimize
from scipy.stats import norm
from sklearn.metrics.pairwise import euclidean_distances, rbf_kernel
from dppy.finite_dpps import FiniteDPP

from ray.tune.error import TuneError
from ray.tune.result import TRAINING_ITERATION
from ray.tune.logger import _SafeFallbackEncoder
from ray.tune.schedulers import FIFOScheduler, TrialScheduler
from ray.tune.suggest.variant_generator import format_vars
from ray.tune.trial import Trial, Checkpoint

from kernel import TV_SquaredExp

logger = logging.getLogger(__name__)


class PBTTrialState(object):
    """Internal PBT state tracked per-trial."""

    def __init__(self, trial):
        self.orig_tag = trial.experiment_tag
        self.last_score = None
        self.last_checkpoint = None
        self.last_perturbation_time = 0

    def __repr__(self):
        return str((self.last_score, self.last_checkpoint,
                    self.last_perturbation_time))


def normalize(data, wrt):
    # data = data to normalize
    # wrt = data will be normalized with respect to this
    return (data - np.min(wrt, axis=0))/(np.max(wrt,axis=0) - np.min(wrt,axis=0))

def standardize(data):
    data = (data - np.mean(data, axis=0))/(np.std(data, axis=0)+1e-8)
    return np.clip(data, -2, 2)

def UCB(m, m1, x, fixed, kappa=0.5):
    
    c1 = 0.2 # from TV-GP-UCB
    c2 = 0.4
    beta_t = c1 * np.log(c2 * m.X.shape[0])
    kappa = np.sqrt(beta_t)
    
    xtest = np.concatenate((fixed.reshape(-1, 1), np.array(x).reshape(-1,1))).T
    
    preds = m.predict(xtest)
    mean = preds[0][0][0] 
    
    preds = m1.predict(xtest)
    var = preds[1][0][0]
    return mean + kappa * var


def optimize_acq(func, m, m1, fixed, num_f):
    
    print("Optimizing Acquisition Function...\n")
    
    opts = {'maxiter':200, 'maxfun':200, 'disp':False}
    
    T=10
    best_value=-999
    best_theta = m1.X[0,:]
    
    bounds = [(0,1) for _ in range(m.X.shape[1]-num_f)]
    
    for ii in range(T):
        x0 = np.random.uniform(0,1, m.X.shape[1]-num_f)
        
        res = minimize(lambda x: -func(m, m1, x, fixed), x0, bounds=bounds, method="L-BFGS-B", options=opts)
        
        val = func(m, m1, res.x, fixed)
        if val > best_value:
            best_value=val
            best_theta =res.x
    
    return(np.clip(best_theta, 0, 1))

def get_diverse(X, y, size=10):

    # This is a heuristic if the covariance matrix is ill conditioned. 
    
    if X.shape[0] > 10:
        size = int(X.shape[0]/2)
    else:
        return(X, y)

    K = rbf_kernel(X, X)    
    
    L = np.matmul(K, np.linalg.inv(np.eye(K.shape[0]) - K))

    DPP = FiniteDPP('likelihood', **{'L': L})
    DPP.flush_samples()
    try:
        DPP.sample_exact_k_dpp(size=size)
    except ValueError:
        return(X, y)
    newX = X[DPP.list_of_samples[0], :]
    newy = y[DPP.list_of_samples[0], :]
    return(newX, newy)

def select_length(Xraw, yraw, current, newpoint, bounds, num_f, num):

    # block size.
    
    min_len = 200
    
    if Xraw.shape[0] < min_len:
        return(Xraw.shape[0])
    else:
        length = min_len-10   
        scores = []
        while length+10 <= Xraw.shape[0]:
            length += 10
            
            base_vals = np.array(list(bounds.values())).T
            X_len = Xraw[-length:, :]
            y_len = yraw[-length:]
            oldpoints = X_len[:, :num_f]
            old_lims = np.concatenate((np.max(oldpoints, axis=0), np.min(oldpoints, axis=0))).reshape(2, oldpoints.shape[1])
            limits = np.concatenate((old_lims, base_vals),axis=1)
            
            X = normalize(X_len, limits)
            y = standardize(y_len).reshape(y_len.size, 1)
            
            kernel = TV_SquaredExp(input_dim=X.shape[1], variance=1., lengthscale=1., epsilon=0.1)
            try:
                m = GPy.models.GPRegression(X, y, kernel)
            except np.linalg.LinAlgError:
                X, y = get_diverse(X, y)
                m = GPy.models.GPRegression(X, y, kernel)
            try:
                m.optimize(messages=True)
            except np.linalg.LinAlgError:
                X, y = get_diverse(X, y)
                m = GPy.models.GPRegression(X, y, kernel)
                m.optimize(messages=True)
            scores.append(m.log_likelihood())
        idx = np.argmax(scores)
        length = (idx+int((min_len/10))) * 10
        return(length)
    
def select_config(Xraw, yraw, current, newpoint, bounds, num_f, num):
    
    length = select_length(Xraw, yraw, current, newpoint, bounds, num_f, num)
    print("\n\nUsing length = {}\n\n".format(length))
    
    Xraw = Xraw[-length:, :]
    yraw = yraw[-length: ]
        
    base_vals = np.array(list(bounds.values())).T
    oldpoints = Xraw[:, :num_f]
    old_lims = np.concatenate((np.max(oldpoints, axis=0), np.min(oldpoints, axis=0))).reshape(2, oldpoints.shape[1])
    limits = np.concatenate((old_lims, base_vals),axis=1)
    
    X = normalize(Xraw, limits)
    y = standardize(yraw).reshape(yraw.size, 1)
    
    fixed = normalize(newpoint, oldpoints)

    kernel = TV_SquaredExp(input_dim=X.shape[1], variance=1., lengthscale=1., epsilon=0.1)
    try:
        m = GPy.models.GPRegression(X, y, kernel)
    except np.linalg.LinAlgError:
        X, y = get_diverse(X, y)
        m = GPy.models.GPRegression(X, y, kernel)
    
    try:
        m.optimize(messages=True)
    except np.linalg.LinAlgError:
        X, y = get_diverse(X, y)
        m = GPy.models.GPRegression(X, y, kernel)
        m.optimize(messages=True)
        
    m.kern.lengthscale.fix(m.kern.lengthscale.clip(1e-5,1))
    
    if current is None:
        m1 = deepcopy(m)
    else:
        padding = np.array([fixed for _ in range(current.shape[0])])
        current = normalize(current, base_vals)
        current = np.hstack((padding, current))

        Xnew = np.vstack((X, current))
        ypad = np.zeros(current.shape[0])
        ypad = ypad.reshape(-1,1)
        ynew = np.vstack((y, ypad))
        
        kernel = TV_SquaredExp(input_dim=X.shape[1], variance=1., lengthscale=1., epsilon=0.1)
        m1 = GPy.models.GPRegression(Xnew, ynew, kernel)
        m1.optimize()
    
    time_param = m1.time_se.epsilon[0]
    lengthscale = m1.time_se.lengthscale[0]
    xt = optimize_acq(UCB, m, m1, fixed, num_f)
    
    # check dist vs other points:
    dists = euclidean_distances(X[:, num_f:], X[:, num_f:])
    meandist = np.mean(dists)
    min_new = np.min(euclidean_distances(X[:, num_f:], xt.reshape(-1,1).T))
    
    # convert back...
    xt = xt * (np.max(base_vals,axis=0) - np.min(base_vals,axis=0)) + np.min(base_vals, axis=0)
    
    print("\n**\n\n New Point: {} \n\n**\n".format(str(xt)))
    print("\n**\n\n Length Scale: {} \n\n**\n".format(str(lengthscale)))
    print("\n**\n\n Min Dist: {} \n\n**\n".format(str(min_new)))
    print("\n**\n\n vs. Mean Dist: {} \n\n**\n".format(str(meandist)))
    
    xt = xt.astype(np.float32)
    return(xt, lengthscale, min_new, meandist)

def explore(data, bounds, current, base, old, config, mutations, resample_probability):
    
    print("\n\nGP Bandit Time :) \n\n")
    
    data.to_csv("checks.csv", index=False)
    data['Trial'] = data['Trial'].astype(str)
    
    # df = pd.read_csv("checks.csv")

    df = data.sort_values(by='T').reset_index(drop=True)
    df['y'] = df.groupby('Trial')['Reward'].diff()
    df['t_change'] = df.groupby('Trial')['T'].diff()
    df = df[df['t_change'] > 0].reset_index(drop=True)
    df['R_before'] = df.Reward - df.y
    df['y'] = df.y / df.t_change
    df = df[~df.y.isna()].reset_index(drop=True)
    df = df.sort_values(by='T').reset_index(drop=True)
    
    # replace previous data for the trial being replaced
    # otherwise we have a huge jump
    marker=0
    done = False
    while not done:
        old_rename = str(old) +'_*_'+ str(marker)
        if data[data['Trial'] == old_rename].empty:
            data = data.replace(str(old), old_rename)
            done = True
        else:
            marker += 1
    
    dfnewpoint = df[df['Trial']==str(base)]
    
    if not dfnewpoint.empty:
        
        # override previous entries of this trial
        # we don't want to compare against them as the weights were overriden
        
        ref = dfnewpoint[[c for c in data.columns]]
        ref.Trial = str(old)
        ref = ref.tail(1).reset_index(drop=True)
        data = pd.concat([data, ref]).reset_index(drop=True)
        
        # now specify the dataset for the GP
                
        y = np.array(df.y.values)
        # meta data we keep -> episodes and reward (TODO: convert to curve)
        t_r = df[['T', 'R_before']] # df[['T', 'R_before']]
        h = df[[key for key in bounds.keys()]]
        X = pd.concat([t_r, h], axis=1).values 
        newpoint = df[df['Trial']==str(base)].iloc[-1, :][['T', 'R_before']].values
        new, lengthscale, mindist, meandist = select_config(X,y, current, newpoint, bounds, num_f = len(t_r.columns), num=len(h.columns))
        new_config = config.copy()
        for i in range(len(h.columns)):
            if type(config[h.columns[i]]) is int:
                new_config[h.columns[i]] = int(new[i])
            else:
                new_config[h.columns[i]] = new[i]
    else:
        new_config = config
        lengthscale = -1
        mindist = -1
        meandist = -1

    return new_config, lengthscale, mindist, meandist, data


def make_experiment_tag(orig_tag, config, mutations):
    """Appends perturbed params to the trial name to show in the console."""

    resolved_vars = {}
    for k in mutations.keys():
        resolved_vars[("config", k)] = config[k]
    return "{}@perturbed[{}]".format(orig_tag, format_vars(resolved_vars))


class PB2(FIFOScheduler):
    """
    Below text is from PBT... which this was adapted from.

    Implements the Population Based Training (PBT) algorithm.

    https://deepmind.com/blog/population-based-training-neural-networks

    PBT trains a group of models (or agents) in parallel. Periodically, poorly
    performing models clone the state of the top performers, and a random
    mutation is applied to their hyperparameters in the hopes of
    outperforming the current top models.

    Unlike other hyperparameter search algorithms, PBT mutates hyperparameters
    during training time. This enables very fast hyperparameter discovery and
    also automatically discovers good annealing schedules.

    This Tune PBT implementation considers all trials added as part of the
    PBT population. If the number of trials exceeds the cluster capacity,
    they will be time-multiplexed as to balance training progress across the
    population. To run multiple trials, use `tune.run(num_samples=<int>)`.

    In {LOG_DIR}/{MY_EXPERIMENT_NAME}/, all mutations are logged in
    `pbt_global.txt` and individual policy perturbations are recorded
    in pbt_policy_{i}.txt. Tune logs: [target trial tag, clone trial tag,
    target trial iteration, clone trial iteration, old config, new config]
    on each perturbation step.

    Args:
        time_attr (str): The training result attr to use for comparing time.
            Note that you can pass in something non-temporal such as
            `training_iteration` as a measure of progress, the only requirement
            is that the attribute should increase monotonically.
        metric (str): The training result objective value attribute. Stopping
            procedures will use this attribute.
        mode (str): One of {min, max}. Determines whether objective is
            minimizing or maximizing the metric attribute.
        perturbation_interval (float): Models will be considered for
            perturbation at this interval of `time_attr`. Note that
            perturbation incurs checkpoint overhead, so you shouldn't set this
            to be too frequent.
        hyperparam_mutations (dict): Hyperparams to mutate. The format is
            as follows: for each key, either a list or function can be
            provided. A list specifies an allowed set of categorical values.
            A function specifies the distribution of a continuous parameter.
            You must specify at least one of `hyperparam_mutations` or
            `custom_explore_fn`.
        quantile_fraction (float): Parameters are transferred from the top
            `quantile_fraction` fraction of trials to the bottom
            `quantile_fraction` fraction. Needs to be between 0 and 0.5.
            Setting it to 0 essentially implies doing no exploitation at all.
        resample_probability (float): The probability of resampling from the
            original distribution when applying `hyperparam_mutations`. If not
            resampled, the value will be perturbed by a factor of 1.2 or 0.8
            if continuous, or changed to an adjacent value if discrete.
        custom_explore_fn (func): You can also specify a custom exploration
            function. This function is invoked as `f(config)` after built-in
            perturbations from `hyperparam_mutations` are applied, and should
            return `config` updated as needed. You must specify at least one of
            `hyperparam_mutations` or `custom_explore_fn`.
        log_config (bool): Whether to log the ray config of each model to
            local_dir at each exploit. Allows config schedule to be
            reconstructed.

    Example:
        >>> pbt = PopulationBasedTraining(
        >>>     time_attr="training_iteration",
        >>>     metric="episode_reward_mean",
        >>>     mode="max",
        >>>     perturbation_interval=10,  # every 10 `time_attr` units
        >>>                                # (training_iterations in this case)
        >>>     hyperparam_mutations={
        >>>         # Perturb factor1 by scaling it by 0.8 or 1.2. Resampling
        >>>         # resets it to a value sampled from the lambda function.
        >>>         "factor_1": lambda: random.uniform(0.0, 20.0),
        >>>         # Perturb factor2 by changing it to an adjacent value, e.g.
        >>>         # 10 -> 1 or 10 -> 100. Resampling will choose at random.
        >>>         "factor_2": [1, 10, 100, 1000, 10000],
        >>>     })
        >>> tune.run({...}, num_samples=8, scheduler=pbt)
    """

    def __init__(self,
                 time_attr="time_total_s",
                 reward_attr=None,
                 metric="episode_reward_mean",
                 mode="max",
                 perturbation_interval=60.0,
                 hyperparam_mutations={},
                 quantile_fraction=0.25,
                 resample_probability=0.25,
                 custom_explore_fn=None,
                 log_config=True):
        if not hyperparam_mutations and not custom_explore_fn:
            raise TuneError(
                "You must specify at least one of `hyperparam_mutations` or "
                "`custom_explore_fn` to use PBT.")

        if quantile_fraction > 0.5 or quantile_fraction < 0:
            raise TuneError(
                "You must set `quantile_fraction` to a value between 0 and"
                "0.5. Current value: '{}'".format(quantile_fraction))

        assert mode in ["min", "max"], "`mode` must be 'min' or 'max'!"

        if reward_attr is not None:
            mode = "max"
            metric = reward_attr
            logger.warning(
                "`reward_attr` is deprecated and will be removed in a future "
                "version of Tune. "
                "Setting `metric={}` and `mode=max`.".format(reward_attr))

        FIFOScheduler.__init__(self)
        self._metric = metric
        if mode == "max":
            self._metric_op = 1.
        elif mode == "min":
            self._metric_op = -1.
        self._time_attr = time_attr
        self._perturbation_interval = perturbation_interval
        self._hyperparam_mutations = hyperparam_mutations
        self._quantile_fraction = quantile_fraction
        self._resample_probability = resample_probability
        self._trial_state = {}
        self._custom_explore_fn = custom_explore_fn
        self._log_config = log_config
        
        self.meta = {'timesteps': [],'lengthscales': [], 'closest': [], 'meandist': []}
        self.latest = 0 # when we last did bayesopt
        self.data = pd.DataFrame()
        
        self.bounds = {}
        for key, distribution in self._hyperparam_mutations.items():
            self.bounds[key] = [np.min([distribution() for _ in range(999999)]),np.max([distribution() for _ in range(999999)])]

        # Metrics
        self._num_checkpoints = 0
        self._num_perturbations = 0

    def on_trial_add(self, trial_runner, trial):
        self._trial_state[trial] = PBTTrialState(trial)

    def on_trial_result(self, trial_runner, trial, result):
        
        score = self._metric_op * result[self._metric]

        names = []
        values = []
        for key, distribution in self._hyperparam_mutations.items():
            names.append(str(key))
            values.append(trial.config[key])
    
        lst = [[trial, result["timesteps_total"]] + values + [score]]
        cols = ['Trial', 'T'] + names + ['Reward']
        entry = pd.DataFrame(lst, columns = cols) 

        self.data = pd.concat([self.data, entry]).reset_index(drop=True)
        self.data.Trial = self.data.Trial.astype('str')

        if self._time_attr not in result or self._metric not in result:
            return TrialScheduler.CONTINUE
        time = result[self._time_attr]
        state = self._trial_state[trial]

        if time - state.last_perturbation_time < self._perturbation_interval:
            return TrialScheduler.CONTINUE  # avoid checkpoint overhead

        score = self._metric_op * result[self._metric]
        state.last_score = score
        state.last_perturbation_time = time
        lower_quantile, upper_quantile = self._quantiles()

        
        if trial in upper_quantile:
            state.last_checkpoint = trial_runner.trial_executor.save(
                trial, Checkpoint.MEMORY)
            self._num_checkpoints += 1
        else:
            state.last_checkpoint = None  # not a top trial

        if trial in lower_quantile:
            trial_to_clone = random.choice(upper_quantile)
            assert trial is not trial_to_clone
            self._exploit(trial_runner.trial_executor, trial, trial_to_clone)

        for trial in trial_runner.get_trials():
            if trial.status in [Trial.PENDING, Trial.PAUSED]:
                return TrialScheduler.PAUSE  # yield time to other trials

        return TrialScheduler.CONTINUE

    def _log_config_on_step(self, trial_state, new_state, trial,
                            trial_to_clone, new_config):
        """Logs transition during exploit/exploit step.

        For each step, logs: [target trial tag, clone trial tag, target trial
        iteration, clone trial iteration, old config, new config].

        """
        trial_name, trial_to_clone_name = (trial_state.orig_tag,
                                           new_state.orig_tag)
        trial_id = "".join(itertools.takewhile(str.isdigit, trial_name))
        trial_to_clone_id = "".join(
            itertools.takewhile(str.isdigit, trial_to_clone_name))
        trial_path = os.path.join(trial.local_dir,
                                  "pbt_policy_" + trial_id + ".txt")
        trial_to_clone_path = os.path.join(
            trial_to_clone.local_dir,
            "pbt_policy_" + trial_to_clone_id + ".txt")
        policy = [
            trial_name, trial_to_clone_name,
            trial.last_result.get(TRAINING_ITERATION, 0),
            trial_to_clone.last_result.get(TRAINING_ITERATION, 0),
            trial_to_clone.config, new_config
        ]
        # Log to global file.
        with open(os.path.join(trial.local_dir, "pbt_global.txt"), "a+") as f:
            print(json.dumps(policy, cls=_SafeFallbackEncoder), file=f)
        # Overwrite state in target trial from trial_to_clone.
        if os.path.exists(trial_to_clone_path):
            shutil.copyfile(trial_to_clone_path, trial_path)
        # Log new exploit in target trial log.
        with open(trial_path, "a+") as f:
            f.write(json.dumps(policy, cls=_SafeFallbackEncoder) + "\n")

    def _exploit(self, trial_executor, trial, trial_to_clone):
        """Transfers perturbed state from trial_to_clone -> trial.

        If specified, also logs the updated hyperparam state.

        """

        trial_state = self._trial_state[trial]
        new_state = self._trial_state[trial_to_clone]
        
        if not new_state.last_checkpoint:
            logger.info("[pbt]: no checkpoint for trial."
                        " Skip exploit for Trial {}".format(trial))
            return
        
        # if we are at a new timestep, we dont want to penalise for trials still going
        if self.data['T'].max() > self.latest:
            self.current = None
        
        print("\n\n\n\n Copying: \n{} \n with:{} \n\n".format(str(trial), str(trial_to_clone)))
        new_config, lengthscale, mindist, meandist, data = explore(self.data, self.bounds,
                             self.current,
                             trial_to_clone,
                             trial,
                             trial_to_clone.config,
                             self._hyperparam_mutations,
                             self._resample_probability)
        
        # important to replace the old values, since we are copying across
        self.data = data.copy()
        
        # if the current guy youre selecting is at a point youve already done, 
        # then append the data to the "current" which is the points in the current batch
        
        new = []
        for key in self._hyperparam_mutations.keys():
            new.append(new_config[key])
    
        new  = np.array(new)
        new = new.reshape(1, new.size)
        if self.data['T'].max() > self.latest:
            self.latest = self.data['T'].max()
            self.current = new.copy()
        else:
            self.current = np.concatenate((self.current, new), axis=0)
            print("\n\n\n\n\n Currently Evaluating \n\n\n\n\n")
            print(self.current)
            print("\n\n\n\n\n")
        
        # log the lengthscale
        self.meta['timesteps'].append(self.data['T'].values[-1])
        self.meta['lengthscales'].append(lengthscale)
        self.meta['closest'].append(mindist)
        self.meta['meandist'].append(meandist)
        meta = pd.DataFrame({'timesteps': self.meta['timesteps'], 
                             'lengthscales': self.meta['lengthscales'],
                             'closest': self.meta['closest'],
                             'meandist': self.meta['meandist']})
        meta.to_csv('meta_data.csv')
        
        logger.info("[exploit] transferring weights from trial "
                    "{} (score {}) -> {} (score {})".format(
                        trial_to_clone, new_state.last_score, trial,
                        trial_state.last_score))

        if self._log_config:
            self._log_config_on_step(trial_state, new_state, trial,
                                     trial_to_clone, new_config)

        new_tag = make_experiment_tag(trial_state.orig_tag, new_config,
                                      self._hyperparam_mutations)
        reset_successful = trial_executor.reset_trial(trial, new_config,
                                                      new_tag)
        if reset_successful:
            trial_executor.restore(
                trial, Checkpoint.from_object(new_state.last_checkpoint))
        else:
            trial_executor.stop_trial(trial, stop_logger=False)
            trial.config = new_config
            trial.experiment_tag = new_tag
            trial_executor.start_trial(
                trial, Checkpoint.from_object(new_state.last_checkpoint))

        self._num_perturbations += 1
        # Transfer over the last perturbation time as well
        trial_state.last_perturbation_time = new_state.last_perturbation_time

    def _quantiles(self):
        """Returns trials in the lower and upper `quantile` of the population.

        If there is not enough data to compute this, returns empty lists.

        """

        trials = []
        for trial, state in self._trial_state.items():
            if state.last_score is not None and not trial.is_finished():
                trials.append(trial)
        trials.sort(key=lambda t: self._trial_state[t].last_score)

        if len(trials) <= 1:
            return [], []
        else:
            num_trials_in_quantile = int(
                math.ceil(len(trials) * self._quantile_fraction))
            if num_trials_in_quantile > len(trials) / 2:
                num_trials_in_quantile = int(math.floor(len(trials) / 2))
            return (trials[:num_trials_in_quantile],
                    trials[-num_trials_in_quantile:])

    def choose_trial_to_run(self, trial_runner):
        """Ensures all trials get fair share of time (as defined by time_attr).

        This enables the PBT scheduler to support a greater number of
        concurrent trials than can fit in the cluster at any given time.

        """

        candidates = []
        for trial in trial_runner.get_trials():
            if trial.status in [Trial.PENDING, Trial.PAUSED] and \
                    trial_runner.has_resources(trial.resources):
                candidates.append(trial)
        candidates.sort(
            key=lambda trial: self._trial_state[trial].last_perturbation_time)
        return candidates[0] if candidates else None

    def reset_stats(self):
        self._num_perturbations = 0
        self._num_checkpoints = 0

    def last_scores(self, trials):
        scores = []
        for trial in trials:
            state = self._trial_state[trial]
            if state.last_score is not None and not trial.is_finished():
                scores.append(state.last_score)
        return scores

    def debug_string(self):
        return "PopulationBasedTraining: {} checkpoints, {} perturbs".format(
            self._num_checkpoints, self._num_perturbations)

File Path: run_impala.py
Content:

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import os
import random
import argparse
import pandas as pd
from datetime import datetime

import ray
from ray.tune import run, sample_from
from ray.tune.schedulers import PopulationBasedTraining, AsyncHyperBandScheduler
from pb2 import PB2
#from ppbt_noreward import PPBT

# Postprocess the perturbed config to ensure it's still valid
def explore(config):
    # ensure we collect enough timesteps to do sgd
    if config["train_batch_size"] < config["sgd_minibatch_size"] * 2:
        config["train_batch_size"] = config["sgd_minibatch_size"] * 2
    # ensure we run at least one sgd iter
    if config["num_sgd_iter"] < 1:
        config["num_sgd_iter"] = 1
    config['target_delay'] = int(config['target_delay'])
    return config

if __name__ == "__main__":
    
    parser = argparse.ArgumentParser()
    parser.add_argument("--max", type=int, default=3000000)
    parser.add_argument("--algo", type=str, default='IMPALA')
    parser.add_argument("--num_workers", type=int, default=1)
    parser.add_argument("--num_samples", type=int, default=4)
    parser.add_argument("--freq", type=int, default=500000)
    parser.add_argument("--seed", type=int, default=0)
    parser.add_argument("--horizon", type=int, default=1000)
    parser.add_argument("--perturb", type=float, default=0.25)
    parser.add_argument("--env_name", type=str, default="SpaceInvadersNoFrameskip-v4")
    parser.add_argument("--criteria", type=str, default="timesteps_total") # "training_iteration"
    parser.add_argument("--filename", type=str, default="")
    parser.add_argument("--method", type=str, default="pb2") # ['pbt', 'pb2', 'asha']
    
    args = parser.parse_args()
    ray.init()
    
    args.dir = "{}_{}_{}_Size{}_{}_{}".format(args.algo, args.filename, args.method, str(args.num_samples), args.env_name, args.criteria)
    if not(os.path.exists('data/'+args.dir)):
        os.makedirs('data/'+args.dir)

    pbt = PopulationBasedTraining(
        time_attr= args.criteria,
        metric="episode_reward_mean",
        mode="max",
        perturbation_interval=args.freq,
        resample_probability=args.perturb,
        quantile_fraction = args.perturb, # copy bottom % with top %
        # Specifies the mutations of these hyperparams
        hyperparam_mutations={
            "epsilon":  lambda: random.uniform(0.01, 0.5), # 0.1
            "entropy_coeff": lambda: random.uniform(0.001, 0.1), # 0.01
            "lr": lambda: random.uniform(1e-5, 1e-2), # 5e-3
       },
        custom_explore_fn=explore)
    
    pb2 = PB2(
        time_attr= args.criteria,
        metric="episode_reward_mean",
        mode="max",
        perturbation_interval=args.freq,
        resample_probability=0,
        quantile_fraction = args.perturb, # copy bottom % with top %
        # Specifies the mutations of these hyperparams
        hyperparam_mutations={
            "epsilon":  lambda: random.uniform(0.01, 0.5), # 0.1
            "entropy_coeff": lambda: random.uniform(0.001, 0.1), # 0.01
            "lr": lambda: random.uniform(1e-5, 1e-2), # 5e-3
       },
        custom_explore_fn=explore)

    asha = AsyncHyperBandScheduler(
        time_attr=args.criteria,
        metric="episode_reward_mean",
        mode="max",
        grace_period=args.freq,
        max_t=args.max)

    
    methods = {'pbt': pbt,
               'pb2': pb2,
               'asha': asha}
    
    timelog = str(datetime.date(datetime.now())) + '_' + str(datetime.time(datetime.now()))
    
    analysis = run(
        args.algo,
        name="{}_{}_{}_seed{}_{}".format(timelog, args.method, args.env_name, str(args.seed), args.filename),
        scheduler=methods[args.method],
        verbose=1,
        num_samples= args.num_samples,
        stop= {args.criteria: args.max},
        
        config= {
            "env": args.env_name,
            "log_level": "INFO",
            "seed": args.seed,
            "num_gpus": 0,
            "num_workers": args.num_workers,
            "horizon": args.horizon,
            "rollout_fragment_length": 50,
            "train_batch_size": 500,
            "num_envs_per_worker": 5,
            "epsilon": sample_from(
                    lambda spec: random.uniform(0.1, 0.5)),
            "entropy_coeff": sample_from(
                    lambda spec: random.uniform(0.001, 0.1)),
            "lr": sample_from(
                    lambda spec: random.uniform(1e-5, 1e-2)),
            }
        )
            

    
    all_dfs = analysis.trial_dataframes
    names = list(all_dfs.keys())
    
    results = pd.DataFrame()    
    for i in range(args.num_samples):
        df = all_dfs[names[i]]
        df = df[['timesteps_total', 'episodes_total', 'episode_reward_mean']]
        df['Agent'] = i
        results = pd.concat([results, df]).reset_index(drop=True)
    
    results.to_csv("data/{}/seed{}.csv".format(args.dir, str(args.seed)))



File Path: run_ppo.py
Content:


from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import os
import random
import argparse
import matplotlib.pyplot as plt
import pandas as pd
from datetime import datetime

import ray
from ray.tune import run, sample_from
from ray.tune.schedulers import PopulationBasedTraining, AsyncHyperBandScheduler

from pb2 import PB2

# Postprocess the perturbed config to ensure it's still valid
def explore(config):
    # ensure we collect enough timesteps to do sgd
    if config["train_batch_size"] < config["sgd_minibatch_size"] * 2:
        config["train_batch_size"] = config["sgd_minibatch_size"] * 2
    # ensure we run at least one sgd iter
    if config["lambda"] > 1:
        config["lambda"] = 1
    config["train_batch_size"] = int(config["train_batch_size"])
    return config

if __name__ == "__main__":
    
    parser = argparse.ArgumentParser()
    parser.add_argument("--max", type=int, default=1000000)
    parser.add_argument("--algo", type=str, default='PPO')
    parser.add_argument("--num_workers", type=int, default=4)
    parser.add_argument("--num_samples", type=int, default=4)
    parser.add_argument("--freq", type=int, default=50000)
    parser.add_argument("--seed", type=int, default=0)
    parser.add_argument("--horizon", type=int, default=1600) # make this 1000 for other envs
    parser.add_argument("--perturb", type=float, default=0.25)
    parser.add_argument("--env_name", type=str, default="BipedalWalker-v2")
    parser.add_argument("--criteria", type=str, default="timesteps_total") # "training_iteration"
    parser.add_argument("--net", type=str, default="32_32") # didn't play with this, but may be important for bigger tasks
    parser.add_argument("--batchsize", type=str, default="1000_60000")
    parser.add_argument("--num_sgd_iter", type=int, default=10)
    parser.add_argument("--sgd_minibatch_size", type=int, default=128)
    parser.add_argument("--use_lstm", type=int, default=0) # for future, not used
    parser.add_argument("--filename", type=str, default="")
    parser.add_argument("--method", type=str, default="pb2") # ['pbt', 'pb2', 'asha']
    
    args = parser.parse_args()
    ray.init()
    
    args.dir = "{}_{}_{}_Size{}_{}_{}".format(args.algo, args.filename, args.method, str(args.num_samples), args.env_name, args.criteria)
    if not(os.path.exists('data/'+args.dir)):
        os.makedirs('data/'+args.dir)

    pbt = PopulationBasedTraining(
        time_attr= args.criteria,
        metric="episode_reward_mean",
        mode="max",
        perturbation_interval=args.freq,
        resample_probability=args.perturb,
        quantile_fraction = args.perturb, # copy bottom % with top %
        # Specifies the mutations of these hyperparams
        hyperparam_mutations={
            "lambda": lambda: random.uniform(0.9, 1.0),
            "clip_param": lambda: random.uniform(0.1, 0.5),
            "lr": lambda: random.uniform(1e-3, 1e-5),
            "train_batch_size": lambda: random.randint(int(args.batchsize.split("_")[0]), int(args.batchsize.split("_")[1])),
        },
        custom_explore_fn=explore)
    
    pb2 = PB2(
        time_attr= args.criteria,
        metric="episode_reward_mean",
        mode="max",
        perturbation_interval=args.freq,
        resample_probability=0,
        quantile_fraction = args.perturb, # copy bottom % with top %
        # Specifies the mutations of these hyperparams
        hyperparam_mutations={
            "lambda": lambda: random.uniform(0.9, 1.0),
            "clip_param": lambda: random.uniform(0.1, 0.5),
            "lr": lambda: random.uniform(1e-3, 1e-5),
            "train_batch_size": lambda: random.randint(int(args.batchsize.split("_")[0]), int(args.batchsize.split("_")[1])),
        },
        custom_explore_fn=explore)

    asha = AsyncHyperBandScheduler(
        time_attr=args.criteria,
        metric="episode_reward_mean",
        mode="max",
        grace_period=args.freq,
        max_t=args.max)
    
    
    methods = {'pbt': pbt,
               'pb2': pb2,
               'asha': asha}
    
    timelog = str(datetime.date(datetime.now())) + '_' + str(datetime.time(datetime.now()))
    
    analysis = run(
            args.algo,
            name="{}_{}_{}_seed{}_{}".format(timelog, args.method, args.env_name, str(args.seed), args.filename),
            scheduler=methods[args.method],
            verbose=1,
            num_samples= args.num_samples,
            stop= {args.criteria: args.max},
            config= {
            "env": args.env_name,
            "log_level": "INFO",
            "seed": args.seed,
            "kl_coeff": 1.0,
            #"monitor": True, uncomment this for videos... it may slow it down a LOT, but hey :)
            "num_gpus": 0,
            "horizon": args.horizon,
            "observation_filter": "MeanStdFilter",
            "model": {'fcnet_hiddens': [int(args.net.split('_')[0]),int(args.net.split('_')[1])],
                    'free_log_std': True,
                    'use_lstm': args.use_lstm
                    },
            "num_sgd_iter":args.num_sgd_iter,
            "sgd_minibatch_size":args.sgd_minibatch_size,
            "lambda": sample_from(
                lambda spec: random.uniform(0.9, 1.0)),
            "clip_param": sample_from(
                lambda spec: random.uniform(0.1, 0.5)),
            "lr": sample_from(
                lambda spec: random.uniform(1e-3, 1e-5)),          
            "train_batch_size": sample_from(
                lambda spec: random.choice([1000 * i for i in range(int(int(args.batchsize.split("_")[0])/1000), int(int(args.batchsize.split("_")[1])/1000))]))
            }
    )
        
    all_dfs = analysis.trial_dataframes
    names = list(all_dfs.keys())
    
    results = pd.DataFrame()    
    for i in range(args.num_samples):
        df = all_dfs[names[i]]
        df = df[['timesteps_total', 'time_total_s','episodes_total', 'episode_reward_mean', 'info/learner/default_policy/cur_kl_coeff']]
        df['Agent'] = i
        results = pd.concat([results, df]).reset_index(drop=True)
    
    results.to_csv("data/{}/seed{}.csv".format(args.dir, str(args.seed)))



Output:
{
    "experimental_code": "from GPy.kern import Kern\nfrom GPy.core import Param\nfrom sklearn.metrics import pairwise_distances\nfrom sklearn.metrics.pairwise import euclidean_distances\n\nimport numpy as np\n\nclass TV_SquaredExp(Kern):\n    def __init__(self,input_dim, variance=1.,lengthscale=1.,epsilon=0.,active_dims=None):\n        super().__init__(input_dim, active_dims, 'time_se')\n        self.variance = Param('variance', variance)\n        self.lengthscale = Param('lengthscale', lengthscale)\n        self.epsilon = Param('epsilon', epsilon)\n        self.link_parameters(self.variance, self.lengthscale, self.epsilon)\n        \n    def K(self,X,X2):\n        # time must be in the far left column\n        if self.epsilon > 0.5: # 0.5\n            self.epsilon = 0.5\n        if X2 is None: X2 = np.copy(X)\n        T1 = X[:, 0].reshape(-1, 1)\n        T2 = X2[:, 0].reshape(-1, 1)\n        dists = pairwise_distances(T1,T2, 'cityblock')\n        timekernel=(1-self.epsilon)**(0.5*dists)\n        \n        X = X[:, 1:]\n        X2 = X2[:, 1:]\n\n        RBF = self.variance*np.exp(-np.square(euclidean_distances(X,X2))/self.lengthscale)\n        \n        return RBF * timekernel\n    \n    def Kdiag(self,X):\n        return self.variance*np.ones(X.shape[0])\n    \n    def update_gradients_full(self, dL_dK, X, X2):\n        if X2 is None: X2 = np.copy(X)\n        T1 = X[:, 0].reshape(-1, 1)\n        T2 = X2[:, 0].reshape(-1, 1)\n        \n        X = X[:, 1:]\n        X2 = X2[:, 1:]\n        dist2 = np.square(euclidean_distances(X,X2))/self.lengthscale\n    \n        dvar = np.exp(-np.square((euclidean_distances(X,X2))/self.lengthscale))\n        dl =  - (2 * euclidean_distances(X,X2)**2 * self.variance * np.exp(-dist2)) * self.lengthscale**(-2)\n        n = pairwise_distances(T1,T2, 'cityblock')/2\n        deps = -n * (1-self.epsilon)**(n-1)\n    \n        self.variance.gradient = np.sum(dvar*dL_dK)\n        self.lengthscale.gradient = np.sum(dl*dL_dK)\n        self.epsilon.gradient = np.sum(deps*dL_dK)\n\n\ndef UCB(m, m1, x, fixed, kappa=0.5):\n    \n    c1 = 0.2 #\tfrom TV-GP-UCB\n    c2 = 0.4\n    beta_t = c1 * np.log(c2 * m.X.shape[0])\n    kappa = np.sqrt(beta_t)\n    \n    xtest = np.concatenate((fixed.reshape(-1, 1), np.array(x).reshape(-1,1))).T\n    \n    preds = m.predict(xtest)\n    mean = preds[0][0][0] \n    \n    preds = m1.predict(xtest)\n    var = preds[1][0][0]\n    return mean + kappa * var\n\n\ndef optimize_acq(func, m, m1, fixed, num_f):\n    \n    print(\"Optimizing Acquisition Function...\\n\")\n    \n    opts = {'maxiter':200, 'maxfun':200, 'disp':False}\n    \n    T=10\n    best_value=-999\n    best_theta = m1.X[0,:]\n    \n    bounds = [(0,1) for _ in range(m.X.shape[1]-num_f)]\n    \n    for ii in range(T):\n        x0 = np.random.uniform(0,1, m.X.shape[1]-num_f)\n        \n        res = minimize(lambda x: -func(m, m1, x, fixed), x0, bounds=bounds, method=\"L-BFGS-B\", options=opts)\n        \n        val = func(m, m1, res.x, fixed)\n        if val > best_value:\n            best_value=val\n            best_theta =res.x\n    \n    return(np.clip(best_theta, 0, 1))\n\n\ndef select_config(Xraw, yraw, current, newpoint, bounds, num_f, num):\n    \n    length = select_length(Xraw, yraw, current, newpoint, bounds, num_f, num)\n    print(\"\\n\\nUsing length = {}\\n\\n\".format(length))\n    \n    Xraw = Xraw[-length:, :]\n    yraw = yraw[-length: ]\n        \n    base_vals = np.array(list(bounds.values())).T\n    oldpoints = Xraw[:, :num_f]\n    old_lims = np.concatenate((np.max(oldpoints, axis=0), np.min(oldpoints, axis=0))).reshape(2, oldpoints.shape[1])\n    limits = np.concatenate((old_lims, base_vals),axis=1)\n    \n    X = normalize(Xraw, limits)\n    y = standardize(yraw).reshape(yraw.size, 1)\n    \n    fixed = normalize(newpoint, oldpoints)\n\n    kernel = TV_SquaredExp(input_dim=X.shape[1], variance=1., lengthscale=1., epsilon=0.1)\n    try:\n        m = GPy.models.GPRegression(X, y, kernel)\n    except np.linalg.LinAlgError:\n        X, y = get_diverse(X, y)\n        m = GPy.models.GPRegression(X, y, kernel)\n    \n    try:\n        m.optimize(messages=True)\n    except np.linalg.LinAlgError:\n        X, y = get_diverse(X, y)\n        m = GPy.models.GPRegression(X, y, kernel)\n        m.optimize(messages=True)\n        \n    m.kern.lengthscale.fix(m.kern.lengthscale.clip(1e-5,1))\n    \n    if current is None:\n        m1 = deepcopy(m)\n    else:\n        padding = np.array([fixed for _ in range(current.shape[0])])\n        current = normalize(current, base_vals)\n        current = np.hstack((padding, current))\n\n        Xnew = np.vstack((X, current))\n        ypad = np.zeros(current.shape[0])\n        ypad = ypad.reshape(-1,1)\n        ynew = np.vstack((y, ypad))\n        \n        kernel = TV_SquaredExp(input_dim=X.shape[1], variance=1., lengthscale=1., epsilon=0.1)\n        m1 = GPy.models.GPRegression(Xnew, ynew, kernel)\n        m1.optimize()\n    \n    time_param = m1.time_se.epsilon[0]\n    lengthscale = m1.time_se.lengthscale[0]\n    xt = optimize_acq(UCB, m, m1, fixed, num_f)\n    \n    # check dist vs other points:\n    dists = euclidean_distances(X[:, num_f:], X[:, num_f:])\n    meandist = np.mean(dists)\n    min_new = np.min(euclidean_distances(X[:, num_f:], xt.reshape(-1,1).T))\n    \n    # convert back...\n    xt = xt * (np.max(base_vals,axis=0) - np.min(base_vals,axis=0)) + np.min(base_vals, axis=0)\n    \n    print(\"\\n**\\n\\n New Point: {} \\n\\n**\\n\".format(str(xt)))\n    print(\"\\n**\\n\\n Length Scale: {} \\n\\n**\\n\".format(str(lengthscale)))\n    print(\"\\n**\\n\\n Min Dist: {} \\n\\n**\\n\".format(str(min_new)))\n    print(\"\\n**\\n\\n vs. Mean Dist: {} \\n\\n**\\n\".format(str(meandist)))\n    \n    xt = xt.astype(np.float32)\n    return(xt, lengthscale, min_new, meandist)\n\n\n\nclass PB2(FIFOScheduler):\n    # ... (rest of class definition)\n    def __init__(self,\n                 time_attr=\"time_total_s\",\n                 reward_attr=None,\n                 metric=\"episode_reward_mean\",\n                 mode=\"max\",\n                 perturbation_interval=60.0,\n                 hyperparam_mutations={},\n                 quantile_fraction=0.25,\n                 resample_probability=0.25,\n                 custom_explore_fn=None,\n                 log_config=True):\n        # ... (initialization code) ...\n        self.meta = {'timesteps': [],'lengthscales': [], 'closest': [], 'meandist': []}\n        self.latest = 0 # when we last did bayesopt\n        self.data = pd.DataFrame()\n        \n        self.bounds = {}\n        for key, distribution in self._hyperparam_mutations.items():\n            self.bounds[key] = [np.min([distribution() for _ in range(999999)]),np.max([distribution() for _ in range(999999)])]\n\n    def _exploit(self, trial_executor, trial, trial_to_clone):\n        # ... (exploit logic)\n        print(\"\\n\\n\\n\\n Copying: \\n{} \\n with:{} \\n\\n\".format(str(trial), str(trial_to_clone)))\n        new_config, lengthscale, mindist, meandist, data = explore(self.data, self.bounds,\n                             self.current,\n                             trial_to_clone,\n                             trial,\n                             trial_to_clone.config,\n                             self._hyperparam_mutations,\n                             self._resample_probability)\n        \n        # important to replace the old values, since we are copying across\n        self.data = data.copy()\n        \n        # if the current guy youre selecting is at a point youve already done, \n        # then append the data to the \"current\" which is the points in the current batch\n        \n        new = []\n        for key in self._hyperparam_mutations.keys():\n            new.append(new_config[key])\n    \n        new  = np.array(new)\n        new = new.reshape(1, new.size)\n        if self.data['T'].max() > self.latest:\n            self.latest = self.data['T'].max()\n            self.current = new.copy()\n        else:\n            self.current = np.concatenate((self.current, new), axis=0)\n            print(\"\\n\\n\\n\\n\\n Currently Evaluating \\n\\n\\n\\n\\n\")\n            print(self.current)\n            print(\"\\n\\n\\n\\n\\n\")\n        \n        # log the lengthscale\n        self.meta['timesteps'].append(self.data['T'].values[-1])\n        self.meta['lengthscales'].append(lengthscale)\n        self.meta['closest'].append(mindist)\n        self.meta['meandist'].append(meandist)\n        meta = pd.DataFrame({'timesteps': self.meta['timesteps'], \n                             'lengthscales': self.meta['lengthscales'],\n                             'closest': self.meta['closest'],\n                             'meandist': self.meta['meandist']})\n        meta.to_csv('meta_data.csv')\n        \n        # ... (rest of _exploit method)",
    "experimental_info": "The PB2 scheduler is initialized with the following general settings:\n- `time_attr`: `args.criteria` (default \"timesteps_total\" in both `run_impala.py` and `run_ppo.py`).\n- `metric`: \"episode_reward_mean\".\n- `mode`: \"max\".\n- `resample_probability`: 0 (explicitly set for PB2, indicating no random resampling).\n- `quantile_fraction`: `args.perturb` (default 0.25), meaning the bottom 25% of trials copy from the top 25%.\n- `custom_explore_fn`: An `explore` function is used to postprocess perturbed configurations, ensuring `train_batch_size >= sgd_minibatch_size * 2`, `num_sgd_iter >= 1`, and casting specific parameters (`target_delay` for Impala, `train_batch_size` for PPO, `lambda` <= 1 for PPO) to valid types or ranges.\n\nSpecific settings for different algorithms:\n\n**For IMPALA (`run_impala.py`):\n**- `perturbation_interval`: `args.freq` (default 500,000 timesteps).\n- `num_samples`: `args.num_samples` (default 4).\n- `stop` criteria: `args.max` (default 3,000,000 timesteps).\n- Default environment: \"SpaceInvadersNoFrameskip-v4\".\n- `hyperparam_mutations` for PB2:\n    - \"epsilon\": `lambda: random.uniform(0.01, 0.5)`\n    - \"entropy_coeff\": `lambda: random.uniform(0.001, 0.1)`\n    - \"lr\": `lambda: random.uniform(1e-5, 1e-2)`\n\n**For PPO (`run_ppo.py`):\n**- `perturbation_interval`: `args.freq` (default 50,000 timesteps).\n- `num_samples`: `args.num_samples` (default 4).\n- `stop` criteria: `args.max` (default 1,000,000 timesteps).\n- Default environment: \"BipedalWalker-v2\".\n- `hyperparam_mutations` for PB2:\n    - \"lambda\": `lambda: random.uniform(0.9, 1.0)`\n    - \"clip_param\": `lambda: random.uniform(0.1, 0.5)`\n    - \"lr\": `lambda: random.uniform(1e-3, 1e-5)`\n    - \"train_batch_size\": `lambda: random.randint(int(args.batchsize.split(\"_\")[0]), int(args.batchsize.split(\"_\")[1]))` (where `args.batchsize` defaults to \"1000_60000\")"
}
