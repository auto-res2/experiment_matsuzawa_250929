
Input:
You are a researcher with expertise in engineering in the field of machine learning.

# Instructions
- The content described in “Repository Content” corresponds to the GitHub repository of the method described in “Method.”
- Please extract the following two pieces of information from “Repository Content”:
    - experimental_code：Extract the implementation sections that are directly related to the method described in “Method.”
    - experimental_info：Extract and output the experimental settings related to the method described in “Method.”

# Method
The core methodology involves a deep kernel network where a neural network transforms input features, which then feed into a Gaussian Process kernel for probabilistic predictions. The model is meta-learned using an adaptation of Model-Agnostic Meta-Learning (MAML) for GPs, where task-independent kernel parameters are optimized by maximizing the log marginal likelihood across a collection of source tasks via stochastic gradient ascent. To address label normalization challenges across diverse tasks, a task augmentation strategy is introduced, randomly scaling labels for each training batch to promote scale-invariant representations. For new target tasks, the deep kernel parameters are fine-tuned. A data-driven warm-start initialization is also proposed, utilizing an evolutionary algorithm to identify an optimal initial set of hyperparameters based on the surrogate model's predictions on source tasks, minimizing normalized regret.

# Repository Content
File Path: evaluate_metabo_adaboost.py
Content:
# Copyright (c) 2019 Robert Bosch GmbH
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>.

# ******************************************************************
# evaluate_metabo_adaboost.py
# Reproduce results from MetaBO paper on ADABOOST-hyperparameter optimization
# For convenience, we provide the pretrained weights resulting from the experiments described in the paper.
# These weights can be reproduced using train_metabo_adaboost.py
# ******************************************************************

# Note: due to licensing issues, the datasets used in this experiment cannot be shipped with the MetaBO package.
# However, you can download the datasets yourself from https://github.com/nicoschilling/ECML2016
# Put the folder "data/adaboost" from this repository into metabo/environment/hpo/data

import os
from metabo.eval.evaluate import eval_experiment
from metabo.eval.plot_results import plot_results
from metabo.environment.hpo.prepare_data import prepare_hpo_data
from metabo.policies.taf.generate_taf_data_hpo import generate_taf_data_hpo
from gym.envs.registration import register, registry
from datetime import datetime

# set evaluation parameters
afs_to_evaluate = ["MetaBO", "TAF-ME", "TAF-RANKING", "EI"]
rootdir = os.path.join(os.path.dirname(os.path.realpath(__file__)), "metabo")
logpath = os.path.join(rootdir, "iclr2020", "hpo", "MetaBO-ADABOOST-v0")
savepath = os.path.join(logpath, "eval", datetime.strftime(datetime.now(), "%Y-%m-%d-%H-%M-%S"))
n_workers = 1
n_episodes = 15  # 15 test sets

prepare_hpo_data(model="adaboost", datapath=os.path.join(rootdir, "environment", "hpo", "data", "adaboost"))

# evaluate all afs
for af in afs_to_evaluate:
    # set af-specific parameters
    if af == "MetaBO":
        features = ["posterior_mean", "posterior_std", "timestep", "budget", "x"]
        pass_X_to_pi = False
        n_init_samples = 0
        load_iter = 499  # determined via leave-one-out cross-validation on the training set
        deterministic = True
        policy_specs = {}  # will be loaded from the logfiles
    elif af == "TAF-ME" or af == "TAF-RANKING":
        generate_taf_data_hpo(model="adaboost", datapath=os.path.join(rootdir, "environment", "hpo", "processed"))
        features = ["posterior_mean", "posterior_std", "incumbent", "timestep", "x"]
        pass_X_to_pi = True
        n_init_samples = 0
        load_iter = None  # does only apply for MetaBO
        deterministic = None  # does only apply for MetaBO
        policy_specs = {"TAF_datafile": os.path.join(rootdir, "policies", "taf", "taf_adaboost_M_35_N_108.pkl")}
    else:
        features = ["posterior_mean", "posterior_std", "incumbent", "timestep"]
        pass_X_to_pi = False
        n_init_samples = 0  # no initial design for discrete domain
        load_iter = None  # does only apply for MetaBO
        deterministic = None  # does only apply for MetaBO
        if af == "EI":
            policy_specs = {}
        elif af == "Random":
            policy_specs = {}
        else:
            raise ValueError("Unknown AF!")

    # define environment
    env_spec = {
        "env_id": "MetaBO-ADABOOST-v0",
        "D": 2,
        "f_type": "HPO",
        "f_opts": {
            "hpo_data_file": os.path.join(rootdir, "environment", "hpo", "processed", "adaboost", "objectives.pkl"),
            "hpo_gp_hyperparameters_file": os.path.join(rootdir, "environment", "hpo", "processed", "adaboost",
                                                        "gp_hyperparameters.pkl"),
            "hpo_datasets_file": os.path.join(rootdir, "environment", "hpo", "processed", "adaboost",
                                              "test_datasets_iclr2020.txt"),
            "draw_random_datasets": False,  # present each test function once
            "min_regret": 0.0},
        "features": features,
        "T": 15,
        "n_init_samples": n_init_samples,
        "pass_X_to_pi": pass_X_to_pi,
        # GP hyperparameters will be set individually for each new function, the parameters were determined off-line
        # via type-2-ML on all available data
        "kernel_lengthscale": None,
        "kernel_variance": None,
        "noise_variance": None,
        "use_prior_mean_function": True,
        "local_af_opt": False,  # discrete domain
        "cardinality_domain": 108,
        "reward_transformation": "none",
    }

    # register gym environment
    if env_spec["env_id"] in registry.env_specs:
        del registry.env_specs[env_spec["env_id"]]
    register(
        id=env_spec["env_id"],
        entry_point="metabo.environment.metabo_gym:MetaBO",
        max_episode_steps=env_spec["T"],
        reward_threshold=None,
        kwargs=env_spec
    )

    # define evaluation run
    eval_spec = {
        "env_id": env_spec["env_id"],
        "env_seed_offset": 100,
        "policy": af,
        "logpath": logpath,
        "load_iter": load_iter,
        "deterministic": deterministic,
        "policy_specs": policy_specs,
        "savepath": savepath,
        "n_workers": n_workers,
        "n_episodes": n_episodes,
        "T": env_spec["T"],
    }

    # perform evaluation
    print("Evaluating {} on {}...".format(af, env_spec["env_id"]))
    eval_experiment(eval_spec)
    print("Done! Saved result in {}".format(savepath))
    print("**********************\n\n")

# plot (plot is saved to savepath)
print("Plotting...")
plot_results(path=savepath, logplot=True)
print("Done! Saved plot in {}".format(savepath))

File Path: evaluate_metabo_branin.py
Content:
# Copyright (c) 2019 Robert Bosch GmbH
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>.

# ******************************************************************
# evaluate_metabo_branin.py
# Reproduce results from MetaBO paper on Branin-Function
# For convenience, we provide the pretrained weights resulting from the experiments described in the paper.
# These weights can be reproduced using train_metabo_branin.py
# ******************************************************************

import os
from metabo.eval.evaluate import eval_experiment
from metabo.eval.plot_results import plot_results
from metabo.policies.taf.generate_taf_data_branin import generate_taf_data_branin
from gym.envs.registration import register, registry
from datetime import datetime

# set evaluation parameters
afs_to_evaluate = ["MetaBO", "TAF-ME", "TAF-RANKING", "EI", "Random"]
# afs_to_evaluate = ["MetaBO", "EPS-GREEDY", "GMM-UCB", "EI"]
rootdir = os.path.join(os.path.dirname(os.path.realpath(__file__)), "metabo")
logpath = os.path.join(rootdir, "iclr2020", "gobfcts", "bra", "M_50", "MetaBO-BRA-v0")
savepath = os.path.join(logpath, "eval", datetime.strftime(datetime.now(), "%Y-%m-%d-%H-%M-%S"))
n_workers = 10
n_episodes = 100

# evaluate all afs
for af in afs_to_evaluate:
    # set af-specific parameters
    if af == "MetaBO":
        features = ["posterior_mean", "posterior_std", "timestep", "budget", "x"]
        pass_X_to_pi = False
        n_init_samples = 0
        load_iter = 1635  # best ppo iteration during training, determined via metabo/ppo/util/get_best_iter_idx
        deterministic = True
        policy_specs = {}  # will be loaded from the logfiles
    elif af == "TAF-ME" or af == "TAF-RANKING":
        generate_taf_data_branin(M=50, N=100)
        features = ["posterior_mean", "posterior_std", "incumbent", "timestep", "x"]
        pass_X_to_pi = True
        n_init_samples = 0
        load_iter = None  # does only apply for MetaBO
        deterministic = None  # does only apply for MetaBO
        policy_specs = {"TAF_datafile": os.path.join(rootdir, "policies", "taf", "taf_branin_M_50_N_100.pkl")}
    elif af == "EPS-GREEDY":
        # we use the same data as we used for TAF/METABO-M
        generate_taf_data_branin(M=50, N=100)
        features = ["posterior_mean", "posterior_std", "incumbent", "timestep", "budget", "x"]
        pass_X_to_pi = False
        n_init_samples = 0
        load_iter = None  # does only apply for MetaBO
        deterministic = None  # does only apply for MetaBO
        policy_specs = {"datafile": os.path.join(rootdir, "policies", "taf", "taf_branin_M_50_N_100.pkl"),
                        "eps": "linear_schedule"}
    elif af == "GMM-UCB":
        # we use the same data as we used for TAF/METABO-M
        generate_taf_data_branin(M=50, N=100)
        features = ["posterior_mean", "posterior_std", "x", "timestep", "budget"]
        pass_X_to_pi = False
        n_init_samples = 0
        load_iter = None  # does only apply for MetaBO
        deterministic = None  # does only apply for MetaBO
        policy_specs = {"datafile": os.path.join(rootdir, "policies", "taf", "taf_branin_M_50_N_100.pkl"),
                        "ucb_kappa": 2.0, "w": 0.22, "n_components": 3}
    else:
        features = ["posterior_mean", "posterior_std", "incumbent", "timestep"]
        pass_X_to_pi = False
        n_init_samples = 1
        load_iter = None  # does only apply for MetaBO
        deterministic = None  # does only apply for MetaBO
        if af == "EI":
            policy_specs = {}
        elif af == "Random":
            policy_specs = {}
        else:
            raise ValueError("Unknown AF!")

    # define environment
    env_spec = {
        "env_id": "MetaBO-BRA-v0",
        "D": 2,
        "f_type": "BRA-var",
        "f_opts": {"bound_translation": 0.1,
                   "bound_scaling": 0.1},
        "features": features,
        "T": 30,
        "n_init_samples": n_init_samples,
        "pass_X_to_pi": pass_X_to_pi,
        # parameters were determined offline via type-2-ML on a GP with 100 datapoints
        "kernel_lengthscale": [0.235, 0.578],
        "kernel_variance": 2.0,
        "noise_variance": 8.9e-16,
        "use_prior_mean_function": False,
        "local_af_opt": True,
        "N_MS": 1000,
        "N_LS": 1000,
        "k": 5,
        "reward_transformation": "none",
    }

    # register gym environment
    if env_spec["env_id"] in registry.env_specs:
        del registry.env_specs[env_spec["env_id"]]
    register(
        id=env_spec["env_id"],
        entry_point="metabo.environment.metabo_gym:MetaBO",
        max_episode_steps=env_spec["T"],
        reward_threshold=None,
        kwargs=env_spec
    )

    # define evaluation run
    eval_spec = {
        "env_id": env_spec["env_id"],
        "env_seed_offset": 100,
        "policy": af,
        "logpath": logpath,
        "load_iter": load_iter,
        "deterministic": deterministic,
        "policy_specs": policy_specs,
        "savepath": savepath,
        "n_workers": n_workers,
        "n_episodes": n_episodes,
        "T": env_spec["T"],
    }

    # perform evaluation
    print("Evaluating {} on {}...".format(af, env_spec["env_id"]))
    eval_experiment(eval_spec)
    print("Done! Saved result in {}".format(savepath))
    print("**********************\n\n")

# plot (plot is saved to savepath)
print("Plotting...")
plot_results(path=savepath, logplot=True)
print("Done! Saved plot in {}".format(savepath))

File Path: evaluate_metabo_furuta.py
Content:
# Copyright (c) 2019 Robert Bosch GmbH
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>.

# ******************************************************************
# evaluate_metabo_furuta.py
# Reproduce results from MetaBO paper on Furuta control task in simulation
# For convenience, we provide the pretrained weights resulting from the experiments described in the paper.
# These weights can be reproduced using train_metabo_furuta.py
# ******************************************************************

import os
from metabo.eval.evaluate import eval_experiment
from metabo.eval.plot_results import plot_results
from metabo.policies.taf.generate_taf_data_furuta import generate_taf_data_furuta
from gym.envs.registration import register, registry
from datetime import datetime

# set evaluation parameters
afs_to_evaluate = ["MetaBO", "TAF-ME", "TAF-RANKING", "EI"]
rootdir = os.path.join(os.path.dirname(os.path.realpath(__file__)), "metabo")
logpath = os.path.join(rootdir, "iclr2020", "furuta", "full", "MetaBO-Furuta-v0")
savepath = os.path.join(logpath, "eval", datetime.strftime(datetime.now(), "%Y-%m-%d-%H-%M-%S"))
n_workers = 10
n_episodes = 100

# evaluate all afs
for af in afs_to_evaluate:
    # set af-specific parameters
    if af == "MetaBO":
        features = ["posterior_mean", "posterior_std", "x"]
        pass_X_to_pi = False
        n_init_samples = 0
        load_iter = 1405  # best ppo iteration during training, determined via metabo/ppo/util/get_best_iter_idx
        deterministic = True
        policy_specs = {}  # will be loaded from the logfiles
    elif af == "TAF-ME" or af == "TAF-RANKING":
        generate_taf_data_furuta(M=100)
        features = ["posterior_mean", "posterior_std", "incumbent", "timestep", "x"]
        pass_X_to_pi = True
        n_init_samples = 0
        load_iter = None  # does only apply for MetaBO
        deterministic = None  # does only apply for MetaBO
        policy_specs = {"TAF_datafile": os.path.join(rootdir, "policies", "taf", "taf_furuta_M_100_N_200.pkl")}
    else:
        features = ["posterior_mean", "posterior_std", "incumbent", "timestep"]
        pass_X_to_pi = False
        n_init_samples = 1
        load_iter = None  # does only apply for MetaBO
        deterministic = None  # does only apply for MetaBO
        if af == "EI":
            policy_specs = {}
        elif af == "Random":
            policy_specs = {}
        else:
            raise ValueError("Unknown AF!")

    # define environment
    true_mass_arm = 0.095
    true_mass_pendulum = 0.024
    true_length_arm = 0.085
    true_length_pendulum = 0.129
    low_mult = 0.1
    high_mult = 2.0
    env_spec = {
        "env_id": "MetaBO-Furuta-v0",
        "D": 4,
        "f_type": "Furuta",
        "f_opts": {"furuta_domain": [[-0.5, 0.2],
                                     [-1.6, 4.0],
                                     [-0.1, 0.04],
                                     [-0.04, 0.1]],
                   "mass_arm_low": low_mult * true_mass_arm,
                   "mass_arm_high": high_mult * true_mass_arm,
                   "mass_pendulum_low": low_mult * true_mass_pendulum,
                   "mass_pendulum_high": high_mult * true_mass_pendulum,
                   "length_arm_low": low_mult * true_length_arm,
                   "length_arm_high": high_mult * true_length_arm,
                   "length_pendulum_low": low_mult * true_length_pendulum,
                   "length_pendulum_high": high_mult * true_length_pendulum,
                   "pos": [0, 1, 2, 3]},
        "features": features,
        "T": 50,
        "n_init_samples": n_init_samples,
        "pass_X_to_pi": pass_X_to_pi,
        "kernel_lengthscale": [0.1, 0.1, 0.1, 0.1],
        "kernel_variance": 1.5,
        "noise_variance": 1e-2,
        "use_prior_mean_function": True,
        "local_af_opt": True,
        "N_MS": 10000,
        "N_LS": 1000,
        "k": 5,
        "reward_transformation": "none",
    }

    # register gym environment
    if env_spec["env_id"] in registry.env_specs:
        del registry.env_specs[env_spec["env_id"]]
    register(
        id=env_spec["env_id"],
        entry_point="metabo.environment.metabo_gym:MetaBO",
        max_episode_steps=env_spec["T"],
        reward_threshold=None,
        kwargs=env_spec
    )

    # define evaluation run
    eval_spec = {
        "env_id": env_spec["env_id"],
        "env_seed_offset": 100,
        "policy": af,
        "logpath": logpath,
        "load_iter": load_iter,
        "deterministic": deterministic,
        "policy_specs": policy_specs,
        "savepath": savepath,
        "n_workers": n_workers,
        "n_episodes": n_episodes,
        "T": env_spec["T"],
    }

    # perform evaluation
    print("Evaluating {} on {}...".format(af, env_spec["env_id"]))
    eval_experiment(eval_spec)
    print("Done! Saved result in {}".format(savepath))
    print("**********************\n\n")

# plot (plot is saved to savepath)
print("Plotting...")
plot_results(path=savepath, logplot=False)
print("Done! Saved plot in {}".format(savepath))

File Path: evaluate_metabo_gprice.py
Content:
# Copyright (c) 2019 Robert Bosch GmbH
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>.

# ******************************************************************
# evaluate_metabo_gprice.py
# Reproduce results from MetaBO paper on GPrice-Function
# For convenience, we provide the pretrained weights resulting from the experiments described in the paper.
# These weights can be reproduced using train_metabo_gprice.py
# ******************************************************************

import os
from metabo.eval.evaluate import eval_experiment
from metabo.eval.plot_results import plot_results
from metabo.policies.taf.generate_taf_data_gprice import generate_taf_data_gprice
from gym.envs.registration import register, registry
from datetime import datetime

# set evaluation parameters
afs_to_evaluate = ["MetaBO", "TAF-ME", "TAF-RANKING", "EI", "Random"]
# afs_to_evaluate = ["MetaBO", "EPS-GREEDY", "GMM-UCB", "EI"]
rootdir = os.path.join(os.path.dirname(os.path.realpath(__file__)), "metabo")
logpath = os.path.join(rootdir, "iclr2020", "gobfcts", "gprice", "M_50", "MetaBO-GPRICE-v0")
savepath = os.path.join(logpath, "eval", datetime.strftime(datetime.now(), "%Y-%m-%d-%H-%M-%S"))
n_workers = 10
n_episodes = 100

# evaluate all afs
for af in afs_to_evaluate:
    # set af-specific parameters
    if af == "MetaBO":
        features = ["posterior_mean", "posterior_std", "timestep", "budget", "x"]
        pass_X_to_pi = False
        n_init_samples = 0
        load_iter = 1703  # best ppo iteration during training, determined via metabo/ppo/util/get_best_iter_idx
        deterministic = True
        policy_specs = {}  # will be loaded from the logfiles
    elif af == "TAF-ME" or af == "TAF-RANKING":
        generate_taf_data_gprice(M=50, N=100)
        features = ["posterior_mean", "posterior_std", "incumbent", "timestep", "x"]
        pass_X_to_pi = True
        n_init_samples = 0
        load_iter = None  # does only apply for MetaBO
        deterministic = None  # does only apply for MetaBO
        policy_specs = {"TAF_datafile": os.path.join(rootdir, "policies", "taf", "taf_gprice_M_50_N_100.pkl")}
    elif af == "EPS-GREEDY":
        # we use the same data as we used for TAF/METABO-M
        generate_taf_data_gprice(M=50, N=100)
        features = ["posterior_mean", "posterior_std", "incumbent", "timestep", "budget", "x"]
        pass_X_to_pi = False
        n_init_samples = 0
        load_iter = None  # does only apply for MetaBO
        deterministic = None  # does only apply for MetaBO
        policy_specs = {"datafile": os.path.join(rootdir, "policies", "taf", "taf_gprice_M_50_N_100.pkl"),
                        "eps": 0.55}
    elif af == "GMM-UCB":
        # we use the same data as we used for TAF/METABO-M
        generate_taf_data_gprice(M=50, N=100)
        features = ["posterior_mean", "posterior_std", "x", "timestep", "budget"]
        pass_X_to_pi = False
        n_init_samples = 0
        load_iter = None  # does only apply for MetaBO
        deterministic = None  # does only apply for MetaBO
        policy_specs = {"datafile": os.path.join(rootdir, "policies", "taf", "taf_gprice_M_50_N_100.pkl"),
                        "ucb_kappa": 2.0, "w": 0.22, "n_components": 1}
    else:
        features = ["posterior_mean", "posterior_std", "incumbent", "timestep"]
        pass_X_to_pi = False
        n_init_samples = 1
        load_iter = None  # does only apply for MetaBO
        deterministic = None  # does only apply for MetaBO
        if af == "EI":
            policy_specs = {}
        elif af == "Random":
            policy_specs = {}
        else:
            raise ValueError("Unknown AF!")

    # define environment
    env_spec = {
        "env_id": "MetaBO-GPRICE-v0",
        "D": 2,
        "f_type": "GPRICE-var",
        "f_opts": {"bound_translation": 0.1,
                   "bound_scaling": 0.1},
        "features": features,
        "T": 30,
        "n_init_samples": n_init_samples,
        "pass_X_to_pi": pass_X_to_pi,
        # parameters were determined offline via type-2-ML on a GP with 100 datapoints
        "kernel_lengthscale": [0.130, 0.07],
        "kernel_variance": 0.616,
        "noise_variance": 1e-6,
        "use_prior_mean_function": False,
        "local_af_opt": True,
        "N_MS": 1000,
        "N_LS": 1000,
        "k": 5,
        "reward_transformation": "none",
    }

    # register gym environment
    if env_spec["env_id"] in registry.env_specs:
        del registry.env_specs[env_spec["env_id"]]
    register(
        id=env_spec["env_id"],
        entry_point="metabo.environment.metabo_gym:MetaBO",
        max_episode_steps=env_spec["T"],
        reward_threshold=None,
        kwargs=env_spec
    )

    # define evaluation run
    eval_spec = {
        "env_id": env_spec["env_id"],
        "env_seed_offset": 100,
        "policy": af,
        "logpath": logpath,
        "load_iter": load_iter,
        "deterministic": deterministic,
        "policy_specs": policy_specs,
        "savepath": savepath,
        "n_workers": n_workers,
        "n_episodes": n_episodes,
        "T": env_spec["T"],
    }

    # perform evaluation
    print("Evaluating {} on {}...".format(af, env_spec["env_id"]))
    eval_experiment(eval_spec)
    print("Done! Saved result in {}".format(savepath))
    print("**********************\n\n")

# plot (plot is saved to savepath)
print("Plotting...")
plot_results(path=savepath, logplot=True)
print("Done! Saved plot in {}".format(savepath))

File Path: evaluate_metabo_gps.py
Content:
# Copyright (c) 2019 Robert Bosch GmbH
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>.

# ******************************************************************
# evaluate_metabo_gps.py
# Reproduce results from MetaBO paper on GP-samples
# For convenience, we provide the pretrained weights resulting from the experiments described in the paper.
# These weights can be reproduced using train_metabo_gps.py
# ******************************************************************

import os
from metabo.eval.evaluate import eval_experiment
from metabo.eval.plot_results2 import plot_results2
from gym.envs.registration import register, registry
from datetime import datetime

# set evaluation parameters
afs_to_evaluate = ["MetaBO", "EI", "Random"]
rootdir = os.path.join(os.path.dirname(os.path.realpath(__file__)), "metabo")
logpath = os.path.join(rootdir, "iclr2020", "MetaBO-GP-v0", "Matern52", "2019-11-11-21-05-02")
savepath = os.path.join(logpath, "eval", datetime.strftime(datetime.now(), "%Y-%m-%d-%H-%M-%S"))
n_workers = 10
n_episodes = 100

# evaluate all afs
for af in afs_to_evaluate:
    # set af-specific parameters
    if af == "MetaBO":
        features = ["posterior_mean", "posterior_std", "incumbent", "timestep_perc", "timestep",
                    "budget"]  # dimensionality agnostic
        pass_X_to_pi = False
        n_init_samples = 0
        load_iter = 1161  # determined via metabo.ppo.util.get_best_iter_idx()
        T_training = None
        deterministic = False
        policy_specs = {}  # will be loaded from the logfiles
    else:
        features = ["posterior_mean", "posterior_std", "incumbent", "timestep"]
        T_training = None
        pass_X_to_pi = False
        n_init_samples = 1
        load_iter = None  # does only apply for MetaBO
        deterministic = None  # does only apply for MetaBO
        if af == "EI":
            policy_specs = {}
        elif af == "Random":
            policy_specs = {}
        else:
            raise ValueError("Unknown AF!")

    # define environment
    kernel = "Matern52"
    env_spec = {
        "env_id": "MetaBO-GP-v0",
        "D": 5,  # MetaBO is dimensionality agnostic and can be evaluated for any D
        "f_type": "GP",
        "f_opts": {"kernel": kernel,
                   "lengthscale_low": 0.05,
                   "lengthscale_high": 0.5,
                   "noise_var_low": 0.1,
                   "noise_var_high": 0.1,
                   "signal_var_low": 1.0,
                   "signal_var_high": 1.0,
                   "min_regret": 0},
        "features": features,
        "T": 140,
        "T_training": T_training,
        "n_init_samples": n_init_samples,
        "pass_X_to_pi": pass_X_to_pi,
        # will be set individually for each new function to the sampled hyperparameters
        "kernel": kernel,
        "kernel_lengthscale": None,
        "kernel_variance": None,
        "noise_variance": None,
        "use_prior_mean_function": True,
        "local_af_opt": True,
        "N_MS": 4000,
        "N_LS": 4000,
        "k": 5,
        "reward_transformation": "none",
    }

    # register gym environment
    if env_spec["env_id"] in registry.env_specs:
        del registry.env_specs[env_spec["env_id"]]
    register(
        id=env_spec["env_id"],
        entry_point="metabo.environment.metabo_gym:MetaBO",
        max_episode_steps=env_spec["T"],
        reward_threshold=None,
        kwargs=env_spec
    )

    # define evaluation run
    eval_spec = {
        "env_id": env_spec["env_id"],
        "env_seed_offset": 100,
        "policy": af,
        "logpath": logpath,
        "load_iter": load_iter,
        "deterministic": deterministic,
        "policy_specs": policy_specs,
        "savepath": savepath,
        "n_workers": n_workers,
        "n_episodes": n_episodes,
        "T": env_spec["T"],
    }

    # perform evaluation
    print("Evaluating {} on {}...".format(af, env_spec["env_id"]))
    eval_experiment(eval_spec)
    print("Done! Saved result in {}".format(savepath))
    print("**********************\n\n")

# plot (plot is saved to savepath)
print("Plotting...")
plot_results2(path=savepath, logplot=True)
print("Done! Saved plot in {}".format(savepath))

File Path: evaluate_metabo_hm3.py
Content:
# Copyright (c) 2019 Robert Bosch GmbH
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>.

# ******************************************************************
# evaluate_metabo_hm3.py
# Reproduce results from MetaBO paper on Hartmann-3-Function
# For convenience, we provide the pretrained weights resulting from the experiments described in the paper.
# These weights can be reproduced using train_metabo_hm3.py
# ******************************************************************

import os
from metabo.eval.evaluate import eval_experiment
from metabo.eval.plot_results import plot_results
from metabo.policies.taf.generate_taf_data_hm3 import generate_taf_data_hm3
from gym.envs.registration import register, registry
from datetime import datetime

# set evaluation parameters
afs_to_evaluate = ["MetaBO", "TAF-ME", "TAF-RANKING", "EI", "Random"]
# afs_to_evaluate = ["MetaBO", "EPS-GREEDY", "GMM-UCB", "EI"]
rootdir = os.path.join(os.path.dirname(os.path.realpath(__file__)), "metabo")
logpath = os.path.join(rootdir, "iclr2020", "gobfcts", "hm3", "M_50", "MetaBO-HM3-v0")
savepath = os.path.join(logpath, "eval", datetime.strftime(datetime.now(), "%Y-%m-%d-%H-%M-%S"))
n_workers = 10
n_episodes = 100

# evaluate all afs
for af in afs_to_evaluate:
    # set af-specific parameters
    if af == "MetaBO":
        features = ["posterior_mean", "posterior_std", "timestep", "budget", "x"]
        pass_X_to_pi = False
        n_init_samples = 0
        load_iter = 1988  # best ppo iteration during training, determined via metabo/ppo/util/get_best_iter_idx
        deterministic = True
        policy_specs = {}  # will be loaded from the logfiles
    elif af == "TAF-ME" or af == "TAF-RANKING":
        generate_taf_data_hm3(M=50, N=100)
        features = ["posterior_mean", "posterior_std", "incumbent", "timestep", "x"]
        pass_X_to_pi = True
        n_init_samples = 0
        load_iter = None  # does only apply for MetaBO
        deterministic = None  # does only apply for MetaBO
        policy_specs = {"TAF_datafile": os.path.join(rootdir, "policies", "taf", "taf_hm3_M_50_N_100.pkl")}
    elif af == "EPS-GREEDY":
        # we use the same data as we used for TAF/METABO-M
        generate_taf_data_hm3(M=50, N=100)
        features = ["posterior_mean", "posterior_std", "incumbent", "timestep", "budget", "x"]
        pass_X_to_pi = False
        n_init_samples = 0
        load_iter = None  # does only apply for MetaBO
        deterministic = None  # does only apply for MetaBO
        policy_specs = {"datafile": os.path.join(rootdir, "policies", "taf", "taf_hm3_M_50_N_100.pkl"),
                        "eps": "linear_schedule"}
    elif af == "GMM-UCB":
        # we use the same data as we used for TAF/METABO-M
        generate_taf_data_hm3(M=50, N=100)
        features = ["posterior_mean", "posterior_std", "x", "timestep", "budget"]
        pass_X_to_pi = False
        n_init_samples = 0
        load_iter = None  # does only apply for MetaBO
        deterministic = None  # does only apply for MetaBO
        policy_specs = {"datafile": os.path.join(rootdir, "policies", "taf", "taf_hm3_M_50_N_100.pkl"),
                        "ucb_kappa": 2.0, "w": 0.22, "n_components": 3}
    else:
        features = ["posterior_mean", "posterior_std", "incumbent", "timestep"]
        pass_X_to_pi = False
        n_init_samples = 1
        load_iter = None  # does only apply for MetaBO
        deterministic = None  # does only apply for MetaBO
        if af == "EI":
            policy_specs = {}
        elif af == "Random":
            policy_specs = {}
        else:
            raise ValueError("Unknown AF!")

    # define environment
    env_spec = {
        "env_id": "MetaBO-HM3-v0",
        "D": 3,
        "f_type": "HM3-var",
        "f_opts": {"bound_translation": 0.1,
                   "bound_scaling": 0.1},
        "features": features,
        "T": 30,
        "n_init_samples": n_init_samples,
        "pass_X_to_pi": pass_X_to_pi,
        # parameters were determined offline via type-2-ML on a GP with 100 datapoints
        "kernel_lengthscale": [0.716, 0.298, 0.186],
        "kernel_variance": 0.83,
        "noise_variance": 1.688e-11,
        "use_prior_mean_function": False,
        "local_af_opt": True,
        "N_MS": 2000,
        "N_LS": 2000,
        "k": 5,
        "reward_transformation": "none",
    }

    # register gym environment
    if env_spec["env_id"] in registry.env_specs:
        del registry.env_specs[env_spec["env_id"]]
    register(
        id=env_spec["env_id"],
        entry_point="metabo.environment.metabo_gym:MetaBO",
        max_episode_steps=env_spec["T"],
        reward_threshold=None,
        kwargs=env_spec
    )

    # define evaluation run
    eval_spec = {
        "env_id": env_spec["env_id"],
        "env_seed_offset": 100,
        "policy": af,
        "logpath": logpath,
        "load_iter": load_iter,
        "deterministic": deterministic,
        "policy_specs": policy_specs,
        "savepath": savepath,
        "n_workers": n_workers,
        "n_episodes": n_episodes,
        "T": env_spec["T"],
    }

    # perform evaluation
    print("Evaluating {} on {}...".format(af, env_spec["env_id"]))
    eval_experiment(eval_spec)
    print("Done! Saved result in {}".format(savepath))
    print("**********************\n\n")

# plot (plot is saved to savepath)
print("Plotting...")
plot_results(path=savepath, logplot=True)
print("Done! Saved plot in {}".format(savepath))

File Path: evaluate_metabo_svm.py
Content:
# Copyright (c) 2019 Robert Bosch GmbH
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>.

# ******************************************************************
# evaluate_metabo_svm.py
# Reproduce results from MetaBO paper on SVM-hyperparameter optimization
# For convenience, we provide the pretrained weights resulting from the experiments described in the paper.
# These weights can be reproduced using train_metabo_svm.py
# ******************************************************************

# Note: due to licensing issues, the datasets used in this experiment cannot be shipped with the MetaBO package.
# However, you can download the datasets yourself from https://github.com/nicoschilling/ECML2016
# Put the folder "data/svm" from this repository into metabo/environment/hpo/data

import os
from metabo.eval.evaluate import eval_experiment
from metabo.eval.plot_results import plot_results
from metabo.environment.hpo.prepare_data import prepare_hpo_data
from metabo.policies.taf.generate_taf_data_hpo import generate_taf_data_hpo
from gym.envs.registration import register, registry
from datetime import datetime

# set evaluation parameters
afs_to_evaluate = ["MetaBO", "TAF-ME", "TAF-RANKING", "EI"]
rootdir = os.path.join(os.path.dirname(os.path.realpath(__file__)), "metabo")
logpath = os.path.join(rootdir, "iclr2020", "hpo", "MetaBO-SVM-v0")
savepath = os.path.join(logpath, "eval", datetime.strftime(datetime.now(), "%Y-%m-%d-%H-%M-%S"))
n_workers = 1
n_episodes = 15  # 15 test sets

prepare_hpo_data(model="svm", datapath=os.path.join(rootdir, "environment", "hpo", "data", "svm"))

# evaluate all afs
for af in afs_to_evaluate:
    # set af-specific parameters
    if af == "MetaBO":
        features = ["posterior_mean", "posterior_std", "timestep", "budget", "x"]
        pass_X_to_pi = False
        n_init_samples = 0
        load_iter = 363  # determined via leave-one-out cross-validation on the training set
        deterministic = True
        policy_specs = {}  # will be loaded from the logfiles
    elif af == "TAF-ME" or af == "TAF-RANKING":
        generate_taf_data_hpo(model="svm", datapath=os.path.join(rootdir, "environment", "hpo", "processed"))
        features = ["posterior_mean", "posterior_std", "incumbent", "timestep", "x"]
        pass_X_to_pi = True
        n_init_samples = 0
        load_iter = None  # does only apply for MetaBO
        deterministic = None  # does only apply for MetaBO
        policy_specs = {"TAF_datafile": os.path.join("policies", "taf", "taf_svm_M_35_N_143.pkl")}
    else:
        features = ["posterior_mean", "posterior_std", "incumbent", "timestep"]
        pass_X_to_pi = False
        n_init_samples = 0  # no initial design for discrete domain
        load_iter = None  # does only apply for MetaBO
        deterministic = None  # does only apply for MetaBO
        if af == "EI":
            policy_specs = {}
        elif af == "Random":
            policy_specs = {}
        else:
            raise ValueError("Unknown AF!")

    # define environment
    env_spec = {
        "env_id": "MetaBO-SVM-v0",
        "D": 2,
        "f_type": "HPO",
        "f_opts": {
            "hpo_data_file": os.path.join(rootdir, "environment", "hpo", "processed", "svm", "objectives.pkl"),
            "hpo_gp_hyperparameters_file": os.path.join(rootdir, "environment", "hpo", "processed", "svm",
                                                        "gp_hyperparameters.pkl"),
            "hpo_datasets_file": os.path.join(rootdir, "environment", "hpo", "processed", "svm",
                                              "test_datasets_iclr2020.txt"),
            "draw_random_datasets": False,  # present each test function once
            "min_regret": 0.0},
        "features": features,
        "T": 20,
        "n_init_samples": n_init_samples,
        "pass_X_to_pi": pass_X_to_pi,
        # GP hyperparameters will be set individually for each new function, the parameters were determined off-line
        # via type-2-ML on all available data
        "kernel_lengthscale": None,
        "kernel_variance": None,
        "noise_variance": None,
        "use_prior_mean_function": True,
        "local_af_opt": False,  # discrete domain
        "cardinality_domain": 143,
        "reward_transformation": "none",
    }

    # register gym environment
    if env_spec["env_id"] in registry.env_specs:
        del registry.env_specs[env_spec["env_id"]]
    register(
        id=env_spec["env_id"],
        entry_point="metabo.environment.metabo_gym:MetaBO",
        max_episode_steps=env_spec["T"],
        reward_threshold=None,
        kwargs=env_spec
    )

    # define evaluation run
    eval_spec = {
        "env_id": env_spec["env_id"],
        "env_seed_offset": 100,
        "policy": af,
        "logpath": logpath,
        "load_iter": load_iter,
        "deterministic": deterministic,
        "policy_specs": policy_specs,
        "savepath": savepath,
        "n_workers": n_workers,
        "n_episodes": n_episodes,
        "T": env_spec["T"],
    }

    # perform evaluation
    print("Evaluating {} on {}...".format(af, env_spec["env_id"]))
    eval_experiment(eval_spec)
    print("Done! Saved result in {}".format(savepath))
    print("**********************\n\n")

# plot (plot is saved to savepath)
print("Plotting...")
plot_results(path=savepath, logplot=True)
print("Done! Saved plot in {}".format(savepath))

File Path: metabo/__init__.py
Content:

File Path: metabo/environment/__init__.py
Content:

File Path: metabo/environment/furuta.py
Content:
# Copyright (c) 2019 Robert Bosch GmbH
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>.

# ******************************************************************
# furuta.py
# Implementation of the objective function for the Furuta control experiment in simulation.
# ******************************************************************

import numpy as np
import warnings

from metabo.environment.simcore.controller import dlqr
from metabo.environment.simcore.environment import furuta as env_furuta
from metabo.environment.simcore.parameters import simulation_parameters as simparam


def init_furuta_simulation(mass_pendulum, mass_arm, length_pendulum, length_arm, damp_arm=0.0, damp_pendulum=0.0):
    sim_param = simparam.SimulationParameters(runtime=3.00, dt=0.01)
    environment = env_furuta.FurutaPendulum(sim_param, noise=False, estimate=False, animate=False, visualize=False,
                                            mass_arm=mass_arm, mass_pendulum=mass_pendulum,
                                            length_arm=length_arm, length_pendulum=length_pendulum,
                                            damp_arm=damp_arm, damp_pendulum=damp_pendulum)

    x0 = np.array([0.0, np.deg2rad(10), 0.0, 0.0])  # Initial state
    xr = np.array([0.0, 0.0, 0.0, 0.0])  # Reference state

    # Setup controller
    q = np.diag([5, 5, 0.1, 0.1])
    r = np.diag([0.1])
    ctrl = dlqr.Dlqr(environment, q, r, xr)

    proc_noise = 0 * np.array([0.01, np.deg2rad(0.1), 0.0, 0.0])

    return sim_param, environment, x0, xr, q, r, ctrl, proc_noise


def compute_cost(X, xr, U, q, r, runtime):
    max_computation_length = X.shape[0] - 1
    # -1 as the last X is already corrupted and last U was not computed anymore:
    n_computed = np.min((np.sum(~np.isnan(X[:, 0])),
                         np.sum(~np.isnan(X[:, 1])),
                         np.sum(~np.isnan(X[:, 2])),
                         np.sum(~np.isnan(X[:, 3])))) - 1
    assert n_computed >= 0

    cost = np.einsum("ik,kl,il", X[:n_computed, :] - xr, q, X[:n_computed, :] - xr) \
           + np.einsum("ik,kl,il", U[:n_computed], r, U[:n_computed, :])

    # if we got only nans, we want to have cost_per_second = 1e5
    cost += (runtime * 1e5) * (max_computation_length - n_computed) / max_computation_length

    cost_per_second = cost / runtime
    return cost_per_second


def furuta_simulation(init_tuple, params, D, pos):
    sim_param, environment, x0, xr, q, r, ctrl, proc_noise = init_tuple
    if D != 0 and params.ndim != 2:
        params = params.reshape(-1, D)
    elif D == 0:
        params = np.array([[]])

    logcosts = []
    for param in params:
        # use the parameters given in params instead of the LQR parameters
        assert param.size == D
        if D != 0:
            K = ctrl._K
            K[0, pos] = param

        # perform simulation
        n_simsteps = int(sim_param.runtime / sim_param.dt)
        u = np.zeros([n_simsteps, environment.param.nu]) * np.nan
        x = np.zeros([n_simsteps + 1, environment.param.nx]) * np.nan
        x[0, :] = x0
        for k in np.arange(n_simsteps):
            u[k, :] = ctrl.calc_input(x[k, :], xr)
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                next_state = environment.euler_next(x[k, :], u[k, :]) + proc_noise * np.random.randn(4, )
            x[k + 1, :] = next_state
            if np.isnan(next_state).any() or (np.abs(next_state) > 100.0).any():
                break

        logcosts.append(np.log10(compute_cost(x, xr, u, q, r, runtime=sim_param.runtime)))

    return np.array(logcosts)

File Path: metabo/environment/hpo/__init__.py
Content:

File Path: metabo/environment/hpo/prepare_data.py
Content:
# Copyright (c) 2019 Robert Bosch GmbH
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>.

# ******************************************************************
# prepare_data.py
# Process the raw datasets from the HPO experiments for usage in MetaBO training and evaluation.
# Processed data is stored in metabo/environment/hpo/processed/<model>/objectives.pkl
#
# Due to licensing issues, the datasets used in this experiment cannot be shipped with the MetaBO package.
# However, you can download the datasets yourself from https://github.com/nicoschilling/ECML2016
# Put the folders "svm" and "adaboost" from this repository into metabo/environment/hpo/data
#
# The gp-hyperparameters for each dataset were estimated offline using type-2-ML on a GP with 100 datasets and
# stored in metabo/environment/hpo/processed/<model>/gp_hyperparameters.pkl
# ******************************************************************

import os
import numpy as np
import pickle as pkl


def prepare_hpo_data(model, datapath):
    # read in data
    if model == "svm":
        param_nos_to_extract = [3, 4]
    elif model == "adaboost":
        param_nos_to_extract = [0, 1]
    else:
        raise ValueError("Unknown model!")
    data_dict = {}

    try:
        for file in os.listdir(datapath):
            if not os.path.isfile(os.path.join(datapath, file)):
                continue
            with open(os.path.join(datapath, file), "r") as f:
                cur_X = None
                cur_Y = None
                for line in f:
                    cur_x = np.zeros((1, len(param_nos_to_extract)))
                    cur_y = np.zeros((1, 1))
                    fields = line.split()
                    found_params = len(param_nos_to_extract) * [False]
                    for field in fields:
                        if ":" in field:
                            colon_pos = field.find(":")
                            param_no = int(field[:colon_pos])
                            param_val = float(field[colon_pos + 1:])
                            if param_no in param_nos_to_extract:
                                param_index_in_x = param_nos_to_extract.index(param_no)
                                found_params[param_index_in_x] = True
                                cur_x[0, param_index_in_x] = param_val
                        else:
                            objective_val = float(field)
                            cur_y[0, 0] = objective_val
                    if all(entry is True for entry in found_params):
                        cur_X = np.concatenate((cur_X, cur_x), axis=0) if cur_X is not None else cur_x
                        cur_Y = np.concatenate((cur_Y, cur_y), axis=0) if cur_Y is not None else cur_y
            data_dict[file] = {"X": cur_X,
                               "Y": cur_Y}
    except FileNotFoundError:
        raise FileNotFoundError("Could not find HPO datasets. Please download the datasets from "
                                "https://github.com/nicoschilling/ECML2016 and put the content of data into metabo/environment/hpo/data")

    if not len(data_dict) == 50:
        raise ValueError(
            "Number of datasets incorrect. Please download the datasets from "
            "https://github.com/nicoschilling/ECML2016 and put the content of data into metabo/environment/hpo/data")

    # normalize domain to unitsquare
    # find domain bounds
    elt = next(iter(data_dict.values()))
    X = elt["X"]
    domain = np.zeros((len(param_nos_to_extract), 2))
    for i in range(len(param_nos_to_extract)):
        domain[i, 0] = np.min(X[:, i])
        domain[i, 1] = np.max(X[:, i])
    # normalize
    for key, val in data_dict.items():
        val["X"] = (val["X"] - domain[:, 0]) / np.ptp(domain, axis=1)

    # store data
    this_path = os.path.dirname(os.path.realpath(__file__))
    savepath = os.path.join(this_path, "processed", model)
    os.makedirs(savepath, exist_ok=True)
    with open(os.path.join(savepath, "objectives.pkl"), "wb") as f:
        pkl.dump(data_dict, f)

File Path: metabo/environment/metabo_gym.py
Content:
# Copyright (c) 2019 Robert Bosch GmbH
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>.

# ******************************************************************
# metabo_gym.py
# Implementation of the basic gym-environment for the MetaBO framework.
# ******************************************************************

import gym
import gym.spaces
import numpy as np
import sobol_seq
import GPy
import pickle as pkl
import json
from metabo.environment.util import create_uniform_grid, scale_from_unit_square_to_domain, \
    scale_from_domain_to_unit_square, get_cube_around
from metabo.environment.objectives import SparseSpectrumGP, bra_var, bra_max_min_var, gprice_var, gprice_max_min_var, \
    hm3_var, hm3_max_min_var, hpo, hpo_max_min, get_hpo_domain, rhino_translated, rhino_max_min_translated, \
    rhino2, rhino2_max_min
from metabo.environment.furuta import init_furuta_simulation, furuta_simulation
from matplotlib import pyplot as plt
import os

class MetaBO(gym.Env):
    def __init__(self, **kwargs):
        self.kwargs = kwargs

        # number of dimensions
        self.D = kwargs["D"]

        # the domain (unit hypercube)
        self.domain = np.zeros((self.D,))
        self.domain = np.stack([self.domain, np.ones(self.D, )], axis=1)

        # optimization horizon
        self.T = None  # will be set in self.reset
        if "T" in kwargs:
            self.T_min = self.T_max = kwargs["T"]
        else:
            self.T_min = kwargs["T_min"]
            self.T_max = kwargs["T_max"]
        assert self.T_min > 0
        assert self.T_min <= self.T_max

        # the initial design
        self.n_init_samples = kwargs["n_init_samples"]
        assert self.n_init_samples <= self.T_max
        self.initial_design = sobol_seq.i4_sobol_generate(self.D, self.n_init_samples)

        # the AF and its optimization
        self.af = None
        self.neg_af_and_d_neg_af_d_state = None
        self.do_local_af_opt = kwargs["local_af_opt"]
        if self.do_local_af_opt:
            self.discrete_domain = False

            # prepare xi_t
            self.xi_t = None  # is determined adaptively in each BO step
            self.af_opt_startpoints_t = None  # best k evaluations of af on multistart_grid
            self.af_maxima_t = None  # the resulting local af_maxima
            self.N_MS = kwargs["N_MS"]
            N_MS_per_dim = np.int(np.floor(self.N_MS ** (1 / self.D)))
            self.multistart_grid, _ = create_uniform_grid(self.domain, N_MS_per_dim)
            self.N_MS = self.multistart_grid.shape[0]
            self.k = kwargs["k"]  # number of multistarts
            self.cardinality_xi_local_t = self.k
            self.cardinality_xi_global_t = self.N_MS
            self.cardinality_xi_t = self.cardinality_xi_local_t + self.cardinality_xi_global_t

            # hierarchical gridding or gradient-based optimization?
            self.N_LS = kwargs["N_LS"]
            self.local_search_grid = sobol_seq.i4_sobol_generate(self.D, self.N_LS)
            self.af_max_search_diam = 2 * 1 / N_MS_per_dim
        else:
            self.discrete_domain = True
            # self.xi_t = None  # will be set for each new function
            self.cardinality_xi_t = kwargs["cardinality_domain"]
            self.xi_t = sobol_seq.i4_sobol_generate(self.D, self.cardinality_xi_t)

        # the features
        self.features = kwargs["features"]
        self.feature_order_eval_envs = ["posterior_mean", "posterior_std", "incumbent", "timestep"]
        self.feature_order_eps_greedy = ["posterior_mean", "posterior_std"] + \
                                        ["x"] * self.D + ["incumbent", "timestep", "budget"]
        self.feature_order_gmm_ucb = ["posterior_mean", "posterior_std"] + ["x"] * self.D + ["timestep", "budget"]

        # observation space
        self.n_features = 0
        if "posterior_mean" in self.features:
            self.n_features += 1
        if "posterior_std" in self.features:
            self.n_features += 1
        if "left_budget" in self.features:
            self.n_features += 1
        if "budget" in self.features:
            self.n_features += 1
        if "incumbent" in self.features:
            self.n_features += 1
        if "timestep_perc" in self.features:
            self.n_features += 1
        if "timestep" in self.features:
            self.n_features += 1
        if "x" in self.features:
            self.n_features += self.D
        self.observation_space = gym.spaces.Box(low=-100000.0, high=100000.0,
                                                shape=(self.cardinality_xi_t, self.n_features),
                                                dtype=np.float32)
        self.pass_X_to_pi = kwargs["pass_X_to_pi"]

        # action space: index of one of the grid points
        self.action_space = gym.spaces.Discrete(self.cardinality_xi_t)

        # optimization step
        self.t = None

        # the reward
        self.reward_transformation = kwargs["reward_transformation"]

        # the ground truth function
        self.f_type = kwargs["f_type"]
        self.f_opts = kwargs["f_opts"]
        self.f = None
        self.y_max = None
        self.y_min = None
        self.x_max = None

        # the training data
        self.X = self.Y = None  # None means empty
        self.gp_is_empty = True

        # the surrogate GP
        self.mf = None
        self.gp = None
        self.kernel_variance = kwargs["kernel_variance"]
        self.kernel_lengthscale = np.array(kwargs["kernel_lengthscale"])
        self.noise_variance = kwargs["noise_variance"]
        if "use_prior_mean_function" in kwargs and kwargs["use_prior_mean_function"]:
            self.use_prior_mean_function = True
        else:
            self.use_prior_mean_function = False

        # seeding
        self.rng = None
        self.seeded_with = None

        # plot count
        self.plot_count = 0

    def seed(self, seed=None):
        # sets up the environment-internal random number generator and seeds it with seed
        self.rng = np.random.RandomState()
        self.seeded_with = seed
        self.rng.seed(self.seeded_with)

    def set_af_functions(self, af_fun):
        # connect the policy with the environment for setting up the adaptive grid

        if not self.pass_X_to_pi:
            self.af = af_fun
        else:
            self.af = lambda state: af_fun(state, self.X, self.gp)

    def reset(self):
        # reset step counters
        self.reset_step_counters()

        # draw a new function from self.f_type
        self.draw_new_function()

        # reset the GP
        self.reset_gp()

        # optimize the AF
        self.optimize_AF()

        # plot
        if self.kwargs.get("plot"):
            self.plot()

        return self.get_state(self.xi_t)

    def step(self, action):
        assert self.t < self.T  # if self.t == self.T one should have called self.reset() before calling this method
        if self.Y is None:
            assert self.t == 0
        else:
            assert self.t == self.Y.size

        if self.t < self.n_init_samples:
            # ignore action, x_action if there are points from the initial design left
            x_action = self.initial_design[self.t, :].reshape(1, self.D)
        else:
            # sample function, add it to the GP and retrieve resulting state
            x_action = self.convert_idx_to_x(action)
        self.add_data(x_action)  # do this BEFORE calling get_reward()
        reward = self.get_reward()
        self.update_gp()  # do this AFTER calling get_reward()
        self.optimize_AF()
        if self.kwargs.get("plot"):
            self.plot()
        next_state = self.get_state(self.xi_t)
        done = self.is_terminal()

        return next_state, reward, done, {}

    def reset_step_counters(self):
        self.t = 0

        if self.T_min == self.T_max:
            self.T = self.T_min
        else:
            self.T = self.rng.randint(low=self.T_min, high=self.T_max)
        assert self.T > 0  # if T was set outside of init

    def close(self):
        pass

    def draw_new_function(self):
        if self.f_type == "GP":
            seed = self.rng.randint(100000)
            n_features = 500
            lengthscale = self.rng.uniform(low=self.f_opts["lengthscale_low"],
                                           high=self.f_opts["lengthscale_high"])
            noise_var = self.rng.uniform(low=self.f_opts["noise_var_low"],
                                         high=self.f_opts["noise_var_high"])
            signal_var = self.rng.uniform(low=self.f_opts["signal_var_low"],
                                          high=self.f_opts["signal_var_high"])
            kernel = self.f_opts["kernel"] if "kernel" in self.f_opts else "RBF"
            ssgp = SparseSpectrumGP(seed=seed, input_dim=self.D, noise_var=noise_var, length_scale=lengthscale,
                                    signal_var=signal_var, n_features=n_features, kernel=kernel)
            x_train = np.array([]).reshape(0, self.D)
            y_train = np.array([]).reshape(0, 1)
            ssgp.train(x_train, y_train, n_samples=1)
            self.f = lambda x: ssgp.sample_posterior_handle(x).reshape(-1, 1)

            # load gp-hyperparameters
            self.kernel_lengthscale = lengthscale
            self.kernel_variance = signal_var
            self.noise_variance = 1e-20

            if self.do_local_af_opt:
                # determine approximate maximum
                N_samples_dim = np.ceil(1000 ** (1 / self.D))
                assert N_samples_dim > 3
                x_vec, _ = create_uniform_grid(self.domain, N_samples_dim=N_samples_dim)
            else:
                # determine true maximum on grid
                x_vec = self.xi_t
            y_vec = self.f(x_vec)
            self.x_max = x_vec[np.argmax(y_vec)].reshape(1, self.D)
            self.y_max = np.max(y_vec)
            self.y_min = np.min(y_vec)

            self.f = lambda x: ssgp.sample_posterior_handle(x).reshape(-1, 1) - self.f_opts["min_regret"]
        elif self.f_type == "BRA-var":
            # the branin function with translations and scalings
            if "M" in self.f_opts and self.f_opts["M"] is not None:
                # use M fixed source datasets evenly spread over the training set
                M = self.f_opts["M"]
                bound_scaling = self.f_opts["bound_scaling"]
                bound_translation = self.f_opts["bound_translation"]
                fct_params_domain = np.array([[-bound_translation, bound_translation],
                                              [-bound_translation, bound_translation],
                                              [1 - bound_scaling, 1 + bound_scaling]])
                fct_params_grid = sobol_seq.i4_sobol_generate(dim_num=3, n=M)  # 2 translations, 1 scaling
                fct_params_grid = scale_from_unit_square_to_domain(X=fct_params_grid, domain=fct_params_domain)

                param_idx = self.rng.choice(M)

                t = fct_params_grid[param_idx, 0:2]
                s = fct_params_grid[param_idx, 2]
            else:
                if "bound_translation" in self.f_opts:
                    # sample translation
                    t = self.rng.uniform(low=-self.f_opts["bound_translation"],
                                         high=self.f_opts["bound_translation"], size=(1, 2))

                    # sample scaling
                    s = self.rng.uniform(low=1 - self.f_opts["bound_scaling"], high=1 + self.f_opts["bound_scaling"])
                elif "translation_min" in self.f_opts:
                    # sample translation
                    t = self.rng.uniform(low=self.f_opts["translation_min"],
                                         high=self.f_opts["translation_max"], size=(1, 2))
                    # sample signs of translation
                    for i in range(t.shape[1]):
                        coin = self.rng.uniform(low=0.0, high=1.0)
                        if coin > 0.5:
                            t[0, i] = -t[0, i]

                    # sample scaling
                    s = self.rng.uniform(low=self.f_opts["scaling_min"], high=self.f_opts["scaling_max"])
                else:
                    raise ValueError("Missspecified translation/scaling parameters!")

            self.f = lambda x: bra_var(x, t=t, s=s)

            max_pos, max, _, min = bra_max_min_var(t=t, s=s)
            self.x_max = max_pos
            self.y_max = max
            self.y_min = min
        elif self.f_type == "GPRICE-var":
            # the goldstein-price function with translations and scalings
            if "M" in self.f_opts and self.f_opts["M"] is not None:
                # use M fixed source datasets evenly spread over the training set
                M = self.f_opts["M"]
                bound_scaling = self.f_opts["bound_scaling"]
                bound_translation = self.f_opts["bound_translation"]
                fct_params_domain = np.array([[-bound_translation, bound_translation],
                                              [-bound_translation, bound_translation],
                                              [1 - bound_scaling, 1 + bound_scaling]])
                fct_params_grid = sobol_seq.i4_sobol_generate(dim_num=3, n=M)  # 2 translations, 1 scaling
                fct_params_grid = scale_from_unit_square_to_domain(X=fct_params_grid, domain=fct_params_domain)

                param_idx = self.rng.choice(M)

                t = fct_params_grid[param_idx, 0:2]
                s = fct_params_grid[param_idx, 2]
            else:
                if "bound_translation" in self.f_opts:
                    # sample translation
                    t = self.rng.uniform(low=-self.f_opts["bound_translation"],
                                         high=self.f_opts["bound_translation"], size=(1, 2))

                    # sample scaling
                    s = self.rng.uniform(low=1 - self.f_opts["bound_scaling"], high=1 + self.f_opts["bound_scaling"])
                elif "translation_min" in self.f_opts:
                    # sample translation
                    t = self.rng.uniform(low=self.f_opts["translation_min"],
                                         high=self.f_opts["translation_max"], size=(1, 2))
                    # sample signs of translation
                    for i in range(t.shape[1]):
                        coin = self.rng.uniform(low=0.0, high=1.0)
                        if coin > 0.5:
                            t[0, i] = -t[0, i]

                    # sample scaling
                    s = self.rng.uniform(low=self.f_opts["scaling_min"], high=self.f_opts["scaling_max"])
                else:
                    raise ValueError("Missspecified translation/scaling parameters!")

            self.f = lambda x: gprice_var(x, t=t, s=s)

            max_pos, max, _, min = gprice_max_min_var(t=t, s=s)
            self.x_max = max_pos
            self.y_max = max
            self.y_min = min
        elif self.f_type == "HM3-var":
            # the hartmann-3D function with translations and scalings
            if "M" in self.f_opts and self.f_opts["M"] is not None:
                # use M fixed source datasets evenly spread over the training set
                M = self.f_opts["M"]
                bound_scaling = self.f_opts["bound_scaling"]
                bound_translation = self.f_opts["bound_translation"]
                fct_params_domain = np.array([[-bound_translation, bound_translation],
                                              [-bound_translation, bound_translation],
                                              [-bound_translation, bound_translation],
                                              [1 - bound_scaling, 1 + bound_scaling]])
                fct_params_grid = sobol_seq.i4_sobol_generate(dim_num=4, n=M)  # 3 translations, 1 scaling
                fct_params_grid = scale_from_unit_square_to_domain(X=fct_params_grid, domain=fct_params_domain)

                param_idx = self.rng.choice(M)

                t = fct_params_grid[param_idx, 0:3]
                s = fct_params_grid[param_idx, 3]
            else:
                if "bound_translation" in self.f_opts:
                    # sample translation
                    t = self.rng.uniform(low=-self.f_opts["bound_translation"],
                                         high=self.f_opts["bound_translation"], size=(1, 3))

                    # sample scaling
                    s = self.rng.uniform(low=1 - self.f_opts["bound_scaling"], high=1 + self.f_opts["bound_scaling"])
                elif "translation_min" in self.f_opts:
                    # sample translation
                    t = self.rng.uniform(low=self.f_opts["translation_min"],
                                         high=self.f_opts["translation_max"], size=(1, 3))
                    # sample signs of translation
                    for i in range(t.shape[1]):
                        coin = self.rng.uniform(low=0.0, high=1.0)
                        if coin > 0.5:
                            t[0, i] = -t[0, i]

                    # sample scaling
                    s = self.rng.uniform(low=self.f_opts["scaling_min"], high=self.f_opts["scaling_max"])
                else:
                    raise ValueError("Missspecified translation/scaling parameters!")

            self.f = lambda x: hm3_var(x, t=t, s=s)

            max_pos, max, _, min = hm3_max_min_var(t=t, s=s)
            self.x_max = max_pos
            self.y_max = max
            self.y_min = min
        elif self.f_type == "HPO":
            # load data
            if not hasattr(self, "hpo_data"):
                with open(self.f_opts["hpo_data_file"], "rb") as f:
                    self.hpo_data = pkl.load(f)
                with open(self.f_opts["hpo_gp_hyperparameters_file"], "rb") as f:
                    self.hpo_gp_hyperparameters = pkl.load(f)
                with open(self.f_opts["hpo_datasets_file"], "r") as f:
                    self.hpo_datasets = json.load(f)

            # sample dataset
            if self.f_opts["draw_random_datasets"]:
                dataset = self.rng.choice(self.hpo_datasets, size=1)[0]
            else:
                if not hasattr(self, "dataset_counter"):
                    self.dataset_counter = 0
                if self.dataset_counter >= len(self.hpo_datasets):
                    self.dataset_counter = 0
                dataset = self.hpo_datasets[self.dataset_counter]
                self.dataset_counter += 1

            # load gp-hyperparameters
            self.kernel_lengthscale = self.hpo_gp_hyperparameters[dataset]["lengthscale"]
            self.kernel_variance = self.hpo_gp_hyperparameters[dataset]["variance"]
            self.noise_variance = self.hpo_gp_hyperparameters[dataset]["noise_variance"]

            # set xi_t
            self.xi_t = get_hpo_domain(data=self.hpo_data, dataset=dataset)
            assert self.xi_t.shape[0] == self.cardinality_xi_t

            self.f = lambda x: hpo(x, data=self.hpo_data, dataset=dataset) - self.f_opts["min_regret"]

            max_pos, max, _, min = hpo_max_min(data=self.hpo_data, dataset=dataset)
            self.x_max = max_pos
            self.y_max = max
            self.y_min = min
        elif self.f_type == "Furuta":
            if "M" in self.f_opts and self.f_opts["M"] is not None:
                # use M fixed sets of physical parameters evenly spread over the training set
                M = self.f_opts["M"]
                furuta_domain = np.array(self.f_opts["furuta_domain"])
                length_arm_low = self.f_opts["length_arm_low"]
                length_arm_high = self.f_opts["length_arm_high"]
                length_pendulum_low = self.f_opts["length_pendulum_low"]
                length_pendulum_high = self.f_opts["length_pendulum_high"]
                mass_arm_low = self.f_opts["mass_arm_low"]
                mass_arm_high = self.f_opts["mass_arm_high"]
                mass_pendulum_low = self.f_opts["mass_pendulum_low"]
                mass_pendulum_high = self.f_opts["mass_pendulum_high"]

                physical_params_domain = np.array([[mass_pendulum_low, mass_pendulum_high],
                                                   [mass_arm_low, mass_arm_high],
                                                   [length_pendulum_low, length_pendulum_high],
                                                   [length_arm_low, length_arm_high]])
                physical_params_grid = sobol_seq.i4_sobol_generate(dim_num=4, n=M)
                physical_params_grid = scale_from_unit_square_to_domain(X=physical_params_grid,
                                                                        domain=physical_params_domain)

                param_idx = self.rng.choice(M)

                mass_pendulum = physical_params_grid[param_idx, 0]
                mass_arm = physical_params_grid[param_idx, 1]
                length_pendulum = physical_params_grid[param_idx, 2]
                length_arm = physical_params_grid[param_idx, 3]
            else:
                # sample physical parameters randomly
                furuta_domain = np.array(self.f_opts["furuta_domain"])
                length_arm = self.rng.uniform(low=self.f_opts["length_arm_low"],
                                              high=self.f_opts["length_arm_high"])
                length_pendulum = self.rng.uniform(low=self.f_opts["length_pendulum_low"],
                                                   high=self.f_opts["length_pendulum_high"])
                mass_arm = self.rng.uniform(low=self.f_opts["mass_arm_low"],
                                            high=self.f_opts["mass_arm_high"])
                mass_pendulum = self.rng.uniform(low=self.f_opts["mass_pendulum_low"],
                                                 high=self.f_opts["mass_pendulum_high"])

            init_tuple = init_furuta_simulation(mass_arm=mass_arm,
                                                length_arm=length_arm,
                                                mass_pendulum=mass_pendulum,
                                                length_pendulum=length_pendulum)

            K = np.copy(init_tuple[6]._K)
            self.f = lambda x: -furuta_simulation(
                init_tuple=init_tuple,
                params=scale_from_unit_square_to_domain(x, furuta_domain),
                D=self.D,
                pos=self.f_opts["pos"]).reshape(-1, 1)

            # we don't know the optimal values
            # the lqr_params and lqr_logcost are not really the optimum (nonlinear system) but they are good guesses
            lqr_params = K.squeeze()[self.f_opts["pos"]]
            self.x_max = scale_from_domain_to_unit_square(lqr_params, furuta_domain).reshape(1, self.D)
            self.y_max = self.f(self.x_max)
            self.y_min = -5.0  # = -log(chrashcost)
        elif self.f_type == "RHINO":
            # use a discrete domain
            self.xi_t = np.linspace(0.0, 1.0, self.f_opts["cardinality_domain"]).reshape(
                self.f_opts["cardinality_domain"], self.D)
            assert self.xi_t.shape[0] == self.cardinality_xi_t

            # sample translation
            t = self.rng.uniform(low=-self.f_opts["bound_translation"],
                                 high=self.f_opts["bound_translation"], size=(1, 1))

            self.f = lambda x: rhino_translated(x=x, t=t)

            max_pos, max, _, min = rhino_max_min_translated(t=t)
            self.x_max = max_pos
            self.y_max = max
            self.y_min = min

            # y = self.f(self.xi_t)
            # self.x_max = self.xi_t[np.argmax(y)]
            # self.y_max = np.max(y)
            # self.y_min = np.min(y)
        elif self.f_type == "RHINO2":
            # use a discrete domain
            self.xi_t = np.linspace(0.0, 1.0, self.f_opts["cardinality_domain"]).reshape(
                self.f_opts["cardinality_domain"], self.D)
            assert self.xi_t.shape[0] == self.cardinality_xi_t

            # sample translation
            h = self.rng.uniform(low=self.f_opts["h_min"],
                                 high=self.f_opts["h_max"], size=(1, 1))

            self.f = lambda x: rhino2(x=x, h=h)

            max_pos, max, _, min = rhino2_max_min(h=h)
            self.x_max = max_pos
            self.y_max = max
            self.y_min = min

            # y = self.f(self.xi_t)
            # self.x_max = self.xi_t[np.argmax(y)]
            # self.y_max = np.max(y)
            # self.y_min = np.min(y)
        else:
            raise ValueError("Unknown f_type!")

        assert self.y_max is not None  # we need this for the reward
        assert self.y_min is not None  # we need this for the incumbent of empty training set

    def reset_gp(self):
        # reset training data
        self.X = self.Y = None

        # reset gp
        if "kernel" in self.kwargs:
            if self.kwargs["kernel"] == "RBF":
                kernel_fun = GPy.kern.RBF
            elif self.kwargs["kernel"] == "Matern32":
                kernel_fun = GPy.kern.Matern32
            elif self.kwargs["kernel"] == "Matern52":
                kernel_fun = GPy.kern.Matern52
            else:
                raise ValueError("Unknown kernel function for GP model!")
        else:
            kernel_fun = GPy.kern.RBF

        self.kernel = kernel_fun(input_dim=self.D,
                                 variance=self.kernel_variance,
                                 lengthscale=self.kernel_lengthscale,
                                 ARD=True)

        if self.use_prior_mean_function:
            self.mf = GPy.core.Mapping(self.D, 1)
            self.mf.f = lambda X: np.mean(self.Y, axis=0)[0] if self.Y is not None else 0.0
            self.mf.update_gradients = lambda a, b: 0
            self.mf.gradients_X = lambda a, b: 0
        else:
            self.mf = None

        normalizer = False

        # this is only dummy data as GPy is not able to have empty training set
        # for prediction, the GP is not used for empty training set
        X = np.zeros((1, self.D))
        Y = np.zeros((1, 1))
        self.gp_is_empty = True
        self.gp = GPy.models.gp_regression.GPRegression(X, Y,
                                                        noise_var=self.noise_variance,
                                                        kernel=self.kernel,
                                                        mean_function=self.mf,
                                                        normalizer=normalizer)
        self.gp.Gaussian_noise.variance = self.noise_variance
        self.gp.kern.lengthscale = self.kernel_lengthscale
        self.gp.kern.variance = self.kernel_variance

    def add_data(self, X):
        assert X.ndim == 2

        # evaluate f at X and add the result to the GP
        Y = self.f(X)

        if self.X is None:
            self.X = X
            self.Y = Y
        else:
            self.X = np.concatenate((self.X, X), axis=0)
            self.Y = np.concatenate((self.Y, Y), axis=0)

        self.t += X.shape[0]

    def update_gp(self):
        assert self.Y is not None
        self.gp.set_XY(self.X, self.Y)
        self.gp_is_empty = False

    def optimize_AF(self):
        if self.do_local_af_opt:
            # obtain maxima of af
            self.get_af_maxima()
            self.xi_t = np.concatenate((self.af_maxima_t, self.multistart_grid), axis=0)
            assert self.xi_t.shape[0] == self.cardinality_xi_t
        else:
            pass  # nothing to be done, we just use the grid self.xi_t which is static in this case

    def get_state(self, X):
        # fill the state
        feature_count = 0
        idx = 0
        state = np.zeros((X.shape[0], self.n_features), dtype=np.float32)
        gp_mean, gp_std = self.eval_gp(X)
        if "posterior_mean" in self.features:
            feature_count += 1
            state[:, idx:idx + 1] = gp_mean.reshape(X.shape[0], 1)
            idx += 1
        if "posterior_std" in self.features:
            feature_count += 1
            state[:, idx:idx + 1] = gp_std.reshape(X.shape[0], 1)
            idx += 1
        if "x" in self.features:
            feature_count += 1
            state[:, idx:idx + self.D] = X
            idx += self.D
        if "incumbent" in self.features:
            feature_count += 1
            incumbent_vec = np.ones((X.shape[0],)) * self.get_incumbent()
            state[:, idx] = incumbent_vec
            idx += 1
        if "timestep_perc" in self.features:
            feature_count += 1
            t_perc = self.t / self.T
            t_perc_vec = np.ones((X.shape[0],)) * t_perc
            state[:, idx] = t_perc_vec
            idx += 1
        if "timestep" in self.features:
            feature_count += 1
            # clip timestep
            if "T_training" in self.kwargs and self.kwargs["T_training"] is not None:
                t = np.min([self.t, self.kwargs["T_training"]])
            else:
                t = self.t
            t_vec = np.ones((X.shape[0],)) * t
            state[:, idx] = t_vec
            idx += 1
        if "budget" in self.features:
            feature_count += 1
            if "T_training" in self.kwargs and self.kwargs["T_training"] is not None:
                T = self.kwargs["T_training"]
            else:
                T = self.T
            budget_vec = np.ones((X.shape[0],)) * T
            state[:, idx] = budget_vec
            idx += 1

        assert idx == self.n_features  # make sure the full state has been filled
        if not feature_count == len(self.features):
            raise ValueError("Invalid feature specification!")

        return state

    def get_reward(self):
        # make sure you already increased the step counter self.t before calling this method!
        # make sure you updated the training set but did NOT update the gp before calling this method!
        assert self.Y is not None  # this method should not be called with empty training set
        negativity_check = False

        # compute the simple regret
        y_diffs = self.y_max - self.Y
        simple_regret = np.min(y_diffs)
        reward = np.asscalar(simple_regret)

        # apply reward transformation
        if self.reward_transformation == "none":
            reward = reward
        elif self.reward_transformation == "neg_linear":
            reward = -reward
        elif self.reward_transformation == "neg_log10":
            if reward < 1e-20:
                print("Warning: logarithmic reward may be invalid!")
            reward, negativity_check = np.max((1e-20, reward)), True
            assert negativity_check
            reward = -np.log10(reward)
        else:
            raise ValueError("Unknown reward transformation!")

        return reward

    def get_af_maxima(self):
        state_at_multistarts = self.get_state(self.multistart_grid)
        af_at_multistarts = self.af(state_at_multistarts)
        self.af_opt_startpoints_t = self.multistart_grid[np.argsort(-af_at_multistarts)[:self.k, ...]]

        local_grids = [scale_from_unit_square_to_domain(self.local_search_grid,
                                                        domain=get_cube_around(x,
                                                                               diam=self.af_max_search_diam,
                                                                               domain=self.domain))
                       for x in self.af_opt_startpoints_t]
        local_grids = np.concatenate(local_grids, axis=0)
        state_on_local_grid = self.get_state(local_grids)
        af_on_local_grid = self.af(state_on_local_grid)
        self.af_maxima_t = local_grids[np.argsort(-af_on_local_grid)[:self.cardinality_xi_local_t]]

        assert self.af_maxima_t.shape[0] == self.cardinality_xi_local_t

    def get_incumbent(self):
        if self.Y is None:
            Y = np.array([self.y_min])
        else:
            Y = self.Y

        incumbent = np.max(Y)
        return incumbent

    def eval_gp(self, X_star):
        # evaluate the GP on X_star
        assert X_star.shape[1] == self.D

        if self.gp_is_empty:
            gp_mean = np.zeros((X_star.shape[0],))
            gp_var = self.kernel_variance * np.ones((X_star.shape[0],))
        else:
            gp_mean, gp_var = self.gp.predict_noiseless(X_star)
            gp_mean = gp_mean[:, 0]
            gp_var = gp_var[:, 0]
        gp_std = np.sqrt(gp_var)

        return gp_mean, gp_std

    def neg_af(self, x):
        x = x.reshape(1, self.D)  # the optimizer queries one point at a time
        state = self.get_state(x)
        neg_af = -self.af(state)

        return neg_af

    def get_random_sampling_reward(self):
        self.reset_step_counters()
        self.draw_new_function()

        self.X, self.Y = None, None
        rewards = []
        for t in range(self.T):
            if not self.discrete_domain:
                random_sample = self.rng.rand(1, self.D)
            else:
                random_sample = self.xi_t[self.rng.choice(np.arange(self.cardinality_xi_t)), :].reshape(1, -1)
            self.X = np.concatenate((self.X, random_sample), axis=0) if self.X is not None else random_sample
            f_x = self.f(random_sample)
            self.Y = np.concatenate((self.Y, f_x), axis=0) if self.Y is not None else f_x
            rewards.append(self.get_reward())
            self.t += 1

        assert self.is_terminal()

        return rewards

    def plot(self):
        assert self.D == 1 or self.D == 2

        if self.D == 1:
            width = 2.1
            height = width / 1.618
            fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(width, height))
            fig.subplots_adjust(hspace=0.1)
            fig.subplots_adjust(left=.04, bottom=.14, right=.96, top=.97)

            # grid for plotting
            X = np.linspace(0.0, 1.0, 250).reshape(-1, 1)

            # Plot GP, ground truth function, and training set
            ax = axes[0]
            # ax.set_title("GP and ground truth function")

            # plot ground truth function and maximum position
            ax.plot(X, self.f(X), color="k", ls="-", label="objective")
            ax.axvline(self.x_max, color="r", ls="--", alpha=.5)

            # plot the GP
            gp_mean, gp_std = self.eval_gp(X)
            ax.plot(X, gp_mean, "C0", ls="--", label="GP")
            ax.fill_between(X.squeeze(), gp_mean + gp_std, gp_mean - gp_std, color="C0", alpha=0.2)
            # ax.plot(X, gp_mean + gp_std, "b--")
            # ax.plot(X, gp_mean - gp_std, "b--")
            ax.xaxis.set_ticklabels([])
            ax.get_yaxis().set_visible(False)
            ax.set_ylim([-1.5, 1.5])
            # ax.legend()

            # plot the training set
            if self.X is not None and self.X.size > 0:
                ax.scatter(self.X[:-1], self.Y[:-1], color="g", marker="x", s=20)
                ax.scatter(self.X[-1], self.Y[-1], color="r", marker="x", s=20)
            # ax.grid()

            # Plot AF
            ax = axes[1]
            # ax.set_title("Neural Acquisition Function (MetaBO)")

            # plot the af
            state = self.get_state(X)
            af = self.af(state)
            ax.plot(X, af.reshape(X.shape), color="C0")
            if self.t >= self.n_init_samples:
                if self.kwargs.get("local_af_opt"):
                    for af_max in self.af_maxima_t:
                        ax.axvline(x=af_max)
                else:
                    ax.axvline(x=X[np.argmax(af)], color="r", ls="--", alpha=0.5)
            ax.yaxis.set_ticklabels([])
            ax.get_yaxis().set_visible(False)
            # ax.grid()

        elif self.D == 2:
            raise NotImplementedError

        fig.savefig(fname=os.path.join(self.kwargs["plotpath"], "plot_{:d}.pdf".format(self.plot_count)))
        plt.close(fig)
        self.plot_count += 1

    def convert_idx_to_x(self, idx):
        if not isinstance(idx, np.ndarray):
            idx = np.array([idx])

        return self.xi_t[idx, :].reshape(idx.size, self.D)

    def is_terminal(self):
        return self.t == self.T

File Path: metabo/environment/objectives.py
Content:
# Copyright (c) 2019 Robert Bosch GmbH
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>.

# ******************************************************************
# objectives.py
# Implementation of objectives for the experiments on global optimization benchmark functions, GP samples and HPO.
# ******************************************************************

import numpy as np


## Global optimization benchmark functions
# Branin
def bra(x):
    # the Branin function (2D)
    # https://www.sfu.ca/~ssurjano/branin.html
    x1 = x[:, 0]
    x2 = x[:, 1]

    # scale x
    x1 = x1 * 15.
    x1 = x1 - 5.
    x2 = x2 * 15.

    # parameters
    a = 1
    b = 5.1 / (4 * np.pi ** 2)
    c = 5 / np.pi
    r = 6
    s = 10
    t = 1 / (8 * np.pi)

    bra = a * (x2 - b * x1 ** 2 + c * x1 - r) ** 2 + s * (1 - t) * np.cos(x1) + s

    # normalize
    mean = 54.44
    std = 51.44
    bra = 1 / std * (bra - mean)

    # maximize
    bra = -bra

    return bra.reshape(x.shape[0], 1)


def bra_max_min():
    max_pos = np.array([[-np.pi, 12.275]])
    max_pos[0, 0] += 5.
    max_pos[0, 0] /= 15.
    max_pos[0, 1] /= 15.
    max = bra(max_pos)

    min_pos = np.array([[0.0, 0.0]])
    min = bra(min_pos)

    return max_pos, max, min_pos, min


def bra_var(x, t, s):
    x_new = x.copy()
    # apply translation
    # bound the translations s.t. upper left max lies in domain
    t_range = np.array([[-0.12, 0.87],
                        [-0.81, 0.18]])
    t = np.clip(t, t_range[:, 0], t_range[:, 1])
    x_new = x_new - t

    return s * bra(x_new)


def bra_max_min_var(t, s):
    max_pos, max, min_pos, min = bra_max_min()
    # apply translation
    # clip the translations s.t. upper left max lies in domain
    t_range = np.array([[-0.12, 0.87],
                        [-0.81, 0.18]])
    t = np.clip(t, t_range[:, 0], t_range[:, 1])
    max_pos = max_pos + t
    min_pos = min_pos + t

    return max_pos, s * max, min_pos, s * min


# Goldstein-Price
def gprice(x):
    # the goldstein price function (2D)
    # https://www.sfu.ca/~ssurjano/goldpr.html
    x1 = x[:, 0]
    x2 = x[:, 1]

    # scale x
    x1 = x1 * 4.
    x1 = x1 - 2.
    x2 = x2 * 4.
    x2 = x2 - 2.

    gprice = (1 + (x1 + x2 + 1) ** 2 * (19 - 14 * x1 + 3 * x1 ** 2 - 14 * x2 + 6 * x1 * x2 + 3 * x2 ** 2)) * \
             (30 + (2 * x1 - 3 * x2) ** 2 * (18 - 32 * x1 + 12 * x1 ** 2 + 48 * x2 - 36 * x1 * x2 + 27 * x2 ** 2))

    # lognormalize
    mean = 8.693
    std = 2.427
    gprice = 1 / std * (np.log(gprice) - mean)

    # maximize
    gprice = -gprice

    return gprice.reshape(x.shape[0], 1)


def gprice_max_min():
    max_pos = np.array([[0.0, -1.0]])
    max_pos[0, 0] += 2.
    max_pos[0, 0] /= 4.
    max_pos[0, 1] += 2.
    max_pos[0, 1] /= 4.
    max = gprice(max_pos)

    min_pos = np.array([[0.066, 1.0]])
    min = gprice(min_pos)

    return max_pos, max, min_pos, min


def gprice_var(x, t, s):
    x_new = x.copy()
    # apply translation
    # clip the translations s.t. upper left max lies in domain
    t_range = np.array([[-0.5, 0.5],
                        [-0.25, 0.75]])
    t = np.clip(t, t_range[:, 0], t_range[:, 1])
    x_new = x_new - t

    return s * gprice(x_new)


def gprice_max_min_var(t, s):
    # do the transformation in opposite order as in hm3_var!

    max_pos, max, min_pos, min = gprice_max_min()

    # apply translation
    t_range = np.array([[-0.5, 0.5],
                        [-0.25, 0.75]])
    t = np.clip(t, t_range[:, 0], t_range[:, 1])
    max_pos = max_pos + t
    min_pos = min_pos + t

    return max_pos, s * max, min_pos, s * min


# Hartmann-3
def hm3(x):
    # the hartmann3 function (3D)
    # https://www.sfu.ca/~ssurjano/hart3.html

    # parameters
    alpha = np.array([1.0, 1.2, 3.0, 3.2])
    A = np.array([[3.0, 10, 30],
                  [0.1, 10, 35],
                  [3.0, 10, 30],
                  [0.1, 10, 35]])
    P = 1e-4 * np.array([[3689, 1170, 2673],
                         [4699, 4387, 7470],
                         [1091, 8732, 5547],
                         [381, 5743, 8828]])

    x = x.reshape(x.shape[0], 1, -1)
    B = x - P
    B = B ** 2
    exponent = A * B
    exponent = np.einsum("ijk->ij", exponent)
    C = np.exp(-exponent)
    hm3 = -np.einsum("i, ki", alpha, C)

    # normalize
    mean = -0.93
    std = 0.95
    hm3 = 1 / std * (hm3 - mean)

    # maximize
    hm3 = -hm3

    return hm3.reshape(x.shape[0], 1)


def hm3_max_min():
    max_pos = np.array([[0.114614, 0.555649, 0.852547]])
    max = hm3(max_pos)

    min_pos = np.array([[1.0, 1.0, 0.0]])
    min = hm3(min_pos)

    return max_pos, max, min_pos, min


def hm3_var(x, t, s):
    x_new = x.copy()
    # apply translation
    # clip the translations s.t. upper left max lies in domain
    t_range = np.array([[-0.11, 0.88],
                        [-0.55, 0.44],
                        [-0.85, 0.14]])
    t = np.clip(t, t_range[:, 0], t_range[:, 1])
    x_new = x_new - t

    return s * hm3(x_new)


def hm3_max_min_var(t, s):
    # do the transformation in opposite order as in hm3_var!

    max_pos, max, min_pos, min = hm3_max_min()

    # apply translation
    t_range = np.array([[-0.11, 0.88],
                        [-0.55, 0.44],
                        [-0.85, 0.14]])
    t = np.clip(t, t_range[:, 0], t_range[:, 1])
    max_pos = max_pos + t
    min_pos = min_pos + t

    return max_pos, s * max, min_pos, s * min


# General function class (GP samples)
class SparseSpectrumGP:
    """
    Implements the sparse spectrum approximation of a GP following the predictive
    entropy search paper.

    Note: This approximation assumes that we use a GP with squared exponential kernel.
    """

    def __init__(self, input_dim, seed, noise_var=1.0, length_scale=1.0, signal_var=1.0, n_features=100, kernel="RBF"):
        self.seed = seed
        self.rng = np.random.RandomState()
        self.rng.seed(self.seed)

        self.input_dim = input_dim
        self.noise_var = noise_var
        self.length_scale = length_scale
        self.signal_var = signal_var
        self.n_features = n_features
        self.kernel = kernel
        assert kernel == "RBF" or kernel == "Matern32" or kernel == "Matern52"
        self.phi = self._compute_phi()
        self.jitter = 1e-10

        self.X = None
        self.Y = None

        # Statistics of the weights that give us random function samples
        # f(x) ~ phi(x).T @ theta, theta ~ N(theta_mu, theta_var)
        self.theta_mu = None
        self.theta_var = None

    def train(self, X, Y, n_samples):
        """
        Pre-compute all necessary variables for efficient prediction and sampling.
        """
        self.X = X
        self.Y = Y

        phi_train = self.phi(X)
        a = phi_train.T @ phi_train + self.noise_var * np.eye(self.n_features)
        a_inv = np.linalg.inv(a)
        self.theta_mu = a_inv @ phi_train.T @ Y
        self.theta_var = self.noise_var * a_inv

        # Generate handle to n_samples function samples that can be evaluated at x.
        var = self.theta_var + self.jitter * np.eye(self.theta_var.shape[0])
        var = (var + var.T) / 2
        chol = np.linalg.cholesky(var)
        self.theta_samples = self.theta_mu + chol @ self.rng.randn(self.n_features, n_samples)

    def predict(self, Xs, full_variance=False):
        raise NotImplementedError

    def sample_posterior(self, Xs):
        """
        Generate n_samples function samples from GP posterior at points Xs.
        """
        h = self.sample_posterior_handle
        return h(Xs)

    def sample_posterior_handle(self, x):
        x = np.atleast_2d(x).T if x.ndim == 1 else x
        return self.theta_samples.T @ self.phi(x).T

    def _compute_phi(self):
        """
        Compute random features.
        """
        if self.kernel == "RBF":
            w = self.rng.randn(self.n_features, self.input_dim) / self.length_scale
        elif self.kernel == "Matern32":
            w = self.rng.standard_t(3, (self.n_features, self.input_dim)) / self.length_scale
        elif self.kernel == "Matern52":
            w = self.rng.standard_t(5, (self.n_features, self.input_dim)) / self.length_scale
        b = self.rng.uniform(0, 2 * np.pi, size=self.n_features)
        return lambda x: np.sqrt(2 * self.signal_var / self.n_features) * np.cos(x @ w.T + b)


# Hyperparameter optimization experiments
def hpo(x, data, dataset):
    X = data[dataset]["X"]
    Y = data[dataset]["Y"]
    idx = np.where(np.all(x == X, axis=1))
    ret = Y[idx, :][0]
    assert np.size(ret) == x.shape[0]
    return ret


def hpo_max_min(data, dataset):
    X = data[dataset]["X"]
    Y = hpo(X, data=data, dataset=dataset)
    max_idx = np.argmax(Y)
    max_pos = X[max_idx, :].reshape(1, 2)
    max = Y[max_idx]
    min_idx = np.argmin(Y)
    min_pos = X[min_idx, :].reshape(1, 2)
    min = Y[min_idx]
    return max_pos, max, min_pos, min


def get_hpo_domain(data, dataset):
    return data[dataset]["X"]


# Rhino-function
def rhino(x):
    def bump(x, mu, sigma):
        return np.exp(-1 / 2 * (x - mu) ** 2 / sigma ** 2)

    bump1 = bump(x, mu=0.3, sigma=0.1)
    bump2 = bump(x, mu=0.7, sigma=0.01)
    rhino = 0.5 * bump1 + 3.0 * bump2

    return rhino


def rhino_max_min():
    max_pos = np.array(0.7).reshape(1, 1)
    min_pos = np.array(1.0).reshape(1, 1)
    max = rhino(max_pos)
    min = rhino(min_pos)

    return max_pos, max, min_pos, min


def rhino_translated(x, t):
    x_new = x.copy()
    # apply translation
    # clip the translations s.t. both maxima stay well in domain
    t_range = np.array([[-0.25, 0.25]])
    t = np.clip(t, t_range[:, 0], t_range[:, 1])
    x_new = x_new - t

    return rhino(x_new)


def rhino_max_min_translated(t):
    max_pos, max, min_pos, min = rhino_max_min()

    # apply translation
    t_range = np.array([[-0.25, 0.25]])
    t = np.clip(t, t_range[:, 0], t_range[:, 1])

    max_pos = max_pos + t
    min_pos = min_pos + t

    return max_pos, max, min_pos, min


# Rhino2-function
def rhino2(x, h):
    assert .5 <= h <= 1.0

    def bump(x, mu, sigma):
        return np.exp(-1 / 2 * (x - mu) ** 2 / sigma ** 2)

    # bump1 = bump(x, mu=0.2, sigma=0.05)
    bump1 = bump(x, mu=0.2, sigma=0.1)
    # bump2 = bump(x, mu=h, sigma=0.03)
    bump2 = bump(x, mu=h, sigma=0.01)
    rhino = h * bump1 + 2 * bump2
    rhino = rhino - 1.0

    return rhino


def rhino2_max_min(h):
    max_pos = np.array(h).reshape(1, 1)
    min_pos = np.array(0.0).reshape(1, 1)
    max = rhino2(max_pos, h)
    min = rhino2(min_pos, h)

    return max_pos, max, min_pos, min

File Path: metabo/environment/simcore/__init__.py
Content:

File Path: metabo/environment/simcore/controller/__init__.py
Content:

File Path: metabo/environment/simcore/controller/base_controller.py
Content:
## Copyright (c) 2019 Robert Bosch GmbH
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>.

class Controller:
    """
    Base class for control strategy dependent on the used environment
    """

    def __init__(self):
        self.output = 0
        pass

    def calc_input(self, state, xr):
        raise NotImplementedError

File Path: metabo/environment/simcore/controller/dlqr.py
Content:
# Copyright (c) 2019 Robert Bosch GmbH
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>.

import numpy as np
from scipy.linalg import solve_discrete_are

import metabo.environment.simcore.controller.base_controller as cont


class Dlqr(cont.Controller):
    """
    Time-discrete linear-quadratic regulator

    """

    def __init__(self, environment, q, r, xr=None, ur=None, ctrl_dt=None):
        self.nx = environment.param.nx
        self.nu = environment.param.nu
        cont.Controller.__init__(self)

        if xr is None:
            xr = np.zeros(self.nx)

        if ur is None:
            ur = np.zeros(self.nu)

        # rest position and u0 is currently set to zero
        if ctrl_dt is None:
            ad, bd = environment.linearize(xr, ur)
        else:
            ad, bd = environment.linearize(xr, ur, dt=ctrl_dt)

        # get the solution of the discrete riccati equation
        p = np.array(solve_discrete_are(ad, bd, q, r))

        # calculate feedback gain
        self._K = np.dot(np.linalg.inv(
            np.array(r + np.dot(bd.T.dot(p), bd), dtype=float)),
            np.dot(bd.T.dot(p), ad))

    def calc_input(self, state, xr):
        self.output = -self._K @ (state - xr)
        return self.output

File Path: metabo/environment/simcore/environment/__init__.py
Content:

File Path: metabo/environment/simcore/environment/base_environment.py
Content:
# Copyright (c) 2019 Robert Bosch GmbH
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>.

import numpy as np
from sympy import symbols, Matrix
import sympy as sym
import matplotlib.pyplot as plt

from metabo.environment.simcore import utils as utl


class Environment:
    """
    Base-Class for all environments, containing simulation and model parameters
    The dynamics of a system are represented as xd = f(x, u), y = h(x).
    Every environment provides a linearization method which returns the
    system matrices Ad and Bd of a discrete state space model
    xk+1 = Ad*xk + Bd*uk.
    In addition, a plot function is provided, plotting the trajectories of
    state, input and output.
    """

    def __init__(self, simulation_parameters, model_parameters, animate,
                 estimate, noise, visualize):
        """
        :param simulation_parameters: runtime, sample-time, and substeps
        :param model_parameters: parameters describing the environment
        :param animate: enable for animation
        :param estimate: enable for state estimation
        :param noise: enable for noise addition
        :param visualize: enable for plotting
        """
        self.simulation_parameters = simulation_parameters
        self.param = model_parameters

        self._sym_state = symbols('x:%i' % self.param.nx, real=True)
        self._sym_input = symbols('u:%i' % self.param.nu, real=True)

        # ac, bc, and cc are python lambda-functions for a fast evaluation of
        # the jacobians with a specific state/action
        self.ac = sym.utilities.lambdify((self._sym_state, self._sym_input),
                                         self.fsym().jacobian(self._sym_state))

        self.bc = sym.utilities.lambdify((self._sym_state, self._sym_input),
                                         Matrix(self.fsym()).
                                         jacobian(Matrix(self._sym_input)))

        self.cc = sym.utilities.lambdify((self._sym_state, self._sym_input),
                                         Matrix(self.hsym()).
                                         jacobian(Matrix(self._sym_state)))

        self.f = sym.utilities.lambdify((self._sym_state, self._sym_input),
                                        self.fsym())

        del self._sym_input, self._sym_state

        # state and action constraints
        self.x_constraints = None
        self.u_constraints = None

        # noise
        self.transit_noise_deviation = None
        self.observation_noise_deviation = None

        # enables
        self.noise = noise
        self.estimate = estimate
        self.animate = animate
        self.visualize = visualize

        # for animation
        self.n_lines = 0
        self.coordinates_dict = dict()

        self.state_history = []
        self.input_history = []
        self.noisy_output_history = []
        self.output_history = []  # for animation only, without noise

    def next_(self, old_state, u):
        """
        returns the state for the next time step based on a given input u and a
        previous state old_state

        solves differential equation using the classical Runge Kutta (RK4)
        depending on 'substeps' RK4 executes 'substeps'
        sub steps for a higher accuracy
        """
        nx = self.param.nx
        n = self.simulation_parameters.rk_substeps
        dt = self.simulation_parameters.dt
        step_size = dt / n
        k1 = np.empty(len(old_state))
        k2 = np.empty(len(old_state))
        k3 = np.empty(len(old_state))
        k4 = np.empty(len(old_state))
        for k in range(0, n):
            k1[:] = self.f(old_state, u).flatten()
            k2[:] = self.f(old_state + step_size * k1 / 2, u).flatten()
            k3[:] = self.f(old_state + step_size * k2 / 2, u).flatten()
            k4[:] = self.f(old_state + step_size * k3, u).flatten()

            phi = k1 / 6 + k2 / 3 + k3 / 3 + k4 / 6

            old_state = old_state + step_size * phi

        self.state_history.append(old_state)
        self.input_history.append(u)
        self.output_history.append(self.get_output(old_state))
        return old_state

    def fsym(self):
        """
        returns rhs of nonlinear state space model depending on symbolic
        variables _sym_state and _sym_input. Use only for initialisation
        purposes
        :return: sympy.Matrix type
        """
        raise NotImplementedError

    def euler_next(self, state, u):
        """
        prediction of the next state using euler approximation
        :param state:  current state
        :param u:   current input
        :return:    state prediction
        """
        return state + self.simulation_parameters.dt * self.f(state, u).squeeze()

    def hsym(self):
        """
        :return: Symbolic output y = h(x), dx/dt = f(x, u)
        :rtype: sympy.Matrix
        """
        raise NotImplementedError

    def get_current_a_matrix(self, x0, u0):
        """
        returns the A matrix of a linear state space system xk+1 = A*xk + B*uk
        yk = C*xk by linearisation of the non-linear state
        space model using jacobian at x0, u0
        """
        assert type(x0) is np.ndarray, \
            "rest_position must be an array containing floats"

        a = self.ac(x0, u0)
        return utl.matrix_exponential(self.simulation_parameters.dt * a)

    def get_current_b_matrix(self, x0, u0):
        """
        returns the B matrix of a linear state space system xk+1 = A*xk + B*uk,
        yk = C*xk by linearisation of the non-linear state
        space model using jacobian at x0, u0
        """
        b = self.bc(x0, u0)
        return utl.discretize_b(self.get_current_a_matrix(
            x0, u0), b, self.simulation_parameters.dt)

    def get_current_c_matrix(self, x0, u0):
        """
        returns the C matrix of a linear state space system xk+1 = Ad*xk + Bd*uk
        yk = Cd*xk by linearisation of the non-linear state
        space model using jacobian at x0, u0
        """
        return self.cc(x0, u0)

    def linearize(self, x0, u0, dt=None):
        """
        Linearize the System around the given set-point x0 and u0
        :param x0: state at evaluation point
        :param u0: input at evaluation point
        :param dt: sample time which is used for linearization
        :return: Discrete system matrices Ad and Bd
        :rtype: tuple of numpy-arrays
        """

        ac = self.ac(x0.squeeze(), u0)
        bc = self.bc(x0.squeeze(), u0)
        if dt is None:
            sample_time = self.simulation_parameters.dt
        else:
            sample_time = dt

        ad = np.eye(ac.shape[0]) + sample_time * ac
        bd = sample_time * bc

        return ad, bd

    def observe(self, x_next):
        """
        the observe method returns the output of the System, depending on the
        configurations in the environment initialization:
        If neither estimate nor noise are activated, the state will just get
        passed through.
        If estimate is activated, the function returns the output y = h(x).
        In addition, if noise is activated too, observe returns the out
        :param x_next: current state
        :return: system-output
        """
        out = 0
        if self.noise and not self.estimate:
            out = x_next + utl.get_noise(
                self.transit_noise_deviation, self.simulation_parameters.dt)
        elif not self.noise and not self.estimate:
            out = x_next

        if self.estimate:
            if self.noise:
                out = self.get_output(x_next) + np.reshape(utl.get_noise(
                    self.observation_noise_deviation,
                    self.simulation_parameters.dt), (self.param.ny, 1))
                self.noisy_output_history.append(out)
            else:
                out = self.get_output(x_next)

        self.output_history.append(out)
        return out

    def get_output(self, state):
        """
        :param state: current state
        :return: current output
        """
        raise NotImplementedError

    def plot(self, est_state_history=None):
        """
        This function generates a plot for all state variables and for the input
        u of the system. If the state was estimated, you have to give the
        history as input.

        :param est_state_history: History of the estimated state.
        :return: None
        """
        if est_state_history is None:
            est_state_history = np.zeros(self.param.nx)
        t = np.linspace(0, (len(self.state_history) - 1)
                        * self.simulation_parameters.dt,
                        len(self.state_history))

        # convert histories to arrays
        input_history = np.asarray(self.input_history).reshape(self.param.nu, -1)
        state_history = np.asanyarray(self.state_history)
        est_state_history = np.asanyarray(est_state_history)

        x_data = np.empty([self.param.nx, len(t)])
        xest_data = np.empty([self.param.nx, len(t)])

        # plot state
        for i in range(0, self.param.nx):
            for j in range(0, len(t)):
                x_data[i, j] = state_history[j, i]

        xfig1 = plt.figure()
        x_ax = []
        for i in range(1, self.param.nx + 1):
            x_ax.append(xfig1.add_subplot(int(100 * self.param.nx + 10 + i)))

        for i in range(0, self.param.nx):
            x_ax[i].plot(np.array(t), np.array(x_data[i]), 'b')

        x_ax[0].legend(["x"]),

        # plot input
        u_data = np.empty([self.param.nu, len(t)])
        for i in range(0, self.param.nu):
            for j in range(0, len(t)):
                u_data[i, j] = input_history[i, j]

        ufig = plt.figure()
        u_ax = []
        for i in range(1, self.param.nu + 1):
            u_ax.append(ufig.add_subplot(int(100 * self.param.nu + 10 + i)))

        for i in range(0, self.param.nu):
            u_ax[i].plot(np.array(t), np.array(u_data[i]), 'r')
        u_ax[0].legend(["u"]),

        # if estimator is activated plot estimated state history
        if self.estimate:
            # plot estimated state in same the figure
            for i in range(0, self.param.nx):
                for j in range(0, len(t)):
                    xest_data[i, j] = est_state_history[j, i]

            for i in range(0, self.param.nx):
                x_ax[i].plot(np.array(t), np.array(xest_data[i]), 'r')
                x_ax[i].set_ylabel('x%i' % (i + 1))
                if i == self.param.nx - 1:
                    x_ax[i].set_xlabel('t [s]')

            x_ax[0].legend(["x", "xest"])

            # if noise is activated plot the output history too
            if self.noise:
                # Plot noisy output signals into a separate figure
                yfig = plt.figure()
                y_ax = []
                for i in range(1, self.param.ny + 1):
                    y_ax.append(yfig.add_subplot(100 * self.param.ny + 10 + i))

                y_data = np.empty([self.param.ny, len(t)])

                for i in range(0, self.param.ny):
                    for j in range(0, len(t)):
                        y_data[i, j] = self.noisy_output_history[j][i, 0]

                for i in range(0, self.param.ny):
                    y_ax[i].plot(np.array(t), np.array(y_data[i]), 'g')
                    y_ax[i].set_ylabel('y%i' % (i + 1))
                    if i == self.param.ny - 1:
                        y_ax[i].set_xlabel('t [s]')

        if not self.animate:
            plt.show()

    def generate_step_traj(self, xr, x0=None, percentage_runtime=0.5):
        """
        Generates a reference-trajectory for a step response. Initial reference
        is 0 for all state-entries
        :param xr: final value
        :param x0: initial value
        :type x0: numpy array, shape (nx, 1)
        :param percentage_runtime: percentage of runtime, when the reference
            step should appear
        :return: array containing the trajectory
        """
        ref_traj = np.zeros((int(self.simulation_parameters.runtime /
                                 self.simulation_parameters.dt), self.param.nx))

        for i in range(ref_traj.shape[0]):
            if x0 is not None:
                if i <= self.simulation_parameters.runtime / \
                        self.simulation_parameters.dt * percentage_runtime:
                    ref_traj[i] = x0.reshape(self.param.nx)

            if i > self.simulation_parameters.runtime / \
                    self.simulation_parameters.dt * percentage_runtime:
                ref_traj[i] = xr.reshape(self.param.nx)

        return ref_traj

File Path: metabo/environment/simcore/environment/furuta.py
Content:
# Copyright (c) 2019 Robert Bosch GmbH
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>.

import numpy as np
from sympy import sin, cos, Matrix

import metabo.environment.simcore.environment.base_environment as env
import metabo.environment.simcore.parameters.furuta_parameters as parameter


class FurutaPendulum(env.Environment):
    """
    x = [q, dq]^T, with q = [theta_arm, theta_pendulum]
    default parameters are related to the Quanser Cube Servo 2:
    https://www.quanser.com/products/qube-servo-2/
    Initial position (x = [0, 0, 0, 0]^T) is at the upper equilibrium point
    """

    def __init__(self, simulation_parameters, mass_arm=0.095,
                 mass_pendulum=0.024, damp_arm=0.0005, length_arm=0.112,
                 length_pendulum=0.129, damp_pendulum=0.00005, noise=False,
                 animate=True, estimate=True, visualize=True):
        # set up the parameters
        self.param = parameter.FurutaParameters(m_a=mass_arm, m_p=mass_pendulum,
                                                d_a=damp_arm, d_p=damp_pendulum,
                                                l_a=length_arm,
                                                l_p=length_pendulum)

        # pass through the environment parameters for environment setup
        env.Environment.__init__(self,
                                 simulation_parameters=simulation_parameters,
                                 model_parameters=self.param, animate=animate,
                                 estimate=estimate, noise=noise,
                                 visualize=visualize)

        # constraints
        x_upper_lim = np.array([np.pi / 2, np.inf, np.inf, np.inf])
        x_lower_lim = -1 * x_upper_lim
        u_upper_lim = np.array([np.inf])
        u_lower_lim = -1 * u_upper_lim
        self.x_constraints = np.vstack([x_upper_lim, x_lower_lim])
        self.u_constraints = np.vstack([u_upper_lim, u_lower_lim])

        # deviation of noise
        self.transit_noise_deviation = np.array([0.0, 0.0, 0., 0.])
        self.observation_noise_deviation = np.array([0.0, 0.0])

        # FurutaPendulum can be represented by 2 lines
        self.n_lines = 2
        for i in range(self.n_lines):
            self.coordinates_dict[i] = []

    def fsym(self):
        # load parameters
        # l_p here is the distance to center of mass
        l_a, l_p, m_a, m_p, d_a, d_p, k_a, g = (self.param.l_a,
                                                self.param.l_p / 2,
                                                self.param.m_a, self.param.m_p,
                                                self.param.d_a, self.param.d_p,
                                                self.param.k_c, self.param.g)

        # compute the pendulum's inertia, inertia along the roll axis is
        # neglected
        j2 = 0
        j0 = (m_a * l_a ** 2) / 12
        j1 = (m_p * l_p ** 2) / 12

        u1 = self._sym_input[0]
        x = self._sym_state

        # mass matrix
        m = Matrix([[1.0 * j0 + 2 * j2 + m_p * (l_a ** 2 + l_p ** 2 * sin(x[1]) ** 2),
                     -l_a * l_p * m_p * cos(x[1])], [-l_a * l_p * m_p * cos(x[1]),
                                                     2 * j1 + 1.0 * l_p ** 2 * m_p]])

        # coriolis accelerations and all this stuff
        n = Matrix([[x[3] * l_p * m_p * (2 * x[2] * l_p * cos(x[1]) + x[3] * l_a) * sin(x[1])],
                    [-l_p * m_p * (x[2] * (x[2] * l_p * cos(x[1]) + x[3] * l_a)
                                   + 2 * g) * sin(x[1])]])

        # elasticity and damping forces
        fda = -d_a * Matrix([[x[2]], [0]])
        fdp = -d_p * Matrix([[0], [x[3]]])
        fc = -k_a * Matrix([[x[0]], [0]])

        # torque vector
        u = Matrix([[u1], [0]])

        # build up the equations for accelerations
        qdd = (m ** -1) * (fda + fdp + fc + u - n)

        # concatenate kinetics to end up with a state space model
        dx = qdd.row_insert(0, Matrix([[x[2]], [x[3]]]))

        return dx

    def hsym(self):
        # define output-equation
        return Matrix([[self._sym_state[0]], [self._sym_state[1]]])

    def get_output(self, state):
        return np.array([state[0], state[1]]).reshape(2, 1)

File Path: metabo/environment/simcore/furuta_model.py
Content:
# Copyright (c) 2019 Robert Bosch GmbH
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>.


import sympy as sym
from sympy import symbols, Matrix, sin, cos

'''
Script to derive the dynamics of a furuta pendulum

xd = f(x,u) with f(x,u) = M^-1*(sum(F)-N) 
derivation Lagrangian mechanics
Frames: when you have the Qube in front of you, the pendulum aiming at yourself,
the x axis aims at you, the y-axis to the top and the z-axis to the left.
In the initial position (pendulum is in the upper equilibrium point) all frames 
have the same orientation 
'''

# define symbolic parameters
J1, J2, J3 = symbols("J1, J2, J3")
J0 = symbols("J0")
m1, m2 = symbols("m1, m2")
l1, l2 = symbols("l1, l2")
g = symbols("g")
theta1, theta2 = symbols("theta1, theta2")
dtheta1, dtheta2 = symbols("dtheta1, dtheta2")
# the inertia of the pendulum for rotation around x- or z axis are the same
J = Matrix.diag((J1, J2, J1))

# generalized coordinates
q = Matrix([[theta1], [theta2]])
dq = Matrix([[dtheta1], [dtheta2]])

# kinetic energy of joint arm. has only rotational kinetic energy
k1 = Matrix([[0.5 * J0 * dtheta1 ** 2]])

# kinetics of pendulum
omega2 = Matrix([[cos(theta1) * dtheta2], [dtheta1], [sin(theta1) * dtheta2]])
v2 = l1 * dtheta1 * Matrix([[-sin(theta1)], [0], [-cos(theta1)]]) \
     + l2 * dtheta1 * Matrix([[cos(theta1) * sin(theta2)], [0],
                              [-sin(theta1) * sin(theta2)]]) \
     + dtheta2 * l2 * Matrix([[sin(theta1) * cos(theta2)], [-sin(theta2)],
                              [cos(theta2) * cos(theta1)]])

# kinetic energy (rotational and translational)
k2r = sym.simplify(omega2.T * J * omega2)
k2t = 0.5 * m2 * sym.simplify(v2.T * v2)

# potential energy of pendulum
p = Matrix([[m2 * 2 * l2 * g * cos(theta2)]])

# total kinetic energy
K = k1 + k2t + k2r

# calculate Lagrangian
L = K - p

# partial derivative to dq
Kp = sym.simplify(K.jacobian(dq))
kpp = sym.simplify(Kp.jacobian(q))
print("unknown k = ", kpp)

M = sym.simplify(Kp.jacobian(dq))
N = sym.simplify(-sym.simplify(L.jacobian(q)).T + Matrix(kpp.dot(dq)))
print("M = ", M)
print("N = ", N)

File Path: metabo/environment/simcore/parameters/__init__.py
Content:

File Path: metabo/environment/simcore/parameters/base_parameters.py
Content:
class Parameter:
    """The Parameter class defines objects containing parameters which describe
    an environment or a simulation configuration
    """
    def __init__(self):
        pass

File Path: metabo/environment/simcore/parameters/furuta_parameters.py
Content:
import metabo.environment.simcore.parameters.base_parameters as param


class FurutaParameters(param.Parameter):
    """Parameters:
       nx: dimension state-space
       nu: dimension input
       m_a: mass of arm [kg]
       l_a: length of arm [m]
       d_a: damping constant for rotation of the arm [N*s/rad]
       m_p: mass of pendulum [kg]
       l_p: length of pendulum [m]
       d_p: damping constant for rotation of the pendulum [N*s/rad]
       k_c: stiffness modelling the forced induced by the cable attachment
    """
    def __init__(self, m_a=0.095, l_a=0.112, d_a=0.0005, m_p=0.024, l_p=0.129,
                 d_p=0.00005, k_c=0.016, g=9.81):
        param.Parameter.__init__(self)
        self.ny = 2
        self.nx = 4
        self.nu = 1

        self.m_a = m_a
        self.l_a = l_a
        self.d_a = d_a

        self.m_p = m_p
        self.l_p = l_p
        self.d_p = d_p

        self.k_c = k_c
        self.g = g

File Path: metabo/environment/simcore/parameters/simulation_parameters.py
Content:
import metabo.environment.simcore.parameters.base_parameters as param


class SimulationParameters(param.Parameter):
    """
    SimulationParameters contains the parameters for the simulation
    configuration, like runtime, sample time
    Additionally it contains a parameter (substeps) determining the number of
    sub steps
    per time step applied by the ode solver
    """
    def __init__(self, runtime, dt, rk_substeps=5):
        param.Parameter.__init__(self)
        self.runtime = runtime
        self.dt = dt
        self.rk_substeps = rk_substeps
        self.n_simsteps = int(runtime/dt)

File Path: metabo/environment/simcore/utils.py
Content:
# Copyright (c) 2019 Robert Bosch GmbH
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>.

import numpy as np
from scipy.linalg import expm


def matrix_exponential(a):
    """
    Computing the matrix exponential of a matrix A using taylor-series.
    """
    exp = expm(a)
    return exp


def discretize_b(ac, bc, sample_time):
    """
    approximates the integral int(e^(A*nu)*B)d_nu, nu=0...sample_time using step
    size 'step'
    """
    nx = ac.shape[0]
    nu = bc.shape[1]
    tmp = np.hstack([ac, bc])
    tmp2 = np.hstack([np.zeros((nu, nx)), np.zeros((nu, nu))])

    exponent = sample_time * np.vstack((tmp, tmp2))
    exp = expm(exponent)
    bd = exp[0:nx, nx:(nx + nu)]

    return bd


def solve_dariccati(ad, bd, q, r, epsilon=0.001):
    """
    Solving discrete algebraic riccati equation (convergence of finite-horizon
    equation),
    just for testing, use np.array(solve_discrete_are(ad, bd, q, r)) instead
    """
    tmp = np.zeros(ad.shape)
    p = q
    while np.any(abs(p - tmp) / np.linalg.norm(p, ord=1) >= epsilon):
        # break up when (P(k)-P(k-1))/norm(P(k)) < epsilon
        tmp = p
        k1 = np.dot(ad.T, np.dot(tmp, ad))
        k2 = np.dot(ad.T, np.dot(tmp, bd))
        k3 = np.array(r + np.dot(bd.T, np.dot(tmp, bd)),
                      dtype=float)
        k4 = np.dot(bd.T, np.dot(tmp, ad))
        p = k1 - np.dot(k2, np.dot(np.linalg.inv(k3), k4)) + q
        print(p[0][0])  # check convergence

    return p


def get_noise(cov_diag, sample_time):
    cov = np.diag(cov_diag * sample_time)
    noise = np.zeros(cov.shape[0])
    noise = np.random.multivariate_normal(np.zeros(len(noise)), cov)
    return noise.reshape(len(noise), 1)


def rotmat_x(angle):
    return np.array([[1, 0, 0], [0, np.cos(angle), -np.sin(angle)],
                     [0, np.sin(angle), np.cos(angle)]])


def rotmat_y(angle):
    return np.array([[np.cos(angle), 0, np.sin(angle)], [0, 1, 0],
                     [-np.sin(angle), 0, np.cos(angle)]])


def rotmat_z(angle):
    return np.array([[np.cos(angle), -np.sin(angle), 0],
                     [np.sin(angle), np.cos(angle), 0], [0, 0, 1]])

File Path: metabo/environment/util.py
Content:
# Copyright (c) 2019 Robert Bosch GmbH
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>.

# ******************************************************************
# util.py
# Utilities for the MetaBO framework.
# ******************************************************************

import numpy as np


def create_uniform_grid(domain, N_samples_dim):
    D = domain.shape[0]
    x_grid = []
    for i in range(D):
        x_grid.append(np.linspace(domain[i, 0], domain[i, 1], N_samples_dim))
    X_mesh = np.meshgrid(*x_grid)
    X = np.vstack(X_mesh).reshape((D, -1)).T

    return X, X_mesh


def scale_from_unit_square_to_domain(X, domain):
    # X contains elements in unit square, stretch and translate them to lie domain
    return X * domain.ptp(axis=1) + domain[:, 0]


def scale_from_domain_to_unit_square(X, domain):
    # X contains elements in domain, translate and stretch them to lie in unit square
    return (X - domain[:, 0]) / domain.ptp(axis=1)


def get_cube_around(X, diam, domain):
    assert X.ndim == 1
    assert domain.ndim == 2
    cube = np.zeros(domain.shape)
    cube[:, 0] = np.max((X - 0.5 * diam, domain[:, 0]), axis=0)
    cube[:, 1] = np.min((X + 0.5 * diam, domain[:, 1]), axis=0)
    return cube

File Path: metabo/eval/__init__.py
Content:

File Path: metabo/eval/evaluate.py
Content:
# Copyright (c) 2019 Robert Bosch GmbH
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>.

# ******************************************************************
# evaluate.py
# Evaluation of performance of MetaBO and benchmark AFs.
# ******************************************************************

import os
import json
import numpy as np
import gym
import pickle as pkl
import torch
from datetime import datetime
from collections import namedtuple
from metabo.policies.policies import NeuralAF, UCB, EI, PI, TAF, EpsGreedy, GMM_UCB
from metabo.ppo.batchrecorder import BatchRecorder, Transition

Result = namedtuple("Result",
                    "logpath env_id env_specs policy policy_specs deterministic load_iter T n_episodes rewards")


def write_overview_logfile(savepath, timestamp, env, env_seeds, policy, policy_specs, taf_datafile=None, verbose=False):
    fname = "000_eval_overview_{}.txt".format(policy)
    s = ""
    s += "********* OVERVIEW ENVIRONMENT PARAMETERS *********\n"
    s += "Evaluation timestamp: {}\n".format(timestamp)
    s += "Environment-ID: {}\n".format(env.spec.id)
    s += "Environment-kwargs:\n"
    s += json.dumps(env.spec._kwargs, indent=2)
    s += "\n"
    s += "Environment-seeds:\n"
    s += str(env_seeds)
    s += "\n"
    s += "Policy-specs:\n"
    s += json.dumps(policy_specs, indent=2)
    if taf_datafile is not None:
        s += "\n"
        s += "TAF-Datafile: {}".format(taf_datafile)
    with open(os.path.join(savepath, fname), "w") as f:
        print(s, file=f)
    if not verbose:
        print(s)


def load_metabo_policy(logpath, load_iter, env, device, deterministic):
    with open(os.path.join(logpath, "params_" + str(load_iter)), "rb") as f:
        train_params = pkl.load(f)

    pi = NeuralAF(observation_space=env.observation_space,
                  action_space=env.action_space,
                  deterministic=deterministic,
                  options=train_params["policy_options"]).to(device)
    with open(os.path.join(logpath, "weights_" + str(load_iter)), "rb") as f:
        pi.load_state_dict(torch.load(f))
    with open(os.path.join(logpath, "stats_" + str(load_iter)), "rb") as f:
        stats = pkl.load(f)

    return pi, train_params, stats


def eval_experiment(eval_spec):
    env_id = eval_spec["env_id"]
    env_seed_offset = eval_spec["env_seed_offset"]
    policy = eval_spec["policy"]
    logpath = eval_spec["logpath"]
    policy_specs = eval_spec["policy_specs"]
    savepath = eval_spec["savepath"]
    n_workers = eval_spec["n_workers"]
    n_episodes = eval_spec["n_episodes"]
    assert n_episodes % n_workers == 0
    T = eval_spec["T"]
    if policy != "MetaBO":
        pi = None
        deterministic = None
        load_iter = None

    os.makedirs(savepath, exist_ok=True)

    env_seeds = env_seed_offset + np.arange(n_workers)
    dummy_env = gym.make(env_id)
    timestamp = datetime.strftime(datetime.now(), "%Y-%m-%d-%H-%M-%S")
    taf_datafile = policy_specs["TAF_datafile"] if "TAF_datafile" in policy_specs else None
    write_overview_logfile(savepath=savepath, timestamp=timestamp, env=dummy_env, policy=policy,
                           env_seeds=env_seeds, taf_datafile=taf_datafile, policy_specs=policy_specs)
    env_specs = dummy_env.spec._kwargs

    # prepare the policies
    if policy == "GP-UCB":
        feature_order = dummy_env.unwrapped.feature_order_eval_envs
        D = dummy_env.unwrapped.D
        policy_fn = lambda *_: UCB(feature_order=feature_order,
                                   kappa=policy_specs["kappa"],
                                   D=D,
                                   delta=policy_specs["delta"])
    elif policy == "EI":
        feature_order = dummy_env.unwrapped.feature_order_eval_envs
        policy_fn = lambda *_: EI(feature_order=feature_order)
    elif policy == "TAF-ME":
        policy_fn = lambda *_: TAF(datafile=policy_specs["TAF_datafile"], mode="me")
    elif policy == "TAF-RANKING":
        policy_fn = lambda *_: TAF(datafile=policy_specs["TAF_datafile"], mode="ranking", rho=1.0)
    elif policy == "PI":
        feature_order = dummy_env.unwrapped.feature_order_eval_envs
        policy_fn = lambda *_: PI(feature_order=feature_order, xi=policy_specs["xi"])
    elif policy == "EPS-GREEDY":
        feature_order = dummy_env.unwrapped.feature_order_eps_greedy
        policy_fn = lambda *_: EpsGreedy(datafile=policy_specs["datafile"], feature_order=feature_order,
                                         eps=policy_specs["eps"])
    elif policy == "GMM-UCB":
        feature_order = dummy_env.unwrapped.feature_order_gmm_ucb
        policy_fn = lambda *_: GMM_UCB(datafile=policy_specs["datafile"], feature_order=feature_order,
                                       ucb_kappa=policy_specs["ucb_kappa"], w=policy_specs["w"],
                                       n_components=policy_specs["n_components"])
    elif policy == "MetaBO":
        load_iter = eval_spec["load_iter"]
        deterministic = eval_spec["deterministic"]
        pi, policy_specs, _ = load_metabo_policy(logpath=logpath, load_iter=load_iter, env=dummy_env,
                                                 device="cpu", deterministic=deterministic)

        policy_fn = lambda osp, asp, det: NeuralAF(observation_space=osp,
                                                   action_space=asp,
                                                   deterministic=det,
                                                   options=policy_specs["policy_options"])
    elif policy == "Random":
        pass  # will be dealt with separately below
    else:
        raise ValueError("Unknown policy!")
    dummy_env.close()

    # evaluate the experiment
    if policy != "Random":
        br = BatchRecorder(size=T * n_episodes, env_id=env_id, env_seeds=env_seeds, policy_fn=policy_fn,
                           n_workers=n_workers, deterministic=deterministic)
        if policy == "MetaBO":
            br.set_worker_weights(pi=pi)
        br.record_batch(gamma=1.0, lam=1.0)  # gamma, lam do not matter for evaluation
        transitions = Transition(*zip(*br.memory.copy()))
        rewards = transitions.reward
        br.cleanup()
    else:
        env = gym.make(env_id)
        env.seed(env_seed_offset)
        rewards = []
        for _ in range(n_episodes):
            rewards = rewards + env.unwrapped.get_random_sampling_reward()
        env.close()

    # save result
    result = Result(logpath=logpath, env_id=env_id, env_specs=env_specs, policy=policy, policy_specs=policy_specs,
                    deterministic=deterministic, load_iter=load_iter, T=T, n_episodes=n_episodes, rewards=rewards)
    fn = "result_metabo_iter_{:04d}".format(load_iter) if policy == "MetaBO" else "result_{}".format(policy)
    with open(os.path.join(savepath, fn), "wb") as f:
        pkl.dump(result, f)

File Path: metabo/eval/plot_results.py
Content:
# Copyright (c) 2019 Robert Bosch GmbH
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>.

# ******************************************************************
# plot_results.py
# Functionality for plotting performance of AFs.
# ******************************************************************

import os
from matplotlib import pyplot as plt
import pickle as pkl
import numpy as np
from metabo.eval.evaluate import Result  # for unpickling


def plot_results(path, logplot=False):
    fig, ax = plt.subplots(nrows=1, ncols=1)

    # collect results in savepath
    results = []
    for fn in os.listdir(path):
        if fn.startswith("result"):
            with open(os.path.join(path, fn), "rb") as f:
                result = pkl.load(f)
                results.append(result)

    env_id = results[0].env_id

    # do the plot
    for result in results:
        # prepare rewards_dict
        rewards_dict = {}
        for i, rew in enumerate(result.rewards):
            if isinstance(rew, tuple):
                t = rew[1]
                reward = rew[0]
            else:
                t = i % result.T + 1
                reward = rew

            if str(t) in rewards_dict:
                rewards_dict[str(t)].append(reward)
            else:
                rewards_dict[str(t)] = [reward]

        t_vec, loc, err_low, err_high = [], [], [], []
        for key, val in rewards_dict.items():
            t_vec.append(int(key))
            cur_loc = np.median(val)
            cur_err_low = np.percentile(val, q=70)
            cur_err_high = np.percentile(val, q=30)
            loc.append(cur_loc)
            err_low.append(cur_err_low)
            err_high.append(cur_err_high)

        t_vec, loc, err_low, err_high = np.array(t_vec), np.array(loc), np.array(err_low), np.array(err_high)
        # sort the arrays according to T
        sort_idx = np.argsort(t_vec)
        t_vec = t_vec[sort_idx]
        loc = loc[sort_idx]
        err_low = err_low[sort_idx]
        err_high = err_high[sort_idx]

        if not logplot:
            line = ax.plot(t_vec, loc, label=result.policy)[0]
            ax.fill_between(t_vec, err_low, err_high, alpha=0.2, facecolor=line.get_color())
        else:
            line = ax.semilogy(t_vec, loc, label=result.policy)[0]
            ax.fill_between(t_vec, err_low, err_high, alpha=0.2, facecolor=line.get_color())

    fig.suptitle(env_id)
    ax.grid(alpha=0.3)
    ax.set_xlabel("t", labelpad=0)
    ax.set_ylabel("simple regret")
    ax.legend()

    fig.savefig(fname=os.path.join(path, "plot.png"))
    plt.close(fig)


File Path: metabo/eval/plot_results2.py
Content:
# Copyright (c) 2019 Robert Bosch GmbH
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>.

# ******************************************************************
# plot_results.py
# Functionality for plotting performance of AFs.
# ******************************************************************

import os
from matplotlib import pyplot as plt
import pickle as pkl
import numpy as np
from metabo.eval.evaluate import Result  # for unpickling


def plot_results2(path, logplot=False):
    # this function corrects for approximate maxima in contrast to plot_results1
    fig, ax = plt.subplots(nrows=1, ncols=1)

    # collect results in savepath
    results = []
    for fn in os.listdir(path):
        if fn.startswith("result"):
            with open(os.path.join(path, fn), "rb") as f:
                result = pkl.load(f)
                results.append(result)

    env_id = results[0].env_id

    # determine best final regret to correct for approximate maxima
    R_best = np.ones((results[0].n_episodes,)) * np.inf
    for result in results:
        # prepare rewards_dict
        rewards_dict = {}
        for i, rew in enumerate(result.rewards):
            if isinstance(rew, tuple):
                t = rew[1]
                reward = rew[0]
            else:
                t = i % result.T + 1
                reward = rew

            if str(t) in rewards_dict:
                rewards_dict[str(t)].append(reward)
            else:
                rewards_dict[str(t)] = [reward]

        # correct for approximate maxima
        for i in range(result.n_episodes):
            cur_R_best = rewards_dict[str(result.T)][i]
            if cur_R_best < R_best[i]:
                R_best[i] = cur_R_best

    # do the plot
    for result in results:
        # prepare rewards_dict
        rewards_dict = {}
        for i, rew in enumerate(result.rewards):
            if isinstance(rew, tuple):
                t = rew[1]
                reward = rew[0]
            else:
                t = i % result.T + 1
                reward = rew

            if str(t) in rewards_dict:
                rewards_dict[str(t)].append(reward)
            else:
                rewards_dict[str(t)] = [reward]

        # correct for approximate maxima
        for i in range(result.n_episodes):
            if R_best[i] < 0:
                for t in range(1, result.T + 1):
                    rewards_dict[str(t)][i] += -R_best[i]

        t_vec, loc, err_low, err_high = [], [], [], []
        for key, val in rewards_dict.items():
            t_vec.append(int(key))
            cur_loc = np.median(val)
            cur_err_low = np.percentile(val, q=70)
            cur_err_high = np.percentile(val, q=30)
            loc.append(cur_loc)
            err_low.append(cur_err_low)
            err_high.append(cur_err_high)

        t_vec, loc, err_low, err_high = np.array(t_vec), np.array(loc), np.array(err_low), np.array(err_high)
        # sort the arrays according to T
        sort_idx = np.argsort(t_vec)
        t_vec = t_vec[sort_idx]
        loc = loc[sort_idx]
        err_low = err_low[sort_idx]
        err_high = err_high[sort_idx]

        if not logplot:
            line = ax.plot(t_vec, loc, label=result.policy)[0]
            ax.fill_between(t_vec, err_low, err_high, alpha=0.2, facecolor=line.get_color())
        else:
            line = ax.semilogy(t_vec, loc, label=result.policy)[0]
            ax.fill_between(t_vec, err_low, err_high, alpha=0.2, facecolor=line.get_color())

    fig.suptitle(env_id)
    ax.grid(alpha=0.3)
    ax.set_xlabel("t", labelpad=0)
    ax.set_ylabel("simple regret")
    ax.legend()

    fig.savefig(fname=os.path.join(path, "plot2.png"))
    plt.close(fig)

File Path: metabo/policies/__init__.py
Content:

File Path: metabo/policies/mlp.py
Content:
# Copyright (c) 2019 Robert Bosch GmbH
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>.

# ******************************************************************
# mlp.py
# A simple multi-layer perceptron in pytorch.
# ******************************************************************

from torch import nn as nn
from torch.nn import functional as F


class MLP(nn.Module):
    def __init__(self, d_in: int, d_out: int, arch_spec: list, f_act=None):
        """
        A standard multi-layer perceptron.
        :param d_in: number of input features.
        :param d_out: number of output features.
        :param arch_spec: list containing the number of units in each hidden layer. If arch_spec == [], this is a
                          linear model.
        :param f_act: nonlinear activation function (if arch_spec != [])
        """
        super(MLP, self).__init__()

        self.arch_spec = arch_spec
        self.f_act = f_act
        self.is_linear = (arch_spec == [])  # no hidden layers --> linear model
        if not self.is_linear:
            assert f_act is not None

        # define the network
        if self.is_linear:
            self.fc = nn.ModuleList([nn.Linear(in_features=d_in, out_features=d_out)])
        else:
            self.fc = nn.ModuleList([nn.Linear(in_features=d_in, out_features=arch_spec[0])])
            for i in range(1, len(arch_spec)):
                self.fc.append(nn.Linear(in_features=self.fc[-1].out_features, out_features=arch_spec[i]))
            self.fc.append(nn.Linear(in_features=self.fc[-1].out_features, out_features=d_out))

    def forward(self, X):
        Y = X
        if self.is_linear:
            Y = self.fc[0](Y)
        else:
            for layer in self.fc[:-1]:
                Y = self.f_act(layer(Y))
            Y = self.fc[-1](Y)

        return Y

File Path: metabo/policies/policies.py
Content:
# Copyright (c) 2019 Robert Bosch GmbH
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>.

# ******************************************************************
# policies.py
# Implementation of the MetaBO neural AF as well as benchmark AFs.
# ******************************************************************

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.distributions import Categorical
import numpy as np
from metabo.policies.mlp import MLP
from scipy.stats import norm
import pickle as pkl
import GPy
from sklearn import mixture


class NeuralAF(nn.Module):
    """
    Base class for MetaBO-Policies. Subclasses have to implement init_structure() and forward().

    SHAPES:
    forward()
     states: (N_batch, N_grid, N_features)
     logits: (N_batch, N_grid)
     values: (N_batch, )
    act(): only one action/value at a time in self.act()
     state: (N_grid, N_features)
     action: ()
     value: ()
    predict_vals_logps_ents()
     states: (N_batch, N_grid, N_features)
     actions: (N_batch, )
     values: (N_batch, )
     logprobs: (N_batch, )
     entropies: (N_batch, )
    """

    def __init__(self, observation_space, action_space, deterministic, options):
        super(NeuralAF, self).__init__()
        self.N_features = None  # has to be set in init_structure()
        self.deterministic = deterministic

        # initialize the network structure
        self.init_structure(observation_space=observation_space, action_space=action_space, options=options)

        # initialize weights
        self.apply(self.init_weights)

    def init_structure(self, observation_space, action_space, options):
        self.N_features = observation_space.shape[1]

        # activation function
        if options["activations"] == "relu":
            f_act = F.relu
        elif options["activations"] == "tanh":
            f_act = torch.tanh
        else:
            raise NotImplementedError("Unknown activation function!")

        # policy network
        self.N_features_policy = self.N_features
        if "exclude_t_from_policy" in options:
            self.exclude_t_from_policy = options["exclude_t_from_policy"]
            assert "t_idx" in options
            self.t_idx = options["t_idx"]
            self.N_features_policy = self.N_features_policy - 1 if self.exclude_t_from_policy else self.N_features_policy
        else:
            self.exclude_t_from_policy = False
        if "exclude_T_from_policy" in options:
            self.exclude_T_from_policy = options["exclude_T_from_policy"]
            assert "T_idx" in options
            self.T_idx = options["T_idx"]
            self.N_features_policy = self.N_features_policy - 1 if self.exclude_T_from_policy else self.N_features_policy
        else:
            self.exclude_T_from_policy = False

        self.policy_net = MLP(d_in=self.N_features_policy, d_out=1, arch_spec=options["arch_spec"], f_act=f_act)

        # value network
        if "use_value_network" in options and options["use_value_network"]:
            self.use_value_network = True
            self.value_net = MLP(d_in=2, d_out=1, arch_spec=options["arch_spec_value"], f_act=f_act)
            self.t_idx = options["t_idx"]
            self.T_idx = options["T_idx"]
        else:
            self.use_value_network = False

    def forward(self, states):
        assert states.dim() == 3
        assert states.shape[-1] == self.N_features

        # policy network
        mask = [True] * self.N_features
        if self.exclude_t_from_policy:
            mask[self.t_idx] = False
        if self.exclude_T_from_policy:
            mask[self.T_idx] = False
        logits = self.policy_net.forward(states[:, :, mask])
        logits.squeeze_(2)

        # value network
        if self.use_value_network:
            tT = states[:, [0], [self.t_idx, self.T_idx]]
            values = self.value_net.forward(tT)
            values.squeeze_(1)
        else:
            values = torch.zeros(states.shape[0]).to(logits.device)

        return logits, values

    def af(self, state):
        state = torch.from_numpy(state[None, :].astype(np.float32))
        with torch.no_grad():
            out = self.forward(state)
        af = out[0].to("cpu").numpy().squeeze()

        return af

    def act(self, state):
        # here, state is assumed to contain a single state, i.e. no batch dimension
        state = state.unsqueeze(0)  # add batch dimension
        out = self.forward(state)
        logits = out[0]
        value = out[1]
        if self.deterministic:
            action = torch.argmax(logits)
        else:
            distr = Categorical(logits=logits)
            # to sample the action, the policy uses the current PROCESS-local random seed, don't re-seed in pi.act
            action = distr.sample()

        return action.squeeze(0), value.squeeze(0)

    def predict_vals_logps_ents(self, states, actions):
        assert actions.dim() == 1
        assert states.shape[0] == actions.shape[0]
        out = self.forward(states)
        logits = out[0]
        values = out[1]

        distr = Categorical(logits=logits)
        logprobs = distr.log_prob(actions)
        entropies = distr.entropy()

        return values, logprobs, entropies

    def set_requires_grad(self, requires_grad):
        for p in self.parameters():
            p.requires_grad = requires_grad

    def reset(self):
        pass

    @staticmethod
    def num_flat_features(x):
        return np.prod(x.size()[1:])

    @staticmethod
    def init_weights(m):
        if type(m) == nn.Linear:
            m.weight.data.normal_(mean=0.0, std=0.01)
            m.bias.data.fill_(0.0)


class UCB():
    def __init__(self, feature_order, kappa, D=None, delta=None):
        self.feature_order = feature_order
        self.kappa = kappa
        self.D = D
        self.delta = delta
        assert not (self.kappa == "gp_ucb" and self.D is None)
        assert not (self.kappa == "gp_ucb" and self.delta is None)
        np.random.seed(0)  # make UCB behave deterministically

    def act(self, state):
        state = state.numpy()
        ucbs = self.af(state)
        action = np.random.choice(np.flatnonzero(ucbs == ucbs.max()))
        value = 0.0

        action = torch.tensor([action], dtype=torch.int64)
        value = torch.tensor([value])
        return action.squeeze(0), value.squeeze(0)

    def af(self, state):
        mean_idx = self.feature_order.index("posterior_mean")
        means = state[:, mean_idx]
        std_idx = self.feature_order.index("posterior_std")
        stds = state[:, std_idx]
        if self.kappa == "gp_ucb":
            timestep_idx = self.feature_order.index("timestep")
            timesteps = state[:, timestep_idx] + 1  # MetaBO timesteps start at 0
        else:
            timesteps = None

        kappa = self.compute_kappa(timesteps)
        ucbs = means + kappa * stds
        return ucbs

    def compute_kappa(self, timesteps):
        # https: // arxiv.org / pdf / 0912.3995.pdf
        # https: // arxiv.org / pdf / 1012.2599.pdf
        if self.kappa == "gp_ucb":
            assert timesteps is not None
            nu = 1
            tau_t = 2 * np.log(timesteps ** (self.D / 2 + 2) * np.pi ** 2 / (3 * self.delta))
            kappa = np.sqrt(nu * tau_t)
        else:
            assert timesteps is None
            kappa = self.kappa
        return kappa

    def set_requires_grad(self, flag):
        pass

    def reset(self):
        pass


class EI():
    def __init__(self, feature_order):
        self.feature_order = feature_order

    def act(self, state):
        state = state.numpy()
        eis = self.af(state)
        action = np.random.choice(np.flatnonzero(eis == eis.max()))
        value = 0.0

        action = torch.tensor([action], dtype=torch.int64)
        value = torch.tensor([value])
        return action.squeeze(0), value.squeeze(0)

    def af(self, state):
        mean_idx = self.feature_order.index("posterior_mean")
        means = state[:, mean_idx]
        std_idx = self.feature_order.index("posterior_std")
        stds = state[:, std_idx]
        incumbent_idx = self.feature_order.index("incumbent")
        incumbents = state[:, incumbent_idx]

        mask = stds != 0.0
        eis, zs = np.zeros((means.shape[0],)), np.zeros((means.shape[0],))
        zs[mask] = (means[mask] - incumbents[mask]) / stds[mask]
        pdf_zs = norm.pdf(zs)
        cdf_zs = norm.cdf(zs)
        eis[mask] = (means[mask] - incumbents[mask]) * cdf_zs + stds[mask] * pdf_zs
        return eis

    def set_requires_grad(self, flag):
        pass

    def reset(self):
        pass


class PI():
    def __init__(self, feature_order, xi):
        self.feature_order = feature_order
        self.xi = xi

    def act(self, state):
        state = state.numpy()
        pis = self.af(state)
        action = np.random.choice(np.flatnonzero(pis == pis.max()))
        value = 0.0

        action = torch.tensor([action], dtype=torch.int64)
        value = torch.tensor([value])
        return action.squeeze(0), value.squeeze(0)

    def af(self, state):
        mean_idx = self.feature_order.index("posterior_mean")
        means = state[:, mean_idx]
        std_idx = self.feature_order.index("posterior_std")
        stds = state[:, std_idx]
        incumbent_idx = self.feature_order.index("incumbent")
        incumbents = state[:, incumbent_idx]

        mask = stds != 0.0
        pis, zs = np.zeros((means.shape[0],)), np.zeros((means.shape[0],))
        zs[mask] = (means[mask] - (incumbents[mask] + self.xi)) / stds[mask]
        cdf_zs = norm.cdf(zs)
        pis[mask] = cdf_zs
        return pis

    def set_requires_grad(self, flag):
        pass

    def reset(self):
        pass


class TAF():
    # implements the Transfer Acquisition Function from Wistuba et. al., Mach Learn (2018)
    # https://rd.springer.com/content/pdf/10.1007%2Fs10994-017-5684-y.pdf
    def __init__(self, datafile, mode="me", rho=None):
        self.datafile = datafile
        self.models_source = []  # will be filled in self.generate_source_models()
        self.generate_source_models()
        self.mode = mode
        self.rho = rho
        if self.mode == "me":
            assert self.rho is None
        elif self.mode == "ranking":
            assert self.rho > 0
        else:
            raise ValueError("Unknown TAF-mode!")

    def generate_source_models(self):
        with open(self.datafile, "rb") as f:
            data = pkl.load(f)
        self.data = data

        self.D = data["D"]
        self.M = data["M"]
        for i in range(self.M):
            self.models_source.append(self.train_gp(X=data["X"][i], Y=data["Y"][i],
                                                    kernel_lengthscale=data["kernel_lengthscale"][i],
                                                    kernel_variance=data["kernel_variance"][i],
                                                    noise_variance=data["noise_variance"][i],
                                                    use_prior_mean_function=data["use_prior_mean_function"][i]))

    def act(self, state, X_target, model_target):
        state = state.numpy()
        tafs = self.af(state, X_target, model_target)
        action = np.random.choice(np.flatnonzero(tafs == tafs.max()))
        value = 0.0

        action = torch.tensor([action], dtype=torch.int64)
        value = torch.tensor([value])
        return action.squeeze(0), value.squeeze(0)

    def train_gp(self, X, Y, kernel_lengthscale, kernel_variance, noise_variance, use_prior_mean_function):
        kernel = GPy.kern.RBF(input_dim=self.D,
                              variance=kernel_variance,
                              lengthscale=kernel_lengthscale,
                              ARD=True)

        if use_prior_mean_function:
            mf = GPy.core.Mapping(self.D, 1)
            mf.f = lambda X: np.mean(Y, axis=0)[0] if Y is not None else 0.0
            mf.update_gradients = lambda a, b: 0
            mf.gradients_X = lambda a, b: 0
        else:
            mf = None

        normalizer = False

        gp = GPy.models.gp_regression.GPRegression(X, Y,
                                                   noise_var=noise_variance,
                                                   kernel=kernel,
                                                   mean_function=mf,
                                                   normalizer=normalizer)
        gp.Gaussian_noise.variance = noise_variance
        gp.rbf.lengthscale = kernel_lengthscale
        gp.rbf.variance = kernel_variance

        return gp

    def af(self, state, X_target, model_target):
        # gather predictions of target gp
        mean_idx = 0
        means_target = state[:, mean_idx]
        std_idx = 1
        stds_target = state[:, std_idx]
        incumbent_idx = std_idx + self.D + 1
        incumbents_target = state[:, incumbent_idx]

        # gather predicitions of source gps
        xs = state[:, std_idx + 1:std_idx + 1 + self.D]
        means_source, stds_source = [], []
        for i in range(self.M):
            cur_means, cur_vars = self.models_source[i].predict_noiseless(xs)
            cur_stds = np.sqrt(cur_vars)
            means_source.append(cur_means)
            stds_source.append(cur_stds)
        means_source = np.concatenate(means_source, axis=1)
        stds_source = np.concatenate(stds_source, axis=1)

        # compute weights
        if self.mode == "me":  # product of experts
            beta = 1 / (self.M + 1)
            weights = [beta * stds_source[:, i] ** (-2) for i in range(self.M)]
            weights.append(beta * stds_target ** (-2))
            weights = np.array(weights).T
        elif self.mode == "ranking":  # ranking-based
            t = X_target.shape[0] if X_target is not None else 0

            # Epanechnikov quadratic kernel
            def kern(a, b, rho):
                def gamma(x):
                    gamma = 3 / 4 * (1 - x ** 2) if x <= 1 else 0.0
                    return gamma

                kern = gamma(np.linalg.norm(a - b) / rho)
                return kern

            # compute ranking-based meta-features
            chi = [np.zeros((t ** 2,)) for _ in range(self.M + 1)]
            for k in range(self.M + 1):
                for i in range(t):
                    xi = X_target[i, :].reshape(1, self.D)
                    mu_k_i, _ = self.models_source[k].predict_noiseless(xi) if k < self.M \
                        else model_target.predict_noiseless(xi)
                    for j in range(t):
                        xj = X_target[j, :].reshape(1, self.D)
                        mu_k_j, _ = self.models_source[k].predict_noiseless(xj) if k < self.M \
                            else model_target.predict_noiseless(xj)
                        chi[k][j + i * t] = 1 / (t * (t - 1)) if mu_k_i.item() > mu_k_j.item() else 0.0

            # compute weights
            weights = []
            for i in range(self.M + 1):
                weights.append(kern(chi[i], chi[self.M + 1 - 1], self.rho))

            weights = np.array(weights)
            weights = np.tile(weights, (xs.shape[0], 1))

        # compute EI(x) of target model
        mask = stds_target != 0.0
        eis_target, zs = np.zeros((means_target.shape[0],)), np.zeros((means_target.shape[0],))
        zs[mask] = (means_target[mask] - incumbents_target[mask]) / stds_target[mask]
        pdf_zs = norm.pdf(zs)
        cdf_zs = norm.cdf(zs)
        eis_target[mask] = (means_target[mask] - incumbents_target[mask]) * cdf_zs + stds_target[mask] * pdf_zs

        # compute predicted improvements of source models
        incumbents_source = []
        for i in range(self.M):
            if X_target is None:
                cur_incumbent = incumbents_target[0]
            else:
                cur_incumbent = np.max(self.models_source[i].predict_noiseless(X_target)[0])
            incumbents_source.append(cur_incumbent)
        incumbents_source = np.array(incumbents_source)
        Is_source = means_source - incumbents_source
        Is_source[Is_source < 0.0] = 0.0

        # compute TAF
        source_af = np.sum((weights[:, :-1] * Is_source), axis=1)
        target_af = weights[:, -1] * eis_target
        weight_sum = np.sum(weights, axis=1)
        taf = (source_af + target_af) / weight_sum

        return taf

    def set_requires_grad(self, flag):
        pass

    def reset(self):
        pass


class EpsGreedy():
    def __init__(self, datafile, eps, feature_order):
        self.datafile = datafile
        self.eps = eps
        if not isinstance(self.eps, str):
            assert 0.0 <= self.eps <= 1.0
        else:
            assert self.eps == "linear_schedule"
        self.feature_order = feature_order
        self.best_designs = None  # will be filled in self.determine_best_designs()
        self.determine_best_designs()
        self.ei = EI(feature_order=self.feature_order)

    def determine_best_designs(self):
        with open(self.datafile, "rb") as f:
            data = pkl.load(f)
        self.data = data

        self.D = data["D"]
        self.M = data["M"]
        self.best_designs = []
        for i in range(self.M):
            best_value_idx = np.argmax(data["Y"][i], axis=0)
            self.best_designs.append(data["X"][i][best_value_idx])
        self.best_designs = np.array(self.best_designs).squeeze()

    def act(self, state):
        state = state.numpy()
        af = self.af(state)
        action = np.random.choice(np.flatnonzero(af == af.max()))
        value = 0.0

        action = torch.tensor([action], dtype=torch.int64)
        value = torch.tensor([value])
        return action.squeeze(0), value.squeeze(0)

    def af(self, state):
        t_idx = self.feature_order.index("timestep")
        t = int(state[0, t_idx])

        # throw T coins to determine at which steps to be greedy and the corresponding designs to choose
        if self.is_reset:
            self.is_reset = False

            assert t == 0

            T_idx = self.feature_order.index("budget")
            T = int(state[0, T_idx])

            # the af is evaluated at most T+1 times to get its state after T steps also
            self.coins = np.random.rand(T + 1)
            if self.eps != "linear_schedule":
                self.be_greedy = self.coins < self.eps
            else:
                self.eps = np.linspace(1.0, 0.0, T + 1)
                self.be_greedy = [self.coins[i] < self.eps[i] for i in range(T + 1)]
            n_greedy_steps = np.sum(self.be_greedy)
            np.random.shuffle(self.best_designs)
            self.episode_best_designs = np.ones((T + 1, self.D)) * np.nan
            self.episode_best_designs[self.be_greedy, :] = self.best_designs[:n_greedy_steps, :] \
                if n_greedy_steps < self.M else self.best_designs[:, :]

        if not self.be_greedy[t]:
            af = self.ei.af(state)
        else:
            chosen_design = self.episode_best_designs[t]
            assert not np.isnan(chosen_design).any()
            x_idx = self.feature_order.index("x")  # returns index of first occurence
            x = state[:, x_idx:x_idx + self.D]
            # return the negative norm of the vector difference between chosen design and all x values to choose from
            # to make sure that only x-values in the domain can be chosen
            af = -np.linalg.norm(x - chosen_design, axis=1)
        return af

    def set_requires_grad(self, flag):
        pass

    def reset(self):
        self.is_reset = True


class GMM_UCB():
    def __init__(self, datafile, w, n_components, ucb_kappa, feature_order):
        self.datafile = datafile
        self.w = w
        self.n_components = n_components
        if not isinstance(self.w, str):
            assert 0.0 <= self.w <= 1.0
        else:
            assert self.w == "linear_schedule"
        self.feature_order = feature_order
        self.ucb = UCB(feature_order=self.feature_order, kappa=ucb_kappa)
        self.best_designs = None  # will be filled in self.determine_best_designs()
        self.determine_best_designs()
        self.fit_gmm()

    def determine_best_designs(self):
        with open(self.datafile, "rb") as f:
            data = pkl.load(f)
        self.data = data

        self.D = data["D"]
        self.M = data["M"]
        self.best_designs = []
        for i in range(self.M):
            best_value_idx = np.argmax(data["Y"][i], axis=0)
            self.best_designs.append(data["X"][i][best_value_idx])
        self.best_designs = np.array(self.best_designs).squeeze()

    def fit_gmm(self):
        self.gmm = mixture.GaussianMixture(n_components=self.n_components)
        self.gmm.fit(self.best_designs)

    def act(self, state):
        state = state.numpy()
        af = self.af(state)
        action = np.random.choice(np.flatnonzero(af == af.max()))
        value = 0.0

        action = torch.tensor([action], dtype=torch.int64)
        value = torch.tensor([value])
        return action.squeeze(0), value.squeeze(0)

    def af(self, state):
        x_idx = self.feature_order.index("x")
        t_idx = self.feature_order.index("timestep")
        T_idx = self.feature_order.index("budget")
        x = state[:, x_idx:x_idx + self.D]
        t = state[0, t_idx]
        T = state[0, T_idx]
        ucb = self.ucb.af(state)
        gmm = self.gmm.score_samples(x)
        if self.w != "linear_schedule":
            w = self.w
        else:
            w = 1.0 - t / T
        af = w * gmm + (1 - w) * ucb
        return af

    def set_requires_grad(self, flag):
        pass

    def reset(self):
        pass

File Path: metabo/policies/taf/__init__.py
Content:

File Path: metabo/policies/taf/generate_taf_data_branin.py
Content:
import os
import numpy as np
from metabo.environment.util import scale_from_unit_square_to_domain
from metabo.environment.objectives import bra_var
import sobol_seq
import pickle as pkl


def generate_taf_data_branin(M, N):
    # dimension of task
    D = 2

    # generate grid of function parameters for source tasks
    # number of source tasks
    bound_scaling = 0.1
    bound_translation = 0.1
    fct_params_domain = np.array([[-bound_translation, bound_translation],
                                  [-bound_translation, bound_translation],
                                  [1 - bound_scaling, 1 + bound_scaling]])
    fct_params_grid = sobol_seq.i4_sobol_generate(dim_num=3, n=M)  # 2 translations, 1 scaling
    fct_params_grid = scale_from_unit_square_to_domain(X=fct_params_grid, domain=fct_params_domain)

    # generate grid of control parameters
    # number of parameter configurations
    input_grid = sobol_seq.i4_sobol_generate(dim_num=D, n=N)

    # generate data
    branin_kernel_lengthscale = [0.235, 0.578]
    branin_kernel_variance = 2.0
    branin_noise_variance = 8.9e-16
    use_prior_mean_function = False
    data = {"D": D,
            "M": M,
            "X": M * [input_grid],
            "Y": M * [None],  # is computed below
            "kernel_lengthscale": M * [branin_kernel_lengthscale],
            "kernel_variance": M * [branin_kernel_variance],
            "noise_variance": M * [branin_noise_variance],
            "use_prior_mean_function": M * [use_prior_mean_function]}

    for i, fct_params in enumerate(fct_params_grid):
        t = np.array([fct_params[0], fct_params[1]])
        s = fct_params[2]
        fct_eval = bra_var(x=input_grid, t=t, s=s)
        data["Y"][i] = fct_eval

    this_path = os.path.dirname(os.path.realpath(__file__))
    datafile = os.path.join(this_path, "taf_branin_M_{:d}_N_{:d}.pkl".format(M, N))
    with open(datafile, "wb") as f:
        pkl.dump(data, f)

File Path: metabo/policies/taf/generate_taf_data_furuta.py
Content:
import os
import numpy as np
from metabo.environment.util import scale_from_unit_square_to_domain
from metabo.environment.furuta import init_furuta_simulation, furuta_simulation
import sobol_seq
import pickle as pkl


def generate_taf_data_furuta(M):
    # generate grid of physical parameters for source tasks
    # number of source tasks
    true_length_arm = 0.085
    true_length_pendulum = 0.129
    true_mass_arm = 0.095
    true_mass_pendulum = 0.024
    low_mult = 0.75
    high_mult = 1.25
    length_arm_low = low_mult * true_length_arm
    length_arm_high = high_mult * true_length_arm
    length_pendulum_low = low_mult * true_length_pendulum
    length_pendulum_high = high_mult * true_length_pendulum
    mass_arm_low = low_mult * true_mass_arm
    mass_arm_high = high_mult * true_mass_arm
    mass_pendulum_low = low_mult * true_mass_pendulum
    mass_pendulum_high = high_mult * true_mass_pendulum
    physical_params_domain = np.array([[mass_pendulum_low, mass_pendulum_high],
                                       [mass_arm_low, mass_arm_high],
                                       [length_pendulum_low, length_pendulum_high],
                                       [length_arm_low, length_arm_high]])
    physical_params_grid = sobol_seq.i4_sobol_generate(dim_num=4, n=M)
    physical_params_grid = scale_from_unit_square_to_domain(X=physical_params_grid, domain=physical_params_domain)

    # generate grid of control parameters
    # number of control parameter configurations
    N = 200
    control_params_domain = np.array([[-0.5, 0.2],
                                      [-1.6, 4.0],
                                      [-0.1, 0.04],
                                      [-0.04, 0.1]])
    simcore_idx = [0, 1, 2, 3]
    d = 4
    idx = np.array(simcore_idx)
    furuta_pos = np.array([0, 1, 2, 3])
    furuta_pos = furuta_pos[idx[0:d]].tolist()
    control_params_domain_domain = control_params_domain[idx[0:d], :].tolist()
    control_params_grid = sobol_seq.i4_sobol_generate(dim_num=4, n=N)

    # generate data
    furuta_kernel_lengthscale = 0.10 * np.ones((4,))
    furuta_kernel_variance = 1.5
    furuta_noise_variance = 1e-2
    use_prior_mean_function = True
    data = {"D": 4,
            "M": M,
            "X": M * [control_params_grid],
            "Y": M * [None],  # is computed below
            "kernel_lengthscale": M * [furuta_kernel_lengthscale],
            "kernel_variance": M * [furuta_kernel_variance],
            "noise_variance": M * [furuta_noise_variance],
            "use_prior_mean_function": M * [use_prior_mean_function]}

    for i, physical_params in enumerate(physical_params_grid):
        init_tuple = init_furuta_simulation(mass_pendulum=physical_params[0],
                                            mass_arm=physical_params[1],
                                            length_pendulum=physical_params[2],
                                            length_arm=physical_params[3])

        neg_logcosts = -furuta_simulation(init_tuple=init_tuple,
                                          params=scale_from_unit_square_to_domain(X=control_params_grid,
                                                                                  domain=control_params_domain),
                                          D=data["D"],
                                          pos=furuta_pos)
        data["Y"][i] = neg_logcosts[:, None]

    this_path = os.path.dirname(os.path.realpath(__file__))
    datafile = os.path.join(this_path, "taf_furuta_M_{:d}_N_{:d}.pkl".format(M, N))
    with open(datafile, "wb") as f:
        pkl.dump(data, f)

File Path: metabo/policies/taf/generate_taf_data_gprice.py
Content:
import os
import numpy as np
from metabo.environment.util import scale_from_unit_square_to_domain
from metabo.environment.objectives import gprice_var
import sobol_seq
import pickle as pkl


def generate_taf_data_gprice(M, N):
    # dimension of task
    D = 2

    # generate grid of function parameters for source tasks
    # number of source tasks
    bound_scaling = 0.1
    bound_translation = 0.1
    fct_params_domain = np.array([[-bound_translation, bound_translation],
                                  [-bound_translation, bound_translation],
                                  [1 - bound_scaling, 1 + bound_scaling]])
    fct_params_grid = sobol_seq.i4_sobol_generate(dim_num=3, n=M)  # 2 translations, 1 scaling
    fct_params_grid = scale_from_unit_square_to_domain(X=fct_params_grid, domain=fct_params_domain)

    # generate grid of control parameters
    # number of parameter configurations
    input_grid = sobol_seq.i4_sobol_generate(dim_num=D, n=N)

    # generate data
    gprice_kernel_lengthscale = [0.130, 0.07]
    gprice_kernel_variance = 0.616
    gprice_noise_variance = 1e-6
    use_prior_mean_function = False
    data = {"D": D,
            "M": M,
            "X": M * [input_grid],
            "Y": M * [None],  # is computed below
            "kernel_lengthscale": M * [gprice_kernel_lengthscale],
            "kernel_variance": M * [gprice_kernel_variance],
            "noise_variance": M * [gprice_noise_variance],
            "use_prior_mean_function": M * [use_prior_mean_function]}

    for i, fct_params in enumerate(fct_params_grid):
        t = np.array([fct_params[0], fct_params[1]])
        s = fct_params[2]
        fct_eval = gprice_var(x=input_grid, t=t, s=s)
        data["Y"][i] = fct_eval

    this_path = os.path.dirname(os.path.realpath(__file__))
    datafile = os.path.join(this_path, "taf_gprice_M_{:d}_N_{:d}.pkl".format(M, N))
    with open(datafile, "wb") as f:
        pkl.dump(data, f)

File Path: metabo/policies/taf/generate_taf_data_hm3.py
Content:
import os
import numpy as np
from metabo.environment.util import scale_from_unit_square_to_domain
from metabo.environment.objectives import hm3_var
import sobol_seq
import pickle as pkl


def generate_taf_data_hm3(M, N):
    # dimension of task
    D = 3

    # generate grid of function parameters for source tasks
    # number of source tasks
    bound_scaling = 0.1
    bound_translation = 0.1
    fct_params_domain = np.array([[-bound_translation, bound_translation],
                                  [-bound_translation, bound_translation],
                                  [-bound_translation, bound_translation],
                                  [1 - bound_scaling, 1 + bound_scaling]])
    fct_params_grid = sobol_seq.i4_sobol_generate(dim_num=4, n=M)  # 3 translations, 1 scaling
    fct_params_grid = scale_from_unit_square_to_domain(X=fct_params_grid, domain=fct_params_domain)

    # generate grid of control parameters
    # number of parameter configurations
    input_grid = sobol_seq.i4_sobol_generate(dim_num=D, n=N)

    # generate data
    hm3_kernel_lengthscale = np.array([0.716, 0.298, 0.186])
    hm3_kernel_variance = 0.83
    hm3_noise_variance = 1.688e-11
    use_prior_mean_function = False
    data = {"D": D,
            "M": M,
            "X": M * [input_grid],
            "Y": M * [None],  # is computed below
            "kernel_lengthscale": M * [hm3_kernel_lengthscale],
            "kernel_variance": M * [hm3_kernel_variance],
            "noise_variance": M * [hm3_noise_variance],
            "use_prior_mean_function": M * [use_prior_mean_function]}

    for i, fct_params in enumerate(fct_params_grid):
        t = np.array([fct_params[0], fct_params[1], fct_params[2]])
        s = fct_params[3]
        fct_eval = hm3_var(x=input_grid, t=t, s=s)
        data["Y"][i] = fct_eval

    this_path = os.path.dirname(os.path.realpath(__file__))
    datafile = os.path.join(this_path, "taf_hm3_M_{:d}_N_{:d}.pkl".format(M, N))
    with open(datafile, "wb") as f:
        pkl.dump(data, f)

File Path: metabo/policies/taf/generate_taf_data_hpo.py
Content:
from metabo.environment.objectives import hpo
import pickle as pkl
import os
import json


# choose model
def generate_taf_data_hpo(model, datapath):
    # dimension of task
    D = 2

    # load training datasets
    trainsets_file = os.path.join(datapath, model, "train_datasets_iclr2020.txt")
    with open(trainsets_file, "r") as f:
        trainsets = json.load(f)
    M = len(trainsets)

    # load training data
    data_file = os.path.join(datapath, model, "objectives.pkl")
    with open(data_file, "rb") as f:
        hpo_data = pkl.load(f)

    # load gp hyperparameters
    gp_hyperparameters_file = os.path.join(datapath, model, "gp_hyperparameters.pkl")
    with open(gp_hyperparameters_file, "rb") as f:
        gp_hyperparameters = pkl.load(f)

    # load input grid
    Xs = []
    Ys = []
    kernel_lengthscales = []
    kernel_variances = []
    noise_variances = []
    use_prior_mean_function = True
    N = None
    for dataset in trainsets:
        Xs.append(hpo_data[dataset]["X"])
        if N is None:
            N = Xs[-1].shape[0]
        else:
            assert Xs[-1].shape[0] == N  # all X shall contain the same number of points
        Ys.append(hpo(Xs[-1], data=hpo_data, dataset=dataset))
        kernel_lengthscales.append(gp_hyperparameters[dataset]["lengthscale"])
        kernel_variances.append(gp_hyperparameters[dataset]["variance"])
        noise_variances.append(gp_hyperparameters[dataset]["noise_variance"])

    data = {"D": D,
            "M": M,
            "X": Xs,
            "Y": Ys,
            "kernel_lengthscale": kernel_lengthscales,
            "kernel_variance": kernel_variances,
            "noise_variance": noise_variances,
            "use_prior_mean_function": M * [use_prior_mean_function]}

    this_path = os.path.dirname(os.path.realpath(__file__))
    datafile = os.path.join(this_path, "taf_{}_M_{:d}_N_{:d}.pkl".format(model, M, N))
    with open(datafile, "wb") as f:
        pkl.dump(data, f)

File Path: metabo/ppo/__init__.py
Content:

File Path: metabo/ppo/batchrecorder.py
Content:
# Copyright (c) 2019 Robert Bosch GmbH
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>.

# ******************************************************************
# batchrecorder.py
# Implementation of the batchrecorder used in ppo.py
# The Batchrecorder spawns n_workers parallel CPU worker processes (EnvRunner) to step the environments.
# ******************************************************************

import os

os.environ["OMP_NUM_THREADS"] = "1"  # on some machines this is needed to restrict torch to one core

from namedlist import namedlist
import random
import gym
import numpy as np
import multiprocessing as mp
import torch
import itertools
import time

Transition = namedlist("Transition", ["state", "action", "reward", "value", "new", "tdlamret", "adv"])


class EnvRunner(mp.Process):
    def __init__(self, worker_id, size, env_id, seed, policy_fn, task_queue, res_queue, deterministic=False):
        mp.Process.__init__(self)
        self.worker_id = worker_id
        self.env = gym.make(env_id)
        self.seed = seed
        self.task_queue = task_queue
        self.res_queue = res_queue

        # policy
        self.pi = policy_fn(self.env.observation_space, self.env.action_space, deterministic)
        self.pi.set_requires_grad(False)  # we need no gradients here

        # connect policy and environment
        self.env.unwrapped.set_af_functions(af_fun=self.pi.af)

        # empty batch recorder
        assert size > 0
        self.size = size
        self.clear()

        self.set_all_seeds()

    def clear(self):
        self.memory = []
        self.cur_size = self.size
        self.reward_sum = 0
        self.n_new = 0
        self.initial_rewards = []
        self.terminal_rewards = []
        self.next_new = None
        self.next_state = None
        self.next_value = None

    def set_all_seeds(self):
        self.env.seed(self.seed)
        # these seeds are PROCESS-local
        np.random.seed(self.seed)
        random.seed(self.seed)
        torch.manual_seed(self.seed)
        torch.cuda.manual_seed_all(self.seed)

    def push(self, state, action, reward, value, new):
        assert not self.is_full()
        assert self.env.observation_space.contains(state)
        assert self.env.action_space.contains(action)
        self.memory.append(Transition(state, action, reward, value, new, None, None))
        self.reward_sum += reward
        self.n_new += int(new)

    def record_batch(self, gamma, lam):
        if self.next_state is None:
            self.pi.reset()
            state = self.env.reset()
            new = 1
        else:
            state = self.next_state
            new = self.next_new
        self.clear()

        while not self.is_full():
            action, value = self.act(state)
            next_state, reward, done, _ = self.env.step(action)
            self.push(state, action, reward, value, new)

            if done:
                self.pi.reset()
                state = self.env.reset()
                new = 1
            else:
                state = next_state
                new = 0

        self.next_new = new
        self.next_state = state
        self.next_value = value

        self.add_tdlamret_and_adv(gamma=gamma, lam=lam)

    def add_tdlamret_and_adv(self, gamma, lam):
        assert self.is_full()
        self.initial_rewards = []  # extraction of initial rewards can happen here w/o overhead
        self.terminal_rewards = []  # extraction of terminal rewards can happen here w/o overhead
        next_new = self.next_new
        next_value = self.next_value
        next_adv = 0
        for i in reversed(range(len(self))):
            nonterminal = 1 - next_new
            value = self.memory[i].value
            reward = self.memory[i].reward
            if self.memory[i].new:
                self.initial_rewards.append(reward)
            if not nonterminal:
                self.terminal_rewards.append(reward)

            delta = -value + reward + gamma * nonterminal * next_value
            self.memory[i].adv = next_adv = delta + lam * gamma * nonterminal * next_adv
            self.memory[i].tdlamret = self.memory[i].adv + value
            next_new = self.memory[i].new
            next_value = value

    def is_full(self):
        return len(self) == self.cur_size

    def is_empty(self):
        return len(self) == 0

    def strip_to_monte_carlo(self):
        assert self.is_full()
        last_new_pos = None
        for i in reversed(range(len(self))):
            if self.memory[i].new == 1:
                last_new_pos = i
                break
            else:
                self.reward_sum -= self.memory[i].reward
        self.next_state = self.memory[last_new_pos].state
        self.next_new = self.memory[last_new_pos].new
        assert self.next_new == 1
        self.next_value = self.memory[last_new_pos].value
        self.memory = self.memory[:last_new_pos]
        self.cur_size = len(self)

    def act(self, state):
        torch.set_num_threads(1)
        with torch.no_grad():
            # to sample the action, the policy uses the current PROCESS-local random seed, don't re-seed in pi.act
            if not self.env.unwrapped.pass_X_to_pi:
                action, value = self.pi.act(torch.from_numpy(state.astype(np.float32)))
            else:
                action, value = self.pi.act(torch.from_numpy(state.astype(np.float32)),
                                            self.env.unwrapped.X,
                                            self.env.unwrapped.gp)
        action = action.numpy()
        value = value.numpy()

        return action, value

    def update_weights(self, pi_state_dict):
        self.pi.load_state_dict(pi_state_dict)

    def __len__(self):
        return len(self.memory)

    def run(self):
        while True:
            task = self.task_queue.get(block=True)
            if task["desc"] == "record_batch":
                self.record_batch(gamma=task["gamma"],
                                  lam=task["lambda"])
                self.res_queue.put((self.worker_id, self.memory, self.reward_sum, self.n_new, self.initial_rewards,
                                    self.terminal_rewards))
                self.task_queue.task_done()
            elif task["desc"] == "set_pi_weights":
                self.update_weights(task["pi_state_dict"])
                self.task_queue.task_done()
            elif task["desc"] == "cleanup":
                self.env.close()
                self.task_queue.task_done()


class BatchRecorder():
    def __init__(self, size, env_id, env_seeds, policy_fn, n_workers, deterministic=False):
        self.env_id = env_id
        self.deterministic = deterministic

        # empty batch recorder
        assert size > 0
        self.n_workers = n_workers
        self.size = size
        self.clear()

        # parallelization
        assert len(env_seeds) == n_workers
        self.env_seeds = env_seeds
        self.task_queue = mp.JoinableQueue()
        self.res_queue = mp.Queue()
        self.worker_batch_sizes = [self.size // self.n_workers] * self.n_workers
        delta_size = self.size - sum(self.worker_batch_sizes)
        assert delta_size == 0, 'All workers shall get assigned the same batch size!'
        self.workers = []
        for i in range(self.n_workers):
            self.workers.append(
                EnvRunner(worker_id=i, size=self.worker_batch_sizes[i], env_id=self.env_id, seed=self.env_seeds[i],
                          policy_fn=policy_fn, task_queue=self.task_queue, res_queue=self.res_queue,
                          deterministic=self.deterministic))
        for i, worker in enumerate(self.workers):
            worker.start()

    def clear(self):
        self.cur_size = self.size
        self.worker_sizes = [0 for _ in range(self.n_workers)]
        self.memory = []
        self.worker_memories = [[] for _ in range(self.n_workers)]
        self.reward_sum = 0
        self.worker_reward_sums = [0 for _ in range(self.n_workers)]
        self.n_new = 0
        self.worker_n_news = [0 for _ in range(self.n_workers)]
        self.initial_rewards = []
        self.worker_initial_rewards = [[] for _ in range(self.n_workers)]
        self.terminal_rewards = []
        self.worker_terminal_rewards = [[] for _ in range(self.n_workers)]

    def overview_dict(self):
        d = {"size": self.size,
             "n_workers": self.n_workers,
             "worker_batch_sizes": self.worker_batch_sizes,
             "env_seeds": self.env_seeds,
             "deterministic": self.deterministic}

        return d

    def record_batch(self, gamma, lam):
        now = time.time()
        task = dict([("desc", "record_batch"),
                     ("gamma", gamma),
                     ("lambda", lam)])
        for _ in range(self.n_workers):
            self.task_queue.put(task)

        self.clear()
        res_count = 0
        while res_count < self.n_workers:
            res_count += 1
            worker_id, cur_memory, cur_rew_sum, cur_n_new, cur_initial_reward, cur_terminal_reward = self.res_queue.get()
            self.worker_memories[worker_id] += cur_memory.copy()
            self.worker_sizes[worker_id] += len(cur_memory)
            self.worker_reward_sums[worker_id] += cur_rew_sum
            self.worker_n_news[worker_id] += cur_n_new
            self.worker_initial_rewards[worker_id] += cur_initial_reward
            self.worker_terminal_rewards[worker_id] += cur_terminal_reward

        self.task_queue.join()

        self.memory = list(itertools.chain.from_iterable([self.worker_memories[i] for i in range(self.n_workers)]))
        self.initial_rewards = list(itertools.chain.from_iterable(self.worker_initial_rewards))
        self.terminal_rewards = list(itertools.chain.from_iterable(self.worker_terminal_rewards))

        assert self.is_full()

        self.reward_sum = sum(self.worker_reward_sums)
        self.n_new = sum(self.worker_n_news)

        return time.time() - now

    def set_worker_weights(self, pi):
        now = time.time()
        pi.to("cpu")
        task = dict([("desc", "set_pi_weights"),
                     ("pi_state_dict", pi.state_dict())])

        for _ in self.workers:
            self.task_queue.put(task)
        self.task_queue.join()

        return time.time() - now

    def cleanup(self):
        for _ in range(self.n_workers):
            self.task_queue.put(dict([("desc", "cleanup")]))
        for worker in self.workers:
            worker.terminate()

    def sample(self, batch_size):
        return random.sample(self.memory, batch_size)

    def get_batch_stats(self):
        assert self.is_full()
        batch_stats = dict()
        batch_stats["size"] = len(self)
        batch_stats["worker_sizes"] = self.worker_sizes
        batch_stats["avg_step_reward"] = self.reward_sum / len(self)
        batch_stats["avg_initial_reward"] = np.mean(self.initial_rewards)
        batch_stats["avg_terminal_reward"] = np.mean(self.terminal_rewards)
        batch_stats["avg_ep_reward"] = self.reward_sum / self.n_new
        batch_stats["avg_ep_len"] = len(self) / self.n_new
        batch_stats["n_new"] = self.n_new
        batch_stats["worker_n_news"] = self.worker_n_news
        return batch_stats

    def iterate(self, minibatch_size, shuffle):
        assert self.is_full()
        pos = 0
        idx = list(range(len(self)))
        if shuffle:
            # we use the random state of the main process here, NO re-seeding
            random.shuffle(idx)
        while pos < len(self):
            if pos + 2 * minibatch_size > len(self):
                # enlarge the last minibatch s.t. all minibatches are at least of size minibatch_size
                cur_minibatch_size = len(self) - pos
            else:
                cur_minibatch_size = minibatch_size
            cur_idx = idx[pos:pos + cur_minibatch_size]
            yield [self.memory[i] for i in cur_idx]
            pos += cur_minibatch_size

    def is_empty(self):
        return len(self) == 0

    def is_full(self):
        return len(self) == self.cur_size

    def __len__(self):
        return len(self.memory)

File Path: metabo/ppo/plot_learning_curve_online.py
Content:
# Copyright (c) 2019 Robert Bosch GmbH
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>.

# ******************************************************************
# evaluate_metabo_adaboost.py
# Function to plot learning curves online from PPO-log output.
# ******************************************************************

from matplotlib import pyplot as plt
import numpy as np
import pickle as pkl
import os
import time
from metabo.ppo.util import plot_learning_curve
import datetime


def plot_learning_curve_online(logpath, reload=True):
    plt.rc('text', usetex=False)
    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(13, 5))

    while True:
        time.sleep(.1)

        files = os.listdir(os.path.join(logpath))
        last_iter = -np.inf
        for file in files:
            if file.startswith("stats_"):
                pos = file.find("_")
                iter = int(file[pos + 1:])
                if iter > last_iter:
                    last_iter = iter
        if last_iter != -np.inf:
            with open(os.path.join(logpath, "params_{:d}".format(int(last_iter))), "rb") as f:
                params = pkl.load(f)
            with open(os.path.join(logpath, "stats_{:d}".format(int(last_iter))), "rb") as f:
                stats = pkl.load(f)
        else:
            continue

        n_steps_to_go = 0
        avg_step_rew = np.array(stats["avg_step_rews"])
        avg_init_rew = np.array(stats["avg_init_rews"])
        avg_term_rew = np.array(stats["avg_term_rews"])
        perc = stats["perc"]
        n_steps_to_go += (1 - stats["perc"] / 100) * params["max_steps"]
        if stats["perc"] < 100:
            sps = stats["batch_stats"]["sps"]
            n_workers = params["n_workers"]
        else:
            sps = 0
            n_workers = 0

        x = np.arange(avg_step_rew.size)

        eta_sec = n_steps_to_go // sps if sps != 0 else (0 if perc >= 100 else None)
        eta = datetime.timedelta(seconds=eta_sec)
        fig.suptitle(
            "Env: {}, Date: {}\n n_workers_running: {}, {:.0f}sps, topt: {:.0f}s, {:.2f}%, ETA = {}".format(
                params["env_id"], os.path.basename(logpath), n_workers, sps, stats["t_optim"], perc, eta))

        ax.cla()
        ax.plot(x, avg_step_rew, color="r", alpha=0.5, label="avg. step reward")
        ax.plot(x, avg_init_rew, color="g", alpha=0.5, label="avg. initial reward")
        ax.plot(x, avg_term_rew, color="b", alpha=0.5, label="avg. final reward")
        plot_learning_curve(ax, avg_step_rew, color="r", smoothing_range=25, plot_only_smoothed=True)
        plot_learning_curve(ax, avg_init_rew, color="g", smoothing_range=25, plot_only_smoothed=True)
        plot_learning_curve(ax, avg_term_rew, color="b", smoothing_range=25, plot_only_smoothed=True)
        ax.set_xlabel("Iteration")
        ax.grid()
        ax.legend()
        fig.savefig(os.path.join(logpath, "learning_curves.png"))

        if not reload:
            break

        time.sleep(5)

File Path: metabo/ppo/ppo.py
Content:
# Copyright (c) 2019 Robert Bosch GmbH
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>.

# ******************************************************************
# ppo.py
# Implementation of Proximal Policy Optimization as proposed in Schulman et al., https://arxiv.org/abs/1707.06347
# ******************************************************************

import os

os.environ["OMP_NUM_THREADS"] = "1"  # on some machines this is needed to restrict torch to one core

import random
import torch
import torch.optim
import time
import numpy as np
import gym
import os
import pickle as pkl
import copy
import json
from datetime import datetime
import collections
from metabo.ppo.batchrecorder import BatchRecorder, Transition
from metabo.ppo.util import get_best_iter_idx
from metabo.policies.policies import NeuralAF


class PPO:
    def __init__(self, policy_fn, params, logpath, save_interval, verbose=False):
        self.params = params

        # set up the environment (only for reading out observation and action spaces)
        self.env = gym.make(self.params["env_id"])
        self.set_all_seeds()

        # logging
        self.logpath = logpath
        try:
            os.makedirs(logpath, exist_ok=False)
        except FileExistsError:
            raise ValueError("Logpath is not empty!")
        self.save_interval = save_interval
        self.verbose = verbose

        # policies, optimizer
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.pi = policy_fn(self.env.observation_space, self.env.action_space,
                            deterministic=False).to(self.device)
        self.old_pi = policy_fn(self.env.observation_space, self.env.action_space,
                                deterministic=False).to(self.device)
        self.optimizer = torch.optim.Adam(self.pi.parameters(), lr=self.params["lr"])

        # set up the batch recorder
        self.batch_recorder = BatchRecorder(size=self.params["batch_size"],
                                            env_id=self.params["env_id"],
                                            env_seeds=self.params["env_seeds"],
                                            policy_fn=policy_fn,
                                            n_workers=self.params["n_workers"])

        self.stats = dict()
        self.stats["n_timesteps"] = 0
        self.stats["n_optsteps"] = 0
        self.stats["n_iters"] = 0
        self.stats["t_train"] = 0
        self.stats["avg_step_rews"] = np.array([])
        self.stats["avg_init_rews"] = np.array([])
        self.stats["avg_term_rews"] = np.array([])
        self.stats["avg_ep_rews"] = np.array([])

        self.t_batch = None

        self.rew_buffer = collections.deque(maxlen=50)

        self.write_overview_logfile()

    def set_all_seeds(self):
        np.random.seed(self.params["seed"])
        random.seed(self.params["seed"])
        torch.manual_seed(self.params["seed"])
        torch.cuda.manual_seed_all(self.params["seed"])

    def write_overview_logfile(self):
        s = ""
        s += "********* OVERVIEW OF RUN *********\n"
        s += "Logpath        : {}\n".format(self.logpath)
        s += "Logfile created: {}\n".format(datetime.strftime(datetime.now(), "%Y-%m-%d-%H-%M-%S"))
        s += "Environment-ID:  {}\n".format(self.params["env_id"])
        s += "Environment-kwargs:\n"
        s += json.dumps(self.env.unwrapped.kwargs, indent=2)
        s += "\n"
        s += "PPO-parameters:\n"
        s += json.dumps(self.params, indent=2)
        s += "\n"
        s += "Batchrecorder:\n"
        s += json.dumps(self.batch_recorder.overview_dict(), indent=2)
        fname = "000_overview.txt"
        with open(os.path.join(self.logpath, fname), "w") as f:
            print(s, file=f)
        if not self.verbose:
            print(s)

    def optimize_on_batch(self):
        now = time.time()

        self.iter_log_str += " OPTIMIZING...\n"
        self.old_pi.load_state_dict(self.pi.state_dict())

        for ep in range(self.params["n_epochs"]):
            loss_ppo_ep = 0
            loss_value_ep = 0
            loss_ent_ep = 0
            loss_ep = 0

            n_minibatches = 0
            for minibatch in self.batch_recorder.iterate(self.params["minibatch_size"], shuffle=True):
                transitions = Transition(*zip(*minibatch))
                states = torch.from_numpy(np.stack(transitions.state).astype(np.float32)).to(self.device)
                actions = torch.from_numpy(np.stack(transitions.action).astype(np.float32)).to(self.device)
                tdlamrets = torch.from_numpy(np.array(transitions.tdlamret).astype(np.float32)).to(self.device)

                if self.params["loss_type"] == "GAElam":
                    advs = torch.from_numpy(np.array(transitions.adv).astype(np.float32)).to(self.device)
                else:
                    advs = torch.from_numpy(np.array(transitions.tdlamret).astype(np.float32)).to(self.device)

                # normalize advantages
                if self.params["normalize_advs"]:
                    advs_std = torch.std(advs, unbiased=False)
                    if not advs_std == 0 and not torch.isnan(advs_std):
                        advs = (advs - torch.mean(advs)) / advs_std

                # compute values and entropies at current theta, and logprobs at current and old theta
                with torch.no_grad():
                    _, logprobs_old, _ = self.old_pi.predict_vals_logps_ents(states=states, actions=actions)
                vpreds, logprobs, entropies = self.pi.predict_vals_logps_ents(states=states, actions=actions)
                assert logprobs_old.dim() == vpreds.dim() == logprobs.dim() == entropies.dim() == 1

                # ppo-loss
                ratios = torch.exp(logprobs - logprobs_old)
                clipped_ratios = ratios.clamp(1 - self.params["epsilon"], 1 + self.params["epsilon"])
                advs = advs.squeeze()
                loss_cpi = ratios * advs
                assert loss_cpi.dim() == 1
                loss_clipped = clipped_ratios * advs
                assert loss_clipped.dim() == 1
                loss_ppo = -torch.mean(torch.min(loss_cpi, loss_clipped))

                # value-function loss
                loss_value = torch.mean((vpreds - tdlamrets) ** 2)

                # entropy loss
                loss_ent = -torch.mean(entropies)

                loss = loss_ppo + self.params["value_coeff"] * loss_value + self.params["ent_coeff"] * loss_ent

                with torch.no_grad():
                    loss_ppo_ep += loss_ppo
                    loss_value_ep += loss_value
                    loss_ent_ep += loss_ent
                    loss_ep += loss

                self.optimizer.zero_grad()
                loss.backward()
                self.optimizer.step()
                self.stats["n_optsteps"] += 1

                n_minibatches += 1

            loss_ppo_avg = loss_ppo_ep / n_minibatches
            loss_value_avg = loss_value_ep / n_minibatches
            loss_ent_avg = loss_ent_ep / n_minibatches
            loss_avg = loss_ep / n_minibatches
            self.iter_log_str += "   loss_ppo = {: .4g}, loss_value = {: .4g}, loss_ent = {: .4g}, loss = {: .4g}\n".format(
                loss_ppo_avg, loss_value_avg, loss_ent_avg, loss_avg)
        t_optim = time.time() - now
        self.iter_log_str += "  Took {:.2f}s".format(t_optim)

        return t_optim

    def train(self):
        while self.stats["n_timesteps"] < self.params["max_steps"]:
            self.iter_log_str = ""

            t_weights = self.batch_recorder.set_worker_weights(copy.deepcopy(self.pi))
            self.stats["t_train"] += t_weights

            t_batch = self.batch_recorder.record_batch(gamma=self.params["gamma"],
                                                       lam=self.params["lambda"])
            self.stats["t_train"] += t_batch
            batch_stats = self.batch_recorder.get_batch_stats()
            self.stats["n_timesteps"] += len(self.batch_recorder)
            self.stats["batch_stats"] = batch_stats
            self.stats["avg_step_rews"] = np.append(self.stats["avg_step_rews"], batch_stats["avg_step_reward"])
            self.stats["avg_init_rews"] = np.append(self.stats["avg_init_rews"], batch_stats["avg_initial_reward"])
            self.stats["avg_term_rews"] = np.append(self.stats["avg_term_rews"], batch_stats["avg_terminal_reward"])
            self.stats["avg_ep_rews"] = np.append(self.stats["avg_ep_rews"], batch_stats["avg_ep_reward"])
            self.stats["perc"] = 100 * self.stats["n_timesteps"] / self.params["max_steps"]
            batch_stats["t_batch"] = t_batch
            batch_stats["sps"] = batch_stats["size"] / batch_stats["t_batch"]
            self.add_iter_log_str(batch_stats=batch_stats)

            if self.stats["n_iters"] % self.save_interval == 0 or \
                    self.stats["n_iters"] == 0 or \
                    self.stats["n_timesteps"] >= self.params["max_steps"]:
                self.store_weights()

            t_optim = self.optimize_on_batch()
            self.stats["t_train"] += t_optim
            self.stats["t_optim"] = t_optim
            self.store_log()

            self.stats["n_iters"] += 1
        self.batch_recorder.cleanup()

    def store_log(self):
        with open(os.path.join(self.logpath, "log"), "a") as f:
            print(self.iter_log_str, file=f)
        if not self.verbose:
            print(self.iter_log_str)
        with open(os.path.join(self.logpath, "stats_" + str(self.stats["n_iters"])), "wb") as f:
            pkl.dump(self.stats, f)
        with open(os.path.join(self.logpath, "params_" + str(self.stats["n_iters"])), "wb") as f:
            pkl.dump(self.params, f)

    def store_weights(self):
        with open(os.path.join(self.logpath, "weights_" + str(self.stats["n_iters"])), "wb") as f:
            torch.save(self.pi.state_dict(), f)

    def add_iter_log_str(self, batch_stats):
        self.iter_log_str += "\n******************** ITERATION {:2d} ********************\n".format(
            self.stats["n_iters"])
        self.iter_log_str += " RUN STATISTICS (BEFORE OPTIMIZATION):\n"
        self.iter_log_str += "   environment          = {}\n".format(self.params["env_id"])
        self.iter_log_str += "   n_timesteps  = {:d} ({:.2f}%)\n".format(self.stats["n_timesteps"], self.stats["perc"])
        self.iter_log_str += "   n_optsteps   = {:d}\n".format(self.stats["n_optsteps"])
        self.iter_log_str += "   t_total      = {:.2f}s\n".format(self.stats["t_train"])
        self.iter_log_str += " BATCH STATISTICS (BEFORE OPTIMIZATION):\n"
        self.iter_log_str += "   n_workers    = {:d}\n".format(self.batch_recorder.n_workers)
        self.iter_log_str += "   worker_seeds = {}\n".format(self.batch_recorder.env_seeds)
        self.iter_log_str += "   size         = {:d}\n".format(batch_stats["size"])
        self.iter_log_str += "    per_worker  = {:}\n".format(batch_stats["worker_sizes"])
        self.iter_log_str += "   avg_step_rew = {:.4g}\n".format(batch_stats["avg_step_reward"])
        self.iter_log_str += "   avg_init_rew = {:.4g}\n".format(batch_stats["avg_initial_reward"])
        self.iter_log_str += "   avg_term_rew = {:.4g}\n".format(batch_stats["avg_terminal_reward"])
        self.iter_log_str += "   avg_ep_rew   = {:.4g}\n".format(batch_stats["avg_ep_reward"])
        self.iter_log_str += "   n_new        = {:d}\n".format(batch_stats["n_new"])
        self.iter_log_str += "    per_worker  = {:}\n".format(batch_stats["worker_n_news"])
        self.iter_log_str += "   avg_ep_len   = {:.2f}\n".format(batch_stats["avg_ep_len"])
        self.iter_log_str += "   t_batch      = {:.2f}s ({:.0f}sps)\n".format(batch_stats["t_batch"],
                                                                              batch_stats["sps"])

File Path: metabo/ppo/util.py
Content:
# Copyright (c) 2019 Robert Bosch GmbH
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>.

# ******************************************************************
# util.py
# Utilities for ppo package.
# ******************************************************************

import numpy as np
import os
import pickle as pkl


def plot_learning_curve(ax, data, smoothing_range, color, plot_only_smoothed=False):
    if not isinstance(data, np.ndarray):
        data = np.array(data)
    data_smooth = np.array([])
    for i in range(len(data)):
        if i >= smoothing_range:
            data_smooth = np.append(data_smooth,
                                    np.mean(data[i - smoothing_range:i + 1]))
        else:
            data_smooth = np.append(data_smooth, None)

    assert data.size == data_smooth.size
    if not plot_only_smoothed:
        ax.plot(data)
    ax.plot(data_smooth, color=color, lw=4)


def get_last_iter_idx(logpath):
    # returns the index of the last iteration stored in logpath
    files = os.listdir(logpath)
    last_iter = -np.inf
    for file in files:
        if file.startswith("stats_"):
            pos = file.find("_")
            iter = int(file[pos + 1:])
            if iter > last_iter:
                last_iter = iter

    return last_iter


def get_best_iter_idx(logpath):
    # returns the index of the best iteration (w.r.t. average step reward) stored in logpath
    last_iter = get_last_iter_idx(logpath)
    best_iter = -np.inf
    best_avg_step_rew = -np.inf
    with open(os.path.join(logpath, "stats_{:d}".format(last_iter)), "rb") as f:
        stats = pkl.load(f)
    for iter, avg_step_rew in enumerate(stats["avg_step_rews"]):
        if avg_step_rew > best_avg_step_rew:
            best_avg_step_rew = avg_step_rew
            best_iter = iter

    # consistency check
    with open(os.path.join(logpath, "stats_{:d}".format(best_iter)), "rb") as f:
        best_stats = pkl.load(f)
    assert best_stats["batch_stats"]["avg_step_reward"] == best_avg_step_rew

    return best_iter

File Path: train_metabo_adaboost.py
Content:
# Copyright (c) 2019 Robert Bosch GmbH
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>.

# ******************************************************************
# train_metabo_adaboost.py
# Train MetaBO on ADABOOST-hyperparameter optimization
# The weights, stats, logs, and the learning curve are stored in metabo/log and can
# be evaluated using metabo/eval/evaluate.py
# ******************************************************************

# Note: due to licensing issues, the datasets used in this experiment cannot be shipped with the MetaBO package.
# However, you can download the datasets yourself from https://github.com/nicoschilling/ECML2016
# Put the folder "data/adaboost" from this repository into metabo/environment/hpo/data

import os
import multiprocessing as mp
from datetime import datetime
from metabo.policies.policies import NeuralAF
from metabo.ppo.ppo import PPO
from metabo.ppo.plot_learning_curve_online import plot_learning_curve_online
from gym.envs.registration import register

rootdir = os.path.join(os.path.dirname(os.path.realpath(__file__)), "metabo")

# specifiy environment
env_spec = {
    "env_id": "MetaBO-ADABOOST-v0",
    "D": 2,
    "f_type": "HPO",
    "f_opts": {
        "hpo_data_file": os.path.join(rootdir, "environment", "hpo", "processed", "adaboost", "objectives.pkl"),
        "hpo_gp_hyperparameters_file": os.path.join(rootdir, "environment", "hpo", "processed", "adaboost",
                                                    "gp_hyperparameters.pkl"),
        "hpo_datasets_file": os.path.join(rootdir, "environment", "hpo", "processed", "adaboost",
                                          "train_datasets_iclr2020.txt"),
        "draw_random_datasets": True,  # present each test function once
        # to make logarithmic regret well-defined (applied only during training)
        "min_regret": 1e-5},
    "features": ["posterior_mean", "posterior_std", "timestep", "budget", "x"],
    "T": 15,
    "n_init_samples": 0,
    "pass_X_to_pi": False,
    # GP hyperparameters will be set individually for each new function, the parameters were determined off-line
    # via type-2-ML on all available data
    "kernel_lengthscale": None,
    "kernel_variance": None,
    "noise_variance": None,
    "use_prior_mean_function": True,
    "local_af_opt": False,  # discrete domain
    "cardinality_domain": 108,
    "reward_transformation": "neg_log10"
}

# specify PPO parameters
n_iterations = 2000
batch_size = 1200
n_workers = 10
arch_spec = 4 * [200]
ppo_spec = {
    "batch_size": batch_size,
    "max_steps": n_iterations * batch_size,
    "minibatch_size": batch_size // 20,
    "n_epochs": 4,
    "lr": 1e-4,
    "epsilon": 0.15,
    "value_coeff": 1.0,
    "ent_coeff": 0.01,
    "gamma": 0.98,
    "lambda": 0.98,
    "loss_type": "GAElam",
    "normalize_advs": True,
    "n_workers": n_workers,
    "env_id": env_spec["env_id"],
    "seed": 0,
    "env_seeds": list(range(n_workers)),
    "policy_options": {
        "activations": "relu",
        "arch_spec": arch_spec,
        "use_value_network": True,
        "t_idx": -2,
        "T_idx": -1,
        "arch_spec_value": arch_spec
    }
}

# register environment
register(
    id=env_spec["env_id"],
    entry_point="metabo.environment.metabo_gym:MetaBO",
    max_episode_steps=env_spec["T"],
    reward_threshold=None,
    kwargs=env_spec
)

# log data and weights go here, use this folder for evaluation afterwards
logpath = os.path.join(rootdir, "log", env_spec["env_id"], datetime.strftime(datetime.now(), "%Y-%m-%d-%H-%M-%S"))

# set up policy
policy_fn = lambda observation_space, action_space, deterministic: NeuralAF(observation_space=observation_space,
                                                                            action_space=action_space,
                                                                            deterministic=deterministic,
                                                                            options=ppo_spec["policy_options"])

# do training
print("Training on {}.\nFind logs, weights, and learning curve at {}\n\n".format(env_spec["env_id"], logpath))
ppo = PPO(policy_fn=policy_fn, params=ppo_spec, logpath=logpath, save_interval=1)
# learning curve is plotted online in separate process
p = mp.Process(target=plot_learning_curve_online, kwargs={"logpath": logpath, "reload": True})
p.start()
ppo.train()
p.terminate()
plot_learning_curve_online(logpath=logpath, reload=False)

File Path: train_metabo_branin.py
Content:
# Copyright (c) 2019 Robert Bosch GmbH
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>.

# ******************************************************************
# train_metabo_branin.py
# Train MetaBO on Branin-Function
# The weights, stats, logs, and the learning curve are stored in metabo/log and can
# be evaluated using metabo/eval/evaluate.py
# ******************************************************************

import os
import multiprocessing as mp
from datetime import datetime
from metabo.policies.policies import NeuralAF
from metabo.ppo.ppo import PPO
from metabo.ppo.plot_learning_curve_online import plot_learning_curve_online
from gym.envs.registration import register

rootdir = os.path.join(os.path.dirname(os.path.realpath(__file__)), "metabo")

# specifiy environment
env_spec = {
    "env_id": "MetaBO-BRA-v0",
    "D": 2,
    "f_type": "BRA-var",
    "f_opts": {"bound_translation": 0.1,
               "bound_scaling": 0.1,
               "M": 50},
    "features": ["posterior_mean", "posterior_std", "timestep", "budget", "x"],
    "T": 30,
    "n_init_samples": 0,
    "pass_X_to_pi": False,
    # parameters were determined offline via type-2-ML on a GP with 100 datapoints
    "kernel_lengthscale": [0.235, 0.578],
    "kernel_variance": 2.0,
    "noise_variance": 8.9e-16,
    "use_prior_mean_function": False,
    "local_af_opt": True,
    "N_MS": 1000,
    "N_LS": 1000,
    "k": 5,
    "reward_transformation": "neg_log10"
}

# specify PPO parameters
n_iterations = 2000
batch_size = 1200
n_workers = 10
arch_spec = 4 * [200]
ppo_spec = {
    "batch_size": batch_size,
    "max_steps": n_iterations * batch_size,
    "minibatch_size": batch_size // 20,
    "n_epochs": 4,
    "lr": 1e-4,
    "epsilon": 0.15,
    "value_coeff": 1.0,
    "ent_coeff": 0.01,
    "gamma": 0.98,
    "lambda": 0.98,
    "loss_type": "GAElam",
    "normalize_advs": True,
    "n_workers": n_workers,
    "env_id": env_spec["env_id"],
    "seed": 0,
    "env_seeds": list(range(n_workers)),
    "policy_options": {
        "activations": "relu",
        "arch_spec": arch_spec,
        "use_value_network": True,
        "t_idx": -2,
        "T_idx": -1,
        "arch_spec_value": arch_spec
    }
}

# register environment
register(
    id=env_spec["env_id"],
    entry_point="metabo.environment.metabo_gym:MetaBO",
    max_episode_steps=env_spec["T"],
    reward_threshold=None,
    kwargs=env_spec
)

# log data and weights go here, use this folder for evaluation afterwards
logpath = os.path.join(rootdir, "log", env_spec["env_id"], datetime.strftime(datetime.now(), "%Y-%m-%d-%H-%M-%S"))

# set up policy
policy_fn = lambda observation_space, action_space, deterministic: NeuralAF(observation_space=observation_space,
                                                                            action_space=action_space,
                                                                            deterministic=deterministic,
                                                                            options=ppo_spec["policy_options"])

# do training
print("Training on {}.\nFind logs, weights, and learning curve at {}\n\n".format(env_spec["env_id"], logpath))
ppo = PPO(policy_fn=policy_fn, params=ppo_spec, logpath=logpath, save_interval=1)
# learning curve is plotted online in separate process
p = mp.Process(target=plot_learning_curve_online, kwargs={"logpath": logpath, "reload": True})
p.start()
ppo.train()
p.terminate()
plot_learning_curve_online(logpath=logpath, reload=False)

File Path: train_metabo_furuta.py
Content:
# Copyright (c) 2019 Robert Bosch GmbH
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>.

# ******************************************************************
# train_metabo_furuta.py
# Train MetaBO on Furuta control task in simulation
# The weights, stats, logs, and the learning curve are stored in metabo/log and can
# be evaluated using metabo/eval/evaluate.py
# ******************************************************************

import os
import multiprocessing as mp
from datetime import datetime
from metabo.policies.policies import NeuralAF
from metabo.ppo.ppo import PPO
from metabo.ppo.plot_learning_curve_online import plot_learning_curve_online
from gym.envs.registration import register

rootdir = os.path.join(os.path.dirname(os.path.realpath(__file__)), "metabo")

# specifiy environment
true_mass_arm = 0.095
true_mass_pendulum = 0.024
true_length_arm = 0.085
true_length_pendulum = 0.129
low_mult = 0.75
high_mult = 1.25
env_spec = {
    "env_id": "MetaBO-Furuta-v0",
    "D": 4,
    "f_type": "Furuta",
    "f_opts": {"furuta_domain": [[-0.5, 0.2],
                                 [-1.6, 4.0],
                                 [-0.1, 0.04],
                                 [-0.04, 0.1]],
               "mass_arm_low": low_mult * true_mass_arm,
               "mass_arm_high": high_mult * true_mass_arm,
               "mass_pendulum_low": low_mult * true_mass_pendulum,
               "mass_pendulum_high": high_mult * true_mass_pendulum,
               "length_arm_low": low_mult * true_length_arm,
               "length_arm_high": high_mult * true_length_arm,
               "length_pendulum_low": low_mult * true_length_pendulum,
               "length_pendulum_high": high_mult * true_length_pendulum,
               "pos": [0, 1, 2, 3]},
    "features": ["posterior_mean", "posterior_std", "x"],
    "T_min": 1,
    "T_max": 25,
    "n_init_samples": 0,
    "pass_X_to_pi": False,
    "kernel_lengthscale": [0.1, 0.1, 0.1, 0.1],
    "kernel_variance": 1.5,
    "noise_variance": 1e-2,
    "use_prior_mean_function": True,
    "local_af_opt": True,
    "N_MS": 10000,
    "N_LS": 1000,
    "k": 1,
    "reward_transformation": "neg_linear"  # true maximum not known
}

# specify PPO parameters
n_iterations = 2000
batch_size = 1200
n_workers = 10
arch_spec = 4 * [200]
ppo_spec = {
    "batch_size": batch_size,
    "max_steps": n_iterations * batch_size,
    "minibatch_size": batch_size // 50,
    "n_epochs": 4,
    "lr": 1e-4,
    "epsilon": 0.15,
    "value_coeff": 1.0,
    "ent_coeff": 0.01,
    "gamma": 0.98,
    "lambda": 0.98,
    "loss_type": "GAElam",
    "normalize_advs": True,
    "n_workers": n_workers,
    "env_id": env_spec["env_id"],
    "seed": 0,
    "env_seeds": list(range(n_workers)),
    "policy_options": {
        "activations": "relu",
        "arch_spec": arch_spec,
        "use_value_network": True,
        "t_idx": -2,
        "T_idx": -1,
        "arch_spec_value": arch_spec
    }
}

# register environment
register(
    id=env_spec["env_id"],
    entry_point="metabo.environment.metabo_gym:MetaBO",
    max_episode_steps=env_spec["T_max"],
    reward_threshold=None,
    kwargs=env_spec
)

# log data and weights go here, use this folder for evaluation afterwards
logpath = os.path.join(rootdir, "log", env_spec["env_id"], datetime.strftime(datetime.now(), "%Y-%m-%d-%H-%M-%S"))

# set up policy
policy_fn = lambda observation_space, action_space, deterministic: NeuralAF(observation_space=observation_space,
                                                                            action_space=action_space,
                                                                            deterministic=deterministic,
                                                                            options=ppo_spec["policy_options"])

# do training
print("Training on {}.\nFind logs, weights, and learning curve at {}\n\n".format(env_spec["env_id"], logpath))
ppo = PPO(policy_fn=policy_fn, params=ppo_spec, logpath=logpath, save_interval=1)
# learning curve is plotted online in separate process
p = mp.Process(target=plot_learning_curve_online, kwargs={"logpath": logpath, "reload": True})
p.start()
ppo.train()
p.terminate()
plot_learning_curve_online(logpath=logpath, reload=False)

File Path: train_metabo_gprice.py
Content:
# Copyright (c) 2019 Robert Bosch GmbH
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>.

# ******************************************************************
# train_metabo_gprice.py
# Train MetaBO on GPrice-Function
# The weights, stats, logs, and the learning curve are stored in metabo/log and can
# be evaluated using metabo/eval/evaluate.py
# ******************************************************************

import os
import multiprocessing as mp
from datetime import datetime
from metabo.policies.policies import NeuralAF
from metabo.ppo.ppo import PPO
from metabo.ppo.plot_learning_curve_online import plot_learning_curve_online
from gym.envs.registration import register

rootdir = os.path.join(os.path.dirname(os.path.realpath(__file__)), "metabo")

# specifiy environment
env_spec = {
    "env_id": "MetaBO-GPRICE-v0",
    "D": 2,
    "f_type": "GPRICE-var",
    "f_opts": {"bound_translation": 0.1,
               "bound_scaling": 0.1,
               "M": 50},
    "features": ["posterior_mean", "posterior_std", "timestep", "budget", "x"],
    "T": 30,
    "n_init_samples": 0,
    "pass_X_to_pi": False,
    # parameters were determined offline via type-2-ML on a GP with 100 datapoints
    "kernel_lengthscale": [0.130, 0.07],
    "kernel_variance": 0.616,
    "noise_variance": 1e-6,
    "use_prior_mean_function": False,
    "local_af_opt": True,
    "N_MS": 1000,
    "N_LS": 1000,
    "k": 5,
    "reward_transformation": "neg_log10"
}

# specify PPO parameters
n_iterations = 2000
batch_size = 1200
n_workers = 10
arch_spec = 4 * [200]
ppo_spec = {
    "batch_size": batch_size,
    "max_steps": n_iterations * batch_size,
    "minibatch_size": batch_size // 20,
    "n_epochs": 4,
    "lr": 1e-4,
    "epsilon": 0.15,
    "value_coeff": 1.0,
    "ent_coeff": 0.01,
    "gamma": 0.98,
    "lambda": 0.98,
    "loss_type": "GAElam",
    "normalize_advs": True,
    "n_workers": n_workers,
    "env_id": env_spec["env_id"],
    "seed": 0,
    "env_seeds": list(range(n_workers)),
    "policy_options": {
        "activations": "relu",
        "arch_spec": arch_spec,
        "use_value_network": True,
        "t_idx": -2,
        "T_idx": -1,
        "arch_spec_value": arch_spec
    }
}

# register environment
register(
    id=env_spec["env_id"],
    entry_point="metabo.environment.metabo_gym:MetaBO",
    max_episode_steps=env_spec["T"],
    reward_threshold=None,
    kwargs=env_spec
)

# log data and weights go here, use this folder for evaluation afterwards
logpath = os.path.join(rootdir, "log", env_spec["env_id"], datetime.strftime(datetime.now(), "%Y-%m-%d-%H-%M-%S"))

# set up policy
policy_fn = lambda observation_space, action_space, deterministic: NeuralAF(observation_space=observation_space,
                                                                            action_space=action_space,
                                                                            deterministic=deterministic,
                                                                            options=ppo_spec["policy_options"])

# do training
print("Training on {}.\nFind logs, weights, and learning curve at {}\n\n".format(env_spec["env_id"], logpath))
ppo = PPO(policy_fn=policy_fn, params=ppo_spec, logpath=logpath, save_interval=1)
# learning curve is plotted online in separate process
p = mp.Process(target=plot_learning_curve_online, kwargs={"logpath": logpath, "reload": True})
p.start()
ppo.train()
p.terminate()
plot_learning_curve_online(logpath=logpath, reload=False)

File Path: train_metabo_gps.py
Content:
# Copyright (c) 2019 Robert Bosch GmbH
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>.

# ******************************************************************
# train_metabo_gps.py
# Train MetaBO on GP-samples
# The weights, stats, logs, and the learning curve are stored in metabo/log and can
# be evaluated using metabo/eval/evaluate.py
# ******************************************************************

import os
import multiprocessing as mp
from datetime import datetime
from metabo.policies.policies import NeuralAF
from metabo.ppo.ppo import PPO
from metabo.ppo.plot_learning_curve_online import plot_learning_curve_online
from gym.envs.registration import register

rootdir = os.path.join(os.path.dirname(os.path.realpath(__file__)), "metabo")

# specifiy environment
kernel = "Matern52"
env_spec = {
    "env_id": "MetaBO-GP-v0",
    "D": 3,
    "f_type": "GP",
    "f_opts": {"kernel": kernel,
               "lengthscale_low": 0.05,
               "lengthscale_high": 0.5,
               "noise_var_low": 0.1,
               "noise_var_high": 0.1,
               "signal_var_low": 1.0,
               "signal_var_high": 1.0,
               "min_regret": 1e-6},
    "features": ["posterior_mean", "posterior_std", "incumbent", "timestep_perc", "timestep", "budget"],
    "T": 30,
    "n_init_samples": 0,
    "pass_X_to_pi": False,
    # will be set individually for each new function to the sampled hyperparameters
    "kernel": kernel,
    "kernel_lengthscale": None,
    "kernel_variance": None,
    "noise_variance": None,
    "use_prior_mean_function": True,
    "local_af_opt": False,
    "cardinality_domain": 2000,
    "reward_transformation": "neg_log10"  # true maximum not known
}

# specify PPO parameters
n_iterations = 2000
batch_size = 1200
n_workers = 10
arch_spec = 2 * [20]
ppo_spec = {
    "batch_size": batch_size,
    "max_steps": n_iterations * batch_size,
    "minibatch_size": batch_size // 20,
    "n_epochs": 4,
    "lr": 1e-4,
    "epsilon": 0.15,
    "value_coeff": 1.0,
    "ent_coeff": 0.01,
    "gamma": 0.98,
    "lambda": 0.98,
    "loss_type": "GAElam",
    "normalize_advs": True,
    "n_workers": n_workers,
    "env_id": env_spec["env_id"],
    "seed": 0,
    "env_seeds": list(range(n_workers)),
    "policy_options": {
        "activations": "relu",
        "arch_spec": arch_spec,
        "exclude_t_from_policy": True,
        "exclude_T_from_policy": True,
        "use_value_network": True,
        "t_idx": -2,
        "T_idx": -1,
        "arch_spec_value": arch_spec
    }
}

# register environment
register(
    id=env_spec["env_id"],
    entry_point="metabo.environment.metabo_gym:MetaBO",
    max_episode_steps=env_spec["T"],
    reward_threshold=None,
    kwargs=env_spec
)

# log data and weights go here, use this folder for evaluation afterwards
logpath = os.path.join(rootdir, "log", env_spec["env_id"], datetime.strftime(datetime.now(), "%Y-%m-%d-%H-%M-%S"))

# set up policy
policy_fn = lambda observation_space, action_space, deterministic: NeuralAF(observation_space=observation_space,
                                                                            action_space=action_space,
                                                                            deterministic=deterministic,
                                                                            options=ppo_spec["policy_options"])

# do training
print("Training on {}.\nFind logs, weights, and learning curve at {}\n\n".format(env_spec["env_id"], logpath))
ppo = PPO(policy_fn=policy_fn, params=ppo_spec, logpath=logpath, save_interval=1)
# learning curve is plotted online in separate process
p = mp.Process(target=plot_learning_curve_online, kwargs={"logpath": logpath, "reload": True})
p.start()
ppo.train()
p.terminate()
plot_learning_curve_online(logpath=logpath, reload=False)

File Path: train_metabo_hm3.py
Content:
# Copyright (c) 2019 Robert Bosch GmbH
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>.

# ******************************************************************
# train_metabo_hm3.py
# Train MetaBO on Hartmann-3-Function
# The weights, stats, logs, and the learning curve are stored in metabo/log and can
# be evaluated using metabo/eval/evaluate.py
# ******************************************************************

import os
import multiprocessing as mp
from datetime import datetime
from metabo.policies.policies import NeuralAF
from metabo.ppo.ppo import PPO
from metabo.ppo.plot_learning_curve_online import plot_learning_curve_online
from gym.envs.registration import register

rootdir = os.path.join(os.path.dirname(os.path.realpath(__file__)), "metabo")

# specifiy environment
env_spec = {
    "env_id": "MetaBO-HM3-v0",
    "D": 3,
    "f_type": "HM3-var",
    "f_opts": {"bound_translation": 0.1,
               "bound_scaling": 0.1,
               "M": 50},
    "features": ["posterior_mean", "posterior_std", "timestep", "budget", "x"],
    "T": 30,
    "n_init_samples": 0,
    "pass_X_to_pi": False,
    # parameters were determined offline via type-2-ML on a GP with 100 datapoints
    "kernel_lengthscale": [0.716, 0.298, 0.186],
    "kernel_variance": 0.83,
    "noise_variance": 1.688e-11,
    "use_prior_mean_function": False,
    "local_af_opt": True,
    "N_MS": 2000,
    "N_LS": 2000,
    "k": 5,
    "reward_transformation": "neg_log10"
}

# specify PPO parameters
n_iterations = 2000
batch_size = 1200
n_workers = 10
arch_spec = 4 * [200]
ppo_spec = {
    "batch_size": batch_size,
    "max_steps": n_iterations * batch_size,
    "minibatch_size": batch_size // 20,
    "n_epochs": 4,
    "lr": 1e-4,
    "epsilon": 0.15,
    "value_coeff": 1.0,
    "ent_coeff": 0.01,
    "gamma": 0.98,
    "lambda": 0.98,
    "loss_type": "GAElam",
    "normalize_advs": True,
    "n_workers": n_workers,
    "env_id": env_spec["env_id"],
    "seed": 0,
    "env_seeds": list(range(n_workers)),
    "policy_options": {
        "activations": "relu",
        "arch_spec": arch_spec,
        "use_value_network": True,
        "t_idx": -2,
        "T_idx": -1,
        "arch_spec_value": arch_spec
    }
}

# register environment
register(
    id=env_spec["env_id"],
    entry_point="metabo.environment.metabo_gym:MetaBO",
    max_episode_steps=env_spec["T"],
    reward_threshold=None,
    kwargs=env_spec
)

# log data and weights go here, use this folder for evaluation afterwards
logpath = os.path.join(rootdir, "log", env_spec["env_id"], datetime.strftime(datetime.now(), "%Y-%m-%d-%H-%M-%S"))

# set up policy
policy_fn = lambda observation_space, action_space, deterministic: NeuralAF(observation_space=observation_space,
                                                                            action_space=action_space,
                                                                            deterministic=deterministic,
                                                                            options=ppo_spec["policy_options"])

# do training
print("Training on {}.\nFind logs, weights, and learning curve at {}\n\n".format(env_spec["env_id"], logpath))
ppo = PPO(policy_fn=policy_fn, params=ppo_spec, logpath=logpath, save_interval=1)
# learning curve is plotted online in separate process
p = mp.Process(target=plot_learning_curve_online, kwargs={"logpath": logpath, "reload": True})
p.start()
ppo.train()
p.terminate()
plot_learning_curve_online(logpath=logpath, reload=False)

File Path: train_metabo_svm.py
Content:
# Copyright (c) 2019 Robert Bosch GmbH
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>.

# ******************************************************************
# train_metabo_svm.py
# Train MetaBO on SVM-hyperparameter optimization
# The weights, stats, logs, and the learning curve are stored in metabo/log and can
# be evaluated using metabo/eval/evaluate.py
# ******************************************************************

# Note: due to licensing issues, the datasets used in this experiment cannot be shipped with the MetaBO package.
# However, you can download the datasets yourself from https://github.com/nicoschilling/ECML2016
# Put the folder "data/svm" from this repository into metabo/environment/hpo/data

import os
import multiprocessing as mp
from datetime import datetime
from metabo.policies.policies import NeuralAF
from metabo.ppo.ppo import PPO
from metabo.ppo.plot_learning_curve_online import plot_learning_curve_online
from gym.envs.registration import register

rootdir = os.path.join(os.path.dirname(os.path.realpath(__file__)), "metabo")

# specifiy environment
env_spec = {
    "env_id": "MetaBO-SVM-v0",
    "D": 2,
    "f_type": "HPO",
    "f_opts": {
        "hpo_data_file": os.path.join(rootdir, "environment", "hpo", "processed", "svm", "objectives.pkl"),
        "hpo_gp_hyperparameters_file": os.path.join(rootdir, "environment", "hpo", "processed", "svm",
                                                    "gp_hyperparameters.pkl"),
        "hpo_datasets_file": os.path.join(rootdir, "environment", "hpo", "processed", "svm",
                                          "train_datasets_iclr2020.txt"),
        "draw_random_datasets": True,  # present each test function once
        # to make logarithmic regret well-defined (applied only during training)
        "min_regret": 1e-6},
    "features": ["posterior_mean", "posterior_std", "timestep", "budget", "x"],
    "T": 20,
    "n_init_samples": 0,
    "pass_X_to_pi": False,
    # GP hyperparameters will be set individually for each new function, the parameters were determined off-line
    # via type-2-ML on all available data
    "kernel_lengthscale": None,
    "kernel_variance": None,
    "noise_variance": None,
    "use_prior_mean_function": True,
    "local_af_opt": False,  # discrete domain
    "cardinality_domain": 143,
    "reward_transformation": "neg_log10"
}

# specify PPO parameters
n_iterations = 2000
batch_size = 1200
n_workers = 10
arch_spec = 4 * [200]
ppo_spec = {
    "batch_size": batch_size,
    "max_steps": n_iterations * batch_size,
    "minibatch_size": batch_size // 20,
    "n_epochs": 4,
    "lr": 1e-4,
    "epsilon": 0.15,
    "value_coeff": 1.0,
    "ent_coeff": 0.01,
    "gamma": 0.98,
    "lambda": 0.98,
    "loss_type": "GAElam",
    "normalize_advs": True,
    "n_workers": n_workers,
    "env_id": env_spec["env_id"],
    "seed": 0,
    "env_seeds": list(range(n_workers)),
    "policy_options": {
        "activations": "relu",
        "arch_spec": arch_spec,
        "use_value_network": True,
        "t_idx": -2,
        "T_idx": -1,
        "arch_spec_value": arch_spec
    }
}

# register environment
register(
    id=env_spec["env_id"],
    entry_point="metabo.environment.metabo_gym:MetaBO",
    max_episode_steps=env_spec["T"],
    reward_threshold=None,
    kwargs=env_spec
)

# log data and weights go here, use this folder for evaluation afterwards
logpath = os.path.join(rootdir, "log", env_spec["env_id"], datetime.strftime(datetime.now(), "%Y-%m-%d-%H-%M-%S"))

# set up policy
policy_fn = lambda observation_space, action_space, deterministic: NeuralAF(observation_space=observation_space,
                                                                            action_space=action_space,
                                                                            deterministic=deterministic,
                                                                            options=ppo_spec["policy_options"])

# do training
print("Training on {}.\nFind logs, weights, and learning curve at {}\n\n".format(env_spec["env_id"], logpath))
ppo = PPO(policy_fn=policy_fn, params=ppo_spec, logpath=logpath, save_interval=1)
# learning curve is plotted online in separate process
p = mp.Process(target=plot_learning_curve_online, kwargs={"logpath": logpath, "reload": True})
p.start()
ppo.train()
p.terminate()
plot_learning_curve_online(logpath=logpath, reload=False)

Output:
{
    "experimental_code": "# Copyright (c) 2019 Robert Bosch GmbH# This program is free software: you can redistribute it and/or modify# it under the terms of the GNU Affero General Public License as published# by the Free Software Foundation, either version 3 of the License, or# (at your option) any later version.)# This program is distributed in the hope that it will be useful,# but WITHOUT ANY WARRANTY; without even the implied warranty of# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the# GNU Affero General Public License for more details.# You should have received a copy of the GNU Affero General Public License# along with this program.  If not, see <https://www.gnu.org/licenses/>.)# ******************************************************************# policies.py# Implementation of the MetaBO neural AF as well as benchmark AFs.# ******************************************************************import torchimport torch.nn as nnimport torch.nn.functional as Ffrom torch.distributions import Categoricalimport numpy as npfrom metabo.policies.mlp import MLPclass NeuralAF(nn.Module):    \"\"\"    Base class for MetaBO-Policies. Subclasses have to implement init_structure() and forward().    SHAPES:    forward()     states: (N_batch, N_grid, N_features)     logits: (N_batch, N_grid)     values: (N_batch, )    act(): only one action/value at a time in self.act()     state: (N_grid, N_features)     action: ()     value: ()    predict_vals_logps_ents()     states: (N_batch, N_grid, N_features)     actions: (N_batch, )     values: (N_batch, )     logprobs: (N_batch, )     entropies: (N_batch, )    \"\"\"    def __init__(self, observation_space, action_space, deterministic, options):        super(NeuralAF, self).__init__()        self.N_features = None  # has to be set in init_structure()        self.deterministic = deterministic        # initialize the network structure        self.init_structure(observation_space=observation_space, action_space=action_space, options=options)        # initialize weights        self.apply(self.init_weights)    def init_structure(self, observation_space, action_space, options):        self.N_features = observation_space.shape[1]        # activation function        if options[\"activations\"] == \"relu\":            f_act = F.relu        elif options[\"activations\"] == \"tanh\":            f_act = torch.tanh        else:            raise NotImplementedError(\"Unknown activation function!\")        # policy network        self.N_features_policy = self.N_features        if \"exclude_t_from_policy\" in options:            self.exclude_t_from_policy = options[\"exclude_t_from_policy\"]            assert \"t_idx\" in options            self.t_idx = options[\"t_idx\"]            self.N_features_policy = self.N_features_policy - 1 if self.exclude_t_from_policy else self.N_features_policy        else:            self.exclude_t_from_policy = False        if \"exclude_T_from_policy\" in options:            self.exclude_T_from_policy = options[\"exclude_T_from_policy\"]            assert \"T_idx\" in options            self.T_idx = options[\"T_idx\"]            self.N_features_policy = self.N_features_policy - 1 if self.exclude_T_from_policy else self.N_features_policy        else:            self.exclude_T_from_policy = False        self.policy_net = MLP(d_in=self.N_features_policy, d_out=1, arch_spec=options[\"arch_spec\"], f_act=f_act)        # value network        if \"use_value_network\" in options and options[\"use_value_network\"]:            self.use_value_network = True            self.value_net = MLP(d_in=2, d_out=1, arch_spec=options[\"arch_spec_value\"], f_act=f_act)            self.t_idx = options[\"t_idx\"]            self.T_idx = options[\"T_idx\"]        else:            self.use_value_network = False    def forward(self, states):        assert states.dim() == 3        assert states.shape[-1] == self.N_features        # policy network        mask = [True] * self.N_features        if self.exclude_t_from_policy:            mask[self.t_idx] = False        if self.exclude_T_from_policy:            mask[self.T_idx] = False        logits = self.policy_net.forward(states[:, :, mask])        logits.squeeze_(2)        # value network        if self.use_value_network:            tT = states[:, [0], [self.t_idx, self.T_idx]]            values = self.value_net.forward(tT)            values.squeeze_(1)        else:            values = torch.zeros(states.shape[0]).to(logits.device)        return logits, values    def af(self, state):        state = torch.from_numpy(state[None, :].astype(np.float32))        with torch.no_grad():            out = self.forward(state)        af = out[0].to(\"cpu\").numpy().squeeze()        return af    def act(self, state):        # here, state is assumed to contain a single state, i.e. no batch dimension        state = state.unsqueeze(0)  # add batch dimension        out = self.forward(state)        logits = out[0]        value = out[1]        if self.deterministic:            action = torch.argmax(logits)        else:            distr = Categorical(logits=logits)            # to sample the action, the policy uses the current PROCESS-local random seed, don't re-seed in pi.act            action = distr.sample()        return action.squeeze(0), value.squeeze(0)    def predict_vals_logps_ents(self, states, actions):        assert actions.dim() == 1        assert states.shape[0] == actions.shape[0]        out = self.forward(states)        logits = out[0]        values = out[1]        distr = Categorical(logits=logits)        logprobs = distr.log_prob(actions)        entropies = distr.entropy()        return values, logprobs, entropies    def set_requires_grad(self, requires_grad):        for p in self.parameters():            p.requires_grad = requires_grad    def reset(self):        pass    @staticmethod    def num_flat_features(x):        return np.prod(x.size()[1:])    @staticmethod    def init_weights(m):        if type(m) == nn.Linear:            m.weight.data.normal_(mean=0.0, std=0.01)            m.bias.data.fill_(0.0)class MLP(nn.Module):    def __init__(self, d_in: int, d_out: int, arch_spec: list, f_act=None):        \"\"\"        A standard multi-layer perceptron.        :param d_in: number of input features.        :param d_out: number of output features.        :param arch_spec: list containing the number of units in each hidden layer. If arch_spec == [], this is a        linear model.        :param f_act: nonlinear activation function (if arch_spec != [])        \"\"\"        super(MLP, self).__init__()        self.arch_spec = arch_spec        self.f_act = f_act        self.is_linear = (arch_spec == [])  # no hidden layers --> linear model        if not self.is_linear:            assert f_act is not None        # define the network        if self.is_linear:            self.fc = nn.ModuleList([nn.Linear(in_features=d_in, out_features=d_out)])        else:            self.fc = nn.ModuleList([nn.Linear(in_features=d_in, out_features=arch_spec[0])])            for i in range(1, len(arch_spec)):                self.fc.append(nn.Linear(in_features=self.fc[-1].out_features, out_features=arch_spec[i]))            self.fc.append(nn.Linear(in_features=self.fc[-1].out_features, out_features=d_out))    def forward(self, X):        Y = X        if self.is_linear:            Y = self.fc[0](Y)        else:            for layer in self.fc[:-1]:                Y = self.f_act(layer(Y))            Y = self.fc[-1](Y)        return Y",
    "experimental_info": "The Neural Acquisition Function (NeuralAF) is implemented as an MLP using ReLU activations and an architecture of 4 hidden layers, each with 200 units. A separate value network is also used with the same architecture. The policy is trained using Proximal Policy Optimization (PPO) with a batch size of 1200, 2000 iterations (total 2.4 million steps), 4 epochs, and a learning rate of 1e-4. The PPO algorithm uses an epsilon of 0.15 for clipping, value function coefficient of 1.0, and entropy coefficient of 0.01. GAE-Lambda with gamma=0.98 and lambda=0.98 is used for advantage estimation, and advantages are normalized. Training is performed on 10 workers with seeds from 0 to 9. The environment is 'MetaBO-GP-v0', a 3-dimensional task sampling Gaussian Process objective functions (Matern52 kernel). Lengthscales are sampled between 0.05 and 0.5, and signal/noise variances are 1.0/0.1 respectively. The optimization horizon (T) is 30 steps with 0 initial samples. Rewards are transformed using 'neg_log10' and prior mean function is used. The domain is treated as continuous with local acquisition function optimization enabled, using 2000 multi-start points and 2000 local search points with k=5 top multi-start points for local search."
}
