
Input:
You are a researcher with expertise in engineering in the field of machine learning.

# Instructions
- The content described in “Repository Content” corresponds to the GitHub repository of the method described in “Method.”
- Please extract the following two pieces of information from “Repository Content”:
    - experimental_code：Extract the implementation sections that are directly related to the method described in “Method.”
    - experimental_info：Extract and output the experimental settings related to the method described in “Method.”

# Method
FedEx leverages a novel connection between hyperparameter tuning in Federated Learning (FL) and the weight-sharing paradigm from Neural Architecture Search (NAS). It formalizes the personalized FL objective as a single-level empirical risk minimization, enabling a stochastic relaxation approach similar to NAS. Instead of architectural hyperparameters, FedEx tunes local training hyperparameters (e.g., learning rate, momentum, epochs) by setting up a categorical distribution over a fixed number of sampled configurations (often drawn using a local perturbation scheme around an initial sample). This distribution is then updated using exponentiated gradient updates, alternating with standard SGD updates to the shared model weights. The method applies to FL algorithms decomposable into local training (Locc) and aggregation (Aggb) subroutines (e.g., FedAvg, FedProx, SCAFFOLD, Reptile). For theoretical guarantees, FedEx's approach is analyzed within the Average Regret-Upper-Bound Analysis (ARUBA) framework for online convex optimization.

# Repository Content
File Path: cifar.py
Content:
import argparse
import json
import os
import pdb
import pickle
import random
import re
import string
import math
from copy import deepcopy
from collections import defaultdict
from glob import glob
import numpy as np
import torch; #torch.backends.cudnn.deterministic = False; 
torch.backends.cudnn.benchmark = True
from torch import nn
from torch import optim
from hyper import wrapped_fedex
from hyper import Server
import torch.nn.functional as F
import torchvision.datasets as datasets
import torchvision.transforms as transforms 



BATCH = 100
SERVER = lambda: {
                  'lr': 10.0 ** np.random.uniform(low=-1.0, high=1.0),
                  'momentum': np.random.choice([0.0, 0.9]),
                  'step': 1,
                  'gamma': 1.0 - 10.0 ** np.random.uniform(low=-4.0, high=-2.0),
                  }
CLIENT = lambda: {
                  'lr': 10.0 ** np.random.uniform(low=-4.0, high=0.0),
                  'momentum': np.random.uniform(low=0.0,high=1.0),
                  'weight_decay': 10.0 ** np.random.uniform(low=-5.0, high=-1.0),
                  'epochs': np.random.choice(np.arange(1, 6)), 
                  'batch': 2 ** np.random.choice(np.arange(3, 8)),
                  'mu': 10.0 ** np.random.uniform(low=-5.0, high=0.0),
                  'dropout': np.random.uniform(low=0.0, high=0.5),
                  }


def parse():

    parser = argparse.ArgumentParser()
    parser.add_argument('logdir')
    parser.add_argument('--seed', default=0, type=int, help='random seed')
    parser.add_argument('--debug', default=0, type=int,
                        help='run in DEBUG mode if >0; sets number of clients and batches')

    # wrapper algorithm settings
    parser.add_argument('--rounds', default=800, type=int,
                        help='maximum number of communication rounds')
    parser.add_argument('--total', default=4000, type=int,
                        help='total number of communication rounds')
    parser.add_argument('--rate', default=3, type=int, help='elimination rate')
    parser.add_argument('--elim', default=0, type=int, help='number of elimination rounds')
    parser.add_argument('--eval', default=1, type=int, help='number of evaluation rounds')
    parser.add_argument('--discount', default=0.0, type=float,
                        help='discount factor for computing the validation score of an arm')

    # FedEx settings
    parser.add_argument('--batch', default=10, type=int, help='number of tasks per round')
    parser.add_argument('--configs', default=1, type=int,
                        help='''number of configs to optimize over with FedEx (use 1 for FedAvg):
                                - <-1: sample a random number between 1 and abs(args.configs)
                                - =-1: sample the number of arms given by the wrapper
                                - =0: sample a random number between 1 and the number of arms
                                - >0: sample the provided number, ignoring the number of arms''')
    parser.add_argument('--lr_only', action='store_true', help='tune only learning rate as a hyperparameter')
    parser.add_argument('--eps', default=0.0, type=float, help='multiplicative perturbation to client config, eps=0 is fedavg')
    parser.add_argument('--uniform', action='store_true',
                        help='run FedEx over a product set of single-parameter uniform grids')
    parser.add_argument('--random', action='store_true',
                        help='run FedEx over a product set of single-parameter random grids')
    parser.add_argument('--eta0', default=0.0, type=float,
                        help='FedEx initial step size; if 0.0 uses FedEx default')
    parser.add_argument('--sched', default='aggressive', type=str, help='FedEx step size sched')
    parser.add_argument('--cutoff', default=0.0, type=float,
                        help='stop updating FedEx config distribution if entropy below this cutoff')
    parser.add_argument('--baseline', default=-1.0, type=float,
                        help='''how FedEx computes the baseline:
                                - >=-1.0,<0.0: sample discount factor from [0.0, abs(args.baseline))
                                - =0.0: use the most recent value
                                - >0.0,<1.0: use geometrically discounted mean with this factor
                                - =1.0: use the mean of all values''')
    parser.add_argument('--diff', action='store_true',
                        help='use difference between refine and global as FedEx objective')
    parser.add_argument('--stop', action='store_true',
                        help='stop updating FedEx config distribution after last elimination')

    # evaluation settings
    parser.add_argument('--mle', action='store_true', help='use MLE config at test time')
    parser.add_argument('--loss', action='store_true', help='use loss instead of error')
    parser.add_argument('--eval_global', action='store_true', help='use global error as elimination metric instead of refine')

# data settings
    parser.add_argument('--val', default=0.2, type=float, help='proportion of training set to use for validation')
    parser.add_argument('--num-clients', default=500, type=int, help='number of clients')


    return parser.parse_args()



def file2tensor(fname):

    with open(fname, 'r') as f:
        data = json.load(f)

    X = torch.from_numpy(np.asarray(data['x'])).float()
    Y = torch.from_numpy(np.asarray(data['y'])).long()

    return X, Y

def get_loader(train_idx, test_idx, train_data, test_data, val=0.2):

    data = {}
    m = int((1.-val) * len(train_idx))
    data['train'] = torch.utils.data.DataLoader(train_data, 
                                                sampler=torch.utils.data.SubsetRandomSampler(train_idx[:m]), 
                                                batch_size=m,
                                                shuffle=False,
                                                pin_memory=True)
    data['val'] = torch.utils.data.DataLoader(train_data, 
                                              sampler=torch.utils.data.SubsetRandomSampler(train_idx[m:]), 
                                              batch_size=len(train_idx)-m,
                                              shuffle=False,
                                              pin_memory=True)
    data['test'] = torch.utils.data.DataLoader(test_data, 
                                               sampler=torch.utils.data.SubsetRandomSampler(test_idx), 
                                               batch_size=len(test_idx),
                                               shuffle=False,
                                               pin_memory=True)


    def loader(*args):
        output = []
        for arg in args:
            Xarg, Yarg = next(iter(data[arg]))
            output.append(Xarg.cuda(non_blocking=True))
            output.append(Yarg.cuda(non_blocking=True))
        return output

    return loader


class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        self.conv1 = nn.Sequential(
                                   nn.Conv2d(3, 32, 3, padding=1),
                                   nn.ReLU(),
                                   nn.MaxPool2d(2),
                                   )
        self.conv2 = nn.Sequential(
                                   nn.Conv2d(32, 64, 3, padding=1),
                                   nn.ReLU(),
                                   nn.MaxPool2d(2),
                                   )
        self.conv3 = nn.Sequential(
                                   nn.Conv2d(64, 64, 3, padding=1),
                                   nn.ReLU(),
                                   nn.MaxPool2d(2),
                                   )
        self.dropout = nn.Dropout(0.0)
        self.fc = nn.Sequential(
                                nn.Linear(1024, 64),
                                nn.ReLU(),
                                )
        self.clf = nn.Linear(64, 10)

    def forward(self, x):
        x = self.conv1(x)
        x = self.conv2(x)
        x = self.conv3(x)
        x = self.fc(self.dropout(x.flatten(1)))
        return self.clf(self.dropout(x))

def get_prox(model, criterion=nn.CrossEntropyLoss(), mu=0.0):

    if not mu:
        return criterion

    mu *= 0.5
    model0 = [param.data.clone() for param in model.parameters()]

    def objective(*args, **kwargs):

        prox = sum((param-param0).pow(2).sum()
                   for param, param0 in zip(model.parameters(), model0))
        return criterion(*args, **kwargs) + mu * prox

    return objective


def train(model, X, Y, batch=32, dropout=0.0, epochs=1, mu=0.0, **kwargs):

    optimizer = optim.SGD(model.parameters(), **kwargs)
    criterion = get_prox(model, mu=mu)
    model.dropout.p = dropout
    model.train()
    m = len(Y)
    for e in range(epochs):
        randperm = torch.randperm(m)
        X, Y = X[randperm], Y[randperm]
        for i in range(0, m, batch):
            Xbatch, Ybatch =X[i:i+batch], Y[i:i+batch]
            pred = model(Xbatch)
            loss = criterion(pred, Ybatch)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
    model.eval()
    return model


def test_batch(model, X, Y):

    pred = model(X)
    return (Y != pred.argmax(1)).sum().float(), nn.CrossEntropyLoss(reduction='sum')(pred, Y).float()


def test(model, X, Y, batch=BATCH):

    model.eval()
    with torch.no_grad():
        errors, losses = zip(*(test_batch(model, X[i:i+batch], Y[i:i+batch])
                               for i in range(0, len(Y), batch)))
        return float(sum(errors)) / len(Y), float(sum(losses)) / len(Y)


def main():

    args = parse()
    random.seed(args.seed)
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)

    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                     std=[0.229, 0.224, 0.225])
    train_data = datasets.CIFAR10(root='./data', 
                                  train=True, 
                                  transform=transforms.Compose([
                                                                transforms.RandomHorizontalFlip(),
                                                                transforms.RandomCrop(32, 4),
                                                                transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),
                                                                transforms.ToTensor(),
                                                                normalize,
                                                                ]), 
                                                                download=True)
    test_data =  datasets.CIFAR10(root='./data', 
                                  train=False, 
                                  transform=transforms.Compose([
                                                                transforms.ToTensor(),
                                                                normalize,
                                                                ]),
                                  download=True)
    train_perm, test_perm = torch.randperm(50000), torch.randperm(10000)
    tasks = [get_loader(train_perm[i:i+50000//args.num_clients], test_perm[j:j+10000//args.num_clients], train_data, test_data, val=args.val)
             for i, j in zip(range(0, 50000, 50000//args.num_clients), range(0, 10000, 10000//args.num_clients))]
    if args.debug:
        tasks = tasks[:args.debug]
        print('DEBUG MODE')

    def local_train(model, X, Y, **kwargs):

        if args.debug:
            return train(model, X[:args.debug*args.batch], Y[:args.debug*args.batch], **kwargs)
        return train(model, X, Y, **kwargs)

    def local_test(model, X, Y, **kwargs):

        return test(model, X, Y, **kwargs)[args.loss]

    def get_server():

        model = CNN()
        return Server(model, tasks, local_train, local_test, batch=args.batch, **SERVER())
    
    def get_client(n_clients=1):
        '''performs local tuning for each hyperparameter'''
        if args.lr_only:
            return [SIMPLE_CLIENT()]

        initial_client = CLIENT()
        client_arr = [initial_client]
        eps = args.eps

        for i in range(n_clients-1):
            other_client = deepcopy(initial_client)
            
            log_lr = np.log10(other_client['lr'])
            other_client['lr'] = 10 ** np.clip(log_lr + np.random.uniform(4*-eps, 4*eps), -4.0, 0.0)
            
            other_client['momentum'] = np.clip(initial_client['momentum'] + np.random.uniform(-eps, eps), 0, 1.0)
            
            log_wd = np.log10(other_client['weight_decay'])
            other_client['weight_decay'] = 10 ** np.clip(log_wd + np.random.uniform(4*-eps, 4*eps),-5.0, -1.0)
            
            epochs_range = math.ceil(eps * 4)
            other_client['epochs'] = np.clip(np.random.choice(np.arange(initial_client['epochs']-epochs_range, initial_client['epochs']+epochs_range+1)), 1, 5)

            log_batch = int(np.log2(other_client['batch']))
            batch_range = math.ceil(eps * 4)
            other_client['batch'] = 2 ** np.clip(np.random.choice(np.arange(log_batch-batch_range, log_batch+batch_range+1)), 3, 7)

            
            log_mu = np.log10(other_client['mu'])
            other_client['mu'] = 10 ** np.clip(log_mu + np.random.uniform(5*-eps, 5*eps), -5.0 , 0.0)
            
            other_client['dropout'] = np.clip(initial_client['dropout'] + np.random.uniform(0.5*-eps, 0.5*eps),0, 0.5)

            client_arr.append(other_client)

        return [UNIFORM()] if args.uniform else [RANDOM()] if args.random else client_arr

    
    
    print('Tuning',
          'FedAvg' if args.configs == 1 and not (args.uniform or args.random) else 'FedEx',
          'on Cifar10')
    os.makedirs(args.logdir, exist_ok=True)
    with open(os.path.join(args.logdir, 'args.json'), 'w') as f:
        json.dump(vars(args), f, indent=4)
    wrapped_fedex(
                  get_server,
                  get_client,
                  num_configs=args.configs,
                  prod=args.uniform or args.random,
                  stepsize_init=args.eta0 if args.eta0 else 'auto',
                  stepsize_sched=args.sched,
                  cutoff=args.cutoff,
                  baseline_discount=args.baseline,
                  diff=args.diff,
                  mle=args.mle,
                  logdir=args.logdir,
                  val_discount=args.discount,
                  last_stop=args.stop,
                  max_resources=args.rounds,
                  total_resources=args.total,
                  elim_rate=args.rate,
                  num_elim=args.elim,
                  num_eval=args.eval,
                  eval_global=args.eval_global
                  )

if __name__ == '__main__':

    main()


File Path: femnist.py
Content:
import argparse
import json
import os
import pdb
import pickle
import random
import re
import string
import math
from copy import deepcopy
from collections import defaultdict
from glob import glob
import numpy as np
import torch; #torch.backends.cudnn.deterministic = False; 
torch.backends.cudnn.benchmark = True
from torch import nn
from torch import optim
from hyper import wrapped_fedex
from hyper import Server
import torch.nn.functional as F



BATCH = 100
DATA = './data/leaf_femnist_all/'
SERVER = lambda: {
                  'lr': 10.0 ** np.random.uniform(low=-1.0, high=1.0),
                  'momentum': np.random.choice([0.0, 0.9]),
                  'step': 1,
                  'gamma': 1.0 - 10.0 ** np.random.uniform(low=-4.0, high=-2.0),
                  }
CLIENT = lambda: {
                  'lr': 10.0 ** np.random.uniform(low=-4.0, high=0.0),
                  'momentum': np.random.uniform(low=0.0, high=1.0),
                  'weight_decay': 10.0 ** np.random.uniform(low=-5.0, high=-1.0),
                  'epochs': np.random.choice(np.arange(1, 6)),
                  'batch': 2 ** np.random.choice(np.arange(3, 8)),
                  'mu': 10.0 ** np.random.uniform(low=-5.0, high=0.0),
                  'dropout': np.random.uniform(low=0.0, high=0.5),
                  }


def parse():

    parser = argparse.ArgumentParser()
    parser.add_argument('logdir')
    parser.add_argument('--seed', default=0, type=int, help='random seed')
    parser.add_argument('--debug', default=0, type=int,
                        help='run in DEBUG mode if >0; sets number of clients and batches')

    # wrapper algorithm settings
    parser.add_argument('--rounds', default=50, type=int,
                        help='maximum number of communication rounds')
    parser.add_argument('--total', default=450, type=int,
                        help='total number of communication rounds')
    parser.add_argument('--rate', default=3, type=int, help='elimination rate')
    parser.add_argument('--elim', default=0, type=int, help='number of elimination rounds')
    parser.add_argument('--eval', default=1, type=int, help='number of evaluation rounds')
    parser.add_argument('--discount', default=0.0, type=float,
                        help='discount factor for computing the validation score of an arm')

    # FedEx settings
    parser.add_argument('--batch', default=10, type=int, help='number of tasks per round')
    parser.add_argument('--configs', default=1, type=int,
                        help='''number of configs to optimize over with FedEx (use 1 for FedAvg):
                                - <-1: sample a random number between 1 and abs(args.configs)
                                - =-1: sample the number of arms given by the wrapper
                                - =0: sample a random number between 1 and the number of arms
                                - >0: sample the provided number, ignoring the number of arms''')
    parser.add_argument('--lr_only', action='store_true', help='tune only learning rate as a hyperparameter')
    parser.add_argument('--eps', default=0.0, type=float, help='multiplicative perturbation to client config, eps=0 is fedavg')
    parser.add_argument('--uniform', action='store_true',
                        help='run FedEx over a product set of single-parameter uniform grids')
    parser.add_argument('--random', action='store_true',
                        help='run FedEx over a product set of single-parameter random grids')
    parser.add_argument('--eta0', default=0.0, type=float,
                        help='FedEx initial step size; if 0.0 uses FedEx default')
    parser.add_argument('--sched', default='aggressive', type=str, help='FedEx step size sched')
    parser.add_argument('--cutoff', default=0.0, type=float,
                        help='stop updating FedEx config distribution if entropy below this cutoff')
    parser.add_argument('--baseline', default=-1.0, type=float,
                        help='''how FedEx computes the baseline:
                                - >=-1.0,<0.0: sample discount factor from [0.0, abs(args.baseline))
                                - =0.0: use the most recent value
                                - >0.0,<1.0: use geometrically discounted mean with this factor
                                - =1.0: use the mean of all values''')
    parser.add_argument('--diff', action='store_true',
                        help='use difference between refine and global as FedEx objective')
    parser.add_argument('--stop', action='store_true',
                        help='stop updating FedEx config distribution after last elimination')

    # evaluation settings
    parser.add_argument('--mle', action='store_true', help='use MLE config at test time')
    parser.add_argument('--loss', action='store_true', help='use loss instead of error')
    parser.add_argument('--eval_global', action='store_true', help='use global error as elimination metric instead of refine')

    # data settings
    parser.add_argument('--iid', action='store_true', help='client data is iid')
    parser.add_argument('--val', default=0.1, type=float, help='val proportion')
    parser.add_argument('--test', default=0.1, type=float, help='test proportion')


    return parser.parse_args()



def file2tensor(fname):

    with open(fname, 'r') as f:
        data = json.load(f)

    X = torch.from_numpy(np.asarray(data['x'])).float()
    Y = torch.from_numpy(np.asarray(data['y'])).long()

    return X, Y


def get_loader(task, val=0.1, test=0.1, iid=False):
    m = len(torch.load(task)['Y'])
    if m < 1.0 / min(1.0-val-test, val, test):
        return None
    partitions = {'train': (0, int((1.0-val-test)*m)), 
                  'val': (int((1.0-val-test)*m), int((1.0-test)*m)), 
                  'test': (int((1.0-test)*m), m)}
    if iid:
        randperm = torch.randperm(m)
    dump = torch.load(task)
    X, Y = (dump['X'][randperm], dump['Y'][randperm]) if iid else (dump['X'], dump['Y'])
    data = {key: (X[a:b].pin_memory(), Y[a:b].pin_memory()) for key, (a, b) in partitions.items()}
    def loader(*args):
        output = []
        for arg in args:
            Xarg, Yarg = data[arg]
            output.append(Xarg.cuda(non_blocking=True))
            output.append(Yarg.cuda(non_blocking=True))
        return output
    return loader



class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, 3, 1)
        self.conv2 = nn.Conv2d(32, 64, 3, 1)
        self.dropout = nn.Dropout2d(0.0)
        self.fc1 = nn.Linear(9216, 1024)
        self.fc2 = nn.Linear(1024, 62)

    def forward(self, x):
        x = x.view(-1, 1, 28, 28)
        x = self.conv1(x)
        x = F.relu(x)
        x = self.conv2(x)
        x = F.relu(x)
        x = F.max_pool2d(x, 2)
        x = torch.flatten(x, 1)
        x = self.dropout(x)
        x = self.fc1(x)
        x = F.relu(x)
        x = self.dropout(x)
        x = self.fc2(x)
        return x





def get_prox(model, criterion=nn.CrossEntropyLoss(), mu=0.0):

    if not mu:
        return criterion

    mu *= 0.5
    model0 = [param.data.clone() for param in model.parameters()]

    def objective(*args, **kwargs):

        prox = sum((param-param0).pow(2).sum()
                   for param, param0 in zip(model.parameters(), model0))
        return criterion(*args, **kwargs) + mu * prox

    return objective



def train(model, X, Y, batch=32, dropout=0.0, epochs=1, mu=0.0, **kwargs):

    optimizer = optim.SGD(model.parameters(), **kwargs)
    criterion = get_prox(model, mu=mu)
    model.dropout.p = dropout
    model.train()
    m = len(Y)
    for e in range(epochs):
        randperm = torch.randperm(m)
        X, Y = X[randperm], Y[randperm]
        for i in range(0, m, batch):
            Xbatch, Ybatch =X[i:i+batch], Y[i:i+batch]
            pred = model(Xbatch)
            loss = criterion(pred, Ybatch)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
    model.eval()
    return model


def test_batch(model, X, Y):

    pred = model(X)
    return (Y != pred.argmax(1)).sum().float(), nn.CrossEntropyLoss(reduction='sum')(pred, Y).float()


def test(model, X, Y, batch=BATCH):

    model.eval()
    with torch.no_grad():
        errors, losses = zip(*(test_batch(model, X[i:i+batch], Y[i:i+batch])
                               for i in range(0, len(Y), batch)))
        return float(sum(errors)) / len(Y), float(sum(losses)) / len(Y)


def main():

    args = parse()
    random.seed(args.seed)
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)


    tasks = []
    for i, fname in enumerate(glob(os.path.join(DATA, '*.json'))):
        task = os.path.join(DATA, fname.split('/')[-1][:-5] + '.pt')
        if not os.path.isfile(task):
            print('\rCaching task', i+1, end='')
            dump={}
            dump['X'], dump['Y'] = file2tensor(fname)
            torch.save(dump, task)
        loader = get_loader(task, val=args.val, test=args.test, iid=args.iid)
        if not loader is None:
            tasks.append(loader)
    if args.debug:
        tasks = tasks[:args.debug]
        print('DEBUG MODE')
    print('\rCompleted cache-check')

    def local_train(model, X, Y, **kwargs):

        if args.debug:
            return train(model, X[:args.debug*args.batch], Y[:args.debug*args.batch], **kwargs)
        return train(model, X, Y, **kwargs)

    def local_test(model, X, Y, **kwargs):

        return test(model, X, Y, **kwargs)[args.loss]

    def get_server():

        model = CNN()
        return Server(model, tasks, local_train, local_test, batch=args.batch, **SERVER())
    
    def get_client(n_clients=1):
        '''performs local tuning for each hyperparameter'''
        if args.lr_only:
            return [SIMPLE_CLIENT()]

        initial_client = CLIENT()
        client_arr = [initial_client]
        eps = args.eps

        for i in range(n_clients-1):
            other_client = deepcopy(initial_client)
            
            log_lr = np.log10(other_client['lr'])
            other_client['lr'] = 10 ** np.clip(log_lr + np.random.uniform(-eps * 4, eps * 4), -4.0, 0.0)
            
            other_client['momentum'] = np.clip(initial_client['momentum'] + np.random.uniform(-eps, eps), 0, 1.0)
            
            log_wd = np.log10(other_client['weight_decay'])
            other_client['weight_decay'] = 10 ** np.clip(log_wd + np.random.uniform(-eps*4, eps*4),-5.0, -1.0)

            epochs_range = math.ceil(eps * 4)
            other_client['epochs'] = np.clip(np.random.choice(np.arange(initial_client['epochs']-epochs_range, initial_client['epochs']+epochs_range+1)), 1, 5)

            log_batch = int(np.log2(other_client['batch']))
            batch_range = math.ceil(eps * 4)
            other_client['batch'] = 2 ** np.clip(np.random.choice(np.arange(log_batch-batch_range, log_batch+batch_range+1)), 3, 7)

            log_mu = np.log10(other_client['mu'])
            other_client['mu'] = 10 ** np.clip(log_mu + np.random.uniform(-eps*5, eps*5), -5.0 , 0.0)
            
            other_client['dropout'] = np.clip(initial_client['dropout'] + np.random.uniform(-eps*0.5, eps*0.5),0, 0.5)

            client_arr.append(other_client)

        return [UNIFORM()] if args.uniform else [RANDOM()] if args.random else client_arr

    
    
    
    print('Tuning',
          'FedAvg' if args.configs == 1 and not (args.uniform or args.random) else 'FedEx',
          'on Femnist')
    os.makedirs(args.logdir, exist_ok=True)
    with open(os.path.join(args.logdir, 'args.json'), 'w') as f:
        json.dump(vars(args), f, indent=4)
    wrapped_fedex(
                  get_server,
                  get_client,
                  num_configs=args.configs,
                  prod=args.uniform or args.random,
                  stepsize_init=args.eta0 if args.eta0 else 'auto',
                  stepsize_sched=args.sched,
                  cutoff=args.cutoff,
                  baseline_discount=args.baseline,
                  diff=args.diff,
                  mle=args.mle,
                  logdir=args.logdir,
                  val_discount=args.discount,
                  last_stop=args.stop,
                  max_resources=args.rounds,
                  total_resources=args.total,
                  elim_rate=args.rate,
                  num_elim=args.elim,
                  num_eval=args.eval,
                  eval_global=args.eval_global,
                  )

if __name__ == '__main__':

    main()

File Path: hyper.py
Content:

import argparse
import os
import pdb
import pickle
import random
from copy import deepcopy
from glob import glob
from heapq import nsmallest
from itertools import product
from math import ceil
from math import log
from operator import itemgetter
import numpy as np
import torch
from numpy.linalg import norm
from scipy.special import logsumexp
from tensorboardX import SummaryWriter
from torch import optim


def discounted_mean(trace, factor=1.0):

    weight = factor ** np.flip(np.arange(len(trace)), axis=0)

    return np.inner(trace, weight) / weight.sum()


class FedEx:
    '''runs hyperparameter optimization given a federated learning server'''

    def entropy(self):

        entropy = 0.0
        for probs in product(*(theta[theta>0.0] for theta in self._theta)):
            prob = np.prod(probs)
            entropy -= prob * np.log(prob)
        return entropy

    def mle(self):
    
        return np.prod([theta.max() for theta in self._theta])

    def __init__(
                 self, 
                 server, 
                 configs, 
                 eta0='auto', 
                 sched='auto', 
                 cutoff=0.0, 
                 baseline=0.0, 
                 diff=False,
                 ):
        '''
        Args:
            server: Object that implements two methods, 'communication_round' and 'full_evaluation'
                    taking as input a single argument, 'get_config', itself a function that takes 
                    no inputs and outputs an element of the provided list 'configs'. 
                    - 'communication_round' samples a batch of clients, assigns a config to each 
                    using 'get_config', and runs local training using that config. It then 
                    aggregates the local models to to take a training step and returns three lists 
                    or arrays: a list of each client's validation error before local training, a 
                    list of each client's validation error after local training, and a list of each 
                    client's weight (e.g. size of its validation set). 
                    - 'full_evaluation' assigns a config to each client using 'get_config' and runs
                    local training using that config. It then returns three lists or arrays: a list
                    of each client's test error before local training, a list of each client's test
                    error after local training, and a list of each client's weight (e.g. size of 
                    its test set).
            configs: list of configs used for local training and testing by 'server' 
                     OR dict of (string, list) pairs denoting a grid of configs
            eta0: base exponentiated gradient step size; if 'auto' uses sqrt(2*log(len(configs)))
            sched: learning rate schedule for exponentiated gradient:
                    - 'adaptive': uses eta0 / sqrt(sum of squared gradient l-infinity norms)
                    - 'aggressive': uses eta0 / gradient l-infinity norm
                    - 'auto': uses eta0 / sqrt(t) for t the number of rounds
                    - 'constant': uses eta0
                    - 'scale': uses sched * sqrt(2 * log(len(configs)))
            cutoff: entropy level below which to stop updating the config probability and use MLE
            baseline: discount factor when computing baseline; 0.0 is most recent, 1.0 is mean
            diff: if True uses performance difference; otherwise uses absolute performance
        '''

        self._server = server
        self._configs = configs
        self._grid = [] if type(configs) == list else sorted(configs.keys())

        sizes = [len(configs[param]) for param in self._grid] if self._grid else [len(configs)]
        self._eta0 = [np.sqrt(2.0 * np.log(size)) if eta0 == 'auto' else eta0 for size in sizes]
        self._sched = sched
        self._cutoff = cutoff
        self._baseline = baseline
        self._diff = diff
        self._z = [np.full(size, -np.log(size)) for size in sizes]
        self._theta = [np.exp(z) for z in self._z]

        self._store = [0.0 for _ in sizes]
        self._stopped = False
        self._trace = {'global': [], 'refine': [], 'entropy': [self.entropy()], 'mle': [self.mle()]}

    def stop(self):

        self._stopped = True

    def sample(self, mle=False, _index=[]):
        '''samples from configs using current probability vector'''

        if mle or self._stopped:
            if self._grid:
                return {param: self._configs[param][theta.argmax()] 
                        for theta, param in zip(self._theta, self._grid)}
            return self._configs[self._theta[0].argmax()]
        _index.append([np.random.choice(len(theta), p=theta) for theta in self._theta])

        if self._grid:
            return {param: self._configs[param][i] for i, param in zip(_index[-1], self._grid)}
        return self._configs[_index[-1][0]]

    def settings(self):
        '''returns FedEx input settings'''

        output = {'configs': deepcopy(self._configs)}
        output['eta0'], output['sched'] = self._eta0, self._sched
        output['cutoff'], output['baseline'] = self._cutoff, self._baseline 
        if self._trace['refine']:
            output['theta'] = self.theta()
        return output

    def step(self):
        '''takes exponentiated gradient step (calls 'communication_round' once)'''

        index = []
        before, after, weight = self._server.communication_round(lambda: self.sample(_index=index))        
        before, after = np.array(before), np.array(after)
        weight = np.array(weight, dtype=np.float64) / sum(weight)

        if self._trace['refine']:
            trace = self.trace('refine')
            if self._diff:
                trace -= self.trace('global')
            baseline = discounted_mean(trace, self._baseline)
        else:
            baseline = 0.0
        self._trace['global'].append(np.inner(before, weight))
        self._trace['refine'].append(np.inner(after, weight))
        if not index:
            self._trace['entropy'].append(0.0)
            self._trace['mle'].append(1.0)
            return

        for i, (z, theta) in enumerate(zip(self._z, self._theta)):
            grad = np.zeros(len(z))
            for idx, s, w in zip(index, after-before if self._diff else after, weight):
                grad[idx[i]] += w * (s - baseline) / theta[idx[i]]
            if self._sched == 'adaptive':
                self._store[i] += norm(grad, float('inf')) ** 2
                denom = np.sqrt(self._store[i])
            elif self._sched == 'aggressive':
                denom = 1.0 if np.all(grad == 0.0) else norm(grad, float('inf'))
            elif self._sched == 'auto':
                self._store[i] += 1.0
                denom = np.sqrt(self._store[i])
            elif self._sched == 'constant':
                denom = 1.0
            elif self._sched == 'scale':
                denom = 1.0 / np.sqrt(2.0 * np.log(len(grad))) if len(grad) > 1 else float('inf')
            else:
                raise NotImplementedError
            eta = self._eta0[i] / denom
            z -= eta * grad
            z -= logsumexp(z)
            self._theta[i] = np.exp(z)

        self._trace['entropy'].append(self.entropy())
        self._trace['mle'].append(self.mle())
        if self._trace['entropy'][-1] < self._cutoff:
            self.stop()

    def test(self, mle=False):
        '''evaluates found config (calls 'full_evaluation' once)
        Args:
            mle: use MLE config instead of sampling
        Returns:
            output of 'full_evaluation'
        '''

        before, after, weight = self._server.full_evaluation(lambda: self.sample(mle=mle))
        return {'global': np.inner(before, weight) / weight.sum(),
                'refine': np.inner(after, weight) / weight.sum()}

    def theta(self):
        '''returns copy of config probability vector'''

        return deepcopy(self._theta)

    def trace(self, key):
        '''returns trace of one of three tracked quantities
        Args:
            key: 'entropy', 'global', or 'refine'
        Returns:
            numpy vector with length equal to number of calls to 'step'
        '''

        return np.array(self._trace[key])


def frac(p, q):

    return str(p) + '/' + str(q)


class Server:
    '''object for federated training implementing methods required by FedEx'''

    def _set_test_state(self):

        state = (np.random.get_state(), torch.get_rng_state(), torch.cuda.get_rng_state())
        if self._state is None:
            self._state = state
        else:
            np.random.set_state(self._state[0])
            torch.set_rng_state(self._state[1])
            torch.cuda.set_rng_state(self._state[2])
        return state

    def _reset_state(self, state):

        np.random.set_state(state[0])
        torch.set_rng_state(state[1])
        torch.cuda.set_rng_state(state[2])

    def __init__(
                 self, 
                 model, 
                 clients, 
                 train, 
                 test, 
                 lr=1.0, 
                 momentum=0.0, 
                 step=1, 
                 gamma=1.0, 
                 batch=10,
                 state=None,
                 ):
        '''
        Args:
            model: PyTorch model
            clients: list of clients, each a function that takes one or more strings 'train',
                     'val', 'test' and returns, as one tuple, input and output tensors for each
            train: method that takes as argument a PyTorch model, an input tensor, an output
                   tensor, and optional kwargs and returns the same PyTorch model
            test: method that takes as argument a PyTorch model, an input tensor, and an output
                  tensor and returns the model's error
            lr: server learning rate
            momentum: server momentum
            step: server learning rate decay interval
            gamma: server learning rate decay factor
            batch: number of clients to sample per communication round
            state: np.random, torch, torch.cuda random state tuple; if None uses current states
        '''

        self._model = model
        self._clients = clients
        self._train = train
        self._test = test
        self._opt = optim.SGD(self._model.parameters(), lr=lr, momentum=momentum)
        self._sched = optim.lr_scheduler.StepLR(self._opt, step, gamma=gamma)
        self._batch = batch
        self._state = state
        self._reset_state(self._set_test_state())

    def communication_round(self, get_config):
        '''runs one step of local training and model aggregation
        Args:
            get_config: returns kwargs for 'train' as a dict
        Returns:
            np.array objects for global val error, local val error, and val size of each client
        '''

        self._model.cuda()
        before, after, weight = [np.zeros(self._batch) for _ in range(3)]
        total = 0.0

        for i in range(self._batch):
            Xtrain, Ytrain, Xval, Yval = random.choice(self._clients)('train', 'val')
            before[i] = self._test(self._model, Xval, Yval)
            model = self._train(deepcopy(self._model), Xtrain, Ytrain, **get_config())
            after[i] = self._test(model, Xval, Yval)
            weight[i] = len(Yval)
            total += len(Ytrain)
            if i:
                for agg, param in zip(aggregate.parameters(), model.parameters()):
                    agg.data += len(Ytrain) * param.data
            else:
                for param in model.parameters():
                    param.data *= len(Ytrain)
                aggregate = model

        self._opt.zero_grad()
        for agg, param in zip(aggregate.parameters(), self._model.parameters()):
            param.grad = param.data - agg / total
        self._opt.step()
        self._opt.zero_grad()
        self._sched.step()
        self._model.cpu()
        return before, after, weight

    def full_evaluation(self, get_config):
        '''evaluates personalization on each client
        Args:
            get_config: returns kwargs for 'train' as a dict
        Returns:
            np.array objects for global test error, local test error, and test size of each client
        '''

        state = self._set_test_state()
        self._model.cuda()
        before, after, weight = [np.zeros(len(self._clients)) for _ in range(3)]
        for i, client in enumerate(self._clients):
            Xtrain, Ytrain, Xtest, Ytest = client('train', 'test')
            before[i] = self._test(self._model, Xtest, Ytest)
            after[i] = self._test(self._train(deepcopy(self._model), 
                                              Xtrain, Ytrain, **get_config()), 
                                  Xtest, Ytest)
            weight[i] = len(Ytest)
            print('\r\tEvaluated client', frac(i+1, len(self._clients)),
                  '    global error:', round(np.inner(before, weight) / weight.sum(), 4),
                  '    refine error:', round(np.inner(after, weight) / weight.sum(), 4), 
                  end=32*' ')
        self._model.cpu()
        self._reset_state(state)
        return before, after, weight


def random_search(max_resources=500, total_resources=2000):
    '''returns a random search rate and schedule for use by 'successive_elimination'
    Args:
        max_resources: most resources (steps) assigned to single arm
        total_resources: overall resource limit
    Returns:
        elimination rate as an int, elimination schedule as a list
    '''

    assert max_resources > 0, "max_resources must be positive"
    return int(total_resources / max_resources), [max_resources]


def get_schedule(
                 max_resources=500, 
                 total_resources=2000, 
                 elim_rate=3, 
                 num_elim=0, 
                 num_eval=1,
                 ):
    '''returns rate and schedule for use by 'successive_elimination'
    Args:
        max_resources: most resources (steps) assigned to single arm
        total_resources: overall resource limit
        elim_rate: multiplicative elimination rate
        num_elim: number of elimination rounds; if 0 runs random search
        num_eval: number of evaluation rounds
    Returns:
        elimination rate as an int, elimination schedule as a list, evaluation schedule as a list
    '''

    assert max_resources <= total_resources, "max_resources cannot be greater than total_resources"
    assert elim_rate > 1, "elim_rate must be greater than 1"
    assert num_eval <= total_resources, "num_eval cannot be greater than total_resources"

    if num_elim:
        diff = total_resources - max_resources
        geos = (elim_rate**(num_elim+1) - 1) / (elim_rate-1)
        u = int(diff / (geos-num_elim-1))
        resources = 0
        v = lambda i: 1 + ceil((diff+(num_elim-geos+elim_rate**i)*u) / (elim_rate**i-1))
        for opt in product(*(range(u, v(i)) for i in reversed(range(1, num_elim+1)))):
            used = max_resources + sum((elim_rate**i-1)*r 
                                       for i, r in zip(reversed(range(1, num_elim+1)), opt))
            if resources <= used <= total_resources:
                best, resources = opt, used
        assert not 0 in best, "invalid: use more resources or fewer eliminations, or increase rate"
        elim_sched = list(np.cumsum(best)) + [max_resources]
    else:
        elim_rate, elim_sched = random_search(max_resources=max_resources, 
                                              total_resources=total_resources)

    eval_sched = [int(step) for step in np.linspace(0, total_resources, num_eval+1)[1:]]
    return elim_rate, elim_sched, eval_sched



def successive_elimination(
                           sampler, 
                           eval_traces, 
                           logdir=None, 
                           val_discount=0.0, 
                           elim_rate=3, 
                           elim_sched=[1], 
                           eval_sched=[-1], 
                           traces=[], 
                           last_round=None,
                           eval_global=False,
                           **kwargs,
                           ):
    '''runs successive elimination according to provided schedule
    Args:
        sampler: function of n returning an iterable of n objects with methods 'step' and 'trace'
        eval_traces: list of strings of traces measuring performance; element 0 used for elimination
        logdir: directory to store tensorboard logs; if None does not log anything
        val_discount: discount factor when computing score for a trace; 0.0 is most recent, 1.0 is mean
        elim_rate: multiplicative elimination rate
        elim_sched: list of steps at which to run an elimination
        eval_sched: list of steps at which to call 'test' method of the best config
        traces: list of strings of traces to collect
        last_round: str name of function that last config executes before final round
        kwargs: passed to 'test' method of each config
    Returns:
        best config;
        also dumps tensorboard logs and results.pkl to folder 'logdir', if specified
    '''
    assert len(elim_sched) > 0, "'elim_sched' must be a list of positive length"
    assert type(elim_rate) == int, "'elim_rate' must be an int"
    logger = False if logdir is None else SummaryWriter(logdir)
    traces, eval_sched = deepcopy(traces), deepcopy(eval_sched)
    for trace in eval_traces:
        if not trace in traces:
            traces.append(trace)
    #each config is (index, config_settings())
    configs = list(enumerate(sampler(elim_rate ** max(1, len(elim_sched)-1))))
    output = {index: {'settings': config.settings()} for index, config in configs}
    for trace in eval_traces:
        output[trace+' val'] = []
        output[trace+' test'] = []
    output['eval step'] = []
    changed = {index: True for index, _ in configs}

    #evaluate fedex object with best score across all time
    best_score = 100.0
    best_config = None 
    best_config_idx = None

    start, last_start, used = 0, 0, 0
    for i, stop in enumerate(elim_sched):
        if len(configs) == 1 and not last_round is None:
            getattr(configs[0][1], last_round)()
        scores = []
        for j, (index, config) in enumerate(configs):
            scores.append(float('inf'))
            for k in range(start, stop):
                changed[index] = True
                print('\r\tRound', frac(i+1, len(elim_sched)), 
                      'config', frac(j+1, len(configs)), 
                      'step', frac(k+1, stop), end=4*' ')
                config.step()
                for trace in traces:
                    output[index][trace] = config.trace(trace)
                    print(trace+':', round(output[index][trace][-1], 4), end=4*' ')
                    if logger:
                        logger.add_scalars(trace, {str(index): output[index][trace][-1]}, k+1)
                #use refine error, if eval_global use global error
                if eval_global:
                    scores[-1] = discounted_mean(output[index][eval_traces[1]][start:],val_discount)
                else:
                    scores[-1] = discounted_mean(output[index][eval_traces[0]][start:], val_discount)
                used += 1
                current_best, score = min(enumerate(scores), key=itemgetter(1))

                #check if val score beats best score so far
                if score < best_score: 
                    best_config_idx = configs[current_best][0]
                    best_config = deepcopy(configs[current_best][1])
                    best_score = score 

                print('best:', round(best_score, 4), end=8*' ')

                for trace in eval_traces:
                    if len(output[best_config_idx][trace][start:])==0:
                        val = discounted_mean(output[best_config_idx][trace][last_start:], val_discount)
                    else:
                        val = discounted_mean(output[best_config_idx][trace][start:], val_discount)
                    output[trace+' val'].append(val)
                    if logger:
                        logger.add_scalar(trace+' val', val, used)
                if used in eval_sched:
                    if changed[best_config_idx]:
                        results = best_config.test(**kwargs)
                        changed[best_config_idx] = False
                    print('\r\tStep', used, 'test error', end='')
                    for trace in reversed(eval_traces):
                        output[trace+' test'].append(results[trace])
                        if logger:
                            logger.add_scalar(trace+' test', results[trace], used)
                        print('    '+trace, round(results[trace], 4), end='')
                    print(64*' ')
                    output['eval step'].append(eval_sched.pop(eval_sched.index(used)))
        if len(configs) == 1:
            break

        #select top n configs, index of each config is preserved
        _, configs = zip(*nsmallest(int(len(configs) / elim_rate), 
                                    zip(scores, configs), 
                                    key=itemgetter(0)))
        last_start = start 
        start = stop

    #best, config = configs[0]
    best, config = best_config_idx, best_config 
    output['best'], output[best]['settings'] = best, config.settings()
    if eval_sched:
        if changed[best]:
            results = config.test(**kwargs)
        print('\r\tStep', used, 'test error', end='')
        for trace in reversed(eval_traces):
            output[trace+' test'].append(results[trace])
            if logger:
                logger.add_scalar(trace+' test', results[trace], used)
            print('    '+trace, round(results[trace], 4), end='')
        print(64*' ')
        output['eval step'].append(used)

    if logger:
        with open(os.path.join(logdir, 'results.pkl'), 'wb') as f:
            pickle.dump(output, f)
        try:
            logger.flush()
        except AttributeError:
            pass
    return config



def wrapped_fedex(
                  get_server,
                  get_client,
                  num_configs=1,
                  prod=False,
                  stepsize_init='auto', 
                  stepsize_sched='aggressive', 
                  cutoff=1E-4, 
                  baseline_discount=-1.0, 
                  diff=False,
                  mle=False, 
                  logdir=None,
                  val_discount=0.0, 
                  last_stop=False,
                  eval_global=False,
                  **kwargs,
                  ):
    '''evaluates FedEx wrapped with successive elimination algorithm;
       uses FedAvg when num_configs = 1 and prod = False
    Args:
        get_server: function that takes no input and returns an object that can be passed as the 
                    first argument to FedEx.__init__, e.g. a Server object
        get_client: function that takes no input and returns a dict of local training configs, a
                    list of which is passed as the second argument to 'FedEx.__init__'; can also
                    return a dict of (string, list) pairs to be passed directly to 'FedEx.__init__'
        num_configs: determines number of configs in the list passed to 'FedEx.__init__':
                     - >0: use this value directly
                     - =0: value drawn at random from Unif[1, number of arms given by the wrapper]
                     - =-1: use the number of arms given by the wrapper
                     - else: value drawn at random from Unif{1, ..., abs(num_configs)}
        prod: run FedEx over a product set of single-parameter grids; must be 'True' in the case
                  when 'get_client' returns an object to be passed directly to 'FedEx.__init__'
        stepsize_init: passed to 'eta0' kwarg of 'FedEx.__init__'
        stepsize_sched: passed to 'sched' kwarg of 'FedEx.__init__'
        baseline_discount: determines 'baseline' kwarg of 'FedEx.__init__':
                           - >0.0: use this value directly
                           - else: value drawn at random from Unif[0.0, abs(baseline_discount)]
        diff: passed to 'diff' kwarg of 'FedEx.__init__'
        mle: passed to 'mle' kwarg of 'FedEx.test' via the kwargs of 'successive_elimination'
        logdir: passed to 'logdir' kwarg of 'successive_elimination'
        val_discount: passed to 'val_discount' kwarg of 'successive_elimination'
        last_stop: if True sets 'last_round' kwarg of 'successive_elimination' to 'stop'
        kwargs: passed to 'get_schedule'
    Returns:
        FedEx object
    '''

    elim_rate, elim_sched, eval_sched = get_schedule(**kwargs)
    print('Wrapping with', 'random search' if len(elim_sched) == 1 else 'successive elimination')

    if num_configs < -1:
        samples = lambda n: random.randint(1, -num_configs)
    elif num_configs == -1:
        samples = lambda n: n
    elif num_configs == 0:
        samples = lambda n: random.randint(1, n)
    else:
        samples = lambda n: num_configs

    if baseline_discount < 0.0:
        baseline = lambda: random.uniform(0.0, -baseline_discount)
    else:
        baseline = lambda: baseline_discount

    def sampler(n):

        for _ in range(n):
            yield FedEx(
                        get_server(), 
                        get_client() if prod else get_client(samples(n)),
                        eta0=stepsize_init, 
                        sched=stepsize_sched, 
                        cutoff=cutoff, 
                        baseline=baseline(),
                        diff=diff,
                        )

    return successive_elimination(
                                  sampler, 
                                  ['refine', 'global'], 
                                  logdir=logdir, 
                                  val_discount=val_discount,
                                  elim_rate=elim_rate, 
                                  elim_sched=elim_sched, 
                                  eval_sched=eval_sched,
                                  traces=['entropy', 'mle', 'global', 'refine'], 
                                  last_round='stop' if last_stop else None,
                                  mle=mle,
                                  eval_global=eval_global,
                                  )


def parse():

    parser = argparse.ArgumentParser()
    parser.add_argument('--input', default='*', help='parent directory of input logdirs')
    parser.add_argument('--output', default='.', help='output directory for tensorboard log')
    return parser.parse_args()


if __name__ == '__main__':

    args = parse()
    results = {}
    for fname in glob(os.path.join(args.input, '*/results.pkl')):
        with open(fname, 'rb') as f:
            key = '/'.join(fname.split('/')[:-1])
            results[key] = pickle.load(f)
    
    logger = SummaryWriter(args.output)
    for mode in ['global', 'refine']:
        for partition in ['val', 'test']:
            trace = mode + ' ' + partition
            for j, scores in enumerate(zip(*(val[trace] for val in results.values()))):
                step = j+1 if partition == 'val' else results[key]['eval step'][j]
                logger.add_scalar('avg '+trace+' error', np.mean(scores), step)
                logger.add_scalar('std '+trace+' error', np.std(scores), step)
                logger.add_histogram(trace+' error', np.array(scores), step)
            if partition == 'test':
                print('Average final '+trace+' error:', np.mean(scores))
                print('Standard deviation', np.std(scores))
    try:
        logger.flush()
    except AttributeError:
        pass

File Path: shakespeare.py
Content:
import argparse
import json
import os
import pdb
import pickle
import random
import re
import string
from collections import defaultdict
from glob import glob
from copy import deepcopy
import numpy as np
import torch; #torch.backends.cudnn.deterministic = False; 
torch.backends.cudnn.benchmark = True
from torch import nn
from torch import optim
from torch.nn.utils import rnn
from hyper import wrapped_fedex
from hyper import Server


BATCH = 100
CHARMAP = defaultdict(lambda: 1)
CHARMAP.update({char: i+2 for i, char in enumerate(string.printable)})
VOCAB = len(set(CHARMAP.values())) + 1
DATA = './data/shakespeare/'
SERVER = lambda: {
                  'lr': 10.0 ** np.random.uniform(low=-1.0, high=1.0),
                  'momentum': np.random.choice([0.0, 0.9]),
                  'step': 1,
                  'gamma': 1.0 - 10.0 ** np.random.uniform(low=-4.0, high=-2.0),
                  }
CLIENT = lambda: {
                  'lr': 10.0 ** np.random.uniform(low=-4.0, high=0.0),
                  'momentum': np.random.uniform(low=0.0, high=1.0),
                  'weight_decay': 10.0 ** np.random.uniform(low=-5.0, high=-1.0),
                  'epochs': 1, 
                  'batch': 2 ** np.random.choice(np.arange(3, 8)),
                  'mu': 10.0 ** np.random.uniform(low=-5.0, high=0.0),
                  'dropout': np.random.uniform(low=0.0, high=0.5),
                  }


def parse():

    parser = argparse.ArgumentParser()
    parser.add_argument('logdir')
    parser.add_argument('--seed', default=0, type=int, help='random seed')
    parser.add_argument('--debug', default=0, type=int,
                        help='run in DEBUG mode if >0; sets number of clients and batches')

    # wrapper algorithm settings
    parser.add_argument('--rounds', default=800, type=int, 
                        help='maximum number of communication rounds')
    parser.add_argument('--total', default=4000, type=int, 
                        help='total number of communication rounds')
    parser.add_argument('--rate', default=3, type=int, help='elimination rate')
    parser.add_argument('--elim', default=0, type=int, help='number of elimination rounds')
    parser.add_argument('--eval', default=1, type=int, help='number of evaluation rounds')
    parser.add_argument('--discount', default=0.0, type=float,
                        help='discount factor for computing the validation score of an arm')

    # FedEx settings
    parser.add_argument('--batch', default=10, type=int, help='number of tasks per round')
    parser.add_argument('--configs', default=1, type=int,
                        help='''number of configs to optimize over with FedEx (use 1 for FedAvg):
                                - <-1: sample a random number between 1 and abs(args.configs)
                                - =-1: sample the number of arms given by the wrapper
                                - =0: sample a random number between 1 and the number of arms
                                - >0: sample the provided number, ignoring the number of arms''')
    parser.add_argument('--lr_only', action='store_true', help='tune only learning rate as a hyperparameter')
    parser.add_argument('--eps', default=0.1, type=float, help='multiplicative perturbation to client config, eps=0 is fedavg')
    parser.add_argument('--uniform', action='store_true',
                        help='run FedEx over a product set of single-parameter uniform grids')
    parser.add_argument('--random', action='store_true',
                        help='run FedEx over a product set of single-parameter random grids')
    parser.add_argument('--eta0', default=0.0, type=float, 
                        help='FedEx initial step size; if 0.0 uses FedEx default')
    parser.add_argument('--sched', default='aggressive', type=str, help='FedEx step size sched')
    parser.add_argument('--cutoff', default=0.0, type=float, 
                        help='stop updating FedEx config distribution if entropy below this cutoff')
    parser.add_argument('--baseline', default=-1.0, type=float,
                        help='''how FedEx computes the baseline:
                                - >=-1.0,<0.0: sample discount factor from [0.0, abs(args.baseline))
                                - =0.0: use the most recent value
                                - >0.0,<1.0: use geometrically discounted mean with this factor
                                - =1.0: use the mean of all values''')
    parser.add_argument('--diff', action='store_true', 
                        help='use difference between refine and global as FedEx objective')
    parser.add_argument('--stop', action='store_true',
                        help='stop updating FedEx config distribution after last elimination')

    # evaluation settings
    parser.add_argument('--mle', action='store_true', help='use MLE config at test time')
    parser.add_argument('--loss', action='store_true', help='use loss instead of error')
    parser.add_argument('--eval_global', action='store_true', help='use global error as elimination metric instead of refine')

    # data settings
    parser.add_argument('--length', default=80, type=int, help='sequence length')
    parser.add_argument('--iid', action='store_true', help='client data is iid')
    parser.add_argument('--val', default=0.1, type=float, help='val proportion')
    parser.add_argument('--test', default=0.1, type=float, help='test proportion')

    # model settings
    parser.add_argument('--hidden', default=256, type=int, help='number of hidden units in LSTM')
    parser.add_argument('--layers', default=2, type=int, help='number of layers in LSTM')

    return parser.parse_args()


def line2data(text, length=80):

    text = re.sub('  *', ' ', text.replace('\n', ' '))
    return [[text[i:i+length], text[i+length]] for i in range(len(text)-length)]


def file2tensor(fname, length=80):

    with open(fname, 'r') as f:
        data = line2data(f.read(), length=length)
    X, Y = torch.zeros(len(data), length).long(), torch.empty(len(data)).long()
    for i, (x, y) in enumerate(data):
        for j, char in enumerate(x):
            X[i,j] = CHARMAP[char]
        Y[i] = CHARMAP[y]
    return X, Y


def get_loader(task, val=0.1, test=0.1, iid=False):
    m = len(torch.load(task)['Y'])
    if m < 1.0 / min(1.0-val-test, val, test):
        return None
    partitions = {'train': (0, int((1.0-val-test)*m)), 
                  'val': (int((1.0-val-test)*m), int((1.0-test)*m)), 
                  'test': (int((1.0-test)*m), m)}
    if iid:
        randperm = torch.randperm(m)
    dump = torch.load(task)
    X, Y = (dump['X'][randperm], dump['Y'][randperm]) if iid else (dump['X'], dump['Y'])
    data = {key: (X[a:b].pin_memory(), Y[a:b].pin_memory()) for key, (a, b) in partitions.items()}
    def loader(*args):
        output = []
        for arg in args:
            Xarg, Yarg = data[arg]
            output.append(Xarg.cuda(non_blocking=True))
            output.append(Yarg.cuda(non_blocking=True))
        return output
    return loader


class CharLSTM(nn.Module):

    def __init__(self, input_size=8, hidden_size=256, **kwargs):

        super().__init__()
        self.embedding = nn.Embedding(num_embeddings=VOCAB, 
                                      embedding_dim=input_size, padding_idx=0)
        self.lstm= nn.LSTM(input_size=input_size, hidden_size=hidden_size, 
                           batch_first=True, bidirectional=False, **kwargs)
        self.linear = nn.Linear(hidden_size, VOCAB)

    def forward(self, X, lengths):

        X = self.embedding(X)
        X = rnn.pack_padded_sequence(X, lengths, batch_first=True)
        self.lstm.flatten_parameters()
        X, _ = self.lstm(X)
        X, _ = rnn.pad_packed_sequence(X, batch_first=True)
        return self.linear(X[:,-1])


def get_lengths(X):

    lengths = X.shape[1] - (X == 0).sum(1)
    return lengths.sort(0, descending=True)


def get_prox(model, criterion=nn.CrossEntropyLoss(), mu=0.0):

    if not mu:
        return criterion

    mu *= 0.5
    model0 = [param.data.clone() for param in model.parameters()]

    def objective(*args, **kwargs):

        prox = sum((param-param0).pow(2).sum() 
                   for param, param0 in zip(model.parameters(), model0))
        return criterion(*args, **kwargs) + mu * prox

    return objective


def train(model, X, Y, batch=32, dropout=0.0, epochs=1, mu=0.0, **kwargs):

    optimizer = optim.SGD(model.parameters(), **kwargs)
    criterion = get_prox(model, mu=mu)
    model.lstm.dropout = dropout
    model.train()
    m = len(Y)
    for e in range(epochs):
        randperm = torch.randperm(m)
        X, Y = X[randperm], Y[randperm]
        for i in range(0, m, batch):
            Xbatch, Ybatch =X[i:i+batch], Y[i:i+batch]
            lengths, sortperm = get_lengths(Xbatch)
            pred = model(Xbatch[sortperm], lengths.cpu())
            loss = criterion(pred, Ybatch[sortperm])
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
    model.eval()
    return model


def test_batch(model, X, Y):

    lengths, sortperm = get_lengths(X)
    pred = model(X[sortperm], lengths.cpu())
    Yperm = Y[sortperm]
    return (Yperm != pred.argmax(1)).sum().float(), nn.CrossEntropyLoss(reduction='sum')(pred, Yperm).float()


def test(model, X, Y, batch=BATCH):

    model.eval()
    with torch.no_grad():
        errors, losses = zip(*(test_batch(model, X[i:i+batch], Y[i:i+batch]) 
                               for i in range(0, len(Y), batch)))
        return float(sum(errors)) / len(Y), float(sum(losses)) / len(Y)


def main():

    args = parse()
    random.seed(args.seed)
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)

    tasks = []
    for i, fname in enumerate(glob(os.path.join(DATA, '*.txt'))):
        task = os.path.join(DATA, fname.split('/')[-1][:-4] + '-' + str(args.length) + 'char.pt')
        if not os.path.isfile(task):
            print('\rCaching task', i+1, end='')
            dump = {}
            dump['X'], dump['Y'] = file2tensor(fname, length=args.length)
            torch.save(dump, task)
        loader = get_loader(task, val=args.val, test=args.test, iid=args.iid)
        if not loader is None:
            tasks.append(loader)
    if args.debug:
        tasks = tasks[:args.debug]
        print('DEBUG MODE')
    print('\rCompleted cache-check')

    def local_train(model, X, Y, **kwargs):

        if args.debug:
            return train(model, X[:args.debug*args.batch], Y[:args.debug*args.batch], **kwargs)
        return train(model, X, Y, **kwargs)

    def local_test(model, X, Y, **kwargs):

        return test(model, X, Y, **kwargs)[args.loss]

    def get_server():

        model = CharLSTM(hidden_size=args.hidden, num_layers=args.layers)
        return Server(model, tasks, local_train, local_test, batch=args.batch, **SERVER())


    def get_client(n_clients=1):
        '''performs local tuning for each hyperparameter'''
        if args.lr_only:
            return [SIMPLE_CLIENT()]

        initial_client = CLIENT()
        client_arr = [initial_client]
        eps = args.eps

        for i in range(n_clients-1):
            other_client = deepcopy(initial_client)
            
            log_lr = np.log10(other_client['lr'])
            other_client['lr'] = 10 ** np.clip(log_lr + np.random.uniform(-eps * 4, eps * 4), -4.0, 0.0)
            
            other_client['momentum'] = np.clip(initial_client['momentum'] + np.random.uniform(-eps, eps), 0, 1.0)
            
            log_wd = np.log10(other_client['weight_decay'])
            other_client['weight_decay'] = 10 ** np.clip(log_wd + np.random.uniform(-eps*4, eps*4),-5.0, -1.0)

            log_batch = int(np.log2(other_client['batch']))
            other_client['batch'] = 2 ** np.clip(np.random.choice([log_batch-1, log_batch, log_batch+1]), 3, 7)
            
            log_mu = np.log10(other_client['mu'])
            other_client['mu'] = 10 ** np.clip(log_mu + np.random.uniform(-eps*5, eps*5), -5.0 , 0.0)
            
            other_client['dropout'] = np.clip(initial_client['dropout'] + np.random.uniform(-eps*0.5, eps*0.5),0, 0.5)

            client_arr.append(other_client)

        return [UNIFORM()] if args.uniform else [RANDOM()] if args.random else client_arr


    print('Tuning',
          'FedAvg' if args.configs == 1 and not (args.uniform or args.random) else 'FedEx',
          'on Shakespeare')
    os.makedirs(args.logdir, exist_ok=True)
    with open(os.path.join(args.logdir, 'args.json'), 'w') as f:
        json.dump(vars(args), f, indent=4)
    wrapped_fedex(
                  get_server, 
                  get_client, 
                  num_configs=args.configs,
                  prod=args.uniform or args.random,
                  stepsize_init=args.eta0 if args.eta0 else 'auto',
                  stepsize_sched=args.sched,
                  cutoff=args.cutoff, 
                  baseline_discount=args.baseline,
                  diff=args.diff, 
                  mle=args.mle, 
                  logdir=args.logdir, 
                  val_discount=args.discount,
                  last_stop=args.stop,
                  max_resources=args.rounds, 
                  total_resources=args.total,
                  elim_rate=args.rate, 
                  num_elim=args.elim,
                  num_eval=args.eval,
                  eval_global=args.eval_global
                  )


if __name__ == '__main__':

    main()

Output:
{
    "experimental_code": "class FedEx:\n    '''runs hyperparameter optimization given a federated learning server'''\n\n    def entropy(self):\n\n        entropy = 0.0\n        for probs in product(*(theta[theta>0.0] for theta in self._theta)):\n            prob = np.prod(probs)\n            entropy -= prob * np.log(prob)\n        return entropy\n\n    def mle(self):\n    \n        return np.prod([theta.max() for theta in self._theta])\n\n    def __init__(\n                 self, \n                 server, \n                 configs, \n                 eta0='auto', \n                 sched='auto', \n                 cutoff=0.0, \n                 baseline=0.0, \n                 diff=False,\n                 ):\n        '''\n        Args:\n            server: Object that implements two methods, 'communication_round' and 'full_evaluation'\n                    taking as input a single argument, 'get_config', itself a function that takes \n                    no inputs and outputs an element of the provided list 'configs'. \n                    - 'communication_round' samples a batch of clients, assigns a config to each \n                    using 'get_config', and runs local training using that config. It then \n                    aggregates the local models to to take a training step and returns three lists \n                    or arrays: a list of each client's validation error before local training, a \n                    list of each client's validation error after local training, and a list of each \n                    client's weight (e.g. size of its validation set). \n                    - 'full_evaluation' assigns a config to each client using 'get_config' and runs\n                    local training using that config. It then returns three lists or arrays: a list\n                    of each client's test error before local training, a list of each client's test\n                    error after local training, and a list of each client's weight (e.g. size of \n                    its test set).\n            configs: list of configs used for local training and testing by 'server' \n                     OR dict of (string, list) pairs denoting a grid of configs\n            eta0: base exponentiated gradient step size; if 'auto' uses sqrt(2*log(len(configs)))\n            sched: learning rate schedule for exponentiated gradient:\n                    - 'adaptive': uses eta0 / sqrt(sum of squared gradient l-infinity norms)\n                    - 'aggressive': uses eta0 / gradient l-infinity norm\n                    - 'auto': uses eta0 / sqrt(t) for t the number of rounds\n                    - 'constant': uses eta0\n                    - 'scale': uses sched * sqrt(2 * log(len(configs)))\n            cutoff: entropy level below which to stop updating the config probability and use MLE\n            baseline: discount factor when computing baseline; 0.0 is most recent, 1.0 is mean\n            diff: if True uses performance difference; otherwise uses absolute performance\n        '''\n\n        self._server = server\n        self._configs = configs\n        self._grid = [] if type(configs) == list else sorted(configs.keys())\n\n        sizes = [len(configs[param]) for param in self._grid] if self._grid else [len(configs)]\n        self._eta0 = [np.sqrt(2.0 * np.log(size)) if eta0 == 'auto' else eta0 for size in sizes]\n        self._sched = sched\n        self._cutoff = cutoff\n        self._baseline = baseline\n        self._diff = diff\n        self._z = [np.full(size, -np.log(size)) for size in sizes]\n        self._theta = [np.exp(z) for z in self._z]\n\n        self._store = [0.0 for _ in sizes]\n        self._stopped = False\n        self._trace = {'global': [], 'refine': [], 'entropy': [self.entropy()], 'mle': [self.mle()]}\n\n    def stop(self):\n\n        self._stopped = True\n\n    def sample(self, mle=False, _index=[]):\n        '''samples from configs using current probability vector'''\n\n        if mle or self._stopped:\n            if self._grid:\n                return {param: self._configs[param][theta.argmax()] \n                        for theta, param in zip(self._theta, self._grid)}\n            return self._configs[self._theta[0].argmax()]\n        _index.append([np.random.choice(len(theta), p=theta) for theta in self._theta])\n\n        if self._grid:\n            return {param: self._configs[param][i] for i, param in zip(_index[-1], self._grid)}\n        return self._configs[_index[-1][0]]\n\n    def settings(self):\n        '''returns FedEx input settings'''\n\n        output = {'configs': deepcopy(self._configs)}\n        output['eta0'], output['sched'] = self._eta0, self._sched\n        output['cutoff'], output['baseline'] = self._cutoff, self._baseline \n        if self._trace['refine']:\n            output['theta'] = self.theta()\n        return output\n\n    def step(self):\n        '''takes exponentiated gradient step (calls 'communication_round' once)'''\n\n        index = []\n        before, after, weight = self._server.communication_round(lambda: self.sample(_index=index))        \n        before, after = np.array(before), np.array(after)\n        weight = np.array(weight, dtype=np.float64) / sum(weight)\n\n        if self._trace['refine']:\n            trace = self.trace('refine')\n            if self._diff:\n                trace -= self.trace('global')\n            baseline = discounted_mean(trace, self._baseline)\n        else:\n            baseline = 0.0\n        self._trace['global'].append(np.inner(before, weight))\n        self._trace['refine'].append(np.inner(after, weight))\n        if not index:\n            self._trace['entropy'].append(0.0)\n            self._trace['mle'].append(1.0)\n            return\n\n        for i, (z, theta) in enumerate(zip(self._z, self._theta)):\n            grad = np.zeros(len(z))\n            for idx, s, w in zip(index, after-before if self._diff else after, weight):\n                grad[idx[i]] += w * (s - baseline) / theta[idx[i]]\n            if self._sched == 'adaptive':\n                self._store[i] += norm(grad, float('inf')) ** 2\n                denom = np.sqrt(self._store[i])\n            elif self._sched == 'aggressive':\n                denom = 1.0 if np.all(grad == 0.0) else norm(grad, float('inf'))\n            elif self._sched == 'auto':\n                self._store[i] += 1.0\n                denom = np.sqrt(self._store[i])\n            elif self._sched == 'constant':\n                denom = 1.0\n            elif self._sched == 'scale':\n                denom = 1.0 / np.sqrt(2.0 * np.log(len(grad))) if len(grad) > 1 else float('inf')\n            else:\n                raise NotImplementedError\n            eta = self._eta0[i] / denom\n            z -= eta * grad\n            z -= logsumexp(z)\n            self._theta[i] = np.exp(z)\n\n        self._trace['entropy'].append(self.entropy())\n        self._trace['mle'].append(self.mle())\n        if self._trace['entropy'][-1] < self._cutoff:\n            self.stop()\n\n    def test(self, mle=False):\n        '''evaluates found config (calls 'full_evaluation' once)\n        Args:\n            mle: use MLE config instead of sampling\n        Returns:\n            output of 'full_evaluation'\n        '''\n\n        before, after, weight = self._server.full_evaluation(lambda: self.sample(mle=mle))\n        return {'global': np.inner(before, weight) / weight.sum(),\n                'refine': np.inner(after, weight) / weight.sum()}\n\n    def theta(self):\n        '''returns copy of config probability vector'''\n\n        return deepcopy(self._theta)\n\n    def trace(self, key):\n        '''returns trace of one of three tracked quantities\n        Args:\n            key: 'entropy', 'global', or 'refine'\n        Returns:\n            numpy vector with length equal to number of calls to 'step'\n        '''\n\n        return np.array(self._trace[key])\n\ndef wrapped_fedex(\n                  get_server,\n                  get_client,\n                  num_configs=1,\n                  prod=False,\n                  stepsize_init='auto', \n                  stepsize_sched='aggressive', \n                  cutoff=1E-4, \n                  baseline_discount=-1.0, \n                  diff=False,\n                  mle=False, \n                  logdir=None,\n                  val_discount=0.0, \n                  last_stop=False,\n                  eval_global=False,\n                  **kwargs,\n                  ):\n    '''evaluates FedEx wrapped with successive elimination algorithm;\n       uses FedAvg when num_configs = 1 and prod = False\n    Args:\n        get_server: function that takes no input and returns an object that can be passed as the \n                    first argument to FedEx.__init__, e.g. a Server object\n        get_client: function that takes no input and returns a dict of local training configs, a\n                    list of which is passed as the second argument to 'FedEx.__init__'; can also\n                    return a dict of (string, list) pairs to be passed directly to 'FedEx.__init__'\n        num_configs: determines number of configs in the list passed to 'FedEx.__init__':\n                     - >0: use this value directly\n                     - =0: value drawn at random from Unif[1, number of arms given by the wrapper]\n                     - =-1: use the number of arms given by the wrapper\n                     - else: value drawn at random from Unif{1, ..., abs(num_configs)}\n        prod: run FedEx over a product set of single-parameter grids; must be 'True' in the case\n                  when 'get_client' returns an object to be passed directly to 'FedEx.__init__'\n        stepsize_init: passed to 'eta0' kwarg of 'FedEx.__init__'\n        stepsize_sched: passed to 'sched' kwarg of 'FedEx.__init__'\n        baseline_discount: determines 'baseline' kwarg of 'FedEx.__init__':\n                           - >0.0: use this value directly\n                           - else: value drawn at random from Unif[0.0, abs(baseline_discount)]\n        diff: passed to 'diff' kwarg of 'FedEx.__init__'\n        mle: passed to 'mle' kwarg of 'FedEx.test' via the kwargs of 'successive_elimination'\n        logdir: passed to 'logdir' kwarg of 'successive_elimination'\n        val_discount: passed to 'val_discount' kwarg of 'successive_elimination'\n        last_stop: if True sets 'last_round' kwarg of 'successive_elimination' to 'stop'\n        kwargs: passed to 'get_schedule'\n    Returns:\n        FedEx object\n    '''\n\n    elim_rate, elim_sched, eval_sched = get_schedule(**kwargs)\n    print('Wrapping with', 'random search' if len(elim_sched) == 1 else 'successive elimination')\n\n    if num_configs < -1:\n        samples = lambda n: random.randint(1, -num_configs)\n    elif num_configs == -1:\n        samples = lambda n: n\n    elif num_configs == 0:\n        samples = lambda n: random.randint(1, n)\n    else:\n        samples = lambda n: num_configs\n\n    if baseline_discount < 0.0:\n        baseline = lambda: random.uniform(0.0, -baseline_discount)\n    else:\n        baseline = lambda: baseline_discount\n\n    def sampler(n):\n\n        for _ in range(n):\n            yield FedEx(\n                        get_server(), \n                        get_client() if prod else get_client(samples(n)),\n                        eta0=stepsize_init, \n                        sched=stepsize_sched, \n                        cutoff=cutoff, \n                        baseline=baseline(),\n                        diff=diff,\n                        )\n\n    return successive_elimination(\n                                  sampler, \n                                  ['refine', 'global'], \n                                  logdir=logdir, \n                                  val_discount=val_discount,\n                                  elim_rate=elim_rate, \n                                  elim_sched=elim_sched, \n                                  eval_sched=eval_sched,\n                                  traces=['entropy', 'mle', 'global', 'refine'], \n                                  last_round='stop' if last_stop else None,\n                                  mle=mle,\n                                  eval_global=eval_global,\n                                  )\n\ndef get_client(n_clients=1):\n    '''performs local tuning for each hyperparameter'''\n    # Example from cifar.py, similar in other dataset files\n    if args.lr_only:\n        return [SIMPLE_CLIENT()]\n\n    initial_client = CLIENT()\n    client_arr = [initial_client]\n    eps = args.eps\n\n    for i in range(n_clients-1):\n        other_client = deepcopy(initial_client)\n        \n        log_lr = np.log10(other_client['lr'])\n        other_client['lr'] = 10 ** np.clip(log_lr + np.random.uniform(4*-eps, 4*eps), -4.0, 0.0)\n        \n        other_client['momentum'] = np.clip(initial_client['momentum'] + np.random.uniform(-eps, eps), 0, 1.0)\n        \n        log_wd = np.log10(other_client['weight_decay'])\n        other_client['weight_decay'] = 10 ** np.clip(log_wd + np.random.uniform(4*-eps, 4*eps),-5.0, -1.0)\n        \n        epochs_range = math.ceil(eps * 4)\n        other_client['epochs'] = np.clip(np.random.choice(np.arange(initial_client['epochs']-epochs_range, initial_client['epochs']+epochs_range+1)), 1, 5)\n\n        log_batch = int(np.log2(other_client['batch']))\n        batch_range = math.ceil(eps * 4)\n        other_client['batch'] = 2 ** np.clip(np.random.choice(np.arange(log_batch-batch_range, log_batch+batch_range+1)), 3, 7)\n\n        \n        log_mu = np.log10(other_client['mu'])\n        other_client['mu'] = 10 ** np.clip(log_mu + np.random.uniform(5*-eps, 5*eps), -5.0 , 0.0)\n        \n        other_client['dropout'] = np.clip(initial_client['dropout'] + np.random.uniform(0.5*-eps, 0.5*eps),0, 0.5)\n\n        client_arr.append(other_client)\n\n    return [UNIFORM()] if args.uniform else [RANDOM()] if args.random else client_arr",
    "experimental_info": "The FedEx method tunes local training hyperparameters for personalized Federated Learning. The specific settings for the CIFAR-10 experiment are as follows:\n\n**1. Local Training Hyperparameter Search Space (Client Configuration):**\nFedEx optimizes over a categorical distribution of client configurations, which are generated by perturbing an initial random sample. The initial random sampling ranges for client hyperparameters are:\n-   **Learning Rate (lr):** Uniformly sampled from 10^(-4.0) to 10^(0.0) (log-uniform).\n-   **Momentum:** Uniformly sampled from 0.0 to 1.0.\n-   **Weight Decay:** Uniformly sampled from 10^(-5.0) to 10^(-1.0) (log-uniform).\n-   **Epochs:** Randomly chosen integer from 1 to 5.\n-   **Batch Size (batch):** Randomly chosen power of 2 from 2^3 to 2^7.\n-   **Proximal Term (mu):** Uniformly sampled from 10^(-5.0) to 10^(0.0) (log-uniform), used in FedProx (if mu > 0.0).\n-   **Dropout:** Uniformly sampled from 0.0 to 0.5.\n\nWhen `args.eps > 0.0`, additional client configurations are generated by perturbing the `initial_client`'s hyperparameters using `np.random.uniform(-eps * factor, eps * factor)` where `factor` varies per hyperparameter (e.g., 4 for log_lr, 1 for momentum, 5 for log_mu, 0.5 for dropout). For discrete parameters like epochs and batch size, a range `math.ceil(eps * 4)` is used for random choices.\n\n**2. FedEx Algorithm Settings:**\n-   `--configs`: Number of configurations to optimize over. Default is 1 (FedAvg). Can be set to sample a random number of configs or use the number of arms from the wrapper.\n-   `--eps`: Multiplicative perturbation factor (default 0.0 for FedAvg) applied during client config generation, enabling the local perturbation scheme.\n-   `--uniform`: Flag to run FedEx over a product set of single-parameter uniform grids.\n-   `--random`: Flag to run FedEx over a product set of single-parameter random grids.\n-   `--eta0`: Initial step size for exponentiated gradient updates (default 'auto').\n-   `--sched`: Step size schedule for exponentiated gradient ('aggressive', 'adaptive', 'auto', 'constant', 'scale'). Default is 'aggressive'.\n-   `--cutoff`: Entropy level below which FedEx stops updating the config probability distribution (default 0.0).\n-   `--baseline`: Discount factor for computing the baseline in exponentiated gradient (default -1.0, samples from [0.0, 1.0)).\n-   `--diff`: If set, uses the performance difference between refined and global models as the FedEx objective.\n-   `--stop`: If set, stops updating the FedEx config distribution after the last elimination round.\n-   `--mle`: If set, uses the Maximum Likelihood Estimate (MLE) configuration at test time.\n-   `--loss`: If set, uses loss instead of error as the evaluation metric.\n\n**3. Wrapper Algorithm (Successive Elimination) Settings:**\n-   `--rounds`: Maximum number of communication rounds (resources assigned to a single arm, default 800 for CIFAR-10).\n-   `--total`: Total number of communication rounds/resources (default 4000 for CIFAR-10).\n-   `--rate`: Elimination rate (multiplicative, default 3).\n-   `--elim`: Number of elimination rounds (default 0, runs random search if 0).\n-   `--eval`: Number of evaluation rounds (default 1).\n-   `--discount`: Discount factor for computing the validation score of an arm (default 0.0, uses most recent value).\n-   `--batch`: Number of clients sampled per communication round for FedEx (default 10).\n-   `--eval_global`: If set, uses global error as the elimination metric instead of the refined error.\n\n**4. Dataset and Model Settings (CIFAR-10 Specific):**\n-   **Dataset:** CIFAR-10. Input images are 32x32. Preprocessing includes random horizontal flip, random crop, color jitter, ToTensor, and normalization (mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]).\n-   **Model:** A Convolutional Neural Network (CNN) with three convolutional layers (32, 64, 64 filters respectively), ReLU activations, MaxPool2d, followed by a dropout layer and two fully connected layers (1024 to 64, then 64 to 10 for 10 classes).\n-   `--val`: Proportion of training data used for validation (default 0.2).\n-   `--num-clients`: Number of clients (default 500). Each client receives a partition of the CIFAR-10 dataset (50000 images for train, 10000 for test, split across clients)."
}
