
Input:
You are an accomplished researcher in machine learning. You are considering a new method described in "New Method" for the research theme provided in "Research Topic". "Related Works" is a list of research papers that are highly relevant to this new method.
Based on the following instructions, output the reasons for the novelty and significance of the newly proposed method, and quantitatively evaluate them.

# Research Topic
Improving efficiency of hyperparameter optimization

# New Method
{
    "Open Problems": "Current progressive multi-fidelity schedulers such as PASHA decide when to raise the maximum training budget by monitoring whether the ranking of the two top rungs has stabilised.  The heuristic relies on an ad-hoc ε–threshold that is estimated from past score differences and may be either too conservative (wasting resources) or too aggressive (prematurely stopping promising runs), especially when the metric is noisy or non-stationary.",
    "Methods": "Surrogate-Assisted PASHA (SA-PASHA)\n1. Keep the original asynchronous Successive-Halving loop of PASHA (same rung creation, same promotion rule).\n2. Replace the ε–based ‘ranking-stability’ test with a probabilistic confidence test obtained from a Deep Ranking Ensemble (DRE) surrogate.\n   2.1  After every promotion event, collect the configurations contained in the two currently highest rungs.\n   2.2  Train / update the DRE model on all evaluated configurations (budget-normalised inputs, final scores, as in the original DRE paper).\n   2.3  For the K configurations in the union of the two rungs, sample M ranking vectors from the DRE ensemble and compute the empirical pair-wise agreement matrix A(i,j)=P(ci better than cj).\n   2.4  Compute the average rank-confidence ρ =  (1/K) Σ_i ( |{ j : A(i,j)>0.5 }| / (K−1) ).  ρ≈1 indicates a very stable ranking.\n3. Decision rule: keep the current maximum budget T_max as long as ρ ≥ τ (τ=0.9 by default); otherwise double T_max exactly as in PASHA.  No hand-crafted ε is required.\n4. All other PASHA components (soft vs. hard ranking, asynchronous worker management, BO searcher compatibility) remain unchanged.",
    "Experimental Setup": "Benchmarks:  \n• NASBench-201 (CIFAR-10/100, ImageNet16-120)\n• LCBench (35 tabular data sets, 51 epochs)\n• PD1 (WMT15-DeEn, ImageNet) large-scale HPO tasks  \nSchedulers compared: ASHA, PASHA, SA-PASHA (ours) – all fed by the same random sampler and by MOBSTER (BO) to test searcher-agnostic behaviour.  \nResources: 4 parallel GPUs (NASBench), 4 CPU workers (LCBench/PD1).  Hyper-parameters: τ∈{0.8,0.9,0.95}; DRE ensemble size=10, list-wise loss, 100 meta-epochs per update.  5 random seeds.",
    "Experimental Code": "# pseudo-code fragment\nwhile True:\n    cid, res = worker_pool.wait_next_result()\n    pasha_state.update(cid, res)\n    if pasha_state.promotion_event():\n        top, prev = pasha_state.top_two_rungs()\n        X, y = pasha_state.all_evaluations()\n        dre.fit(X, y)                      # incremental update\n        ranks = dre.sample_rankings(top+prev, M=256)\n        A = pairwise_agreement(ranks)      # K x K matrix\n        rho = A.mean(dim=1).mean()\n        if rho < tau:\n            pasha_state.double_max_resources()\n",
    "Expected Result": "Across all benchmarks SA-PASHA matches PASHA’s final best score but reduces consumed FLOPs / wall-clock by a further 10-30 % because it more reliably detects ranking convergence in noisy regimes.  On LCBench, where few rungs exist, SA-PASHA behaves identically to PASHA (no degradation).  Surrogate update time adds <3 % overhead.",
    "Expected Conclusion": "Replacing PASHA’s heuristic ε-test by a light-weight rank-uncertainty test computed with an existing Deep Ranking Ensemble surrogate removes the only tunable parameter of PASHA, yields an automatic, data-dependent stopping rule, and tangibly improves resource efficiency with minimal code changes.  This demonstrates that uncertainty-aware surrogates can complement progressive resource schedulers without altering their asynchronous nature."
}

# Related Works
{
    "Title": "Deep Ranking Ensembles for Hyperparameter Optimization",
    "Main Contributions": "The paper introduces Deep Ranking Ensembles (DRE), a novel neural network Bayesian Optimization (BO) surrogate optimized with Learning-to-Rank (L2R) losses. It addresses the sub-optimal performance of existing Hyperparameter Optimization (HPO) methods that train surrogates as regression tasks, hypothesizing that preserving the ranks of hyperparameter configuration performances is a superior strategy. DRE meta-learns an ensemble of neural networks for ranking and models their uncertainty. This method, along with a new technique for meta-learning from large-scale public meta-datasets, achieves new state-of-the-art results in HPO across extensive experimental protocols.",
    "Methodology": "Deep Ranking Ensembles (DRE) utilize an ensemble of diverse neural networks, each trained to output a ranking score for hyperparameter configurations. These networks are optimized using a weighted list-wise L2R loss, which assigns higher penalties to top-performing configurations, emphasizing their correct rank prediction. Uncertainty estimation is achieved by training multiple diverse neural scorers with stochastic gradient descent and different random seeds. The method incorporates meta-learning by pre-training these rankers on evaluations from previous datasets and boosts transfer quality by integrating dataset meta-features, extracted using a Deep Set architecture, as additional input to the scorers. During Bayesian Optimization, the ensemble's posterior mean and variance of estimated ranks are used by acquisition functions like Expected Improvement to guide the search for the next optimal configuration.",
    "Experimental Setup": "Experiments were conducted on HPO-B, the largest public HPO benchmark, featuring 16 search spaces and 86 meta-test datasets/tasks. The DRE method was rigorously compared against 12 state-of-the-art HPO baselines, including both transfer and non-transfer algorithms. The evaluation protocol involved starting with 5 initial random configurations and performing 100 Bayesian Optimization iterations, with results averaged across 5 runs per task. Ablation studies were performed to analyze the impact of different L2R losses (point-wise, pair-wise, list-wise, and weighted list-wise), the utility of meta-features, various acquisition functions (Expected Improvement, Upper Confidence Bound, Average Rank), and different scorer network architectures (number of layers and neurons). The meta-feature extractor used a Deep Set architecture with 5 hidden layers and 32 neurons, while the ensemble consisted of 10 MLPs, each with 4 hidden layers and 32 neurons.",
    "Limitations": "Not mentioned",
    "Future Research Directions": "The authors suggest that the DRE surrogate opens up effective avenues to improve HPO performance in various sub-problems, specifically mentioning multi-fidelity HPO, multi-objective HPO, and neural architecture search.",
    "Experiment Code": "File Path: AIRBO/kernels/expected_rbf_kernel.py\nContent:\nfrom gpytorch.kernels.kernel import Kernel\nfrom typing import Optional, Tuple\nfrom gpytorch.priors import Prior\nfrom gpytorch.constraints import Interval\nimport torch\n\n\ndef postprocess_rbf(dist_mat):\n    return dist_mat.div_(-2).exp_()\n\n\ndef default_postprocess_script(x):\n    return x\n\n\nclass ExpectedRBFKernel(Kernel):\n    \"\"\"\n    Expected RBF kernel with close-form integral solution\n    \"\"\"\n    has_lengthscale = True\n\n    def __init__(self, ard_num_dims: Optional[int] = None,\n                 batch_shape: Optional[torch.Size] = torch.Size([]),\n                 active_dims: Optional[Tuple[int, ...]] = None,\n                 lengthscale_prior: Optional[Prior] = None,\n                 lengthscale_constraint: Optional[Interval] = None,\n                 eps: Optional[float] = 1e-6,\n                 **kwargs):\n        super(ExpectedRBFKernel, self).__init__(\n            ard_num_dims, batch_shape, active_dims,\n            lengthscale_prior, lengthscale_constraint, eps\n        )\n\n    def forward(self, x1, x2, diag=False, **params):\n        \"\"\"\n        We assume that x1 and x2 are Gaussian variables\n        :param x1: a tensor of M * 2D or B * M * 2D, the first D dimensions are the means and the rests are stds\n        :param x2: a tensor of N * 2D or B * M * 2D, the first D dimensions are the means and the rests are stds\n        :param diag: whether it only needs to compute the diagonal\n        :param params: configuration keywords\n        :return: a tensor of M * N\n        \"\"\"\n        B = x1.shape[:-2] if x1.ndim >= 3 else None\n        M = x1.shape[-2]\n        N = x2.shape[-2]\n        D = x1.shape[-1] // 2\n        dtype = x1.dtype\n        device = x1.device\n\n        x1_mean = x1[..., 0:D]\n        x1_var = x1[..., D:] ** 2.0\n        x2_mean = x2[..., 0:D]\n        x2_var = x2[..., D:] ** 2.0\n\n        Var1 = torch.diag_embed(x1_var)  # B * M * D * D\n        Var2 = torch.diag_embed(x2_var)  # B * N * D * D\n\n        if self.ard_num_dims is None:\n            ls_vec = self.lengthscale.repeat(1, D).view(-1)\n        else:\n            ls_vec = self.lengthscale.view(-1)\n        W = torch.diag(ls_vec ** 2.0)  # lengthscale in (D, D)\n\n        # numerator\n        AB = x1_mean.unsqueeze(-2) - x2_mean.unsqueeze(-3)  # AB=A-B: broadcast --> (B, M, N, D)\n        VAVB = Var1.unsqueeze(-3) + Var2.unsqueeze(-4)  # VAVB = VarA + VarB: (M, N, D, D)\n        Z = W.unsqueeze(0).unsqueeze(0) if B is None \\\n            else W.unsqueeze(0).unsqueeze(0).unsqueeze(\n            0) + VAVB  # Z = W + VarA + VarB: (M, N, D, D)\n        Z_inv = torch.inverse(Z)  # (M, N, D, D)\n\n        ABZ_eq = 'mnpd,mndd->mnpd' if B is None else 'bmnpd,bmndd->bmnpd'\n        ABZ = torch.einsum(ABZ_eq, [AB.unsqueeze(-2), Z_inv])  # (A-B)Z^-1, (M, N, 1, D)\n        nu_eq = 'mnpd,mndq->mnpq' if B is None else 'bmnpd,bmndq->bmnpq'\n        nu = torch.einsum(nu_eq, [ABZ, AB.unsqueeze(-1)]).squeeze(-1).squeeze(\n            -1)  # (A-B)Z^-1(A-B)^T:(M, N, 1, 1)\n\n        # denominator\n        x1_eq_x2 = torch.equal(x1, x2)\n        if diag:\n            # Special case the diagonal because we can return all zeros most of the time.\n            if x1_eq_x2:\n                res = torch.zeros(*x1.shape[:-1], x1.shape[-2], dtype=x1.dtype, device=x1.device)\n                res = postprocess_rbf(res)\n                return res\n            else:\n                res = torch.norm(x1 - x2, p=2, dim=-1)\n                res = res.pow(2)\n                res = postprocess_rbf(res)\n                return res\n        else:\n            W_inv = W.inverse().unsqueeze(0).unsqueeze(0)\n            if B is not None:\n                W_inv = W_inv.unsqueeze(0)\n            de_coeff = (W_inv.matmul(VAVB)).det().sqrt()\n            if x1_eq_x2:\n                # if the x1 == x2, then it needs separate computations for the diagonal\n                # and non-diagonal elements, see the paper appendix C for the details.\n                zero_diag_identity = (torch.ones((M, M), dtype=dtype).to(device) -\n                                      torch.eye(M, M, dtype=dtype).to(device))\n                one_diag = torch.eye(M, M, dtype=dtype).to(device)\n                if B is not None:\n                    zero_diag_identity = zero_diag_identity.unsqueeze(0)\n                    one_diag = one_diag.unsqueeze(0)\n                de = de_coeff * zero_diag_identity + one_diag\n            else:\n                de = de_coeff\n\n            covar = nu / de\n\n            return postprocess_rbf(covar)\n\nFile Path: AIRBO/kernels/kme_kernel.py\nContent:\n\"\"\"\nKernel-mean-embedding Kernel defined in uGP-UCB:\n    K(X, X`) = \\int_x \\int_x` k(x, x`) dP(x) dP(x`)\n\"\"\"\nfrom gpytorch.kernels.kernel import Kernel\nimport gc\nfrom utils import commons as cm\nfrom functools import partial\nimport torch\n\n\ndef postprocess_linear(dist_mat):\n    sym_dist_mat = ((dist_mat + dist_mat.transpose(-2, -1)) * 0.5)\n    return sym_dist_mat\n\n\ndef integral_KME_batch(X, Y, kernel: Kernel):\n    \"\"\"\n    Estimate the KME via sampling and integral\n    :param X: B * M * C * D tensor, B is the batch size, M is the sample size,\n              C is the sampling size, and D is the data dim.\n    :param Y: B * N * H * D tensor, B is the batch size, N is the sample size,\n              H is the sampling size, and D is the data dim.\n    :param kernel: A gpytorch.kernel.Kernel instance\n    :return: a tensor of M * N\n    \"\"\"\n    dist_mat = None\n    try:\n        B = X.shape[:-3]\n        M, C, D = X.shape[-3:]\n        N, H, D = Y.shape[-3:]\n        _original_shape = kernel.batch_shape\n        kernel.batch_shape = torch.Size([M, N])\n        dist_mat = kernel(\n            X.unsqueeze(-3),  # cast into B * M * 1 * m * D\n            Y.unsqueeze(-4)  # cast into B * 1 * N * m * D\n        ).evaluate()  # B * M * N * m * m\n\n        kme = dist_mat.mean(dim=[-1, -2])\n        kernel.batch_shape = _original_shape\n    finally:\n        cm.free_memory(\n            [\n                dist_mat,\n            ],\n            debug=False\n        )\n\n    return kme\n\n\ndef estimate_KME_in_chunks(X, Y, estimator, chunk_size=10):\n    \"\"\"\n    Estimate the KME in chunks, decrease the chunk size if GPU memory error happens\n    :param X: B * M * C * D or M * C * D tensor\n    :param Y: B * N * H * D or N * H * D tensor\n    :param estimator:\n    :param chunk_size:\n    :return:\n    \"\"\"\n    N = Y.shape[-3]\n\n    retry_i = 0\n    success = False\n    _practicable_chunk_size = chunk_size\n    kme_results = None\n    while not success:\n        kme_results = []\n        _Y = None\n        _kme = None\n        try:\n            _practicable_chunk_size = max(chunk_size // (2 ** retry_i), 1)\n            for bs in range(0, N, _practicable_chunk_size):\n                be = min(N, bs + _practicable_chunk_size)\n                _Y = Y[..., bs:be, :, :]\n                _kme = estimator(X, _Y)\n                kme_results.append(_kme)\n\n                gc.collect()\n                torch.cuda.empty_cache()\n            success = True\n        except RuntimeError as e:\n            if 'CUDA out of memory' in e.args[0] or 'not enough memory' in e.args[0]:\n                if _practicable_chunk_size > 1:  # we can still try to reduce the chunk size\n                    print(f'Chunk size {_practicable_chunk_size} is too large, '\n                          f'reduce it by a half:', e)\n                    retry_i += 1\n\n                    if len(kme_results) > 0:\n                        cm.free_memory(kme_results)\n                        for _m in kme_results:\n                            del _m\n                        del kme_results\n                    if '_kme' in locals():\n                        cm.free_memory([_kme])\n                    if '_Y' in locals():\n                        cm.free_memory([_Y])\n                else:\n                    raise ValueError('Chunk size has been reduced to 1 but still out of memory, '\n                                     'try cpu.')\n            else:\n                raise e\n        finally:\n            gc.collect()\n            torch.cuda.empty_cache()\n\n    kme = torch.concat(kme_results, dim=-1)  # concat to columns as we chuck the Y\n    return kme\n\n\nclass KMEKernel(Kernel):\n    \"\"\"\n    Decorating an existing kernel with KME\n    \"\"\"\n\n    has_lengthscale = True\n\n    def __init__(self, base_kernel, **kwargs):\n        super(KMEKernel, self).__init__(**kwargs)\n        self.base_kernel = base_kernel\n        self.chunk_size = kwargs.get('chunk_size', 100)\n        self.estimator = kwargs.get('estimator', 'integral')\n        self.estimation_trials = kwargs.get('estimation_trials', 1)\n        if self.estimator == 'integral':\n            self.estimation_func = partial(\n                integral_KME_batch, kernel=self.base_kernel\n            )\n        else:\n            raise ValueError('Unsupported estimator name', self.estimator)\n\n    @property\n    def is_stationary(self) -> bool:\n        \"\"\"\n        Kernel is stationary if base kernel is stationary.\n        \"\"\"\n        return self.base_kernel.is_stationary\n\n    def compute_distance_covariance_matrix(self, x1, x2, diag=False, **params):\n        chunk_size = params.get('chunk_size', self.chunk_size)\n        avg_dist_mat = None\n        for _ in range(self.estimation_trials):\n            dist_mat = estimate_KME_in_chunks(x1, x2, self.estimation_func, chunk_size)\n            avg_dist_mat = dist_mat if avg_dist_mat is None else avg_dist_mat + dist_mat\n        avg_dist_mat = avg_dist_mat / self.estimation_trials\n        cov_mat = postprocess_linear(avg_dist_mat.div(self.lengthscale ** 2.0))\n        return avg_dist_mat, cov_mat\n\n    def forward(self, x1, x2, diag=False, **params):\n        avg_dist_mat, cov_mat = self.compute_distance_covariance_matrix(x1, x2, diag, **params)\n        return cov_mat\n\nFile Path: AIRBO/kernels/mmd_kernel.py\nContent:\n\"\"\"\nMaximum Mean Discrepancy (MMD) Kernel\n\"\"\"\nimport gpytorch as gpyt\nfrom gpytorch.kernels.kernel import Kernel\nimport torch\nfrom tqdm.auto import trange\nimport gc\nfrom utils import commons as cm\nfrom functools import partial\n\n\ndef additive_RQ_kernel(alphas=(0.2, 0.5, 1, 2, 5), ls=1.0, learnable_ls=False):\n    assert len(alphas) > 0\n    _k_list = []\n    for a in alphas:\n        _k = gpyt.kernels.RQKernel()\n        _k.alpha = a\n        _k.lengthscale = ls\n        _k.raw_lengthscale.require_grad = learnable_ls\n        _k_list.append(_k)\n    k = gpyt.kernels.AdditiveKernel(*_k_list)\n    return k\n\n\ndef combo_kernel(alphas=(0.2, 0.5, 1, 2, 5), ls=1.0, learnable_ls=False):\n    assert len(alphas) > 0\n    _k_list = []\n    for a in alphas:\n        _k = gpyt.kernels.RQKernel()\n        _k.alpha = a\n        _k.lengthscale = ls\n        _k.raw_lengthscale.require_grad = learnable_ls\n        _k_list.append(_k)\n    _k_list.append(gpyt.kernels.LinearKernel())\n    k = gpyt.kernels.AdditiveKernel(*_k_list)\n    return k\n\n\ndef postprocess_mmd(dist_mat):\n    sym_dist_mat = (dist_mat + dist_mat.T) * 0.5\n    _dist_mat = sym_dist_mat.clamp_(min=0) ** 0.5\n    return torch.clamp(1.0 - _dist_mat, 0.0, 1.0)\n\n\ndef postprocess_mmd_rbf(dist_mat):\n    _dist_mat = ((dist_mat + dist_mat.transpose(-2, -1)) * 0.5)\n    return _dist_mat.div_(-2).exp_()\n\n\ndef nystrom_mmd(X, Y, kernel: Kernel, sub_samp_size: int = 100):\n    \"\"\"\n    nystrom estimator for MMD2\n    :param X: B * D tensor\n    :param Y: H * D tensor\n    :param kernel: gpytorch.kernel.Kernel instance\n    :param sub_samp_size: the subsample number\n    :return: a scalar\n    \"\"\"\n    # sub-sampling\n    B, D = X.shape\n    x_sub_inds = torch.randperm(B)[:sub_samp_size]\n    X_sub = X[x_sub_inds, :]\n\n    H, D = Y.shape\n    y_sub_inds = torch.randperm(H)[:sub_samp_size]\n    Y_sub = Y[y_sub_inds, :]\n\n    # compute alpha_x\n    k_m_x = kernel(X_sub, X_sub).evaluate()\n    k_m_x_inv = torch.linalg.pinv(k_m_x)\n    k_mn_x = kernel(X_sub, X).evaluate()\n    alpha_x = (k_m_x_inv @ k_mn_x @ torch.ones(X.shape[0], 1).type(X.dtype).to(X.device)) \\\n              / X.shape[0]\n\n    # compute alpha_y\n    k_m_y = kernel(Y_sub, Y_sub).evaluate()\n    k_m_y_inv = torch.linalg.pinv(k_m_y)\n    k_mn_y = kernel(Y_sub, Y).evaluate()\n    alpha_y = (k_m_y_inv @ k_mn_y @ torch.ones(Y.shape[0], 1).type(Y.dtype).to(Y.device)) \\\n              / Y.shape[0]\n\n    # nystrom estimator\n    part1 = alpha_x.T @ k_m_x @ alpha_x\n    part2 = alpha_y.T @ k_m_y @ alpha_y\n    part3 = alpha_x.T @ kernel(X_sub, Y_sub).evaluate() @ alpha_y * -2\n\n    mmd2 = part1 + part2 + part3\n\n    return mmd2\n\n\ndef nystrom_mmd_batch(X, Y, kernel: Kernel, sub_samp_size: int = 100):\n    \"\"\"\n    nystrom estimator for MMD2 in batches\n    :param X: B * M * C * D tensor, B is the batch size, M is the sample size,\n              C is the sampling size, and D is the data dim.\n    :param Y: B * N * H * D tensor, B is the batch size, N is the sample size,\n              H is the sampling size, and D is the data dim.\n    :param kernel: A gpytorch.kernel.Kernel instance\n    :param sub_samp_size: the subsampling size\n    :return: a tensor of M * N\n    \"\"\"\n    km_x_inv = None\n    kmn_x = None\n    ones_x = None\n    km_y_inv = None\n    kmn_y = None\n    ones_y = None\n    km_x, km_y = None, None\n    alpha_x, km_xy = None, None\n    alpha_y, X_sub, Y_sub = None, None, None\n    part1, part2, part3 = None, None, None\n    try:\n        B = X.shape[:-3]\n        M, C, D = X.shape[-3:]\n        N, H, D = Y.shape[-3:]\n        _original_shape = kernel.batch_shape\n\n        # sub-sampling\n        x_sub_inds = torch.randperm(C)[:sub_samp_size]\n        X_sub = X[..., x_sub_inds, :]  # M * m * D, m is the sub-sampling size\n\n        y_sub_inds = torch.randperm(H)[:sub_samp_size]\n        Y_sub = Y[..., y_sub_inds, :]  # N * m * D\n\n        # compute alpha for x variables\n        kernel.batch_shape = torch.Size([M])\n        km_x = kernel(X_sub, X_sub).evaluate()  # B * M * m * m\n        km_x_inv = torch.linalg.pinv(km_x)  # B * M * m * m\n        kmn_x = kernel(X_sub, X).evaluate()  # B * M * m * C\n        ones_x = torch.ones(M, C, 1).type(X.dtype).to(X.device)  # M * C * 1\n        alpha_x = (km_x_inv @ kmn_x @ ones_x) / C  # B * M * m * 1\n        # cm.free_memory([km_x_inv, kmn_x, ones_x], debug=False)\n\n        # compute alpha for y variables\n        kernel.batch_shape = torch.Size([N])\n        km_y = kernel(Y_sub, Y_sub).evaluate()  # B * N * m * m\n        km_y_inv = torch.linalg.pinv(km_y)  # B * N * m * m\n        kmn_y = kernel(Y_sub, Y).evaluate()  # B * N * m * H\n        ones_y = torch.ones(N, H, 1).type(Y.dtype).to(Y.device)  # M * H * 1\n        alpha_y = (km_y_inv @ kmn_y @ ones_y) / H  # B * N * m * 1\n        # cm.free_memory([km_y_inv, kmn_y, ones_y], debug=False)\n\n        # nystrom estimator\n        part1 = (alpha_x.transpose(-2, -1) @ km_x @ alpha_x).view(*B, M, 1)  # a_x^T * km_x * a_x\n        part2 = (alpha_y.transpose(-2, -1) @ km_y @ alpha_y).view(*B, 1, N)  # a_y^T * km_y * a_y\n        # cm.free_memory([km_x, km_y], debug=False)\n\n        kernel.batch_shape = torch.Size([M, N])\n        km_xy = kernel(X_sub.unsqueeze(-3),\n                       Y_sub.unsqueeze(-4)).evaluate()  # broadcast to M * N --> M * N * m * m\n        # a_x^T * km_xy * a_y\n        part3 = (alpha_x.unsqueeze(-3).transpose(-2, -1) @ km_xy @ alpha_y.unsqueeze(-4)).view(*B,\n                                                                                               M, N)\n\n        mmd2 = part1 + part2 - part3 * 2.0  # broadcast to M * n and add\n        # cm.free_memory([alpha_x, km_xy, alpha_y, X_sub, Y_sub, part1, part2, part3], debug=False)\n        kernel.batch_shape = _original_shape\n    finally:\n        cm.free_memory(\n            [\n                km_x_inv, kmn_x, ones_x,\n                km_y_inv, kmn_y, ones_y,\n                km_x, km_y,\n                alpha_x, km_xy, alpha_y, X_sub, Y_sub, part1, part2, part3,\n            ],\n            debug=False\n        )\n\n    return mmd2\n\n\ndef empirical_mmd(X, Y, kernel: Kernel):\n    \"\"\"\n    estimate the MMD\n    :param X: B*D tensor\n    :param Y: H*D tensor\n    :param kernel: gpytorch.kernels.Kernel\n    :return: a scalar\n    \"\"\"\n    # xx\n    cm_xx = kernel(X, X).evaluate()\n    avg_xx_mmd = (cm_xx.sum() - torch.diagonal(cm_xx).sum()) / (X.shape[0] * (X.shape[0] - 1))\n\n    # yy\n    cm_yy = kernel(Y, Y).evaluate()\n    avg_yy_mmd = (cm_yy.sum() - torch.diagonal(cm_yy).sum()) / (Y.shape[0] * (Y.shape[0] - 1))\n\n    # xy\n    cm_xy = kernel(X, Y).evaluate()\n    avg_xy_mmd = cm_xy.sum() / (X.shape[0] * Y.shape[0])\n\n    mmd = avg_xx_mmd + avg_yy_mmd - 2.0 * avg_xy_mmd\n\n    return mmd\n\n\ndef estimate_mmd_in_chunks(X, Y, estimator, chunk_size=10):\n    \"\"\"\n    estimate the MMD in chunks, decrease the chunk size if GPU memory error happens\n    :param X: B * M * C * D or M * C * D tensor\n    :param Y: B * N * H * D or N * H * D tensor\n    :param estimator:\n    :param chunk_size:\n    :return:\n    \"\"\"\n    N = Y.shape[-3]\n\n    retry_i = 0\n    success = False\n    _practicable_chunk_size = chunk_size\n    while not success:\n        try:\n            _practicable_chunk_size = max(chunk_size // (2 ** retry_i), 1)\n            mmd_results = []\n            for bs in range(0, N, _practicable_chunk_size):\n                be = min(N, bs + _practicable_chunk_size)\n                _Y = Y[..., bs:be, :, :]\n                _mmd = estimator(X, _Y)\n                mmd_results.append(_mmd)\n\n                gc.collect()\n                torch.cuda.empty_cache()\n            success = True\n        except RuntimeError as e:\n            if 'CUDA out of memory' in e.args[0] or 'not enough memory' in e.args[0]:\n                if _practicable_chunk_size > 1:  # we can still try to reduce the chunk size\n                    print(f'Chunk size {_practicable_chunk_size} is too large, '\n                          f'reduce it by a half:', e)\n                    retry_i += 1\n\n                    if len(mmd_results) > 0:\n                        cm.free_memory(mmd_results)\n                        for _m in mmd_results:\n                            del _m\n                        del mmd_results\n                    if '_mmd' in locals():\n                        cm.free_memory([_mmd])\n                    if '_Y' in locals():\n                        cm.free_memory([_Y])\n                else:\n                    raise ValueError('Chunk size has been reduced to 1 but still out of memory, '\n                                     'try cpu.')\n            else:\n                raise e\n        finally:\n            gc.collect()\n            torch.cuda.empty_cache()\n\n    mmd = torch.concat(mmd_results, dim=-1)  # concat to columns as we chuck the Y\n    return mmd\n\n\ndef empirical_mmd_batch(X, Y, kernel: gpyt.kernels.Kernel):\n    \"\"\"\n    Empirically estimate the MMD in batches\n    :param X: M * B * D tensor, M is the batch size, B is the sample size, D is the data dimension\n    :param Y: N * H * D tensor, N is the batch size, H is the sample size, D is the data dimension\n    :param kernel: the kernel to use\n    :return: a tensor of M*N\n    \"\"\"\n    B = X.shape[:-3]\n    M, C, D = X.shape[-3:]\n    N, H, D = Y.shape[-3:]\n    _original_shape = kernel.batch_shape\n\n    cm_xx = None\n    cm_yy = None\n    cm_xy = None\n    try:\n\n        # compute the covariance btw X and X\n        kernel.batch_shape = torch.Size([M])  # align along M, and compute kernel for each pair\n        cm_xx = kernel(X, X).evaluate()  # M * B * B\n        avg_xx_mmd = (cm_xx.sum((-2, -1)) - torch.diagonal(cm_xx, dim1=-2, dim2=-1).sum(-1)) \\\n                     / (C * (C - 1))  # M * 1\n\n        # compute the covariance btw Y and Y\n        kernel.batch_shape = torch.Size([N])  # align along N, and compute kernel for each pair\n        cm_yy = kernel(Y, Y).evaluate()  # N x H x H\n        avg_yy_mmd = (cm_yy.sum((-2, -1)) - torch.diagonal(cm_yy, dim1=-2, dim2=-1).sum(-1)) \\\n                     / (H * (H - 1))\n\n        # compute the covariance btw X and Y\n        kernel.batch_shape = torch.Size([M, N])  # make a grid of M * N, apply kernel on each pair\n        cm_xy = kernel(X.unsqueeze(-3), Y.unsqueeze(-4)).evaluate()  # broadcast to M * N, output M x N x B x H\n        avg_xy_mmd = cm_xy.sum((-2, -1)) / (C * H)\n\n        mmd2 = avg_xx_mmd.unsqueeze(-1) + avg_yy_mmd.unsqueeze(-2) - 2.0 * avg_xy_mmd  # broadcast and elementwise add\n\n    finally:\n        kernel.batch_shape = _original_shape\n        cm.free_memory(\n            [cm_xx, cm_yy, cm_xy, ],\n            debug=False\n        )\n    return mmd2\n\n\nclass MMDKernel(Kernel):\n    \"\"\"\n    Decorating an existing kernel with MMD distance\n    \"\"\"\n\n    has_lengthscale = True\n\n    def __init__(self, base_kernel, **kwargs):\n        super(MMDKernel, self).__init__(**kwargs)\n        self.base_kernel = base_kernel\n        self.chunk_size = kwargs.get('chunk_size', 100)\n        self.estimator = kwargs.get('estimator', 'nystrom')\n        self.sub_samp_size = kwargs.get('sub_samp_size', 100)\n        self.estimation_trials = kwargs.get('estimation_trials', 1)\n        if self.estimator == 'nystrom':\n            self.estimation_func = partial(\n                nystrom_mmd_batch, kernel=self.base_kernel, sub_samp_size=self.sub_samp_size\n            )\n        elif self.estimator == 'empirical':\n            self.estimation_func = partial(empirical_mmd_batch, kernel=self.base_kernel)\n        else:\n            raise ValueError('Unsupported estimator name', self.estimator)\n\n    # @property\n    # def lengthscale(self):\n    #     ls = None\n    #     if isinstance(self.base_kernel, gpyt.kernels.AdditiveKernel):\n    #         ls = torch.concat([_k.lengthscale for _k in self.base_kernel.kernels], dim=0)\n    #     else:\n    #         ls = self.base_kernel.lengthscale\n    #     return ls\n\n    @property\n    def is_stationary(self) -> bool:\n        \"\"\"\n        Kernel is stationary if base kernel is stationary.\n        \"\"\"\n        return self.base_kernel.is_stationary\n\n    def compute_distance_covariance_matrix(self, x1, x2, diag=False, **params):\n        chunk_size = params.get('chunk_size', self.chunk_size)\n        avg_dist_mat = None\n        for _ in range(self.estimation_trials):\n            dist_mat = estimate_mmd_in_chunks(x1, x2, self.estimation_func, chunk_size)\n            avg_dist_mat = dist_mat if avg_dist_mat is None else avg_dist_mat + dist_mat\n        avg_dist_mat = avg_dist_mat / self.estimation_trials\n        cov_mat = postprocess_mmd_rbf(avg_dist_mat.div(self.lengthscale ** 2.0))\n        return avg_dist_mat, cov_mat\n\n    def forward(self, x1, x2, diag=False, **params):\n        avg_dist_mat, cov_mat = self.compute_distance_covariance_matrix(x1, x2, diag, **params)\n        return cov_mat\n\nFile Path: AIRBO/model_utils/common_model_parts.py\nContent:\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom collections.abc import Iterable\nimport math\n\n\ndef compute_conv2d_output_shape(h_in, w_in, kernel_size, padding, stride, dilation=1):\n    \"\"\"\n    compute the output shape of the conv2D operation\n    \"\"\"\n    if isinstance(kernel_size, Iterable) is False:\n        kernel_size = (kernel_size, kernel_size)\n    if isinstance(padding, Iterable) is False:\n        padding = (padding, padding)\n    if isinstance(stride, Iterable) is False:\n        stride = (stride, stride)\n    if isinstance(dilation, Iterable) is False:\n        dilation = (dilation, dilation)\n\n    h_out = int(math.floor(\n        (h_in + 2 * padding[0] - dilation[0] * (kernel_size[0] - 1) - 1) / stride[0] + 1\n    ))\n\n    w_out = int(math.floor(\n        (w_in + 2 * padding[1] - dilation[1] * (kernel_size[1] - 1) - 1) / stride[1] + 1\n    ))\n\n    return (h_out, w_out)\n\n\ndef compute_conv2dTranspose_output_shape(h_in, w_in, kernel_size, padding, stride,\n                                         output_padding=0, dilation=1):\n    \"\"\"\n    compute the output shape of conv2DTranspose operation\n    \"\"\"\n    if isinstance(kernel_size, Iterable) is False:\n        kernel_size = (kernel_size, kernel_size)\n    if isinstance(padding, Iterable) is False:\n        padding = (padding, padding)\n    if isinstance(stride, Iterable) is False:\n        stride = (stride, stride)\n    if isinstance(dilation, Iterable) is False:\n        dilation = (dilation, dilation)\n    if isinstance(output_padding, Iterable) is False:\n        output_padding = (output_padding, output_padding)\n\n    h_out = (h_in - 1) * stride[0] - 2 * padding[0] + dilation[0] * (kernel_size[0] - 1) \\\n            + output_padding[0] + 1\n\n    w_out = (w_in - 1) * stride[1] - 2 * padding[1] + dilation[1] * (kernel_size[1] - 1) \\\n            + output_padding[1] + 1\n\n    return (h_out, w_out)\n\n\nclass MLP(nn.Module):\n    def __init__(self, input_dim, hidden_dims, output_dim, activation=\"leaky_relu\",\n                 act_in_last_layer=False, skip_connection=False, norm_method=None):\n        \"\"\"\n        A multi-layer perceptron\n        :param input_dim: input dimension\n        :param hidden_dims: a list of hidden dims\n        :param output_dim: output dimension\n        :param activation: which activation to use, can be \"relu\", \"tanh\", \"sigmoid\", \"leaky_relu\"\n        :param act_in_last_layer: whether to use activation in the last layer\n        \"\"\"\n        super(MLP, self).__init__()\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.hidden_dims = hidden_dims\n        self.layer_dims = [input_dim, *hidden_dims, output_dim]\n        self.layer_num = len(self.layer_dims) - 1\n        self.act_in_last_layer = act_in_last_layer\n        self.skip_connection = skip_connection\n        self.norm_method = norm_method\n\n        self.layers = []\n        for i in range(self.layer_num):\n            linear_layer = nn.Linear(self.layer_dims[i], self.layer_dims[i + 1])\n            if self.norm_method is not None and i != self.layer_num - 1:  # no need bn at the last layer:\n                if self.norm_method == 'spectral_norm':\n                    sn_l = nn.utils.spectral_norm(linear_layer)\n                    self.layers.append(sn_l)\n                elif self.norm_method == 'batch_norm':\n                    bn = nn.BatchNorm1d(self.layer_dims[i+1])\n                    self.layers.append(linear_layer)\n                    self.layers.append(bn)\n                else:\n                    raise ValueError(\"Invalid norm_method:\", self.norm_method)\n            else:\n                self.layers.append(linear_layer)\n\n            if i != self.layer_num - 1 or self.act_in_last_layer:\n                if activation == \"relu\":\n                    self.layers.append(torch.nn.ReLU())\n                elif activation == \"tanh\":\n                    self.layers.append(torch.nn.Tanh())\n                elif activation == \"sigmoid\":\n                    self.layers.append(torch.nn.Sigmoid())\n                elif activation == \"leaky_relu\":\n                    self.layers.append(torch.nn.LeakyReLU())\n                else:\n                    raise ValueError(\"Unsupported activation type: \", activation)\n        self.model = nn.Sequential(*self.layers)\n\n    def forward(self, x):\n        if self.skip_connection:\n            o = x + self.model(x)\n        else:\n            o = self.model(x)\n        return o\n\n\nclass CopyModule(nn.Module):\n    def __init__(self):\n        \"\"\"\n        A layer doing nothing but return the inputs\n        \"\"\"\n        super(CopyModule, self).__init__()\n\n    def forward(self, x):\n        return x\n\n\nclass MonoNet(torch.nn.Module):\n    def __init__(self, input_dim, hidden_dims, group_dims):\n        super(MonoNet, self).__init__()\n        self.input_dim = input_dim\n        self.hidden_dims = hidden_dims\n        self.group_dims = group_dims\n        self.n_group_dim = sum(group_dims)\n        self.model = MLP(self.input_dim, self.hidden_dims, self.n_group_dim)\n\n    def forward(self, x):\n        h = self.model(x)\n        group_outputs = []\n        d_ind = 0\n        for pgd in self.group_dims:\n            pgh = h[:, d_ind:d_ind + pgd]\n            pgo, _ = torch.max(pgh, dim=1, keepdim=True)\n            group_outputs.append(pgo)\n            d_ind += pgd\n        output, _ = torch.min(\n            torch.concat(group_outputs, dim=1),\n            dim=1, keepdim=True\n        )\n        return output\n\n    def __call__(self, *args, **kwargs):\n        for module in self.model.modules():\n            if hasattr(module, 'weight'):\n                w = module.weight.data\n                w = w.clamp(0.0, None)\n                module.weight.data = w\n        return self.forward(*args, **kwargs)\n\n\nclass ConvLayer(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=3,\n                 stride=1, padding=0, dilation=1,\n                 deconv=False, output_padding=0,\n                 use_batch_norm=True, activation=\"leaky_relu\"):\n        \"\"\"\n        A single conv2d/deconv2d layer + batch norm + activation\n        :param in_channels: input channel num\n        :param out_channels: output channel num\n        :param kernel_size: kernel size\n        :param stride: stride\n        :param padding: padding\n        :param dilation: dilation\n        :param deconv: whether to use DeconvTranspose2D\n        :param output_padding: only work if deconv is true\n        :param use_batch_norm: whether to use batch normalization\n        :param activation: which activation to use\n        \"\"\"\n        super(ConvLayer, self).__init__()\n\n        layer_list = []\n        conv_filter = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride,\n                                padding=padding, dilation=dilation) if deconv is False \\\n            else nn.ConvTranspose2d(in_channels, out_channels, kernel_size,\n                                    stride, padding, output_padding)\n        layer_list.append(conv_filter)\n\n        if use_batch_norm:\n            layer_list.append(nn.BatchNorm2d(num_features=out_channels))\n\n        act = None\n        if activation == \"leaky_relu\":\n            act = nn.LeakyReLU()\n        elif activation == \"relu\":\n            act = nn.ReLU()\n        elif activation == \"tanh\":\n            act = nn.Tanh()\n        elif activation == \"sigmoid\":\n            act = nn.Sigmoid()\n        else:\n            raise ValueError(\"Unsupported activation: {}\".format(activation))\n\n        if act is not None:\n            layer_list.append(act)\n\n        self.model = nn.Sequential(*layer_list)\n\n    def forward(self, x):\n        return self.model(x)\n\n\nclass MultiConvLayers(nn.Module):\n    def __init__(self, layer_configurations):\n        \"\"\"\n        Multiple conv2d/deconv2d layers\n        :param layer_configurations: a list of dict, each of which has key list= [in_channels,\n                                out_channels, kernel_size, stride, padding, dilation, is_deconv,\n                                output_padding, use_batch_norm, activation]\n        \"\"\"\n        super(MultiConvLayers, self).__init__()\n        self.layer_configurations = layer_configurations\n        layer_list = []\n        for i in range(len(layer_configurations)):\n            if i > 0 and layer_configurations[i][\"in_channels\"] \\\n                    != layer_configurations[i - 1][\"out_channels\"]:\n                raise ValueError(\"Unmatched in_channels with previous layer's out_channels\")\n\n            cfg = layer_configurations[i]\n            in_channels = cfg[\"in_channels\"]\n            out_channels = cfg[\"out_channels\"]\n            kernel_size = cfg[\"kernel_size\"]\n            stride = cfg.get(\"stride\", 1)\n            padding = cfg.get(\"padding\", 0)\n            dilation = cfg.get(\"dilation\", 1)\n            is_deconv = cfg.get(\"is_deconv\", False)\n            output_padding = cfg.get(\"output_padding\", None)\n            use_batch_norm = cfg.get(\"use_batch_norm\", True)\n            activation = cfg.get(\"activation\", \"relu\")\n\n            layer_list.append(ConvLayer(in_channels, out_channels, kernel_size,\n                                        stride, padding, dilation, is_deconv, output_padding,\n                                        use_batch_norm, activation))\n\n        self.model = nn.Sequential(*layer_list)\n\n    def forward(self, x):\n        return self.model(x)\n\n\nclass Encoder(nn.Module):\n    def __init__(self, sample_shape, latent_dim, conv_filter_cfgs,\n                 enc_fc_hidden_dims=(64, 32), enc_fc_out_dim=32,\n                 enc_mu_hdims=(16,), enc_logvar_hdims=(16,)):\n        super(Encoder, self).__init__()\n        self.sample_shape = sample_shape\n        self.latent_dim = latent_dim\n        self.conv_filter_cfgs = conv_filter_cfgs\n\n        self.shape_changes = [i for i in range(len(self.conv_filter_cfgs))\n                              if self.conv_filter_cfgs[i][\"stride\"] > 1]\n\n        # encoder\n        self.enc_conv_model = MultiConvLayers(conv_filter_cfgs) if len(\n            conv_filter_cfgs) > 0 else None\n        self.enc_flatten = nn.Flatten()\n        self.latent_shape = sample_shape // (2 ** len(self.shape_changes))\n        self.enc_fc_input_dim = self.latent_shape * self.latent_shape \\\n                                * (conv_filter_cfgs[-1][\"out_channels\"]\n                                   if len(conv_filter_cfgs) > 0 else 1)\n        self.enc_fc_model = MLP(self.enc_fc_input_dim, enc_fc_hidden_dims, enc_fc_out_dim)\n        self.enc_mu_model = MLP(enc_fc_out_dim, enc_mu_hdims, latent_dim)\n        self.enc_logvar_model = MLP(enc_fc_out_dim, enc_logvar_hdims, latent_dim)\n\n    def forward(self, x):\n        h = x\n        if self.enc_conv_model is not None:\n            h = self.enc_conv_model(h)\n        h = self.enc_flatten(h)\n        h = F.relu(self.enc_fc_model(h))\n        return self.enc_mu_model(h), self.enc_logvar_model(h)\n\n\nclass EncoderWithY(nn.Module):\n    def __init__(self, sample_shape, latent_dim, y_dim, conv_filter_cfgs,\n                 enc_fc_hidden_dims=(64, 32), enc_fc_out_dim=32,\n                 enc_mu_hdims=(16,), enc_logvar_hdims=(16,)):\n        super(EncoderWithY, self).__init__()\n        self.sample_shape = sample_shape\n        self.latent_dim = latent_dim\n        self.conv_filter_cfgs = conv_filter_cfgs\n        self.y_dim = y_dim\n\n        self.shape_changes = [i for i in range(len(self.conv_filter_cfgs))\n                              if self.conv_filter_cfgs[i][\"stride\"] > 1]\n\n        # encoder\n        self.enc_conv_model = MultiConvLayers(conv_filter_cfgs) \\\n            if len(conv_filter_cfgs) > 0 else None\n        self.enc_flatten = nn.Flatten()\n        self.latent_shape = (sample_shape[0] // (2 ** len(self.shape_changes)),\n                             sample_shape[1] // (2 ** len(self.shape_changes)))\n        self.enc_fc_input_dim = self.latent_shape[0] * self.latent_shape[1] \\\n                                * (conv_filter_cfgs[-1][\"out_channels\"]\n                                   if len(conv_filter_cfgs) > 0 else 1)\n        self.enc_fc_model = MLP(self.enc_fc_input_dim, enc_fc_hidden_dims, enc_fc_out_dim)\n        self.enc_mu_model = MLP(enc_fc_out_dim, enc_mu_hdims, latent_dim)\n        self.enc_logstd_model = MLP(enc_fc_out_dim, enc_logvar_hdims, latent_dim)\n        # self.enc_mu_model = MLP(enc_fc_out_dim+y_dim, enc_mu_hdims, latent_dim)\n        # self.enc_logvar_model = MLP(enc_fc_out_dim+y_dim, enc_logvar_hdims, latent_dim)\n\n    def forward(self, x, y):\n        h = x\n        if self.enc_conv_model is not None:\n            h = self.enc_conv_model(h)\n        h = self.enc_flatten(h)\n        h = F.leaky_relu(self.enc_fc_model(h))\n        # mu = self.enc_mu_model(torch.cat([h, y], dim=1))\n        mu = self.enc_mu_model(h)\n        # logvar = self.enc_logvar_model(torch.cat([h, y], dim=1))\n        logstd = self.enc_logstd_model(h)\n        return mu, logstd\n\n\nclass Decoder(nn.Module):\n    def __init__(self, latent_dim, latent_shape, latent_chns, deconv_filter_cfgs,\n                 dec_fc_hidden_dims=(64, 128), ):\n        super(Decoder, self).__init__()\n        self.latent_dim = latent_dim\n        self.latent_shape = latent_shape\n        self.latent_chns = latent_chns\n        self.deconv_filter_cfs = deconv_filter_cfgs\n\n        self.dec_fc_out_dim = self.latent_shape[0] * self.latent_shape[1] * self.latent_chns\n        self.dec_fc_model = MLP(latent_dim, dec_fc_hidden_dims, self.dec_fc_out_dim)\n        self.deconv_model = MultiConvLayers(deconv_filter_cfgs) \\\n            if len(deconv_filter_cfgs) > 0 else None\n\n    def forward(self, z):\n        h = F.leaky_relu(self.dec_fc_model(z))\n        if self.deconv_model is not None:\n            h = h.view(-1,\n                       self.latent_chns,\n                       self.latent_shape[0],\n                       self.latent_shape[1])\n            h = self.deconv_model(h)\n        return h\n\n\ndef reparameterize(mu, logvar):\n    std = torch.exp(0.5 * logvar)\n    eps = torch.randn_like(std)\n    return mu + eps * std\n\nFile Path: AIRBO/model_utils/input_transform.py\nContent:\nimport numpy as np\nimport torch\nimport botorch.models.transforms as trans\nfrom torch.nn import ModuleDict\nfrom collections import OrderedDict\nfrom typing import Tuple\nfrom torch import Tensor\nfrom botorch.exceptions.errors import BotorchTensorDimensionError\nfrom typing import Any, Callable, Dict, List, Optional, Union\n\n\ndef additional_std(X, std):\n    ret = torch.ones_like(X) * torch.tensor(std, dtype=X.dtype, device=X.device) \\\n        if isinstance(X, torch.Tensor) else np.ones_like(X) * std\n    return ret\n\n\ndef additional_xc_samples(X, n_sample, n_var, sampling_func, sampling_cfg={}, **kwargs):\n    # more samples around the raw\n    batch_shape = X.shape[:-1]\n    noises = sampling_func(\n        **{**sampling_cfg, 'x': X}, size=(*batch_shape, n_sample)\n    ).reshape(*batch_shape, n_sample, n_var)\n    if isinstance(X, torch.Tensor):\n        noises = torch.tensor(noises, dtype=X.dtype, device=X.device)\n    samples = X[..., None, :] + noises\n    return samples\n\n\ndef add_noise(X, sampling_func, sampling_cfg={}, **kwargs):\n    batch_shape = X.shape[:-1]\n    event_dim = X.shape[-1]\n    noise = sampling_func(**sampling_cfg, size=(*batch_shape, 1) ).reshape(*batch_shape, event_dim)\n    if isinstance(X, torch.Tensor):\n        noise = torch.tensor(noise, dtype=X.dtype, device=X.device)\n    return X + noise\n\n\nclass AdditionalFeatures(trans.input.AppendFeatures):\n    def transform(self, X):\n        expanded_features = self._f(X[..., self.indices], **self.fkwargs)\n        return X, expanded_features, torch.zeros(size=(*X.shape[:-1], 0), dtype=X.dtype,\n                                                 device=X.device)  # xc, xc_std/samples, xe\n\n\nclass TransformFeature(trans.input.AppendFeatures):\n    def transform(self, X):\n        transformed_features = self._f(X[..., self.indices], **self.fkwargs)\n        return transformed_features, torch.zeros(size=(*X.shape[:-1], 0), dtype=X.dtype,\n                                                 device=X.device)  # xc, xc_std/samples, xe\n\n\nclass SelectMultiInputs(trans.input.InputTransform, torch.nn.Module):\n    def __init__(\n            self,\n            sel_indices,\n            transform_on_train: bool = True,\n            transform_on_eval: bool = True,\n            transform_on_fantasize: bool = True,\n    ) -> None:\n        super().__init__()\n        self.transform_on_train = transform_on_train\n        self.transform_on_eval = transform_on_eval\n        self.transform_on_fantasize = transform_on_fantasize\n        self.register_buffer(\"sel_indices\", sel_indices)\n\n    def transform(self, X):\n        return tuple(X[i] for i in self.sel_indices)\n\n\nclass MultiInputTransform(trans.input.InputTransform, ModuleDict):\n    r\"\"\"An input transform representing the chaining of individual transforms.\"\"\"\n\n    def __init__(self, **transforms) -> None:\n        r\"\"\"Chaining of input transforms.\n\n        Args:\n            transforms: The transforms to chain. Internally, the names of the\n                kwargs are used as the keys for accessing the individual\n                transforms on the module.\n\n        \"\"\"\n        super().__init__(OrderedDict(transforms))\n        self.transform_on_train = False\n        self.transform_on_eval = False\n        self.transform_on_fantasize = False\n        for tf in transforms.values():\n            self.is_one_to_many |= tf.is_one_to_many\n            self.transform_on_train |= tf.transform_on_train\n            self.transform_on_eval |= tf.transform_on_eval\n            self.transform_on_fantasize |= tf.transform_on_fantasize\n\n    def transform(self, X):\n        ret = tuple([tf.transform(X[ind]) for ind, tf in enumerate(self.values())])\n        return ret\n\n    def untransform(self, X):\n        ret = tuple([tf.untransform(X[ind]) for ind, tf in enumerate(self.values())])\n        return ret\n\n    def equals(self, other: trans.input.InputTransform) -> bool:\n        return super().equals(other=other) and all(\n            t1.equals(t2) for t1, t2 in zip(self.values(), other.values())\n        )\n\n    def preprocess_transform(self, X):\n        ret = tuple([tf.preprocess_transform(X[ind]) for ind, tf in enumerate(self.values())])\n        return ret\n\n\nclass DummyTransform(trans.input.ReversibleInputTransform, torch.nn.Module):\n    def __init__(\n            self,\n            transform_on_train: bool = True,\n            transform_on_eval: bool = True,\n            transform_on_fantasize: bool = True,\n            reverse: bool = False,\n    ) -> None:\n        super().__init__()\n        self.transform_on_train = transform_on_train\n        self.transform_on_eval = transform_on_eval\n        self.transform_on_fantasize = transform_on_fantasize\n        self.reverse = reverse\n\n    def _transform(self, X: Tensor) -> Tensor:\n        return X\n\n    def _untransform(self, X: Tensor) -> Tensor:\n        return X\n\n\nclass ScaleTransform(trans.input.Normalize):\n    def _transform(self, X: Tensor) -> Tensor:\n        return X / self.coefficient\n\n    def _untransform(self, X: Tensor) -> Tensor:\n        return X * self.coefficient\n\nFile Path: AIRBO/model_utils/model_common_utils.py\nContent:\nimport model_utils.input_transform as tfx\n\nfrom sklearn.preprocessing import PowerTransformer, StandardScaler, MinMaxScaler\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom collections.abc import Iterable\nfrom typing import Callable\n\n\ndef safe_scaling(y, scaler=None, scaling_method=\"standardize\"):\n    \"\"\"\n    Apply scaling to the target\n    \"\"\"\n    try:\n        if scaler is None:  # fit a new scaler\n            if scaling_method == \"standardize\":\n                scaler = StandardScaler()\n                y_scaled = scaler.fit_transform(y)\n            elif scaling_method == \"power_transform\":\n                if y.min() <= 0:\n                    scaler = PowerTransformer(method='johnson', standardize=True)\n                    y_scaled = scaler.fit_transform(y)\n                else:\n                    scaler = PowerTransformer(method='box-cox', standardize=True)\n                    y_scaled = scaler.fit_transform(y)\n                    # try johnson\n                    if y_scaled.std() < 0.5:\n                        scaler = PowerTransformer(method='yeo-johnson', standardize=True)\n                        y_scaled = scaler.fit_transform(y)\n                    if y_scaled.std() < 0.5:\n                        raise RuntimeError('Power transformation failed')\n            elif scaling_method == \"min_max\":\n                scaler = MinMaxScaler()\n                y_scaled = scaler.fit_transform(y)\n            else:\n                raise ValueError(\"Unknown scaling method:\", scaling_method)\n        else:  # transform using the given scaler\n            y_scaled = scaler.transform(y)\n    except Exception as e:\n        print(f\"[Warn] scaling fails:\", e)\n        y_scaled = y.copy()\n        scaler = None\n    return y_scaled, scaler\n\n\ndef filter_nan(x, xe, y, keep_rule='any'):\n    assert x is None or np.isfinite(x).all()\n    assert xe is None or np.isfinite(xe).all()\n    assert torch.isfinite(y).any(), \"No valid data in the dataset\"\n\n    if keep_rule == 'any':\n        valid_id = torch.isfinite(y).any(dim=1)\n    else:\n        valid_id = torch.isfinite(y).all(dim=1)\n    x_filtered = x[valid_id] if x is not None else None\n    xe_filtered = xe[valid_id] if xe is not None else None\n    y_filtered = y[valid_id]\n    return x_filtered, xe_filtered, y_filtered\n\n\ndef get_gp_prediction(model, x, scaler, **kwargs):\n    pred = model.predict(x, **kwargs)\n    pred_lcb, pred_ucb = pred.confidence_region()\n    pred_mean = pred.mean\n    if scaler is not None:\n        mean = scaler.inverse_transform(pred_mean.detach().numpy().reshape(-1, 1)).flatten()\n        lcb = scaler.inverse_transform(pred_lcb.detach().numpy().reshape(-1, 1)).flatten()\n        ucb = scaler.inverse_transform(pred_ucb.detach().numpy().reshape(-1, 1)).flatten()\n    else:\n        mean = pred_mean.detach().numpy().flatten()\n        lcb = pred_lcb.detach().numpy().flatten()\n        ucb = pred_ucb.detach().numpy().flatten()\n    return pred, mean, lcb, ucb\n\n\nclass OneHotTransform(torch.nn.Module):\n    def __init__(self, num_uniqs):\n        super().__init__()\n        self.num_uniqs = num_uniqs\n\n    @property\n    def num_out(self) -> int:\n        return sum(self.num_uniqs)\n\n    def forward(self, xe):\n        return torch.cat(\n            [torch.nn.functional.one_hot(xe[:, i].long(), self.num_uniqs[i])\n             for i in range(xe.shape[1])], dim=1\n        ).float()\n\n\nclass EmbTransform(nn.Module):\n    def __init__(self, num_uniqs, **conf):\n        super().__init__()\n        self.emb_sizes = conf.get('emb_sizes')\n        if self.emb_sizes is None:\n            self.emb_sizes = [min(50, 1 + v // 2) for v in num_uniqs]\n\n        self.emb = nn.ModuleList([])\n        for num_uniq, emb_size in zip(num_uniqs, self.emb_sizes):\n            self.emb.append(nn.Embedding(num_uniq, emb_size))\n\n    @property\n    def num_out(self) -> int:\n        return sum(self.emb_sizes)\n\n    def forward(self, xe):\n        return torch.cat(\n            [self.emb[i](xe[:, i]).view(xe.shape[0], -1) for i in range(len(self.emb))], dim=1)\n\n\ndef get_model_prediction(model, Xc_te, support_decomposed_pred):\n    \"\"\"\n    Given a model and test data, return model predictions\n    :param model:\n    :param Xc_te:\n    :param support_decomposed_pred: whether to return a list of decomposed prediction\n    :return:\n    \"\"\"\n    preds = []\n    # full prediction\n    if support_decomposed_pred:\n        py_m0, ps2_m0 = model.predict(\n            torch.FloatTensor(Xc_te), torch.zeros(Xc_te.shape[0], 0), with_noise=True, mode=0\n        )\n    else:\n        py_m0, ps2_m0 = model.predict(torch.FloatTensor(Xc_te), torch.zeros(Xc_te.shape[0], 0))\n    ucb_m0 = py_m0 + (torch.sqrt(ps2_m0) * 2.0)\n    lcb_m0 = py_m0 - (torch.sqrt(ps2_m0) * 2.0)\n    preds.append(\n        (py_m0.detach().numpy(),\n         ps2_m0.detach().numpy(),\n         lcb_m0.detach().numpy(),\n         ucb_m0.detach().numpy(),\n         )\n    )\n\n    if support_decomposed_pred:\n        # mode 1 predict\n        py_m1, ps2_m1 = model.predict(\n            torch.FloatTensor(Xc_te), torch.zeros(Xc_te.shape[0], 0), with_noise=True, mode=1\n        )\n        ucb_m1 = py_m1 + (torch.sqrt(ps2_m1) * 2.0)\n        lcb_m1 = py_m1 - (torch.sqrt(ps2_m1) * 2.0)\n        preds.append(\n            (py_m1.detach().numpy(),\n             ps2_m1.detach().numpy(),\n             lcb_m1.detach().numpy(),\n             ucb_m1.detach().numpy(),\n             )\n        )\n\n        # mode 2 predict\n        py_m2, ps2_m2 = model.predict(\n            torch.FloatTensor(Xc_te), torch.zeros(Xc_te.shape[0], 0), with_noise=True, mode=2\n        )\n        ucb_m2 = py_m2 + (torch.sqrt(ps2_m2) * 2.0)\n        lcb_m2 = py_m2 - (torch.sqrt(ps2_m2) * 2.0)\n        preds.append(\n            (py_m2.detach().numpy(),\n             ps2_m2.detach().numpy(),\n             lcb_m2.detach().numpy(),\n             ucb_m2.detach().numpy(),\n             )\n        )\n    return preds\n\n\ndef get_kernel_lengthscale(kern_model):\n    # get lengthscale\n    ls = kern_model.lengthscale\n    km = kern_model\n    while ls is None and getattr(km, 'base_kernel', None) is not None:\n        km = km.base_kernel\n        ls = km.lengthscale\n    if ls is not None:\n        ls = ls.detach().cpu().numpy().flatten()\n\n    if isinstance(ls, Iterable) and len(ls) >= 1:\n        str_output = [f'{i:.3f}' for i in ls] if len(ls) > 1 else f'{ls[0]:.3f}'\n    else:\n        str_output = f'{ls}'\n\n    return ls, str_output\n\n\ndef get_kernel_output_scale(kernel):\n    oscale = kernel.outputscale.item() if hasattr(kernel, 'outputscale') else None\n    oscale_str = f'{oscale:.3f}' if oscale is not None else f'{oscale}'\n    return oscale, oscale_str\n\n\n\ndef prepare_data(input_type: str,\n                 n_var: int, raw_input_mean: [float, np.array], raw_input_std: [float, np.array],\n                 xc_sample_size: int, input_sampling_func: Callable,\n                 xc_raw: np.array, y: np.array,\n                 dtype: torch.dtype, device: torch.device,\n                 **data_cfg):\n    \"\"\"\n    Prepare the data acd. to the input type, transform them into tensor and put on right device\n    \"\"\"\n    if input_type == INPUT_TYPE_NOISED or input_type == INPUT_TYPE_MEAN:  # only the x_raw\n        x_ts = torch.tensor(xc_raw, dtype=dtype, device=device)\n    elif input_type == INPUT_TYPE_SAMPLES:  # observe some samples around x\n        tf_add_xsamp = tfx.AdditionalFeatures(\n            f=tfx.additional_xc_samples, transform_on_train=False,\n            fkwargs={'n_sample': xc_sample_size, 'n_var': n_var,\n                     'sampling_func': input_sampling_func}\n        )\n        x_ts = tf_add_xsamp.transform(torch.tensor(xc_raw, dtype=dtype, device=device))\n    elif input_type == INPUT_TYPE_DISTRIBUTION:\n        tf_add_std = tfx.AdditionalFeatures(f=tfx.additional_std, transform_on_train=False,\n                                            fkwargs={'std': raw_input_std})\n        x_ts = tf_add_std.transform(\n            torch.tensor(\n                xc_raw + raw_input_mean if raw_input_mean is not None else xc_raw,\n                dtype=dtype, device=device\n            )\n        )\n\n    else:\n        raise ValueError('Unknown input type:', input_type)\n\n    y_ts = torch.tensor(y.reshape(-1, 1), dtype=dtype, device=device)\n\n    return x_ts, y_ts\n\n\nNOISE_LB = 1e-4\nINPUT_TYPE_NOISED = 'exact_input'\nINPUT_TYPE_MEAN = 'mean_input'\nINPUT_TYPE_SAMPLES = 'sample_input'\nINPUT_TYPE_DISTRIBUTION = 'distribution_input'\n\nFile Path: AIRBO/model_utils/model_fit_utils.py\nContent:\nimport gpytorch as gpyt\nimport numpy as np\nimport torch\nfrom typing import Dict, List, Tuple\n\n\ndef fit_model_restarts(model_cls: torch.nn.Module, model_cfg: Dict,\n                       tr_Xc: torch.Tensor, tr_Xe: torch.Tensor, tr_y:torch.Tensor,\n                       fit_cfg: Dict) -> Tuple[torch.nn.Module, List]:\n    \"\"\"\n    Train a GP model with restarts (if NotPSD Error happens)\n    :param model_cls: model class\n    :param model_cfg: model configuration\n    :param tr_Xc: continuous inputs\n    :param tr_Xe: enumerate inputs\n    :param tr_y: training target\n    :param fit_cfg: fitting configurations\n    :return: a list of training history\n    \"\"\"\n    torch.autograd.set_detect_anomaly(True)\n    fit_restarts = fit_cfg.get('fit_restarts', 3)\n    results = []\n    while len(results) < fit_restarts:\n        try:\n            model_i = model_cls(**model_cfg)\n            tr_hist_i = model_i.fit(\n                tr_Xc,\n                tr_Xe,\n                tr_y.flatten(),\n                **fit_cfg\n            )\n        except gpyt.utils.errors.NotPSDError as e:\n            print('[WARN] model fit fails, try again:', e)\n            continue\n        results.append((model_i, tr_hist_i))\n\n    if len(results) > 0:\n        # find the optimal model\n        opt_ind = np.nanargmin([h[-1]['loss'] for (m, h) in results])\n        opt_model, opt_tr_hist = results[opt_ind]\n    else:\n        raise ValueError(f\"All the {fit_restarts} restarts fail!\")\n\n    return opt_model, opt_tr_hist\nFile Path: AIRBO/models/mmd_gp.py\nContent:\nfrom model_utils import input_transform as tfx\nfrom model_utils.common_model_parts import MLP, CopyModule\nfrom kernels.mmd_kernel import MMDKernel, additive_RQ_kernel\nfrom models.robust_gp import RobustGP\n\nimport gpytorch as gpyt\nfrom botorch.models import transforms as tf\nimport torch\nimport warnings\nfrom gpytorch import settings\nfrom gpytorch.utils.warnings import GPInputWarning\nfrom gpytorch.models.exact_prediction_strategies import prediction_strategy\nfrom gpytorch.models import ExactGP\nfrom gpytorch.distributions import MultivariateNormal\n\n\nclass MMDGP(RobustGP):\n    def __init__(self, train_inputs, train_targets, likelihood, num_inputs,\n                 input_transform=None, outcome_transform=None, additional_transform=None,\n                 hidden_dims=(4, 2), latent_dim=1, **kwargs):\n        super(MMDGP, self).__init__(train_inputs, train_targets, likelihood, num_inputs,\n                                    input_transform, outcome_transform,\n                                    additional_transform, **kwargs)\n\n        # latent mapping\n        self.norm_method = kwargs.get('latent_norm_method', None)\n        self.skip_conn = kwargs.get('skip_conn', False)\n        if hidden_dims is not None:\n            self.latent_dim = latent_dim\n            self.latent_mapping_module = MLP(\n                train_inputs[0].shape[-1], hidden_dims, latent_dim,\n                norm_method=self.norm_method\n            )\n        else:\n            self.latent_dim = train_inputs[0].shape[1] + train_inputs[0].shape[1]\n            self.latent_mapping_module = CopyModule()\n\n    def define_default_input_transform(self, **kwargs):\n        n_var = kwargs['n_var']\n        input_bounds = kwargs['input_bounds']\n        return tfx.MultiInputTransform(\n            tf1=tf.Normalize(d=n_var, bounds=input_bounds, transform_on_train=True),\n            tf2=tf.Normalize(d=n_var, bounds=input_bounds, transform_on_train=True),\n            tf3=tfx.DummyTransform(transform_on_train=True),\n        )\n\n    def define_covar_module(self, **kwargs):\n        xc_kern_params = {}\n        xc_lscale_constr = kwargs.get('xc_ls_constr', None)\n        if xc_lscale_constr is not None:\n            xc_kern_params['lengthscale_constraint'] = xc_lscale_constr\n\n        xc_mmd_inner_k = kwargs.get('base_kernel', None)\n        if xc_mmd_inner_k is None:\n            xc_mmd_inner_k = additive_RQ_kernel(\n                alphas=(0.2, 0.5, 1, 2, 5), ls=1.0, learnable_ls=False\n            )\n\n        estimator_name = kwargs.get('estimator_name', 'nystrom')\n        chunk_size = kwargs.get('chunk_size', 100)\n        sub_samp_size = kwargs.get('sub_samp_size', 100)\n        covar_module = gpyt.kernels.ScaleKernel(\n            MMDKernel(xc_mmd_inner_k, estimator=estimator_name, sub_samp_size=sub_samp_size,\n                      chunk_size=chunk_size)\n        )\n        return covar_module\n\n    def forward(self, xc_raw, xc_samp, xe):\n        # note that we assume X is already applied with additional transform\n        # input transform\n        X = (xc_raw, xc_samp, xe)\n        if self.training:\n            X = self.transform_inputs(X)\n        mean_x, covar = self.compute_mean_cov(X)\n        return gpyt.distributions.MultivariateNormal(mean_x, covar)\n\n    def compute_mean_cov(self, x, **kwargs):\n        \"\"\"\n        compute the mean and covariance matrix\n        :param x: a tuple of (Xc_raw, Xc_samples, Xe),\n        where Xc_raw is raw inputs, size= (M * D),\n        Xc_samples represents the nearby samples around the raw inputs, size= M * B * D tensor,\n        and Xe is the raw inputs of enumerate features, M * 0 tensor.\n        \"\"\"\n        Xc_raw, Xc_samples, Xe = x\n        Xe_trans = Xe\n        mean_x, covar = None, None\n        if Xc_raw.shape[-1] > 0 and Xc_samples.shape[-1] > 0:\n            _s = Xc_samples.shape[:-1]\n            D = Xc_samples.shape[-1]\n            proj_X_samples = self.latent_mapping_module(Xc_samples.view(-1, D)).view(*_s, -1)\n\n            # covar\n            with gpyt.settings.debug(True) and gpyt.settings.lazily_evaluate_kernels(False):\n                k_c = self.covar_module(proj_X_samples, **kwargs)\n            covar = k_c if covar is None else (k_c * covar)\n\n            # mean\n            Xc_samples_mean = Xc_samples.mean(dim=-2)\n            proj_X_raw = self.latent_mapping_module(Xc_samples_mean)\n            mean_x = self.mean_module(proj_X_raw)\n\n        if Xe.shape[-1] > 0:\n            Xe_trans = self.xe_transformer(Xe)\n            k_e = self.xe_covar_module(Xe_trans, **kwargs)\n            covar = k_e if covar is None else (k_e * covar)\n\n        return mean_x, covar\n\n    def define_additional_transform(self, **kwargs):\n        xc_sample_size = kwargs.get('xc_sample_size', 1000)\n        input_sampling_func = kwargs['input_sampling_func']\n        n_var = kwargs['n_var']\n        return tfx.AdditionalFeatures(f=tfx.additional_xc_samples, transform_on_train=False,\n                                      fkwargs={'n_sample': xc_sample_size, 'n_var': n_var,\n                                               'sampling_func': input_sampling_func})\n\n    def __call__(self, *args, **kwargs):\n        train_inputs = list(self.train_inputs) if self.train_inputs is not None else []\n        inputs = [i.unsqueeze(-1) if i.ndimension() == 1 else i for i in args]\n\n        # Training mode: optimizing\n        if self.training:\n            if self.train_inputs is None:\n                raise RuntimeError(\n                    \"train_inputs, train_targets cannot be None in training mode. \"\n                    \"Call .eval() for prior predictions, or call .set_train_data() to add training data.\"\n                )\n            if settings.debug.on():\n                if not all(torch.equal(train_input, input) for train_input, input in\n                           zip(train_inputs, inputs)):\n                    raise RuntimeError(\"You must train on the training inputs!\")\n            res = super().__call__(*inputs, **kwargs)\n            return res\n\n        # Prior mode\n        elif settings.prior_mode.on() or self.train_inputs is None or self.train_targets is None:\n            full_inputs = args\n            full_output = super(ExactGP, self).__call__(*full_inputs, **kwargs)\n            if settings.debug().on():\n                if not isinstance(full_output, MultivariateNormal):\n                    raise RuntimeError(\"ExactGP.forward must return a MultivariateNormal\")\n            return full_output\n\n        # Posterior mode\n        else:\n            if settings.debug.on():\n                if all(torch.equal(train_input, input) for train_input, input in\n                       zip(train_inputs, inputs)):\n                    warnings.warn(\n                        \"The input matches the stored training data. Did you forget to call model.train()?\",\n                        GPInputWarning,\n                    )\n\n            # Get the terms that only depend on training data\n            if self.prediction_strategy is None:\n                train_output = super(ExactGP, self).__call__(*train_inputs, **kwargs)\n\n                # Create the prediction strategy for\n                self.prediction_strategy = prediction_strategy(\n                    train_inputs=train_inputs,\n                    train_prior_dist=train_output,\n                    train_labels=self.train_targets,\n                    likelihood=self.likelihood,\n                )\n\n            # Concatenate the input to the training input\n            full_inputs = []\n            batch_shape = train_inputs[0].shape[:-2]\n            for i, (train_input, input) in enumerate(zip(train_inputs, inputs)):\n                # Make sure the batch shapes agree for training/test data\n                # special operations for MMD kernel\n                dim_2_concat = -3 if i == 1 else -2\n                batch_reserved_dim = -3 if i == 1 else -2\n                batch_shape = train_inputs[i].shape[:batch_reserved_dim]\n                if batch_shape != train_input.shape[:batch_reserved_dim]:\n                    batch_shape = torch.broadcast_shapes(batch_shape,\n                                                         train_input.shape[:batch_reserved_dim])\n                    train_input = train_input.expand(*batch_shape,\n                                                     *train_input.shape[batch_reserved_dim:])\n                if batch_shape != input.shape[:batch_reserved_dim]:\n                    batch_shape = torch.broadcast_shapes(batch_shape,\n                                                         input.shape[:batch_reserved_dim])\n                    train_input = train_input.expand(*batch_shape,\n                                                     *train_input.shape[batch_reserved_dim:])\n                    input = input.expand(*batch_shape, *input.shape[batch_reserved_dim:])\n                full_inputs.append(torch.cat([train_input, input], dim=dim_2_concat))\n\n            # Get the joint distribution for training/test data\n            full_output = super(ExactGP, self).__call__(*full_inputs, **kwargs)\n            if settings.debug().on():\n                if not isinstance(full_output, MultivariateNormal):\n                    raise RuntimeError(\"ExactGP.forward must return a MultivariateNormal\")\n            full_mean, full_covar = full_output.loc, full_output.lazy_covariance_matrix\n\n            # Determine the shape of the joint distribution\n            batch_shape = full_output.batch_shape\n            joint_shape = full_output.event_shape\n            tasks_shape = joint_shape[1:]  # For multitask learning\n            test_shape = torch.Size(\n                [joint_shape[0] - self.prediction_strategy.train_shape[0], *tasks_shape])\n\n            # Make the prediction\n            with settings.cg_tolerance(settings.eval_cg_tolerance.value()):\n                predictive_mean, predictive_covar = self.prediction_strategy.exact_prediction(\n                    full_mean, full_covar)\n\n            # Reshape predictive mean to match the appropriate event shape\n            predictive_mean = predictive_mean.view(*batch_shape, *test_shape).contiguous()\n            return full_output.__class__(predictive_mean, predictive_covar)\n\nFile Path: AIRBO/models/robust_gp.py\nContent:\n\"\"\"\nRobust model that is compatible with gpytorch and botorch.\n\"\"\"\nfrom model_utils import model_common_utils as mcu\n\nimport gpytorch as gpyt\nimport botorch as bot\nfrom botorch.models.gpytorch import GPyTorchModel\nfrom botorch.models import transforms as tf\nfrom typing import Any, Optional, Union\nimport torch\nfrom botorch.acquisition.objective import PosteriorTransform\nfrom botorch.models.utils import gpt_posterior_settings\nfrom botorch.posteriors.gpytorch import GPyTorchPosterior\nfrom torch import Tensor\nimport warnings\nfrom botorch.posteriors.transformed import TransformedPosterior  # pragma: no cover\n\n\n# %%\nclass RobustGP(gpyt.models.ExactGP, GPyTorchModel):\n    def __init__(self, train_inputs, train_targets, likelihood, num_inputs,\n                 input_transform=None, outcome_transform=None, additional_transform=None,\n                 **kwargs):\n        # Note:\n        # we first need to warm up the input and outcome transformers (say the params in\n        # the outcome standardize), then save the raw inputs and transformed outcome in the model\n        # via feeding them to the ExactGP.__init__().\n        # During the training, the input transform is applied before each forward() to\n        # normalize the train inputs.\n        # After that, if the model.eval() is called, the model will apply the input\n        # transformation on its saved train inputs and prepare to concatenate with the transformed\n        # test inputs.\n        # Moreover, the outcome is untransformed in the posterior() and the additional transform is\n        # only applied in the posterior\n        \"\"\"\n        A GP model that is compatible with BoTorch\n        :param train_inputs: a tensor or a tuple of tensors for training inputs\n        :param train_targets: a tensor of training targets\n        :param likelihood: likelihood to use\n        :param num_inputs: the required number of train inputs\n        :param input_transform: transformations to be applied on the inputs, e.g., Normalization\n        :param outcome_transform: transformation for the outcome, say standardization\n        :param additional_transform: additional transformation to apply on the inputs\n        :param kwargs: other model configurations.\n        \"\"\"\n        _in_tf = input_transform if input_transform is not None \\\n            else self.define_default_input_transform(**kwargs)\n        _out_tf = outcome_transform if outcome_transform is not None \\\n            else self.define_default_outcome_transform(m=1, **kwargs)\n\n        # Apply the transformers to warm up the params (say the params in standardization)\n        with torch.no_grad():\n            _in_tf.transform(train_inputs)\n        if _out_tf is not None:\n            # transform the outcome\n            train_targets, _ = _out_tf(train_targets)\n            train_targets = train_targets.squeeze(-1)\n\n        # save the raw inputs and transformed outcome\n        gpyt.models.ExactGP.__init__(self, train_inputs, train_targets, likelihood)\n\n        self.input_transform = _in_tf\n        self.outcome_transform = _out_tf\n        self.num_inputs = num_inputs\n        self.additional_transform = additional_transform if additional_transform is not None \\\n            else self.define_additional_transform(**kwargs)\n\n        self.mean_module = self.define_mean_module(**kwargs)\n        self.covar_module = self.define_covar_module(**kwargs)\n\n        self.kwargs = kwargs\n\n    def define_default_input_transform(self, **kwargs):\n        n_var = kwargs['n_var']\n        return tf.Normalize(d=n_var, transform_on_train=True)\n\n    def define_default_outcome_transform(self, m=1, **kwargs):\n        return tf.outcome.Standardize(m=m)\n\n    def define_mean_module(self, **kwargs):\n        return gpyt.means.ConstantMean()\n\n    def define_covar_module(self, **kwargs):\n        return gpyt.kernels.ScaleKernel(gpyt.kernels.MaternKernel())\n\n    def define_additional_transform(self, **kwargs):\n        return None\n\n    def transform_inputs_additional(self, X):\n        # apply the additional transform\n        n_inputs = 1 if isinstance(X, torch.Tensor) else len(X)\n        if n_inputs != self.num_inputs:\n            if self.additional_transform is not None:\n                X = self.additional_transform(X)\n            else:\n                raise ValueError(f\"Expect {self.num_inputs} inputs but found {len(X)} \"\n                                 f\"and no additional transformer.\")\n        return X\n\n    def forward(self, X):\n        # note that we assume X is already applied with additional transform\n        if self.training:\n            X = self.transform_inputs(X)\n        xc_raw = X\n        mean_x = self.mean_module(xc_raw)\n        covar_x = self.covar_module(xc_raw)\n        return gpyt.distributions.MultivariateNormal(mean_x, covar_x)\n\n    def posterior(\n            self, X,\n            observation_noise: Union[bool, Tensor] = False,\n            posterior_transform: Optional[PosteriorTransform] = None,\n            **kwargs: Any,\n    ) -> Union[GPyTorchPosterior, TransformedPosterior]:\n        r\"\"\"Computes the posterior over model outputs at the provided points.\n\n        Args:\n            X: A `(batch_shape) x q x d`-dim Tensor, where `d` is the dimension\n                of the feature space and `q` is the number of points considered\n                jointly.\n            observation_noise: If True, add the observation noise from the\n                likelihood to the posterior. If a Tensor, use it directly as the\n                observation noise (must be of shape `(batch_shape) x q`).\n            posterior_transform: An optional PosteriorTransform.\n\n        Returns:\n            A `GPyTorchPosterior` object, representing a batch of `b` joint\n            distributions over `q` points. Includes observation noise if\n            specified.\n        \"\"\"\n        self.eval()  # make sure model is in eval mode\n        # apply the additional transform\n        X = self.transform_inputs_additional(X)\n\n        # input transforms are applied at `posterior` in `eval` mode, and at\n        # `model.forward()` at the training time\n        X = self.transform_inputs(X)\n        with gpt_posterior_settings():\n            mvn = self(*X) if self.num_inputs > 1 else self(X)  # support multiple inputs\n            if observation_noise is not False:\n                if isinstance(observation_noise, torch.Tensor):\n                    # TODO: Make sure observation noise is transformed correctly\n                    self._validate_tensor_args(X=X, Y=observation_noise)\n                    if observation_noise.size(-1) == 1:\n                        observation_noise = observation_noise.squeeze(-1)\n                    mvn = self.likelihood(mvn, X, noise=observation_noise)\n                else:\n                    mvn = self.likelihood(mvn, X)\n        posterior = GPyTorchPosterior(distribution=mvn)\n        if hasattr(self, \"outcome_transform\"):\n            posterior = self.outcome_transform.untransform_posterior(posterior)\n        if posterior_transform is not None:\n            return posterior_transform(posterior)\n        return posterior\n\n    def _set_transformed_inputs(self) -> None:\n        r\"\"\"Update training inputs with transformed inputs.\"\"\"\n        if hasattr(self, \"input_transform\") and not self._has_transformed_inputs:\n            if hasattr(self, \"train_inputs\"):\n                self._original_train_inputs = self.train_inputs[0] if self.num_inputs == 1 \\\n                    else self.train_inputs  # support multiple inputs\n                with torch.no_grad():\n                    X_tf = self.input_transform.preprocess_transform(\n                        self.train_inputs[0] if self.num_inputs == 1 else self.train_inputs\n                    )\n                self.set_train_data(X_tf, strict=False)\n                self._has_transformed_inputs = True\n            else:\n                warnings.warn(\n                    \"Could not update `train_inputs` with transformed inputs \"\n                    f\"since {self.__class__.__name__} does not have a `train_inputs` \"\n                    \"attribute. Make sure that the `input_transform` is applied to \"\n                    \"both the train inputs and test inputs.\",\n                    RuntimeWarning,\n                )\n\n\nclass RobustGPModel():\n    def __init__(self, m_cls, num_inputs=1, **kwargs):\n        \"\"\"\n        A holistic model for easy use\n        :param m_cls: model class\n        :param num_inputs: number of inputs\n        :param kwargs: model configurations\n        \"\"\"\n        self.model = None\n        self.likelihood = None\n        self.mll = None\n        self.optimizer = None\n        self.m_cls = m_cls\n        self.num_inputs = num_inputs\n\n        # model config\n        self.noise_free = kwargs.get('noise_free', False)\n        self.dtype = kwargs.get('dtype', torch.float)\n        self.device = kwargs.get('device', torch.device('cpu'))\n        self.kwargs = kwargs\n\n    def define_optimizer(self, model: torch.nn.Module, **kwargs):\n        optimizer = None\n        if not kwargs.get('fit_with_scipy', False):\n            lr = kwargs.get('lr', 1e-2)\n            optimizer = torch.optim.Adam(\n                params=[{'params': model.parameters()}],\n                lr=lr\n            )\n\n        return optimizer\n\n    def define_likelihood(self, **kwargs):\n        noise_free = kwargs.get(\"noise_free\", False)\n        if noise_free:\n            lkh = gpyt.likelihoods.GaussianLikelihood()\n            lkh.noise = mcu.NOISE_LB\n            lkh.raw_noise.requires_grad = False\n        else:\n            noise_prior = kwargs.get(\"noise_prior\", None)\n            noise_constr = kwargs.get(\"noise_constr\", None)\n            lkh = gpyt.likelihoods.GaussianLikelihood(\n                noise_prior=noise_prior, noise_constraint=noise_constr\n            )\n\n        return lkh\n\n    def define_model(self, tr_x, tr_y, likelihood, **kwargs):\n        model = self.m_cls(tr_x, tr_y, likelihood, self.num_inputs, **kwargs)\n        return model\n\n    def define_mll(self, likelihood, model):\n        return gpyt.mlls.ExactMarginalLogLikelihood(likelihood, model)\n\n    def post_initialize(self, tr_x, tr_y):\n        self.likelihood = self.define_likelihood(**self.kwargs)\n        self.model = self.define_model(tr_x, tr_y, self.likelihood, **self.kwargs)\n        self.mll = self.define_mll(self.likelihood, self.model)\n\n        # dtype and device\n        self.likelihood = self.likelihood.to(self.dtype).to(self.device)\n        self.model = self.model.to(self.dtype).to(self.device)\n\n    def fit(self, **kwargs):\n        \"\"\"\n        Fit the model, retry if NotPSD Error happens\n        :param tr_x: training inputs\n        :param tr_y: training target\n        :param kwargs: fit configurations\n        :return:\n        \"\"\"\n        assert (self.model is not None and self.mll is not None and self.likelihood is not None)\n        tr_hist = None\n        success = False\n        max_retries = kwargs.get('max_retries', 5)\n        n_retry = 0\n        while not success:\n            try:\n                with bot.settings.debug(True):\n                    tr_hist = self.do_fit(**kwargs)\n                success = True\n            except Exception as e:\n                if n_retry < max_retries:\n                    n_retry += 1\n                    print(f\"[Warn] Model fit fails, retry cnt={n_retry}.\", e)\n                    success = False\n                else:\n                    raise e\n        return tr_hist\n\n    def do_fit(self, **kwargs):\n        self.model.train()\n        fit_with_scipy = kwargs.get('fit_with_scipy', True)\n        tr_history = None\n        if fit_with_scipy:\n            bot.fit_gpytorch_mll(self.mll)\n        else:\n            tr_history = []\n            epoch_num = kwargs.get('epoch_num', 100)\n            verbose = kwargs.get('verbose', True)\n            print_every = kwargs.get('print_every', 10)\n            if self.optimizer is None:\n                self.optimizer = self.define_optimizer(self.model, **kwargs)\n            for ep_i in range(epoch_num):\n                def closure():\n                    self.optimizer.zero_grad()\n                    output = self.model(self.model.train_inputs[0]) if self.num_inputs == 1 \\\n                        else self.model(*self.model.train_inputs)\n                    loss = -self.mll(output, self.model.train_targets)\n                    loss.backward()\n                    return loss\n\n                loss = self.optimizer.step(closure)\n                xc_ls, xc_ls_str = mcu.get_kernel_lengthscale(self.model.covar_module)\n                xc_os, xc_os_str = mcu.get_kernel_output_scale(self.model.covar_module)\n                y_noise = self.model.likelihood.noise.item()\n                tr_history.append((ep_i, loss.item(), xc_ls, xc_os, y_noise))\n                # print\n                if verbose and ((ep_i % print_every == 0) or (ep_i == epoch_num - 1)):\n                    print(f\"[epoch{ep_i}] loss={loss.item():.3f}, \"\n                          f\"xc_lscale={xc_ls_str}, \"\n                          f\"xc_oscale={xc_os_str}, \"\n                          f\"y_noise={y_noise:.3f}\")\n        return tr_history\n\n    def predict(self, X):\n        pred = self.model.posterior(X)\n        return pred.mean, pred.variance\n\n    def get_posterior(self, X):\n        return self.model.posterior(X)\n\nFile Path: AIRBO/models/ugp.py\nContent:\n\"\"\"\nuGP implementation\n\"\"\"\nfrom models.mmd_gp import MMDGP\nfrom kernels.kme_kernel import KMEKernel\nimport gpytorch as gpyt\n\n\nclass UGP(MMDGP):\n    def __init__(self, train_inputs, train_targets, likelihood, num_inputs,\n                 input_transform=None, outcome_transform=None, additional_transform=None,\n                 hidden_dims=(4, 2), latent_dim=1, **kwargs):\n        super(UGP, self).__init__(train_inputs, train_targets, likelihood, num_inputs,\n                                        input_transform, outcome_transform,\n                                        additional_transform, **kwargs)\n\n    def define_covar_module(self, **kwargs):\n        xc_kern_params = {}\n        xc_lscale_constr = kwargs.get('xc_ls_constr', None)\n        if xc_lscale_constr is not None:\n            xc_kern_params['lengthscale_constraint'] = xc_lscale_constr\n\n        xc_kme_inner_k = kwargs.get('base_kernel', None)\n        if xc_kme_inner_k is None:\n            xc_kme_inner_k = gpyt.kernels.RBFKernel(**xc_kern_params)\n\n        estimator_name = kwargs.get('estimator_name', 'integral')\n        chunk_size = kwargs.get('chunk_size', 100)\n        sub_samp_size = kwargs.get('sub_samp_size', 100)\n        covar_module = gpyt.kernels.ScaleKernel(\n            KMEKernel(xc_kme_inner_k, estimator=estimator_name, chunk_size=chunk_size)\n        )\n        return covar_module\nFile Path: AIRBO/models/uncertain_gp.py\nContent:\nfrom model_utils import input_transform as tfx\nfrom gpytorch.kernels import GaussianSymmetrizedKLKernel, RBFKernel\nfrom kernels.expected_rbf_kernel import ExpectedRBFKernel\nfrom models.robust_gp import RobustGP\n\nimport gpytorch as gpyt\nfrom botorch.models import transforms as tf\nimport torch\n\nKN_EXPECTED_RBF = 'ERBF'\nKN_SKL = \"SKL\"\nKN_RBF = 'rbf'\n\n\nclass UncertainGP(RobustGP):\n    def __init__(self, train_inputs, train_targets, likelihood, num_inputs,\n                 input_transform=None, outcome_transform=None, additional_transform=None,\n                 **kwargs):\n        super(UncertainGP, self).__init__(train_inputs, train_targets, likelihood, num_inputs,\n                                          input_transform, outcome_transform,\n                                          additional_transform, **kwargs)\n        self.kernel_name = kwargs.get('kernel_name', KN_SKL)\n\n    def define_covar_module(self, **kwargs):\n        n_var = kwargs['n_var']\n        self.kernel_name = kwargs['kernel_name']\n        xc_lscale_constr = kwargs.get('xc_lscale_constr', None)\n        xc_kern_params = {'ard_num_dims': n_var}\n        if xc_lscale_constr is not None:\n            xc_kern_params['lengthscale_constraint'] = xc_lscale_constr\n\n        if self.kernel_name == KN_EXPECTED_RBF:\n            xc_kern_params['ard_num_dims'] = None  # ERBF kernel cannot use ARD\n            xc_covar_module = gpyt.kernels.ScaleKernel(ExpectedRBFKernel(**xc_kern_params))\n        elif self.kernel_name == KN_SKL:\n            xc_kern_params['ard_num_dims'] = None  # SKL kernel cannot use ARD\n            xc_covar_module = gpyt.kernels.ScaleKernel(\n                GaussianSymmetrizedKLKernel(**xc_kern_params)\n            )\n        elif self.kernel_name == KN_RBF:\n            xc_covar_module = gpyt.kernels.ScaleKernel(RBFKernel(**xc_kern_params))\n        else:\n            raise ValueError(\"Unsupported kernel type:\", self.kernel_name)\n\n        return xc_covar_module\n\n    def define_default_input_transform(self, **kwargs):\n        n_var = kwargs['n_var']\n        input_bounds = kwargs['input_bounds']\n        return tfx.MultiInputTransform(\n            tf1=tf.Normalize(d=n_var, bounds=input_bounds, transform_on_train=True),\n            tf2=tfx.ScaleTransform(d=n_var, bounds=input_bounds, transform_on_train=True),\n            tf3=tfx.DummyTransform(transform_on_train=True),\n        )\n\n    def define_additional_transform(self, **kwargs):\n        raw_input_std = kwargs['raw_input_std']\n        return tfx.AdditionalFeatures(f=tfx.additional_std, transform_on_train=False,\n                                      fkwargs={'std': raw_input_std})\n\n    def compute_mean_cov(self, X, **kwargs):\n        xc_raw, xc_std, xe = X\n        mean_x, covar = None, None\n        if self.kernel_name == KN_SKL:\n            xc_input = torch.concat((xc_raw, xc_std.pow(2).log()), dim=-1)  # SKL takes log variance\n        elif self.kernel_name == KN_EXPECTED_RBF:\n            xc_input = torch.concat((xc_raw, xc_std.pow(2)), dim=-1)  # eRBF takes variance as input\n        else:\n            xc_input = xc_raw\n\n        # input K\n        mean_x = self.mean_module(xc_raw)\n        covar_x = self.covar_module(xc_input, **kwargs)\n        return mean_x, covar_x\n\n    def forward(self, xc_raw, xc_std, xe):\n        # note that we assume X is already applied with additional transform\n        # input transform\n        X = (xc_raw, xc_std, xe)\n        if self.training:\n            X = self.transform_inputs(X)\n        mean_x, covar = self.compute_mean_cov(X)\n        return gpyt.distributions.MultivariateNormal(mean_x, covar)\n\nFile Path: AIRBO/tests/compare_robust_optimization.py\nContent:\n\"\"\"\nTest the robust BO\n\"\"\"\nimport numpy as np\nimport torch\n\nRND_SEED = 42\nnp.random.seed(RND_SEED)\ntorch.manual_seed(RND_SEED)\n\nfrom model_utils.input_transform import additional_xc_samples, add_noise\nfrom problems.problem_factory import get_test_problem, TestFunctions\nfrom models.robust_gp import RobustGPModel, RobustGP\nfrom models.uncertain_gp import UncertainGP, KN_SKL, KN_EXPECTED_RBF\nfrom models.ugp import UGP\nfrom models.mmd_gp import MMDGP\nfrom utils.tb_logger import OptLogger\nimport utils.commons as cm\nfrom utils import visulaization as vis\nimport model_utils.model_common_utils as mcu\nfrom utils import input_uncertainty as iu\n\nimport matplotlib.pyplot as plt\nimport botorch as bot\nfrom tqdm.auto import tqdm\nfrom functools import partial\nfrom scipy import stats\nimport pandas as pd\nfrom typing import Callable, Dict\nimport os\nfrom torch import multiprocessing\nimport traceback\nimport copy\n\nmp = multiprocessing.get_context('spawn')\n\nOPTIMUM_BOV = 'best_observed_value'\nOPTIMUM_BE = 'best_expectation'\n\n\ndef run_BO(prob: TestFunctions,\n           minimization: bool,\n           num_expectation_eval,\n           raw_input_mean: [float, np.array],\n           raw_input_std: [float, np.array],\n           xc_sample_size: int, n_var: int,\n           input_sampling_func: Callable,\n           input_type: str,\n           init_xc_raw: np.array,\n           init_y,\n           init_expected_y: np.array,\n           model_name,\n           model_cls,\n           model_config: Dict,\n           fit_cfg: Dict,\n           pred_cfgs: Dict,\n           opt_cfg: Dict,\n           x_bounds: np.array,\n           n_iter: int,\n           batch_size: int,\n           n_restarts: int,\n           raw_samples: int,\n           converge_thr: float,\n           oracle_optimum,\n           oracle_opt_x,\n           plot_freq: int,\n           trial_name: str,\n           save_dir: str,\n           **kwargs):\n    \"\"\"\n    Run a BayesOpt\n    \"\"\"\n    print(f\"[pid{os.getpid()}] {trial_name} starts...\")\n    device = model_config['device']\n    cm.set_gmem_usage(device, reserved_gmem=6)\n    dtype = model_config['dtype']\n    max_acq_opt_retries = kwargs.get('max_acq_opt_retries', 3)\n    opt_hist = []\n    ret_que = kwargs.get(\"return_queue\", None)\n    # robust_end_pos = kwargs.get(\"robust_end_pos\", np.array([3, 3]).reshape(1, 2))\n    # env = prob.kwargs['env']\n    try:\n        # Initialize the opt logger\n        opt_logger = OptLogger(['E[y]', ], [1.0, ], constr_names=None, tag=trial_name,\n                               result_save_freq=1000, minimization=minimization)\n\n        # BO loop\n        tr_xc_raw = init_xc_raw.copy()\n        tr_y = init_y.copy()\n        optimum_method = opt_cfg.get('optimum_method', OPTIMUM_BOV)\n        opt_x, opt_py, opt_expected_y = find_optimum(\n            OPTIMUM_BOV, None, prob, n_var, minimization, tr_xc_raw, tr_y, None,\n            input_sampling_func, num_expectation_eval\n        )\n        opt_logger.tb_logger.add_scalar(\"opt_expected_y\", opt_expected_y, global_step=0)\n        opt_hist.append((0, init_xc_raw, init_y, init_expected_y, opt_x, opt_py, opt_expected_y))\n\n        obj2opt = bot.acquisition.objective.ScalarizedPosteriorTransform(\n            weights=torch.tensor([-1.0 if minimization else 1.0], dtype=dtype, device=device)\n        )\n        te_xc_raw, te_y = prob.mesh_coords, prob.mesh_vals\n        te_xc_raw_ts = torch.tensor(te_xc_raw, dtype=dtype, device=device) \\\n            if te_xc_raw is not None else None\n        for iter_i in tqdm(range(1, n_iter + 1), desc=f'{trial_name}',\n                           dynamic_ncols=True, leave=True):\n            # prepare train data\n            tr_x_ts, tr_y_ts = mcu.prepare_data(\n                input_type, n_var, raw_input_mean, raw_input_std, xc_sample_size,\n                input_sampling_func,\n                tr_xc_raw, tr_y, dtype=dtype, device=device\n            )\n\n            # build model\n            model = RobustGPModel(model_cls, **model_config)\n            model.post_initialize(tr_x_ts, tr_y_ts)\n\n            # fit\n            fit_cfg['fit_name'] = f\"{trial_name}/iter{iter_i}\"\n            model.fit(**fit_cfg)\n\n            # find current optimum\n            opt_x, opt_py, opt_expected_y = find_optimum(\n                optimum_method, model, prob, n_var, minimization,\n                tr_xc_raw, tr_y, tr_x_ts, input_sampling_func, num_expectation_eval\n            )\n            regret = oracle_optimum - opt_expected_y if oracle_optimum is not None \\\n                else opt_expected_y\n            dist_2_opt = np.linalg.norm(oracle_opt_x - opt_x) if oracle_opt_x is not None else None\n            opt_logger.tb_logger.add_scalar(\"opt_expected_y\", opt_expected_y, global_step=iter_i)\n            opt_logger.tb_logger.add_scalar(\"regret\", regret, global_step=iter_i)\n            if dist_2_opt is not None:\n                opt_logger.tb_logger.add_scalar(\"distance_2_opt\", dist_2_opt, global_step=iter_i)\n            tqdm.write(f'[{trial_name}] current opt E[y]: {opt_expected_y:.3f}')\n\n            # optimize\n            acq_opt_success = False\n            n_acq_opt_retry = 0\n            x_candidates, candidate_acq_vals, acq = None, None, None\n            while not acq_opt_success:\n                try:\n                    # acq = bot.acquisition.UpperConfidenceBound(\n                    #     model=model.model, beta=4.0, posterior_transform=obj2opt\n                    # )\n                    acq = bot.acquisition.analytic.ExpectedImprovement(\n                        model.model, best_f=min(tr_y) if minimization else max(tr_y),\n                        posterior_transform=obj2opt\n                    )\n                    x_candidates, candidate_acq_vals = bot.optim.optimize_acqf(\n                        acq_function=acq,\n                        bounds=x_bounds,\n                        q=batch_size,\n                        num_restarts=n_restarts,\n                        raw_samples=raw_samples,\n                        options={\"maxiter\": 500},\n                    )\n                    acq_opt_success = True\n                except Exception as e:\n                    acq_opt_success = False\n                    if n_acq_opt_retry >= max_acq_opt_retries:\n                        raise e\n                    else:\n                        print('[Warn] Acq. optimization fails, try again:', e)\n                        n_acq_opt_retry += 1\n\n            # observe new values\n            new_xc_raw = x_candidates.detach().cpu().numpy()\n            new_xc_n = add_noise(new_xc_raw, sampling_func=input_sampling_func)\n            new_y_n = prob.evaluate(new_xc_n)\n            new_expected_y = prob.evaluate(\n                additional_xc_samples(\n                    new_xc_raw, num_expectation_eval, n_var, input_sampling_func\n                ).reshape(-1, n_var),\n            ).reshape(new_xc_raw.shape[0], -1).mean(axis=-1)\n\n            # save to opt history\n            opt_hist.append((iter_i, new_xc_raw, new_y_n, new_expected_y,\n                             opt_x, opt_py, opt_expected_y))\n\n            # visualize model prediction & acq.\n            if n_var == 1 and (iter_i % plot_freq == 0 or iter_i == n_iter - 1):\n                # eval\n                with torch.no_grad():\n                    te_pred = model.get_posterior(te_xc_raw_ts)\n                    te_py = te_pred.mean.detach().cpu().numpy()\n                    te_lcb, te_ucb = te_pred.mvn.confidence_region()\n                    te_lcb, te_ucb = te_lcb.detach().cpu().numpy(), te_ucb.detach().cpu().numpy()\n                    te_acq_vals = acq(te_xc_raw_ts.unsqueeze(-2)).detach().cpu().numpy()\n                    xc_ls, xc_ls_str = mcu.get_kernel_lengthscale(model.model.covar_module)\n                    xc_os, xc_os_str = mcu.get_kernel_output_scale(model.model.covar_module)\n                    lkh_noise = model.likelihood.noise.item()\n\n                # plot\n                nrows, ncols, ax_scale = 2, 1, 1.5\n                fig = plt.figure(figsize=(4 * ncols * ax_scale, 3 * nrows * ax_scale))\n                axes = fig.subplots(nrows=nrows, ncols=ncols, squeeze=True, sharex=True)\n                # plot model pred\n                vis.plot_gp_predictions(\n                    [(model_name, te_py.flatten(), te_ucb.flatten(), te_lcb.flatten(),\n                      None, None), ],\n                    te_xc_raw, prob.mesh_coords, prob.mesh_vals,\n                    tr_xc_raw.flatten(), tr_y.flatten(),\n                    fig=fig, ax=axes[0]\n                )\n                vis.scatter(\n                    new_xc_raw.flatten(), new_y_n.flatten(), fig=fig, ax=axes[0],\n                    label='new_p', marker='^', color='lime'\n                )\n                vis.scatter(\n                    opt_x.flatten(), opt_py.flatten(), fig=fig, ax=axes[0],\n                    label='optimum', marker='*', color='magenta'\n                )\n                axes[0].axvline(opt_x.flatten()[0], color='magenta', ls='--')\n                axes[0].set_title(f\"{model_name}\\n\"\n                                  f\"xc_ls={xc_ls_str}, xc_os={xc_os_str}, noise={lkh_noise:.3f}\")\n                axes[0].legend()\n\n                # plot acq\n                axes[1].plot(te_xc_raw.flatten(), te_acq_vals.flatten(),\n                             label=f'{acq.__class__.__name__}')\n                vis.scatter(\n                    new_xc_raw.flatten(), candidate_acq_vals.detach().cpu().numpy().flatten(),\n                    fig=fig, ax=axes[1],\n                    label='new_p', marker='^', color='lime'\n                )\n                axes[1].legend()\n                fig.tight_layout()\n                opt_logger.tb_logger.add_figure('model_pred', fig, iter_i, close=True)\n\n            # concat to the training data\n            tr_xc_raw = np.concatenate([tr_xc_raw, new_xc_raw], axis=0)\n            tr_y = np.concatenate([tr_y, new_y_n], axis=0)\n\n    except Exception as e:\n        # except ImportError as e:\n        print(f\"[Error] Trial: {trial_name} fails, its opt_hist might be incomplete.\", e)\n        print(traceback.format_exc())\n    finally:\n        cm.serialize_obj(opt_hist, f\"{save_dir}{trial_name}.opt_hist\")\n\n    if ret_que is not None:\n        ret_que.put((trial_name, opt_hist))\n    else:\n        return trial_name, opt_hist\n\n\ndef find_optimum(optimum_method, model, prob, n_var, minimization,\n                 tr_xc_raw, tr_y, tr_x_ts, input_sampling_func, num_expectation_eval):\n    opt_x, opt_py, opt_expected_y = None, None, None\n    with torch.no_grad():\n        if optimum_method == OPTIMUM_BOV:\n            opt_ind = np.argmin(tr_y) if minimization else np.argmax(tr_y)\n            opt_x = tr_xc_raw[opt_ind:opt_ind + 1]\n            if model is not None and tr_x_ts is not None:\n                opt_py = model.get_posterior(tr_x_ts[opt_ind:opt_ind + 1]) \\\n                    .mean.detach().cpu().numpy()\n        elif optimum_method == OPTIMUM_BE:\n            if model is None or tr_x_ts is None:\n                raise ValueError(\"model and tr_x_ts should NOT be none.\")\n            observed_py = model.get_posterior(tr_x_ts).mean.detach().cpu().numpy()\n            opt_ind = np.argmin(observed_py) if minimization else np.argmax(observed_py)\n            opt_x = tr_xc_raw[opt_ind: opt_ind + 1]\n            opt_py = observed_py[opt_ind: opt_ind + 1]\n        else:\n            raise ValueError('Unsupported optimum method:', optimum_method)\n\n    opt_expected_y = prob.evaluate(\n        additional_xc_samples(\n            opt_x.reshape(-1, n_var), num_expectation_eval, n_var,\n            input_sampling_func\n        ).reshape(-1, n_var),\n    ).flatten().mean()\n    return opt_x, opt_py, opt_expected_y\n\nFile Path: AIRBO/tests/compare_optimization_in_push_world.py\nContent:\n\"\"\"\nTest the robust BO in the push world\n\"\"\"\nRND_SEED = 1910\nimport numpy as np\nimport torch\n\nnp.random.seed(RND_SEED)\ntorch.manual_seed(RND_SEED)\nfrom models.ugp import UGP\nfrom model_utils.input_transform import additional_xc_samples, add_noise, AdditionalFeatures\nfrom problems.problem_factory import get_test_problem, TestFunctions\nfrom problems.robot_pushing import push_env as pe\nfrom models.robust_gp import RobustGPModel, RobustGP\nfrom models.mmd_gp import MMDGP\nfrom models.uncertain_gp import UncertainGP, KN_SKL, KN_EXPECTED_RBF\nfrom utils.tb_logger import OptLogger\nimport utils.commons as cm\nfrom utils import visulaization as vis\nimport model_utils.model_common_utils as mcu\nfrom utils import input_uncertainty as iu\n\nimport matplotlib.pyplot as plt\nimport botorch as bot\nfrom tqdm.auto import tqdm\nimport pandas as pd\nfrom typing import Callable, Dict\nimport os\nfrom torch import multiprocessing\nimport traceback\nimport copy\n\nmp = multiprocessing.get_context('spawn')\n\nOPTIMUM_BOV = 'best_observed_value'\nOPTIMUM_BE = 'best_expectation'\n\n\ndef run_BO(prob: TestFunctions,\n           minimization: bool,\n           num_expectation_eval,\n           raw_input_mean: [float, np.array],\n           raw_input_std: [float, np.array],\n           xc_sample_size: int, n_var: int,\n           input_sampling_func: Callable,\n           input_type: str,\n           init_xc_raw: np.array,\n           init_y,\n           init_expected_y: np.array,\n           model_name,\n           model_cls,\n           model_config: Dict,\n           fit_cfg: Dict,\n           pred_cfgs: Dict,\n           opt_cfg: Dict,\n           x_bounds: np.array,\n           n_iter: int,\n           batch_size: int,\n           n_restarts: int,\n           raw_samples: int,\n           converge_thr: float,\n           oracle_optimum,\n           oracle_opt_x,\n           plot_freq: int,\n           trial_name: str,\n           save_dir: str,\n           **kwargs):\n    \"\"\"\n    Run a BayesOpt\n    \"\"\"\n    print(f\"[pid{os.getpid()}] {trial_name} starts...\")\n    device = model_config['device']\n    cm.set_gmem_usage(device, reserved_gmem=6)\n    dtype = model_config['dtype']\n    max_acq_opt_retries = kwargs.get('max_acq_opt_retries', 3)\n    opt_hist = []\n    ret_que = kwargs.get(\"return_queue\", None)\n    robust_end_pos = kwargs.get(\"robust_end_pos\", np.array([3, 3]).reshape(1, 2))\n    env = prob.kwargs['env']\n    try:\n        # Initialize the opt logger\n        opt_logger = OptLogger(['E[y]', ], [1.0, ], constr_names=None, tag=trial_name,\n                               result_save_freq=1000, minimization=minimization,\n                               log_root_dir='./tb_logs/')\n\n        # BO loop\n        tr_xc_raw = init_xc_raw.copy()\n        tr_y = init_y.copy()\n        optimum_method = opt_cfg.get('optimum_method', OPTIMUM_BOV)\n        opt_x, opt_py, opt_expected_y = find_optimum(\n            OPTIMUM_BOV, None, prob, n_var, minimization, tr_xc_raw, tr_y, None,\n            input_sampling_func, num_expectation_eval\n        )\n        opt_logger.tb_logger.add_scalar(\"opt_expected_y\", opt_expected_y, global_step=0)\n        opt_hist.append((0, init_xc_raw, init_y, init_expected_y, opt_x, opt_py, opt_expected_y))\n\n        obj2opt = bot.acquisition.objective.ScalarizedPosteriorTransform(\n            weights=torch.tensor([-1.0 if minimization else 1.0], dtype=dtype, device=device)\n        )\n        te_xc_raw, te_y = prob.mesh_coords, prob.mesh_vals\n        te_xc_raw_ts = torch.tensor(te_xc_raw, dtype=dtype, device=device) \\\n            if te_xc_raw is not None else None\n        for iter_i in tqdm(range(1, n_iter + 1), desc=f'{trial_name}',\n                           dynamic_ncols=True, leave=True):\n            # prepare train data\n            tr_x_ts, tr_y_ts = mcu.prepare_data(\n                input_type, n_var, raw_input_mean, raw_input_std, xc_sample_size,\n                input_sampling_func,\n                tr_xc_raw, tr_y, dtype=dtype, device=device\n            )\n\n            # build model\n            model = RobustGPModel(model_cls, **model_config)\n            model.post_initialize(tr_x_ts, tr_y_ts)\n\n            # fit\n            fit_cfg['fit_name'] = f\"{trial_name}/iter{iter_i}\"\n            model.fit(**fit_cfg)\n\n            # find current optimum\n            opt_x, opt_py, opt_expected_y = find_optimum(\n                optimum_method, model, prob, n_var, minimization,\n                tr_xc_raw, tr_y, tr_x_ts, input_sampling_func, num_expectation_eval\n            )\n\n            # log\n            goals, opt_end_pos, dist = env.evaluate_ex(*opt_x)\n            dist = float(dist)\n            regret = dist\n            dist_2_opt = np.linalg.norm(robust_end_pos - opt_end_pos)\n            opt_logger.tb_logger.add_scalar(\"opt_expected_y\", opt_expected_y, global_step=iter_i)\n            opt_logger.tb_logger.add_scalar(\"regret\", regret, global_step=iter_i)\n            if dist_2_opt is not None:\n                opt_logger.tb_logger.add_scalar(\"distance_2_opt\", dist_2_opt, global_step=iter_i)\n            tqdm.write(f'[{trial_name}] current opt E[y]: {opt_expected_y:.3f}')\n            rx, ry, rt = opt_x.flatten()[:3]\n            fig, ax = pe.plot_push_world(goals, opt_end_pos, dist, rx, ry, rt,\n                                         x_range=(-6, 6), y_range=(-6, 6))\n            opt_logger.tb_logger.add_figure(\"push_world\", fig, global_step=iter_i, close=True)\n\n            # optimize\n            acq_opt_success = False\n            n_acq_opt_retry = 0\n            x_candidates, candidate_acq_vals, acq = None, None, None\n            while not acq_opt_success:\n                try:\n                    # acq = bot.acquisition.UpperConfidenceBound(\n                    #     model=model.model, beta=4.0, posterior_transform=obj2opt\n                    # )\n                    acq = bot.acquisition.analytic.ExpectedImprovement(\n                        model.model, best_f=min(tr_y) if minimization else max(tr_y),\n                        posterior_transform=obj2opt\n                    )\n                    x_candidates, candidate_acq_vals = bot.optim.optimize_acqf(\n                        acq_function=acq,\n                        bounds=x_bounds,\n                        q=batch_size,\n                        num_restarts=n_restarts,\n                        raw_samples=raw_samples,\n                        options={\"maxiter\": 500},\n                    )\n                    acq_opt_success = True\n                except Exception as e:\n                    acq_opt_success = False\n                    if n_acq_opt_retry >= max_acq_opt_retries:\n                        raise e\n                    else:\n                        print('[Warn] Acq. optimization fails, try again:', e)\n                        n_acq_opt_retry += 1\n\n            # observe new values\n            new_xc_raw = x_candidates.detach().cpu().numpy()\n            new_xc_n = add_noise(new_xc_raw, sampling_func=input_sampling_func)\n            new_y_n = prob.evaluate(new_xc_n)\n            new_expected_y = prob.evaluate(\n                additional_xc_samples(\n                    new_xc_raw, num_expectation_eval, n_var, input_sampling_func\n                ).reshape(-1, n_var),\n            ).reshape(new_xc_raw.shape[0], -1).mean(axis=-1)\n\n            # save to opt history\n            opt_hist.append((iter_i, new_xc_raw, new_y_n, new_expected_y,\n                             opt_x, opt_py, opt_expected_y, opt_end_pos))\n\n            # visualize model prediction & acq.\n            if n_var == 1 and (iter_i % plot_freq == 0 or iter_i == n_iter - 1):\n                # eval\n                with torch.no_grad():\n                    te_pred = model.get_posterior(te_xc_raw_ts)\n                    te_py = te_pred.mean.detach().cpu().numpy()\n                    te_lcb, te_ucb = te_pred.mvn.confidence_region()\n                    te_lcb, te_ucb = te_lcb.detach().cpu().numpy(), te_ucb.detach().cpu().numpy()\n                    te_acq_vals = acq(te_xc_raw_ts.unsqueeze(-2)).detach().cpu().numpy()\n                    xc_ls, xc_ls_str = mcu.get_kernel_lengthscale(model.model.covar_module)\n                    xc_os, xc_os_str = mcu.get_kernel_output_scale(model.model.covar_module)\n                    lkh_noise = model.likelihood.noise.item()\n\n                # plot\n                nrows, ncols, ax_scale = 2, 1, 1.5\n                fig = plt.figure(figsize=(4 * ncols * ax_scale, 3 * nrows * ax_scale))\n                axes = fig.subplots(nrows=nrows, ncols=ncols, squeeze=True, sharex=True)\n                # plot model pred\n                vis.plot_gp_predictions(\n                    [(model_name, te_py.flatten(), te_ucb.flatten(), te_lcb.flatten(),\n                      None, None), ],\n                    te_xc_raw, prob.mesh_coords, prob.mesh_vals,\n                    tr_xc_raw.flatten(), tr_y.flatten(),\n                    fig=fig, ax=axes[0]\n                )\n                vis.scatter(\n                    new_xc_raw.flatten(), new_y_n.flatten(), fig=fig, ax=axes[0],\n                    label='new_p', marker='^', color='lime'\n                )\n                vis.scatter(\n                    opt_x.flatten(), opt_py.flatten(), fig=fig, ax=axes[0],\n                    label='optimum', marker='*', color='magenta'\n                )\n                axes[0].axvline(opt_x.flatten()[0], color='magenta', ls='--')\n                axes[0].set_title(f\"{model_name}\\n\"\n                                  f\"xc_ls={xc_ls_str}, xc_os={xc_os_str}, noise={lkh_noise:.3f}\")\n                axes[0].legend()\n\n                # plot acq\n                axes[1].plot(te_xc_raw.flatten(), te_acq_vals.flatten(),\n                             label=f'{acq.__class__.__name__}')\n                vis.scatter(\n                    new_xc_raw.flatten(), candidate_acq_vals.detach().cpu().numpy().flatten(),\n                    fig=fig, ax=axes[1],\n                    label='new_p', marker='^', color='lime'\n                )\n                axes[1].legend()\n                fig.tight_layout()\n                opt_logger.tb_logger.add_figure('model_pred', fig, iter_i, close=True)\n\n            # concat to the training data\n            tr_xc_raw = np.concatenate([tr_xc_raw, new_xc_raw], axis=0)\n            tr_y = np.concatenate([tr_y, new_y_n], axis=0)\n\n    except Exception as e:\n        # except ImportError as e:\n        print(f\"[Error] Trial: {trial_name} fails, its opt_hist might be incomplete.\", e)\n        print(traceback.format_exc())\n    finally:\n        cm.serialize_obj(opt_hist, f\"{save_dir}{trial_name}.opt_hist\")\n\n    if ret_que is not None:\n        ret_que.put((trial_name, opt_hist))\n    else:\n        return trial_name, opt_hist\n\n\ndef find_optimum(optimum_method, model, prob, n_var, minimization,\n                 tr_xc_raw, tr_y, tr_x_ts, input_sampling_func, num_expectation_eval):\n    opt_x, opt_py, opt_expected_y = None, None, None\n    with torch.no_grad():\n        if optimum_method == OPTIMUM_BOV:\n            opt_ind = np.argmin(tr_y) if minimization else np.argmax(tr_y)\n            opt_x = tr_xc_raw[opt_ind:opt_ind + 1]\n            if model is not None and tr_x_ts is not None:\n                opt_py = model.get_posterior(tr_x_ts[opt_ind:opt_ind + 1]) \\\n                    .mean.detach().cpu().numpy()\n        elif optimum_method == OPTIMUM_BE:\n            if model is None or tr_x_ts is None:\n                raise ValueError(\"model and tr_x_ts should NOT be none.\")\n            observed_py = model.get_posterior(tr_x_ts).mean.detach().cpu().numpy()\n            opt_ind = np.argmin(observed_py) if minimization else np.argmax(observed_py)\n            opt_x = tr_xc_raw[opt_ind: opt_ind + 1]\n            opt_py = observed_py[opt_ind: opt_ind + 1]\n        else:\n            raise ValueError('Unsupported optimum method:', optimum_method)\n\n    opt_expected_y = prob.evaluate(\n        additional_xc_samples(\n            opt_x.reshape(-1, n_var), num_expectation_eval, n_var,\n            input_sampling_func\n        ).reshape(-1, n_var),\n    ).flatten().mean()\n    return opt_x, opt_py, opt_expected_y",
    "Experiment Result": "General Setup:\n- `debug_mode`: False\n- `use_gpu`: Varies based on available resources, `torch.cuda.is_available()` check.\n- `use_multiprocess`: True (parallel execution of trials).\n- `use_double_precision`: False (default `torch.float32`).\n- `n_trial`: 10 (number of trials for each optimization setting).\n- `n_iter`: 100 (number of optimization iterations).\n- `xc_sample_size`: 160 (sample size for each uncertain input sample, for MMD/KME).\n- `sub_samp_size`: 10 (sub-sampling size for Nystrom MMD estimator).\n- `naive_xc_sample_size`: derived as `int((xc_sample_size * sub_samp_size) ** 0.5)`.\n- `init_samp_num`: 10 (number of initial samples).\n- `save_root_dir`: \"./results/\".\n\nProblem Setup (from `compare_robust_optimization.py`):\n- `raw_fname`: 'RKHS-S' (default, can be 'BumpedBowlHD', 'RKHS-S', 'CustomK').\n- `n_var`: 1 (default, input dimension of test function).\n- `minimization`: True (problem is minimization).\n- `num_expectation_eval`: 500 (number of evaluations for true expectation).\n- `input_uncertainty_type`: 'step_chi2' (default, can be 'norm', 'gmm', 'beta', 'chi2', 'varying_beta', 'uniform', 'concated_circular').\n- `raw_input_std`: 0.01 (standard deviation of input uncertainty).\n- The true optimum (`oracle_robust_optimum`, `oracle_robust_opt_x`) and convergence thresholds (`converge_thr`) are computed based on the chosen function and input distribution.\n\nProblem Setup (from `compare_optimization_in_push_world.py`):\n- `raw_fname`: \"TripleGoalsP3\".\n- `robust_end_pos`: `np.array([5.3, 3.0]).reshape(1, 2)`. This is the target end position for the robot pushing task.\n- `unknown_optimum`: True.\n- `n_var`: 3.\n- `minimization`: True.\n- `num_expectation_eval`: 700.\n- `input_distrib`: `iu.GMMInputDistribution` with 2 components, specific means and tied covariance (including `min_cov = 1e-6`).\n- Initial samples (`init_xc_raw`, `init_y_n`, `init_expected_y`) are generated by random uniform sampling within input bounds, adding noise, and evaluating expected values.\n\nModel Configuration (`default_model_cfg`):\n- `noise_free`: False.\n- `n_var`: Problem's `n_var`.\n- `input_bounds`: Problem's `x_bounds`.\n- `num_inputs`: 1 (default for `RobustGP`), 3 for MMDGP/UGP/UncertainGP.\n- `dtype`: `torch.double` if `use_double_precision`, else `torch.float32`.\n- `device`: `torch.device('cuda:0')` if `use_gpu`, else `torch.device('cpu')`.\n- `input_type`: 'exact_input' (default for `RobustGP`), 'sample_input' for MMDGP/UGP, 'distribution_input' for UncertainGP.\n- `raw_input_std`: Problem's `raw_input_std`.\n- `raw_input_mean`: Problem's `raw_input_mean`.\n\nModel Training Configuration (`default_fit_cfg`):\n- `epoch_num`: 150 (5 if `debug_mode`).\n- `lr`: 5e-2.\n- `fit_with_scipy`: False.\n- `dtype`, `device`: Same as model config.\n- `print_every`: 10 (logging frequency).\n\nOptimization Settings:\n- `batch_size`: 1 (number of new points acquired in each iteration).\n- `n_restarts`: 10 (number of restarts for acquisition function optimization).\n- `raw_samples`: 512 (number of raw samples for acquisition function optimization).\n- `outcome_plot_freq` (or `pred_plot_freq`): 10 if `n_var == 1`, else 9e99 (visualization frequency).\n\nAcquisition Function:\n- `bot.acquisition.analytic.ExpectedImprovement` is used, optimized using `bot.optim.optimize_acqf`.\n\nOptimum Finding:\n- `find_optimum` function used to identify the current best configuration.\n- `optimum_method`: 'best_observed_value' or 'best_expectation' depending on the model.\n\nModel Candidates Compared:\n- **MMDGP-nystrom**: `MMDGP` with `nystrom` estimator, `input_type='sample_input'`, `num_inputs=3`, `xc_sample_size=160`, `sub_samp_size=10`.\n- **uGP**: `UGP` with `integral` estimator, `input_type='sample_input'`, `num_inputs=3`, `xc_sample_size` = `naive_xc_sample_size`.\n- **GP**: `RobustGP` (standard GP).\n- **skl**: `UncertainGP` with `kernel_name='SKL'`, `input_type='distribution_input'`, `num_inputs=3`.\n- **ERBF**: `UncertainGP` with `kernel_name='ERBF'`, `input_type='distribution_input'`, `num_inputs=3`."
}{
    "Title": "Few-Shot Bayesian Optimization with Deep Kernel Surrogates",
    "Main Contributions": "The paper proposes rethinking hyperparameter optimization (HPO) as a few-shot learning problem, introducing Few-Shot Bayesian Optimization (FSBO). It uses a meta-learned deep kernel network for a Gaussian process surrogate to quickly adapt to new HPO tasks with minimal evaluations. This approach achieves new state-of-the-art results in HPO transfer learning, outperforming strong baselines on diverse metadata sets. Additionally, the work presents an evolutionary algorithm that leverages the surrogate model to provide a data-driven warm-start initialization for Bayesian optimization.",
    "Methodology": "The core methodology involves a deep kernel network where a neural network transforms input features, which then feed into a Gaussian Process kernel for probabilistic predictions. The model is meta-learned using an adaptation of Model-Agnostic Meta-Learning (MAML) for GPs, where task-independent kernel parameters are optimized by maximizing the log marginal likelihood across a collection of source tasks via stochastic gradient ascent. To address label normalization challenges across diverse tasks, a task augmentation strategy is introduced, randomly scaling labels for each training batch to promote scale-invariant representations. For new target tasks, the deep kernel parameters are fine-tuned. A data-driven warm-start initialization is also proposed, utilizing an evolutionary algorithm to identify an optimal initial set of hyperparameters based on the surrogate model's predictions on source tasks, minimizing normalized regret.",
    "Experimental Setup": "Experiments were conducted on three hyperparameter optimization problems: AdaBoost, GLMNet, and SVM. The GLMNet and SVM metadata sets were compiled from 30 OpenML datasets, specifically from 'OpenML Bot R' entries for consistency. The AdaBoost metadata was a publicly available set. Performance was evaluated using the aggregated mean of normalized regrets, with statistical significance determined by the Wilcoxon signed rank test. Validation employed a leave-one-task-out cross-validation for GLMNet and SVM, and a fixed train/test split for AdaBoost. Experiments were repeated ten times. The FSBO model utilized a deep kernel composed of a two-layer neural network (128 → 128 with ReLU) and a squared-exponential kernel, trained with the Adam optimizer (learning rate 10⁻³) and a batch size of 50. A warm start length of five was used. Baselines included Random Search, vanilla Gaussian Process (LHS and Warm Start), RGPE, MetaBO, ABLR (vanilla and Warm Start), Multi-Head GPs (various configurations), and Reptile.",
    "Limitations": "Existing Gaussian Process-based transfer learning methods face scalability issues due to cubic computational complexity with respect to the number of training points. Accurate label normalization for target tasks with only a few observations is challenging, hindering the reliability of surrogate models. The MetaBO baseline, while a strong competitor, showed degraded performance with an increased number of trials, suggesting limitations in its reinforcement learning approach for extended HPO sequences. Furthermore, while the proposed warm-start initialization is generally beneficial, it did not consistently yield a statistically significant improvement over simpler initializations like Latin Hypercube Sampling for all problem types, such as AdaBoost.",
    "Future Research Directions": "A direct future research direction implied by the paper is to explore the combination of the proposed few-shot surrogate model with other model-agnostic meta-learning approaches beyond the specific MAML adaptation and Reptile already tested.",
    "Experiment Code": "# Copyright (c) 2019 Robert Bosch GmbH# This program is free software: you can redistribute it and/or modify# it under the terms of the GNU Affero General Public License as published# by the Free Software Foundation, either version 3 of the License, or# (at your option) any later version.)# This program is distributed in the hope that it will be useful,# but WITHOUT ANY WARRANTY; without even the implied warranty of# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the# GNU Affero General Public License for more details.# You should have received a copy of the GNU Affero General Public License# along with this program.  If not, see <https://www.gnu.org/licenses/>.)# ******************************************************************# policies.py# Implementation of the MetaBO neural AF as well as benchmark AFs.# ******************************************************************import torchimport torch.nn as nnimport torch.nn.functional as Ffrom torch.distributions import Categoricalimport numpy as npfrom metabo.policies.mlp import MLPclass NeuralAF(nn.Module):    \"\"\"    Base class for MetaBO-Policies. Subclasses have to implement init_structure() and forward().    SHAPES:    forward()     states: (N_batch, N_grid, N_features)     logits: (N_batch, N_grid)     values: (N_batch, )    act(): only one action/value at a time in self.act()     state: (N_grid, N_features)     action: ()     value: ()    predict_vals_logps_ents()     states: (N_batch, N_grid, N_features)     actions: (N_batch, )     values: (N_batch, )     logprobs: (N_batch, )     entropies: (N_batch, )    \"\"\"    def __init__(self, observation_space, action_space, deterministic, options):        super(NeuralAF, self).__init__()        self.N_features = None  # has to be set in init_structure()        self.deterministic = deterministic        # initialize the network structure        self.init_structure(observation_space=observation_space, action_space=action_space, options=options)        # initialize weights        self.apply(self.init_weights)    def init_structure(self, observation_space, action_space, options):        self.N_features = observation_space.shape[1]        # activation function        if options[\"activations\"] == \"relu\":            f_act = F.relu        elif options[\"activations\"] == \"tanh\":            f_act = torch.tanh        else:            raise NotImplementedError(\"Unknown activation function!\")        # policy network        self.N_features_policy = self.N_features        if \"exclude_t_from_policy\" in options:            self.exclude_t_from_policy = options[\"exclude_t_from_policy\"]            assert \"t_idx\" in options            self.t_idx = options[\"t_idx\"]            self.N_features_policy = self.N_features_policy - 1 if self.exclude_t_from_policy else self.N_features_policy        else:            self.exclude_t_from_policy = False        if \"exclude_T_from_policy\" in options:            self.exclude_T_from_policy = options[\"exclude_T_from_policy\"]            assert \"T_idx\" in options            self.T_idx = options[\"T_idx\"]            self.N_features_policy = self.N_features_policy - 1 if self.exclude_T_from_policy else self.N_features_policy        else:            self.exclude_T_from_policy = False        self.policy_net = MLP(d_in=self.N_features_policy, d_out=1, arch_spec=options[\"arch_spec\"], f_act=f_act)        # value network        if \"use_value_network\" in options and options[\"use_value_network\"]:            self.use_value_network = True            self.value_net = MLP(d_in=2, d_out=1, arch_spec=options[\"arch_spec_value\"], f_act=f_act)            self.t_idx = options[\"t_idx\"]            self.T_idx = options[\"T_idx\"]        else:            self.use_value_network = False    def forward(self, states):        assert states.dim() == 3        assert states.shape[-1] == self.N_features        # policy network        mask = [True] * self.N_features        if self.exclude_t_from_policy:            mask[self.t_idx] = False        if self.exclude_T_from_policy:            mask[self.T_idx] = False        logits = self.policy_net.forward(states[:, :, mask])        logits.squeeze_(2)        # value network        if self.use_value_network:            tT = states[:, [0], [self.t_idx, self.T_idx]]            values = self.value_net.forward(tT)            values.squeeze_(1)        else:            values = torch.zeros(states.shape[0]).to(logits.device)        return logits, values    def af(self, state):        state = torch.from_numpy(state[None, :].astype(np.float32))        with torch.no_grad():            out = self.forward(state)        af = out[0].to(\"cpu\").numpy().squeeze()        return af    def act(self, state):        # here, state is assumed to contain a single state, i.e. no batch dimension        state = state.unsqueeze(0)  # add batch dimension        out = self.forward(state)        logits = out[0]        value = out[1]        if self.deterministic:            action = torch.argmax(logits)        else:            distr = Categorical(logits=logits)            # to sample the action, the policy uses the current PROCESS-local random seed, don't re-seed in pi.act            action = distr.sample()        return action.squeeze(0), value.squeeze(0)    def predict_vals_logps_ents(self, states, actions):        assert actions.dim() == 1        assert states.shape[0] == actions.shape[0]        out = self.forward(states)        logits = out[0]        values = out[1]        distr = Categorical(logits=logits)        logprobs = distr.log_prob(actions)        entropies = distr.entropy()        return values, logprobs, entropies    def set_requires_grad(self, requires_grad):        for p in self.parameters():            p.requires_grad = requires_grad    def reset(self):        pass    @staticmethod    def num_flat_features(x):        return np.prod(x.size()[1:])    @staticmethod    def init_weights(m):        if type(m) == nn.Linear:            m.weight.data.normal_(mean=0.0, std=0.01)            m.bias.data.fill_(0.0)class MLP(nn.Module):    def __init__(self, d_in: int, d_out: int, arch_spec: list, f_act=None):        \"\"\"        A standard multi-layer perceptron.        :param d_in: number of input features.        :param d_out: number of output features.        :param arch_spec: list containing the number of units in each hidden layer. If arch_spec == [], this is a        linear model.        :param f_act: nonlinear activation function (if arch_spec != [])        \"\"\"        super(MLP, self).__init__()        self.arch_spec = arch_spec        self.f_act = f_act        self.is_linear = (arch_spec == [])  # no hidden layers --> linear model        if not self.is_linear:            assert f_act is not None        # define the network        if self.is_linear:            self.fc = nn.ModuleList([nn.Linear(in_features=d_in, out_features=d_out)])        else:            self.fc = nn.ModuleList([nn.Linear(in_features=d_in, out_features=arch_spec[0])])            for i in range(1, len(arch_spec)):                self.fc.append(nn.Linear(in_features=self.fc[-1].out_features, out_features=arch_spec[i]))            self.fc.append(nn.Linear(in_features=self.fc[-1].out_features, out_features=d_out))    def forward(self, X):        Y = X        if self.is_linear:            Y = self.fc[0](Y)        else:            for layer in self.fc[:-1]:                Y = self.f_act(layer(Y))            Y = self.fc[-1](Y)        return Y",
    "Experiment Result": "The Neural Acquisition Function (NeuralAF) is implemented as an MLP using ReLU activations and an architecture of 4 hidden layers, each with 200 units. A separate value network is also used with the same architecture. The policy is trained using Proximal Policy Optimization (PPO) with a batch size of 1200, 2000 iterations (total 2.4 million steps), 4 epochs, and a learning rate of 1e-4. The PPO algorithm uses an epsilon of 0.15 for clipping, value function coefficient of 1.0, and entropy coefficient of 0.01. GAE-Lambda with gamma=0.98 and lambda=0.98 is used for advantage estimation, and advantages are normalized. Training is performed on 10 workers with seeds from 0 to 9. The environment is 'MetaBO-GP-v0', a 3-dimensional task sampling Gaussian Process objective functions (Matern52 kernel). Lengthscales are sampled between 0.05 and 0.5, and signal/noise variances are 1.0/0.1 respectively. The optimization horizon (T) is 30 steps with 0 initial samples. Rewards are transformed using 'neg_log10' and prior mean function is used. The domain is treated as continuous with local acquisition function optimization enabled, using 2000 multi-start points and 2000 local search points with k=5 top multi-start points for local search."
}{
    "Title": "Efficient Hyperparameter Optimization with Adaptive Fidelity Identification",
    "Main Contributions": "The main research problem is the challenge of adaptively determining an appropriate fidelity for each hyperparameter configuration in multi-fidelity Bayesian Optimization (BO) to fit the surrogate model. The paper proposes FastBO, a multi-fidelity BO method that addresses this challenge by adaptively deciding the fidelity for each configuration based on novel concepts of efficient point and saturation point. FastBO is shown to efficiently offer strong performance and its adaptive fidelity identification strategy provides a general way to extend any single-fidelity method to the multi-fidelity setting.",
    "Methodology": "FastBO is a multi-fidelity Bayesian Optimization method built upon two key concepts: the efficient point and the saturation point. The efficient point (ei) for a configuration λi is defined as the minimum resource level 'r' where doubling resources beyond 'r' results in a performance improvement below a small threshold (δ1). This point represents an optimal resource-to-performance balance and is used as the appropriate fidelity for fitting the surrogate model. The saturation point (si) is defined as the minimum resource level 'r' where performance stabilizes (changes below δ2) beyond 'r', serving as an approximate final fidelity. The FastBO process involves an initial warm-up stage, learning curve estimation for configurations, adaptive extraction of efficient and saturation points, evaluation of configurations up to their efficient points to update the surrogate model, and a post-processing stage where a small set of promising configurations are further evaluated to their saturation points to find the optimal. This adaptive strategy can also be generalized to extend single-fidelity methods.",
    "Experimental Setup": "FastBO's performance was evaluated by comparing it against ten other methods: random search (RS), standard BO, ASHA, Hyperband, PASHA, A-BOHB, A-CQR, BOHB, DyHPO, and Hyper-Tune. The experiments were conducted using three established benchmarks: LCBench, NAS-Bench-201, and FCNet. The primary validation metric focused on assessing the \"anytime performance\" of the algorithms.",
    "Limitations": "Not mentioned",
    "Future Research Directions": "Future research could focus on refining and expanding FastBO to effectively operate within larger search spaces and integrate with distributed computing systems. These extensions aim to improve its general applicability and scalability in more complex and resource-intensive scenarios.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "Bayesian Optimization for Iterative Learning",
    "Main Contributions": "The paper presents a Bayesian optimization (BO) approach, termed BOIL, for efficient hyperparameter tuning of iterative learning algorithms, specifically deep learning (DL) and deep reinforcement learning (DRL). The main contributions include an algorithm that optimizes the learning curve by compressing the entire training progress into a single numeric score based on success and stability, rather than relying solely on final performance. It introduces a data augmentation technique that leverages intermediate information from the iterative process by selectively including scores from different training steps, enhancing sample efficiency and addressing potential Gaussian Process (GP) covariance matrix conditioning issues. The algorithm is demonstrated to outperform existing baselines in identifying optimal hyperparameters in minimal wall-clock time for DRL agents and convolutional neural networks.",
    "Methodology": "The BOIL methodology models the cost-sensitive black-box function, representing hyperparameter performance over training iterations, using a Gaussian Process (GP) with a product kernel combining hyperparameter and iteration spaces. The training time cost is approximated by a linear regressor. The algorithm selects the next evaluation point by maximizing a cost-aware Expected Improvement acquisition function. A key component is training curve compression, which transforms the raw learning curve into a numeric score using a dynamically learned Sigmoid (Logistic) preference function. This preference function's parameters (growth and midpoint) are optimized by maximizing the GP's log marginal likelihood. To improve sample efficiency and prevent GP covariance matrix ill-conditioning, BOIL employs a selective data augmentation technique, sampling points from the observed curve that maximize GP predictive uncertainty while adhering to a condition number threshold for the covariance matrix.",
    "Experimental Setup": "Experiments were conducted on two DRL agents (Dueling DQN on CartPole-v0, and Advantage Actor Critic (A2C) on InvertedPendulum-v2 and Reacher-v2) and a convolutional neural network on SVHN and CIFAR10 datasets. All results were averaged over 20 independent runs, with final performance evaluated at the maximum number of iterations. The setup utilized NVIDIA 1080 GTX GPUs with TensorFlow-GPU, OpenAI Gym, Mujoco, and OpenAI Baselines. The GP models used square-exponential kernels, with parameters estimated by maximizing marginal likelihood. A maximum of 15 augmented points were allowed, with a natural log of the GP condition number threshold set to 20. Baselines included Hyperband and Continuous Multi-Task/Fidelity Bayesian Optimization (CM-T/F-BO), with ablation studies also comparing against vanilla BO and BO with compression (BO-L). Detailed hyperparameter search ranges and agent configurations were provided for reproducibility.",
    "Limitations": "The paper highlights that traditional early-stopping criteria, such as the exponential decay assumed in Freeze-thaw BO, are often unsuitable for DRL due to the unpredictable fluctuations and noisiness of DRL reward curves. It also notes that naive data augmentation by adding a full curve of intermediate training steps can lead to redundant information and severe ill-conditioning of the Gaussian Process covariance matrix, a problem BOIL addresses through selective augmentation. In a broader context, the paper discusses a potential societal limitation: as automated training pipelines, which BOIL contributes to, become more prevalent, humans might become further removed from the modeling process, making it harder to detect critical failures and increasing the opacity of machine learning models.",
    "Future Research Directions": "Future research could involve extending the BOIL framework to optimize any iterative process beyond machine learning algorithms, such as adjusting factory settings to increase productivity in manufacturing pipelines. The authors also implicitly suggest further work in constructing fully automated pipelines for ML model training and deployment. Furthermore, while not explicitly stated as future work for BOIL itself, the broader impact discussion points to integrating techniques for analyzing the interpretability of trained machine learning models with automated training procedures to ensure rigorous analysis of final training outcomes and counter the growing opacity of ML models.",
    "Experiment Code": "from bayes_opt.acquisition_functions import unique_rows\nfrom sklearn.preprocessing import MinMaxScaler\nfrom scipy.optimize import minimize\nfrom sklearn.metrics.pairwise import euclidean_distances\nimport scipy.linalg as spla\nfrom bayes_opt.curve_compression import apply_one_transform_logistic, transform_logistic\n\n\nclass ProductGaussianProcess(object):\n    # in this class of Gaussian process, we define k( {x,t}, {x',t'} )= k(x,x')*k(t,t')\n    \n    \n    #def __init__ (self,param):\n    def __init__ (self,SearchSpace,gp_hyper=None,logistic_hyper=None,verbose=0):\n        self.noise_delta=5e-4\n        self.noise_upperbound=1e-2\n        self.mycov=self.cov_RBF_time\n        self.SearchSpace=SearchSpace\n        scaler = MinMaxScaler()\n        scaler.fit(SearchSpace.T)\n        self.Xscaler=scaler\n        self.verbose=verbose\n        self.dim=SearchSpace.shape[0]\n        \n        if gp_hyper is None:\n            self.hyper={}\n            self.hyper['var']=1 # standardise the data\n            self.hyper['lengthscale_x']=0.02 #to be optimised\n            self.hyper['lengthscale_t']=0.2 #to be optimised\n        else:\n            self.hyper=gp_hyper\n\n        \n        if logistic_hyper is None:\n            self.logistic_hyper={}\n            self.logistic_hyper['midpoint']=0.0\n            self.logistic_hyper['growth']=1.0   \n        else:\n            self.logistic_hyper=logistic_hyper\n\n        self.X=[]\n        self.T=[]\n        self.Y=[]\n        self.Y_curves=None\n#        self.hyper['lengthscale_x']_old=self.hyper['lengthscale_x']\n#        self.hyper['lengthscale_x']_old_t=self.hyper['lengthscale_x']_t\n        \n        self.alpha=[] # for Cholesky update\n        self.L=[] # for Cholesky update LL'=A\n        \n        self.MaxEpisode=0\n        \n        return None\n       \n\n    def cov_RBF_time(self, x1,t1,x2,t2,lengthscale,lengthscale_t):\n        \n        Euc_dist=euclidean_distances(x1,x2)\n        exp_dist_x=np.exp(-np.square(Euc_dist)/lengthscale)\n        \n        Euc_dist=euclidean_distances(t1,t2)\n        exp_dist_t=np.exp(-np.square(Euc_dist)/lengthscale_t)\n        \n        return exp_dist_x*exp_dist_t\n                \n    def fit(self,X,T,Y,Y_curves):\n        \"\"\"\n        Fit Gaussian Process model\n\n        Input Parameters\n        ----------\n        x: the observed points \n        t: time or number of episode\n        y: the outcome y=f(x)\n        \n        \"\"\" \n        temp=np.hstack((X,T))\n        ur = unique_rows(temp)\n        \n        T=T[ur]\n        X=X[ur]\n        Y=Y[ur]\n        \n        self.X=X\n        self.Y=Y\n        self.T=T\n        self.Y_curves=[val for idx,val in enumerate(Y_curves) if ur[idx]==True]\n        \n        for curves in self.Y_curves:\n            self.MaxEpisode=max(len(curves),self.MaxEpisode)\n        #self.Y_curves=Y_curves[myidx]\n            \n        Euc_dist_x=euclidean_distances(X,X)\n        #exp_dist_x=np.exp(-np.square(Euc_dist)/self.hyper['lengthscale_x'])+np.eye(len(X))*self.noise_delta\n    \n        Euc_dist_t=euclidean_distances(T,T)\n        #exp_dist_t=np.exp(-np.square(Euc_dist)/self.hyper['lengthscale_x']_t)+np.eye(len(X))*self.noise_delta       \n    \n        self.KK_x_x=np.exp(-np.square(Euc_dist_x)/self.hyper['lengthscale_x']\\\n                           -np.square(Euc_dist_t)/self.hyper['lengthscale_t'])+np.eye(len(X))*self.noise_delta\n          \n        if np.isnan(self.KK_x_x).any(): #NaN\n            print(\"nan in KK_x_x\")\n        \n        #self.KK_x_x_inv=np.linalg.pinv(self.KK_x_x)\n        self.L=np.linalg.cholesky(self.KK_x_x)\n        temp=np.linalg.solve(self.L,self.Y)\n        self.alpha=np.linalg.solve(self.L.T,temp)\n        self.cond_num=self.compute_condition_number()\n        \n    def compute_condition_number(self):\n        cond_num=np.linalg.cond(self.KK_x_x)\n        return cond_num\n    \n\n    def log_marginal_lengthscale_logistic_hyper(self,hyper,noise_delta):\n        \"\"\"\n        Compute Log Marginal likelihood of the GP model w.r.t. the provided lengthscale, noise_delta and Logistic hyperparameter\n        \"\"\"\n\n        def compute_log_marginal_with_logistic_hyper(lengthscale, lengthscale_t,midpoint,growth,noise_delta):\n            # compute K\n            temp=np.hstack((self.X,self.T))\n            ur = unique_rows(temp)\n            myX=self.X[ur]\n            myT=self.T[ur]\n            \n            # transform Y_curve to Y_original, then to Y\n            Y_original=transform_logistic(self.Y_curves,midpoint,growth,self.MaxEpisode)\n            myY=(Y_original-np.mean(Y_original))/np.std(Y_original)\n            \n            myY=myY[ur]\n          \n            self.Euc_dist_x=euclidean_distances(myX,myX)\n            self.Euc_dist_t=euclidean_distances(myT,myT)\n        \n            KK=np.exp(-np.square(self.Euc_dist_x)/lengthscale-np.square(self.Euc_dist_t)/lengthscale_t)\n                +np.eye(len(myX))*noise_delta\n                    \n            \n            try:\n                temp_inv=np.linalg.solve(KK,myY)\n            except: # singular\n                return -np.inf\n            \n            try:\n                #logmarginal=-0.5*np.dot(self.Y.T,temp_inv)-0.5*np.log(np.linalg.det(KK+noise_delta))-0.5*len(X)*np.log(2*3.14)\n                first_term=-0.5*np.dot(myY.T,temp_inv)\n                \n                # if the matrix is too large, we randomly select a part of the data for fast computation\n                if KK.shape[0]>200:\n                    idx=np.random.permutation(KK.shape[0])\n                    idx=idx[:200]\n                    KK=KK[np.ix_(idx,idx)]\n                #Wi, LW, LWi, W_logdet = pdinv(KK)\n                #sign,W_logdet2=np.linalg.slogdet(KK)\n                chol  = spla.cholesky(KK, lower=True)\n                W_logdet=np.sum(np.log(np.diag(chol)))\n                # Uses the identity that log det A = log prod diag chol A = sum log diag chol A\n    \n                #second_term=-0.5*W_logdet2\n                second_term=-W_logdet\n            except: # singular\n                return -np.inf\n            \n\n            logmarginal=first_term+second_term-0.5*len(myY)*np.log(2*3.14)\n                \n            if np.isnan(np.asscalar(logmarginal))==True:\n                print(\"lengthscale_x={:f} lengthscale_t={:f} first term ={:.4f} second  term ={:.4f}\".format(\n                        lengthscale,lengthscale_t,np.asscalar(first_term),np.asscalar(second_term)))\n\n            #print(lengthscale, lengthscale_t,midpoint,growth,\"logmarginal:\",logmarginal)\n            return np.asscalar(logmarginal)\n        \n        logmarginal=0\n\n        if not isinstance(hyper,list) and len(hyper.shape)==2:\n            logmarginal=[0]*hyper.shape[0]\n            growth=hyper[:,3]\n            midpoint=hyper[:,2]\n            lengthscale_t=hyper[:,1]\n            lengthscale_x=hyper[:,0]\n            for idx in range(hyper.shape[0]):\n                logmarginal[idx]=compute_log_marginal_with_logistic_hyper(lengthscale_x[idx],\\\n                           lengthscale_t[idx],midpoint[idx],growth[idx],noise_delta)\n        else:\n            lengthscale_x,lengthscale_t,midpoint,growth=hyper\n            logmarginal=compute_log_marginal_with_logistic_hyper(lengthscale_x,lengthscale_t,\\\n                                                                 midpoint,growth,noise_delta)\n        return logmarginal\n\n#    def optimize_lengthscale_SE_maximizing(self,previous_theta,noise_delta):\n#        \"\"\"\n#        Optimize to select the optimal lengthscale parameter\n#        \"\"\"\n#                \n#        # define a bound on the lengthscale\n#        SearchSpace_lengthscale_min=0.01\n#        SearchSpace_lengthscale_max=0.5\n#        #mySearchSpace=[np.asarray([SearchSpace_lengthscale_min,SearchSpace_lengthscale_max]).T]\n#        \n#        mySearchSpace=np.asarray([[SearchSpace_lengthscale_min,SearchSpace_lengthscale_max],\\\n#                             [10*SearchSpace_lengthscale_min,2*SearchSpace_lengthscale_max]])\n#        \n#        # Concatenate new random points to possible existing\n#        # points from self.explore method.\n#        lengthscale_tries = np.random.uniform(mySearchSpace[:, 0], mySearchSpace[:, 1],size=(20, mySearchSpace.shape[0]))\n\n#        #print lengthscale_tries\n\n#        # evaluate\n#        self.flagOptimizeHyperFirst=0 # for efficiency\n\n#        logmarginal_tries=self.log_marginal_lengthscale(lengthscale_tries,noise_delta)\n#        #print logmarginal_tries\n\n#        #find x optimal for init\n#        idx_max=np.argmax(logmarginal_tries)\n#        lengthscale_init_max=lengthscale_tries[idx_max]\n#        #print lengthscale_init_max\n#        \n#        myopts ={'maxiter':20*self.dim,'maxfun':20*self.dim}\n\n#        x_max=[]\n#        max_log_marginal=None\n#        \n#        res = minimize(lambda x: -self.log_marginal_lengthscale(x,noise_delta),lengthscale_init_max,\n#                       SearchSpace=mySearchSpace,method=\"L-BFGS-B\",options=myopts)#L-BFGS-B\n#        if 'x' not in res:\n#            val=self.log_marginal_lengthscale(res,noise_delta)    \n#        else:\n#            val=self.log_marginal_lengthscale(res.x,noise_delta)  \n#        \n#        # Store it if better than previous minimum(maximum).\n#        if max_log_marginal is None or val >= max_log_marginal:\n#            if 'x' not in res:\n#                x_max = res\n#            else:\n#                x_max = res.x\n#            max_log_marginal = val\n#            #print res.x\n\n#        return x_max\n    \n    def optimize_lengthscale_SE_logistic_hyper(self,previous_hyper,noise_delta):\n        \"\"\"\n        Optimize to select the optimal lengthscale parameter\n        \"\"\"\n        \n        # define a bound on the lengthscale\n        SearchSpace_l_min=0.03\n        SearchSpace_l_max=0.3\n        \n        SearchSpace_midpoint_min=-2\n        SearchSpace_midpoint_max=3\n        \n        SearchSpace_growth_min=0.5\n        SearchSpace_growth_max=2\n        #mySearchSpace=[np.asarray([SearchSpace_lengthscale_min,SearchSpace_lengthscale_max]).T]\n        \n        mySearchSpace=np.asarray([[SearchSpace_l_min,SearchSpace_l_max],[10*SearchSpace_l_min,2*SearchSpace_l_max],\n                             [SearchSpace_midpoint_min,SearchSpace_midpoint_max],[SearchSpace_growth_min,SearchSpace_growth_max]])\n        \n        lengthscale_tries = np.random.uniform(mySearchSpace[:, 0], mySearchSpace[:, 1],size=(20, 4))\n\n        # evaluate\n        self.flagOptimizeHyperFirst=0 # for efficiency\n\n        logmarginal_tries=self.log_marginal_lengthscale_logistic_hyper(lengthscale_tries,noise_delta)\n\n        #find x optimal for init\n        idx_max=np.argmax(logmarginal_tries)\n        lengthscale_init_max=lengthscale_tries[idx_max]\n        #print lengthscale_init_max\n        \n        myopts ={'maxiter':30*self.dim,'maxfun':30*self.dim}\n\n        x_max=[]\n        max_log_marginal=None\n        \n        res = minimize(lambda x: -self.log_marginal_lengthscale_logistic_hyper(x,noise_delta),lengthscale_init_max,\n                       bounds=mySearchSpace,method=\"L-BFGS-B\",options=myopts)#L-BFGS-B\n        if 'x' not in res:\n            val=self.log_marginal_lengthscale_logistic_hyper(res,noise_delta)    \n        else:\n            val=self.log_marginal_lengthscale_logistic_hyper(res.x,noise_delta)  \n        \n        # Store it if better than previous minimum(maximum).\n        if max_log_marginal is None or val >= max_log_marginal:\n            if 'x' not in res:\n                x_max = res\n            else:\n                x_max = res.x\n            max_log_marginal = val\n            #print res.x\n\n        return x_max\n\n\n#    def optimize_lengthscale(self,previous_theta_x, previous_theta_t,noise_delta):\n#\n#        prev_theta=[previous_theta_x,previous_theta_t]\n#        newlengthscale,newlengthscale_t=self.optimize_lengthscale_SE_maximizing(prev_theta,noise_delta)\n#        self.hyper['lengthscale_x']=newlengthscale\n#        self.hyper['lengthscale_t']=newlengthscale_t\n#        \n#        # refit the model\n#        temp=np.hstack((self.X,self.T))\n#        ur = unique_rows(temp)\n#        \n#        self.fit(self.X[ur],self.T[ur],self.Y[ur],self.Y_curves)\n#        \n#        return newlengthscale,newlengthscale_t\n            \n    def optimize_lengthscale_logistic_hyper(self,prev_hyper,noise_delta):\n        # optimize both GP lengthscale and logistic hyperparameter\n\n            \n        #prev_theta=[prev_theta_x,prev_theta_t,prev_midpoint,prev_growth]\n        newlengthscale,newlengthscale_t,newmidpoint,newgrowth=self.optimize_lengthscale_SE_logistic_hyper(prev_hyper,noise_delta)\n        self.hyper['lengthscale_x']=newlengthscale\n        self.hyper['lengthscale_t']=newlengthscale_t\n        \n        # refit the model\n        temp=np.hstack((self.X,self.T))\n        ur = unique_rows(temp)\n\n        # update Y here\n        Y_original=transform_logistic(self.Y_curves,newmidpoint,newgrowth,self.SearchSpace[-1,1])\n        Y=(Y_original-np.mean(Y_original))/np.std(Y_original)\n        self.Y=Y\n        #\n        self.fit(self.X[ur],self.T[ur],self.Y[ur],self.Y_curves)\n        \n        return newlengthscale,newlengthscale_t,newmidpoint,newgrowth\n\n\n    def compute_var(self,X,T,xTest,tTest):\n        \"\"\"\n        compute variance given X and xTest\n        \n        Input Parameters\n        ----------\n        X: the observed points\n        xTest: the testing points \n        \n        Returns\n        -------\n        diag(var)\n        \"\"\" \n        \n        xTest=np.asarray(xTest)\n        xTest=np.atleast_2d(xTest)\n        \n        tTest=np.asarray(tTest)\n        tTest=np.atleast_2d(tTest)\n        tTest=np.reshape(tTest,(-1,1))\n        \n        if self.kernel_name=='SE':\n            #Euc_dist=euclidean_distances(xTest,xTest)\n            #KK_xTest_xTest=np.exp(-np.square(Euc_dist)/self.hyper['lengthscale_x'])+np.eye(xTest.shape[0])*self.noise_delta\n            #ur = unique_rows(X)\n            myX=X\n            myT=T\n            \n            Euc_dist_x=euclidean_distances(myX,myX)\n            #exp_dist_x=np.exp(-np.square(self.Euc_dist_x)/lengthscale)+np.eye(len(myX))*noise_delta\n        \n            Euc_dist_t=euclidean_distances(myT,myT)\n            #exp_dist_t=np.exp(-np.square(self.Euc_dist_t)/lengthscale_t)+np.eye(len(myX))*noise_delta      \n        \n            KK=np.exp(-np.square(Euc_dist_x)/self.hyper['lengthscale_x']-np.square(Euc_dist_t)/self.hyper['lengthscale_t'])\n                +np.eye(len(myX))*self.noise_delta\n                    \n                 \n            Euc_dist_test_train_x=euclidean_distances(xTest,X)\n            #Exp_dist_test_train_x=np.exp(-np.square(Euc_dist_test_train_x)/self.hyper['lengthscale_x'])\n            \n            Euc_dist_test_train_t=euclidean_distances(tTest,T)\n            #Exp_dist_test_train_t=np.exp(-np.square(Euc_dist_test_train_t)/self.hyper['lengthscale_t'])\n            \n            KK_xTest_xTrain=np.exp(-np.square(Euc_dist_test_train_x)/self.hyper['lengthscale_x']-np.square(Euc_dist_test_train_t)/self.hyper['lengthscale_t'])\n                \n        try:\n            temp=np.linalg.solve(KK,KK_xTest_xTrain.T)\n        except:\n            temp=np.linalg.lstsq(KK,KK_xTest_xTrain.T, rcond=-1)\n            temp=temp[0]\n            \n        #var=KK_xTest_xTest-np.dot(temp.T,KK_xTest_xTrain.T)\n        var=np.eye(xTest.shape[0])-np.dot(temp.T,KK_xTest_xTrain.T)\n        var=np.diag(var)\n        var.flags['WRITEABLE']=True\n        var[var<1e-100]=0\n        return var \n\n    \n        \n    def predict(self,xTest, eval_MSE=True):\n        \"\"\"\n        compute predictive mean and variance\n        Input Parameters\n        ----------\n        xTest: the testing points \n        \n        Returns\n        -------\n        mean, var\n        \"\"\"    \n\n        if len(xTest.shape)==1: # 1d\n            xTest=xTest.reshape((-1,self.X.shape[1]+1))\n            \n        tTest=xTest[:,-1]\n        tTest=np.atleast_2d(tTest)\n        tTest=np.reshape(tTest,(xTest.shape[0],-1))\n        \n        xTest=xTest[:,:-1]\n        \n        # prevent singular matrix\n        temp=np.hstack((self.X,self.T))\n        ur = unique_rows(temp)\n        \n        X=self.X[ur]\n        T=self.T[ur]\n                \n        Euc_dist_x=euclidean_distances(xTest,xTest)\n        Euc_dist_t=euclidean_distances(tTest,tTest)\n\n        KK_xTest_xTest=np.exp(-np.square(Euc_dist_x)/self.hyper['lengthscale_x']-np.square(Euc_dist_t)/self.hyper['lengthscale_t'])\n            +np.eye(xTest.shape[0])*self.noise_delta\n        \n        Euc_dist_test_train_x=euclidean_distances(xTest,X)\n        \n        Euc_dist_test_train_t=euclidean_distances(tTest,T)\n        \n        KK_xTest_xTrain=np.exp(-np.square(Euc_dist_test_train_x)/self.hyper['lengthscale_x']-np.square(Euc_dist_test_train_t)/self.hyper['lengthscale_t'])\n            \n        #Exp_dist_test_train_x*Exp_dist_test_train_t\n  \n        # using Cholesky update\n        mean=np.dot(KK_xTest_xTrain,self.alpha)\n        v=np.linalg.solve(self.L,KK_xTest_xTrain.T)\n        var=KK_xTest_xTest-np.dot(v.T,v)\n        \n\n        return mean.ravel(),np.diag(var)  \n\n    def posterior(self,x):\n        # compute mean function and covariance function\n        return self.predict(self,x)\n        \n    \ndef apply_one_transform_logistic(curve, midpoint=-2, growth=1,MaxEpisode=1000,IsReturnCurve=False):\n    # this is the Logistic transformation, used in the paper\n    if isinstance(curve, (list,)):\n        curve=curve[0]\n        \n    def logistic_func(x):\n        return 1.0/(1+np.exp(-growth*(x-midpoint)))\n\t\n    #print(MaxEpisode)\n    my_xrange_scaled=np.linspace(-6,6, int(MaxEpisode))\n\n    my_logistic_value_scaled=logistic_func(my_xrange_scaled)\n\n    my_logistic_value_scaled=my_logistic_value_scaled[:len(curve)]\n\n    # if curve is negative, add a constant to make it positive\n    if np.max(curve)<=0 and np.min(curve)<=0:\n        curve=curve+500\n    \n    threshold=(midpoint+6-2)*len(curve)/(12)\n    threshold=np.int(threshold)\n    \n    prod_func=curve*my_logistic_value_scaled\n    \n    average=[np.mean(prod_func[threshold:pos+1]) for pos in range(threshold,len(prod_func))]\n\n    if IsReturnCurve==True:\n        return average[-1],my_logistic_value_scaled\n    else:\n        return average[-1]\n\n\ndef transform_logistic(curves, midpoint=0, growth=1,MaxEpisode=1000):\n    # curve is a matrix [nParameter x MaxIter]\n    # or curve is a vector [1 x MaxIter]\n\n    if len(curves)==1:\n        output=apply_one_transform_logistic(curves[0], midpoint, growth,MaxEpisode)\n    else:\n        output=[0]*len(curves)\n        for idx, curve in enumerate(curves):\n            output[idx]=apply_one_transform_logistic(curve, midpoint, growth,MaxEpisode)\n    return output\n    \nimport numpy as np\nfrom scipy.stats import norm\n\n\ncounter = 0\n\n\nclass AcquisitionFunction(object):\n    \"\"\"\n    An object to compute the acquisition functions.\n    \"\"\"\n\n    def __init__(self, acq):\n\n        self.acq=acq\n        acq_name=acq['name']\n        \n        if 'mu_max' in acq:\n            self.mu_max=acq['mu_max'] # this is for ei_mu acquisition function\n        \n        ListAcq=['bucb','ucb', 'ei','poi','random','ucb_pe',\n                 'pure_exploration','mu','lcb','ei_mu_max'                          ]\n        \n        # check valid acquisition function\n        IsTrue=[val for idx,val in enumerate(ListAcq) if val in acq_name]\n        #if  not in acq_name:\n        if  IsTrue == []:\n            err = \"The utility function \" \\\n                  \"{} has not been implemented, \" \\\n                  \"please choose one of ucb, ei, or poi.\".format(acq_name)\n            raise NotImplementedError(err)\n        else:\n            self.acq_name = acq_name\n            \n        self.dim=acq['dim']\n        \n        if 'scalebounds' not in acq:\n            self.scalebounds=[0,1]*self.dim\n            \n        else:\n            self.scalebounds=acq['scalebounds']\n               \n\n    def acq_kind(self, x, gp):\n        \n        #if type(meta) is dict and 'y_max' in meta.keys():\n        #   y_max=meta['y_max']\n        y_max=np.max(gp.Y)\n        #print self.kind\n        if np.any(np.isnan(x)):\n            return 0\n       \n        if self.acq_name == 'ucb':\n            return self._ucb(x, gp)\n        if self.acq_name == 'lcb':\n            return self._lcb(x, gp)\n        if self.acq_name == 'ei':\n            return self._ei(x, gp, y_max)\n        if self.acq_name == 'ei_mu_max': # using max mu(x) as incumbent\n            return self._ei(x, gp, self.mu_max)\n        if self.acq_name == 'poi':\n            return self._poi(x, gp, y_max)\n        \n        if self.acq_name == 'pure_exploration':\n            return self._pure_exploration(x, gp) \n      \n        if self.acq_name == 'mu':\n            return self._mu(x, gp)\n        \n        if self.acq_name == 'ucb_pe':\n            return self._ucb_pe(x, gp,self.acq['kappa'],self.acq['maxlcb'])\n       \n            \n    def utility_plot(self, x, gp, y_max):\n        if np.any(np.isnan(x)):\n            return 0\n        if self.acq_name == 'ei':\n            return self._ei_plot(x, gp, y_max)\n  \n   \n    @staticmethod\n    def _mu(x, gp):\n        mean, var = gp.predict(x, eval_MSE=True)\n        mean=np.atleast_2d(mean).T\n        return mean\n                \n    @staticmethod\n    def _lcb(x, gp):\n        mean, var = gp.predict(x, eval_MSE=True)\n        var.flags['WRITEABLE']=True\n        #var=var.copy()\n        var[var<1e-10]=0\n        mean=np.atleast_2d(mean).T\n        var=np.atleast_2d(var).T\n        #beta_t = gp.X.shape[1] * np.log(len(gp.Y))\n        beta_t = 2 * np.log(len(gp.Y));\n\n        return mean - np.sqrt(beta_t) * np.sqrt(var) \n        \n    \n    @staticmethod\n    def _ucb(x, gp):\n        mean, var = gp.predict(x, eval_MSE=True)\n        var.flags['WRITEABLE']=True\n        #var=var.copy()\n        var[var<1e-10]=0\n        mean=np.atleast_2d(mean).T\n        var=np.atleast_2d(var).T                \n        \n        # Linear in D, log in t https://github.com/kirthevasank/add-gp-bandits/blob/master/BOLibkky/getUCBUtility.m\n        #beta_t = gp.X.shape[1] * np.log(len(gp.Y))\n        beta_t = 2 * np.log(len(gp.Y));\n  \n        #beta=300*0.1*np.log(5*len(gp.Y))# delta=0.2, gamma_t=0.1\n        return mean + np.sqrt(beta_t) * np.sqrt(var) \n    \n    \n    @staticmethod\n    def _ucb_pe(x, gp, kappa, maxlcb):\n        mean, var = gp.predict_bucb(x, eval_MSE=True)\n        var.flags['WRITEABLE']=True\n        var[var<1e-10]=0\n        mean=np.atleast_2d(mean).T\n        var=np.atleast_2d(var).T\n\n        value=mean + kappa * np.sqrt(var)        \n        myidx=[idx for idx,val in enumerate(value) if val<maxlcb]\n        var[myidx]=0        \n        return var\n    \n   \n    @staticmethod\n    def _pure_exploration(x, gp):\n        mean, var = gp.predict(x, eval_MSE=True)\n        var.flags['WRITEABLE']=True\n        #var=var.copy()\n        var[var<1e-10]=0\n        mean=np.atleast_2d(mean).T\n        var=np.atleast_2d(var).T\n        return np.sqrt(var)\n        \n   \n    @staticmethod\n    def _ei(x, gp, y_max):\n        y_max=np.asscalar(y_max)\n        mean, var = gp.predict(x, eval_MSE=True)\n        var2 = np.maximum(var, 1e-10 + 0 * var)\n        z = (mean - y_max)/np.sqrt(var2)        \n        out=(mean - y_max) * norm.cdf(z) + np.sqrt(var2) * norm.pdf(z)\n        \n        out[var2<1e-10]=0\n        return out\n \n \n    @staticmethod      \n    def _poi(x, gp,y_max): # run Predictive Entropy Search using Spearmint\n        mean, var = gp.predict(x, eval_MSE=True)    \n        # Avoid points with zero variance\n        var = np.maximum(var, 1e-9 + 0 * var)\n        z = (mean - y_max)/np.sqrt(var)        \n        return norm.cdf(z)\n\n   \ndef unique_rows(a):\n    \"\"\"\n    A functions to trim repeated rows that may appear when optimizing.\n    This is necessary to avoid the sklearn GP object from breaking\n\n    :param a: array to trim repeated rows from\n\n    :return: mask of unique rows\n    \"\"\"\n\n    # Sort array and kep track of where things should go back to\n    order = np.lexsort(a.T)\n    reorder = np.argsort(order)\n\n    a = a[order]\n    diff = np.diff(a, axis=0)\n    ui = np.ones(len(a), 'bool')\n    ui[1:] = (diff != 0).any(axis=1)\n\n    return ui[reorder]\n\n\n\nclass BColours(object):\n    BLUE = '\\033[94m'\n    CYAN = '\\033[36m'\n    GREEN = '\\033[32m'\n    MAGENTA = '\\033[35m'\n    RED = '\\033[31m'\n    ENDC = '\\033[0m'\n\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom bayes_opt.acquisition_functions import AcquisitionFunction\nimport sobol_seq\n\n\ndef acq_max_with_name(gp,scaleSearchSpace,acq_name=\"ei\",IsReturnY=False,IsMax=True,fstar_scaled=None):\n    acq={}\n    acq['name']=acq_name\n    acq['dim']=scaleSearchSpace.shape[0]\n    acq['scaleSearchSpace']=scaleSearchSpace   \n    if fstar_scaled:\n        acq['fstar_scaled']=fstar_scaled   \n\n    myacq=AcquisitionFunction(acq)\n    if IsMax:\n        x_max = acq_max(ac=myacq.acq_kind,gp=gp,bounds=scaleSearchSpace,opt_toolbox='scipy')\n    else:\n        x_max = acq_min_scipy(ac=myacq.acq_kind,gp=gp,bounds=scaleSearchSpace)\n    if IsReturnY==True:\n        y_max=myacq.acq_kind(x_max,gp=gp)\n        return x_max,y_max\n    return x_max\n\ndef generate_sobol_seq(dim,nSobol):\n    mysobol_seq = sobol_seq.i4_sobol_generate(dim, nSobol)\n    return mysobol_seq\n    \n\ndef acq_min_scipy_kwargs(myfunc, SearchSpace, **kwargs):\n    \"\"\"\n    A function to find the maximum of the acquisition function using\n    the scipy python\n\n    Input Parameters\n    ----------\n    ac: The acquisition function object that return its point-wise value.\n    gp: A gaussian process fitted to the relevant data.\n    y_max: The current maximum known value of the target function.\n    bounds: The variables bounds to limit the search of the acq max.\n    \n    Returns\n    -------\n    x_max, The arg max of the acquisition function.\n    \"\"\"\n    dim=SearchSpace.shape[0]\n    # Start with the lower bound as the argmax\n    x_max = SearchSpace[:, 0]\n    min_acq = None\n\n    #myopts ={'maxiter':2000,'fatol':0.01,'xatol':0.01}\n    myopts ={'maxiter':10*dim,'maxfun':20*dim}\n    #myopts ={'maxiter':5*dim}\n\n    #sobol_sequence=generate_sobol_seq(dim=dim,nSobol=500*dim)\n\n    # multi start\n    for i in range(3*dim):\n        # Find the minimum of minus the acquisition function        \n        x_tries = np.random.uniform(SearchSpace[:, 0], SearchSpace[:, 1],size=(100*dim, dim))\n        \n        #x_tries=sobol_sequence\n    \n        # evaluate\n        y_tries=myfunc(x_tries,**kwargs)\n        \n        #find x optimal for init\n        idx_min=np.argmin(y_tries)\n\n        x_init_min=x_tries[idx_min]\n    \n        res = minimize(lambda x: myfunc(x.reshape(1, -1), **kwargs),x_init_min.reshape(1, -1),bounds=SearchSpace,\n                       method=\"L-BFGS-B\",options=myopts)#L-BFGS-B\n\n        if 'x' not in res:\n            val=myfunc(res,**kwargs)        \n        else:\n            val=myfunc(res.x,**kwargs) \n        \n        # Store it if better than previous minimum(maximum).\n        if min_acq is None or val <= min_acq:\n            if 'x' not in res:\n                x_max = res\n            else:\n                x_max = res.x\n            min_acq = val\n            #print max_acq\n\n    return np.clip(x_max, SearchSpace[:, 0], SearchSpace[:, 1])\n\n    \ndef acq_max(ac, gp, bounds, opt_toolbox='scipy',seeds=[],IsMax=True):\n    \"\"\"\n    A function to find the maximum of the acquisition function using\n    the scipy python\n\n    Input Parameters\n    ----------\n    ac: The acquisition function object that return its point-wise value.\n    gp: A gaussian process fitted to the relevant data.\n    y_max: The current maximum known value of the target function.\n    bounds: The variables bounds to limit the search of the acq max.\n    \n    Returns\n    -------\n    x_max, The arg max of the acquisition function.\n    \"\"\"\n    y_max=np.max(gp.Y)\n  \n    x_max = acq_max_scipy(ac=ac,gp=gp,y_max=y_max,bounds=bounds)\n\n    return x_max\n\ndef acq_max_scipy(ac, gp, y_max, bounds):\n    \"\"\"\n    A function to find the maximum of the acquisition function using\n    the scipy python\n\n    Input Parameters\n    ----------\n    ac: The acquisition function object that return its point-wise value.\n    gp: A gaussian process fitted to the relevant data.\n    y_max: The current maximum known value of the target function.\n    bounds: The variables bounds to limit the search of the acq max.\n    \n    Returns\n    -------\n    x_max, The arg max of the acquisition function.\n    \"\"\"\n\n    dim=bounds.shape[0]\n    # Start with the lower bound as the argmax\n    x_max = bounds[:, 0]\n    max_acq = None\n\n    myopts ={'maxiter':10*dim,'maxfun':20*dim}\n    #myopts ={'maxiter':5*dim}\n\n\n    # multi start\n    for i in range(1*dim):\n        # Find the minimum of minus the acquisition function        \n        x_tries = np.random.uniform(bounds[:, 0], bounds[:, 1],size=(50*dim, dim))\n    \n        # evaluate\n        y_tries=ac(x_tries,gp=gp)\n        #print \"elapse evaluate={:.5f}\".format(end_eval-start_eval)\n        \n        #find x optimal for init\n        idx_max=np.argmax(y_tries)\n        #print \"max y_tries {:.5f} y_max={:.3f}\".format(np.max(y_tries),y_max)\n\n        x_init_max=x_tries[idx_max]\n        \n    \n        res = minimize(lambda x: -ac(x.reshape(1, -1), gp=gp),x_init_max.reshape(1, -1),bounds=bounds,\n                       method=\"L-BFGS-B\",options=myopts)#L-BFGS-B\n\n\n        \n        if 'x' not in res:\n            val=ac(res,gp)        \n        else:\n            val=ac(res.x,gp) \n\n        # Store it if better than previous minimum(maximum).\n        if max_acq is None or val >= max_acq:\n            if 'x' not in res:\n                x_max = res\n            else:\n                x_max = res.x\n            max_acq = val\n            #print max_acq\n\n    # Clip output to make sure it lies within the bounds. Due to floating\n    # point technicalities this is not always the case.\n    #return np.clip(x_max[0], bounds[:, 0], bounds[:, 1])\n        #print max_acq\n    return np.clip(x_max, bounds[:, 0], bounds[:, 1])\n    \nimport numpy as np\nfrom bayes_opt.acquisition_functions import AcquisitionFunction, unique_rows\nfrom bayes_opt import GaussianProcess\nfrom bayes_opt import ProductGaussianProcess\n\nfrom bayes_opt.acquisition_maximization import acq_max_with_name,acq_min_scipy_kwargs\nimport time\nfrom sklearn import linear_model\nimport copy\nfrom bayes_opt.curve_compression import transform_logistic\nfrom sklearn.preprocessing import MinMaxScaler\n\n\n#======================================================================================================\n#======================================================================================================\n#======================================================================================================\n#======================================================================================================\ncounter = 0\n\n\nclass BOIL(object):\n\n    #def __init__(self, gp_params, func_params, acq_params, verbose=True):\n    def __init__(self, func, SearchSpace,acq_name=\"ei_mu_max\",verbose=1):\n\n        \"\"\"      \n        Input parameters\n        ----------\n        \ngp_params:                  GP parameters\ngp_params.theta:            to compute the kernel\ngp_params.delta:            to compute the kernel\n\nfunc_params:                function to optimize\nfunc_params.init bound:     initial SearchSpace for parameters\nfunc_params.SearchSpace:        SearchSpace on parameters        \nfunc_params.func:           a function to be optimized\n\n\nacq_params:            acquisition function, \nacq_params.acq_func['name']=['ei','ucb','poi']\nacq_params.opt_toolbox:     optimization toolbox 'nlopt','direct','scipy'\n                            \n        Returns\n        -------\n        dim:            dimension\n        SearchSpace:         SearchSpace on original scale\n        scaleSearchSpace:    SearchSpace on normalized scale of 0-1\n        time_opt:       will record the time spent on optimization\n        gp:             Gaussian Process object\n        \"\"\"\n        \n        self.method='boil'\n        self.verbose=verbose\n        if isinstance(SearchSpace,dict):\n            # Get the name of the parameters\n            self.keys = list(SearchSpace.keys())\n            \n            self.SearchSpace = []\n            for key in list(SearchSpace.keys()):\n                self.SearchSpace.append(SearchSpace[key])\n            self.SearchSpace = np.asarray(self.SearchSpace)\n        else:\n            self.SearchSpace=np.asarray(SearchSpace)\n            \n            \n        self.dim = len(SearchSpace)\n\n        scaler = MinMaxScaler()\n        scaler.fit(self.SearchSpace[:-1,:].T)\n        \n        scalerT = MinMaxScaler()\n        SearchSpace_T=np.atleast_2d(self.SearchSpace[-1,:]).T\n        scalerT.fit(SearchSpace_T)\n\n        self.Xscaler=scaler\n        self.Tscaler=scalerT\n\n        # create a scaleSearchSpace 0-1\n        self.scaleSearchSpace=np.array([np.zeros(self.dim), np.ones(self.dim)]).T\n                \n        # function to be optimised\n        self.f = func\n    \n        # store X in original scale\n        self.X_ori= None\n\n        # store X in 0-1 scale\n        self.X = None\n        \n        # store y=f(x)\n        # (y - mean)/(max-min)\n        self.Y = None\n               \n        # y original scale\n        self.Y_ori = None\n        \n        # store the number of episode\n        self.T=None\n        self.T_original=None\n        \n        # store the cost original scale\n        self.Y_cost_original=None\n        \n        self.time_opt=0\n         \n        self.max_min_gap=self.SearchSpace[:,1]-self.SearchSpace[:,0]\n\n\n        # acquisition function\n        self.acq_name = acq_name\n        self.logmarginal=0\n\n        self.gp=ProductGaussianProcess(self.scaleSearchSpace,verbose=verbose)\n\n        # store the curves of performances\n        self.Y_curves=[]\n        \n        # store the cost original scale\n        self.Y_cost_original=None\n        \n        self.time_opt=0\n        \n        # acquisition function\n        self.acq_func = None\n   \n        self.logmarginal=0\n        \n        self.markVirtualObs=[]\n        \n        self.countVirtual=[]\n\n        self.linear_regression = linear_model.LinearRegression()\n\n        self.condition_number=[]\n        \n        # maximum number of augmentations\n        self.max_n_augmentation=10\n        self.threshold_cond=15\n        \n    def init(self, n_init_points=3, seed=1):\n        \"\"\"      \n        Input parameters\n        ----------\n        n_init_points:        # init points\n        \"\"\"\n        np.random.seed(seed)\n\n        # Generate random points\n        SearchSpace=np.copy(self.SearchSpace)\n        SearchSpace[-1,0]=SearchSpace[-1,1] # last dimension, set it to MaxIter\n\n        l = [np.random.uniform(x[0], x[1]) for _ in range(n_init_points) for x in SearchSpace] \n\n        # Concatenate new random points to possible existing\n        # points from self.explore method.\n        temp=np.asarray(l)\n        temp=temp.T\n        init_X=list(temp.reshape((n_init_points,-1)))\n        \n        self.X_original = np.asarray(init_X)\n        self.T_original=self.X_original[:,-1]\n        self.T_original=np.reshape(self.T_original,(n_init_points,-1))\n        \n        self.X_original=self.X_original[:,:-1] # remove the last dimension of MaxEpisode\n        self.X_original=np.reshape(self.X_original,(n_init_points,-1))\n\n        # Evaluate target function at all initialization           \n        y_init_curves, y_init_cost=self.f(init_X)\n\n        y_init_cost=np.atleast_2d(np.asarray(y_init_cost))#.astype('Float64')\n\n        self.Y_curves+=y_init_curves\n\n        # we transform the y_init_curves as the average of [ curves * logistic ]\n        y_init=transform_logistic(y_init_curves,self.gp.logistic_hyper['midpoint'],\\\n                                  self.gp.logistic_hyper['growth'], self.SearchSpace[-1,1])\n        #y_init=y_init_curves\n        y_init=np.reshape(y_init,(n_init_points,1))\n        \n        # record keeping ========================================================\n        self.Y_original = np.asarray(y_init)      \n        self.Y_cost_original=np.reshape(y_init_cost,(-1,1))\n\n        # convert it to scaleX\n        self.X = self.Xscaler.transform(np.asarray(init_X)[:,:-1])#remove the last dimension of MaxEpisode\n        #self.X=self.X[:,:-1]\n        self.X=np.reshape(self.X,(n_init_points,-1))\n\n        self.T = self.Tscaler.transform(self.T_original)\n\n        self.markVirtualObs+=[0]*n_init_points\n\n        # generating virtual observations for each initial point\n        for ii in range(n_init_points):\n            self.generating_virtual_observations(self.X[ii,:],\n                         self.T[ii],[y_init_curves[ii]],y_init_cost[0][ii],IsRandom=False)\n\n        self.Y_cost=(self.Y_cost_original-np.min(self.Y_cost_original))/(np.max(self.Y_cost_original)-np.min(self.Y_cost_original))\n\n        if np.std(self.Y_original)==0:\n            self.Y=(self.Y_original-np.mean(self.Y_original))\n        else:\n            self.Y=(self.Y_original-np.mean(self.Y_original))/np.std(self.Y_original)\n\n       \n    def utility_cost_evaluation(self,x,acq_func,isDebug=False):\n        # this is a wrapper function to evaluate at multiple x(s)\n        \n        \n        def utility_cost_evaluation_single(x,acq_func,isDebug=False):\n            # given a location x, we will evaluate the utility and cost\n            \n            utility=acq_func.acq_kind(x,gp=self.gp)\n            \n            try:\n                mean_cost=self.linear_regression.predict(np.reshape(x,(1,-1)))\n                \n            except:\n                print(x)\n                print(\"bug\")\n    \n            mean_cost=max(0,mean_cost)+0.1 # to avoid <=0 cost\n            \n            #acquisition_function_value= utility_normalized/cost_normalized\n            if 'ei' in acq_func.acq_name:\n                acquisition_function_value= np.log(utility)-np.log(mean_cost)\n            else:\n                acquisition_function_value= np.log(1+np.exp(utility))/np.log(1+np.exp(mean_cost))\n\n            if isDebug==True:\n                print(\"acq_func at the selected point \\t utility:\",np.round(utility,decimals=4),\"\\t cost:\",mean_cost)\n                if utility==0:\n                    print(\"utility =0===============================================================================\")\n       \n            return acquisition_function_value*(-1) # since we will minimize this acquisition function\n        \n        \n        if len(x)==self.dim: # one observation\n            temp=utility_cost_evaluation_single(x,acq_func,isDebug=isDebug)\n            if isDebug==True:\n                return temp\n            else:\n                utility=np.mean(temp)\n        \n        else: # multiple observations\n            utility=[0]*len(x)\n            for idx,val in enumerate(x):\n                temp=utility_cost_evaluation_single(x=val,acq_func=acq_func,isDebug=isDebug)\n                                                     \n                utility[idx]=np.mean(temp)\n                \n            utility=np.asarray(utility)    \t\t\t               \n        return utility   \n    \n        \n    def acq_utility_cost(self):\n        \n        # generate a set of x* at T=MaxIter\n        # instead of running optimization on the whole space, we will only operate on the region of interest\n        # the region of interest in DRL is where the MaxEpisode\n    \n        # we find maximum of EI\n\n        acq={}\n        acq['name']=self.acq_name\n        acq['dim']=self.scaleSearchSpace.shape[0]\n        acq['scaleSearchSpace']=self.scaleSearchSpace   \n    \n        if self.acq_name=='ei_mu_max':# using max of mean(x) as the incumbent\n            \n            # optimie the GP predictive mean function to find the max of mu\n            x_mu_max,mu_max_val=acq_max_with_name(gp=self.gp,scaleSearchSpace=self.scaleSearchSpace,acq_name='mu',IsReturnY=True)\n            acq['mu_max']=  mu_max_val\n\n        myacq=AcquisitionFunction(acq)\n        \n        x_min = acq_min_scipy_kwargs(myfunc=self.utility_cost_evaluation,SearchSpace=self.scaleSearchSpace,\n                        acq_func=myacq, isDebug=False)\n        \n        if self.verbose==True:\n            acq_val=self.utility_cost_evaluation(x_min,myacq,isDebug=False)\n            print(\"selected point from acq func:\",np.round(x_min,decimals=4),\"acq val=log(Utility/Cost)=\",(-1)*np.round(acq_val,decimals=4)) # since we minimize the acq func\n            if np.round(acq_val,decimals=4)==0:\n                print(\"acq value =0\")\n            \n        return x_min\n    \n    \n    def select_informative_location_by_uncertainty(self,n_virtual_obs,x_max,t_max):\n        # this function will select a list of informative locations to place a virtual obs\n        # x_max is the selected hyperparameter\n        # t_max is the selected number of epochs to train\n        \n        \n        SearchSpace=np.copy(self.scaleSearchSpace)\n        for dd in range(self.dim-1):\n            SearchSpace[dd,0],SearchSpace[dd,1]=x_max[dd],x_max[dd]\n            \n        SearchSpace[-1,1]=t_max\n        \n        temp_X,temp_T=self.X.copy(),self.T.copy()\n        temp_gp=copy.deepcopy(self.gp )\n        \n        temp_Y=np.random.random(size=(len(temp_T),1))\n        \n        temp_gp.fit(temp_X,temp_T,temp_Y,self.Y_curves)\n        \n        new_batch_T=None\n\n        pred_var_value=[0]*n_virtual_obs\n        for ii in range(n_virtual_obs):\n            x_max_pred_variance, pred_var_value[ii]=acq_max_with_name(gp=temp_gp,\n                              scaleSearchSpace=SearchSpace,acq_name='pure_exploration',IsReturnY=True)\n            \n            # stop augmenting if the uncertainty is smaller than a threshold\n            # or stop augmenting if the uncertainty is smaller than a threshold\n\n            log_cond=np.log( temp_gp.compute_condition_number() )\n            if log_cond>self.threshold_cond or pred_var_value[ii]<(self.gp.noise_delta+1e-3):\n                break\n          \n            if x_max_pred_variance[-1] in temp_T[-ii:]: # if repetition, stop augmenting\n                break\n            \n            temp_X = np.vstack((temp_X, x_max.reshape((1, -1)))) # append new x\n            temp_T = np.vstack((temp_T, x_max_pred_variance[-1].reshape((1, -1)))) # append new t\n            temp_gp.X,temp_gp.T=temp_X,temp_T\n            temp_Y=np.random.random(size=(len(temp_T),1))\n            \n            temp_gp.fit(temp_X,temp_T,temp_Y,self.Y_curves)\n\n            if new_batch_T is None:\n                new_batch_T=x_max_pred_variance[-1].reshape((1, -1))\n            else:\n                new_batch_T= np.vstack((new_batch_T, x_max_pred_variance[-1].reshape((1, -1))))\n        \n#        if self.verbose:\n#            print(\"pred_var_value at the augmented points:\",np.round( pred_var_value,decimals=4))\n\n        if new_batch_T is None:\n            return [],0\n\n        else:\n            output=np.sort(new_batch_T.ravel()).tolist()\n            return output, len(output)\n\n    \n    def generating_virtual_observations(self,x_max,t_max,y_original_curves,y_cost_original,IsRandom=False):\n        \n        #temp_X_new_original=x_max*self.max_min_gap[:-1]+self.SearchSpace[:-1,0]\n        temp_X_new_original=self.Xscaler.inverse_transform(np.reshape(x_max,(-1,self.dim-1)))\n\n        # selecting MAX number of virtual observations, e.g., we dont want to augment more than 10 points\n        max_n_virtual_obs=np.int(t_max*self.max_n_augmentation)\n        if max_n_virtual_obs==0:\n            self.countVirtual.append(0)\n            return\n        \n        if IsRandom==True:# select informative locations by random uniform   \n            l = [np.random.uniform(0, t_max) for _ in range(max_n_virtual_obs)]\n        else:\n            # select informative locations by uncertainty as in the paper\n            l,n_virtual_obs=self.select_informative_location_by_uncertainty(max_n_virtual_obs,x_max,t_max)        \n            \n        self.countVirtual.append(n_virtual_obs)\n        \n        if self.verbose:\n            np.set_printoptions(suppress=True)\n            print(\"Max #augmented points\",max_n_virtual_obs, \"\\t #augmented points \",len(l),\n                  \"\\t Augmented points: \",np.round(l,decimals=3))\n            \n        l_original=[self.SearchSpace[-1,0]+val*self.max_min_gap[-1] for val in l]\n        #l_original=[self.Tscaler.inverse_transform(val) for val in l]\n                           \n        virtual_obs_t_original=np.asarray(l_original).T\n        virtual_obs_t=np.asarray(l).T\n        \n        # compute y_original for the virtual observations\n        y_virtual_original=[0]*n_virtual_obs\n        for ii in range(n_virtual_obs):\n            \n            idx=np.int(virtual_obs_t_original[ii])\n            \n            temp_curve=y_original_curves[0][:idx+1]\n            self.markVirtualObs.append(1)\n\n            y_virtual_original[ii]=transform_logistic([temp_curve],\\\n                      self.gp.logistic_hyper['midpoint'],self.gp.logistic_hyper['growth'],self.SearchSpace[-1,1])\n           \n            self.X = np.vstack((self.X, x_max.reshape((1, -1))))\n            self.X_original=np.vstack((self.X_original, temp_X_new_original))\n        \n            self.T = np.vstack((self.T, virtual_obs_t[ii].reshape((1, -1))))\n            temp=np.asarray(virtual_obs_t_original[ii])\n            self.T_original=np.vstack((self.T_original, temp.reshape((1, -1))))\n\n\n            self.Y_original = np.append(self.Y_original,[y_virtual_original[ii]])\n            self.Y_curves.append(temp_curve)\n            \n            # interpolating the cost for augmented observation\n            y_cost_estimate=y_cost_original*virtual_obs_t[ii]\n            self.Y_cost_original = np.append(self.Y_cost_original,[y_cost_estimate])\n            \n        \n#        if self.verbose:\n#            temp_y_original_whole_curve=transform_logistic(y_original_curves,\\\n#                               self.gp.logistic_hyper['midpoint'],self.gp.logistic_hyper['growth'],self.SearchSpace[-1,1])\n#            print(np.round(temp_y_original_whole_curve,decimals=4), np.round(y_virtual_original,decimals=4))\n#            \n        \n    def suggest_nextpoint(self): # logistic, time-cost, virtual\n        \"\"\"\n        Main optimization method.\n\n\n        Returns\n        -------\n        x: recommented point for evaluation\n        \"\"\"\n \n        # init a new Gaussian Process============================================\n        self.gp=ProductGaussianProcess(self.scaleSearchSpace,self.gp.hyper,self.gp.logistic_hyper)\n        self.gp.fit(self.X, self.T,self.Y,self.Y_curves)\n            \n        # we store the condition number here=====================================\n        self.condition_number.append(self.gp.cond_num)\n        if self.verbose:\n            print(\"ln of conditioning number of GP covariance matrix\", np.round(np.log(self.gp.cond_num),decimals=1))\n\n        # count number of real observations\n        count=len(self.markVirtualObs)-np.sum(self.markVirtualObs)\n        count=np.int(count)\n\n        # optimize GP hyperparameters and Logistic hyper after 3*d iterations\n        if  len(self.Y)%(2*self.dim)==0:\n\n            hyper=[self.gp.hyper['lengthscale_x'],self.gp.hyper['lengthscale_t'], \\\n                   self.gp.logistic_hyper['midpoint'],self.gp.logistic_hyper['growth']]\n            newlengthscale_x,newlengthscale_t,new_midpoint, new_growth = self.gp.optimize_lengthscale_logistic_hyper(hyper,self.gp.noise_delta)\n            \n            self.gp.hyper['lengthscale_x']=newlengthscale_x\n            self.gp.hyper['lengthscale_t']=self.gp.hyper['lengthscale_t']\n            self.gp.logistic_hyper['midpoint']=new_midpoint\n            self.gp.logistic_hyper['growth']=new_growth\n          \n            if self.verbose:\n                print(\"==estimated lengthscale_x={:.4f}   lengthscale_t={:.3f}   Logistic_m0={:.1f}   Logistic_g0={:.1f}\".format(\n                    newlengthscale_x,newlengthscale_t,new_midpoint,new_growth))\n                \n        # Set acquisition function\n        start_opt=time.time()\n\n        # linear regression is used to fit the cost\n        # fit X and T\n        combine_input=np.hstack((self.X,self.T))\n        self.linear_regression.fit(combine_input,self.Y_cost)\n        \n        # maximize the acquisition function to select the next point =================================\n        x_max_temp=self.acq_utility_cost()\n        x_max=x_max_temp[:-1]\n        t_max=x_max_temp[-1]       \n            \n        # record keeping stuffs ====================================================\n        # record the optimization time\n        finished_opt=time.time()\n        elapse_opt=finished_opt-start_opt\n        self.time_opt=np.hstack((self.time_opt,elapse_opt))\n\n        # this is for house keeping stuff        \n        self.markVirtualObs.append(0)\n\n        self.X = np.vstack((self.X, x_max.reshape((1, -1))))\n        self.T = np.vstack((self.T, t_max.reshape((1, -1))))\n\n        # compute X in original scale\n        temp_X_new_original=self.Xscaler.inverse_transform(np.reshape(x_max,(-1,self.dim-1)))\n        #temp_X_new_original=x_max*self.max_min_gap[:-1]+self.SearchSpace[:-1,0]\n        self.X_original=np.vstack((self.X_original, temp_X_new_original))\n        \n        #temp_T_new_original=t_max*self.max_min_gap[-1]+self.SearchSpace[-1,0]\n        temp_T_new_original=self.Tscaler.inverse_transform(np.reshape(t_max,(-1,1)))\n        self.T_original=np.vstack((self.T_original, temp_T_new_original))\n\n        # evaluate Y using original X\n        x_original_to_test=x_max_temp*self.max_min_gap+self.SearchSpace[:,0]\n\n        # evaluate the black-box function=================================================\n        y_original_curves, y_cost_original= self.f(x_original_to_test)\n        \n        # compute the utility score by transformation\n        y_original=transform_logistic(y_original_curves,\\\n              self.gp.logistic_hyper['midpoint'],self.gp.logistic_hyper['growth'],self.SearchSpace[-1,1])\n        \n        if len(y_original_curves)==1: # list\n            self.Y_curves.append(y_original_curves[0])\n        else:\n            self.Y_curves.append(y_original_curves)\n\n        \n        self.Y_original = np.append(self.Y_original,y_original)\n        self.Y_cost_original = np.append(self.Y_cost_original,y_cost_original)\n\n        # augmenting virtual observations =====================================================\n        self.generating_virtual_observations(x_max,t_max,y_original_curves,y_cost_original[0])\n        \n        # update Y after change Y_original        \n        if np.std(self.Y_original)==0:\n            self.Y=(self.Y_original-np.mean(self.Y_original))\n        else:\n            self.Y=(self.Y_original-np.mean(self.Y_original))/np.std(self.Y_original)\n            \n        self.Y_cost=(self.Y_cost_original-np.min(self.Y_cost_original))/(np.max(self.Y_cost_original)-np.min(self.Y_cost_original))\n                    \n        #if self.verbose:\n        np.set_printoptions(suppress=True)\n\n        print(\"[original scale] x={} t={:.0f} current y={:.2f}, ybest={:.2f}\".format( np.round(self.X_original[-1],decimals=4),\\\n              np.asscalar(self.T_original[-1]),np.asscalar(self.Y_original[-1]), np.asscalar(self.Y_original.max())))",
    "Experiment Result": "GP Noise Delta: 5e-4 (default in ProductGaussianProcess), optimized within bounds [1e-3, 1] for lengthscale_x, [1e-2, 1e-2] for noise_upperbound. GP Lengthscale (hyperparameters for the product kernel): Initial lengthscale_x=0.02, lengthscale_t=0.2. Optimized within bounds [0.03, 0.3] for lengthscale_x and [0.3, 0.6] (10*0.03, 2*0.3) for lengthscale_t. Logistic Preference Function Parameters: Initial midpoint=0.0, growth=1.0. Optimized within bounds [-2, 3] for midpoint and [0.5, 2] for growth. Acquisition Function: Cost-aware Expected Improvement ('ei_mu_max'), maximizing log(utility) - log(mean_cost). Cost Model: LinearRegression from sklearn.linear_model.Hyperparameter Optimization Frequency: GP and Logistic hyperparameters are optimized every `2*dim` iterations (where `dim` is the search space dimension, including time). Acquisition Function Maximization: scipy.optimize.minimize with 'L-BFGS-B' method. Multi-start optimization with 3*dim random initial points for cost-aware acquisition, and 1*dim for pure EI/UCB. Optimization Options: `maxiter=10*dim`, `maxfun=20*dim` for acquisition function optimization; `maxiter=30*dim`, `maxfun=30*dim` for hyperparameter optimization. Data Augmentation: Maximum of `10` virtual observations (`max_n_augmentation=10`) per real observation, selected based on maximizing GP predictive uncertainty ('pure_exploration'). Condition Number Threshold: Log of GP covariance matrix condition number `threshold_cond=15` to limit augmentation when the matrix becomes ill-conditioned. Normalization: MinMaxScaler used for input space and time dimensions. Target function values are standardized to N(0,1). Cost values are min-max scaled to [0,1]. Initial Points: `n_init_points=3` random points are used for initialization. Random Seed: `np.random.seed(seed)` is used for reproducibility of initial points and exploration."
}{
    "Title": "Multi-Fidelity Bayesian Optimization via Deep Neural Networks",
    "Main Contributions": "The paper proposes Deep Neural Network Multi-Fidelity Bayesian Optimization (DNN-MFBO) to address the issue of existing multi-fidelity BO methods either ignoring or over-simplifying the strong, complex correlations across fidelities, which leads to inefficient objective function estimation. DNN-MFBO leverages deep neural networks to flexibly capture all kinds of complicated, potentially nonlinear and nonstationary, relationships between fidelities, thereby improving objective function estimation and overall optimization performance. A tractable and efficient mutual information-based acquisition function is computed using sequential, fidelity-wise Gauss-Hermite quadrature and moment-matching.",
    "Methodology": "DNN-MFBO uses a stacked neural network architecture where each NN models one fidelity. For fidelity m > 1, the NN input is constructed by appending the output from the previous fidelity (f_m-1(x)) to the original input (x), allowing information propagation and capturing complex inter-fidelity relationships. The model defines f_m(x) = w_m^T * phi_theta_m(x_m) + epsilon_m, where w_m are output layer weights (random variables) and theta_m are other weights (hyper-parameters). Stochastic variational learning is developed to jointly estimate the posterior of random weights (q(w_m) = N(w_m|mu_m, Sigma_m)) and hyper-parameters by maximizing an ELBO using the reparameterization trick. The acquisition function is defined as a(x, m) = (1/lambda_m) * I(f*, f_m(x)|D), maximizing mutual information between the objective's maximum (f*) and the queried fidelity's output. To compute this, output posteriors p(f_m(x)|D) are approximated as Gaussian distributions using fidelity-wise moment matching and Gauss-Hermite quadrature, especially for non-linear coupling. A Monte-Carlo approximation and a truncated Gaussian approximation are used for the entropy terms in the acquisition function, with further moment matching to obtain Gaussian forms for analytical entropy calculation.",
    "Experimental Setup": "DNN-MFBO was evaluated on three synthetic benchmark datasets: Branin function (3 fidelities, 2D input), Park1 function (2 fidelities, 4D input), and Levy function (3 fidelities, 2D input). It was also tested on two real-world engineering design applications: Mechanical Plate Vibration Design (optimizing material properties for maximum vibration frequency, 2 fidelities: coarse vs. dense mesh) and Thermal Conductor Design (optimizing central hole shape for fast heat conduction, 2 fidelities: coarse vs. dense mesh). Competing methods included Multi-Fidelity Sequential Kriging (MF-SKO), MF-GP-UCB, Multi-Fidelity Predictive Entropy Search (MF-PES), Multi-Fidelity Maximum Entropy Search (MF-MES), Multi-Task NN-based BO (MTNN-BO), and single-fidelity MES (SF-MES). Evaluation metrics included simple regret (SR) and inference regret (IR) for synthetic tasks, and queried maximum/minimum function values along with query cost for real-world tasks. The average query time was also measured. Hyper-parameters for NNs (depth, width, learning rate) were tuned using SMAC3 and manual adjustments. ADAM optimizer was used for training, and L-BFGS for acquisition function optimization (with random initialization for DNN-MFBO). Initial training points were randomly queried across fidelities.",
    "Limitations": "The existing methods either ignore the strong, complex correlations between fidelities (e.g., MF-GP-UCB, independent GP for each fidelity) or model them with over-simplified structures (e.g., MF-PES requiring simple/smooth kernels for tractability, MF-MES assuming linear correlation), leading to inefficient and inaccurate objective function estimation. While DNN-MFBO addresses these, its computational complexity for the acquisition function, despite optimizations, is noted as 'quite complex'. The current work focuses on discrete fidelities, not continuous ones.",
    "Future Research Directions": "Not mentioned",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "Batch Multi-Fidelity Bayesian Optimization with  Deep Auto-Regressive Networks",
    "Main Contributions": "Proposes Batch Multi-Fidelity Bayesian Optimization with Deep Auto-Regressive Networks (BMBO-DARN) to optimize black-box, expensive-to-evaluate functions across multiple fidelities. Introduces a deep auto-regressive model using Bayesian neural networks to capture strong, complex (nonstationary, highly nonlinear) relationships across all fidelities, improving surrogate learning and optimization performance. Develops an efficient batch querying method that utilizes a novel batch acquisition function based on Max-value Entropy Search (MES), penalizing highly correlated queries and encouraging diversity. The method employs efficient computation of the acquisition function via posterior samples and moment matching, and an alternating optimization algorithm to select query points without combinatorial search. Demonstrates superior performance and reduced optimization cost in four real-world hyperparameter optimization applications.",
    "Methodology": "The methodology centers on a deep auto-regressive model for multi-fidelity surrogate learning. This model consists of a chain of M Bayesian neural networks (BNNs), where each BNN `fm(x)` models a specific fidelity. The input `xm` for each `fm` includes the original input `x` and the outputs of all preceding fidelities (`f1(x), ..., fm-1(x)`), enabling the model to capture complex, possibly nonlinear and nonstationary, relationships across fidelities. Hamiltonian Monte Carlo (HMC) sampling is used for posterior inference of the BNN parameters and noise precisions. For batch querying, a batch acquisition function is proposed based on the Max-value Entropy Search (MES) principle, quantifying the mutual information between the outputs of a batch of queries and the function optimum, normalized by the total query cost to balance benefit and cost. To efficiently compute this acquisition function, posterior samples of NN weights are drawn, and moment matching is applied to approximate the joint posterior of the query outputs and the function optimum as a multivariate Gaussian distribution, allowing for a closed-form mutual information calculation. The maximization of this batch acquisition function, which involves a mix of continuous inputs and discrete fidelities, is performed using an alternating optimization algorithm that cyclically updates each fidelity-input pair to avoid computationally expensive combinatorial search, guaranteeing improvement at each step.",
    "Experimental Setup": "The approach was evaluated on both synthetic benchmarks and real-world hyperparameter optimization tasks. Synthetic benchmarks included the Levy function (two-fidelity, 2D input) and Branin function (three-fidelity, 2D input), where nonlinear/nonstationary transformations existed between fidelities. Performance was measured by normalized root-mean-square-error (nRMSE) and mean-negative-log-likelihood (MNLL) on 100 test samples after training with specific numbers of examples at different fidelities. Real-world applications involved hyperparameter optimization for: (1) Convolutional Neural Networks (CNN) on CIFAR-10 for image classification (3 fidelities: 1, 10, 50 epochs; metric: negative log-loss), (2) Online Latent Dirichlet Allocation (LDA) on 20NewsGroups for topic extraction (3 fidelities: 1, 10, 50 epochs; metric: perplexity), (3) XGBoost for diabetes diagnosis (3 fidelities: 2, 10, 100 weak learners; metric: log nRMSE), and (4) Physics-Informed Neural Networks (PINN) for solving Burger's equation (3 fidelities: 10, 100, 50K max L-BFGS iterations; metric: log nRMSE). All experiments started with 10 random initial queries per fidelity. Batch size was set to 5 for batch methods. BMBO-DARN was implemented in PyTorch, utilizing Hamiltorch for HMC sampling (5K burn-in, 200 posterior samples). The alternating optimization for the batch acquisition function ran for a maximum of 100 iterations or until a tolerance of 10^-3 was reached. Results were averaged over five runs. Comparison methods included state-of-the-art multi-fidelity BO (MF-GP-UCB, MF-MES, SHTL, DNN-MFBO), batch BO (MF-MES-Batch, SF-Batch), and popular hyperparameter tuning methods (SMAC3, Hyperband, BOHB), as well as a single-query version of BMBO-DARN (BMBO-DARN-1).",
    "Limitations": "While BMBO-DARN addresses several limitations of prior multi-fidelity Bayesian optimization methods, its own design involves certain inherent constraints or approximations. The computation of the batch acquisition function relies on approximating the joint posterior distribution of function values and the optimum as a multivariate Gaussian using moment matching, which might not perfectly capture the true, potentially complex, posterior of Bayesian Neural Networks. Additionally, the alternating optimization algorithm used to maximize the batch acquisition function is a heuristic employed to avoid computationally prohibitive combinatorial search, meaning it does not guarantee finding the global optimum for the batch of queries. Furthermore, although the paper mentions the possibility of switching to approximate inference, the primary method used, Hamiltonian Monte Carlo (HMC) sampling, can be computationally intensive, particularly for complex neural network architectures, despite offering high-quality uncertainty quantification.",
    "Future Research Directions": "Not mentioned",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "Supervising the Multi-Fidelity Race of Hyperparameter Configurations",
    "Main Contributions": "The paper introduces DyHPO, a Bayesian Optimization (BO) method designed to dynamically allocate hyperparameter optimization (HPO) budget by learning which configurations to train further in a multi-fidelity setting. It proposes a novel deep kernel for Gaussian Processes (GP) that embeds learning curve dynamics and an acquisition function incorporating multi-budget information. DyHPO significantly outperforms state-of-the-art HPO methods, especially gray-box baselines, across diverse deep learning architectures (MLP, CNN/NAS, RNN) and 50 datasets (tabular, image, NLP), making a step towards scaling HPO for Deep Learning. Key contributions include a novel Bayesian surrogate that predicts validation scores based on hyperparameter configuration, budget, and the learning curve, and a robust integration of this surrogate with Bayesian optimization.",
    "Methodology": "DyHPO is a Bayesian Optimization approach based on Gaussian Processes (GP). Its core methodology involves a dynamic budget allocation strategy, contrasting with static methods. A central component is a deep kernel for the GP surrogate model, which captures the learning dynamics across different budgets. This kernel `K` takes as input the hyperparameter configuration `xi`, the past learning curve `Yi,j-1`, and the budget `j`. A neural network `φ` (composed of linear and 1D convolutional layers followed by global max pooling) extracts features from these inputs, which are then fed into a squared exponential kernel `k`. The parameters of both `k` and `φ` are learned by maximizing the marginal likelihood using gradient descent with Adam. The acquisition function is a multi-fidelity version of Expected Improvement (EIMF), which dynamically determines the next configuration and budget to evaluate. It selects the configuration `xi` to train for one additional budget step `b(xi)+1` by maximizing `EIMF (x, b(x) + 1)`, ensuring a slow, exploratory increase in budget investment.",
    "Experimental Setup": "The experimental setup evaluates DyHPO across three diverse deep learning settings: hyperparameter optimization for tabular, text, and image classification. All experiments were conducted on an Amazon EC2 M5 Instance (m5.xlarge). Performance was measured using the mean of ten repetitions, focusing on two metrics: mean regret (absolute difference to the best possible score) and average rank across datasets. Statistical significance was assessed using the Friedman test followed by a Wilcoxon signed-rank test (α = 0.05). Three benchmarks were used: LCBench (35 tabular datasets, 2,000 neural networks per dataset, 50 epochs), TaskSet (12 NLP tasks with RNNs, Adam8p search space, scores every 200 iterations), and NAS-Bench-201 (15,625 architectures on CIFAR-10, CIFAR-100, ImageNet, 200 epochs). DyHPO was compared against seven baselines: Random Search, HyperBand, BOHB, DEHB, ASHA, MF-DNN, and Dragonfly (BOCA). The Deep Kernel Gaussian Process was implemented using GPyTorch 1.5 with an RBF kernel, dense layers of 128 and 256 units, and a convolutional layer with a kernel size of three and four filters. Training involved Adam optimizer with a learning rate of 0.1 and batch size of 64, with early stopping after 10 epochs without improvement or 1,000 total epochs.",
    "Limitations": "The current work has several limitations. Firstly, there is a lack of suitable tabular benchmarks for evaluating HPO on very large deep learning models, such as Transformer-based architectures, which restricts the scope of current experiments. Secondly, the 'pause and resume' capability of the training procedure, crucial for multi-fidelity optimization, is only applicable to parametric models; for non-parametric models, training a configuration would require a complete restart. Lastly, for datasets where training is inherently fast, the computational overhead associated with model-based HPO techniques like DyHPO might outweigh their benefits, making simpler methods such as random search more practical and appealing.",
    "Future Research Directions": "Future research directions include expanding the evaluation to HPO for very large deep learning models, such as Transformer-based architectures, which currently lack suitable tabular benchmarks. Further exploration is needed to adapt the 'pause and resume' training procedure to non-parametric models, where current multi-fidelity methods might require full restarts. For tasks with short training times, research could focus on reducing the overhead of model-based techniques to make them more competitive against simpler methods like random search. Additionally, the authors suggest that the community create sparse benchmarks with surrogates, rather than dense tabular ones, to save energy and computational resources in HPO research.",
    "Experiment Code": "import copy\nimport json\nimport logging\nimport math\nimport os\nimport time\nfrom typing import Dict, List, Optional, Tuple\n\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nfrom scipy.stats import norm, t\nimport torch\n\nfrom surrogate_models.dyhpo import DyHPO\n\n\nclass DyHPOAlgorithm:\n\n    def __init__(\n        self,\n        hp_candidates: np.ndarray,\n        log_indicator: List,\n        seed: int = 11,\n        max_benchmark_epochs: int = 52,\n        fantasize_step: int = 1,\n        minimization: bool = True,\n        total_budget: int = 500,\n        device: str = None,\n        dataset_name: str = 'unknown',\n        output_path: str = '.',\n        surrogate_config: dict = None,\n        verbose: bool = True,\n    ):\n        \"\"\"\n        Args:\n            hp_candidates: np.ndarray\n                The full list of hyperparameter candidates for\n                a given dataset.\n            log_indicator: List\n                A list with boolean values indicating if a\n                hyperparameter has been log sampled or not.\n            seed: int\n                The seed that will be used for the surrogate.\n            max_benchmark_epochs: int\n                The maximal budget that a hyperparameter configuration\n                has been evaluated in the benchmark for.\n            fantasize_step: int\n                The number of steps for which we are looking ahead to\n                evaluate the performance of a hpc.\n            minimization: bool\n                If the objective should be maximized or minimized.\n            total_budget: int\n                The total budget given for hyperparameter optimization.\n            device: str\n                The device where the experiment will be run on.\n            dataset_name: str\n                The name of the dataset that the experiment will be run on.\n            output_path: str\n                The path where all the output will be stored.\n            surrogate_config: dict\n                The model configurations for the surrogate.\n            verbose: boolean\n                If detailed information is preferred in the log file.\n        \"\"\"\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        torch.manual_seed(seed)\n        np.random.seed(seed)\n\n        if device is None:\n            self.dev = torch.device(\n                'cuda') if torch.cuda.is_available() else torch.device('cpu')\n        else:\n            self.dev = torch.device(device)\n\n        self.hp_candidates = hp_candidates\n        self.log_indicator = log_indicator\n\n        self.scaler = MinMaxScaler()\n        self.hp_candidates = self.preprocess_hp_candidates()\n\n        self.minimization = minimization\n        self.seed = seed\n\n        if verbose:\n            logging_level = logging.DEBUG\n        else:\n            logging_level = logging.INFO\n        self.logger = logging.getLogger()\n\n        logging.basicConfig(\n            format='%(levelname)s:%(asctime)s:%(message)s',\n            filename=f'dyhpo_surrogate_{dataset_name}_{seed}.log',\n            level=logging_level,\n        )\n\n        # the keys will be hyperparameter indices while the value\n        # will be a list with all the budgets evaluated for examples\n        # and with all performances for the performances\n        self.examples = dict()\n        self.performances = dict()\n\n        # set a seed already, so that it is deterministic when\n        # generating the seeds of the ensemble\n        torch.manual_seed(seed)\n        np.random.seed(seed)\n\n        self.max_benchmark_epochs = max_benchmark_epochs\n        self.total_budget = total_budget\n        self.fantasize_step = fantasize_step\n        self.nr_features = self.hp_candidates.shape[1]\n\n        initial_configurations_nr = 1\n        conf_individual_budget = 1\n        self.init_conf_indices = np.random.choice(self.hp_candidates.shape[0], initial_configurations_nr, replace=False)\n        self.init_budgets = [conf_individual_budget] * initial_configurations_nr\n        # with what percentage configurations will be taken randomly instead of being sampled from the model\n        self.fraction_random_configs = 0.1\n\n        self.model = None\n        # An index keeping track of where we are in the init_conf_indices\n        # list of hyperparmeters that are not sampled from the model.\n        self.initial_random_index = 0\n\n        if surrogate_config is None:\n            self.surrogate_config = {\n                'nr_layers': 2,\n                'nr_initial_features': self.nr_features,\n                'layer1_units': 64,\n                'layer2_units': 128,\n                'cnn_nr_channels': 4,\n                'cnn_kernel_size': 3,\n                'batch_size': 64,\n                'nr_epochs': 1000,\n                'nr_patience_epochs': 10,\n                'learning_rate': 0.001,\n            }\n        else:\n            self.surrogate_config = surrogate_config\n\n        # the incumbent value observed during the hpo process.\n        self.best_value_observed = np.NINF\n        # a set which will keep track of the hyperparameter configurations that diverge.\n        self.diverged_configs = set()\n\n        # info dict to drop every surrogate iteration\n        self.info_dict = dict()\n\n        # the start time for the overhead of every surrogate optimization iteration\n        # will be recorded here\n        self.suggest_time_duration = 0\n        # the total budget consumed so far\n        self.budget_spent = 0\n\n        self.output_path = output_path\n        self.dataset_name = dataset_name\n\n        self.no_improvement_threshold = int(self.max_benchmark_epochs + 0.2 * self.max_benchmark_epochs)\n        self.no_improvement_patience = 0\n\n\n    def _prepare_dataset_and_budgets(self) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Prepare the data that will be the input to the surrogate.\n\n        Returns:\n            data: A Dictionary that contains inside the training examples,\n            the budgets, the curves and lastly the labels.\n        \"\"\"\n\n        train_examples, train_labels, train_budgets, train_curves = self.history_configurations()\n\n        train_examples = np.array(train_examples, dtype=np.single)\n        train_labels = np.array(train_labels, dtype=np.single)\n        train_budgets = np.array(train_budgets, dtype=np.single)\n        train_curves = self.patch_curves_to_same_length(train_curves)\n        train_curves = np.array(train_curves, dtype=np.single)\n\n        # scale budgets to [0, 1]\n        train_budgets = train_budgets / self.max_benchmark_epochs\n\n        train_examples = torch.tensor(train_examples)\n        train_labels = torch.tensor(train_labels)\n        train_budgets = torch.tensor(train_budgets)\n        train_curves = torch.tensor(train_curves)\n\n        train_examples = train_examples.to(device=self.dev)\n        train_labels = train_labels.to(device=self.dev)\n        train_budgets = train_budgets.to(device=self.dev)\n        train_curves = train_curves.to(device=self.dev)\n\n        data = {\n            'X_train': train_examples,\n            'train_budgets': train_budgets,\n            'train_curves': train_curves,\n            'y_train': train_labels,\n        }\n\n        return data\n\n    def _train_surrogate(self):\n        \"\"\"\n        Train the surrogate model.\n        \"\"\"\n        data = self._prepare_dataset_and_budgets()\n        self.logger.info(f'Started training the model')\n\n        self.model.train_pipeline(\n            data,\n            load_checkpoint=False,\n        )\n\n    def _predict(self) -> Tuple[np.ndarray, np.ndarray, List, List]:\n        \"\"\"\n        Predict the performances of the hyperparameter configurations\n        as well as the standard deviations based on the surrogate model.\n\n        Returns:\n            mean_predictions, std_predictions, hp_indices, non_scaled_budgets:\n                The mean predictions and the standard deviations over\n                all model predictions for the given hyperparameter\n                configurations with their associated indices, scaled and\n                non-scaled budgets.\n        \"\"\"\n        configurations, hp_indices, budgets, learning_curves = self.generate_candidate_configurations()\n        budgets = np.array(budgets, dtype=np.single)\n        non_scaled_budgets = copy.deepcopy(budgets)\n        # scale budgets to [0, 1]\n        budgets = budgets / self.max_benchmark_epochs\n\n        configurations = np.array(configurations, dtype=np.single)\n        configurations = torch.tensor(configurations)\n        configurations = configurations.to(device=self.dev)\n\n        budgets = torch.tensor(budgets)\n        budgets = budgets.to(device=self.dev)\n\n        learning_curves = self.patch_curves_to_same_length(learning_curves)\n        learning_curves = np.array(learning_curves, dtype=np.single)\n        learning_curves = torch.tensor(learning_curves)\n        learning_curves = learning_curves.to(device=self.dev)\n\n        train_data = self._prepare_dataset_and_budgets()\n        test_data = {\n            'X_test': configurations,\n            'test_budgets': budgets,\n            'test_curves': learning_curves,\n        }\n\n        mean_predictions, std_predictions = self.model.predict_pipeline(train_data, test_data)\n\n        return mean_predictions, std_predictions, hp_indices, non_scaled_budgets\n\n    def suggest(self) -> Tuple[int, int]:\n        \"\"\"\n        Suggest a hyperparameter configuration to be evaluated next.\n\n        Returns:\n            best_config_index, budget: The index of the hyperparamter\n                configuration to be evaluated and the budget for\n                what it is going to be evaluated for.\n        \"\"\"\n        suggest_time_start = time.time()\n        # check if we still have random hyperparameters to evaluate\n        if self.initial_random_index < len(self.init_conf_indices):\n            self.logger.info(\n                'Not enough configurations to build a model. '\n                'Returning randomly sampled configuration'\n            )\n\n            random_indice = self.init_conf_indices[self.initial_random_index]\n            budget = self.init_budgets[self.initial_random_index]\n            self.initial_random_index += 1\n\n            return random_indice, budget\n        else:\n            mean_predictions, std_predictions, hp_indices, non_scaled_budgets = self._predict()\n            best_prediction_index = self.find_suggested_config(\n                mean_predictions,\n                std_predictions,\n                non_scaled_budgets,\n            )\n            \"\"\"\n            the best prediction index is not always matching with the actual hp index.\n            Since when evaluating the acq function, we do not consider hyperparameter\n            candidates that diverged or that are evaluated fully.\n            \"\"\"\n            best_config_index = hp_indices[best_prediction_index]\n\n            # decide for what budget we will evaluate the most\n            # promising hyperparameter configuration next.\n            if best_config_index in self.examples:\n                evaluated_budgets = self.examples[best_config_index]\n                max_budget = max(evaluated_budgets)\n                budget = max_budget + self.fantasize_step\n                # this would only trigger if fantasize_step is bigger\n                # than 1\n                if budget > self.max_benchmark_epochs:\n                    budget = self.max_benchmark_epochs\n            else:\n                budget = self.fantasize_step\n\n        suggest_time_end = time.time()\n        self.suggest_time_duration = suggest_time_end - suggest_time_start\n\n        self.budget_spent += self.fantasize_step\n\n        # exhausted hpo budget, finish.\n        if self.budget_spent > self.total_budget:\n            exit(0)\n\n        return best_config_index, budget\n\n    def observe(\n        self,\n        hp_index: int,\n        b: int,\n        learning_curve: np.ndarray,\n        alg_time: Optional[float] = None,\n    ):\n        \"\"\"\n        Args:\n            hp_index: The index of the evaluated hyperparameter configuration.\n            b: The budget for which the hyperparameter configuration was evaluated.\n            learning_curve: The learning curve of the hyperparameter configuration.\n            alg_time: The time taken from the algorithm to evaluate the hp configuration.\n        \"\"\"\n        score = learning_curve[-1]\n        # if y is an undefined value, append 0 as the overhead since we finish here.\n        if np.isnan(learning_curve).any():\n            self.update_info_dict(hp_index, b, np.nan, 0)\n            self.diverged_configs.add(hp_index)\n            return\n\n        observe_time_start = time.time()\n\n        self.examples[hp_index] = np.arange(1, b + 1).tolist()\n        self.performances[hp_index] = learning_curve\n\n        if self.best_value_observed < score:\n            self.best_value_observed = score\n            self.no_improvement_patience = 0\n        else:\n            self.no_improvement_patience += 1\n\n        observe_time_end = time.time()\n        train_time_duration = 0\n\n        # initialization phase over. Now we can sample from the model.\n        if self.initial_random_index >= len(self.init_conf_indices):\n            train_time_start = time.time()\n            # create the model for the first time\n            if self.model is None:\n                # Starting a model from scratch\n                self.model = DyHPO(\n                    self.surrogate_config,\n                    self.dev,\n                    self.dataset_name,\n                    self.output_path,\n                    self.seed,\n                )\n\n            if self.no_improvement_patience == self.no_improvement_threshold:\n                self.model.restart = True\n\n            self._train_surrogate()\n\n            train_time_end = time.time()\n            train_time_duration = train_time_end - train_time_start\n\n        observe_time_duration = observe_time_end - observe_time_start\n        total_duration = observe_time_duration + self.suggest_time_duration + train_time_duration\n        if alg_time is not None:\n            total_duration = total_duration + alg_time\n\n        self.update_info_dict(hp_index, b, score, total_duration)\n\n    def prepare_examples(self, hp_indices: List) -> List[np.ndarray]:\n        \"\"\"\n        Prepare the examples to be given to the surrogate model.\n\n        Args:\n            hp_indices: The list of hp indices that are already evaluated.\n\n        Returns:\n            examples: A list of the hyperparameter configurations.\n        \"\"\"\n        examples = []\n        for hp_index in hp_indices:\n            examples.append(self.hp_candidates[hp_index])\n\n        return examples\n\n    def generate_candidate_configurations(\n        self,\n    ) -> Tuple[List, List, List, List]:\n        \"\"\"\n        Generate candidate configurations that will be\n        fantasized upon.\n\n        Returns:\n            (configurations, hp_indices, hp_budgets, learning_curves): Tuple\n                A tuple of configurations, their indices in the hp list\n                and the budgets that they should be fantasized upon.\n        \"\"\"\n        hp_indices = []\n        hp_budgets = []\n        learning_curves = []\n\n        for hp_index in range(0, self.hp_candidates.shape[0]):\n\n            if hp_index in self.examples:\n                budgets = self.examples[hp_index]\n                # Take the max budget evaluated for a certain hpc\n                max_budget = max(budgets)\n                next_budget = max_budget + self.fantasize_step\n                # take the learning curve until the point we have evaluated so far\n                curve = self.performances[hp_index][:max_budget]\n                # if the curve is shorter than the length of the kernel size,\n                # pad it with zeros\n                difference_curve_length = self.surrogate_config['cnn_kernel_size'] - len(curve)\n                if difference_curve_length > 0:\n                    curve.extend([0.0] * difference_curve_length)\n            else:\n                # The hpc was not evaluated before, so fantasize its\n                # performance\n                next_budget = self.fantasize_step\n                curve = [0, 0, 0]\n\n            # this hyperparameter configuration is not evaluated fully\n            if next_budget <= self.max_benchmark_epochs:\n                hp_indices.append(hp_index)\n                hp_budgets.append(next_budget)\n                learning_curves.append(curve)\n\n        configurations = self.prepare_examples(hp_indices)\n\n        return configurations, hp_indices, hp_budgets, learning_curves\n\n    def history_configurations(\n        self,\n    ) -> Tuple[List, List, List, List]:\n        \"\"\"\n        Generate the configurations, labels, budgets and curves based on\n        the history of evaluated configurations.\n\n        Returns:\n            (train_examples, train_labels, train_budgets, train_curves):\n                A tuple of examples, labels, budgets and curves for the\n                configurations evaluated so far.\n        \"\"\"\n        train_examples = []\n        train_labels = []\n        train_budgets = []\n        train_curves = []\n\n        for hp_index in self.examples:\n            budgets = self.examples[hp_index]\n            performances = self.performances[hp_index]\n            example = self.hp_candidates[hp_index]\n\n            for budget, performance in zip(budgets, performances):\n                train_examples.append(example)\n                train_budgets.append(budget)\n                train_labels.append(performance)\n                train_curve = performances[:budget - 1] if budget > 1 else [0.0]\n                difference_curve_length = self.surrogate_config['cnn_kernel_size'] - len(train_curve)\n                if difference_curve_length > 0:\n                    train_curve.extend([0.0] * difference_curve_length)\n\n                train_curves.append(train_curve)\n\n        return train_examples, train_labels, train_budgets, train_curves\n\n    def acq(\n        self,\n        best_value: float,\n        mean: float,\n        std: float,\n        explore_factor: Optional[float] = 0.25,\n        acq_fc: str = 'ei',\n    ) -> float:\n        \"\"\"\n        The acquisition function that will be called\n        to evaluate the score of a hyperparameter configuration.\n\n        Parameters\n        ----------\n        best_value: float\n            Best observed function evaluation. Individual per fidelity.\n        mean: float\n            Point mean of the posterior process.\n        std: float\n            Point std of the posterior process.\n        explore_factor: float\n            The exploration factor for when ucb is used as the\n            acquisition function.\n        ei_calibration_factor: float\n            The factor used to calibrate expected improvement.\n        acq_fc: str\n            The type of acquisition function to use.\n\n        Returns\n        -------\n        acq_value: float\n            The value of the acquisition function.\n        \"\"\"\n        if acq_fc == 'ei':\n            if std == 0:\n                return 0\n            z = (mean - best_value) / std\n            acq_value = (mean - best_value) * norm.cdf(z) + std * norm.pdf(z)\n        elif acq_fc == 'ucb':\n            acq_value = mean + explore_factor * std\n        elif acq_fc == 'thompson':\n            acq_value = np.random.normal(mean, std)\n        elif acq_fc == 'exploit':\n            acq_value = mean\n        else:\n            raise NotImplementedError(\n                f'Acquisition function {acq_fc} has not been'\n                f'implemented',\n            )\n\n        return acq_value\n\n    def find_suggested_config(\n        self,\n        mean_predictions: np.ndarray,\n        mean_stds: np.ndarray,\n        budgets: List,\n    ) -> int:\n        \"\"\"\n        Find the hyperparameter configuration that has the highest score\n        with the acquisition function.\n\n        Args:\n            mean_predictions: The mean predictions of the posterior.\n            mean_stds: The mean standard deviations of the posterior.\n            budgets: The next budgets that the hyperparameter configurations\n                will be evaluated for.\n\n        Returns:\n            best_index: The index of the hyperparameter configuration with the\n                highest score.\n        \"\"\"\n        highest_acq_value = np.NINF\n        best_index = -1\n\n        index = 0\n        for mean_value, std in zip(mean_predictions, mean_stds):\n            budget = int(budgets[index])\n            best_value = self.calculate_fidelity_ymax(budget)\n            acq_value = self.acq(best_value, mean_value, std, acq_fc='ei')\n            if acq_value > highest_acq_value:\n                highest_acq_value = acq_value\n                best_index = index\n\n            index += 1\n\n        return best_index\n\n    def calculate_fidelity_ymax(self, fidelity: int):\n        \"\"\"\n        Find ymax for a given fidelity level.\n\n        If there are hyperparameters evaluated for that fidelity\n        take the maximum from their values. Otherwise, take\n        the maximum from all previous fidelity levels for the\n        hyperparameters that we have evaluated.\n\n        Args:\n            fidelity: The fidelity of the hyperparameter\n                configuration.\n\n        Returns:\n            best_value: The best value seen so far for the\n                given fidelity.\n        \"\"\"\n        exact_fidelity_config_values = []\n        lower_fidelity_config_values = []\n\n        for example_index in self.examples.keys():\n            try:\n                performance = self.performances[example_index][fidelity - 1]\n                exact_fidelity_config_values.append(performance)\n            except IndexError:\n                learning_curve = self.performances[example_index]\n                # The hyperparameter was not evaluated until fidelity, or more.\n                # Take the maximum value from the curve.\n                lower_fidelity_config_values.append(max(learning_curve))\n\n        if len(exact_fidelity_config_values) > 0:\n            # lowest error corresponds to best value\n            best_value = max(exact_fidelity_config_values)\n        else:\n            best_value = max(lower_fidelity_config_values)\n\n        return best_value\n\n    def update_info_dict(\n        self,\n        hp_index: int,\n        budget: int,\n        performance: float,\n        overhead: float,\n    ):\n        \"\"\"\n        Update the info dict with the current HPO iteration info.\n\n        Dump a new json file that will update with additional information\n        given the current HPO iteration.\n\n        Args:\n            hp_index: The index of the hyperparameter configuration.\n            budget: The budget of the hyperparameter configuration.\n            performance:  The performance of the hyperparameter configuration.\n            overhead: The total overhead (in seconds) of the iteration.\n        \"\"\"\n        hp_index = int(hp_index)\n        if 'hp' in self.info_dict:\n            self.info_dict['hp'].append(hp_index)\n        else:\n            self.info_dict['hp'] = [hp_index]\n\n        if 'scores' in self.info_dict:\n            self.info_dict['scores'].append(performance)\n        else:\n            self.info_dict['scores'] = [performance]\n\n        if 'curve' in self.info_dict:\n            self.info_dict['curve'].append(self.best_value_observed)\n        else:\n            self.info_dict['curve'] = [self.best_value_observed]\n\n        if 'epochs' in self.info_dict:\n            self.info_dict['epochs'].append(budget)\n        else:\n            self.info_dict['epochs'] = [budget]\n\n        if 'overhead' in self.info_dict:\n            self.info_dict['overhead'].append(overhead)\n        else:\n            self.info_dict['overhead'] = [overhead]\n\n        with open(os.path.join(self.output_path, f'{self.dataset_name}_{self.seed}.json'), 'w') as fp:\n            json.dump(self.info_dict, fp)\n\n    def preprocess_hp_candidates(self) -> List:\n        \"\"\"\n        Preprocess the list of all hyperparameter candidates\n        by  performing a log transform for the hyperparameters that\n        were log sampled.\n\n        Returns:\n            log_hp_candidates: The list of all hyperparameter configurations\n                where hyperparameters that were log sampled are log transformed.\n        \"\"\"\n        log_hp_candidates = []\n\n        for hp_candidate in self.hp_candidates:\n            new_hp_candidate = []\n            for index, hp_value in enumerate(hp_candidate):\n                new_hp_candidate.append(math.log(hp_value) if self.log_indicator[index] else hp_value)\n\n            log_hp_candidates.append(new_hp_candidate)\n\n        log_hp_candidates = np.array(log_hp_candidates)\n        # scaler for the hp configurations\n\n        log_hp_candidates = self.scaler.fit_transform(log_hp_candidates)\n\n        return log_hp_candidates\n\n    @staticmethod\n    def patch_curves_to_same_length(curves):\n        \"\"\"\n        Patch the given curves to the same length.\n\n        Finds the maximum curve length and patches all\n        other curves that are shorter in length with zeroes.\n\n        Args:\n            curves: The given hyperparameter curves.\n\n        Returns:\n            curves: The updated array where the learning\n                curves are of the same length.\n        \"\"\"\n        max_curve_length = 0\n        for curve in curves:\n            if len(curve) > max_curve_length:\n                max_curve_length = len(curve)\n\n        for curve in curves:\n            difference = max_curve_length - len(curve)\n            if difference > 0:\n                curve.extend([0.0] * difference)\n\n        return curves\n\nfrom copy import deepcopy\nimport logging\nimport os\nfrom typing import Dict, Tuple\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch import cat\n\nimport gpytorch\n\n\nclass FeatureExtractor(nn.Module):\n    \"\"\"\n    The feature extractor that is part of the deep kernel.\n    \"\"\"\n    def __init__(self, configuration):\n        super(FeatureExtractor, self).__init__()\n\n        self.configuration = configuration\n\n        self.nr_layers = configuration['nr_layers']\n        self.act_func = nn.LeakyReLU()\n        # adding one to the dimensionality of the initial input features\n        # for the concatenation with the budget.\n        initial_features = configuration['nr_initial_features'] + 1\n        self.fc1 = nn.Linear(initial_features, configuration['layer1_units'])\n        self.bn1 = nn.BatchNorm1d(configuration['layer1_units'])\n        for i in range(2, self.nr_layers):\n            setattr(\n                self,f'fc{i + 1}',\n                nn.Linear(configuration[f'layer{i - 1}_units'], configuration[f'layer{i}_units']),\n            )\n            setattr(\n                self,f'bn{i + 1}',\n                nn.BatchNorm1d(configuration[f'layer{i}_units']),\n            )\n\n\n        setattr(\n            self,\n            f'fc{self.nr_layers}',\n            nn.Linear(\n                configuration[f'layer{self.nr_layers - 1}_units'] +\n                configuration['cnn_nr_channels'],  # accounting for the learning curve features\n                configuration[f'layer{self.nr_layers}_units']\n            ),\n        )\n        self.cnn = nn.Sequential(\n            nn.Conv1d(in_channels=1, kernel_size=(configuration['cnn_kernel_size'],), out_channels=4),\n            nn.AdaptiveMaxPool1d(1),\n        )\n\n    def forward(self, x, budgets, learning_curves):\n\n        # add an extra dimensionality for the budget\n        # making it nr_rows x 1.\n        budgets = torch.unsqueeze(budgets, dim=1)\n        # concatenate budgets with examples\n        x = cat((x, budgets), dim=1)\n        x = self.fc1(x)\n        x = self.act_func(self.bn1(x))\n\n        for i in range(2, self.nr_layers):\n            x = self.act_func(\n                getattr(self, f'bn{i}')(\n                    getattr(self, f'fc{i}')(\n                        x\n                    )\n                )\n            )\n\n        # add an extra dimensionality for the learning curve\n        # making it nr_rows x 1 x lc_values.\n        learning_curves = torch.unsqueeze(learning_curves, 1)\n        lc_features = self.cnn(learning_curves)\n        # revert the output from the cnn into nr_rows x nr_kernels.\n        lc_features = torch.squeeze(lc_features, 2)\n\n        # put learning curve features into the last layer along with the higher level features.\n        x = cat((x, lc_features), dim=1)\n        x = self.act_func(getattr(self, f'fc{self.nr_layers}')(x))\n\n        return x\n\n\nclass GPRegressionModel(gpytorch.models.ExactGP):\n    \"\"\"\n    A simple GP model.\n    \"\"\"\n    def __init__(\n        self,\n        train_x: torch.Tensor,\n        train_y: torch.Tensor,\n        likelihood: gpytorch.likelihoods.GaussianLikelihood,\n    ):\n        \"\"\"\n        Constructor of the GPRegressionModel.\n\n        Args:\n            train_x: The initial train examples for the GP.\n            train_y: The initial train labels for the GP.\n            likelihood: The likelihood to be used.\n        \"\"\"\n        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n\n        self.mean_module = gpytorch.means.ConstantMean()\n        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n\n    def forward(self, x):\n\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n\n        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n\n\nclass DyHPO:\n    \"\"\"\n    The DyHPO DeepGP model.\n    \"\"\"\n    def __init__(\n        self,\n        configuration: Dict,\n        device: torch.device,\n        dataset_name: str = 'unknown',\n        output_path: str = '.',\n        seed: int = 11,\n    ):\n        \"\"\"\n        The constructor for the DyHPO model.\n\n        Args:\n            configuration: The configuration to be used\n                for the different parts of the surrogate.\n            device: The device where the experiments will be run on.\n            dataset_name: The name of the dataset for the current run.\n            output_path: The path where the intermediate/final results\n                will be stored.\n            seed: The seed that will be used to store the checkpoint\n                properly.\n        \"\"\"\n        super(DyHPO, self).__init__()\n        self.feature_extractor = FeatureExtractor(configuration)\n        self.batch_size = configuration['batch_size']\n        self.nr_epochs = configuration['nr_epochs']\n        self.early_stopping_patience = configuration['nr_patience_epochs']\n        self.refine_epochs = 50\n        self.dev = device\n        self.seed = seed\n        self.model, self.likelihood, self.mll = \\\n            self.get_model_likelihood_mll(\n                configuration[f'layer{self.feature_extractor.nr_layers}_units']\n            )\n\n        self.model.to(self.dev)\n        self.likelihood.to(self.dev)\n        self.feature_extractor.to(self.dev)\n\n        self.optimizer = torch.optim.Adam([\n            {'params': self.model.parameters(), 'lr': configuration['learning_rate']},\n            {'params': self.feature_extractor.parameters(), 'lr': configuration['learning_rate']}],\n        )\n\n        self.configuration = configuration\n        # the number of initial points for which we will retrain fully from scratch\n        # This is basically equal to the dimensionality of the search space + 1.\n        self.initial_nr_points = 10\n        # keeping track of the total hpo iterations. It will be used during the optimization\n        # process to switch from fully training the model, to refining.\n        self.iterations = 0\n        # flag for when the optimization of the model should start from scratch.\n        self.restart = True\n\n        self.logger = logging.getLogger(__name__)\n\n        self.checkpoint_path = os.path.join(\n            output_path,\n            'checkpoints',\n            f'{dataset_name}',\n            f'{self.seed}',\n        )\n\n        os.makedirs(self.checkpoint_path, exist_ok=True)\n\n        self.checkpoint_file = os.path.join(\n            self.checkpoint_path,\n            'checkpoint.pth'\n        )\n\n    def restart_optimization(self):\n        \"\"\"\n        Restart the surrogate model from scratch.\n        \"\"\"\n        self.feature_extractor = FeatureExtractor(self.configuration).to(self.dev)\n        self.model, self.likelihood, self.mll = \\\n            self.get_model_likelihood_mll(\n                self.configuration[f'layer{self.feature_extractor.nr_layers}_units'],\n            )\n\n        self.optimizer = torch.optim.Adam([\n            {'params': self.model.parameters(), 'lr': self.configuration['learning_rate']},\n            {'params': self.feature_extractor.parameters(), 'lr': self.configuration['learning_rate']}],\n        )\n\n    def get_model_likelihood_mll(\n        self,\n        train_size: int,\n    ) -> Tuple[GPRegressionModel, gpytorch.likelihoods.GaussianLikelihood, gpytorch.mlls.ExactMarginalLogLikelihood]:\n        \"\"\"\n        Called when the surrogate is first initialized or restarted.\n\n        Args:\n            train_size: The size of the current training set.\n\n        Returns:\n            model, likelihood, mll - The GP model, the likelihood and\n                the marginal likelihood.\n        \"\"\"\n        train_x = torch.ones(train_size, train_size).to(self.dev)\n        train_y = torch.ones(train_size).to(self.dev)\n\n        likelihood = gpytorch.likelihoods.GaussianLikelihood().to(self.dev)\n        model = GPRegressionModel(train_x=train_x, train_y=train_y, likelihood=likelihood).to(self.dev)\n        mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model).to(self.dev)\n\n        return model, likelihood, mll\n\n    def train_pipeline(self, data: Dict[str, torch.Tensor], load_checkpoint: bool = False):\n        \"\"\"\n        Train the surrogate model.\n\n        Args:\n            data: A dictionary which has the training examples, training features,\n                training budgets and in the end the training curves.\n            load_checkpoint: A flag whether to load the state from a previous checkpoint,\n                or whether to start from scratch.\n        \"\"\"\n        self.iterations += 1\n        self.logger.debug(f'Starting iteration: {self.iterations}')\n        # whether the state has been changed. Basically, if a better loss was found during\n        # this optimization iteration then the state (weights) were changed.\n        weights_changed = False\n\n        if load_checkpoint:\n            try:\n                self.load_checkpoint()\n            except FileNotFoundError:\n                self.logger.error(f'No checkpoint file found at: {self.checkpoint_file}'\n                                  f'Training the GP from the beginning')\n\n        self.model.train()\n        self.likelihood.train()\n        self.feature_extractor.train()\n\n        self.optimizer = torch.optim.Adam([\n            {'params': self.model.parameters(), 'lr': self.configuration['learning_rate']},\n            {'params': self.feature_extractor.parameters(), 'lr': self.configuration['learning_rate']}],\n        )\n\n        X_train = data['X_train']\n        train_budgets = data['train_budgets']\n        train_curves = data['train_curves']\n        y_train = data['y_train']\n\n        initial_state = self.get_state()\n        training_errored = False\n\n        if self.restart:\n            self.restart_optimization()\n            nr_epochs = self.nr_epochs\n            # 2 cases where the statement below is hit.\n            # - We are switching from the full training phase in the beginning to refining.\n            # - We are restarting because our refining diverged\n            if self.initial_nr_points <= self.iterations:\n                self.restart = False\n        else:\n            nr_epochs = self.refine_epochs\n\n        # where the mean squared error will be stored\n        # when predicting on the train set\n        mse = 0.0\n\n        for epoch_nr in range(0, nr_epochs):\n\n            nr_examples_batch = X_train.size(dim=0)\n            # if only one example in the batch, skip the batch.\n            # Otherwise, the code will fail because of batchnorm\n            if nr_examples_batch == 1:\n                continue\n\n            # Zero backprop gradients\n            self.optimizer.zero_grad()\n\n            projected_x = self.feature_extractor(X_train, train_budgets, train_curves)\n            self.model.set_train_data(projected_x, y_train, strict=False)\n            output = self.model(projected_x)\n\n            try:\n                # Calc loss and backprop derivatives\n                loss = -self.mll(output, self.model.train_targets)\n                loss_value = loss.detach().to('cpu').item()\n                mse = gpytorch.metrics.mean_squared_error(output, self.model.train_targets)\n                self.logger.debug(\n                    f'Epoch {epoch_nr} - MSE {mse:.5f}, '\n                    f'Loss: {loss_value:.3f}, '\n                    f'lengthscale: {self.model.covar_module.base_kernel.lengthscale.item():.3f}, '\n                    f'noise: {self.model.likelihood.noise.item():.3f}, '\n                )\n                loss.backward()\n                self.optimizer.step()\n            except Exception as training_error:\n                self.logger.error(f'The following error happened while training: {training_error}')\n                # An error has happened, trigger the restart of the optimization and restart\n                # the model with default hyperparameters.\n                self.restart = True\n                training_errored = True\n                break\n\n        \"\"\"\n        # metric too high, time to restart, or we risk divergence\n        if mse > 0.15:\n            if not self.restart:\n                self.restart = True\n        \"\"\"\n        if training_errored:\n            self.save_checkpoint(initial_state)\n            self.load_checkpoint()\n\n    def predict_pipeline(\n        self,\n        train_data: Dict[str, torch.Tensor],\n        test_data: Dict[str, torch.Tensor],\n    ) -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n\n        Args:\n            train_data: A dictionary that has the training\n                examples, features, budgets and learning curves.\n            test_data: Same as for the training data, but it is\n                for the testing part and it does not feature labels.\n\n        Returns:\n            means, stds: The means of the predictions for the\n                testing points and the standard deviations.\n        \"\"\"\n        self.model.eval()\n        self.feature_extractor.eval()\n        self.likelihood.eval()\n\n        with torch.no_grad(): # gpytorch.settings.fast_pred_var():\n            projected_train_x = self.feature_extractor(\n                train_data['X_train'],\n                train_data['train_budgets'],\n                train_data['train_curves'],\n            )\n            self.model.set_train_data(inputs=projected_train_x, targets=train_data['y_train'], strict=False)\n            projected_test_x = self.feature_extractor(\n                test_data['X_test'],\n                test_data['test_budgets'],\n                test_data['test_curves'],\n            )\n            preds = self.likelihood(self.model(projected_test_x))\n\n        means = preds.mean.detach().to('cpu').numpy().reshape(-1, )\n        stds = preds.stddev.detach().to('cpu').numpy().reshape(-1, )\n\n        return means, stds\n\n    def load_checkpoint(self):\n        \"\"\"\n        Load the state from a previous checkpoint.\n        \"\"\"\n        checkpoint = torch.load(self.checkpoint_file)\n        self.model.load_state_dict(checkpoint['gp_state_dict'])\n        self.feature_extractor.load_state_dict(checkpoint['feature_extractor_state_dict'])\n        self.likelihood.load_state_dict(checkpoint['likelihood_state_dict'])\n\n    def save_checkpoint(self, state: Dict =None):\n        \"\"\"\n        Save the given state or the current state in a\n        checkpoint file.\n\n        Args:\n            state: The state to save, if none, it will\n            save the current state.\n        \"\"\"\n\n        if state is None:\n            torch.save(\n                self.get_state(),\n                self.checkpoint_file,\n            )\n        else:\n            torch.save(\n                state,\n                self.checkpoint_file,\n            )\n\n    def get_state(self) -> Dict[str, Dict]:\n        \"\"\"\n        Get the current state of the surrogate.\n\n        Returns:\n            current_state: A dictionary that represents\n                the current state of the surrogate model.\n        \"\"\"\n        current_state = {\n            'gp_state_dict': deepcopy(self.model.state_dict()),\n            'feature_extractor_state_dict': deepcopy(self.feature_extractor.state_dict()),\n            'likelihood_state_dict': deepcopy(self.likelihood.state_dict()),\n        }\n\n        return current_state\n",
    "Experiment Result": "DyHPO is a Bayesian Optimization approach based on Gaussian Processes (GP) with a dynamic budget allocation strategy. The core method utilizes a deep kernel for the GP surrogate model. The deep kernel (implemented by `FeatureExtractor`) is a neural network composed of two linear layers (64 and 128 units respectively) and a 1D convolutional layer with 4 output channels and a kernel size of 3, followed by global max pooling (`AdaptiveMaxPool1d(1)`). It takes hyperparameter configurations, budgets, and past learning curves as inputs. The features extracted by this deep kernel are then fed into a squared exponential (RBF) kernel for the GP surrogate model. The parameters of both the deep kernel and the GP kernel are learned by maximizing the marginal likelihood using the Adam optimizer with a learning rate of 0.001. Training occurs for 1000 epochs during full retraining or 50 epochs during refinement, with a patience of 10 epochs for early stopping. The batch size for training the surrogate model is 64. The acquisition function used is a multi-fidelity version of Expected Improvement (EIMF). The budget allocation dynamically determines the next configuration and budget to evaluate, selecting a configuration to train for one additional budget step (controlled by `fantasize_step`, which defaults to 1). Initial exploration consists of 1 randomly chosen configuration evaluated for 1 budget step. Approximately 10% of configurations are taken randomly instead of being sampled from the model during the optimization process. The model restarts optimization if no improvement in the best observed value is seen for `max_benchmark_epochs + 0.2 * max_benchmark_epochs` steps. Hyperparameter candidates are preprocessed by applying a log transformation to specified hyperparameters, followed by `MinMaxScaler` for all. Learning curves are padded with zeros to ensure uniform length. Experiments are conducted on either the LCBench dataset (accuracy maximization) or the TaskSet dataset (loss minimization), both having a maximum budget (`max_benchmark_epochs`) of 51. The total budget for hyperparameter optimization is 1000 iterations (default). Random seeds for experiments are drawn from the range 0-9. The processing device used is CUDA if available, otherwise CPU."
}{
    "Title": "Supervising the Multi-Fidelity Race of Hyperparameter Configurations",
    "Main Contributions": "The paper introduces DyHPO, a Bayesian Optimization (BO) method for multi-fidelity hyperparameter optimization (HPO) that dynamically decides which hyperparameter configuration to train further. Its main contributions include a novel Bayesian surrogate model for gray-box HPO that predicts validation scores based on hyperparameter configuration, budget information, and the learning curve; a robust integration of this surrogate with Bayesian optimization; and demonstrating its superior efficiency and state-of-the-art results against seven strong HPO baselines across diverse Deep Learning architectures and 50 datasets.",
    "Methodology": "DyHPO is a Bayesian Optimization method based on Gaussian Processes (GP). It proposes a deep kernel for GPs that embeds the learning curve dynamics, capturing the similarity of hyperparameter configurations evaluated at different budgets. The feature extractor (φ) for the kernel uses a neural network composed of linear and convolutional layers, taking hyperparameter configuration, normalized budget, and the past learning curve as input. An acquisition function, Multi-Fidelity Expected Improvement (EIMF), is introduced, which extends Expected Improvement to the multi-budget case by dynamically adjusting the incumbent configuration based on the budget. The algorithm dynamically selects the most promising candidate to train for a small additional budget (incrementally increasing budget by one epoch/step) at each HPO step, updating the surrogate model after observing the new data point.",
    "Experimental Setup": "DyHPO was evaluated on three diverse deep learning architectures (MLP, CNN/NAS, RNN) and 50 datasets across three modalities (tabular, image, NLP). Benchmarks used include LCBench (35 tabular datasets, 2,000 neural networks, 50 epochs), TaskSet (12 NLP tasks, Adam8p search space, RNNs, scores every 200 iterations), and NAS-Bench-201 (15,625 architectures for CIFAR-10, CIFAR-100, ImageNet, 200 epochs). Experiments were run on Amazon EC2 M5 Instances (m5.xlarge), with results reported as the mean of ten repetitions using regret and average rank as metrics. Baselines included Random Search, HyperBand, BOHB, DEHB, ASHA, MF-DNN, and Dragonfly (BOCA). Preprocessing involved log-transformations for certain hyperparameters and Min-Max scaling for continuous ones, with one-hot encoding or specific numerical encoding for categorical hyperparameters depending on the baseline.",
    "Limitations": "The method's characterization as a 'step towards' scaling HPO for DL is cautious due to the lack of tabular benchmarks for very large deep learning models (e.g., Transformer-based architectures). The 'pause and resume' training procedure is only applicable to parametric models, requiring restarts for other types. For small datasets that train quickly, the overhead of model-based techniques like DyHPO makes simpler approaches like random search more appealing.",
    "Future Research Directions": "Implicitly, the paper suggests future work on adapting DyHPO for very large deep learning models (like Transformers), addressing the limitations of 'pause and resume' for non-parametric models, and potentially exploring ways to reduce overhead for fast-training tasks. The authors also invite the community to create sparse benchmarks with surrogates to save energy, implying research into more efficient benchmark design.",
    "Experiment Code": "import copy\nimport json\nimport logging\nimport math\nimport os\nimport time\nfrom typing import Dict, List, Optional, Tuple\n\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nfrom scipy.stats import norm, t\nimport torch\n\nfrom surrogate_models.dyhpo import DyHPO\n\n\nclass DyHPOAlgorithm:\n\n    def __init__(\n        self,\n        hp_candidates: np.ndarray,\n        log_indicator: List,\n        seed: int = 11,\n        max_benchmark_epochs: int = 52,\n        fantasize_step: int = 1,\n        minimization: bool = True,\n        total_budget: int = 500,\n        device: str = None,\n        dataset_name: str = 'unknown',\n        output_path: str = '.',\n        surrogate_config: dict = None,\n        verbose: bool = True,\n    ):\n        \"\"\"\n        Args:\n            hp_candidates: np.ndarray\n                The full list of hyperparameter candidates for\n                a given dataset.\n            log_indicator: List\n                A list with boolean values indicating if a\n                hyperparameter has been log sampled or not.\n            seed: int\n                The seed that will be used for the surrogate.\n            max_benchmark_epochs: int\n                The maximal budget that a hyperparameter configuration\n                has been evaluated in the benchmark for.\n            fantasize_step: int\n                The number of steps for which we are looking ahead to\n                evaluate the performance of a hpc.\n            minimization: bool\n                If the objective should be maximized or minimized.\n            total_budget: int\n                The total budget given for hyperparameter optimization.\n            device: str\n                The device where the experiment will be run on.\n            dataset_name: str\n                The name of the dataset that the experiment will be run on.\n            output_path: str\n                The path where all the output will be stored.\n            surrogate_config: dict\n                The model configurations for the surrogate.\n            verbose: boolean\n                If detailed information is preferred in the log file.\n        \"\"\"\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        torch.manual_seed(seed)\n        np.random.seed(seed)\n\n        if device is None:\n            self.dev = torch.device(\n                'cuda') if torch.cuda.is_available() else torch.device('cpu')\n        else:\n            self.dev = torch.device(device)\n\n        self.hp_candidates = hp_candidates\n        self.log_indicator = log_indicator\n\n        self.scaler = MinMaxScaler()\n        self.hp_candidates = self.preprocess_hp_candidates()\n\n        self.minimization = minimization\n        self.seed = seed\n\n        if verbose:\n            logging_level = logging.DEBUG\n        else:\n            logging_level = logging.INFO\n        self.logger = logging.getLogger()\n\n        logging.basicConfig(\n            format='%(levelname)s:%(asctime)s:%(message)s',\n            filename=f'dyhpo_surrogate_{dataset_name}_{seed}.log',\n            level=logging_level,\n        )\n\n        # the keys will be hyperparameter indices while the value\n        # will be a list with all the budgets evaluated for examples\n        # and with all performances for the performances\n        self.examples = dict()\n        self.performances = dict()\n\n        # set a seed already, so that it is deterministic when\n        # generating the seeds of the ensemble\n        torch.manual_seed(seed)\n        np.random.seed(seed)\n\n        self.max_benchmark_epochs = max_benchmark_epochs\n        self.total_budget = total_budget\n        self.fantasize_step = fantasize_step\n        self.nr_features = self.hp_candidates.shape[1]\n\n        initial_configurations_nr = 1\n        conf_individual_budget = 1\n        self.init_conf_indices = np.random.choice(self.hp_candidates.shape[0], initial_configurations_nr, replace=False)\n        self.init_budgets = [conf_individual_budget] * initial_configurations_nr\n        # with what percentage configurations will be taken randomly instead of being sampled from the model\n        self.fraction_random_configs = 0.1\n\n        self.model = None\n        # An index keeping track of where we are in the init_conf_indices\n        # list of hyperparmeters that are not sampled from the model.\n        self.initial_random_index = 0\n\n        if surrogate_config is None:\n            self.surrogate_config = {\n                'nr_layers': 2,\n                'nr_initial_features': self.nr_features,\n                'layer1_units': 64,\n                'layer2_units': 128,\n                'cnn_nr_channels': 4,\n                'cnn_kernel_size': 3,\n                'batch_size': 64,\n                'nr_epochs': 1000,\n                'nr_patience_epochs': 10,\n                'learning_rate': 0.001,\n            }\n        else:\n            self.surrogate_config = surrogate_config\n\n        # the incumbent value observed during the hpo process.\n        self.best_value_observed = np.NINF\n        # a set which will keep track of the hyperparameter configurations that diverge.\n        self.diverged_configs = set()\n\n        # info dict to drop every surrogate iteration\n        self.info_dict = dict()\n\n        # the start time for the overhead of every surrogate optimization iteration\n        # will be recorded here\n        self.suggest_time_duration = 0\n        # the total budget consumed so far\n        self.budget_spent = 0\n\n        self.output_path = output_path\n        self.dataset_name = dataset_name\n\n        self.no_improvement_threshold = int(self.max_benchmark_epochs + 0.2 * self.max_benchmark_epochs)\n        self.no_improvement_patience = 0\n\n\n    def _prepare_dataset_and_budgets(self) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Prepare the data that will be the input to the surrogate.\n\n        Returns:\n            data: A Dictionary that contains inside the training examples,\n            the budgets, the curves and lastly the labels.\n        \"\"\"\n\n        train_examples, train_labels, train_budgets, train_curves = self.history_configurations()\n\n        train_examples = np.array(train_examples, dtype=np.single)\n        train_labels = np.array(train_labels, dtype=np.single)\n        train_budgets = np.array(train_budgets, dtype=np.single)\n        train_curves = self.patch_curves_to_same_length(train_curves)\n        train_curves = np.array(train_curves, dtype=np.single)\n\n        # scale budgets to [0, 1]\n        train_budgets = train_budgets / self.max_benchmark_epochs\n\n        train_examples = torch.tensor(train_examples)\n        train_labels = torch.tensor(train_labels)\n        train_budgets = torch.tensor(train_budgets)\n        train_curves = torch.tensor(train_curves)\n\n        train_examples = train_examples.to(device=self.dev)\n        train_labels = train_labels.to(device=self.dev)\n        train_budgets = train_budgets.to(device=self.dev)\n        train_curves = train_curves.to(device=self.dev)\n\n        data = {\n            'X_train': train_examples,\n            'train_budgets': train_budgets,\n            'train_curves': train_curves,\n            'y_train': train_labels,\n        }\n\n        return data\n\n    def _train_surrogate(self):\n        \"\"\"\n        Train the surrogate model.\n        \"\"\"\n        data = self._prepare_dataset_and_budgets()\n        self.logger.info(f'Started training the model')\n\n        self.model.train_pipeline(\n            data,\n            load_checkpoint=False,\n        )\n\n    def _predict(self) -> Tuple[np.ndarray, np.ndarray, List, List]:\n        \"\"\"\n        Predict the performances of the hyperparameter configurations\n        as well as the standard deviations based on the surrogate model.\n\n        Returns:\n            mean_predictions, std_predictions, hp_indices, non_scaled_budgets:\n                The mean predictions and the standard deviations over\n                all model predictions for the given hyperparameter\n                configurations with their associated indices, scaled and\n                non-scaled budgets.\n        \"\"\"\n        configurations, hp_indices, budgets, learning_curves = self.generate_candidate_configurations()\n        budgets = np.array(budgets, dtype=np.single)\n        non_scaled_budgets = copy.deepcopy(budgets)\n        # scale budgets to [0, 1]\n        budgets = budgets / self.max_benchmark_epochs\n\n        configurations = np.array(configurations, dtype=np.single)\n        configurations = torch.tensor(configurations)\n        configurations = configurations.to(device=self.dev)\n\n        budgets = torch.tensor(budgets)\n        budgets = budgets.to(device=self.dev)\n\n        learning_curves = self.patch_curves_to_same_length(learning_curves)\n        learning_curves = np.array(learning_curves, dtype=np.single)\n        learning_curves = torch.tensor(learning_curves)\n        learning_curves = learning_curves.to(device=self.dev)\n\n        train_data = self._prepare_dataset_and_budgets()\n        test_data = {\n            'X_test': configurations,\n            'test_budgets': budgets,\n            'test_curves': learning_curves,\n        }\n\n        mean_predictions, std_predictions = self.model.predict_pipeline(train_data, test_data)\n\n        return mean_predictions, std_predictions, hp_indices, non_scaled_budgets\n\n    def suggest(self) -> Tuple[int, int]:\n        \"\"\"\n        Suggest a hyperparameter configuration to be evaluated next.\n\n        Returns:\n            best_config_index, budget: The index of the hyperparamter\n                configuration to be evaluated and the budget for\n                what it is going to be evaluated for.\n        \"\"\"\n        suggest_time_start = time.time()\n        # check if we still have random hyperparameters to evaluate\n        if self.initial_random_index < len(self.init_conf_indices):\n            self.logger.info(\n                'Not enough configurations to build a model. '\n                'Returning randomly sampled configuration'\n            )\n\n            random_indice = self.init_conf_indices[self.initial_random_index]\n            budget = self.init_budgets[self.initial_random_index]\n            self.initial_random_index += 1\n\n            return random_indice, budget\n        else:\n            mean_predictions, std_predictions, hp_indices, non_scaled_budgets = self._predict()\n            best_prediction_index = self.find_suggested_config(\n                mean_predictions,\n                std_predictions,\n                non_scaled_budgets,\n            )\n            \"\"\"\n            the best prediction index is not always matching with the actual hp index.\n            Since when evaluating the acq function, we do not consider hyperparameter\n            candidates that diverged or that are evaluated fully.\n            \"\"\"\n            best_config_index = hp_indices[best_prediction_index]\n\n            # decide for what budget we will evaluate the most\n            # promising hyperparameter configuration next.\n            if best_config_index in self.examples:\n                evaluated_budgets = self.examples[best_config_index]\n                max_budget = max(evaluated_budgets)\n                budget = max_budget + self.fantasize_step\n                # this would only trigger if fantasize_step is bigger\n                # than 1\n                if budget > self.max_benchmark_epochs:\n                    budget = self.max_benchmark_epochs\n            else:\n                budget = self.fantasize_step\n\n        suggest_time_end = time.time()\n        self.suggest_time_duration = suggest_time_end - suggest_time_start\n\n        self.budget_spent += self.fantasize_step\n\n        # exhausted hpo budget, finish.\n        if self.budget_spent > self.total_budget:\n            exit(0)\n\n        return best_config_index, budget\n\n    def observe(\n        self,\n        hp_index: int,\n        b: int,\n        learning_curve: np.ndarray,\n        alg_time: Optional[float] = None,\n    ):\n        \"\"\"\n        Args:\n            hp_index: The index of the evaluated hyperparameter configuration.\n            b: The budget for which the hyperparameter configuration was evaluated.\n            learning_curve: The learning curve of the hyperparameter configuration.\n            alg_time: The time taken from the algorithm to evaluate the hp configuration.\n        \"\"\"\n        score = learning_curve[-1]\n        # if y is an undefined value, append 0 as the overhead since we finish here.\n        if np.isnan(learning_curve).any():\n            self.update_info_dict(hp_index, b, np.nan, 0)\n            self.diverged_configs.add(hp_index)\n            return\n\n        observe_time_start = time.time()\n\n        self.examples[hp_index] = np.arange(1, b + 1).tolist()\n        self.performances[hp_index] = learning_curve\n\n        if self.best_value_observed < score:\n            self.best_value_observed = score\n            self.no_improvement_patience = 0\n        else:\n            self.no_improvement_patience += 1\n\n        observe_time_end = time.time()\n        train_time_duration = 0\n\n        # initialization phase over. Now we can sample from the model.\n        if self.initial_random_index >= len(self.init_conf_indices):\n            train_time_start = time.time()\n            # create the model for the first time\n            if self.model is None:\n                # Starting a model from scratch\n                self.model = DyHPO(\n                    self.surrogate_config,\n                    self.dev,\n                    self.dataset_name,\n                    self.output_path,\n                    self.seed,\n                )\n\n            if self.no_improvement_patience == self.no_improvement_threshold:\n                self.model.restart = True\n\n            self._train_surrogate()\n\n            train_time_end = time.time()\n            train_time_duration = train_time_end - train_time_start\n\n        observe_time_duration = observe_time_end - observe_time_start\n        total_duration = observe_time_duration + self.suggest_time_duration + train_time_duration\n        if alg_time is not None:\n            total_duration = total_duration + alg_time\n\n        self.update_info_dict(hp_index, b, score, total_duration)\n\n    def prepare_examples(self, hp_indices: List) -> List[np.ndarray]:\n        \"\"\"\n        Prepare the examples to be given to the surrogate model.\n\n        Args:\n            hp_indices: The list of hp indices that are already evaluated.\n\n        Returns:\n            examples: A list of the hyperparameter configurations.\n        \"\"\"\n        examples = []\n        for hp_index in hp_indices:\n            examples.append(self.hp_candidates[hp_index])\n\n        return examples\n\n    def generate_candidate_configurations(\n        self,\n    ) -> Tuple[List, List, List, List]:\n        \"\"\"\n        Generate candidate configurations that will be\n        fantasized upon.\n\n        Returns:\n            (configurations, hp_indices, hp_budgets, learning_curves): Tuple\n                A tuple of configurations, their indices in the hp list\n                and the budgets that they should be fantasized upon.\n        \"\"\"\n        hp_indices = []\n        hp_budgets = []\n        learning_curves = []\n\n        for hp_index in range(0, self.hp_candidates.shape[0]):\n\n            if hp_index in self.examples:\n                budgets = self.examples[hp_index]\n                # Take the max budget evaluated for a certain hpc\n                max_budget = max(budgets)\n                next_budget = max_budget + self.fantasize_step\n                # take the learning curve until the point we have evaluated so far\n                curve = self.performances[hp_index][:max_budget]\n                # if the curve is shorter than the length of the kernel size,\n                # pad it with zeros\n                difference_curve_length = self.surrogate_config['cnn_kernel_size'] - len(curve)\n                if difference_curve_length > 0:\n                    curve.extend([0.0] * difference_curve_length)\n            else:\n                # The hpc was not evaluated before, so fantasize its\n                # performance\n                next_budget = self.fantasize_step\n                curve = [0, 0, 0]\n\n            # this hyperparameter configuration is not evaluated fully\n            if next_budget <= self.max_benchmark_epochs:\n                hp_indices.append(hp_index)\n                hp_budgets.append(next_budget)\n                learning_curves.append(curve)\n\n        configurations = self.prepare_examples(hp_indices)\n\n        return configurations, hp_indices, hp_budgets, learning_curves\n\n    def history_configurations(\n        self,\n    ) -> Tuple[List, List, List, List]:\n        \"\"\"\n        Generate the configurations, labels, budgets and curves based on\n        the history of evaluated configurations.\n\n        Returns:\n            (train_examples, train_labels, train_budgets, train_curves):\n                A tuple of examples, labels, budgets and curves for the\n                configurations evaluated so far.\n        \"\"\"\n        train_examples = []\n        train_labels = []\n        train_budgets = []\n        train_curves = []\n\n        for hp_index in self.examples:\n            budgets = self.examples[hp_index]\n            performances = self.performances[hp_index]\n            example = self.hp_candidates[hp_index]\n\n            for budget, performance in zip(budgets, performances):\n                train_examples.append(example)\n                train_budgets.append(budget)\n                train_labels.append(performance)\n                train_curve = performances[:budget - 1] if budget > 1 else [0.0]\n                difference_curve_length = self.surrogate_config['cnn_kernel_size'] - len(train_curve)\n                if difference_curve_length > 0:\n                    train_curve.extend([0.0] * difference_curve_length)\n\n                train_curves.append(train_curve)\n\n        return train_examples, train_labels, train_budgets, train_curves\n\n    def acq(\n        self,\n        best_value: float,\n        mean: float,\n        std: float,\n        explore_factor: Optional[float] = 0.25,\n        acq_fc: str = 'ei',\n    ) -> float:\n        \"\"\"\n        The acquisition function that will be called\n        to evaluate the score of a hyperparameter configuration.\n\n        Parameters\n        ----------\n        best_value: float\n            Best observed function evaluation. Individual per fidelity.\n        mean: float\n            Point mean of the posterior process.\n        std: float\n            Point std of the posterior process.\n        explore_factor: float\n            The exploration factor for when ucb is used as the\n            acquisition function.\n        ei_calibration_factor: float\n            The factor used to calibrate expected improvement.\n        acq_fc: str\n            The type of acquisition function to use.\n\n        Returns\n        -------\n        acq_value: float\n            The value of the acquisition function.\n        \"\"\"\n        if acq_fc == 'ei':\n            if std == 0:\n                return 0\n            z = (mean - best_value) / std\n            acq_value = (mean - best_value) * norm.cdf(z) + std * norm.pdf(z)\n        elif acq_fc == 'ucb':\n            acq_value = mean + explore_factor * std\n        elif acq_fc == 'thompson':\n            acq_value = np.random.normal(mean, std)\n        elif acq_fc == 'exploit':\n            acq_value = mean\n        else:\n            raise NotImplementedError(\n                f'Acquisition function {acq_fc} has not been'\n                f'implemented',\n            )\n\n        return acq_value\n\n    def find_suggested_config(\n        self,\n        mean_predictions: np.ndarray,\n        mean_stds: np.ndarray,\n        budgets: List,\n    ) -> int:\n        \"\"\"\n        Find the hyperparameter configuration that has the highest score\n        with the acquisition function.\n\n        Args:\n            mean_predictions: The mean predictions of the posterior.\n            mean_stds: The mean standard deviations of the posterior.\n            budgets: The next budgets that the hyperparameter configurations\n                will be evaluated for.\n\n        Returns:\n            best_index: The index of the hyperparameter configuration with the\n                highest score.\n        \"\"\"\n        highest_acq_value = np.NINF\n        best_index = -1\n\n        index = 0\n        for mean_value, std in zip(mean_predictions, mean_stds):\n            budget = int(budgets[index])\n            best_value = self.calculate_fidelity_ymax(budget)\n            acq_value = self.acq(best_value, mean_value, std, acq_fc='ei')\n            if acq_value > highest_acq_value:\n                highest_acq_value = acq_value\n                best_index = index\n\n            index += 1\n\n        return best_index\n\n    def calculate_fidelity_ymax(self, fidelity: int):\n        \"\"\"\n        Find ymax for a given fidelity level.\n\n        If there are hyperparameters evaluated for that fidelity\n        take the maximum from their values. Otherwise, take\n        the maximum from all previous fidelity levels for the\n        hyperparameters that we have evaluated.\n\n        Args:\n            fidelity: The fidelity of the hyperparameter\n                configuration.\n\n        Returns:\n            best_value: The best value seen so far for the\n                given fidelity.\n        \"\"\"\n        exact_fidelity_config_values = []\n        lower_fidelity_config_values = []\n\n        for example_index in self.examples.keys():\n            try:\n                performance = self.performances[example_index][fidelity - 1]\n                exact_fidelity_config_values.append(performance)\n            except IndexError:\n                learning_curve = self.performances[example_index]\n                # The hyperparameter was not evaluated until fidelity, or more.\n                # Take the maximum value from the curve.\n                lower_fidelity_config_values.append(max(learning_curve))\n\n        if len(exact_fidelity_config_values) > 0:\n            # lowest error corresponds to best value\n            best_value = max(exact_fidelity_config_values)\n        else:\n            best_value = max(lower_fidelity_config_values)\n\n        return best_value\n\n    def update_info_dict(\n        self,\n        hp_index: int,\n        budget: int,\n        performance: float,\n        overhead: float,\n    ):\n        \"\"\"\n        Update the info dict with the current HPO iteration info.\n\n        Dump a new json file that will update with additional information\n        given the current HPO iteration.\n\n        Args:\n            hp_index: The index of the hyperparameter configuration.\n            budget: The budget of the hyperparameter configuration.\n            performance:  The performance of the hyperparameter configuration.\n            overhead: The total overhead (in seconds) of the iteration.\n        \"\"\"\n        hp_index = int(hp_index)\n        if 'hp' in self.info_dict:\n            self.info_dict['hp'].append(hp_index)\n        else:\n            self.info_dict['hp'] = [hp_index]\n\n        if 'scores' in self.info_dict:\n            self.info_dict['scores'].append(performance)\n        else:\n            self.info_dict['scores'] = [performance]\n\n        if 'curve' in self.info_dict:\n            self.info_dict['curve'].append(self.best_value_observed)\n        else:\n            self.info_dict['curve'] = [self.best_value_observed]\n\n        if 'epochs' in self.info_dict:\n            self.info_dict['epochs'].append(budget)\n        else:\n            self.info_dict['epochs'] = [budget]\n\n        if 'overhead' in self.info_dict:\n            self.info_dict['overhead'].append(overhead)\n        else:\n            self.info_dict['overhead'] = [overhead]\n\n        with open(os.path.join(self.output_path, f'{self.dataset_name}_{self.seed}.json'), 'w') as fp:\n            json.dump(self.info_dict, fp)\n\n    def preprocess_hp_candidates(self) -> List:\n        \"\"\"\n        Preprocess the list of all hyperparameter candidates\n        by  performing a log transform for the hyperparameters that\n        were log sampled.\n\n        Returns:\n            log_hp_candidates: The list of all hyperparameter configurations\n                where hyperparameters that were log sampled are log transformed.\n        \"\"\"\n        log_hp_candidates = []\n\n        for hp_candidate in self.hp_candidates:\n            new_hp_candidate = []\n            for index, hp_value in enumerate(hp_candidate):\n                new_hp_candidate.append(math.log(hp_value) if self.log_indicator[index] else hp_value)\n\n            log_hp_candidates = np.array(log_hp_candidates)\n            # scaler for the hp configurations\n\n            log_hp_candidates = self.scaler.fit_transform(log_hp_candidates)\n\n        return log_hp_candidates\n\n    @staticmethod\n    def patch_curves_to_same_length(curves):\n        \"\"\"\n        Patch the given curves to the same length.\n\n        Finds the maximum curve length and patches all\n        other curves that are shorter in length with zeroes.\n\n        Args:\n            curves: The given hyperparameter curves.\n\n        Returns:\n            curves: The updated array where the learning\n                curves are of the same length.\n        \"\"\"\n        max_curve_length = 0\n        for curve in curves:\n            if len(curve) > max_curve_length:\n                max_curve_length = len(curve)\n\n        for curve in curves:\n            difference = max_curve_length - len(curve)\n            if difference > 0:\n                curve.extend([0.0] * difference)\n\n        return curves\n\nfrom copy import deepcopy\nimport logging\nimport os\nfrom typing import Dict, Tuple\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch import cat\n\nimport gpytorch\n\n\nclass FeatureExtractor(nn.Module):\n    \"\"\"\n    The feature extractor that is part of the deep kernel.\n    \"\"\"\n    def __init__(self, configuration):\n        super(FeatureExtractor, self).__init__()\n\n        self.configuration = configuration\n\n        self.nr_layers = configuration['nr_layers']\n        self.act_func = nn.LeakyReLU()\n        # adding one to the dimensionality of the initial input features\n        # for the concatenation with the budget.\n        initial_features = configuration['nr_initial_features'] + 1\n        self.fc1 = nn.Linear(initial_features, configuration['layer1_units'])\n        self.bn1 = nn.BatchNorm1d(configuration['layer1_units'])\n        for i in range(2, self.nr_layers):\n            setattr(\n                self,\n                f'fc{i + 1}',\n                nn.Linear(configuration[f'layer{i - 1}_units'], configuration[f'layer{i}_units']),\n            )\n            setattr(\n                self,\n                f'bn{i + 1}',\n                nn.BatchNorm1d(configuration[f'layer{i}_units']),\n            )\n\n\n        setattr(\n            self,\n            f'fc{self.nr_layers}',\n            nn.Linear(\n                configuration[f'layer{self.nr_layers - 1}_units'] +\n                configuration['cnn_nr_channels'],  # accounting for the learning curve features\n                configuration[f'layer{self.nr_layers}_units']\n            ),\n        )\n        self.cnn = nn.Sequential(\n            nn.Conv1d(in_channels=1, kernel_size=(configuration['cnn_kernel_size'],), out_channels=4),\n            nn.AdaptiveMaxPool1d(1),\n        )\n\n    def forward(self, x, budgets, learning_curves):\n\n        # add an extra dimensionality for the budget\n        # making it nr_rows x 1.\n        budgets = torch.unsqueeze(budgets, dim=1)\n        # concatenate budgets with examples\n        x = cat((x, budgets), dim=1)\n        x = self.fc1(x)\n        x = self.act_func(self.bn1(x))\n\n        for i in range(2, self.nr_layers):\n            x = self.act_func(\n                getattr(self, f'bn{i}')(\n                    getattr(self, f'fc{i}')(\n                        x\n                    )\n                )\n            )\n\n        # add an extra dimensionality for the learning curve\n        # making it nr_rows x 1 x lc_values.\n        learning_curves = torch.unsqueeze(learning_curves, 1)\n        lc_features = self.cnn(learning_curves)\n        # revert the output from the cnn into nr_rows x nr_kernels.\n        lc_features = torch.squeeze(lc_features, 2)\n\n        # put learning curve features into the last layer along with the higher level features.\n        x = cat((x, lc_features), dim=1)\n        x = self.act_func(getattr(self, f'fc{self.nr_layers}')(x))\n\n        return x\n\n\nclass GPRegressionModel(gpytorch.models.ExactGP):\n    \"\"\"\n    A simple GP model.\n    \"\"\"\n    def __init__(\n        self,\n        train_x: torch.Tensor,\n        train_y: torch.Tensor,\n        likelihood: gpytorch.likelihoods.GaussianLikelihood,\n    ):\n        \"\"\"\n        Constructor of the GPRegressionModel.\n\n        Args:\n            train_x: The initial train examples for the GP.\n            train_y: The initial train labels for the GP.\n            likelihood: The likelihood to be used.\n        \"\"\"\n        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n\n        self.mean_module = gpytorch.means.ConstantMean()\n        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n\n    def forward(self, x):\n\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n\n        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n\n\nclass DyHPO:\n    \"\"\"\n    The DyHPO DeepGP model.\n    \"\"\"\n    def __init__(\n        self,\n        configuration: Dict,\n        device: torch.device,\n        dataset_name: str = 'unknown',\n        output_path: str = '.',\n        seed: int = 11,\n    ):\n        \"\"\"\n        The constructor for the DyHPO model.\n\n        Args:\n            configuration: The configuration to be used\n                for the different parts of the surrogate.\n            device: The device where the experiments will be run on.\n            dataset_name: The name of the dataset for the current run.\n            output_path: The path where the intermediate/final results\n                will be stored.\n            seed: The seed that will be used to store the checkpoint\n                properly.\n        \"\"\"\n        super(DyHPO, self).__init__()\n        self.feature_extractor = FeatureExtractor(configuration)\n        self.batch_size = configuration['batch_size']\n        self.nr_epochs = configuration['nr_epochs']\n        self.early_stopping_patience = configuration['nr_patience_epochs']\n        self.refine_epochs = 50\n        self.dev = device\n        self.seed = seed\n        self.model, self.likelihood, self.mll = \\\n            self.get_model_likelihood_mll(\n                configuration[f'layer{self.feature_extractor.nr_layers}_units']\n            )\n\n        self.model.to(self.dev)\n        self.likelihood.to(self.dev)\n        self.feature_extractor.to(self.dev)\n\n        self.optimizer = torch.optim.Adam([\n            {'params': self.model.parameters(), 'lr': configuration['learning_rate']},\n            {'params': self.feature_extractor.parameters(), 'lr': configuration['learning_rate']}],\n        )\n\n        self.configuration = configuration\n        # the number of initial points for which we will retrain fully from scratch\n        # This is basically equal to the dimensionality of the search space + 1.\n        self.initial_nr_points = 10\n        # keeping track of the total hpo iterations. It will be used during the optimization\n        # process to switch from fully training the model, to refining.\n        self.iterations = 0\n        # flag for when the optimization of the model should start from scratch.\n        self.restart = True\n\n        self.logger = logging.getLogger(__name__)\n\n        self.checkpoint_path = os.path.join(\n            output_path,\n            'checkpoints',\n            f'{dataset_name}',\n            f'{self.seed}',\n        )\n\n        os.makedirs(self.checkpoint_path, exist_ok=True)\n\n        self.checkpoint_file = os.path.join(\n            self.checkpoint_path,\n            'checkpoint.pth'\n        )\n\n    def restart_optimization(self):\n        \"\"\"\n        Restart the surrogate model from scratch.\n        \"\"\"\n        self.feature_extractor = FeatureExtractor(self.configuration).to(self.dev)\n        self.model, self.likelihood, self.mll = \\\n            self.get_model_likelihood_mll(\n                self.configuration[f'layer{self.feature_extractor.nr_layers}_units'],\n            )\n\n        self.optimizer = torch.optim.Adam([\n            {'params': self.model.parameters(), 'lr': self.configuration['learning_rate']},\n            {'params': self.feature_extractor.parameters(), 'lr': self.configuration['learning_rate']}],\n        )\n\n    def get_model_likelihood_mll(\n        self,\n        train_size: int,\n    ) -> Tuple[GPRegressionModel, gpytorch.likelihoods.GaussianLikelihood, gpytorch.mlls.ExactMarginalLogLikelihood]:\n        \"\"\"\n        Called when the surrogate is first initialized or restarted.\n\n        Args:\n            train_size: The size of the current training set.\n\n        Returns:\n            model, likelihood, mll - The GP model, the likelihood and\n                the marginal likelihood.\n        \"\"\"\n        train_x = torch.ones(train_size, train_size).to(self.dev)\n        train_y = torch.ones(train_size).to(self.dev)\n\n        likelihood = gpytorch.likelihoods.GaussianLikelihood().to(self.dev)\n        model = GPRegressionModel(train_x=train_x, train_y=train_y, likelihood=likelihood).to(self.dev)\n        mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model).to(self.dev)\n\n        return model, likelihood, mll\n\n    def train_pipeline(self, data: Dict[str, torch.Tensor], load_checkpoint: bool = False):\n        \"\"\"\n        Train the surrogate model.\n\n        Args:\n            data: A dictionary which has the training examples, training features,\n                training budgets and in the end the training curves.\n            load_checkpoint: A flag whether to load the state from a previous checkpoint,\n                or whether to start from scratch.\n        \"\"\"\n        self.iterations += 1\n        self.logger.debug(f'Starting iteration: {self.iterations}')\n        # whether the state has been changed. Basically, if a better loss was found during\n        # this optimization iteration then the state (weights) were changed.\n        weights_changed = False\n\n        if load_checkpoint:\n            try:\n                self.load_checkpoint()\n            except FileNotFoundError:\n                self.logger.error(f'No checkpoint file found at: {self.checkpoint_file}'\n                                  f'Training the GP from the beginning')\n\n        self.model.train()\n        self.likelihood.train()\n        self.feature_extractor.train()\n\n        self.optimizer = torch.optim.Adam([\n            {'params': self.model.parameters(), 'lr': self.configuration['learning_rate']},\n            {'params': self.feature_extractor.parameters(), 'lr': self.configuration['learning_rate']}],\n        )\n\n        X_train = data['X_train']\n        train_budgets = data['train_budgets']\n        train_curves = data['train_curves']\n        y_train = data['y_train']\n\n        initial_state = self.get_state()\n        training_errored = False\n\n        if self.restart:\n            self.restart_optimization()\n            nr_epochs = self.nr_epochs\n            # 2 cases where the statement below is hit.\n            # - We are switching from the full training phase in the beginning to refining.\n            # - We are restarting because our refining diverged\n            if self.initial_nr_points <= self.iterations:\n                self.restart = False\n        else:\n            nr_epochs = self.refine_epochs\n\n        # where the mean squared error will be stored\n        # when predicting on the train set\n        mse = 0.0\n\n        for epoch_nr in range(0, nr_epochs):\n\n            nr_examples_batch = X_train.size(dim=0)\n            # if only one example in the batch, skip the batch.\n            # Otherwise, the code will fail because of batchnorm\n            if nr_examples_batch == 1:\n                continue\n\n            # Zero backprop gradients\n            self.optimizer.zero_grad()\n\n            projected_x = self.feature_extractor(X_train, train_budgets, train_curves)\n            self.model.set_train_data(projected_x, y_train, strict=False)\n            output = self.model(projected_x)\n\n            try:\n                # Calc loss and backprop derivatives\n                loss = -self.mll(output, self.model.train_targets)\n                loss_value = loss.detach().to('cpu').item()\n                mse = gpytorch.metrics.mean_squared_error(output, self.model.train_targets)\n                self.logger.debug(\n                    f'Epoch {epoch_nr} - MSE {mse:.5f}, '\n                    f'Loss: {loss_value:.3f}, '\n                    f'lengthscale: {self.model.covar_module.base_kernel.lengthscale.item():.3f}, '\n                    f'noise: {self.model.likelihood.noise.item():.3f}, '\n                )\n                loss.backward()\n                self.optimizer.step()\n            except Exception as training_error:\n                self.logger.error(f'The following error happened while training: {training_error}')\n                # An error has happened, trigger the restart of the optimization and restart\n                # the model with default hyperparameters.\n                self.restart = True\n                training_errored = True\n                break\n\n        \"\"\"\n        # metric too high, time to restart, or we risk divergence\n        if mse > 0.15:\n            if not self.restart:\n                self.restart = True\n        \"\"\"\n        if training_errored:\n            self.save_checkpoint(initial_state)\n            self.load_checkpoint()\n\n    def predict_pipeline(\n        self,\n        train_data: Dict[str, torch.Tensor],\n        test_data: Dict[str, torch.Tensor],\n    ) -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n\n        Args:\n            train_data: A dictionary that has the training\n                examples, features, budgets and learning curves.\n            test_data: Same as for the training data, but it is\n                for the testing part and it does not feature labels.\n\n        Returns:\n            means, stds: The means of the predictions for the\n                testing points and the standard deviations.\n        \"\"\"\n        self.model.eval()\n        self.feature_extractor.eval()\n        self.likelihood.eval()\n\n        with torch.no_grad(): # gpytorch.settings.fast_pred_var():\n            projected_train_x = self.feature_extractor(\n                train_data['X_train'],\n                train_data['train_budgets'],\n                train_data['train_curves'],\n            )\n            self.model.set_train_data(inputs=projected_train_x, targets=train_data['y_train'], strict=False)\n            projected_test_x = self.feature_extractor(\n                test_data['X_test'],\n                test_data['test_budgets'],\n                test_data['test_curves'],\n            )\n            preds = self.likelihood(self.model(projected_test_x))\n\n        means = preds.mean.detach().to('cpu').numpy().reshape(-1, )\n        stds = preds.stddev.detach().to('cpu').numpy().reshape(-1, )\n\n        return means, stds\n\n    def load_checkpoint(self):\n        \"\"\"\n        Load the state from a previous checkpoint.\n        \"\"\"\n        checkpoint = torch.load(self.checkpoint_file)\n        self.model.load_state_dict(checkpoint['gp_state_dict'])\n        self.feature_extractor.load_state_dict(checkpoint['feature_extractor_state_dict'])\n        self.likelihood.load_state_dict(checkpoint['likelihood_state_dict'])\n\n    def save_checkpoint(self, state: Dict =None):\n        \"\"\"\n        Save the given state or the current state in a\n        checkpoint file.\n\n        Args:\n            state: The state to save, if none, it will\n            save the current state.\n        \"\"\"\n\n        if state is None:\n            torch.save(\n                self.get_state(),\n                self.checkpoint_file,\n            )\n        else:\n            torch.save(\n                state,\n                self.checkpoint_file,\n            )\n\n    def get_state(self) -> Dict[str, Dict]:\n        \"\"\"\n        Get the current state of the surrogate.\n\n        Returns:\n            current_state: A dictionary that represents\n                the current state of the surrogate model.\n        \"\"\"\n        current_state = {\n            'gp_state_dict': deepcopy(self.model.state_dict()),\n            'feature_extractor_state_dict': deepcopy(self.feature_extractor.state_dict()),\n            'likelihood_state_dict': deepcopy(self.likelihood.state_dict()),\n        }\n\n        return current_state",
    "Experiment Result": "HPO Algorithm: DyHPO\nBenchmarks: LCBench, TaskSet\nOptimization goal: Minimization=False (Maximization) for LCBench (accuracy), Minimization=True for TaskSet (loss).\nTotal HPO budget (budget_limit): 1000.\nMaximum budget (epochs/steps) for a single configuration in benchmark (max_benchmark_epochs): 51 for LCBench and TaskSet.\nFantasize step (incremental budget increase): 1 epoch/step.\nRandom seeds: 10 seeds (0-9) are used for experimental runs.\n\nDyHPOAlgorithm settings:\n- Initial configurations (initial_configurations_nr): 1.\n- Budget for initial configurations (conf_individual_budget): 1 epoch/step.\n- Fraction of random configurations: 0.1.\n- No improvement patience threshold (for model restart): `int(max_benchmark_epochs + 0.2 * max_benchmark_epochs)` (e.g., for LCBench, 51 + 0.2 * 51 = 61).\n- Hyperparameter preprocessing: Log transform for log-sampled hyperparameters (indicated by `log_indicator`), followed by `MinMaxScaler`.\n- Learning curve preprocessing: Curves are padded with zeros to the `cnn_kernel_size` length for training input, and to the max observed length for `patch_curves_to_same_length`.\n- Acquisition function: Expected Improvement (`ei`).\n\nDyHPO (Deep Gaussian Process Surrogate) settings:\n- Feature Extractor (`FeatureExtractor`) neural network architecture:\n    - Number of dense layers (`nr_layers`): 2.\n    - `layer1_units`: 64.\n    - `layer2_units`: 128.\n    - CNN layers: `cnn_nr_channels=4`, `cnn_kernel_size=3`.\n    - Input: Hyperparameter configuration + normalized budget (concatenated, initial_features + 1), then concatenated with CNN features from the past learning curve.\n- GP model: `GPRegressionModel` using `gpytorch.means.ConstantMean` and `gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())`.\n- Training settings:\n    - `batch_size`: 64.\n    - `nr_epochs`: 1000 (for full training/from scratch).\n    - `nr_patience_epochs`: 10 (early stopping patience, though not explicitly used for stopping in `train_pipeline` but defined).\n    - `learning_rate`: 0.001.\n    - `refine_epochs`: 50 (for refinement training after initial phase or restart).\n    - `initial_nr_points`: 10 (number of HPO iterations before switching from full training to refining, or for full restart).\n    - Optimizer: Adam."
}{
    "Title": "Bayesian Optimization for Iterative Learning",
    "Main Contributions": "The paper introduces a Bayesian Optimization (BO) approach, named BOIL, for efficient hyperparameter tuning in iterative learning systems like Deep Learning (DL) and Deep Reinforcement Learning (DRL). It addresses the problem of expensive hyperparameter tuning that traditionally ignores intermediate training information. Key contributions include an algorithm to optimize the learning curve by compressing training progress into a single numeric score (considering stability and success), an approach to learn this compression curve from data, and a data augmentation technique for improved sample-efficiency. BOIL balances assessment benefit against computation cost and selectively includes scores from different training steps. It is demonstrated to outperform existing baselines in identifying optimal hyperparameters in minimal wall-clock time across DRL agents and convolutional neural networks.",
    "Methodology": "The BOIL framework models the cost-sensitive black-box function `f(x,t)` (hyperparameter `x`, iterations `t`) as a Gaussian Process (GP) using a product kernel `k(x,x') × k(t,t')`. The cost function `c(x,t)` is approximated by a linear regressor. The next hyperparameter-iteration point is selected by maximizing a cost-aware acquisition function `α(x,t)/µc(x,t)`. A novel 'training curve compression' method transforms the entire learning curve into a single numeric score `y` using a parameterized Sigmoid preference function `l(u|m0,g0)`, where parameters `m0` and `g0` are learned by maximizing the GP log marginal likelihood. To enhance sample efficiency and prevent GP covariance matrix ill-conditioning, a data augmentation technique is used, selectively sampling intermediate points at maximum GP predictive uncertainty, ensuring the natural log of the covariance matrix condition number remains below a threshold (δ = 20).",
    "Experimental Setup": "Experiments were conducted on two DRL agents (Dueling DQN, Advantage Actor Critic) across three OpenAI gym/Mujoco environments (CartPole-v0, InvertedPendulum-v2, Reacher-v2) and a Convolutional Neural Network on two datasets (SVHN, CIFAR10). All results were averaged over 20 independent runs using NVIDIA 1080 GTX GPUs with TensorFlow-GPU. The GP models utilized square-exponential kernels whose parameters were optimized via marginal likelihood maximization. The data augmentation strategy set a maximum of 15 augmented points and a condition number threshold of 20. Baselines included Hyperband and Continuous Multi-task/Multi-fidelity BO (CM-T/F-BO), which extends discrete multi-task BO to continuous settings. Ablation studies with vanilla BO and BO-L (BO with curve compression) were also performed, and a comparison using a Freeze-Thaw-like exponential decay kernel was included.",
    "Limitations": "The unpredictability and significant fluctuations of DRL reward curves make traditional stopping criteria, such as the exponential decay assumed in methods like Freeze-thaw BO, unsuitable. A naive approach to data augmentation by adding an entire curve of points can lead to redundancy and serious ill-conditioning issues for the GP covariance matrix, particularly in noisy DRL environments. The approximation of the cost function with a linear regressor might be insufficient if the cost dependence on hyperparameters and iterations is more complex, requiring alternative models like a second GP.",
    "Future Research Directions": "The proposed framework's applicability extends beyond machine learning, suggesting its use in any iterative process dependent on a set of parameters, such as optimizing manufacturing pipelines to increase productivity. Broader implications include increasing the training efficiency of ML models to reduce computational and environmental costs, facilitating their widespread deployment. However, it also highlights the need for practitioners to implement robust supervision and override mechanisms for deployed models, and to critically reflect on potential biases in datasets and models used in automated pipelines. Future work should continue to address the growing opacity of ML models through rigorous analysis of training outcomes, potentially using rapidly developing interpretability techniques.",
    "Experiment Code": "from bayes_opt.sequentialBO.boil import BOIL\nfrom bayes_opt.product_gaussian_process import ProductGaussianProcess\nfrom bayes_opt.curve_compression import apply_one_transform_logistic, transform_logistic\nfrom bayes_opt.acquisition_functions import AcquisitionFunction\nfrom bayes_opt.acquisition_maximization import acq_max_with_name, acq_min_scipy_kwargs\nfrom sklearn import linear_model\n\n# --- bayes_opt/sequentialBO/boil.py ---\n\nclass BOIL(object):\n\n    def __init__(self, func, SearchSpace,acq_name=\"ei_mu_max\",verbose=1):\n        self.method='boil'\n        self.verbose=verbose\n        if isinstance(SearchSpace,dict):\n            self.keys = list(SearchSpace.keys())\n            self.SearchSpace = []\n            for key in list(SearchSpace.keys()):\n                self.SearchSpace.append(SearchSpace[key])\n            self.SearchSpace = np.asarray(self.SearchSpace)\n        else:\n            self.SearchSpace=np.asarray(SearchSpace)\n            \n        self.dim = len(SearchSpace)\n\n        scaler = MinMaxScaler()\n        scaler.fit(self.SearchSpace[:-1,:].T)\n        \n        scalerT = MinMaxScaler()\n        SearchSpace_T=np.atleast_2d(self.SearchSpace[-1,:]).T\n        scalerT.fit(SearchSpace_T)\n\n        self.Xscaler=scaler\n        self.Tscaler=scalerT\n\n        self.scaleSearchSpace=np.array([np.zeros(self.dim), np.ones(self.dim)]).T\n                \n        self.f = func\n    \n        self.X_ori= None\n        self.X = None\n        self.Y = None\n        self.Y_ori = None\n        self.T=None\n        self.T_original=None\n        self.Y_cost_original=None\n        self.time_opt=0\n         \n        self.max_min_gap=self.SearchSpace[:,1]-self.SearchSpace[:,0]\n\n        self.acq_name = acq_name\n        self.logmarginal=0\n\n        self.gp=ProductGaussianProcess(self.scaleSearchSpace,verbose=verbose)\n\n        self.Y_curves=[]\n        self.Y_cost_original=None\n        self.time_opt=0\n        self.acq_func = None\n        self.logmarginal=0\n        self.markVirtualObs=[]\n        self.countVirtual=[]\n        self.linear_regression = linear_model.LinearRegression()\n        self.condition_number=[]\n        self.max_n_augmentation=10\n        self.threshold_cond=15\n        \n    def init(self, n_init_points=3, seed=1):\n        np.random.seed(seed)\n\n        SearchSpace=np.copy(self.SearchSpace)\n        SearchSpace[-1,0]=SearchSpace[-1,1]\n\n        l = [np.random.uniform(x[0], x[1]) for _ in range(n_init_points) for x in SearchSpace] \n\n        temp=np.asarray(l)\n        temp=temp.T\n        init_X=list(temp.reshape((n_init_points,-1)))\n        \n        self.X_original = np.asarray(init_X)\n        self.T_original=self.X_original[:,-1]\n        self.T_original=np.reshape(self.T_original,(n_init_points,-1))\n        \n        self.X_original=self.X_original[:,:-1]\n        self.X_original=np.reshape(self.X_original,(n_init_points,-1))\n\n        y_init_curves, y_init_cost=self.f(init_X)\n\n        y_init_cost=np.atleast_2d(np.asarray(y_init_cost))\n\n        self.Y_curves+=y_init_curves\n\n        y_init=transform_logistic(y_init_curves,self.gp.logistic_hyper['midpoint'],\\\n                                  self.gp.logistic_hyper['growth'], self.SearchSpace[-1,1])\n        y_init=np.reshape(y_init,(n_init_points,1))\n        \n        self.Y_original = np.asarray(y_init)      \n        self.Y_cost_original=np.reshape(y_init_cost,(-1,1))\n\n        self.X = self.Xscaler.transform(np.asarray(init_X)[:,:-1])\n        self.X=np.reshape(self.X,(n_init_points,-1))\n\n        self.T = self.Tscaler.transform(self.T_original)\n\n        self.markVirtualObs+=[0]*n_init_points\n\n        for ii in range(n_init_points):\n            self.generating_virtual_observations(self.X[ii,:],\n                         self.T[ii],[y_init_curves[ii]],y_init_cost[0][ii],IsRandom=False)\n\n        self.Y_cost=(self.Y_cost_original-np.min(self.Y_cost_original))/(np.max(self.Y_cost_original)-np.min(self.Y_cost_original))\n\n        if np.std(self.Y_original)==0:\n            self.Y=(self.Y_original-np.mean(self.Y_original))\n        else:\n            self.Y=(self.Y_original-np.mean(self.Y_original))/np.std(self.Y_original)\n\n       \n    def utility_cost_evaluation(self,x,acq_func,isDebug=False):\n        def utility_cost_evaluation_single(x,acq_func,isDebug=False):\n            utility=acq_func.acq_kind(x,gp=self.gp)\n            \n            try:\n                mean_cost=self.linear_regression.predict(np.reshape(x,(1,-1)))\n                \n            except:\n                print(x)\n                print(\"bug\")\n    \n            mean_cost=max(0,mean_cost)+0.1\n            \n            if 'ei' in acq_func.acq_name:\n                acquisition_function_value= np.log(utility)-np.log(mean_cost)\n            else:\n                acquisition_function_value= np.log(1+np.exp(utility))/np.log(1+np.exp(mean_cost))\n    \n            if isDebug==True:\n                print(\"acq_func at the selected point \\t utility:\",np.round(utility,decimals=4),\"\\t cost:\",mean_cost)\n                if utility==0:\n                    print(\"utility =0===============================================================================\")\n       \n            return acquisition_function_value*(-1)\n        \n        \n        if len(x)==self.dim:\n            temp=utility_cost_evaluation_single(x,acq_func,isDebug=isDebug)\n            if isDebug==True:\n                return temp\n            else:\n                utility=np.mean(temp)\n        \n        else:\n            utility=[0]*len(x)\n            for idx,val in enumerate(x):\n                temp=utility_cost_evaluation_single(x=val,acq_func=acq_func,isDebug=isDebug)\n                                                     \n                utility[idx]=np.mean(temp)\n                \n            utility=np.asarray(utility)    \n        return utility   \n    \n        \n    def acq_utility_cost(self):\n        acq={}\n        acq['name']=self.acq_name\n        acq['dim']=self.scaleSearchSpace.shape[0]\n        acq['scaleSearchSpace']=self.scaleSearchSpace   \n    \n        if self.acq_name=='ei_mu_max':\n            x_mu_max,mu_max_val=acq_max_with_name(gp=self.gp,scaleSearchSpace=self.scaleSearchSpace,acq_name='mu',IsReturnY=True)\n            acq['mu_max']=  mu_max_val\n\n        myacq=AcquisitionFunction(acq)\n        \n        x_min = acq_min_scipy_kwargs(myfunc=self.utility_cost_evaluation,SearchSpace=self.scaleSearchSpace,\n                        acq_func=myacq, isDebug=False)\n        \n        if self.verbose==True:\n            acq_val=self.utility_cost_evaluation(x_min,myacq,isDebug=False)\n            print(\"selected point from acq func:\",np.round(x_min,decimals=4),\"acq val=log(Utility/Cost)=\",(-1)*np.round(acq_val,decimals=4))\n            if np.round(acq_val,decimals=4)==0:\n                print(\"acq value =0\")\n            \n        return x_min\n    \n    \n    def select_informative_location_by_uncertainty(self,n_virtual_obs,x_max,t_max):\n        SearchSpace=np.copy(self.scaleSearchSpace)\n        for dd in range(self.dim-1):\n            SearchSpace[dd,0],SearchSpace[dd,1]=x_max[dd],x_max[dd]\n            \n        SearchSpace[-1,1]=t_max\n        \n        temp_X,temp_T=self.X.copy(),self.T.copy()\n        temp_gp=copy.deepcopy(self.gp )\n        \n        temp_Y=np.random.random(size=(len(temp_T),1))\n        \n        temp_gp.fit(temp_X,temp_T,temp_Y,self.Y_curves)\n        \n        new_batch_T=None\n\n        pred_var_value=[0]*n_virtual_obs\n        for ii in range(n_virtual_obs):\n            x_max_pred_variance, pred_var_value[ii]=acq_max_with_name(gp=temp_gp,\n                              scaleSearchSpace=SearchSpace,acq_name='pure_exploration',IsReturnY=True)\n            \n            log_cond=np.log( temp_gp.compute_condition_number() )\n            if log_cond>self.threshold_cond or pred_var_value[ii]<(self.gp.noise_delta+1e-3):\n                break\n          \n            if x_max_pred_variance[-1] in temp_T[-ii:]: # if repetition, stop augmenting\n                break\n            \n            temp_X = np.vstack((temp_X, x_max.reshape((1, -1))))\n            temp_T = np.vstack((temp_T, x_max_pred_variance[-1].reshape((1, -1))))\n            temp_gp.X,temp_gp.T=temp_X,temp_T\n            temp_Y=np.random.random(size=(len(temp_T),1))\n            \n            temp_gp.fit(temp_X,temp_T,temp_Y,self.Y_curves)\n\n            if new_batch_T is None:\n                new_batch_T=x_max_pred_variance[-1].reshape((1, -1))\n            else:\n                new_batch_T= np.vstack((new_batch_T, x_max_pred_variance[-1].reshape((1, -1))))\n        \n        if new_batch_T is None:\n            return [],0\n\n        else:\n            output=np.sort(new_batch_T.ravel()).tolist()\n            return output, len(output)\n\n    \n    def generating_virtual_observations(self,x_max,t_max,y_original_curves,y_cost_original,IsRandom=False):\n        \n        temp_X_new_original=self.Xscaler.inverse_transform(np.reshape(x_max,(-1,self.dim-1)))\n\n        max_n_virtual_obs=np.int(t_max*self.max_n_augmentation)\n        if max_n_virtual_obs==0:\n            self.countVirtual.append(0)\n            return\n        \n        if IsRandom==True:\n            l = [np.random.uniform(0, t_max) for _ in range(max_n_virtual_obs)]\n        else:\n            l,n_virtual_obs=self.select_informative_location_by_uncertainty(max_n_virtual_obs,x_max,t_max)\n            \n        self.countVirtual.append(n_virtual_obs)\n        \n        if self.verbose:\n            np.set_printoptions(suppress=True)\n            print(\"Max #augmented points\",max_n_virtual_obs, \"\\t #augmented points \",len(l),\n                  \"\\t Augmented points: \",np.round(l,decimals=3))\n            \n        l_original=[self.SearchSpace[-1,0]+val*self.max_min_gap[-1] for val in l]\n                           \n        virtual_obs_t_original=np.asarray(l_original).T\n        virtual_obs_t=np.asarray(l).T\n        \n        y_virtual_original=[0]*n_virtual_obs\n        for ii in range(n_virtual_obs):\n            \n            idx=np.int(virtual_obs_t_original[ii])\n            \n            temp_curve=y_original_curves[0][:idx+1]\n            self.markVirtualObs.append(1)\n\n            y_virtual_original[ii]=transform_logistic([temp_curve],\\\n                      self.gp.logistic_hyper['midpoint'],self.gp.logistic_hyper['growth'],self.SearchSpace[-1,1])\n           \n            self.X = np.vstack((self.X, x_max.reshape((1, -1))))\n            self.X_original=np.vstack((self.X_original, temp_X_new_original))\n        \n            self.T = np.vstack((self.T, virtual_obs_t[ii].reshape((1, -1))))\n            temp=np.asarray(virtual_obs_t_original[ii])\n            self.T_original=np.vstack((self.T_original, temp.reshape((1, -1))))\n\n\n            self.Y_original = np.append(self.Y_original,[y_virtual_original[ii]])\n            self.Y_curves.append(temp_curve)\n            \n            y_cost_estimate=y_cost_original*virtual_obs_t[ii]\n            self.Y_cost_original = np.append(self.Y_cost_original,[y_cost_estimate])\n            \n        \n    def suggest_nextpoint(self):\n \n        self.gp=ProductGaussianProcess(self.scaleSearchSpace,self.gp.hyper,self.gp.logistic_hyper)\n        self.gp.fit(self.X, self.T,self.Y,self.Y_curves)\n            \n        self.condition_number.append(self.gp.cond_num)\n        if self.verbose:\n            print(\"ln of conditioning number of GP covariance matrix\", np.round(np.log(self.gp.cond_num),decimals=1))\n\n        count=len(self.markVirtualObs)-np.sum(self.markVirtualObs)\n        count=np.int(count)\n\n        if  len(self.Y)%(2*self.dim)==0:\n\n            hyper=[self.gp.hyper['lengthscale_x'],self.gp.hyper['lengthscale_t'], \\\n                   self.gp.logistic_hyper['midpoint'],self.gp.logistic_hyper['growth']]\n            newlengthscale_x,newlengthscale_t,new_midpoint, new_growth = self.gp.optimize_lengthscale_logistic_hyper(hyper,self.gp.noise_delta)\n            \n            self.gp.hyper['lengthscale_x']=newlengthscale_x\n            self.gp.hyper['lengthscale_t']=self.gp.hyper['lengthscale_t']\n            self.gp.logistic_hyper['midpoint']=new_midpoint\n            self.gp.logistic_hyper['growth']=new_growth\n          \n            if self.verbose:\n                print(\"==estimated lengthscale_x={:.4f}   lengthscale_t={:.3f}   Logistic_m0={:.1f}   Logistic_g0={:.1f}\".format(\n                    newlengthscale_x,newlengthscale_t,new_midpoint,new_growth))\n                \n        start_opt=time.time()\n\n        combine_input=np.hstack((self.X,self.T))\n        self.linear_regression.fit(combine_input,self.Y_cost)\n        \n        x_max_temp=self.acq_utility_cost()\n        x_max=x_max_temp[:-1]\n        t_max=x_max_temp[-1]\n            \n        finished_opt=time.time()\n        elapse_opt=finished_opt-start_opt\n        self.time_opt=np.hstack((self.time_opt,elapse_opt))\n\n        self.markVirtualObs.append(0)\n\n        self.X = np.vstack((self.X, x_max.reshape((1, -1))))\n        self.T = np.vstack((self.T, t_max.reshape((1, -1))))\n\n        temp_X_new_original=self.Xscaler.inverse_transform(np.reshape(x_max,(-1,self.dim-1)))\n        self.X_original=np.vstack((self.X_original, temp_X_new_original))\n        \n        temp_T_new_original=self.Tscaler.inverse_transform(np.reshape(t_max,(-1,1)))\n        self.T_original=np.vstack((self.T_original, temp_T_new_original))\n\n        x_original_to_test=x_max_temp*self.max_min_gap+self.SearchSpace[:,0]\n\n        y_original_curves, y_cost_original= self.f(x_original_to_test)\n        \n        y_original=transform_logistic(y_original_curves,\\\n              self.gp.logistic_hyper['midpoint'],self.gp.logistic_hyper['growth'],self.SearchSpace[-1,1])\n        \n        if len(y_original_curves)==1:\n            self.Y_curves.append(y_original_curves[0])\n        else:\n            self.Y_curves.append(y_original_curves)\n\n        \n        self.Y_original = np.append(self.Y_original,y_original)\n        self.Y_cost_original = np.append(self.Y_cost_original,y_cost_original)\n\n        self.generating_virtual_observations(x_max,t_max,y_original_curves,y_cost_original[0])\n        \n        if np.std(self.Y_original)==0:\n            self.Y=(self.Y_original-np.mean(self.Y_original))\n        else:\n            self.Y=(self.Y_original-np.mean(self.Y_original))/np.std(self.Y_original)\n            \n        self.Y_cost=(self.Y_cost_original-np.min(self.Y_cost_original))/(np.max(self.Y_cost_original)-np.min(self.Y_cost_original))\n                    \n        np.set_printoptions(suppress=True)\n\n        print(\"[original scale] x={} t={:.0f} current y={:.2f}, ybest={:.2f}\".format( np.round(self.X_original[-1],decimals=4),\\n              np.asscalar(self.T_original[-1]),np.asscalar(self.Y_original[-1]), np.asscalar(self.Y_original.max())))\n\n\n# --- bayes_opt/product_gaussian_process.py ---\n\nclass ProductGaussianProcess(object):\n    \n    def __init__ (self,SearchSpace,gp_hyper=None,logistic_hyper=None,verbose=0):\n        self.noise_delta=5e-4\n        self.noise_upperbound=1e-2\n        self.mycov=self.cov_RBF_time\n        self.SearchSpace=SearchSpace\n        scaler = MinMaxScaler()\n        scaler.fit(SearchSpace.T)\n        self.Xscaler=scaler\n        self.verbose=verbose\n        self.dim=SearchSpace.shape[0]\n        \n        if gp_hyper is None:\n            self.hyper={}\n            self.hyper['var']=1\n            self.hyper['lengthscale_x']=0.02\n            self.hyper['lengthscale_t']=0.2\n        else:\n            self.hyper=gp_hyper\n\n        \n        if logistic_hyper is None:\n            self.logistic_hyper={}\n            self.logistic_hyper['midpoint']=0.0\n            self.logistic_hyper['growth']=1.0   \n        else:\n            self.logistic_hyper=logistic_hyper\n\n        self.X=[]\n        self.T=[]\n        self.Y=[]\n        self.Y_curves=None\n        \n        self.alpha=[]\n        self.L=[]\n        \n        self.MaxEpisode=0\n        \n        return None\n       \n\n    def cov_RBF_time(self, x1,t1,x2,t2,lengthscale,lengthscale_t):\n        \n        Euc_dist=euclidean_distances(x1,x2)\n        exp_dist_x=np.exp(-np.square(Euc_dist)/lengthscale)\n        \n        Euc_dist=euclidean_distances(t1,t2)\n        exp_dist_t=np.exp(-np.square(Euc_dist)/lengthscale_t)\n        \n        return exp_dist_x*exp_dist_t\n                \n    def fit(self,X,T,Y,Y_curves):\n        temp=np.hstack((X,T))\n        ur = unique_rows(temp)\n        \n        T=T[ur]\n        X=X[ur]\n        Y=Y[ur]\n        \n        self.X=X\n        self.Y=Y\n        self.T=T\n        self.Y_curves=[val for idx,val in enumerate(Y_curves) if ur[idx]==True]\n        \n        for curves in self.Y_curves:\n            self.MaxEpisode=max(len(curves),self.MaxEpisode)\n            \n        Euc_dist_x=euclidean_distances(X,X)\n    \n        Euc_dist_t=euclidean_distances(T,T)\n       \n        self.KK_x_x=np.exp(-np.square(Euc_dist_x)/self.hyper['lengthscale_x']\\\n                           -np.square(Euc_dist_t)/self.hyper['lengthscale_t'])+np.eye(len(X))*self.noise_delta\n          \n        if np.isnan(self.KK_x_x).any():\n            print(\"nan in KK_x_x\")\n        \n        self.L=np.linalg.cholesky(self.KK_x_x)\n        temp=np.linalg.solve(self.L,self.Y)\n        self.alpha=np.linalg.solve(self.L.T,temp)\n        self.cond_num=self.compute_condition_number()\n        \n    def compute_condition_number(self):\n        cond_num=np.linalg.cond(self.KK_x_x)\n        return cond_num\n    \n\n    def log_marginal_lengthscale_logistic_hyper(self,hyper,noise_delta):\n        def compute_log_marginal_with_logistic_hyper(lengthscale, lengthscale_t,midpoint,growth,noise_delta):\n            temp=np.hstack((self.X,self.T))\n            ur = unique_rows(temp)\n            myX=self.X[ur]\n            myT=self.T[ur]\n            \n            Y_original=transform_logistic(self.Y_curves,midpoint,growth,self.MaxEpisode)\n            myY=(Y_original-np.mean(Y_original))/np.std(Y_original)\n            \n            myY=myY[ur]\n          \n            self.Euc_dist_x=euclidean_distances(myX,myX)\n            self.Euc_dist_t=euclidean_distances(myT,myT)\n        \n            KK=np.exp(-np.square(self.Euc_dist_x)/lengthscale-np.square(self.Euc_dist_t)/lengthscale_t)\n                +np.eye(len(myX))*noise_delta\n                    \n            \n            try:\n                temp_inv=np.linalg.solve(KK,myY)\n            except: # singular\n                return -np.inf\n            \n            try:\n                first_term=-0.5*np.dot(myY.T,temp_inv)\n                \n                if KK.shape[0]>200:\n                    idx=np.random.permutation(KK.shape[0])\n                    idx=idx[:200]\n                    KK=KK[np.ix_(idx,idx)]\n                chol  = spla.cholesky(KK, lower=True)\n                W_logdet=np.sum(np.log(np.diag(chol)))\n    \n                second_term=-W_logdet\n            except: # singular\n                return -np.inf\n            \n\n            logmarginal=first_term+second_term-0.5*len(myY)*np.log(2*3.14)\n                \n            if np.isnan(np.asscalar(logmarginal))==True:\n                print(\"lengthscale_x={:f} lengthscale_t={:f} first term ={:.4f} second  term ={:.4f}\".format(\n                        lengthscale,lengthscale_t,np.asscalar(first_term),np.asscalar(second_term)))\n\n            return np.asscalar(logmarginal)\n        \n        logmarginal=0\n\n        if not isinstance(hyper,list) and len(hyper.shape)==2:\n            logmarginal=[0]*hyper.shape[0]\n            growth=hyper[:,3]\n            midpoint=hyper[:,2]\n            lengthscale_t=hyper[:,1]\n            lengthscale_x=hyper[:,0]\n            for idx in range(hyper.shape[0]):\n                logmarginal[idx]=compute_log_marginal_with_logistic_hyper(lengthscale_x[idx],\\\n                           lengthscale_t[idx],midpoint[idx],growth[idx],noise_delta)\n        else:\n            lengthscale_x,lengthscale_t,midpoint,growth=hyper\n            logmarginal=compute_log_marginal_with_logistic_hyper(lengthscale_x,lengthscale_t,\\\n                                                                 midpoint,growth,noise_delta)\n        return logmarginal\n    \n    def optimize_lengthscale_SE_logistic_hyper(self,previous_hyper,noise_delta):\n        SearchSpace_l_min=0.03\n        SearchSpace_l_max=0.3\n        \n        SearchSpace_midpoint_min=-2\n        SearchSpace_midpoint_max=3\n        \n        SearchSpace_growth_min=0.5\n        SearchSpace_growth_max=2\n        \n        mySearchSpace=np.asarray([[SearchSpace_l_min,SearchSpace_l_max],[10*SearchSpace_l_min,2*SearchSpace_l_max],\n                             [SearchSpace_midpoint_min,SearchSpace_midpoint_max],[SearchSpace_growth_min,SearchSpace_growth_max]])\n        \n        lengthscale_tries = np.random.uniform(mySearchSpace[:, 0], mySearchSpace[:, 1],size=(20, 4))\n\n        self.flagOptimizeHyperFirst=0\n\n        logmarginal_tries=self.log_marginal_lengthscale_logistic_hyper(lengthscale_tries,noise_delta)\n\n        idx_max=np.argmax(logmarginal_tries)\n        lengthscale_init_max=lengthscale_tries[idx_max]\n        \n        myopts ={'maxiter':30*self.dim,'maxfun':30*self.dim}\n\n        x_max=[]\n        max_log_marginal=None\n        \n        res = minimize(lambda x: -self.log_marginal_lengthscale_logistic_hyper(x,noise_delta),lengthscale_init_max,\n                       bounds=mySearchSpace,method=\"L-BFGS-B\",options=myopts)\n        if 'x' not in res:\n            val=self.log_marginal_lengthscale_logistic_hyper(res,noise_delta)    \n        else:\n            val=self.log_marginal_lengthscale_logistic_hyper(res.x,noise_delta)  \n        \n        if max_log_marginal is None or val >= max_log_marginal:\n            if 'x' not in res:\n                x_max = res\n            else:\n                x_max = res.x\n            max_log_marginal = val\n\n        return x_max\n\n    def optimize_lengthscale_logistic_hyper(self,prev_hyper,noise_delta):\n        newlengthscale,newlengthscale_t,newmidpoint,newgrowth=self.optimize_lengthscale_SE_logistic_hyper(prev_hyper,noise_delta)\n        self.hyper['lengthscale_x']=newlengthscale\n        self.hyper['lengthscale_t']=newlengthscale_t\n        \n        temp=np.hstack((self.X,self.T))\n        ur = unique_rows(temp)\n\n        Y_original=transform_logistic(self.Y_curves,newmidpoint,newgrowth,self.SearchSpace[-1,1])\n        Y=(Y_original-np.mean(Y_original))/np.std(Y_original)\n        self.Y=Y\n        \n        self.fit(self.X[ur],self.T[ur],self.Y[ur],self.Y_curves)\n        \n        return newlengthscale,newlengthscale_t,newmidpoint,newgrowth\n        \n    def predict(self,xTest, eval_MSE=True):\n        if len(xTest.shape)==1:\n            xTest=xTest.reshape((-1,self.X.shape[1]+1))\n            \ntTest=xTest[:,-1]\ntTest=np.atleast_2d(tTest)\ntTest=np.reshape(tTest,(xTest.shape[0],-1))\n\nxTest=xTest[:,:-1]\n\ntemp=np.hstack((self.X,self.T))\nur = unique_rows(temp)\n\nX=self.X[ur]\nT=self.T[ur]\n        \nEuc_dist_x=euclidean_distances(xTest,xTest)\nEuc_dist_t=euclidean_distances(tTest,tTest)\n\nKK_xTest_xTest=np.exp(-np.square(Euc_dist_x)/self.hyper['lengthscale_x']-np.square(Euc_dist_t)/self.hyper['lengthscale_t'])\n    +np.eye(xTest.shape[0])*self.noise_delta\n\nEuc_dist_test_train_x=euclidean_distances(xTest,X)\n\nEuc_dist_test_train_t=euclidean_distances(tTest,T)\n\nKK_xTest_xTrain=np.exp(-np.square(Euc_dist_test_train_x)/self.hyper['lengthscale_x']-np.square(Euc_dist_test_train_t)/self.hyper['lengthscale_t'])\n    \nmean=np.dot(KK_xTest_xTrain,self.alpha)\nv=np.linalg.solve(self.L,KK_xTest_xTrain.T)\nvar=KK_xTest_xTest-np.dot(v.T,v)\n\nreturn mean.ravel(),np.diag(var)\n\n\n# --- bayes_opt/curve_compression.py ---\n\ndef apply_one_transform_logistic(curve, midpoint=-2, growth=1,MaxEpisode=1000,IsReturnCurve=False):\n    if isinstance(curve, (list,)):\n        curve=curve[0]\n        \n    def logistic_func(x):\n        return 1.0/(1+np.exp(-growth*(x-midpoint)))\n\t\n    my_xrange_scaled=np.linspace(-6,6, int(MaxEpisode))\n\n    my_logistic_value_scaled=logistic_func(my_xrange_scaled)\n\n    my_logistic_value_scaled=my_logistic_value_scaled[:len(curve)]\n\n    if np.max(curve)<=0 and np.min(curve)<=0:\n        curve=curve+500\n    \n    threshold=(midpoint+6-2)*len(curve)/(12)\n    threshold=np.int(threshold)\n    \n    prod_func=curve*my_logistic_value_scaled\n    \n    average=[np.mean(prod_func[threshold:pos+1]) for pos in range(threshold,len(prod_func))]\n\n    if IsReturnCurve==True:\n        return average[-1],my_logistic_value_scaled\n    else:\n        return average[-1]\n\ndef transform_logistic(curves, midpoint=0, growth=1,MaxEpisode=1000):\n    if len(curves)==1:\n        output=apply_one_transform_logistic(curves[0], midpoint, growth,MaxEpisode)\n    else:\n        output=[0]*len(curves)\n        for idx, curve in enumerate(curves):\n            output[idx]=apply_one_transform_logistic(curve, midpoint, growth,MaxEpisode)\n    return output\n\n# --- bayes_opt/acquisition_functions.py ---\n\nclass AcquisitionFunction(object):\n    def __init__(self, acq):\n\n        self.acq=acq\n        acq_name=acq['name']\n        \n        if 'mu_max' in acq:\n            self.mu_max=acq['mu_max']\n        \n        ListAcq=['bucb','ucb', 'ei','poi','random','ucb_pe',\n                 'pure_exploration','mu','lcb','ei_mu_max']\n        \n        IsTrue=[val for idx,val in enumerate(ListAcq) if val in acq_name]\n        if  IsTrue == []:\n            err = \"The utility function \" \\\n                  \"{} has not been implemented, \" \\\n                  \"please choose one of ucb, ei, or poi.\".format(acq_name)\n            raise NotImplementedError(err)\n        else:\n            self.acq_name = acq_name\n            \n        self.dim=acq['dim']\n        \n        if 'scalebounds' not in acq:\n            self.scalebounds=[0,1]*self.dim\n            \n        else:\n            self.scalebounds=acq['scalebounds']\n               \n\n    def acq_kind(self, x, gp):\n        y_max=np.max(gp.Y)\n        if np.any(np.isnan(x)):\n            return 0\n       \n        if self.acq_name == 'ucb':\n            return self._ucb(x, gp)\n        if self.acq_name == 'lcb':\n            return self._lcb(x, gp)\n        if self.acq_name == 'ei':\n            return self._ei(x, gp, y_max)\n        if self.acq_name == 'ei_mu_max':\n            return self._ei(x, gp, self.mu_max)\n        if self.acq_name == 'poi':\n            return self._poi(x, gp, y_max)\n        \n        if self.acq_name == 'pure_exploration':\n            return self._pure_exploration(x, gp) \n      \n        if self.acq_name == 'mu':\n            return self._mu(x, gp)\n        \n        if self.acq_name == 'ucb_pe':\n            return self._ucb_pe(x, gp,self.acq['kappa'],self.acq['maxlcb'])\n       \n            \n    @staticmethod\n    def _mu(x, gp):\n        mean, var = gp.predict(x, eval_MSE=True)\n        mean=np.atleast_2d(mean).T\n        return mean\n                \n    @staticmethod\n    def _ei(x, gp, y_max):\n        y_max=np.asscalar(y_max)\n        mean, var = gp.predict(x, eval_MSE=True)\n        var2 = np.maximum(var, 1e-10 + 0 * var)\n        z = (mean - y_max)/np.sqrt(var2)\n        out=(mean - y_max) * norm.cdf(z) + np.sqrt(var2) * norm.pdf(z)\n        \nout[var2<1e-10]=0\n        return out\n\n    @staticmethod\n    def _pure_exploration(x, gp):\n        mean, var = gp.predict(x, eval_MSE=True)\n        var.flags['WRITEABLE']=True\n        var[var<1e-10]=0\n        mean=np.atleast_2d(mean).T\n        var=np.atleast_2d(var).T\n        return np.sqrt(var)\n\n\n# --- bayes_opt/acquisition_maximization.py ---\n\ndef acq_max_with_name(gp,scaleSearchSpace,acq_name=\"ei\",IsReturnY=False,IsMax=True,fstar_scaled=None):\n    acq={}\n    acq['name']=acq_name\n    acq['dim']=scaleSearchSpace.shape[0]\n    acq['scaleSearchSpace']=scaleSearchSpace   \n    if fstar_scaled:\n        acq['fstar_scaled']=fstar_scaled   \n\n    myacq=AcquisitionFunction(acq)\n    if IsMax:\n        x_max = acq_max(ac=myacq.acq_kind,gp=gp,bounds=scaleSearchSpace,opt_toolbox='scipy')\n    else:\n        x_max = acq_min_scipy(ac=myacq.acq_kind,gp=gp,bounds=scaleSearchSpace)\n    if IsReturnY==True:\n        y_max=myacq.acq_kind(x_max,gp=gp)\n        return x_max,y_max\n    return x_max\n\ndef acq_min_scipy_kwargs(myfunc, SearchSpace, **kwargs):\n    dim=SearchSpace.shape[0]\n    x_max = SearchSpace[:, 0]\n    min_acq = None\n\n    myopts ={'maxiter':10*dim,'maxfun':20*dim}\n\n    for i in range(3*dim):\n        x_tries = np.random.uniform(SearchSpace[:, 0], SearchSpace[:, 1],size=(100*dim, dim))\n        \n        y_tries=myfunc(x_tries,**kwargs)\n        \n        idx_min=np.argmin(y_tries)\n\n        x_init_min=x_tries[idx_min]\n    \n        res = minimize(lambda x: myfunc(x.reshape(1, -1), **kwargs),x_init_min.reshape(1, -1),bounds=SearchSpace,\n                       method=\"L-BFGS-B\",options=myopts)\n\n        if 'x' not in res:\n            val=myfunc(res,**kwargs)        \n        else:\n            val=myfunc(res.x,**kwargs) \n        \n        if min_acq is None or val <= min_acq:\n            if 'x' not in res:\n                x_max = res\n            else:\n                x_max = res.x\n            min_acq = val\n\n    return np.clip(x_max, SearchSpace[:, 0], SearchSpace[:, 1])\n\n\ndef acq_max(ac, gp, bounds, opt_toolbox='scipy',seeds=[],IsMax=True):\n    y_max=np.max(gp.Y)\n  \n    x_max = acq_max_scipy(ac=ac,gp=gp,y_max=y_max,bounds=bounds)\n\n    return x_max\n\ndef acq_max_scipy(ac, gp, y_max, bounds):\n    dim=bounds.shape[0]\n    x_max = bounds[:, 0]\n    max_acq = None\n\n    myopts ={'maxiter':10*dim,'maxfun':20*dim}\n\n    for i in range(1*dim):\n        x_tries = np.random.uniform(bounds[:, 0], bounds[:, 1],size=(50*dim, dim))\n    \n        y_tries=ac(x_tries,gp=gp)\n        \n        idx_max=np.argmax(y_tries)\n\n        x_init_max=x_tries[idx_max]\n        \n    \n        res = minimize(lambda x: -ac(x.reshape(1, -1), gp=gp),x_init_max.reshape(1, -1),bounds=bounds,\n                       method=\"L-BFGS-B\",options=myopts)\n\n        if 'x' not in res:\n            val=ac(res,gp)        \n        else:\n            val=ac(res.x,gp) \n\n        if max_acq is None or val >= max_acq:\n            if 'x' not in res:\n                x_max = res\n            else:\n                x_max = res.x\n            max_acq = val\n\n    return np.clip(x_max, bounds[:, 0], bounds[:, 1])",
    "Experiment Result": "The BOIL framework uses a Product Gaussian Process (GP) with a product kernel `k(x,x') × k(t,t')`. The specific kernel used for both `x` and `t` dimensions is a Radial Basis Function (RBF) kernel. The GP model `ProductGaussianProcess` is initialized with a `noise_delta` of `5e-4`.\n\nThe initial hyperparameters for the GP are:\n- `lengthscale_x`: 0.02\n- `lengthscale_t`: 0.2\n\nThe training curve compression uses a parameterized Sigmoid preference function `l(u|m0,g0)`.\nInitial hyperparameters for the Sigmoid function are:\n- `midpoint` (`m0`): 0.0\n- `growth` (`g0`): 1.0\n\nThese hyperparameters (GP lengthscales and Sigmoid parameters) are optimized by maximizing the GP log marginal likelihood every `2 * dim` iterations (where `dim` is the total dimensionality of the search space including `x` and `t`). The optimization is performed using `L-BFGS-B` with options `{'maxiter': 30 * dim, 'maxfun': 30 * dim}`. The search bounds for these hyperparameters are:\n- `lengthscale_x`: [0.03, 0.3]\n- `lengthscale_t`: [0.3, 0.6] (calculated as [10 * 0.03, 2 * 0.3])\n- `midpoint` (`m0`): [-2, 3]\n- `growth` (`g0`): [0.5, 2]\n\nThe cost function `c(x,t)` is approximated by a `linear_model.LinearRegression` from scikit-learn.\n\nThe next hyperparameter-iteration point is selected by maximizing a cost-aware acquisition function `α(x,t)/µc(x,t)`. The default acquisition function `acq_name` is `'ei_mu_max'`, which is a variant of Expected Improvement (EI) using the maximum of the GP mean as the incumbent.\n\nA data augmentation technique is employed to enhance sample efficiency and prevent GP covariance matrix ill-conditioning. This involves selectively sampling intermediate points at maximum GP predictive uncertainty.\n- Maximum number of augmented points (`max_n_augmentation`): 10 (per real observation).\n- Threshold for the natural logarithm of the covariance matrix condition number (`threshold_cond`): 15. Augmentation stops if this threshold is exceeded or if the predictive uncertainty falls below `noise_delta + 1e-3`."
}{
    "Title": "Scaling Laws for Hyperparameter Optimization",
    "Main Contributions": "The main research problem addressed is the high cost and inefficiency of Hyperparameter Optimization (HPO) for Deep Learning (DL), particularly the underutilization of the inherent power-law nature of learning curves by existing Bayesian optimization methods. The paper introduces Deep Power Laws (DPL), a novel multi-fidelity HPO method that uses an ensemble of neural network models. These models are conditioned to predict learning curve performance following a power-law scaling pattern. DPL integrates this probabilistic surrogate with Bayesian Optimization to dynamically decide when to pause and incrementally train hyperparameter configurations through gray-box evaluations. The key findings demonstrate that DPL achieves state-of-the-art performance across diverse benchmarks (tabular, image, NLP, Large Language Models) and deep learning architectures, consistently outperforming 7 strong HPO baselines in terms of both any-time and final performance.",
    "Methodology": "Deep Power Laws (DPL) is a multi-fidelity HPO method built upon the power-law relationship between validation loss and training epochs. The core of DPL is an ensemble of K (specifically, 5) probabilistic surrogates. Each surrogate is a neural network `g` that maps a hyperparameter configuration `λ` to three power-law coefficients `(α, β, γ)`. These coefficients define the predicted learning curve `f^(λ, b) = g(λ)α + g(λ)β * b^(-g(λ)γ)`. The ensemble of these neural networks is trained to minimize the L1 loss against observed learning curve data, with individual models initialized with different weights and trained on different mini-batches to provide robust posterior mean and variance estimates. Bayesian Optimization is then employed using the Expected Improvement (EI) acquisition function, which leverages these mean and variance predictions to select the next most promising configuration. A novel multi-fidelity strategy is used where selected configurations are advanced by small, incremental budgets (e.g., one epoch) rather than being fully trained immediately, allowing for dynamic pausing and continuation. The neural networks in the ensemble are 2-layer feedforward networks with 128 units per layer, Leaky ReLU non-linearity, and GLU activation on β and γ output units. Training involves an initial 250 epochs with random weights, followed by 20 epochs of refinement per HPO iteration, with a restart mechanism if optimization stagnates.",
    "Experimental Setup": "DPL was evaluated across three primary benchmarks covering diverse modalities: LCBench (2,000 configurations, 7 hyperparameters, 35 tabular datasets, balanced accuracy, 51 epochs), PD1 (807-2807 configurations, 4 hyperparameters, vision and bioinformatics datasets, accuracy, 5-1414 epochs), and TaskSet (1,000 configurations, 8 continuous hyperparameters, 12 RNN text classification tasks, log-likelihood loss, 50 epochs). Additionally, a custom nanoGPT-Bench was created for Large Language Models, involving training smaller GPT-2 models (30M parameters) on OpenWebText to optimize learning rate hyperparameters, with performance then evaluated on larger-scale transformers. A separate experiment was conducted on a continuous search space for EfficientNetV2 models on the CIFAR10 dataset, optimizing learning rate and weight decay. The baselines for comparison included Random Search, Hyperband, ASHA, BOHB, DEHB, multi-fidelity SMAC, and BOCA (from the Dragonfly Library), all using their official public implementations. Performance was measured using the regret of the best-found configuration, averaged and normalized over 10 repetitions with different random seeds. Any-time performance was assessed based on normalized wall clock time. Experiments were conducted on a CPU cluster (Intel Xeon E5-2630v4 CPUs with 2 cores and 12GB memory per experiment), while nanoGPT-Bench experiments utilized NVIDIA RTX 2080 GPUs. The HPO budget was set to allow for the equivalent of 20 full hyperparameter configuration evaluations, with multi-fidelity methods starting with a minimal 1-step evaluation.",
    "Limitations": "The study identified several limitations. The uncertainty estimation provided by the Deep Ensemble approach was found to be suboptimal when compared to traditional Bayesian Optimization surrogates like Gaussian Processes. A significant constraint is the additional computational cost associated with training an ensemble, as it requires fitting multiple power law models. While the power law assumption generally holds, some learning curves do exhibit divergent behavior that does not strictly follow this pattern. Attempts to use more complex power law formulations (e.g., with breaking points or shifts) proved difficult to optimize, being prone to divergence and numerical instability (e.g., division by zero or negative roots). Furthermore, in certain datasets within benchmarks like PD1, the distribution of hyperparameter performances was skewed, with a large number of configurations achieving top performance. This characteristic limited the statistical significance of DPL's superior performance, as even non-model-based techniques could quickly find high-performing configurations in such scenarios.",
    "Future Research Directions": "Future research will focus on improving DPL's capabilities. Specifically, the authors plan to investigate combining power laws with Gaussian Processes, aiming to enhance the accuracy and robustness of uncertainty estimation within the Bayesian Optimization framework. Additionally, they intend to explore the incorporation of various other fidelity types beyond just training epochs, to further expand the applicability and efficiency of multi-fidelity hyperparameter optimization.",
    "Experiment Code": "from copy import deepcopyimport loggingimport osimport timefrom typing import List, Tupleimport numpy as npfrom scipy.stats import normimport torchfrom torch.utils.data import DataLoaderfrom data_loader.tabular_data_loader import WrappedDataLoaderfrom dataset.tabular_dataset import TabularDatasetfrom models.conditioned_power_law import ConditionedPowerLawclass PowerLawSurrogate:    def __init__(        self,        hp_candidates: np.ndarray,        surrogate_configs: dict = None,        seed: int = 11,        max_benchmark_epochs: int = 52,        ensemble_size: int = 5,        nr_epochs: int = 250,        fantasize_step: int = 1,        minimization: bool = True,        total_budget: int = 1000,        device: str = None,        output_path: str = '.',        dataset_name: str = 'unknown',        pretrain: bool = False,        backbone: str = 'power_law',        max_value: float = 100,        min_value: float = 0,        fill_value: str = 'zero',    ):        \"\"\"        Args:            hp_candidates: np.ndarray                The full list of hyperparameter candidates for a given dataset.            surrogate_configs: dict                The model configurations for the surrogate.            seed: int                The seed that will be used for the surrogate.            max_benchmark_epochs: int                The maximal budget that a hyperparameter configuration                has been evaluated in the benchmark for.            ensemble_size: int                The number of members in the ensemble.            nr_epochs: int                Number of epochs for which the surrogate should be                trained.            fantasize_step: int                The number of steps for which we are looking ahead to                evaluate the performance of a hpc.            minimization: bool                If for the evaluation metric, the lower the value the better.            total_budget: int                The total budget given. Used to calculate the initialization                percentage.            device: str                The device where the experiment will be run on.            output_path: str                The path where all the output will be stored.            dataset_name: str                The name of the dataset that the experiment will be run on.            pretrain: bool                If the surrogate will be pretrained before with a synthetic                curve.            backbone: str                The backbone, which can either be 'power_law' or 'nn'.            max_value: float                The maximal value for the dataset.            min_value: float                The minimal value for the dataset.            fill_value: str = 'zero',                The filling strategy for when learning curves are used.                Either 'zero' or 'last' where last represents the last value.        \"\"\"        torch.backends.cudnn.deterministic = True        torch.backends.cudnn.benchmark = False        self.total_budget = total_budget        self.fill_value = fill_value        self.max_value = max_value        self.min_value = min_value        self.backbone = backbone        self.pretrained_path = os.path.join(            output_path,            'power_law',            f'checkpoint_{seed}.pth',        )        self.model_instances = [            ConditionedPowerLaw,            ConditionedPowerLaw,            ConditionedPowerLaw,            ConditionedPowerLaw,            ConditionedPowerLaw,        ]        if device is None:            self.dev = torch.device(                'cuda') if torch.cuda.is_available() else torch.device('cpu')        else:            self.dev = torch.device(device)        self.learning_rate = 0.001        self.batch_size = 64        self.refine_batch_size = 64        self.criterion = torch.nn.L1Loss()        self.hp_candidates = hp_candidates        self.minimization = minimization        self.seed = seed        self.logger = logging.getLogger('power_law')        logging.basicConfig(            filename=f'power_law_surrogate_{dataset_name}_{seed}.log',            level=logging.INFO,            force=True,        )        # with what percentage configurations will be taken randomly instead of being sampled from the model        self.fraction_random_configs = 0.1        self.iteration_probabilities = np.random.rand(self.total_budget)        # the keys will be hyperparameter indices while the value        # will be a list with all the budgets evaluated for examples        # and with all performances for the performances        self.examples = dict()        self.performances = dict()        # set a seed already, so that it is deterministic when        # generating the seeds of the ensemble        torch.manual_seed(seed)        np.random.seed(seed)        self.seeds = np.random.choice(100, ensemble_size, replace=False)        self.max_benchmark_epochs = max_benchmark_epochs        self.ensemble_size = ensemble_size        self.nr_epochs = nr_epochs        self.refine_nr_epochs = 20        self.fantasize_step = fantasize_step        self.pretrain = pretrain        initial_configurations_nr = 1        conf_individual_budget = 1        init_conf_indices = np.random.choice(self.hp_candidates.shape[0], initial_configurations_nr, replace=False)        init_budgets = [i for i in range(1, conf_individual_budget + 1)]        self.rand_init_conf_indices = []        self.rand_init_budgets = []        # basically add every config index up to a certain budget threshold for the initialization        # we will go through both lists during the initialization        for config_index in init_conf_indices:            for config_budget in init_budgets:                self.rand_init_conf_indices.append(config_index)                self.rand_init_budgets.append(config_budget)        self.initial_random_index = 0        if surrogate_configs is None:            self.surrogate_configs = []            for i in range(0, self.ensemble_size):                self.surrogate_configs.append(                    {                        'nr_units': 128,                        'nr_layers': 2,                        'kernel_size': 3,                        'nr_filters': 4,                        'nr_cnn_layers': 2,                        'use_learning_curve': False,                    }                )        else:            self.surrogate_configs = surrogate_configs        self.nr_features = self.hp_candidates.shape[1]        self.best_value_observed = np.inf        self.diverged_configs = set()        # Where the models of the ensemble will be stored        self.models = []        # A tuple which will have the last evaluated point        # It will be used in the refining process        # Tuple(config_index, budget, performance, curve)        self.last_point = None        self.initial_full_training_trials = 10        # a flag if the surrogate should be trained        self.train = True        # the times it was refined        self.refine_counter = 0        # the surrogate iteration counter        self.iterations_counter = 0        # info dict to drop every surrogate iteration        self.info_dict = dict()        # the start time for the overhead of every surrogate iteration        # will be recorded here        self.suggest_time_duration = 0        self.output_path = output_path        self.dataset_name = dataset_name        self.no_improvement_threshold = int(self.max_benchmark_epochs + 0.2 * self.max_benchmark_epochs)        self.no_improvement_patience = 0    def _prepare_dataset(self) -> TabularDataset:        \"\"\"This method is called to prepare the necessary training dataset        for training a model.        Returns:            train_dataset: A dataset consisting of examples, labels, budgets                and learning curves.        \"\"\"        train_examples, train_labels, train_budgets, train_curves = self.history_configurations()        train_curves = self.prepare_training_curves(train_budgets, train_curves)        train_examples = np.array(train_examples, dtype=np.single)        train_labels = np.array(train_labels, dtype=np.single)        train_budgets = np.array(train_budgets, dtype=np.single)        # scale budgets to [0, 1]        train_budgets = train_budgets / self.max_benchmark_epochs        train_dataset = TabularDataset(            train_examples,            train_labels,            train_budgets,            train_curves,        )        return train_dataset    def _refine_surrogate(self):        \"\"\"Refine the surrogate model.        \"\"\"        for model_index, model_seed in enumerate(self.seeds):            train_dataset = self._prepare_dataset()            self.logger.info(f'Started refining model with index: {model_index}')            refined_model = self.train_pipeline(                model_index,                train_dataset,                nr_epochs=self.refine_nr_epochs,                refine=True,                weight_new_example=True,                batch_size=self.refine_batch_size,            )            self.models[model_index] = refined_model    def _train_surrogate(self, pretrain: bool = False):        \"\"\"Train the surrogate model.        Trains all the models of the ensemble        with different initializations and different        data orders.        Args:            pretrain: bool                If we have pretrained weights and we will just                refine the models.        \"\"\"        for model_index, model_seed in enumerate(self.seeds):            train_dataset = self._prepare_dataset()            self.logger.info(f'Started training model with index: {model_index}')            if pretrain:                # refine the models that were already pretrained                trained_model = self.train_pipeline(                    model_index,                    train_dataset,                    nr_epochs=self.refine_nr_epochs,                    refine=True,                    weight_new_example=False,                    batch_size=self.batch_size,                    early_stopping_it=self.refine_nr_epochs,  # basically no early stopping                )                self.models[model_index] = trained_model            else:                # train the models for the first time                trained_model = self.train_pipeline(                    model_index,                    train_dataset,                    nr_epochs=self.nr_epochs,                    refine=False,                    weight_new_example=False,                    batch_size=self.batch_size,                    early_stopping_it=self.nr_epochs,  # basically no early stopping                )                self.models.append(trained_model)    def train_pipeline(        self,        model_index: int,        train_dataset: TabularDataset,        nr_epochs: int,        refine: bool = False,        weight_new_example: bool = True,        batch_size: int = 64,        early_stopping_it: int = 10,        activate_early_stopping: bool = False,    ) -> torch.nn.Module:        \"\"\"Train an algorithm to predict the performance        of the hyperparameter configuration based on the budget.        Args:            model_index: int                The index of the model.            train_dataset: TabularDataset                The tabular dataset featuring the examples, labels,                budgets and curves.            nr_epochs: int                The number of epochs to train the model for.            refine: bool                If an existing model will be refined or if the training                will start from scratch.            weight_new_example: bool                If the last example that was added should be weighted more                by being included in every batch. This is only applicable                when refine is True.            batch_size: int                The batch size to be used for training.            early_stopping_it: int                The early stopping iteration patience.            activate_early_stopping: bool                Flag controlling the activation.        Returns:            model: torch.nn.Module                A trained model.        \"\"\"        if model_index == 0:            self.iterations_counter += 1            self.logger.info(f'Iteration number: {self.iterations_counter}')        surrogate_config = self.surrogate_configs[model_index]        seed = self.seeds[model_index]        torch.manual_seed(seed)        np.random.seed(seed)        if refine:            model = self.models[model_index]        else:            model = self.model_instances[model_index](                nr_initial_features=self.nr_features + 1 if self.backbone == 'nn' else self.nr_features,                nr_units=surrogate_config['nr_units'],                nr_layers=surrogate_config['nr_layers'],                use_learning_curve=surrogate_config['use_learning_curve'],                kernel_size=surrogate_config['kernel_size'],                nr_filters=surrogate_config['nr_filters'],                nr_cnn_layers=surrogate_config['nr_cnn_layers'],            )            model.to(self.dev)        # make the training dataset here        train_dataloader = DataLoader(            train_dataset,            batch_size=batch_size,            shuffle=True,        )        train_dataloader = WrappedDataLoader(train_dataloader, self.dev)        optimizer = torch.optim.Adam(model.parameters(), lr=self.learning_rate)        patience_rounds = 0        best_loss = np.inf        best_state = deepcopy(model.state_dict())        for epoch in range(0, nr_epochs):            running_loss = 0            model.train()            for batch_examples, batch_labels, batch_budgets, batch_curves in train_dataloader:                nr_examples_batch = batch_examples.shape[0]                # if only one example in the batch, skip the batch.                # Otherwise, the code will fail because of batchnormalization.                if nr_examples_batch == 1:                    continue                # zero the parameter gradients                optimizer.zero_grad(set_to_none=True)                # in case we are refining, we add the new example to every                # batch to give it more importance.                if refine and weight_new_example:                    newp_index, newp_budget, newp_performance, newp_curve = self.last_point                    new_example = np.array([self.hp_candidates[newp_index]], dtype=np.single)                    newp_missing_values = self.prepare_missing_values_channel([newp_budget])                    newp_budget = np.array([newp_budget], dtype=np.single) / self.max_benchmark_epochs                    newp_performance = np.array([newp_performance], dtype=np.single)                    modified_curve = deepcopy(newp_curve)                    difference = self.max_benchmark_epochs - len(modified_curve) - 1                    if difference > 0:                        modified_curve.extend([modified_curve[-1] if self.fill_value == 'last' else 0] * difference)                    modified_curve = np.array([modified_curve], dtype=np.single)                    newp_missing_values = np.array(newp_missing_values, dtype=np.single)                    # add depth dimension to the train_curves array and missing_value_matrix                    modified_curve = np.expand_dims(modified_curve, 1)                    newp_missing_values = np.expand_dims(newp_missing_values, 1)                    modified_curve = np.concatenate((modified_curve, newp_missing_values), axis=1)                    new_example = torch.tensor(new_example, device=self.dev)                    newp_budget = torch.tensor(newp_budget, device=self.dev)                    newp_performance = torch.tensor(newp_performance, device=self.dev)                    modified_curve = torch.tensor(modified_curve, device=self.dev)                    batch_examples = torch.cat((batch_examples, new_example))                    batch_budgets = torch.cat((batch_budgets, newp_budget))                    batch_labels = torch.cat((batch_labels, newp_performance))                    batch_curves = torch.cat((batch_curves, modified_curve))                outputs = model(batch_examples, batch_budgets, batch_budgets, batch_curves)                loss = self.criterion(outputs, batch_labels)                loss.backward()                optimizer.step()                # print statistics                running_loss += loss.item()            running_loss = running_loss / len(train_dataloader)            self.logger.info(f'Epoch {epoch +1}, Loss:{running_loss}')            if activate_early_stopping:                if running_loss < best_loss:                    best_state = deepcopy(model.state_dict())                    best_loss = running_loss                    patience_rounds = 0                elif running_loss > best_loss:                    patience_rounds += 1                    if patience_rounds == early_stopping_it:                        model.load_state_dict(best_state)                        self.logger.info(f'Stopping training since validation loss is not improving')                        break        if activate_early_stopping:            model.load_state_dict(best_state)        return model    def _predict(self) -> Tuple[np.ndarray, np.ndarray, List, np.ndarray]:        \"\"\"        Predict the performances of the hyperparameter configurations        as well as the standard deviations based on the ensemble.        Returns:            mean_predictions, std_predictions, hp_indices, real_budgets:            Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]                The mean predictions and the standard deviations over                all model predictions for the given hyperparameter                configurations with their associated indices and budgets.        \"\"\"        configurations, hp_indices, budgets, real_budgets, hp_curves = self.generate_candidate_configurations()        # scale budgets to [0, 1]        budgets = np.array(budgets, dtype=np.single)        hp_curves = self.prepare_training_curves(real_budgets, hp_curves)        budgets = budgets / self.max_benchmark_epochs        real_budgets = np.array(real_budgets, dtype=np.single)        configurations = np.array(configurations, dtype=np.single)        configurations = torch.tensor(configurations)        configurations = configurations.to(device=self.dev)        budgets = torch.tensor(budgets)        budgets = budgets.to(device=self.dev)        hp_curves = torch.tensor(hp_curves)        hp_curves = hp_curves.to(device=self.dev)        network_real_budgets = torch.tensor(real_budgets / self.max_benchmark_epochs)        network_real_budgets.to(device=self.dev)        all_predictions = []        for model in self.models:            model = model.eval()            predictions = model(configurations, budgets, network_real_budgets, hp_curves)            all_predictions.append(predictions.detach().cpu().numpy())        mean_predictions = np.mean(all_predictions, axis=0)        std_predictions = np.std(all_predictions, axis=0)        return mean_predictions, std_predictions, hp_indices, real_budgets    def suggest(self) -> Tuple[int, int]:        \"\"\"Suggest a hyperparameter configuration and a budget        to evaluate.        Returns:            suggested_hp_index, budget: Tuple[int, int]                The index of the hyperparamter configuration to be evaluated                and the budget for what it is going to be evaluated for.        \"\"\"        suggest_time_start = time.time()        if self.initial_random_index < len(self.rand_init_conf_indices):            self.logger.info(                'Not enough configurations to build a model. \\n'                'Returning randomly sampled configuration'            )            suggested_hp_index = self.rand_init_conf_indices[self.initial_random_index]            budget = self.rand_init_budgets[self.initial_random_index]            self.initial_random_index += 1        else:            mean_predictions, std_predictions, hp_indices, real_budgets = self._predict()            best_prediction_index = self.find_suggested_config(                mean_predictions,                std_predictions,            )            # actually do the mapping between the configuration indices and the best prediction            # index            suggested_hp_index = hp_indices[best_prediction_index]            if suggested_hp_index in self.examples:                evaluated_budgets = self.examples[suggested_hp_index]                max_budget = max(evaluated_budgets)                budget = max_budget + self.fantasize_step                if budget > self.max_benchmark_epochs:                    budget = self.max_benchmark_epochs            else:                budget = self.fantasize_step        suggest_time_end = time.time()        self.suggest_time_duration = suggest_time_end - suggest_time_start        return suggested_hp_index, budget    def observe(        self,        hp_index: int,        b: int,        hp_curve: List[float],    ):        \"\"\"Receive information regarding the performance of a hyperparameter        configuration that was suggested.        Args:            hp_index: int                The index of the evaluated hyperparameter configuration.            b: int                The budget for which the hyperparameter configuration was evaluated.            hp_curve: List                The performance of the hyperparameter configuration.        \"\"\"        for index, curve_element in enumerate(hp_curve):            if np.isnan(curve_element):                self.diverged_configs.add(hp_index)                # only use the non-nan part of the curve and the corresponding                # budget to still have the information in the network                hp_curve = hp_curve[0:index + 1]                b = index                break        if not self.minimization:            hp_curve = np.subtract([self.max_value] * len(hp_curve), hp_curve)            hp_curve = hp_curve.tolist()        best_curve_value = min(hp_curve)        self.examples[hp_index] = np.arange(1, b + 1)        self.performances[hp_index] = hp_curve        if self.best_value_observed > best_curve_value:            self.best_value_observed = best_curve_value            self.no_improvement_patience = 0            self.logger.info(f'New Incumbent value found '                             f'{1 - best_curve_value if not self.minimization else best_curve_value}')        else:            self.no_improvement_patience += 1            if self.no_improvement_patience == self.no_improvement_threshold:                self.train = True                self.no_improvement_patience = 0                self.logger.info(                    'No improvement in the incumbent value threshold reached, '                    'restarting training from scratch'                )        initial_empty_value = self.get_mean_initial_value() if self.fill_value == 'last' else 0        if self.initial_random_index >= len(self.rand_init_conf_indices):            performance = self.performances[hp_index]            self.last_point = (hp_index, b, performance[b-1], performance[0:b-1] if b > 1 else [initial_empty_value])            if self.train:                # delete the previously stored models                self.models = []                if self.pretrain:                    # TODO Load the pregiven weights.                    pass                self._train_surrogate(pretrain=self.pretrain)                if self.iterations_counter <= self.initial_full_training_trials:                    self.train = True                else:                    self.train = False            else:                self.refine_counter += 1                self._refine_surrogate()    def prepare_examples(self, hp_indices: List) -> List:        \"\"\"        Prepare the examples to be given to the surrogate model.        Args:            hp_indices: List                The list of hp indices that are already evaluated.        Returns:            examples: List                A list of the hyperparameter configurations.        \"\"\"        examples = []        for hp_index in hp_indices:            examples.append(self.hp_candidates[hp_index])        return examples    def generate_candidate_configurations(self) -> Tuple[List, List, List, List, List]:        \"\"\"Generate candidate configurations that will be        fantasized upon.        Returns:            (configurations, hp_indices, hp_budgets, real_budgets, hp_curves): Tuple                A tuple of configurations, their indices in the hp list,                the budgets that they should be fantasized upon, the maximal                budgets they have been evaluated and their corresponding performance                curves.        \"\"\"        hp_indices = []        hp_budgets = []        hp_curves = []        real_budgets = []        initial_empty_value = self.get_mean_initial_value() if self.fill_value == 'last' else 0        for hp_index in range(0, self.hp_candidates.shape[0]):            if hp_index in self.examples:                budgets = self.examples[hp_index]                # Take the max budget evaluated for a certain hpc                max_budget = budgets[-1]                if max_budget == self.max_benchmark_epochs:                    continue                real_budgets.append(max_budget)                learning_curve = self.performances[hp_index]                hp_curve = learning_curve[0:max_budget-1] if max_budget > 1 else [initial_empty_value]            else:                real_budgets.append(1)                hp_curve = [initial_empty_value]            hp_indices.append(hp_index)            hp_budgets.append(self.max_benchmark_epochs)            hp_curves.append(hp_curve)        configurations = self.prepare_examples(hp_indices)        return configurations, hp_indices, hp_budgets, real_budgets, hp_curves    def history_configurations(self) -> Tuple[List, List, List, List]:        \"\"\"        Generate the configurations, labels, budgets and curves        based on the history of evaluated configurations.        Returns:            (train_examples, train_labels, train_budgets, train_curves): Tuple                A tuple of examples, labels and budgets for the                configurations evaluated so far.        \"\"\"        train_examples = []        train_labels = []        train_budgets = []        train_curves = []        initial_empty_value = self.get_mean_initial_value() if self.fill_value == 'last' else 0        for hp_index in self.examples:            budgets = self.examples[hp_index]            performances = self.performances[hp_index]            example = self.hp_candidates[hp_index]            for budget in budgets:                example_curve = performances[0:budget-1]                train_examples.append(example)                train_budgets.append(budget)                train_labels.append(performances[budget - 1])                train_curves.append(example_curve if len(example_curve) > 0 else [initial_empty_value])        return train_examples, train_labels, train_budgets, train_curves    @staticmethod    def acq(        best_values: np.ndarray,        mean_predictions: np.ndarray,        std_predictions: np.ndarray,        explore_factor: float = 0.25,        acq_choice: str = 'ei',    ) -> np.ndarray:        \"\"\"        Calculate the acquisition function based on the network predictions.        Args:        -----        best_values: np.ndarray            An array with the best value for every configuration.            Depending on the implementation it can be different for every            configuration.        mean_predictions: np.ndarray            The mean values of the model predictions.        std_predictions: np.ndarray            The standard deviation values of the model predictions.        explore_factor: float            The explore factor, when ucb is used as an acquisition            function.        acq_choice: str            The choice for the acquisition function to use.        Returns        -------        acq_values: np.ndarray            The values of the acquisition function for every configuration.        \"\"\"        if acq_choice == 'ei':            z = (np.subtract(best_values, mean_predictions))            difference = deepcopy(z)            not_zero_std_indicator = [False if example_std == 0.0 else True for example_std in std_predictions]            zero_std_indicator = np.invert(not_zero_std_indicator)            z = np.divide(z, std_predictions, where=not_zero_std_indicator)            np.place(z, zero_std_indicator, 0)            acq_values = np.add(np.multiply(difference, norm.cdf(z)), np.multiply(std_predictions, norm.pdf(z)))        elif acq_choice == 'ucb':            # we are working with error rates so we multiply the mean with -1            acq_values = np.add(-1 * mean_predictions, explore_factor * std_predictions)        elif acq_choice == 'thompson':            acq_values = np.random.normal(mean_predictions, std_predictions)        else:            acq_values = mean_predictions        return acq_values    def find_suggested_config(            self,            mean_predictions: np.ndarray,            mean_stds: np.ndarray,    ) -> int:        \"\"\"Return the hyperparameter with the highest acq function value.        Given the mean predictions and mean standard deviations from the DPL        ensemble for every hyperparameter configuraiton, return the hyperparameter        configuration that has the highest acquisition function value.        Args:            mean_predictions: np.ndarray                The mean predictions of the ensemble for every hyperparameter                configuration.            mean_stds: np.ndarray                The standard deviation predictions of the ensemble for every                hyperparameter configuration.        Returns:            max_value_index: int                the index of the maximal value.        \"\"\"        best_values = np.array([self.best_value_observed] * mean_predictions.shape[0])        acq_func_values = self.acq(            best_values,            mean_predictions,            mean_stds,            acq_choice='ei',        )        max_value_index = np.argmax(acq_func_values)        return max_value_index    def calculate_fidelity_ymax(self, fidelity: int) -> float:        \"\"\"Calculate the incumbent for a certain fidelity level.        Args:            fidelity: int                The given budget fidelity.        Returns:            best_value: float                The incumbent value for a certain fidelity level.        \"\"\"        config_values = []        for example_index in self.examples.keys():            try:                performance = self.performances[example_index][fidelity - 1]            except IndexError:                performance = self.performances[example_index][-1]            config_values.append(performance)        # lowest error corresponds to best value        best_value = min(config_values)        return best_value    def patch_curves_to_same_length(self, curves: List):        \"\"\"        Patch the given curves to the same length.        Finds the maximum curve length and patches all        other curves that are shorter with zeroes.        Args:            curves: List                The hyperparameter curves.        \"\"\"        for curve in curves:            difference = self.max_benchmark_epochs - len(curve) - 1            if difference > 0:                fill_value = [curve[-1]] if self.fill_value == 'last' else [0]                curve.extend(fill_value * difference)    def prepare_missing_values_channel(self, budgets: List) -> List:        \"\"\"Prepare an additional channel for learning curves.        The additional channel will represent an existing learning        curve value with a 1 and a missing learning curve value with        a 0.        Args:            budgets: List                A list of budgets for every training point.        Returns:            missing_value_curves: List                A list of curves representing existing or missing                values for the training curves of the training points.        \"\"\"        missing_value_curves = []        for i in range(len(budgets)):            budget = budgets[i]            budget = budget - 1            budget = int(budget)            if budget > 0:                example_curve = [1] * budget            else:                example_curve = []            difference_in_curve = self.max_benchmark_epochs - len(example_curve) - 1            if difference_in_curve > 0:                example_curve.extend([0] * difference_in_curve)            missing_value_curves.append(example_curve)        return missing_value_curves    def get_mean_initial_value(self):        \"\"\"Returns the mean initial value        for all hyperparameter configurations in the history so far.        Returns:            mean_initial_value: float                Mean initial value for all hyperparameter configurations                observed.        \"\"\"        first_values = []        for performance_curve in self.performances.values():            first_values.append(performance_curve[0])        mean_initial_value = np.mean(first_values)        return mean_initial_value    def prepare_training_curves(            self,            train_budgets: List[int],            train_curves: List[float]    ) -> np.ndarray:        \"\"\"Prepare the configuration performance curves for training.        For every configuration training curve, add an extra dimension        regarding the missing values, as well as extend the curve to have        a fixed uniform length for all.        Args:            train_budgets: List                A list of the budgets for all training points.            train_curves: List                A list of curves that pertain to every training point.        Returns:            train_curves: np.ndarray                The transformed training curves.        \"\"\"        missing_value_matrix = self.prepare_missing_values_channel(train_budgets)        self.patch_curves_to_same_length(train_curves)        train_curves = np.array(train_curves, dtype=np.single)        missing_value_matrix = np.array(missing_value_matrix, dtype=np.single)        # add depth dimension to the train_curves array and missing_value_matrix        train_curves = np.expand_dims(train_curves, 1)        missing_value_matrix = np.expand_dims(missing_value_matrix, 1)        train_curves = np.concatenate((train_curves, missing_value_matrix), axis=1)        return train_curvesfrom copy import deepcopyimport loggingimport osimport timefrom typing import List, Tupleimport numpy as npfrom scipy.stats import normimport torchfrom torch.utils.data import DataLoaderfrom data_loader.tabular_data_loader import WrappedDataLoaderfrom dataset.tabular_dataset import TabularDatasetfrom models.conditioned_power_law import ConditionedPowerLawclass PowerLawSurrogate:    def __init__(        self,        hp_candidates: np.ndarray,        surrogate_configs: dict = None,        seed: int = 11,        max_benchmark_epochs: int = 52,        ensemble_size: int = 5,        nr_epochs: int = 250,        fantasize_step: int = 1,        minimization: bool = True,        total_budget: int = 1000,        device: str = None,        output_path: str = '.',        dataset_name: str = 'unknown',        pretrain: bool = False,        backbone: str = 'power_law',        max_value: float = 100,        min_value: float = 0,        fill_value: str = 'zero',    ):        \"\"\"        Args:            hp_candidates: np.ndarray                The full list of hyperparameter candidates for a given dataset.            surrogate_configs: dict                The model configurations for the surrogate.            seed: int                The seed that will be used for the surrogate.            max_benchmark_epochs: int                The maximal budget that a hyperparameter configuration                has been evaluated in the benchmark for.            ensemble_size: int                The number of members in the ensemble.            nr_epochs: int                Number of epochs for which the surrogate should be                trained.            fantasize_step: int                The number of steps for which we are looking ahead to                evaluate the performance of a hpc.            minimization: bool                If for the evaluation metric, the lower the value the better.            total_budget: int                The total budget given. Used to calculate the initialization                percentage.            device: str                The device where the experiment will be run on.            output_path: str                The path where all the output will be stored.            dataset_name: str                The name of the dataset that the experiment will be run on.            pretrain: bool                If the surrogate will be pretrained before with a synthetic                curve.            backbone: str                The backbone, which can either be 'power_law' or 'nn'.            max_value: float                The maximal value for the dataset.            min_value: float                The minimal value for the dataset.            fill_value: str = 'zero',                The filling strategy for when learning curves are used.                Either 'zero' or 'last' where last represents the last value.        \"\"\"        torch.backends.cudnn.deterministic = True        torch.backends.cudnn.benchmark = False        self.total_budget = total_budget        self.fill_value = fill_value        self.max_value = max_value        self.min_value = min_value        self.backbone = backbone        self.pretrained_path = os.path.join(            output_path,            'power_law',            f'checkpoint_{seed}.pth',        )        self.model_instances = [            ConditionedPowerLaw,            ConditionedPowerLaw,            ConditionedPowerLaw,            ConditionedPowerLaw,            ConditionedPowerLaw,        ]        if device is None:            self.dev = torch.device(                'cuda') if torch.cuda.is_available() else torch.device('cpu')        else:            self.dev = torch.device(device)        self.learning_rate = 0.001        self.batch_size = 64        self.refine_batch_size = 64        self.criterion = torch.nn.L1Loss()        self.hp_candidates = hp_candidates        self.minimization = minimization        self.seed = seed        self.logger = logging.getLogger('power_law')        logging.basicConfig(            filename=f'power_law_surrogate_{dataset_name}_{seed}.log',            level=logging.INFO,            force=True,        )        # with what percentage configurations will be taken randomly instead of being sampled from the model        self.fraction_random_configs = 0.1        self.iteration_probabilities = np.random.rand(self.total_budget)        # the keys will be hyperparameter indices while the value        # will be a list with all the budgets evaluated for examples        # and with all performances for the performances        self.examples = dict()        self.performances = dict()        # set a seed already, so that it is deterministic when        # generating the seeds of the ensemble        torch.manual_seed(seed)        np.random.seed(seed)        self.seeds = np.random.choice(100, ensemble_size, replace=False)        self.max_benchmark_epochs = max_benchmark_epochs        self.ensemble_size = ensemble_size        self.nr_epochs = nr_epochs        self.refine_nr_epochs = 20        self.fantasize_step = fantasize_step        self.pretrain = pretrain        initial_configurations_nr = 1        conf_individual_budget = 1        init_conf_indices = np.random.choice(self.hp_candidates.shape[0], initial_configurations_nr, replace=False)        init_budgets = [i for i in range(1, conf_individual_budget + 1)]        self.rand_init_conf_indices = []        self.rand_init_budgets = []        # basically add every config index up to a certain budget threshold for the initialization        # we will go through both lists during the initialization        for config_index in init_conf_indices:            for config_budget in init_budgets:                self.rand_init_conf_indices.append(config_index)                self.rand_init_budgets.append(config_budget)        self.initial_random_index = 0        if surrogate_configs is None:            self.surrogate_configs = []            for i in range(0, self.ensemble_size):                self.surrogate_configs.append(                    {                        'nr_units': 128,                        'nr_layers': 2,                        'kernel_size': 3,                        'nr_filters': 4,                        'nr_cnn_layers': 2,                        'use_learning_curve': False,                    }                )        else:            self.surrogate_configs = surrogate_configs        self.nr_features = self.hp_candidates.shape[1]        self.best_value_observed = np.inf        self.diverged_configs = set()        # Where the models of the ensemble will be stored        self.models = []        # A tuple which will have the last evaluated point        # It will be used in the refining process        # Tuple(config_index, budget, performance, curve)        self.last_point = None        self.initial_full_training_trials = 10        # a flag if the surrogate should be trained        self.train = True        # the times it was refined        self.refine_counter = 0        # the surrogate iteration counter        self.iterations_counter = 0        # info dict to drop every surrogate iteration        self.info_dict = dict()        # the start time for the overhead of every surrogate iteration        # will be recorded here        self.suggest_time_duration = 0        self.output_path = output_path        self.dataset_name = dataset_name        self.no_improvement_threshold = int(self.max_benchmark_epochs + 0.2 * self.max_benchmark_epochs)        self.no_improvement_patience = 0    def _prepare_dataset(self) -> TabularDataset:        \"\"\"This method is called to prepare the necessary training dataset        for training a model.        Returns:            train_dataset: A dataset consisting of examples, labels, budgets                and learning curves.        \"\"\"        train_examples, train_labels, train_budgets, train_curves = self.history_configurations()        train_curves = self.prepare_training_curves(train_budgets, train_curves)        train_examples = np.array(train_examples, dtype=np.single)        train_labels = np.array(train_labels, dtype=np.single)        train_budgets = np.array(train_budgets, dtype=np.single)        # scale budgets to [0, 1]        train_budgets = train_budgets / self.max_benchmark_epochs        train_dataset = TabularDataset(            train_examples,            train_labels,            train_budgets,            train_curves,        )        return train_dataset    def _refine_surrogate(self):        \"\"\"Refine the surrogate model.        \"\"\"        for model_index, model_seed in enumerate(self.seeds):            train_dataset = self._prepare_dataset()            self.logger.info(f'Started refining model with index: {model_index}')            refined_model = self.train_pipeline(                model_index,                train_dataset,                nr_epochs=self.refine_nr_epochs,                refine=True,                weight_new_example=True,                batch_size=self.refine_batch_size,            )            self.models[model_index] = refined_model    def _train_surrogate(self, pretrain: bool = False):        \"\"\"Train the surrogate model.        Trains all the models of the ensemble        with different initializations and different        data orders.        Args:            pretrain: bool                If we have pretrained weights and we will just                refine the models.        \"\"\"        for model_index, model_seed in enumerate(self.seeds):            train_dataset = self._prepare_dataset()            self.logger.info(f'Started training model with index: {model_index}')            if pretrain:                # refine the models that were already pretrained                trained_model = self.train_pipeline(                    model_index,                    train_dataset,                    nr_epochs=self.refine_nr_epochs,                    refine=True,                    weight_new_example=False,                    batch_size=self.batch_size,                    early_stopping_it=self.refine_nr_epochs,  # basically no early stopping                )                self.models[model_index] = trained_model            else:                # train the models for the first time                trained_model = self.train_pipeline(                    model_index,                    train_dataset,                    nr_epochs=self.nr_epochs,                    refine=False,                    weight_new_example=False,                    batch_size=self.batch_size,                    early_stopping_it=self.nr_epochs,  # basically no early stopping                )                self.models.append(trained_model)    def train_pipeline(        self,        model_index: int,        train_dataset: TabularDataset,        nr_epochs: int,        refine: bool = False,        weight_new_example: bool = True,        batch_size: int = 64,        early_stopping_it: int = 10,        activate_early_stopping: bool = False,    ) -> torch.nn.Module:        \"\"\"Train an algorithm to predict the performance        of the hyperparameter configuration based on the budget.        Args:            model_index: int                The index of the model.            train_dataset: TabularDataset                The tabular dataset featuring the examples, labels,                budgets and curves.            nr_epochs: int                The number of epochs to train the model for.            refine: bool                If an existing model will be refined or if the training                will start from scratch.            weight_new_example: bool                If the last example that was added should be weighted more                by being included in every batch. This is only applicable                when refine is True.            batch_size: int                The batch size to be used for training.            early_stopping_it: int                The early stopping iteration patience.            activate_early_stopping: bool                Flag controlling the activation.        Returns:            model: torch.nn.Module                A trained model.        \"\"\"        if model_index == 0:            self.iterations_counter += 1            self.logger.info(f'Iteration number: {self.iterations_counter}')        surrogate_config = self.surrogate_configs[model_index]        seed = self.seeds[model_index]        torch.manual_seed(seed)        np.random.seed(seed)        if refine:            model = self.models[model_index]        else:            model = self.model_instances[model_index](                nr_initial_features=self.nr_features + 1 if self.backbone == 'nn' else self.nr_features,                nr_units=surrogate_config['nr_units'],                nr_layers=surrogate_config['nr_layers'],                use_learning_curve=surrogate_config['use_learning_curve'],                kernel_size=surrogate_config['kernel_size'],                nr_filters=surrogate_config['nr_filters'],                nr_cnn_layers=surrogate_config['nr_cnn_layers'],            )            model.to(self.dev)        # make the training dataset here        train_dataloader = DataLoader(            train_dataset,            batch_size=batch_size,            shuffle=True,        )        train_dataloader = WrappedDataLoader(train_dataloader, self.dev)        optimizer = torch.optim.Adam(model.parameters(), lr=self.learning_rate)        patience_rounds = 0        best_loss = np.inf        best_state = deepcopy(model.state_dict())        for epoch in range(0, nr_epochs):            running_loss = 0            model.train()            for batch_examples, batch_labels, batch_budgets, batch_curves in train_dataloader:                nr_examples_batch = batch_examples.shape[0]                # if only one example in the batch, skip the batch.                # Otherwise, the code will fail because of batchnormalization.                if nr_examples_batch == 1:                    continue                # zero the parameter gradients                optimizer.zero_grad(set_to_none=True)                # in case we are refining, we add the new example to every                # batch to give it more importance.                if refine and weight_new_example:                    newp_index, newp_budget, newp_performance, newp_curve = self.last_point                    new_example = np.array([self.hp_candidates[newp_index]], dtype=np.single)                    newp_missing_values = self.prepare_missing_values_channel([newp_budget])                    newp_budget = np.array([newp_budget], dtype=np.single) / self.max_benchmark_epochs                    newp_performance = np.array([newp_performance], dtype=np.single)                    modified_curve = deepcopy(newp_curve)                    difference = self.max_benchmark_epochs - len(modified_curve) - 1                    if difference > 0:                        modified_curve.extend([modified_curve[-1] if self.fill_value == 'last' else 0] * difference)                    modified_curve = np.array([modified_curve], dtype=np.single)                    newp_missing_values = np.array(newp_missing_values, dtype=np.single)                    # add depth dimension to the train_curves array and missing_value_matrix                    modified_curve = np.expand_dims(modified_curve, 1)                    newp_missing_values = np.expand_dims(newp_missing_values, 1)                    modified_curve = np.concatenate((modified_curve, newp_missing_values), axis=1)                    new_example = torch.tensor(new_example, device=self.dev)                    newp_budget = torch.tensor(newp_budget, device=self.dev)                    newp_performance = torch.tensor(newp_performance, device=self.dev)                    modified_curve = torch.tensor(modified_curve, device=self.dev)                    batch_examples = torch.cat((batch_examples, new_example))                    batch_budgets = torch.cat((batch_budgets, newp_budget))                    batch_labels = torch.cat((batch_labels, newp_performance))                    batch_curves = torch.cat((batch_curves, modified_curve))                outputs = model(batch_examples, batch_budgets, batch_budgets, batch_curves)                loss = self.criterion(outputs, batch_labels)                loss.backward()                optimizer.step()                # print statistics                running_loss += loss.item()            running_loss = running_loss / len(train_dataloader)            self.logger.info(f'Epoch {epoch +1}, Loss:{running_loss}')            if activate_early_stopping:                if running_loss < best_loss:                    best_state = deepcopy(model.state_dict())                    best_loss = running_loss                    patience_rounds = 0                elif running_loss > best_loss:                    patience_rounds += 1                    if patience_rounds == early_stopping_it:                        model.load_state_dict(best_state)                        self.logger.info(f'Stopping training since validation loss is not improving')                        break        if activate_early_stopping:            model.load_state_dict(best_state)        return model    def _predict(self) -> Tuple[np.ndarray, np.ndarray, List, np.ndarray]:        \"\"\"        Predict the performances of the hyperparameter configurations        as well as the standard deviations based on the ensemble.        Returns:            mean_predictions, std_predictions, hp_indices, real_budgets:            Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]                The mean predictions and the standard deviations over                all model predictions for the given hyperparameter                configurations with their associated indices and budgets.        \"\"\"        configurations, hp_indices, budgets, real_budgets, hp_curves = self.generate_candidate_configurations()        # scale budgets to [0, 1]        budgets = np.array(budgets, dtype=np.single)        hp_curves = self.prepare_training_curves(real_budgets, hp_curves)        budgets = budgets / self.max_benchmark_epochs        real_budgets = np.array(real_budgets, dtype=np.single)        configurations = np.array(configurations, dtype=np.single)        configurations = torch.tensor(configurations)        configurations = configurations.to(device=self.dev)        budgets = torch.tensor(budgets)        budgets = budgets.to(device=self.dev)        hp_curves = torch.tensor(hp_curves)        hp_curves = hp_curves.to(device=self.dev)        network_real_budgets = torch.tensor(real_budgets / self.max_benchmark_epochs)        network_real_budgets.to(device=self.dev)        all_predictions = []        for model in self.models:            model = model.eval()            predictions = model(configurations, budgets, network_real_budgets, hp_curves)            all_predictions.append(predictions.detach().cpu().numpy())        mean_predictions = np.mean(all_predictions, axis=0)        std_predictions = np.std(all_predictions, axis=0)        return mean_predictions, std_predictions, hp_indices, real_budgets    def suggest(self) -> Tuple[int, int]:        \"\"\"Suggest a hyperparameter configuration and a budget        to evaluate.        Returns:            suggested_hp_index, budget: Tuple[int, int]                The index of the hyperparamter configuration to be evaluated                and the budget for what it is going to be evaluated for.        \"\"\"        suggest_time_start = time.time()        if self.initial_random_index < len(self.rand_init_conf_indices):            self.logger.info(                'Not enough configurations to build a model. \\n'                'Returning randomly sampled configuration'            )            suggested_hp_index = self.rand_init_conf_indices[self.initial_random_index]            budget = self.rand_init_budgets[self.initial_random_index]            self.initial_random_index += 1        else:            mean_predictions, std_predictions, hp_indices, real_budgets = self._predict()            best_prediction_index = self.find_suggested_config(                mean_predictions,                std_predictions,            )            # actually do the mapping between the configuration indices and the best prediction            # index            suggested_hp_index = hp_indices[best_prediction_index]            if suggested_hp_index in self.examples:                evaluated_budgets = self.examples[suggested_hp_index]                max_budget = max(evaluated_budgets)                budget = max_budget + self.fantasize_step                if budget > self.max_benchmark_epochs:                    budget = self.max_benchmark_epochs            else:                budget = self.fantasize_step        suggest_time_end = time.time()        self.suggest_time_duration = suggest_time_end - suggest_time_start        return suggested_hp_index, budget    def observe(        self,        hp_index: int,        b: int,        hp_curve: List[float],    ):        \"\"\"Receive information regarding the performance of a hyperparameter        configuration that was suggested.        Args:            hp_index: int                The index of the evaluated hyperparameter configuration.            b: int                The budget for which the hyperparameter configuration was evaluated.            hp_curve: List                The performance of the hyperparameter configuration.        \"\"\"        for index, curve_element in enumerate(hp_curve):            if np.isnan(curve_element):                self.diverged_configs.add(hp_index)                # only use the non-nan part of the curve and the corresponding                # budget to still have the information in the network                hp_curve = hp_curve[0:index + 1]                b = index                break        if not self.minimization:            hp_curve = np.subtract([self.max_value] * len(hp_curve), hp_curve)            hp_curve = hp_curve.tolist()        best_curve_value = min(hp_curve)        self.examples[hp_index] = np.arange(1, b + 1)        self.performances[hp_index] = hp_curve        if self.best_value_observed > best_curve_value:            self.best_value_observed = best_curve_value            self.no_improvement_patience = 0            self.logger.info(f'New Incumbent value found '                             f'{1 - best_curve_value if not self.minimization else best_curve_value}')        else:            self.no_improvement_patience += 1            if self.no_improvement_patience == self.no_improvement_threshold:                self.train = True                self.no_improvement_patience = 0                self.logger.info(                    'No improvement in the incumbent value threshold reached, '                    'restarting training from scratch'                )        initial_empty_value = self.get_mean_initial_value() if self.fill_value == 'last' else 0        if self.initial_random_index >= len(self.rand_init_conf_indices):            performance = self.performances[hp_index]            self.last_point = (hp_index, b, performance[b-1], performance[0:b-1] if b > 1 else [initial_empty_value])            if self.train:                # delete the previously stored models                self.models = []                if self.pretrain:                    # TODO Load the pregiven weights.                    pass                self._train_surrogate(pretrain=self.pretrain)                if self.iterations_counter <= self.initial_full_training_trials:                    self.train = True                else:                    self.train = False            else:                self.refine_counter += 1                self._refine_surrogate()    def prepare_examples(self, hp_indices: List) -> List:        \"\"\"        Prepare the examples to be given to the surrogate model.        Args:            hp_indices: List                The list of hp indices that are already evaluated.        Returns:            examples: List                A list of the hyperparameter configurations.        \"\"\"        examples = []        for hp_index in hp_indices:            examples.append(self.hp_candidates[hp_index])        return examples    def generate_candidate_configurations(self) -> Tuple[List, List, List, List, List]:        \"\"\"Generate candidate configurations that will be        fantasized upon.        Returns:            (configurations, hp_indices, hp_budgets, real_budgets, hp_curves): Tuple                A tuple of configurations, their indices in the hp list,                the budgets that they should be fantasized upon, the maximal                budgets they have been evaluated and their corresponding performance                curves.        \"\"\"        hp_indices = []        hp_budgets = []        hp_curves = []        real_budgets = []        initial_empty_value = self.get_mean_initial_value() if self.fill_value == 'last' else 0        for hp_index in range(0, self.hp_candidates.shape[0]):            if hp_index in self.examples:                budgets = self.examples[hp_index]                # Take the max budget evaluated for a certain hpc                max_budget = budgets[-1]                if max_budget == self.max_benchmark_epochs:                    continue                real_budgets.append(max_budget)                learning_curve = self.performances[hp_index]                hp_curve = learning_curve[0:max_budget-1] if max_budget > 1 else [initial_empty_value]            else:                real_budgets.append(1)                hp_curve = [initial_empty_value]            hp_indices.append(hp_index)            hp_budgets.append(self.max_benchmark_epochs)            hp_curves.append(hp_curve)        configurations = self.prepare_examples(hp_indices)        return configurations, hp_indices, hp_budgets, real_budgets, hp_curves    def history_configurations(self) -> Tuple[List, List, List, List]:        \"\"\"        Generate the configurations, labels, budgets and curves        based on the history of evaluated configurations.        Returns:            (train_examples, train_labels, train_budgets, train_curves): Tuple                A tuple of examples, labels and budgets for the                configurations evaluated so far.        \"\"\"        train_examples = []        train_labels = []        train_budgets = []        train_curves = []        initial_empty_value = self.get_mean_initial_value() if self.fill_value == 'last' else 0        for hp_index in self.examples:            budgets = self.examples[hp_index]            performances = self.performances[hp_index]            example = self.hp_candidates[hp_index]            for budget in budgets:                example_curve = performances[0:budget-1]                train_examples.append(example)                train_budgets.append(budget)                train_labels.append(performances[budget - 1])                train_curves.append(example_curve if len(example_curve) > 0 else [initial_empty_value])        return train_examples, train_labels, train_budgets, train_curves    @staticmethod    def acq(        best_values: np.ndarray,        mean_predictions: np.ndarray,        std_predictions: np.ndarray,        explore_factor: float = 0.25,        acq_choice: str = 'ei',    ) -> np.ndarray:        \"\"\"        Calculate the acquisition function based on the network predictions.        Args:        -----        best_values: np.ndarray            An array with the best value for every configuration.            Depending on the implementation it can be different for every            configuration.        mean_predictions: np.ndarray            The mean values of the model predictions.        std_predictions: np.ndarray            The standard deviation values of the model predictions.        explore_factor: float            The explore factor, when ucb is used as an acquisition            function.        acq_choice: str            The choice for the acquisition function to use.        Returns        -------        acq_values: np.ndarray            The values of the acquisition function for every configuration.        \"\"\"        if acq_choice == 'ei':            z = (np.subtract(best_values, mean_predictions))            difference = deepcopy(z)            not_zero_std_indicator = [False if example_std == 0.0 else True for example_std in std_predictions]            zero_std_indicator = np.invert(not_zero_std_indicator)            z = np.divide(z, std_predictions, where=not_zero_std_indicator)            np.place(z, zero_std_indicator, 0)            acq_values = np.add(np.multiply(difference, norm.cdf(z)), np.multiply(std_predictions, norm.pdf(z)))        elif acq_choice == 'ucb':            # we are working with error rates so we multiply the mean with -1            acq_values = np.add(-1 * mean_predictions, explore_factor * std_predictions)        elif acq_choice == 'thompson':            acq_values = np.random.normal(mean_predictions, std_predictions)        else:            acq_values = mean_predictions        return acq_values    def find_suggested_config(            self,            mean_predictions: np.ndarray,            mean_stds: np.ndarray,    ) -> int:        \"\"\"Return the hyperparameter with the highest acq function value.        Given the mean predictions and mean standard deviations from the DPL        ensemble for every hyperparameter configuraiton, return the hyperparameter        configuration that has the highest acquisition function value.        Args:            mean_predictions: np.ndarray                The mean predictions of the ensemble for every hyperparameter                configuration.            mean_stds: np.ndarray                The standard deviation predictions of the ensemble for every                hyperparameter configuration.        Returns:            max_value_index: int                the index of the maximal value.        \"\"\"        best_values = np.array([self.best_value_observed] * mean_predictions.shape[0])        acq_func_values = self.acq(            best_values,            mean_predictions,            mean_stds,            acq_choice='ei',        )        max_value_index = np.argmax(acq_func_values)        return max_value_index    def calculate_fidelity_ymax(self, fidelity: int) -> float:        \"\"\"Calculate the incumbent for a certain fidelity level.        Args:            fidelity: int                The given budget fidelity.        Returns:            best_value: float                The incumbent value for a certain fidelity level.        \"\"\"        config_values = []        for example_index in self.examples.keys():            try:                performance = self.performances[example_index][fidelity - 1]            except IndexError:                performance = self.performances[example_index][-1]            config_values.append(performance)        # lowest error corresponds to best value        best_value = min(config_values)        return best_value    def patch_curves_to_same_length(self, curves: List):        \"\"\"        Patch the given curves to the same length.        Finds the maximum curve length and patches all        other curves that are shorter with zeroes.        Args:            curves: List                The hyperparameter curves.        \"\"\"        for curve in curves:            difference = self.max_benchmark_epochs - len(curve) - 1            if difference > 0:                fill_value = [curve[-1]] if self.fill_value == 'last' else [0]                curve.extend(fill_value * difference)    def prepare_missing_values_channel(self, budgets: List) -> List:        \"\"\"Prepare an additional channel for learning curves.        The additional channel will represent an existing learning        curve value with a 1 and a missing learning curve value with        a 0.        Args:            budgets: List                A list of budgets for every training point.        Returns:            missing_value_curves: List                A list of curves representing existing or missing                values for the training curves of the training points.        \"\"\"        missing_value_curves = []        for i in range(len(budgets)):            budget = budgets[i]            budget = budget - 1            budget = int(budget)            if budget > 0:                example_curve = [1] * budget            else:                example_curve = []            difference_in_curve = self.max_benchmark_epochs - len(example_curve) - 1            if difference_in_curve > 0:                example_curve.extend([0] * difference_in_curve)            missing_value_curves.append(example_curve)        return missing_value_curves    def get_mean_initial_value(self):        \"\"\"Returns the mean initial value        for all hyperparameter configurations in the history so far.        Returns:            mean_initial_value: float                Mean initial value for all hyperparameter configurations                observed.        \"\"\"        first_values = []        for performance_curve in self.performances.values():            first_values.append(performance_curve[0])        mean_initial_value = np.mean(first_values)        return mean_initial_value    def prepare_training_curves(            self,            train_budgets: List[int],            train_curves: List[float]    ) -> np.ndarray:        \"\"\"Prepare the configuration performance curves for training.        For every configuration training curve, add an extra dimension        regarding the missing values, as well as extend the curve to have        a fixed uniform length for all.        Args:            train_budgets: List                A list of the budgets for all training points.            train_curves: List                A list of curves that pertain to every training point.        Returns:            train_curves: np.ndarray                The transformed training curves.        \"\"\"        missing_value_matrix = self.prepare_missing_values_channel(train_budgets)        self.patch_curves_to_same_length(train_curves)        train_curves = np.array(train_curves, dtype=np.single)        missing_value_matrix = np.array(missing_value_matrix, dtype=np.single)        # add depth dimension to the train_curves array and missing_value_matrix        train_curves = np.expand_dims(train_curves, 1)        missing_value_matrix = np.expand_dims(missing_value_matrix, 1)        train_curves = np.concatenate((train_curves, missing_value_matrix), axis=1)        return train_curvesimport torchimport torch.nn as nnclass ConditionedPowerLaw(nn.Module):    def __init__(        self,        nr_initial_features=10,        nr_units=200,        nr_layers=3,        use_learning_curve: bool = True,        kernel_size: int = 3,        nr_filters: int = 4,        nr_cnn_layers: int = 2,    ):        \"\"\"        Args:            nr_initial_features: int                The number of features per example.            nr_units: int                The number of units for every layer.            nr_layers: int                The number of layers for the neural network.            use_learning_curve: bool                If the learning curve should be use in the network.            kernel_size: int                The size of the kernel that is applied in the cnn layer.            nr_filters: int                The number of filters that are used in the cnn layers.            nr_cnn_layers: int                The number of cnn layers to be used.        \"\"\"        super(ConditionedPowerLaw, self).__init__()        self.use_learning_curve = use_learning_curve        self.kernel_size = kernel_size        self.nr_filters = nr_filters        self.nr_cnn_layers = nr_cnn_layers        self.act_func = torch.nn.LeakyReLU()        self.last_act_func = torch.nn.GLU()        self.tan_func = torch.nn.Tanh()        self.batch_norm = torch.nn.BatchNorm1d        layers = []        # adding one since we concatenate the features with the budget        nr_initial_features = nr_initial_features        if self.use_learning_curve:            nr_initial_features = nr_initial_features + nr_filters        layers.append(nn.Linear(nr_initial_features, nr_units))        layers.append(self.act_func)        for i in range(2, nr_layers + 1):            layers.append(nn.Linear(nr_units, nr_units))            layers.append(self.act_func)        last_layer = nn.Linear(nr_units, 3)        layers.append(last_layer)        self.layers = torch.nn.Sequential(*layers)        cnn_part = []        if use_learning_curve:            cnn_part.append(                nn.Conv1d(                    in_channels=2,                    kernel_size=(self.kernel_size,),                    out_channels=self.nr_filters,                ),            )            for i in range(1, self.nr_cnn_layers):                cnn_part.append(self.act_func)                cnn_part.append(                    nn.Conv1d(                        in_channels=self.nr_filters,                        kernel_size=(self.kernel_size,),                        out_channels=self.nr_filters,                    ),                ),            cnn_part.append(nn.AdaptiveAvgPool1d(1))        self.cnn = nn.Sequential(*cnn_part)    def forward(        self,        x: torch.Tensor,        predict_budgets: torch.Tensor,        evaluated_budgets: torch.Tensor,        learning_curves: torch.Tensor,    ):        \"\"\"        Args:            x: torch.Tensor                The examples.            predict_budgets: torch.Tensor                The budgets for which the performance will be predicted for the                hyperparameter configurations.            evaluated_budgets: torch.Tensor                The budgets for which the hyperparameter configurations have been                evaluated so far.            learning_curves: torch.Tensor                The learning curves for the hyperparameter configurations.        \"\"\"        #x = torch.cat((x, torch.unsqueeze(evaluated_budgets, 1)), dim=1)        if self.use_learning_curve:            lc_features = self.cnn(learning_curves)            # revert the output from the cnn into nr_rows x nr_kernels.            lc_features = torch.squeeze(lc_features, 2)            x = torch.cat((x, lc_features), dim=1)        x = self.layers(x)        alphas = x[:, 0]        betas = x[:, 1]        gammas = x[:, 2]        output = torch.add(            alphas,            torch.mul(                self.last_act_func(torch.cat((betas, betas))),                torch.pow(                    predict_budgets,                    torch.mul(self.last_act_func(torch.cat((gammas, gammas))), -1)                )            ),        )        return output",
    "Experiment Result": "Ensemble size: 5. Surrogate model architecture: ConditionedPowerLaw with 2-layer feedforward networks, 128 units per layer. Activation functions: Leaky ReLU for hidden layers, GLU for beta and gamma output units. Training epochs: 250 for initial training, 20 epochs for refinement per HPO iteration. Loss function: L1 loss. Optimizer: Adam with a learning rate of 0.001. Batch size: 64 for both initial training and refinement. Acquisition function: Expected Improvement (EI). Multi-fidelity strategy: Incremental budget increase by 1 epoch. Initialization: 1 configuration evaluated for 1 epoch. Restart mechanism: Models are retrained from scratch if the incumbent value does not improve for `max_benchmark_epochs + 0.2 * max_benchmark_epochs` iterations."
}{
    "Title": "Hyperparameter Optimization through Neural Network Partitioning",
    "Main Contributions": "This paper proposes Partitioned Neural Networks, a novel and computationally efficient method for hyperparameter optimization (HPO) that does not require a validation set and operates within a single training run. Inspired by the marginal likelihood and the 'learning speed' perspective, the method partitions both the training data into K shards and the neural network parameters into K partitions. It optimizes hyperparameters based on an 'out-of-training-sample' loss of subnetworks (each trained on specific data subsets), which is more scalable and truthful in measuring generalization than prior approaches. Key contributions include demonstrating its effectiveness on various tasks such as differentiable input selection, learning affine data augmentations, optimizing general feature extractors, and learning dropout rates. The method significantly outperforms traditional validation-set-based HPO and prior marginal likelihood approximations, especially in low-data regimes and scales to larger architectures where other methods fail. Furthermore, it is shown to be highly beneficial for Federated Learning, enabling on-device HPO with reduced communication costs and no client-side validation data, leading to better-generalizing models.",
    "Methodology": "The core methodology revolves around optimizing an approximation to the marginal likelihood, LML (D,ψ) = C∑ k=1 Eqk-1(w) [log p(Dk|w,ψ)], with respect to hyperparameters ψ. This is achieved by partitioning the neural network's weights (w) into C distinct partitions (w1, ..., wC) and the training dataset (D) into C corresponding shards (D1, ..., DC). For each k, a subnetwork w(k)s is constructed using parameters w1 through wk, while parameters wk+1 through wC are set to default (e.g., initialization) values. This ensures that each subnetwork w(k)s has effectively only been trained on data from shards D1:k. The training procedure interleaves stochastic gradient updates: parameters wk are updated by optimizing the negative log-likelihood on data from D1:k using w(k)s, and hyperparameters ψ are updated using gradients derived from the 'out-of-training-sample' loss of subnetwork w(k-1)s on data shard Dk. This approach provides low-variance gradients for hyperparameters through standard backpropagation, avoiding computationally expensive Hessian computations found in other marginal likelihood approximations. For Federated Learning, clients are assigned to data chunks and compute gradients for the relevant parameter partitions and hyperparameters locally, with only the modified parameters communicated to the server, leading to reduced upload costs.",
    "Experimental Setup": "The method was evaluated across a diverse set of tasks and datasets. For **input selection**, a toy synthetic dataset with 15 informative and 15 spurious features was used with fully-connected MLPs. For **invariance learning through data augmentations**, MNIST, CIFAR10, and TinyImagenet datasets were utilized, along with rotated variants (RotMNIST, RotCIFAR10, RotTinyImagenet). Architectures included CNNs for MNIST, fixupResNets (ResNet-8, ResNet-14) for CIFAR10, and ResNet-50 with GroupNorm(2) for TinyImagenet. **Feature extractor learning** experiments used a Wide ResNet-20 on CIFAR10. **Federated Learning** experiments were conducted on non-i.i.d. splits of MNIST, RotMNIST, CIFAR10, and RotCIFAR10 across 100 clients, using a convolutional network for MNIST and a GroupNormalized ResNet-9 for CIFAR10, with learnable dropout. **Baselines** included standard training, Augerino, Differentiable Laplace, Last-layer Marginal Likelihood, traditional validation set optimization (with fine-tuning), and for FL, FedAvg and FedAvg + Augerino. **Validation** involved test accuracy, log-likelihood, and the proposed LML objective. Experiments also explored low-data regimes by using subsets of the training data and investigated sensitivity to partitioning schemes.",
    "Limitations": "The proposed method, while offering significant advantages, introduces its own set of limitations. Firstly, it requires an additional forward-backward pass to update the hyperparameters, which introduces some computational overhead, although this is empirically shown to be much less than existing marginal likelihood-based methods. Additionally, partitioned networks typically require more training iterations to converge. Secondly, the act of partitioning the network inherently constrains its capacity, which may result in some performance degradation compared to a fully optimized, non-partitioned network given ideal hyperparameters. Lastly, the partitioning strategy itself (including the number of chunks, and the relative proportions of data and parameters assigned to each) becomes an additional hyperparameter that may need careful tuning to achieve optimal performance, despite empirical evidence suggesting reasonable robustness to these choices.",
    "Future Research Directions": "Future research directions include exploring dynamic partitioning strategies for network parameters during training, rather than fixing them beforehand. Another promising area is to investigate alternative design choices for hyperparameter updates, such as accumulating gradients from different chunks or performing less frequent hyperparameter updates, to potentially further reduce computational overhead or variance. Furthermore, research could focus on developing strategies to alleviate the inherent performance loss caused by network partitioning, for example, by adjusting training rounds or proactively increasing the initial network capacity. The authors also express a general hope that their method will contribute to reducing the carbon footprint associated with hyperparameter search through repeated training.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "PASHA: Efficient HPO and NAS with Progressive Resource Allocation",
    "Main Contributions": "The paper addresses the challenge of high computational cost in Hyperparameter Optimization (HPO) and Neural Architecture Search (NAS), especially for models trained on large datasets. It proposes PASHA (Progressive Asynchronous Successive Halving), an extension of ASHA, that dynamically allocates maximum resources during the tuning procedure. The main contributions are: 1) Introducing PASHA, a new approach that dynamically selects the maximum resources for HPO or NAS, up to a certain budget. 2) Demonstrating empirically that PASHA significantly speeds up HPO and NAS (e.g., 2.3x to 15.5x faster than ASHA) without sacrificing predictive performance. 3) Showing that PASHA can be successfully combined with sample-efficient strategies like Bayesian Optimization, highlighting its generality.",
    "Methodology": "PASHA is an extension of ASHA, inspired by the 'doubling trick' concept. It starts with a small initial maximum amount of resources and progressively increases them only if the ranking of configurations in the top two rungs (rounds of promotion) has not stabilized. The key idea is to stop early when rankings are stable. To handle noise in the training process, PASHA employs a 'soft ranking' mechanism, where configurations are considered equivalent if their performance difference is smaller than a dynamically estimated value ϵ. This ϵ value is automatically estimated by identifying pairs of configurations that repeatedly swap their ranks across different resource levels (criss-crossing behavior) and calculating it as the N-th (90th in experiments) percentile of the performance differences of these criss-crossing pairs.",
    "Experimental Setup": "The evaluation consists of two phases: 1) Running the hyperparameter optimizer until 256 candidate configurations are evaluated. 2) Retraining the best identified configuration from scratch, typically on the training set (or combined training/validation in practice). Experiments were conducted on two sets of tasks: NAS problems using the NASBench201 benchmark (for CIFAR-10, CIFAR-100, and ImageNet16-120 datasets) and HPO problems on two large-scale tasks from the PD1 benchmark (WMT15 German-English and ImageNet, using xformer and ResNet50 models respectively). The experiments used 4 workers for parallel asynchronous evaluations, a default reduction factor η=3, and an N=90th percentile for ϵ calculation. Results were averaged over multiple random seeds (5 for scheduler, 3 for NASBench201; 5 for HPO on PD1). Baselines for comparison included ASHA, 'one-epoch baseline', 'random baseline', and additional two, three, and five-epoch baselines. PASHA was also tested with Bayesian Optimization searchers (MOBSTER) and various alternative ranking functions.",
    "Limitations": "The benefits of PASHA are reduced in scenarios where the number of rungs (resource levels, e.g., epochs) is small, as it provides fewer opportunities for the algorithm to interrupt tuning and achieve large speedups. This limitation was observed on benchmarks like LCBench. Public benchmarks often fix minimum and maximum resource levels, which can further constrain PASHA's ability to demonstrate its full potential. For practical usage, the authors recommend having a maximum amount of resources at least 100 times larger than the minimum amount when using a reduction factor η=3. This can be mitigated by defining resources with higher granularity, such as in terms of gradient updates instead of epochs.",
    "Future Research Directions": "Future work could investigate how the definition of rungs and resource levels impacts the decisions of multi-fidelity algorithms, including PASHA, to further optimize their performance. Additionally, the authors plan to test combinations of PASHA with transfer-learning techniques for multi-fidelity HPO and NAS, such as RUSH, to achieve even greater reductions in tuning time.",
    "Experiment Code": "class PASHARungSystem(PromotionRungSystem):\n    \"\"\"\n    Implements PASHA algorithm. It is very similar to ASHA, but it progressively\n    extends the maximum resources if the ranking in the top two current rungs changes.\n\n    A report introducing and evaluating the approach is available at\n    TODO: add link\n    \"\"\"\n\n    def __init__(\n        self,\n        rung_levels,\n        promote_quantiles,\n        metric,\n        mode,\n        resource_attr,\n        max_t,\n        ranking_criterion,\n        epsilon,\n        epsilon_scaling,\n    ):\n        super().__init__(\n            rung_levels, promote_quantiles, metric, mode, resource_attr, max_t\n        )\n        self.ranking_criterion = ranking_criterion\n        # define the index of the current top rung, starting from 1 for the lowest rung\n        #\n        self.current_rung_idx = 2\n        self.rung_levels = rung_levels\n\n        # initialize current maximum resources\n        self.current_max_t = rung_levels[self.current_rung_idx - 1]\n\n        self.epsilon = epsilon\n        self.epsilon_scaling = epsilon_scaling\n        if ranking_criterion == 'soft_ranking_auto':\n            self.per_epoch_results = {}\n            self.epoch_to_trials = {}\n            self.current_max_epoch = -1\n\n    # overriding the method in HB promotion to accomodate the increasing max resources level\n    def _effective_max_t(self):\n        return self.current_max_t\n\n    def _get_top_rungs_rankings(self, num_rungs=2):\n        \"\"\"\n        Look at the current top two rungs and get the rankings of the configurations.\n        The rungs can be empty, in which case we will return a list with 0 or 1 elements.\n        Normally the list will include rankings for both rungs.\n\n        The rankings are stored as a list of tuples (trial_id, rank, value).\n\n        Lower values have lower ranks, starting from zero. For example:\n        [('0', 0, 10.0), ('1', 3, 19.6), ('2', 2, 14.3), ('3', 1, 11.6)]\n\n        :param num_rungs: int describing how many top rungs to return\n        :return: rankings\n            List of at most two lists with tuple(trial_id, rank, score)\n        \"\"\"\n        rankings = []\n        # be careful, self._rungs is ordered with the highest resources level in the beginning\n        rungs = [self._rungs[-self.current_rung_idx + e] for e in range(num_rungs)]\n        for rung in rungs:\n            if rung.data != {}:\n                trial_ids = rung.data.keys()\n                values = []\n                for trial_id in trial_ids:\n                    values.append(rung.data[trial_id][0])\n                # order specifies where the value should be placed in the sorted list\n                values_order = np.array(values).argsort()\n                # calling argsort on the order will give us the ranking\n                values_ranking = values_order.argsort()\n                ranking = list(zip(trial_ids, values_ranking, values))\n\n                rankings.append(ranking)\n\n        return rankings\n\n    def _get_sorted_top_rungs(self, rankings):\n        \"\"\"\n        Sort the configurations in the top rung and the previous rung.\n        Filter out the configurations from the previous rung that\n        are not in the top rung.\n\n        :param rankings: list of at most two lists with tuple(trial_id, rank, score)\n        return: sorted_top_rung, sorted_previous_rung\n        \"\"\"\n        # filter only the relevant configurations from the earlier rung\n        top_rung_keys = set([e[0] for e in rankings[0]])\n        corresponding_previous_rung_trials = filter(\n            lambda e: e[0] in top_rung_keys, rankings[1]\n        )\n        # if we try to maximize the objective, we need to reverse the ranking\n        if self._mode == \"max\":\n            reverse = True\n        else:\n            reverse = False\n\n        sorted_top_rung = sorted(rankings[0], key=lambda e: e[1], reverse=reverse)\n        sorted_previous_rung = sorted(\n            corresponding_previous_rung_trials, key=lambda e: e[1], reverse=reverse\n        )\n        return sorted_top_rung, sorted_previous_rung\n\n    def _evaluate_soft_ranking(self, sorted_top_rung, sorted_previous_rung) -> bool:\n        \"\"\"\n        Soft ranking creates groups of similarly performing configurations\n        and increases the resources only if a configuration goes outside of\n        its group.\n\n        :param sorted_top_rung: list of tuple(trial_id, rank, score)\n        :param sorted_previous_rung: list of tuple(sorted_top_rung, rank, score)\n        :return: keep_current_budget\n        \"\"\"\n        keep_current_budget = True\n        if len(sorted_previous_rung) < 2:\n            epsilon = 0.0\n        elif self.ranking_criterion == \"soft_ranking_std\":\n            epsilon = (\n                np.std([e[2] for e in sorted_previous_rung]) * self.epsilon_scaling\n            )\n        elif (\n            self.ranking_criterion == \"soft_ranking_median_dst\"\n            or self.ranking_criterion == \"soft_ranking_mean_dst\"\n        ):\n            scores = [e[2] for e in sorted_previous_rung]\n            distances = [\n                abs(e1 - e2)\n                for idx1, e1 in enumerate(scores)\n                for idx2, e2 in enumerate(scores)\n                if idx1 != idx2\n            ]\n            if self.ranking_criterion == \"soft_ranking_mean_dst\":\n                epsilon = np.mean(distances) * self.epsilon_scaling\n            elif self.ranking_criterion == \"soft_ranking_median_dst\":\n                epsilon = np.median(distances) * self.epsilon_scaling\n            else:\n                raise ValueError(\n                    \"Ranking criterion {} is not supported\".format(\n                        self.ranking_criterion\n                    )\n                )\n        else:\n            epsilon = self.epsilon\n\n        # create groups of configurations with similar performance\n        previous_rung_groups = []\n        for idx, item in enumerate(sorted_previous_rung):\n            current_rung_group = [item[0]]\n            # add configurations that are after the current configuration\n            for idx_after in range(idx + 1, len(sorted_previous_rung)):\n                new_item = sorted_previous_rung[idx_after]\n\n                if self._mode == \"max\":\n                    if new_item[2] < item[2] - epsilon:\n                        break\n                else:\n                    if new_item[2] > item[2] + epsilon:\n                        break\n                current_rung_group.append(new_item[0])\n            # add configurations that are before the current configuration\n            for idx_before in range(idx - 1, -1, -1):\n                new_item = sorted_previous_rung[idx_before]\n                if self._mode == \"max\":\n                    if new_item[2] > item[2] + epsilon:\n                        break\n                else:\n                    if new_item[2] < item[2] - epsilon:\n                        break\n                current_rung_group.append(new_item[0])\n            previous_rung_groups.append(set(current_rung_group))\n\n        # evaluate if a configuration has switched its group\n        for idx, item in enumerate(sorted_top_rung):\n            if item[0] not in previous_rung_groups[idx]:\n                keep_current_budget = False\n                break\n\n        return keep_current_budget\n\n    def _update_epsilon(self):\n        \"\"\"\n        This function is used to automatically calculate the value of epsilon.\n        It finds the configurations which swapped their rankings across rungs\n        and estimates the value of epsilon as the 90th percentile of the difference\n        between their performance in the previous rung.\n\n        The original value of epsilon is kept if no suitable configurations were found.\n        \"\"\"\n\n        seen_pairs = set()\n        noisy_cfg_distances = []\n        top_epoch = min(self.current_max_epoch, self._rungs[-self.current_rung_idx].level)\n        bottom_epoch = min(self._rungs[-self.current_rung_idx+1].level, self.current_max_epoch)\n        for epoch in range(top_epoch, bottom_epoch, -1):\n            if len(self.epoch_to_trials[epoch]) > 1:\n                for pair in itertools.combinations(self.epoch_to_trials[epoch], 2):\n                    c1, c2 = pair[0], pair[1]\n                    if (c1, c2) not in seen_pairs:\n                        seen_pairs.add((c1, c2))\n                        p1, p2 = self.per_epoch_results[c1][epoch], self.per_epoch_results[c2][epoch]\n                        cond = p1 > p2\n\n                        opposite_order = False\n                        same_order_after_opposite = False\n                        # now we need to check the earlier epochs to see if at any point they had a different order\n                        for prev_epoch in range(epoch - 1, 0, -1):\n                            pp1, pp2 = self.per_epoch_results[c1][prev_epoch], self.per_epoch_results[c2][prev_epoch]\n                            p_cond = pp1 > pp2\n                            if p_cond == (not cond):\n                                opposite_order = True\n                            if opposite_order and p_cond == cond:\n                                same_order_after_opposite = True\n                                break\n\n                        if opposite_order and same_order_after_opposite:\n                            noisy_cfg_distances.append(abs(p1 - p2))\n\n        if len(noisy_cfg_distances) > 0:\n            self.epsilon = np.percentile(noisy_cfg_distances, 90)\n            if str(self.epsilon) == 'nan':\n                raise ValueError('Epsilon became nan') \n\n    def _update_per_epoch_results(self, trial_id, result):\n        if trial_id not in self.per_epoch_results:\n            self.per_epoch_results[trial_id] = {}\n        self.per_epoch_results[trial_id][result[self._resource_attr]] = result[self._metric]\n\n        if result[self._resource_attr] not in self.epoch_to_trials:\n            self.epoch_to_trials[result[self._resource_attr]] = set() \n        self.epoch_to_trials[result[self._resource_attr]].add(trial_id)\n\n        if result[self._resource_attr] > self.current_max_epoch:\n            self.current_max_epoch = result[self._resource_attr]\n\n    def _decide_resource_increase(self, rankings) -> bool:\n        \"\"\"\n        Decide if to increase the resources given the current rankings.\n        Currently we look at the rankings and if elements in the first list\n        have the same order also in the second list, we keep the current resource\n        budget. If the rankings are different, we will increase the budget.\n\n        The rankings can only be incorrect if we have rankings for both rungs.\n\n        :param rankings: list of at most two lists with tuple(trial_id, rank, score)\n        return: not keep_current_budget\n        \"\"\"\n        if len(rankings) == 2:\n            sorted_top_rung, sorted_previous_rung = self._get_sorted_top_rungs(rankings)\n        else:\n            return False\n\n        keep_current_budget = self._evaluate_soft_ranking(\n            sorted_top_rung, sorted_previous_rung\n        )\n\n        return not keep_current_budget\n\n    def on_task_report(self, trial_id: str, result: dict, skip_rungs: int) -> dict:\n        \"\"\"\n        Apart from calling the superclass method, we also check the rankings\n        and decides if to increase the current maximum resources.\n        \"\"\"\n        ret_dict = super().on_task_report(trial_id, result, skip_rungs)\n\n        if self.ranking_criterion == \"soft_ranking_auto\":\n            self._update_per_epoch_results(trial_id, result)\n            self._update_epsilon()\n\n        # check the rankings and decide if to increase the current maximum resources\n        rankings = self._get_top_rungs_rankings(num_rungs=2)\n        increase_resources = self._decide_resource_increase(rankings)\n\n        # we have a maximum amount of resources that PASHA can use\n        # the resources should not increase indefinitely\n        if increase_resources:\n            if self.current_rung_idx < len(self._rungs):\n                self.current_rung_idx += 1\n                # be careful, self.rung_levels is ordered with the highest resources level at the end\n                # moreover, since we use rung levels for counting both from the beginning and from the end of the list\n                # we need to remember that counting from the beginning it's zero indexed\n                self.current_max_t = self.rung_levels[self.current_rung_idx - 1]\n            else:\n                self.current_max_t = self.max_t\n\n        return ret_dict",
    "Experiment Result": "PASHA is an extension of ASHA that dynamically adjusts the maximum amount of resources (`current_max_t`) based on the stability of rankings between configurations in the top two rungs. The ranking stability is assessed using a 'soft ranking' mechanism, where configurations are considered equivalent if their performance difference is smaller than an `epsilon` value.\n\nKey configurable parameters for PASHA, set via `rung_system_kwargs` when initializing the `HyperbandScheduler` (with `type=\"pasha\"`), include:\n\n- **`ranking_criterion`**: Determines how ranking stability is evaluated. Supported options are:\n    - `'soft_ranking'`: Uses a fixed `epsilon` value.\n    - `'soft_ranking_std'`, `'soft_ranking_median_dst'`, `'soft_ranking_mean_dst'`: These dynamically estimate `epsilon` based on statistical properties (standard deviation, median distance, mean distance) of performance differences among configurations, with an optional `epsilon_scaling` factor.\n    - `'soft_ranking_auto'`: Dynamically estimates `epsilon` by identifying configurations that repeatedly swap ranks across different resource levels ('criss-crossing pairs'). `epsilon` is then calculated as the 90th percentile of the performance differences of these criss-crossing pairs. When this criterion is selected, a provided `epsilon` value in `rung_system_kwargs` is ignored.\n\n- **`epsilon`**: A float value used directly for the `'soft_ranking'` criterion to define the performance difference threshold for considering configurations equivalent.\n\n- **`epsilon_scaling`**: A float factor applied to the dynamically estimated `epsilon` when using criteria like `'soft_ranking_std'`, `'soft_ranking_median_dst'`, `'soft_ranking_mean_dst'`, or `'soft_ranking_auto'`.\n\n**Operational Details:**\n- The algorithm starts by considering the top two current rungs (`num_rungs=2` in `_get_top_rungs_rankings`).\n- Initially, `current_rung_idx` is set to 2, and `current_max_t` is set to `rung_levels[1]` (the second rung level from the lowest, assuming `rung_levels` is 0-indexed and sorted ascendingly for levels).\n- Resources (`current_max_t`) are increased by moving to the next rung level (`self.current_rung_idx += 1` and `self.current_max_t = self.rung_levels[self.current_rung_idx - 1]`) only if the ranking is deemed unstable (`_decide_resource_increase` returns True). This process continues up to the `max_t` defined for Hyperband.\n- When `ranking_criterion='soft_ranking_auto'`, the `_update_epsilon` method automatically computes `epsilon` as the 90th percentile of the absolute performance differences observed in configurations that exhibit 'criss-crossing' behavior (repeatedly swap ranks across different resource levels)."
}{
    "Title": "Reshuffling Resampling Splits Can Improve Generalization of Hyperparameter Optimization",
    "Main Contributions": "The paper addresses the standard practice in hyperparameter optimization (HPO) of using fixed resampling splits for evaluating configurations, and surprisingly demonstrates that reshuffling these splits for every configuration often improves the final model's generalization performance on unseen data. The key contributions include a theoretical analysis explaining how reshuffling impacts the asymptotic behavior of the validation loss surface and providing a bound on the expected regret, connecting benefits to signal and noise characteristics. This theoretical insight is confirmed through controlled simulation studies and practical usefulness is shown in large-scale HPO experiments. Reshuffling drastically improves results for single train-validation holdout protocols, making it competitive with standard cross-validation while being computationally cheaper.",
    "Methodology": "The methodology involves both theoretical analysis and empirical studies. Theoretically, the paper investigates how reshuffling affects the empirical loss surface by deriving the limiting distribution of the sequence of validation losses. It models the observed loss as a true loss plus a zero-mean Gaussian process with a covariance kernel that changes with reshuffling. A bound on the expected regret is derived, dependent on the loss surface's curvature, noise correlation, and a reshuffling parameter (τ). Empirically, a simulation study uses a univariate quadratic loss surface with a squared exponential kernel for the noise process to systematically vary curvature (m), noise correlation (κ), and reshuffling extent (τ). Benchmark experiments are conducted using random search, HEBO, and SMAC3 as HPO algorithms on various real-world tabular datasets and learning algorithms (CatBoost, XGBoost, Elastic Net, MLP) to measure generalization performance using ROC AUC, accuracy, and logloss. Resampling strategies like holdout, M-fold CV, M-fold holdout, and their reshuffled variants are compared.",
    "Experimental Setup": "For the simulation study, a univariate quadratic loss surface µ(λ) = m(λ - 0.5)^2/2 is minimized, combined with a squared exponential kernel K(λ, λ') = σ^2_K exp(-κ(λ - λ')^2/2) for the noise process ϵ. In each run, the observed objective is simulated, the minimizer identified, and its true risk calculated, repeated 10,000 times for various combinations of τ, m, and κ. For benchmark experiments, a subset of AutoML benchmark tabular datasets (10 DGPs with 10k-1M observations, <100 features) are used. Tasks are created by sampling train-validation sizes n ∈ {500, 1000, 5000} points from DGPs, with an additional 5000 points reserved for robust assessment of generalization error (outer test set). Learning algorithms include CatBoost, XGBoost, Elastic Net, and a funnel-shaped MLP, with detailed training pipelines and search spaces provided. Random search is performed with 500 HPC evaluations, while Bayesian optimization (HEBO, SMAC3) is run with 250 HPC evaluations. Resampling methods are 80/20 train-validation holdout, 5-fold CV, 5-fold holdout, and 5x 5-fold CV, each with fixed and reshuffled splits. Test performance is assessed by retraining the incumbent HPC on all available train/validation data and evaluating on the outer test set. Experiments are replicated 10 times, with metrics including ROC AUC, accuracy, and logloss.",
    "Limitations": "The theoretical analysis relies on an asymptotic approximation of the empirical loss surface, assuming Gaussian loss surfaces for tractability, which may not hold for general distributions. A loss stability assumption regarding learning algorithms is made, which is generally mild but can fail for highly sensitive losses (e.g., logloss at small sample sizes, where reshuffling sometimes hurts generalization). The focus is on generalization after search through a fixed, finite set of candidates, ignoring the dynamic nature of many HPO algorithms. Experiments are limited to tabular data and binary classification, avoiding extremely small or large datasets, which might affect the generalizability of findings to other data types or scales.",
    "Future Research Directions": "Future work could focus on developing a unified formal definition of 'oversearching', 'overtuning', or 'overfitting to the validation set' and thoroughly analyzing its relationship to validation performance measurements. Investigating less naive implementations of reshuffling to address its negative impact on highly sensitive losses like logloss is another direction. Further research into adaptive cross-validation (CV) techniques to reduce the computational burden of HPO while exploiting the benefits of more intensive resamplings is suggested. Finally, designing more advanced HPO algorithms that explicitly exploit the reshuffling effect, potentially in combination with existing methods like LOOCVCV or early stopping, could lead to further improvements. Exploring reshuffling's effect on multi-class datasets where less overtuning is expected is also mentioned.",
    "Experiment Code": "def simulate_gp(x: torch.Tensor, mu: callable, cov: callable) -> torch.Tensor:\n    \"\"\"\n    Simulate a Gaussian process.\n\n    :param x: A tensor of shape (n, d) where n is the number of points and d is the dimensionality of each point.\n    :param mu: Function to compute the mean of the Gaussian process.\n    :param cov: Function to compute the covariance matrix of the Gaussian process.\n\n    :return: A tensor of shape (n, d) representing the simulated Gaussian process.\n    \"\"\"\n    sigma_2 = 1e-5\n    n = x.shape[0]\n    d = x.shape[1]\n    mu_x = mu(x)\n    K_x = cov(x, x)\n    K_x += sigma_2 * torch.eye(n)\n    eigenvalues, eigenvectors = torch.linalg.eigh(K_x)\n    positive_eigenvalues = torch.clamp(eigenvalues, min=0)\n    sqrt_K_x = eigenvectors @ torch.diag(positive_eigenvalues.sqrt()) @ eigenvectors.T\n    z = torch.normal(0, 1, (n, d))\n    return mu_x + sqrt_K_x @ z\n\n\ndef mu_factory(x: torch.Tensor, alpha: float) -> torch.Tensor:\n    \"\"\"\n    The mean function of the Gaussian process.\n\n    :param x: A tensor of shape (n, d) where n is the number of points and d is the dimensionality of each point.\n    :param alpha: The alpha parameter of the mean function.\n\n    :return: A tensor of shape (n, 1) representing the mean of the Gaussian process at each point.\n    \"\"\"\n    values = alpha * (x - 0.5).pow(2)\n    return values\n\n\ndef cov_factory(\n    x1: torch.Tensor,\n    x2: torch.Tensor,\n    lengthscale: float,\n    tau: float = None,\n    shuffled: bool = False,\n) -> torch.Tensor:\n    \"\"\"\n    Vectorized computation of the covariance matrix of the Gaussian process.\n\n    :param x1: A tensor of shape (n, d) where n is the number of points and d is the dimensionality of each point.\n    :param x2: A tensor of shape (n, d) where n is the number of points and d is the dimensionality of each point.\n    :param lengthscale: The lengthscale parameter of the covariance function.\n    :param shuffled: Whether to assume a shuffled version of the covariance function.\n    :param tau: The tau parameter of the shuffled covariance function.\n\n    :return: A tensor of shape (n, n) representing the covariance matrix of the Gaussian process.\n    \"\"\"\n    sq_dist = torch.sum((x1[:, None, :] - x2[None, :, :]) ** 2, dim=-1)\n    K = torch.exp(-sq_dist / (2 * (lengthscale**2)))\n    if shuffled:\n        K = (1 - torch.eye(K.shape[0])) * (tau**2) * K + torch.eye(K.shape[0]) * K\n    return K\n\n\nif __name__ == \"__main__\":\n    # ... (code for argument parsing and setup)\n    # Relevant part of the simulation loop:\n    # Varying alpha (curvature), lengthscale (noise correlation), and tau (reshuffling extent)\n    # ...\n    taus = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n    # ...\n    for tau in taus:\n        # ... (inner loop for replicates)\n            y = simulate_gp(x, mu, cov)\n            y_shuffled = simulate_gp(x, mu, partial(cov, shuffled=True, tau=tau))\n            y_mu_y = mu(x[y.argmin(dim=0)])\n            y_mu_y_shuffled = mu(x[y_shuffled.argmin(dim=0)])\n            # ... (store results)\n\n\n# From reshufflebench/learner/learner_random_cv.py\nclass LearnerRandomCV(LearnerRandom):\n    # ... (init and other methods)\n\n    def objective(self, trial: Trial) -> float:\n        \"\"\"\n        Objective function for the optimization.\n        \"\"\"\n        # construct classifier pipeline\n        self.classifier.construct_pipeline(\n            trial,\n            refit=False,\n            cat_features=self.cat_features,\n            num_features=self.num_features,\n            n_train_samples=self.train_size,\n        )\n\n        if self.reshuffle:\n            self.cv = [\n                StratifiedKFold(\n                    n_splits=self.n_splits,\n                    shuffle=True,\n                    random_state=self.seed + (trial.number * 500000) + (i * 1000),\n                )\n                for i in range(self.n_repeats)\n            ]\n            self.cv_splits = []\n            for cv in self.cv:\n                self.cv_splits.append(\n                    list(cv.split(self.x_valid_train, self.y_valid_train))\n                )\n\n            # partition add_valid_use data into n_splits folds and repeat it n_repeats times\n            self.cv_add_valid = [\n                StratifiedKFold(\n                    n_splits=self.n_splits,\n                    shuffle=True,\n                    random_state=self.seed + (trial.number * 500000) + (i * 1000),\n                )\n                for i in range(self.n_repeats)\n            ]\n            self.cv_splits_add_valid = []\n            for cv in self.cv_add_valid:\n                self.cv_splits_add_valid.append(\n                    list(cv.split(self.x_add_valid_use, self.y_add_valid_use))\n                )\n\n        # ... (code for storing splits, y_train_hist, y_valid_hist, etc.)\n\n        # for each repeat and each fold fit the classifier and predict and compute metrics\n        # ...\n        for repeat in range(self.n_repeats):\n            for fold in range(self.n_splits):\n                train_index, valid_index = self.cv_splits[repeat][fold]\n                # ... (construct x_train, x_valid, y_train, y_valid)\n                # ... (construct x_add_valid, y_add_valid for additional validation)\n\n                self.classifier.fit(\n                    trial=trial,\n                    x_train=x_train,\n                    y_train=y_train,\n                    x_valid=x_valid,\n                    y_valid=y_valid,\n                    cat_features=self.cat_features,\n                )\n                # ... (predict and compute metrics for train, valid, add_valid, test sets)\n\n        # ... (code for storing predictions and metrics)\n\n        # refit on the train_valid set and predict on train_valid and test_retrained\n        self.classifier.construct_pipeline(\n            trial,\n            refit=True,\n            cat_features=self.cat_features,\n            num_features=self.num_features,\n        )\n        self.classifier.fit(\n            trial=trial,\n            x_train=self.x_valid_train,\n            y_train=self.y_valid_train,\n            cat_features=self.cat_features,\n        )\n\n        # ... (predict on valid_train and test_retrained, compute metrics)\n\n        self.classifier.reset()\n\n        return metrics_valid[\"accuracy\"]\n\n\n# From analyze/result_analyzer.py\nclass ResultAnalyzer(object):\n    # ... (init and other methods)\n\n    def calculate_curvature(self) -> None:\n        \"\"\"\n        Fit a GP on observed values and calculate some curvature metrics at the empirical optimum.\n        \"\"\"\n        for metric in self.params[\"metrics\"]:\n            dat = self.results_raw[metric]\n            relevant_columns_valid = [\n                column for column in dat.columns if \"params_\" in column\n            ] + [\"valid\"]\n            dat_valid = dat.loc[:, relevant_columns_valid]\n            dat_valid.rename(columns={\"valid\": \"y\"}, inplace=True)\n            X = dat_valid.drop(columns=[\"y\"])\n            y = dat_valid[\"y\"].values.reshape(-1, 1)\n            # ... (prepare search space and model config for GP)\n            model = get_model(\"gp\", space.num_numeric, space.num_categorical, 1, **model_config)\n            model.fit(X, Xe, y)\n\n            empirical_argmin = model.predict(X, Xe)[0].argmin()\n            X_argmin = X[empirical_argmin, :].unsqueeze(0)\n            Xe_argmin = Xe[empirical_argmin, :].unsqueeze(0)\n\n            def posterior_mean_wrapper(x, model, Xe_argmin):\n                x_tensor = torch.FloatTensor(x).unsqueeze(0).requires_grad_(True)\n                return model.predict(x_tensor, Xe_argmin)[0][0, 0].detach().numpy()\n\n            x0 = X[empirical_argmin, :].numpy()\n            result = opt.minimize(\n                posterior_mean_wrapper,\n                x0,\n                args=(model, Xe_argmin),\n                bounds=bounds,\n                method=\"Nelder-Mead\",\n            )\n\n            x_optimal = result.x\n            hessian_function = numdifftools.Hessian(posterior_mean_wrapper)\n            hessian_optimal = hessian_function(x_optimal, model, Xe_argmin)\n\n            def make_psd(matrix):\n                eigenvalues, eigenvectors = np.linalg.eigh(matrix)\n                already_is_psd = np.all(eigenvalues >= 0)\n                eigenvalues[eigenvalues < 0] = 0\n                return (\n                    already_is_psd,\n                    eigenvectors @ np.diag(eigenvalues) @ eigenvectors.T,\n                )\n\n            already_is_psd, hessian_optimal = make_psd(hessian_optimal)\n\n            det_hessian = np.linalg.det(hessian_optimal)\n            trace_hessian = np.trace(hessian_optimal)\n            eigenvalues_hessian = np.linalg.eigvals(hessian_optimal)\n            smallest_eigenvalue_hessian = np.min(eigenvalues_hessian)\n            biggest_eigenvalue_hessian = np.max(eigenvalues_hessian)\n\n            # ... (store curvature metrics)\n\n\n# From reshufflebench/metrics/metrics.py\ndef compute_accuracy(\n    y_true: np.array,\n    y_pred: np.array,\n    y_pred_proba: np.array,\n    labels: List[int],\n    multiclass: bool,\n) -> float:\n    \"\"\"\n    Compute accuracy score.\n    \"\"\"\n    return accuracy_score(y_true, y_pred)\n\n\ndef compute_balanced_accuracy(\n    y_true: np.array,\n    y_pred: np.array,\n    y_pred_proba: np.array,\n    labels: List[int],\n    multiclass: bool,\n) -> float:\n    \"\"\"\n    Compute balanced accuracy score.\n    \"\"\"\n    return balanced_accuracy_score(y_true, y_pred)\n\n\ndef compute_logloss(\n    y_true: np.array,\n    y_pred: np.array,\n    y_pred_proba: np.array,\n    labels: List[int],\n    multiclass: bool,\n) -> float:\n    \"\"\"\n    Compute logloss score.\n    \"\"\"\n    return log_loss(y_true, y_pred_proba, labels=labels)\n\n\ndef compute_auc(\n    y_true: np.array,\n    y_pred: np.array,\n    y_pred_proba: np.array,\n    labels: List[int],\n    multiclass: bool,\n) -> float:\n    \"\"\"\n    Compute AUC score.\n    \"\"\"\n    if multiclass:\n        return roc_auc_score(\n            y_true, y_pred_proba, average=\"macro\", multi_class=\"ovo\", labels=labels\n        )\n    else:\n        return roc_auc_score(y_true, y_pred_proba[:, 1])\n\n\ndef compute_metric(\n    y_true: np.array,\n    y_pred: np.array,\n    y_pred_proba: np.array,\n    metric: str,\n    labels: List[int],\n    multiclass: bool,\n) -> float:\n    \"\"\"\n    Compute a metric.\n    \"\"\"\n    if metric == \"accuracy\":\n        return compute_accuracy(\n            y_true,\n            y_pred=y_pred,\n            y_pred_proba=y_pred_proba,\n            labels=labels,\n            multiclass=multiclass,\n        )\n    elif metric == \"balanced_accuracy\":\n        return compute_balanced_accuracy(\n            y_true,\n            y_pred=y_pred,\n            y_pred_proba=y_pred_proba,\n            labels=labels,\n            multiclass=multiclass,\n        )\n    elif metric == \"logloss\":\n        return compute_logloss(\n            y_true,\n            y_pred=y_pred,\n            y_pred_proba=y_pred_proba,\n            labels=labels,\n            multiclass=multiclass,\n        )\n    elif metric == \"auc\":\n        return compute_auc(\n            y_true,\n            y_pred=y_pred,\n            y_pred_proba=y_pred_proba,\n            labels=labels,\n            multiclass=multiclass,\n        )\n    else:\n        raise ValueError(f\"Unknown metric: {metric}\")",
    "Experiment Result": "The methodology involves both theoretical analysis and empirical studies.The theoretical analysis investigates how reshuffling affects the empirical loss surface by deriving the limiting distribution of the sequence of validation losses. It models the observed loss as a true loss plus a zero-mean Gaussian process with a covariance kernel that changes with reshuffling. A bound on the expected regret is derived, dependent on the loss surface's curvature (m), noise correlation (κ), and a reshuffling parameter (τ).\n\nEmpirical Simulation Study Settings:\n- **Loss Surface**: Univariate quadratic loss surface, defined by `mu_factory(x, alpha) = alpha * (x - 0.5).pow(2)`, where `alpha` represents curvature.\n- **Noise Process**: Squared exponential kernel, defined by `cov_factory` with `lengthscale` influencing noise correlation.\n- **Reshuffling**: `tau` parameter in `cov_factory` controls reshuffling extent. If `shuffled=True`, `K = (1 - torch.eye(K.shape[0])) * (tau**2) * K + torch.eye(K.shape[0]) * K`.\n- **Parameters Varied**:\n  - `alpha` (curvature): [0.5, 1, 5, 10]\n  - `lengthscale` (noise correlation): [0.1, 0.5, 1, 5]\n  - `tau` (reshuffling extent): [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n- **Simulation Details**:\n  - Number of replicates (`n_replicates`): 10000 per (alpha, lengthscale, tau) combination.\n  - `x` domain: `torch.linspace(0, 1, 101)` (1D points).\n  - Gaussian Process noise variance (`sigma_2`): `1e-5`.\n\nBenchmark Experiment Settings:\n- **HPO Algorithms**: Random Search, HEBO, SMAC3.\n- **Learning Algorithms**: CatBoost, XGBoost, Elastic Net (Logistic Regression), MLP (FunnelMLP).\n- **Real-world Tabular Datasets (OpenML data_ids)**: [23517, 1169, 41147, 4135, 1461, 1590, 41150, 41162, 42733, 42742], plus two synthetic datasets (99999, 11111) for testing.\n- **Data Splitting**: Datasets split into training + validation, test, and additional validation sets.\n  - `train_valid_size`: [500, 1000, 5000]\n  - `test_size`: 5000\n  - `add_valid_size`: 5000\n- **Resampling Strategies**:\n  - **Holdout**: `valid_frac=0.2` (80% train, 20% validation).\n  - **M-fold Cross-Validation (M-fold CV)**: `n_splits=5`.\n    - For HEBO/SMAC, `n_repeats=1` by default (single 5-fold CV).\n    - For Random Search, `n_repeats=5` by default (5-times repeated 5-fold CV).\n  - **M-fold Holdout (Repeated Holdout)**: `valid_frac=0.2`, `n_repeats=5` (5-times repeated holdout, also referred to as Monte Carlo CV).\n  - All resampling strategies are compared with `reshuffle=True` and `reshuffle=False`.\n- **Number of Trials**:\n  - `n_trials=500` for Random Search.\n  - `n_trials=250` for HEBO and SMAC3.\n- **Seeds**: `range(42, 52)` (i.e., 42, 43, ..., 51) are used for reproducibility across experiments.\n- **Generalization Performance Metrics**: ROC AUC, Accuracy, Balanced Accuracy, LogLoss. Note: HEBO and SMAC3 were optimized using AUC as the primary metric."
}{
    "Title": "Differentiable Expected Hypervolume Improvement for Parallel Multi-Objective Bayesian Optimization",
    "Main Contributions": "The paper introduces q-Expected Hypervolume Improvement (qEHVI), a novel acquisition function for parallel and constrained multi-objective Bayesian optimization (MO BO). It provides an exact computation of joint EHVI (up to Monte-Carlo integration error) and leverages auto-differentiation to compute exact gradients of its Monte-Carlo estimator. This enables efficient optimization using first-order and quasi-second-order methods. The work demonstrates qEHVI's computational tractability and superior performance compared to state-of-the-art MO BO algorithms, often at a fraction of their wall time. Additionally, qEHVI supports auxiliary outcome constraints and proves theoretical convergence guarantees under the Sample Average Approximation (SAA) approach. It also shows how auto-differentiation can be used for exact gradients of analytic EHVI for more than two objectives.",
    "Methodology": "The methodology centers on `qEHVI`, an extension of Expected Hypervolume Improvement (EHVI). `qEHVI` calculates the joint Hypervolume Improvement (HVI) of `q` candidate points using box decompositions and the inclusion-exclusion principle to handle non-rectangular polytopes in the non-dominated space. The expected value of HVI is estimated via Monte-Carlo (MC) integration, sampling from the joint posterior of a Gaussian Process (GP) surrogate model; randomized quasi-MC methods are employed to reduce variance. Exact gradients of the MC estimator are computed using auto-differentiation and the re-parameterization trick, enabling gradient-based optimization. The Sample Average Approximation (SAA) approach is utilized for faster convergence with deterministic, higher-order optimizers. For constrained optimization, feasibility weighting is applied at the sample level, and indicator functions are replaced with differentiable sigmoid approximations to maintain differentiability.",
    "Experimental Setup": "The empirical evaluation uses both synthetic and real-world optimization problems. Synthetic benchmarks include the Branin-Currin problem (2 objectives, 2 dimensions), the C2-DTLZ2 constrained problem (2 objectives, 12 dimensions, 1 constraint), and DTLZ2 problems (2, 3, and 4 objectives, 6 dimensions). Real-world benchmarks are Structural Optimization in Automobile Safety Design (3 objectives, 5 design parameters) and Policy Optimization for Adaptive Bitrate Control (2 objectives, 4 parameters). Performance is measured by log hypervolume difference. Baselines include SMS-EGO, PESMO, TS-TCH, analytic EHVI, a novel qPAREGO extension, and a quasi-random baseline. All outcomes are modeled with independent Gaussian processes with Matern 5/2 ARD kernels. Initialization uses 2(d+1) points from a scrambled Sobol sequence. `qEHVI` and `qPAREGO` use N=128 QMC samples. Experiments compare optimization on CPU (2x Intel Xeon E5-2680 v4) and GPU (Tesla V100-SXM2-16GB). An additional experiment evaluates performance on a noisy Branin-Currin function with additive Gaussian noise. Approximate box decompositions with varying fidelity (ζ) are tested on DTLZ2 problems (3 and 4 objectives).",
    "Limitations": "One limitation is that `qEHVI` currently assumes noiseless observations, a common characteristic among existing EHVI formulations. Its scalability is constrained by the underlying partitioning algorithm, which limits its applicability to high-dimensional objective spaces (particularly for M >= 4). Furthermore, memory usage can become a challenge for large numbers of objectives (M) and batch sizes (q) when running on GPUs.",
    "Future Research Directions": "Future research could explore integrating over the uncertainty of previous observations to account for noise in the model. Another key direction is to investigate more scalable partitioning algorithms, including approximate or more efficient exact algorithms, to enhance `qEHVI`'s performance and reduce computation time in higher-dimensional objective spaces. The authors also hope this work encourages further application of modern computational paradigms and tooling to Bayesian optimization. Further theoretical work could focus on deriving results on the convergence rate of the optimizer for the SAA approach and extending current convergence results to randomized Quasi-Monte Carlo methods for base samples.",
    "Experiment Code": "from botorch.acquisition.multi_objective.base import MultiObjectiveMCAcquisitionFunction\nfrom botorch.acquisition.multi_objective.objective import MCMultiOutputObjective\nfrom botorch.models.model import Model\nfrom botorch.sampling.base import MCSampler\nfrom torch import Tensor\nfrom botorch.acquisition.logei import TAU_MAX, TAU_RELU\nfrom botorch.acquisition.multi_objective.base import MultiObjectiveMCAcquisitionFunction\nfrom botorch.acquisition.multi_objective.objective import MCMultiOutputObjective\nfrom botorch.models.model import Model\nfrom botorch.sampling.base import MCSampler\nfrom botorch.utils.multi_objective.box_decompositions.non_dominated import (\n    NondominatedPartitioning,\n)\nfrom botorch.utils.multi_objective.hypervolume import (\n    NoisyExpectedHypervolumeMixin,\n    SubsetIndexCachingMixin,\n)\nfrom botorch.utils.objective import compute_smoothed_feasibility_indicator\nfrom botorch.utils.safe_math import (\n    fatmin,\n    log_fatplus,\n    log_softplus,\n    logdiffexp,\n    logmeanexp,\n    logplusexp,\n    logsumexp,\n    smooth_amin,\n)\nfrom botorch.utils.transforms import (\n    average_over_ensemble_models,\n    concatenate_pending_points,\n    t_batch_mode_transform,\n)\nfrom torch import Tensor\nfrom botorch.exceptions.warnings import legacy_ei_numerics_warning\nfrom botorch.utils.multi_objective.box_decompositions.non_dominated import (\n    NondominatedPartitioning,\n)\nfrom botorch.utils.multi_objective.hypervolume import (\n    NoisyExpectedHypervolumeMixin,\n    SubsetIndexCachingMixin,\n)\nfrom botorch.utils.objective import compute_smoothed_feasibility_indicator\nfrom botorch.utils.transforms import (\n    average_over_ensemble_models,\n    concatenate_pending_points,\n    t_batch_mode_transform,\n)\nfrom torch import Tensor\n\n\nclass MultiObjectiveMCAcquisitionFunction(AcquisitionFunction, MCSamplerMixin, ABC):\n\n    _default_sample_shape = torch.Size([128])\n\n    def __init__(\n        self,\n        model: Model,\n        sampler: MCSampler | None = None,\n        objective: MCMultiOutputObjective | None = None,\n        constraints: list[Callable[[Tensor], Tensor]] | None = None,\n        eta: Tensor | float = 1e-3,\n        X_pending: Tensor | None = None,\n    ) -> None:\n        super().__init__(model=model)\n        MCSamplerMixin.__init__(self, sampler=sampler)\n        if objective is None:\n            objective = IdentityMCMultiOutputObjective()\n        elif not isinstance(objective, MCMultiOutputObjective):\n            raise UnsupportedError(\n                \"Only objectives of type MCMultiOutputObjective are supported for \"\n                \"Multi-Objective MC acquisition functions.\"\n            )\n        if (\n            hasattr(model, \"input_transform\")\n            and isinstance(model.input_transform, InputPerturbation)\n            and constraints is not None\n        ):\n            raise UnsupportedError(\n                \"Constraints are not supported with input perturbations, due to\"\n                \"sample q-batch shape being different than that of the inputs.\"\n                \"Use a composite objective that applies feasibility weighting to\"\n                \"samples before calculating the risk measure.\"\n            )\n        self.add_module(\"objective\", objective)\n        self.constraints = constraints\n        if constraints:\n            if type(eta) is not Tensor:\n                eta = torch.full((len(constraints),), eta)\n            self.register_buffer(\"eta\", eta)\n        self.X_pending = None\n        if X_pending is not None:\n            self.set_X_pending(X_pending)\n\n    @abstractmethod\n    def forward(self, X: Tensor) -> Tensor:\n        pass # pragma: no cover\n\n\nclass qExpectedHypervolumeImprovement(\n    MultiObjectiveMCAcquisitionFunction, SubsetIndexCachingMixin\n):\n    def __init__(\n        self,\n        model: Model,\n        ref_point: list[float] | Tensor,\n        partitioning: NondominatedPartitioning,\n        sampler: MCSampler | None = None,\n        objective: MCMultiOutputObjective | None = None,\n        constraints: list[Callable[[Tensor], Tensor]] | None = None,\n        X_pending: Tensor | None = None,\n        eta: Tensor | float = 1e-3,\n        fat: bool = False,\n    ) -> None:\n        legacy_ei_numerics_warning(legacy_name=type(self).__name__)\n        if len(ref_point) != partitioning.num_outcomes:\n            raise ValueError(\n                \"The length of the reference point must match the number of outcomes. \"\n                f\"Got ref_point with {len(ref_point)} elements, but expected \"\n                f\"{partitioning.num_outcomes}.\"\n            )\n        ref_point = torch.as_tensor(\n            ref_point,\n            dtype=partitioning.pareto_Y.dtype,\n            device=partitioning.pareto_Y.device,\n        )\n        super().__init__(\n            model=model,\n            sampler=sampler,\n            objective=objective,\n            constraints=constraints,\n            eta=eta,\n            X_pending=X_pending,\n        )\n        self.register_buffer(\"ref_point\", ref_point)\n        cell_bounds = partitioning.get_hypercell_bounds()\n        self.register_buffer(\"cell_lower_bounds\", cell_bounds[0])\n        self.register_buffer(\"cell_upper_bounds\", cell_bounds[1])\n        SubsetIndexCachingMixin.__init__(self)\n        self.fat = fat\n\n    def _compute_qehvi(self, samples: Tensor, X: Tensor | None = None) -> Tensor:\n        obj = self.objective(samples, X=X)\n        q = obj.shape[-2]\n        if self.constraints is not None:\n            feas_weights = compute_smoothed_feasibility_indicator(\n                constraints=self.constraints,\n                samples=samples,\n                eta=self.eta,\n                fat=self.fat,\n            ) # `sample_shape x batch-shape x q`\n        device = self.ref_point.device\n        q_subset_indices = self.compute_q_subset_indices(q_out=q, device=device)\n        batch_shape = obj.shape[:-2]\n        areas_per_segment = torch.zeros(\n            *batch_shape,\n            self.cell_lower_bounds.shape[-2],\n            dtype=obj.dtype,\n            device=device,\n        )\n        cell_batch_ndim = self.cell_lower_bounds.ndim - 2\n        sample_batch_view_shape = torch.Size(\n            [\n                batch_shape[0] if cell_batch_ndim > 0 else 1,\n                *[1 for _ in range(len(batch_shape) - max(cell_batch_ndim, 1))],\n                *self.cell_lower_bounds.shape[1:-2],\n            ]\n        )\n        view_shape = (\n            *sample_batch_view_shape,\n            self.cell_upper_bounds.shape[-2],\n            1,\n            self.cell_upper_bounds.shape[-1],\n        )\n        for i in range(1, self.q_out + 1):\n            q_choose_i = q_subset_indices[f\"q_choose_{i}\"]\n            obj_subsets = obj.index_select(dim=-2, index=q_choose_i.view(-1))\n            obj_subsets = obj_subsets.view(\n                obj.shape[:-2] + q_choose_i.shape + obj.shape[-1:]\n            )\n            overlap_vertices = obj_subsets.min(dim=-2).values\n            overlap_vertices = torch.min(\n                overlap_vertices.unsqueeze(-3), self.cell_upper_bounds.view(view_shape)\n            )\n            lengths_i = (\n                overlap_vertices - self.cell_lower_bounds.view(view_shape)\n            ).clamp_min(0.0)\n            areas_i = lengths_i.prod(dim=-1)\n            if self.constraints is not None:\n                feas_subsets = feas_weights.index_select(\n                    dim=-1, index=q_choose_i.view(-1)\n                ).view(feas_weights.shape[:-1] + q_choose_i.shape)\n                areas_i = areas_i * feas_subsets.unsqueeze(-3).prod(dim=-1)\n            areas_i = areas_i.sum(dim=-1)\n            areas_per_segment += (-1) ** (i + 1) * areas_i\n        return areas_per_segment.sum(dim=-1).mean(dim=0)\n\n    @concatenate_pending_points\n    @t_batch_mode_transform()\n    @average_over_ensemble_models\n    def forward(self, X: Tensor) -> Tensor:\n        posterior = self.model.posterior(X)\n        samples = self.get_posterior_samples(posterior)\n        return self._compute_qehvi(samples=samples, X=X)\n\n\nTAU_RELU = 1e-6\nTAU_MAX = 1e-2\n\n\nclass qLogExpectedHypervolumeImprovement(\n    MultiObjectiveMCAcquisitionFunction, SubsetIndexCachingMixin\n):\n    _log: bool = True\n\n    def __init__(\n        self,\n        model: Model,\n        ref_point: list[float] | Tensor,\n        partitioning: NondominatedPartitioning,\n        sampler: MCSampler | None = None,\n        objective: MCMultiOutputObjective | None = None,\n        constraints: list[Callable[[Tensor], Tensor]] | None = None,\n        X_pending: Tensor | None = None,\n        eta: Tensor | float = 1e-2,\n        fat: bool = True,\n        tau_relu: float = TAU_RELU,\n        tau_max: float = TAU_MAX,\n    ) -> None:\n        if len(ref_point) != partitioning.num_outcomes:\n            raise ValueError(\n                \"The dimensionality of the reference point must match the number of \"\n                f\"outcomes. Got ref_point with {len(ref_point)} elements, but expected \"\n                f\"{partitioning.num_outcomes}.\"\n            )\n        ref_point = torch.as_tensor(\n            ref_point,\n            dtype=partitioning.pareto_Y.dtype,\n            device=partitioning.pareto_Y.device,\n        )\n        super().__init__(\n            model=model,\n            sampler=sampler,\n            objective=objective,\n            constraints=constraints,\n            eta=eta,\n            X_pending=X_pending,\n        )\n        self.register_buffer(\"ref_point\", ref_point)\n        cell_bounds = partitioning.get_hypercell_bounds()\n        self.register_buffer(\"cell_lower_bounds\", cell_bounds[0])\n        self.register_buffer(\"cell_upper_bounds\", cell_bounds[1])\n        SubsetIndexCachingMixin.__init__(self)\n        self.tau_relu = tau_relu\n        self.tau_max = tau_max\n        self.fat = fat\n\n    def _compute_log_qehvi(self, samples: Tensor, X: Tensor | None = None) -> Tensor:\n        obj = self.objective(samples, X=X)  # mc_samples x batch_shape x q x m\n        q = obj.shape[-2]\n        if self.constraints is not None:\n            log_feas_weights = compute_smoothed_feasibility_indicator(\n                constraints=self.constraints,\n                samples=samples,\n                eta=self.eta,\n                log=True,\n                fat=self.fat,\n            )\n        device = self.ref_point.device\n        q_subset_indices = self.compute_q_subset_indices(q_out=q, device=device)\n        batch_shape = obj.shape[:-2]  # mc_samples x batch_shape\n        log_areas_per_segment = torch.full(\n            size=(\n                *batch_shape,\n                self.cell_lower_bounds.shape[-2],  # num_cells\n                2,  # for even and odd terms\n            ),\n            fill_value=-torch.inf,\n            dtype=obj.dtype,\n            device=device,\n        )\n\n        cell_batch_ndim = self.cell_lower_bounds.ndim - 2\n        sample_batch_view_shape = torch.Size(\n            [\n                batch_shape[0] if cell_batch_ndim > 0 else 1,\n                *[1 for _ in range(len(batch_shape) - max(cell_batch_ndim, 1))],\n                *self.cell_lower_bounds.shape[1:-2],\n            ]\n        )\n        view_shape = (\n            *sample_batch_view_shape,\n            self.cell_upper_bounds.shape[-2],  # num_cells\n            1,  # adding for q_choose_i dimension\n            self.cell_upper_bounds.shape[-1],  # num_objectives\n        )\n\n        for i in range(1, self.q_out + 1):\n            q_choose_i = q_subset_indices[f\"q_choose_{i}\"]  # q_choose_i x i\n            obj_subsets = obj.index_select(dim=-2, index=q_choose_i.view(-1))\n            obj_subsets = obj_subsets.view(\n                obj.shape[:-2] + q_choose_i.shape + obj.shape[-1:]\n            )\n\n            log_improvement_i = self._log_improvement(obj_subsets, view_shape)\n\n            log_improvement_i = self._smooth_min(\n                log_improvement_i,\n                dim=-2,\n            )  # mc_samples x batch_shape x num_cells x q_choose_i x m\n\n            log_lengths_i = self._log_cell_lengths(log_improvement_i, view_shape)\n\n            log_areas_i = log_lengths_i.sum(dim=-1)  # areas_i = lengths_i.prod(dim=-1)\n\n            if self.constraints is not None:\n                log_feas_subsets = log_feas_weights.index_select(\n                    dim=-1, index=q_choose_i.view(-1)\n                ).view(log_feas_weights.shape[:-1] + q_choose_i.shape)\n                log_areas_i = log_areas_i + log_feas_subsets.unsqueeze(-3).sum(dim=-1)\n\n            log_areas_i = logsumexp(log_areas_i, dim=-1)  # areas_i.sum(dim=-1)\n\n            log_areas_per_segment[..., i % 2] = logplusexp(\n                log_areas_per_segment[..., i % 2],\n                log_areas_i,\n            )\n\n        log_areas_per_segment = logdiffexp(\n            log_a=log_areas_per_segment[..., 0], log_b=log_areas_per_segment[..., 1]\n        )\n\n        return logmeanexp(logsumexp(log_areas_per_segment, dim=-1), dim=0)\n\n    def _log_improvement(\n        self, obj_subsets: Tensor, view_shape: tuple | torch.Size\n    ) -> Tensor:\n        obj_subsets = obj_subsets.unsqueeze(-4)\n        cell_lower_bounds = self.cell_lower_bounds.view(view_shape).unsqueeze(-3)\n        Z = obj_subsets - cell_lower_bounds\n        log_Zi = self._log_smooth_relu(Z)\n        return log_Zi  # mc_samples x batch_shape x num_cells x q_choose_i x i x m\n\n    def _log_cell_lengths(\n        self, log_improvement_i: Tensor, view_shape: tuple | torch.Size\n    ) -> Tensor:\n        cell_upper_bounds = self.cell_upper_bounds.clamp_max(\n            1e10 if log_improvement_i.dtype == torch.double else 1e8\n        )  # num_cells x num_objectives\n        log_cell_lengths = (\n            (cell_upper_bounds - self.cell_lower_bounds).log().view(view_shape)\n        )  # (mc_samples = 1) x (batch_shape = 1) x n_cells x (q_choose_i = 1) x m\n        return self._smooth_minimum(\n            log_improvement_i,\n            log_cell_lengths,\n        )\n\n    def _log_smooth_relu(self, X: Tensor) -> Tensor:\n        f = log_fatplus if self.fat else log_softplus\n        return f(X, tau=self.tau_relu)\n\n    def _smooth_min(self, X: Tensor, dim: int, keepdim: bool = False) -> Tensor:\n        f = fatmin if self.fat else smooth_amin\n        return f(X, tau=self.tau_max, dim=dim)\n\n    def _smooth_minimum(self, X: Tensor, Y: Tensor) -> Tensor:\n        XY = torch.stack(torch.broadcast_tensors(X, Y), dim=-1)\n        return self._smooth_min(XY, dim=-1, keepdim=False)\n\n    @concatenate_pending_points\n    @t_batch_mode_transform()\n    @average_over_ensemble_models\n    def forward(self, X: Tensor) -> Tensor:\n        posterior = self.model.posterior(X)\n        samples = self.get_posterior_samples(posterior)\n        return self._compute_log_qehvi(samples=samples, X=X)\n\n\ndef compute_smoothed_feasibility_indicator(\n    constraints: list[Callable[[Tensor], Tensor]] | None,\n    samples: Tensor,\n    eta: Tensor | float = 1e-3,\n    log: bool = False,\n    fat: bool = False,\n) -> Tensor:\n    if constraints is None:\n        return torch.zeros(samples.shape[:-2], device=samples.device, dtype=samples.dtype)\n\n    if not log:\n        if fat:\n            soft_clamp = fatplus\n        else:\n            soft_clamp = softplus\n        indicator = 1.0\n    else:\n        soft_clamp = log_fatplus if fat else log_softplus\n        indicator = 0.0\n\n    if type(eta) is not Tensor:\n        eta = torch.full((len(constraints),), eta, device=samples.device, dtype=samples.dtype)\n\n    for i, constraint in enumerate(constraints):\n        constrained_obj = -constraint(samples)\n        if not log:\n            indicator = indicator * soft_clamp(constrained_obj, beta=1.0 / eta[i])\n        else:\n            indicator = indicator + soft_clamp(constrained_obj, beta=1.0 / eta[i])\n    return indicator\n",
    "Experiment Result": "The methodology centers on `qEHVI`, an extension of Expected Hypervolume Improvement (EHVI), or its log-transformed variant `qLogEHVI`.\n\n**Hypervolume Improvement (HVI) Calculation:**\n-   `qEHVI` calculates the joint Hypervolume Improvement (HVI) of `q` candidate points using box decompositions and the inclusion-exclusion principle to handle non-rectangular polytopes in the non-dominated space.\n-   A `NondominatedPartitioning` or `FastNondominatedPartitioning` module is used to partition the non-dominated space.\n-   The approximation level `alpha` for partitioning is `0.0` (exact) for up to 4 objectives, `10^-3` for 5-6 objectives, and `10^-2` for more than 6 objectives (determined by `get_default_partitioning_alpha`).\n\n**Expected Value Estimation (Monte-Carlo Integration):**\n-   The expected value of HVI is estimated via Monte-Carlo (MC) integration.\n-   Sampling is performed from the joint posterior of a Gaussian Process (GP) surrogate model.\n-   Randomized quasi-MC methods (`SobolQMCNormalSampler`) are employed by default to reduce variance.\n-   The default number of MC samples (`mc_samples`) for multi-objective acquisition functions is `128` (e.g., in `construct_inputs_qEHVI`), although a general `get_acquisition_function` might default to `512`.\n-   The default sample shape for multi-objective MC acquisition functions is `torch.Size([128])`.\n\n**Gradient-Based Optimization:**\n-   Exact gradients of the MC estimator are computed using auto-differentiation and the re-parameterization trick.\n-   The Sample Average Approximation (SAA) approach is utilized for faster convergence with deterministic, higher-order optimizers.\n\n**Constrained Optimization:**\n-   Feasibility weighting is applied at the sample level.\n-   Indicator functions for constraints are replaced with differentiable sigmoid approximations.\n-   The temperature parameter `eta` for the sigmoid approximation is `1e-3` for `qEHVI` and `1e-2` for `qLogEHVI`.\n-   The `fat` parameter, which toggles the logarithmic/linear asymptotic behavior of the smooth approximation, is `False` for `qEHVI` and `True` for `qLogEHVI`.\n\n**Log-space Calculations (`qLogEHVI` specific):**\n-   `qLogEHVI` computes the logarithm of the Expected Hypervolume Improvement in a numerically robust manner.\n-   It uses specific temperature parameters for smooth approximations:\n    -   `TAU_RELU = 1e-6` for smoothing the ReLU function.\n    -   `TAU_MAX = 1e-2` for smoothing the `max` operator.\n\n**Global Numerical Settings (GPyTorch/LinearOperator):**\n-   Fast computations in `linear_operator` are turned off by default (`_fast_covar_root_decomposition`, `_fast_log_prob`, `_fast_solves` are `False`).\n-   `max_cholesky_size` and `max_eager_kernel_size` are set to `4096`.\n-   `cholesky_max_tries` is set to `6`.\n-   A warning is raised for legacy EI acquisition functions due to known numerical issues, recommending the use of their `LogEI` counterparts."
}{
    "Title": "Hyperparameter Optimization through Neural Network Partitioning",
    "Main Contributions": "The paper proposes a novel and efficient hyperparameter optimization method, called Partitioned Neural Networks, that is inspired by marginal likelihood and requires no validation data. It allows for optimizing a variety of hyperparameters (e.g., neural architecture elements, data augmentation strategies, dropout rates) in a single training run, significantly reducing computational expense compared to other marginal likelihood approximation methods for neural networks. The method also addresses the challenges of hyperparameter optimization in federated learning by reducing communication overhead and achieving better model generalization, especially in low-data regimes and for non-i.i.d. data.",
    "Methodology": "The core methodology involves partitioning the training data into K data shards and a neural network model into K parameter partitions. Each parameter partition is optimized only on specific data shards (D1:k for partition wk). By combining these partitions, subnetworks are formed. The hyperparameter optimization objective, LML, is defined as the 'out-of-training-sample' loss of a subnetwork, i.e., the loss on data shards unseen by that subnetwork (Dk for subnetwork trained on D1:k-1). This objective is an approximation to a lower-bound on the marginal likelihood. Parameter updates for each partition are interleaved with hyperparameter updates, computed via stochastic gradient descent. The network weights can be partitioned randomly or by assigning fixed proportions of a layer's outputs to each partition. Default weight values for unoptimized partitions are typically set to initialization values or zero.",
    "Experimental Setup": "The method was validated on several tasks: a toy input selection task (identifying informative features), learning invariances through data augmentations, optimizing feature extractors as hyperparameters, and hyperparameter optimization in federated learning. Datasets included MNIST, CIFAR10, TinyImagenet, and their rotated variants (rotCIFAR10, rotTinyImagenet, rotMNIST), often with reduced training data sizes. Architectures used were MLPs, Fixup ResNets (ResNet-8, ResNet-14), ResNet-50 with GroupNorm, Wide ResNet-20, and a CNN for MNIST. Baselines for comparison included standard training (no augmentations), Augerino, Differentiable Laplace, Last-layer ML, traditional training/validation split optimization (with fine-tuning), and FedAvg. Federated learning experiments involved non-i.i.d. data partitioning (label-skew and rotation-skew) across 100 clients.",
    "Limitations": "The method introduces additional computational costs due to the necessity of an extra forward-backward pass for hyperparameter updates, although these are generally lower than existing marginal likelihood methods. Partitioned networks typically require more training iterations to converge. The partitioning of the network inherently constrains its capacity, potentially leading to some performance loss compared to a full, non-partitioned network trained with optimal, known hyperparameters. The practitioner must also choose a partitioning strategy (number of chunks, relative data/parameter proportions), which introduces an additional hyperparameter, though empirical results suggest robustness to these choices.",
    "Future Research Directions": "Future work could explore alternative partitioning schemes to potentially side-step the necessity for a separate forward-backward pass for hyperparameters. Investigating methods to alleviate the performance loss caused by network partitioning, such as adjusting training rounds or increasing network capacity, is another direction. Dynamically partitioning network parameters during training could also be explored.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "Federated Hyperparameter Tuning: Challenges, Baselines, and Connections to Weight-Sharing",
    "Main Contributions": "The paper investigates federated hyperparameter tuning, identifying key challenges such as federated validation data, extreme resource limitations, and evaluating personalization. It introduces FedEx, a novel method that connects federated hyperparameter tuning to the neural architecture search technique of weight-sharing. FedEx accelerates tuning for widely-used federated optimization methods like FedAvg and its variants. Theoretically, a FedEx variant is shown to correctly tune the on-device learning rate in the setting of online convex optimization across devices. Empirically, FedEx outperforms natural baselines for federated hyperparameter tuning by several percentage points on Shakespeare, FEMNIST, and CIFAR-10 benchmarks, achieving higher accuracy using the same training budget.",
    "Methodology": "FedEx leverages a novel connection between hyperparameter tuning in Federated Learning (FL) and the weight-sharing paradigm from Neural Architecture Search (NAS). It formalizes the personalized FL objective as a single-level empirical risk minimization, enabling a stochastic relaxation approach similar to NAS. Instead of architectural hyperparameters, FedEx tunes local training hyperparameters (e.g., learning rate, momentum, epochs) by setting up a categorical distribution over a fixed number of sampled configurations (often drawn using a local perturbation scheme around an initial sample). This distribution is then updated using exponentiated gradient updates, alternating with standard SGD updates to the shared model weights. The method applies to FL algorithms decomposable into local training (Locc) and aggregation (Aggb) subroutines (e.g., FedAvg, FedProx, SCAFFOLD, Reptile). For theoretical guarantees, FedEx's approach is analyzed within the Average Regret-Upper-Bound Analysis (ARUBA) framework for online convex optimization.",
    "Experimental Setup": "Experiments instantiate FedEx on FedAvg, FedProx, and Reptile. It is compared against standard hyperparameter tuning baselines: Random Search (RS) and Successive Halving Algorithm (SHA), both when run alone and when wrapping FedEx. The tuned hyperparameters include server learning rate schedule and momentum, as well as local training parameters such as learning rate, momentum, weight-decay, number of local epochs, batch-size, dropout, and proximal regularization. Evaluations are performed on three standard FL benchmarks: Shakespeare (next-character prediction), FEMNIST (image classification), and CIFAR-10 (image classification). Both i.i.d. and non-i.i.d. data partitions are considered for Shakespeare and FEMNIST, while CIFAR-10 uses i.i.d. data. Performance is measured by final test error and online evaluation (test error as a function of communication rounds). Experiments utilize 4K communication rounds for Shakespeare and CIFAR-10 (at most 800 rounds per arm) and 2K for FEMNIST (at most 200 per arm). Confidence intervals are derived from 5 to 10 independent trials.",
    "Limitations": "FedEx is applicable only to FL algorithms that can be decomposed into local fine-tuning and aggregation routines. It requires wrapper algorithms (such as RS or SHA) to tune its own internal hyperparameters (e.g., the baseline λt for gradient estimation) and certain server-side FL hyperparameters (e.g., server learning rate). The local perturbation scheme, while useful for stability, can limit the size of the search space explored by each individual instance of FedEx. The theoretical analysis for tuning the step-size is conducted in a simplified setting (one client per round, online convex optimization), and the theoretical bound on average regret incurs an additional m^(1/3) factor compared to a full-information setting. The paper also acknowledges general privacy and fairness risks associated with FL applications, which are not directly addressed by FedEx.",
    "Future Research Directions": "Future research directions include extending FedEx to tune architectural hyperparameters, effectively enabling federated Neural Architecture Search (NAS). It can also be applied to tune initialization-based meta-learning algorithms such as MAML. While FedEx is naturally applicable to cross-silo settings, further dedicated exploration in this domain could be beneficial. Addressing the time-dependency of federated evaluation is another challenge not fully explored. Furthermore, any practical application of this work must consider and integrate tools being developed by the community for mitigating privacy and fairness issues in FL.",
    "Experiment Code": "class FedEx:\n    '''runs hyperparameter optimization given a federated learning server'''\n\n    def entropy(self):\n\n        entropy = 0.0\n        for probs in product(*(theta[theta>0.0] for theta in self._theta)):\n            prob = np.prod(probs)\n            entropy -= prob * np.log(prob)\n        return entropy\n\n    def mle(self):\n    \n        return np.prod([theta.max() for theta in self._theta])\n\n    def __init__(\n                 self, \n                 server, \n                 configs, \n                 eta0='auto', \n                 sched='auto', \n                 cutoff=0.0, \n                 baseline=0.0, \n                 diff=False,\n                 ):\n        '''\n        Args:\n            server: Object that implements two methods, 'communication_round' and 'full_evaluation'\n                    taking as input a single argument, 'get_config', itself a function that takes \n                    no inputs and outputs an element of the provided list 'configs'. \n                    - 'communication_round' samples a batch of clients, assigns a config to each \n                    using 'get_config', and runs local training using that config. It then \n                    aggregates the local models to to take a training step and returns three lists \n                    or arrays: a list of each client's validation error before local training, a \n                    list of each client's validation error after local training, and a list of each \n                    client's weight (e.g. size of its validation set). \n                    - 'full_evaluation' assigns a config to each client using 'get_config' and runs\n                    local training using that config. It then returns three lists or arrays: a list\n                    of each client's test error before local training, a list of each client's test\n                    error after local training, and a list of each client's weight (e.g. size of \n                    its test set).\n            configs: list of configs used for local training and testing by 'server' \n                     OR dict of (string, list) pairs denoting a grid of configs\n            eta0: base exponentiated gradient step size; if 'auto' uses sqrt(2*log(len(configs)))\n            sched: learning rate schedule for exponentiated gradient:\n                    - 'adaptive': uses eta0 / sqrt(sum of squared gradient l-infinity norms)\n                    - 'aggressive': uses eta0 / gradient l-infinity norm\n                    - 'auto': uses eta0 / sqrt(t) for t the number of rounds\n                    - 'constant': uses eta0\n                    - 'scale': uses sched * sqrt(2 * log(len(configs)))\n            cutoff: entropy level below which to stop updating the config probability and use MLE\n            baseline: discount factor when computing baseline; 0.0 is most recent, 1.0 is mean\n            diff: if True uses performance difference; otherwise uses absolute performance\n        '''\n\n        self._server = server\n        self._configs = configs\n        self._grid = [] if type(configs) == list else sorted(configs.keys())\n\n        sizes = [len(configs[param]) for param in self._grid] if self._grid else [len(configs)]\n        self._eta0 = [np.sqrt(2.0 * np.log(size)) if eta0 == 'auto' else eta0 for size in sizes]\n        self._sched = sched\n        self._cutoff = cutoff\n        self._baseline = baseline\n        self._diff = diff\n        self._z = [np.full(size, -np.log(size)) for size in sizes]\n        self._theta = [np.exp(z) for z in self._z]\n\n        self._store = [0.0 for _ in sizes]\n        self._stopped = False\n        self._trace = {'global': [], 'refine': [], 'entropy': [self.entropy()], 'mle': [self.mle()]}\n\n    def stop(self):\n\n        self._stopped = True\n\n    def sample(self, mle=False, _index=[]):\n        '''samples from configs using current probability vector'''\n\n        if mle or self._stopped:\n            if self._grid:\n                return {param: self._configs[param][theta.argmax()] \n                        for theta, param in zip(self._theta, self._grid)}\n            return self._configs[self._theta[0].argmax()]\n        _index.append([np.random.choice(len(theta), p=theta) for theta in self._theta])\n\n        if self._grid:\n            return {param: self._configs[param][i] for i, param in zip(_index[-1], self._grid)}\n        return self._configs[_index[-1][0]]\n\n    def settings(self):\n        '''returns FedEx input settings'''\n\n        output = {'configs': deepcopy(self._configs)}\n        output['eta0'], output['sched'] = self._eta0, self._sched\n        output['cutoff'], output['baseline'] = self._cutoff, self._baseline \n        if self._trace['refine']:\n            output['theta'] = self.theta()\n        return output\n\n    def step(self):\n        '''takes exponentiated gradient step (calls 'communication_round' once)'''\n\n        index = []\n        before, after, weight = self._server.communication_round(lambda: self.sample(_index=index))        \n        before, after = np.array(before), np.array(after)\n        weight = np.array(weight, dtype=np.float64) / sum(weight)\n\n        if self._trace['refine']:\n            trace = self.trace('refine')\n            if self._diff:\n                trace -= self.trace('global')\n            baseline = discounted_mean(trace, self._baseline)\n        else:\n            baseline = 0.0\n        self._trace['global'].append(np.inner(before, weight))\n        self._trace['refine'].append(np.inner(after, weight))\n        if not index:\n            self._trace['entropy'].append(0.0)\n            self._trace['mle'].append(1.0)\n            return\n\n        for i, (z, theta) in enumerate(zip(self._z, self._theta)):\n            grad = np.zeros(len(z))\n            for idx, s, w in zip(index, after-before if self._diff else after, weight):\n                grad[idx[i]] += w * (s - baseline) / theta[idx[i]]\n            if self._sched == 'adaptive':\n                self._store[i] += norm(grad, float('inf')) ** 2\n                denom = np.sqrt(self._store[i])\n            elif self._sched == 'aggressive':\n                denom = 1.0 if np.all(grad == 0.0) else norm(grad, float('inf'))\n            elif self._sched == 'auto':\n                self._store[i] += 1.0\n                denom = np.sqrt(self._store[i])\n            elif self._sched == 'constant':\n                denom = 1.0\n            elif self._sched == 'scale':\n                denom = 1.0 / np.sqrt(2.0 * np.log(len(grad))) if len(grad) > 1 else float('inf')\n            else:\n                raise NotImplementedError\n            eta = self._eta0[i] / denom\n            z -= eta * grad\n            z -= logsumexp(z)\n            self._theta[i] = np.exp(z)\n\n        self._trace['entropy'].append(self.entropy())\n        self._trace['mle'].append(self.mle())\n        if self._trace['entropy'][-1] < self._cutoff:\n            self.stop()\n\n    def test(self, mle=False):\n        '''evaluates found config (calls 'full_evaluation' once)\n        Args:\n            mle: use MLE config instead of sampling\n        Returns:\n            output of 'full_evaluation'\n        '''\n\n        before, after, weight = self._server.full_evaluation(lambda: self.sample(mle=mle))\n        return {'global': np.inner(before, weight) / weight.sum(),\n                'refine': np.inner(after, weight) / weight.sum()}\n\n    def theta(self):\n        '''returns copy of config probability vector'''\n\n        return deepcopy(self._theta)\n\n    def trace(self, key):\n        '''returns trace of one of three tracked quantities\n        Args:\n            key: 'entropy', 'global', or 'refine'\n        Returns:\n            numpy vector with length equal to number of calls to 'step'\n        '''\n\n        return np.array(self._trace[key])\n\ndef wrapped_fedex(\n                  get_server,\n                  get_client,\n                  num_configs=1,\n                  prod=False,\n                  stepsize_init='auto', \n                  stepsize_sched='aggressive', \n                  cutoff=1E-4, \n                  baseline_discount=-1.0, \n                  diff=False,\n                  mle=False, \n                  logdir=None,\n                  val_discount=0.0, \n                  last_stop=False,\n                  eval_global=False,\n                  **kwargs,\n                  ):\n    '''evaluates FedEx wrapped with successive elimination algorithm;\n       uses FedAvg when num_configs = 1 and prod = False\n    Args:\n        get_server: function that takes no input and returns an object that can be passed as the \n                    first argument to FedEx.__init__, e.g. a Server object\n        get_client: function that takes no input and returns a dict of local training configs, a\n                    list of which is passed as the second argument to 'FedEx.__init__'; can also\n                    return a dict of (string, list) pairs to be passed directly to 'FedEx.__init__'\n        num_configs: determines number of configs in the list passed to 'FedEx.__init__':\n                     - >0: use this value directly\n                     - =0: value drawn at random from Unif[1, number of arms given by the wrapper]\n                     - =-1: use the number of arms given by the wrapper\n                     - else: value drawn at random from Unif{1, ..., abs(num_configs)}\n        prod: run FedEx over a product set of single-parameter grids; must be 'True' in the case\n                  when 'get_client' returns an object to be passed directly to 'FedEx.__init__'\n        stepsize_init: passed to 'eta0' kwarg of 'FedEx.__init__'\n        stepsize_sched: passed to 'sched' kwarg of 'FedEx.__init__'\n        baseline_discount: determines 'baseline' kwarg of 'FedEx.__init__':\n                           - >0.0: use this value directly\n                           - else: value drawn at random from Unif[0.0, abs(baseline_discount)]\n        diff: passed to 'diff' kwarg of 'FedEx.__init__'\n        mle: passed to 'mle' kwarg of 'FedEx.test' via the kwargs of 'successive_elimination'\n        logdir: passed to 'logdir' kwarg of 'successive_elimination'\n        val_discount: passed to 'val_discount' kwarg of 'successive_elimination'\n        last_stop: if True sets 'last_round' kwarg of 'successive_elimination' to 'stop'\n        kwargs: passed to 'get_schedule'\n    Returns:\n        FedEx object\n    '''\n\n    elim_rate, elim_sched, eval_sched = get_schedule(**kwargs)\n    print('Wrapping with', 'random search' if len(elim_sched) == 1 else 'successive elimination')\n\n    if num_configs < -1:\n        samples = lambda n: random.randint(1, -num_configs)\n    elif num_configs == -1:\n        samples = lambda n: n\n    elif num_configs == 0:\n        samples = lambda n: random.randint(1, n)\n    else:\n        samples = lambda n: num_configs\n\n    if baseline_discount < 0.0:\n        baseline = lambda: random.uniform(0.0, -baseline_discount)\n    else:\n        baseline = lambda: baseline_discount\n\n    def sampler(n):\n\n        for _ in range(n):\n            yield FedEx(\n                        get_server(), \n                        get_client() if prod else get_client(samples(n)),\n                        eta0=stepsize_init, \n                        sched=stepsize_sched, \n                        cutoff=cutoff, \n                        baseline=baseline(),\n                        diff=diff,\n                        )\n\n    return successive_elimination(\n                                  sampler, \n                                  ['refine', 'global'], \n                                  logdir=logdir, \n                                  val_discount=val_discount,\n                                  elim_rate=elim_rate, \n                                  elim_sched=elim_sched, \n                                  eval_sched=eval_sched,\n                                  traces=['entropy', 'mle', 'global', 'refine'], \n                                  last_round='stop' if last_stop else None,\n                                  mle=mle,\n                                  eval_global=eval_global,\n                                  )\n\ndef get_client(n_clients=1):\n    '''performs local tuning for each hyperparameter'''\n    # Example from cifar.py, similar in other dataset files\n    if args.lr_only:\n        return [SIMPLE_CLIENT()]\n\n    initial_client = CLIENT()\n    client_arr = [initial_client]\n    eps = args.eps\n\n    for i in range(n_clients-1):\n        other_client = deepcopy(initial_client)\n        \n        log_lr = np.log10(other_client['lr'])\n        other_client['lr'] = 10 ** np.clip(log_lr + np.random.uniform(4*-eps, 4*eps), -4.0, 0.0)\n        \n        other_client['momentum'] = np.clip(initial_client['momentum'] + np.random.uniform(-eps, eps), 0, 1.0)\n        \n        log_wd = np.log10(other_client['weight_decay'])\n        other_client['weight_decay'] = 10 ** np.clip(log_wd + np.random.uniform(4*-eps, 4*eps),-5.0, -1.0)\n        \n        epochs_range = math.ceil(eps * 4)\n        other_client['epochs'] = np.clip(np.random.choice(np.arange(initial_client['epochs']-epochs_range, initial_client['epochs']+epochs_range+1)), 1, 5)\n\n        log_batch = int(np.log2(other_client['batch']))\n        batch_range = math.ceil(eps * 4)\n        other_client['batch'] = 2 ** np.clip(np.random.choice(np.arange(log_batch-batch_range, log_batch+batch_range+1)), 3, 7)\n\n        \n        log_mu = np.log10(other_client['mu'])\n        other_client['mu'] = 10 ** np.clip(log_mu + np.random.uniform(5*-eps, 5*eps), -5.0 , 0.0)\n        \n        other_client['dropout'] = np.clip(initial_client['dropout'] + np.random.uniform(0.5*-eps, 0.5*eps),0, 0.5)\n\n        client_arr.append(other_client)\n\n    return [UNIFORM()] if args.uniform else [RANDOM()] if args.random else client_arr",
    "Experiment Result": "The FedEx method tunes local training hyperparameters for personalized Federated Learning. The specific settings for the CIFAR-10 experiment are as follows:\n\n**1. Local Training Hyperparameter Search Space (Client Configuration):**\nFedEx optimizes over a categorical distribution of client configurations, which are generated by perturbing an initial random sample. The initial random sampling ranges for client hyperparameters are:\n-   **Learning Rate (lr):** Uniformly sampled from 10^(-4.0) to 10^(0.0) (log-uniform).\n-   **Momentum:** Uniformly sampled from 0.0 to 1.0.\n-   **Weight Decay:** Uniformly sampled from 10^(-5.0) to 10^(-1.0) (log-uniform).\n-   **Epochs:** Randomly chosen integer from 1 to 5.\n-   **Batch Size (batch):** Randomly chosen power of 2 from 2^3 to 2^7.\n-   **Proximal Term (mu):** Uniformly sampled from 10^(-5.0) to 10^(0.0) (log-uniform), used in FedProx (if mu > 0.0).\n-   **Dropout:** Uniformly sampled from 0.0 to 0.5.\n\nWhen `args.eps > 0.0`, additional client configurations are generated by perturbing the `initial_client`'s hyperparameters using `np.random.uniform(-eps * factor, eps * factor)` where `factor` varies per hyperparameter (e.g., 4 for log_lr, 1 for momentum, 5 for log_mu, 0.5 for dropout). For discrete parameters like epochs and batch size, a range `math.ceil(eps * 4)` is used for random choices.\n\n**2. FedEx Algorithm Settings:**\n-   `--configs`: Number of configurations to optimize over. Default is 1 (FedAvg). Can be set to sample a random number of configs or use the number of arms from the wrapper.\n-   `--eps`: Multiplicative perturbation factor (default 0.0 for FedAvg) applied during client config generation, enabling the local perturbation scheme.\n-   `--uniform`: Flag to run FedEx over a product set of single-parameter uniform grids.\n-   `--random`: Flag to run FedEx over a product set of single-parameter random grids.\n-   `--eta0`: Initial step size for exponentiated gradient updates (default 'auto').\n-   `--sched`: Step size schedule for exponentiated gradient ('aggressive', 'adaptive', 'auto', 'constant', 'scale'). Default is 'aggressive'.\n-   `--cutoff`: Entropy level below which FedEx stops updating the config probability distribution (default 0.0).\n-   `--baseline`: Discount factor for computing the baseline in exponentiated gradient (default -1.0, samples from [0.0, 1.0)).\n-   `--diff`: If set, uses the performance difference between refined and global models as the FedEx objective.\n-   `--stop`: If set, stops updating the FedEx config distribution after the last elimination round.\n-   `--mle`: If set, uses the Maximum Likelihood Estimate (MLE) configuration at test time.\n-   `--loss`: If set, uses loss instead of error as the evaluation metric.\n\n**3. Wrapper Algorithm (Successive Elimination) Settings:**\n-   `--rounds`: Maximum number of communication rounds (resources assigned to a single arm, default 800 for CIFAR-10).\n-   `--total`: Total number of communication rounds/resources (default 4000 for CIFAR-10).\n-   `--rate`: Elimination rate (multiplicative, default 3).\n-   `--elim`: Number of elimination rounds (default 0, runs random search if 0).\n-   `--eval`: Number of evaluation rounds (default 1).\n-   `--discount`: Discount factor for computing the validation score of an arm (default 0.0, uses most recent value).\n-   `--batch`: Number of clients sampled per communication round for FedEx (default 10).\n-   `--eval_global`: If set, uses global error as the elimination metric instead of the refined error.\n\n**4. Dataset and Model Settings (CIFAR-10 Specific):**\n-   **Dataset:** CIFAR-10. Input images are 32x32. Preprocessing includes random horizontal flip, random crop, color jitter, ToTensor, and normalization (mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]).\n-   **Model:** A Convolutional Neural Network (CNN) with three convolutional layers (32, 64, 64 filters respectively), ReLU activations, MaxPool2d, followed by a dropout layer and two fully connected layers (1024 to 64, then 64 to 10 for 10 classes).\n-   `--val`: Proportion of training data used for validation (default 0.2).\n-   `--num-clients`: Number of clients (default 500). Each client receives a partition of the CIFAR-10 dataset (50000 images for train, 10000 for test, split across clients)."
}{
    "Title": "Reshuffling Resampling Splits Can Improve Generalization of Hyperparameter Optimization",
    "Main Contributions": "The research addresses the problem of hyperparameter optimization (HPO) where fixed resampling splits are commonly used, potentially leading to overfitting to the validation set and reduced generalization performance. The main contribution is demonstrating, theoretically and empirically, that reshuffling resampling splits for every hyperparameter configuration (HPC) often improves the final model's generalization performance on unseen data. Specifically, it provides a theoretical explanation for how reshuffling affects the asymptotic behavior of the validation loss surface and offers a bound on the expected regret. This is supported by controlled simulation studies and large-scale, realistic HPO benchmark experiments showing practical benefits, especially for holdout protocols, making them competitive with standard cross-validation.",
    "Methodology": "The methodology involves a theoretical analysis and an empirical simulation and benchmark study. Theoretically, the paper investigates how reshuffling affects the empirical loss surface by deriving the limiting distribution of the validation loss error, characterizing it with parameters for variance (σ²) and correlation (τ²) changes due to reshuffling. It then provides a bound on the expected regret based on a Gaussian process model for the empirical loss, relating reshuffling's benefits to the signal-to-noise ratio, loss surface curvature (m), and noise correlation (κ). For the simulation study, a univariate quadratic loss surface combined with a squared exponential kernel for the noise process (ϵ) is used to systematically vary m, κ, and τ. The benchmark experiments employ various machine learning algorithms (CatBoost, XGBoost, Elastic Net, Funnel MLP) and HPO optimizers (Random Search, HEBO, SMAC3) across different resampling strategies (holdout, M-fold CV, M-fold holdout, M-times M-fold CV), comparing fixed versus reshuffled splits.",
    "Experimental Setup": "The experimental setup utilized a subset of 10 tabular datasets from the AutoML benchmark for binary classification, serving as data generating processes (DGPs). For each DGP, 5000 data points were reserved as an independent outer test set for robust generalization error assessment. HPO was performed on subsets of varying sizes: 500, 1000, and 5000 observations for combined training and validation data. Learning algorithms included CatBoost, XGBoost, Elastic Net, and a funnel-shaped MLP, each with defined search spaces and preprocessing pipelines. Resampling strategies included holdout (80/20 split), 5-fold CV, 5-fold holdout, and 5x 5-fold CV, all evaluated with both fixed and reshuffled splits. HPO optimizers were Random Search (500 HPC evaluations) and Bayesian Optimization variants HEBO and SMAC3 (250 HPC evaluations). Performance was measured using ROC AUC, Accuracy, and Logloss (for Random Search) or only ROC AUC (for BO). Anytime test performance was assessed by retraining the current incumbent on all available train+validation data and evaluating on the outer test set. Each scenario was run for 10 replications. Total computational resources were estimated at 11.86 CPU years, with total emissions of approximately 6508.67 kg CO2.",
    "Limitations": "The theoretical analysis relies on an asymptotic approximation of the empirical loss surface, assuming Gaussian loss surfaces for mathematical tractability. It also uses a loss stability assumption for learning algorithms which is generally mild but may fail for highly sensitive losses like logloss, empirically observed to sometimes hurt generalization. The study's scope is limited to generalization after searching through a fixed, finite set of candidates, not fully capturing the dynamic nature of many HPO algorithms. Furthermore, experiments were restricted to tabular data and binary classification, avoiding extremely small or large datasets, which may limit the generalizability of empirical findings.",
    "Future Research Directions": "Future research directions include developing a unified formal definition for 'oversearching,' 'overtuning,' or 'overfitting to the validation set' and thoroughly analyzing its relationship to validation performance measurements. It is suggested to further investigate adaptive cross-validation techniques to mitigate computational burden while still allowing more intensive evaluation of HPCs. Designing more advanced HPO algorithms that explicitly exploit the reshuffling effect is a promising avenue. Additionally, exploring the combination of reshuffling with existing overfitting mitigation strategies (e.g., LOOCVCV, extra selection sets, early stopping) could yield further improvements. Investigating reshuffling's impact on multi-class datasets and developing less naive implementations to address its negative effects on metrics like logloss are also recommended.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "DP-HyPO: An Adaptive Private Framework for Hyperparameter Optimization",
    "Main Contributions": "Introduces DP-HyPO, a pioneering framework for adaptive private hyperparameter optimization, addressing the privacy risks often overlooked in HPO for private ML models. It bridges the gap between non-adaptive private HPO methods and adaptive non-private HPO methods by allowing flexible, adaptive selection of hyperparameters while providing sharp Rényi Differential Privacy (RDP) guarantees. The framework can convert virtually any non-private adaptive HPO algorithm into a private one and empirically demonstrates that its Gaussian process-based instantiation outperforms non-adaptive uniform counterparts.",
    "Methodology": "The DP-HyPO framework maintains an adaptive sampling distribution (π) for hyperparameters at each iteration, which is updated based on accumulated information from previous runs. To ensure privacy, the total number of repetitions (T) for training runs is a random variable (e.g., truncated negative binomial distribution). A crucial aspect is enforcing bounded adaptive density, where the ratio of any posterior sampling distribution to the prior (π(j+1)(λ)/π(0)(λ)) must be within a range [c, C]. Non-private update rules are privatized by projecting their resulting distribution onto this space of bounded densities, solving a convex functional programming problem. The privacy guarantees are derived using the Rényi DP framework, providing a strict generalization of previous uniform sampling results. An instantiation with Gaussian process (GP-based DP-HyPO) uses GP to build a surrogate model, assigns scores (e.g., UCB) to hyperparameters, and transforms them into a sampling distribution via a softmax function before applying the density projection.",
    "Experimental Setup": "The performance of GP-based DP-HyPO (referred to as \"GP\") is empirically evaluated against a Uniform DP-HyPO baseline (\"Uniform\"), representing prior non-adaptive methods. Experiments are conducted under two privacy configurations: a white-box setting (adaptive HPO incurs extra privacy cost, reducing base algorithm budget) and a black-box setting (base algorithm privacy budget fixed, extra budget for HPO adaptivity). Datasets include MNIST (white-box) using a standard CNN with DP-SGD, CIFAR-10 (white-box) with the same CNN, and a real-world Federated Learning task on a proprietary industrial dataset (black-box). For MNIST, a semi-real simulation caches mean accuracies of independently trained models, adding Gaussian noise for sampling. For CIFAR-10 and Federated Learning, hyperparameter landscapes (mean and standard error of accuracy/loss) are generated using BoTorch, with an oracle returning noisy scores. Hyperparameters optimized include learning rate and clipping norm, with specific discretization grids (e.g., 320 for MNIST, 2500 for CIFAR-10) and total privacy budgets (e.g., ε=15 for MNIST, ε=12 for CIFAR-10). GP-specific parameters like exploration-exploitation factor (τ=0.1) and inverse temperature (β=1) are also set, with bounds C and c for adaptivity varying by scenario.",
    "Limitations": "The framework requires the total number of repetitions for training runs (T) to be a random variable, not a fixed number, to preserve privacy. A key constraint is the necessity for adaptive sampling distributions to maintain a bounded density ratio (c ≤ π(j+1)(λ)/π(0)(λ) ≤ C), which may not be naturally satisfied by non-private adaptive HPO methods and requires a specific projection technique. For practical computational feasibility, especially with non-discrete hyperparameter spaces, practitioners must discretize the search space. Furthermore, the empirical evaluations on MNIST and CIFAR-10 rely on 'semi-real simulations' or pre-generated hyperparameter landscapes rather than full, independent training runs for every single hyperparameter combination, due to computational resource constraints. The framework also assumes a total ordering on the performance measure and that the base algorithm outputs the model, hyperparameter, and performance measure.",
    "Future Research Directions": "Two main future research directions are suggested: 1) exploring alternative and more practically favorable HPO specifications, leveraging more advanced HPO methods to improve empirical performance; and 2) establishing theoretical utility guarantees for the general DP-HyPO framework, or for specific configurations within it, by employing proof methodologies similar to those found in related prior work.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "Gradient Descent: The Ultimate Optimizer",
    "Main Contributions": "The paper addresses the tedious and error-prone task of manually tuning optimizer hyperparameters by proposing an automated method to compute hypergradients. It introduces a simple, elegant modification to backpropagation that leverages automatic differentiation (AD) to optimize not only the learning rate but also other hyperparameters (e.g., momentum coefficients) for various optimizers like SGD, Adam, AdaGrad, and RMSProp. A key contribution is the ability to recursively apply this method, creating arbitrarily tall 'towers of optimizers' that become increasingly robust to the initial choice of hyperparameters, thus reducing the burden on human practitioners. The method is shown to be efficient, with negligible computational overhead.",
    "Methodology": "The core methodology involves modifying the standard backpropagation mechanism within a differentiable programming system to automatically compute 'hypergradients.' Instead of manually deriving partial derivatives for hyperparameters, the system refrains from detaching the hyperparameters from the computation graph during the weight update step, allowing backpropagation to compute gradients with respect to them. For recursive stacking, the hyperoptimizer itself is refactored to take another optimizer as a parameter, allowing it to optimize its own hyperparameters. Specific considerations for optimizers include clamping hyperparameters like Adam's beta values within their valid domain (0,1) using a scaled sigmoid and initializing Adam's ˆv0 to epsilon instead of zero to prevent division by zero.",
    "Experimental Setup": "Experiments were conducted on various neural network architectures and datasets using a single NVIDIA TITAN Xp GPU. For basic properties and robustness, an MLP with one hidden layer (128 units, tanh activation, batch size 256) was trained on MNIST for 30 epochs. Adam, AdaGrad, and RMSProp optimizers were tested for 5 epochs. For hyperoptimization at scale, a ResNet-20 was trained on CIFAR-10 for 200-500 epochs, comparing with hand-engineered learning rate schedules. A character-level RNN (2-layer LSTM with 128 hidden nodes) was trained on the Tolstoy dataset for 50,000 gradient steps. Higher-order hyperoptimization was validated across all benchmarks (MLP on MNIST, CNN on CIFAR-10, RNN on Tolstoy) by varying initial base-level step sizes and increasing optimizer stack heights. Further scalability tests involved fine-tuning a pretrained ResNet-152 on the Caltech-256 dataset. Performance was measured by test error/accuracy and perplexity for RNNs.",
    "Limitations": "One limitation is that hyperoptimizers currently cannot handle initial hyperparameters that are set \"far too high,\" leading to system instability and divergence before the hyperoptimizer can take effect. The implementation also requires careful management of the computation graph to avoid bugs like memory leaks (e.g., loggers must detach logged tensors) and incorrect gradients due to silently modified computation graphs by certain PyTorch modules (e.g., built-in LSTM).",
    "Future Research Directions": "Future research directions include designing hyperoptimizers that are robust to initial hyperparameters set far too high, potentially requiring a deeper theoretical analysis of convergence. Another direction is to design differentiable programming languages where such methods can be expressed in a modular and composable manner, minimizing the risk of computation graph-related bugs. The paper also broadly implies the potential for reducing the energy consumption of hyperparameter searches through advances in on-line hyperparameter tuning.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "Implicit differentiation of Lasso-type models for hyperparameter optimization",
    "Main Contributions": "The paper introduces an efficient implicit differentiation algorithm, without explicit matrix inversion, specifically designed for hyperparameter optimization of Lasso-type models. It addresses the challenges of setting regularization parameters for non-smooth Lasso-type estimators, which are notoriously difficult due to issues like exponential complexity of grid-search, high memory consumption of iterative differentiation, or numerical instability and computational cost of classical implicit differentiation for smooth problems. The key contributions include demonstrating that forward iterative differentiation of Block Coordinate Descent (BCD) converges linearly to the true gradient once the support is identified, proposing a decoupled algorithm (Implicit Forward Iterative Differentiation) for Jacobian computation that avoids solving ill-conditioned linear systems, and experimentally showing that the proposed method outperforms numerous state-of-the-art hyperparameter optimization techniques in terms of optimizing held-out error or the Stein Unbiased Risk Estimator (SURE), particularly scaling well to high-dimensional data by leveraging solution sparsity.",
    "Methodology": "The methodology frames hyperparameter optimization as a bi-level optimization problem for Lasso-type estimators (Lasso and weighted Lasso) with a differentiable criterion. It leverages implicit differentiation, starting from the fixed-point iteration property of proximal BCD algorithms for Lasso-type problems. The core contribution is deriving a weak Jacobian ˆJ(λ) that is sparse and can be computed efficiently. The proposed 'Implicit Forward Iterative Differentiation' algorithm (Algorithm 2) works in two main steps: first, it solves the inner optimization problem to compute the regression coefficients ˆβ and identify its support, which can be done with any state-of-the-art Lasso solver. Second, it computes the Jacobian by applying forward differentiation recursion steps restricted to the identified support, without explicitly solving a linear system. This approach is proven to converge linearly to the true Jacobian. For experimental comparison, a vanilla BCD algorithm is used for the inner problem across all methods, along with a line-search strategy for gradient-based optimizers. Hyperparameters are parametrized exponentially to avoid positivity constraints and scale issues.",
    "Experimental Setup": "The Python code for all experiments is open-sourced as the 'sparse-ho' package, utilizing Numba for critical sections. The inner Lasso-type solvers are stopped when the relative function decrease is below ϵ_tol = 10^-5. All hypergradient-based methods use a line-search strategy for gradient steps and warm starts. Initial values for Lasso hyperparameters are set at λ_max - log(10). Competitors include implicit differentiation (solving an ˆs×ˆs linear system), forward iterative differentiation, and non-gradient methods such as grid-search, random-search, lattice hypercube sampling, and Bayesian optimization. Experiments are conducted using two main criteria: held-out loss (on three real-world datasets: rcv1 (p=19,959), 20news (p=130,107), and finance (p=1,668,737), split into training, validation, and test sets) and Stein Unbiased Risk Estimator (SURE) with a Finite Differences Monte-Carlo approximation (on synthetic data with n=100, p varying from 200 to 10,000, 50 repetitions per p value). A specific setup for non-unique solution scenarios is also explored. The proposed implicit forward differentiation algorithm uses n_iter_jac = 100 for Jacobian computation, with an early stopping condition based on Jacobian convergence.",
    "Limitations": "The theoretical guarantees for the proposed method do not cover non-convex penalty functions (e.g., MCP), although experimental results suggest it performs numerically well in such cases. The convergence proofs rely on the assumption of a unique solution to the inner Lasso problem; in pathological settings where the solution is not unique or the solution path is non-continuous, the theoretical justification for gradient-based methods is challenged. Classical implicit differentiation (Algorithm 1) can suffer from slow convergence due to ill-conditioned linear systems, especially with large support sizes. Backward iterative differentiation (Algorithm 4) is found to be significantly slower and more memory-consuming than other approaches for Lasso-type problems, especially for the weighted Lasso, making it impractical for the scale of problems considered.",
    "Future Research Directions": "Future work could involve extending the theoretical framework to cover non-convex Lasso-type formulations, such as those employing the Minimax Concave Penalty (MCP). Another promising direction is to further explore the integration of the proposed algorithm with state-of-the-art Lasso solvers that utilize advanced techniques like active sets or screening rules. These solvers, while highly efficient, introduce discontinuities that pose challenges for single-step automatic differentiation approaches, suggesting a need for dedicated research to effectively combine these powerful tools. Further investigation into the algorithm's behavior and theoretical guarantees in 'pathological' settings where the inner optimization problem may have non-unique solutions or non-continuous solution paths is also a relevant area for future exploration.",
    "Experiment Code": "from .ho import grad_search, hyperopt_wrapper\n\nfrom .algo import Backward\nfrom .algo import Forward\nfrom .algo import ImplicitForward\nfrom .algo import Implicit\n\n\n__version__ = '0.1.dev'\n\nfrom sparse_ho.algo.backward import Backward\nfrom sparse_ho.algo.forward import Forward\nfrom sparse_ho.algo.implicit import Implicit\nfrom sparse_ho.algo.implicit_forward import ImplicitForward\n\n__all__ = ['Backward',\n           'Forward',\n           'Implicit',\n           'ImplicitForward']\n\nimport numpy as np\nfrom scipy.sparse import issparse\n\n\nclass Forward():\n    \"\"\"Algorithm to compute the hypergradient using forward differentiation of\n    proximal coordinate descent.\n\n    The algorithm jointly and iteratively computes the regression coefficients\n    and the Jacobian using forward differentiation of proximal\n    coordinate descent.\n\n    Parameters\n    ----------\n    use_stop_crit: bool, optional (default=True)\n        Use stopping criterion in hypergradient computation. If False,\n        run to maximum number of iterations.\n    verbose: bool, optional (default=False)\n        Verbosity of the algorithm.\n    \"\"\"\n\n    def __init__(self, use_stop_crit=True, verbose=False):\n        self.use_stop_crit = use_stop_crit\n        self.verbose = verbose\n\n    def compute_beta_grad(\n            self, X, y, log_alpha, model, get_grad_outer, mask0=None,\n            dense0=None, quantity_to_warm_start=None, max_iter=1000, tol=1e-3,\n            full_jac_v=False):\n        \"\"\"Compute beta and hypergradient, with forward differentiation of\n        proximal coordinate descent.\n\n        Parameters\n        ----------\n        X: array-like, shape (n_samples, n_features)\n            Design matrix.\n        y: ndarray, shape (n_samples,)\n            Observation vector.\n        log_alpha: float or np.array, shape (n_features,)\n            Logarithm of hyperparameter.\n        model:  instance of ``sparse_ho.base.BaseModel``\n            A model that follows the sparse_ho API.\n        get_grad_outer: callable\n            Function which returns the gradient of the outer criterion.\n        mask0: ndarray, shape (n_features,)\n            Boolean of active feature of the previous regression coefficients\n            beta for warm start.\n        dense0: ndarray, shape (mask.sum(),)\n            Initial value of the previous regression coefficients\n            beta for warm start.\n        quantity_to_warm_start: ndarray\n            Previous Jacobian of the inner optimization problem.\n        max_iter: int\n            Maximum number of iteration for the inner solver.\n        tol: float\n            The tolerance for the inner optimization problem.\n        full_jac_v: bool\n            TODO\n        \"\"\"\n        # jointly compute the regression coefficients beta and the Jacobian\n        mask, dense, jac = compute_beta(\n            X, y, log_alpha, model, mask0=mask0, dense0=dense0,\n            jac0=quantity_to_warm_start, max_iter=max_iter, tol=tol,\n            compute_jac=True, verbose=self.verbose,\n            use_stop_crit=self.use_stop_crit)\n        if jac is not None:\n            jac_v = model.get_jac_v(X, y, mask, dense, jac, get_grad_outer)\n            if full_jac_v:\n                jac_v = model.get_full_jac_v(mask, jac_v, X.shape[1])\n        else:\n            jac_v = None\n\n        return mask, dense, jac_v, jac\n\n\ndef compute_beta(\n        X, y, log_alpha, model, mask0=None, dense0=None, jac0=None,\n        max_iter=1000, tol=1e-3, compute_jac=True, return_all=False,\n        save_iterates=False, verbose=False, use_stop_crit=True, gap_freq=10):\n    \"\"\"\n    Parameters\n    --------------\n    X: array-like, shape (n_samples, n_features)\n        Design matrix.\n    y: ndarray, shape (n_samples,)\n        Observation vector.\n    log_alpha: float or np.array, shape (n_features,)\n        Logarithm of hyperparameter.\n    beta0: ndarray, shape (n_features,)\n        initial value of the regression coefficients\n        beta for warm start\n    dbeta0: ndarray, shape (n_features,)\n        initial value of the jacobian dbeta for warm start\n    max_iter: int\n        number of iterations of the algorithm\n    tol: float\n        The tolerance for the optimization: if the updates are\n        smaller than ``tol``, the optimization code checks the\n        primal decrease for optimality and continues until it\n        is smaller than ``tol``\n    compute_jac: bool\n        to compute or not the Jacobian along with the regression\n        coefficients\n    model:  instance of ``sparse_ho.base.BaseModel``\n        A model that follows the sparse_ho API.\n    return_all: bool\n        to store the iterates or not in order to compute the Jacobian in a\n        backward way\n    use_stop_crit: bool\n        use a stopping criterion or do all the iterations\n    gap_freq : int\n        After how many passes on the data the dual gap should be computed\n        to stop the iterations.\n\n    Returns\n    -------\n    mask : ndarray, shape (n_features,)\n        The mask of non-zero coefficients in beta.\n    dense : ndarray, shape (n_nonzeros,)\n        The beta coefficients on the support\n    jac : ndarray, shape (n_nonzeros,) or (n_nonzeros, q)\n        The jacobian restricted to the support. If there are more than\n        one hyperparameter then it has two dimensions.\n    \"\"\"\n    n_samples, n_features = X.shape\n    is_sparse = issparse(X)\n    if not is_sparse and not np.isfortran(X):\n        X = np.asfortranarray(X)\n    L = model.get_L(X)\n\n    ############################################\n    alpha = np.exp(log_alpha)\n\n    if hasattr(model, 'estimator') and model.estimator is not None:\n        return model._use_estimator(X, y, alpha, tol)\n\n    try:\n        alpha.shape[0]\n        alphas = alpha.copy()\n    except Exception:\n        alphas = np.ones(n_features) * alpha\n    ############################################\n    # warm start for beta\n    beta, dual_var = model._init_beta_dual_var(X, y, mask0, dense0)\n    ############################################\n    # warm start for dbeta\n    dbeta, ddual_var = model._init_dbeta_ddual_var(\n        X, y, mask0=mask0, dense0=dense0, jac0=jac0, compute_jac=compute_jac)\n\n    # store the values of the objective\n    pobj0 = model._get_pobj0(dual_var, np.zeros(X.shape[1]), alphas, y)\n    pobj = []\n\n    ############################################\n    # store the iterates if needed\n    if return_all:\n        list_beta = []\n    if save_iterates:\n        list_beta = []\n        list_jac = []\n\n    for i in range(max_iter):\n        if verbose:\n            print(\"%i -st iteration over %i\" % (i, max_iter))\n        if is_sparse:\n            model._update_beta_jac_bcd_sparse(\n                X.data, X.indptr, X.indices, y, n_samples, n_features, beta,\n                dbeta, dual_var, ddual_var, alphas, L,\n                compute_jac=compute_jac)\n        else:\n            model._update_beta_jac_bcd(\n                X, y, beta, dbeta, dual_var, ddual_var, alphas,\n                L, compute_jac=compute_jac)\n\n        pobj.append(model._get_pobj(dual_var, X, beta, alphas, y))\n\n        if i > 1:\n            if verbose:\n                print(\"relative decrease = \", (pobj[-2] - pobj[-1]) / pobj0)\n\n        if use_stop_crit and i % gap_freq == 0 and i > 0:\n            if hasattr(model, \"_get_dobj\"):\n                dobj = model._get_dobj(dual_var, X, beta, alpha, y)\n                dual_gap = pobj[-1] - dobj\n                if verbose:\n                    print(\"dual gap %.2e\" % dual_gap)\n                if verbose:\n                    print(\"gap %.2e\" % dual_gap)\n                if dual_gap < pobj0 * tol:\n                    break\n            else:\n                if (pobj[-2] - pobj[-1] <= pobj0 * tol):\n                    break\n        if return_all:\n            list_beta.append(beta.copy())\n        if save_iterates:\n            list_beta.append(beta.copy())\n            list_jac.append(dbeta.copy())\n    else:\n        if verbose:\n            print('did not converge !')\n\n    mask = beta != 0\n    dense = beta[mask]\n    jac = model._get_jac(dbeta, mask)\n    if hasattr(model, 'dual'):\n        model.dual_var = dual_var\n        if compute_jac:\n            model.ddual_var = ddual_var\n    if save_iterates:\n        return np.array(list_beta), np.array(list_jac)\n    if return_all:\n        return mask, dense, list_beta\n    else:\n        if compute_jac:\n            return mask, dense, jac\n        else:\n            return mask, dense, None\n\nclass ImplicitForward():\n    \"\"\"Algorithm to compute the hypergradient using implicit forward\n    differentiation.\n\n    First the algorithm computes the regression coefficients.\n    Then the iterations of the forward differentiation are applied to compute\n    the Jacobian.\n\n    Parameters\n    ----------\n    tol_jac: float\n        Tolerance for the Jacobian computation.\n    max_iter: int\n        Maximum number of iterations for the inner solver.\n    n_iter_jac: int\n        Maximum number of iterations for the Jacobian computation.\n    use_stop_crit: bool, optional (default=True)\n        Use stopping criterion in hypergradient computation. If False,\n        run to maximum number of iterations.\n    verbose: bool, optional (default=False)\n        Verbosity of the algorithm.\n    \"\"\"\n\n    def __init__(\n            self, tol_jac=1e-3, max_iter=100, n_iter_jac=100,\n            use_stop_crit=True, verbose=False):\n        self.max_iter = max_iter\n        self.tol_jac = tol_jac\n        self.n_iter_jac = n_iter_jac\n        self.use_stop_crit = use_stop_crit\n        self.verbose = verbose\n\n    def get_beta_jac(\n            self, X, y, log_alpha, model, get_grad_outer, mask0=None,\n            dense0=None, quantity_to_warm_start=None, max_iter=1000, tol=1e-3,\n            full_jac_v=False):\n        \"\"\"Compute beta and hypergradient using implicit forward\n        differentiation.\n\n        Parameters\n        ----------\n        X: array-like, shape (n_samples, n_features)\n            Design matrix.\n        y: ndarray, shape (n_samples,)\n            Observation vector.\n        log_alpha: float or np.array, shape (n_features,)\n            Logarithm of hyperparameter.\n        model:  instance of ``sparse_ho.base.BaseModel``\n            A model that follows the sparse_ho API.\n        get_grad_outer: callable\n            Function which returns the gradient of the outer criterion.\n        mask0: ndarray, shape (n_features,)\n            Boolean of active feature of the previous regression coefficients\n            beta for warm start.\n        dense0: ndarray, shape (mask.sum(),)\n            Initial value of the previous regression coefficients\n            beta for warm start.\n        quantity_to_warm_start: ndarray\n            Previous Jacobian of the inner optimization problem.\n        max_iter: int\n            Maximum number of iteration for the inner solver.\n        tol: float\n            The tolerance for the inner optimization problem.\n        full_jac_v: bool\n            TODO\n        \"\"\"\n\n        mask, dense, jac = get_bet_jac_implicit_forward(\n            X, y, log_alpha, mask0=mask0, dense0=dense0,\n            jac0=quantity_to_warm_start,\n            tol_jac=tol, tol=tol, niter_jac=self.n_iter_jac, model=model,\n            max_iter=self.max_iter, verbose=self.verbose)\n        return mask, dense, jac\n\n    def compute_beta_grad(\n            self, X, y, log_alpha, model, get_grad_outer, mask0=None,\n            dense0=None, quantity_to_warm_start=None, max_iter=1000, tol=1e-3,\n            full_jac_v=False):\n        mask, dense, jac = get_bet_jac_implicit_forward(\n            X, y, log_alpha, mask0=mask0, dense0=dense0,\n            jac0=quantity_to_warm_start,\n            tol_jac=self.tol_jac, tol=tol, niter_jac=self.n_iter_jac,\n            model=model, max_iter=self.max_iter, verbose=self.verbose,\n            use_stop_crit=self.use_stop_crit)\n        jac_v = model.get_jac_v(X, y, mask, dense, jac, get_grad_outer)\n        if full_jac_v:\n            jac_v = model.get_full_jac_v(mask, jac_v, X.shape[1])\n\n        return mask, dense, jac_v, jac\n\n\ndef get_bet_jac_implicit_forward(\n        X, y, log_alpha, model, mask0=None, dense0=None, jac0=None,\n        tol=1e-3, max_iter=1000, niter_jac=1000, tol_jac=1e-6, verbose=False,\n        use_stop_crit=True):\n\n    mask, dense, _ = compute_beta(\n        X, y, log_alpha, mask0=mask0, dense0=dense0, jac0=jac0, tol=tol,\n        max_iter=max_iter, compute_jac=False, model=model, verbose=verbose,\n        use_stop_crit=use_stop_crit)\n    dbeta0_new = model._init_dbeta0(mask, mask0, jac0)\n    reduce_alpha = model._reduce_alpha(np.exp(log_alpha), mask)\n\n    _, dual_var = model._init_beta_dual_var(X, y, mask, dense)\n    jac = get_only_jac(\n        model.reduce_X(X, mask), model.reduce_y(y, mask), dual_var,\n        reduce_alpha, model.sign(dense, log_alpha), dbeta=dbeta0_new,\n        niter_jac=niter_jac, tol_jac=tol_jac, model=model, mask=mask,\n        dense=dense, verbose=verbose, use_stop_crit=use_stop_crit)\n\n    return mask, dense, jac\n\n\ndef get_only_jac(\n        Xs, y, dual_var, alpha, sign_beta, dbeta=None, niter_jac=100,\n        tol_jac=1e-4, model=\"lasso\", mask=None, dense=None, verbose=False,\n        use_stop_crit=True):\n    n_samples, n_features = Xs.shape\n\n    L = model.get_L(Xs)\n\n    residual_norm = []\n\n    if hasattr(model, 'dual'):\n        ddual_var = model._init_ddual_var(dbeta, Xs, y, sign_beta, alpha)\n        dbeta = model.dbeta\n    else:\n        if dbeta is None:\n            dbeta = model._init_dbeta(n_features)\n        ddual_var = model._init_ddual_var(dbeta, Xs, y, sign_beta, alpha)\n\n    for i in range(niter_jac):\n        if verbose:\n            print(\"%i -st iterations over %i\" % (i, niter_jac))\n        if issparse(Xs):\n            model._update_only_jac_sparse(\n                Xs.data, Xs.indptr, Xs.indices, y, n_samples,\n                n_features, dbeta, dual_var, ddual_var, L, alpha, sign_beta)\n        else:\n            model._update_only_jac(\n                Xs, y, dual_var, dbeta, ddual_var, L, alpha, sign_beta)\n        residual_norm.append(\n            model.get_jac_residual_norm(\n                Xs, y, n_samples, sign_beta, dbeta, dual_var,\n                ddual_var, alpha))\n        if use_stop_crit and i > 1:\n            # relative stopping criterion for the computation of the jacobian\n            # and absolute stopping criterion to handle warm start\n            rel_tol = np.abs(residual_norm[-2] - residual_norm[-1])\n            if (rel_tol < np.abs(residual_norm[-1]) * tol_jac\n                    or residual_norm[-1] < 1e-10):\n                break\n    # HACK we only need this for one test, do not rely on it\n    get_only_jac.n_iter = i\n\n    return dbeta\n\nimport numpy as np\nfrom numpy.linalg import norm\nfrom scipy.sparse import issparse\nimport scipy.sparse.linalg as slinalg\nfrom numba import njit\nfrom scipy.sparse.linalg import LinearOperator\n\nfrom sparse_ho.utils import init_dbeta0_new, ST\nfrom sparse_ho.utils import sparse_scalar_product\nfrom sparse_ho.models.base import BaseModel\n\n\nclass Lasso(BaseModel):\n    \"\"\"Linear Model trained with L1 prior as regularizer (aka the Lasso).\n\n    The optimization objective for Lasso is:\n    (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n\n    Parameters\n    ----------\n    estimator: sklearn estimator\n        Estimator used to solve the optimization problem. Must follow the\n        scikit-learn API.\n    \"\"\"\n\n    def __init__(self, estimator=None):\n        self.estimator = estimator\n\n    def _init_dbeta_ddual_var(self, X, y, mask0=None, jac0=None,\n                              dense0=None, compute_jac=True):\n        n_samples, n_features = X.shape\n        dbeta = np.zeros(n_features)\n        if jac0 is None or not compute_jac:\n            ddual_var = np.zeros(n_samples)\n        else:\n            dbeta[mask0] = jac0.copy()\n            ddual_var = - X[:, mask0] @ jac0.copy()\n        return dbeta, ddual_var\n\n    def _init_beta_dual_var(self, X, y, mask0=None, dense0=None):\n        beta = np.zeros(X.shape[1])\n        if dense0 is None or len(dense0) == 0:\n            dual_var = y.copy()\n            dual_var = dual_var.astype(np.float)\n        else:\n            beta[mask0] = dense0.copy()\n            dual_var = y - X[:, mask0] @ dense0\n        return beta, dual_var\n\n    @staticmethod\n    @njit\n    def _update_beta_jac_bcd(\n            X, y, beta, dbeta, dual_var, ddual_var,\n            alpha, L, compute_jac=True):\n        n_samples, n_features = X.shape\n        non_zeros = np.where(L != 0)[0]\n\n        for j in non_zeros:\n            beta_old = beta[j]\n            if compute_jac:\n                dbeta_old = dbeta[j]\n                # compute derivatives\n            zj = beta[j] + dual_var @ X[:, j] / (L[j] * n_samples)\n            beta[j] = ST(zj, alpha[j] / L[j])\n            # beta[j:j+1] = ST(zj, alpha[j] / L[j])\n            if compute_jac:\n                dzj = dbeta[j] + X[:, j] @ ddual_var / (L[j] * n_samples)\n                dbeta[j:j+1] = np.abs(np.sign(beta[j])) * dzj\n                dbeta[j:j+1] -= alpha[j] * np.sign(beta[j]) / L[j]\n                # update residuals\n                ddual_var -= X[:, j] * (dbeta[j] - dbeta_old)\n            dual_var -= X[:, j] * (beta[j] - beta_old)\n\n    @staticmethod\n    @njit\n    def _update_beta_jac_bcd_sparse(\n            data, indptr, indices, y, n_samples, n_features, beta,\n            dbeta, dual_var, ddual_var, alphas, L, compute_jac=True):\n\n        non_zeros = np.where(L != 0)[0]\n\n        for j in non_zeros:\n            # get the j-st column of X in sparse format\n            Xjs = data[indptr[j]:indptr[j+1]]\n            # get the non zero indices\n            idx_nz = indices[indptr[j]:indptr[j+1]]\n            beta_old = beta[j]\n            if compute_jac:\n                dbeta_old = dbeta[j]\n            zj = beta[j] + dual_var[idx_nz] @ Xjs / (L[j] * n_samples)\n            beta[j:j+1] = ST(zj, alphas[j] / L[j])\n            if compute_jac:\n                dzj = dbeta[j] + Xjs @ ddual_var[idx_nz] / (L[j] * n_samples)\n                dbeta[j:j+1] = np.abs(np.sign(beta[j])) * dzj\n                dbeta[j:j+1] -= alphas[j] * np.sign(beta[j]) / L[j]\n                # update residuals\n                ddual_var[idx_nz] -= Xjs * (dbeta[j] - dbeta_old)\n            dual_var[idx_nz] -= Xjs * (beta[j] - beta_old)\n\n    @staticmethod\n    @njit\n    def _update_bcd_jac_backward(X, alpha, grad, beta, v_t_jac, L):\n        sign_beta = np.sign(beta)\n        n_samples, n_features = X.shape\n        for j in (np.arange(sign_beta.shape[0] - 1, -1, -1)):\n            grad -= (v_t_jac[j]) * alpha * sign_beta[j] / L[j]\n            v_t_jac[j] *= np.abs(sign_beta[j])\n            v_t_jac -= v_t_jac[j] / (L[j] * n_samples) * X[:, j] @ X\n\n        return grad\n\n    @staticmethod\n    @njit\n    def _update_bcd_jac_backward_sparse(\n            data, indptr, indices, n_samples, n_features,\n            alpha, grad, beta, v_t_jac, L):\n        sign_beta = np.sign(beta)\n        for j in (np.arange(sign_beta.shape[0] - 1, -1, -1)):\n            if L[j] != 0:\n                Xjs = data[indptr[j]:indptr[j+1]]\n                idx_nz = indices[indptr[j]:indptr[j+1]]\n                grad -= (v_t_jac[j]) * alpha * sign_beta[j] / L[j]\n                v_t_jac[j] *= np.abs(sign_beta[j])\n                cste = v_t_jac[j] / (L[j] * n_samples)\n                for i in (np.arange(sign_beta.shape[0] - 1, -1, -1)):\n                    Xis = data[indptr[i]:indptr[i+1]]\n                    idx = indices[indptr[i]:indptr[i+1]]\n                    product = sparse_scalar_product(Xjs, idx_nz, Xis, idx)\n                    v_t_jac[i] -= cste * product\n\n        return grad\n\n    @staticmethod\n    def _get_pobj0(dual_var, beta, alphas, y=None):\n        n_samples = dual_var.shape[0]\n        return norm(y) ** 2 / (2 * n_samples)\n\n    @staticmethod\n    def _get_pobj(dual_var, X, beta, alphas, y=None):\n        n_samples = dual_var.shape[0]\n        return (\n            norm(dual_var) ** 2 / (2 * n_samples) +\n            np.abs(alphas * beta).sum())\n\n    @staticmethod\n    def _get_dobj(dual_var, X, beta, alpha, y=None):\n        # the dual variable is theta = (y - X beta) / (alpha n_samples)\n        n_samples = X.shape[0]\n        theta = dual_var / (alpha * n_samples)\n        norm_inf_XTtheta = np.max(np.abs(X.T @ theta))\n        if norm_inf_XTtheta > 1:\n            theta /= norm_inf_XTtheta\n        dobj = alpha * y @ theta\n        dobj -= alpha ** 2 * n_samples / 2 * (theta ** 2).sum()\n        return dobj\n\n    @staticmethod\n    def _get_jac(dbeta, mask):\n        return dbeta[mask]\n\n    @staticmethod\n    def get_full_jac_v(mask, jac_v, n_features):\n        \"\"\"TODO\n\n        Parameters\n        ----------\n        mask: TODO\n        jac_v: TODO\n        n_features: int\n            Number of features.\n        \"\"\"\n        # MM sorry I don't get what this does\n        return jac_v\n\n    @staticmethod\n    def get_mask_jac_v(mask, jac_v):\n        \"\"\"TODO\n\n        Parameters\n        ----------\n        mask: TODO\n        jac_v: TODO\n        \"\"\"\n        return jac_v\n\n    @staticmethod\n    def _init_dbeta0(mask, mask0, jac0):\n        size_mat = mask.sum()\n        if jac0 is not None:\n            dbeta0_new = init_dbeta0_new(jac0, mask, mask0)\n        else:\n            dbeta0_new = np.zeros(size_mat)\n        return dbeta0_new\n\n    @staticmethod\n    def _init_dbeta(n_features):\n        dbeta = np.zeros(n_features)\n        return dbeta\n\n    @staticmethod\n    def _init_ddual_var(dbeta, X, y, sign_beta, alpha):\n        return - X @ dbeta\n\n    @staticmethod\n    def _init_g_backward(jac_v0, n_features):\n        if jac_v0 is None:\n            return 0.0\n        else:\n            return jac_v0\n\n    @staticmethod\n    @njit\n    def _update_only_jac(Xs, y, dual_var, dbeta, ddual_var,\n                         L, alpha, sign_beta):\n        n_samples, n_features = Xs.shape\n        for j in range(n_features):\n            # dbeta_old = dbeta[j].copy()\n            dbeta_old = dbeta[j]\n            dbeta[j] += Xs[:, j].T @ ddual_var / (L[j] * n_samples)\n            dbeta[j] -= alpha * sign_beta[j] / L[j]\n            ddual_var -= Xs[:, j] * (dbeta[j] - dbeta_old)\n\n    @staticmethod\n    @njit\n    def _update_only_jac_sparse(\n            data, indptr, indices, y, n_samples, n_features,\n            dbeta, dual_var, ddual_var, L, alpha, sign_beta):\n        for j in range(n_features):\n            # get the j-st column of X in sparse format\n            Xjs = data[indptr[j]:indptr[j+1]]\n            # get the non zero idices\n            idx_nz = indices[indptr[j]:indptr[j+1]]\n            # store old beta j for fast update\n            dbeta_old = dbeta[j]\n            # update of the Jacobian dbeta\n            dbeta[j] += Xjs @ ddual_var[idx_nz] / (L[j] * n_samples)\n            dbeta[j] -= alpha * sign_beta[j] / L[j]\n            ddual_var[idx_nz] -= Xjs * (dbeta[j] - dbeta_old)\n\n    @staticmethod\n    @njit\n    def _reduce_alpha(alpha, mask):\n        return alpha\n\n    @staticmethod\n    def _get_grad(X, y, jac, mask, dense, alphas, v):\n        return alphas[mask] * np.sign(dense) @ jac\n\n    def proj_hyperparam(self, X, y, log_alpha):\n        \"\"\"Project hyperparameter on an admissible range of values.\n\n        Parameters\n        ----------\n        X: array-like, shape (n_samples, n_features)\n            Design matrix.\n        y: ndarray, shape (n_samples,)\n            Observation vector.\n        log_alpha: float\n            Logarithm of hyperparameter.\n\n        Returns\n        -------\n        log_alpha: float\n            Logarithm of projected hyperparameter.\n        \"\"\"\n        if not hasattr(self, \"log_alpha_max\"):\n            alpha_max = np.max(np.abs(X.T @ y))\n            alpha_max /= X.shape[0]\n            self.log_alpha_max = np.log(alpha_max)\n        return np.clip(log_alpha, self.log_alpha_max - 12,\n                       self.log_alpha_max + np.log(0.9))\n\n    @staticmethod\n    def get_L(X):\n        \"\"\"Compute Lipschitz constant of datafit.\n\n        Parameters\n        ----------\n        X: array-like, shape (n_samples, n_features)\n            Design matrix.\n\n        Returns\n        -------\n        L: float\n            The Lipschitz constant.\n        \"\"\"\n        if issparse(X):\n            return slinalg.norm(X, axis=0) ** 2 / (X.shape[0])\n        else:\n            return norm(X, axis=0) ** 2 / (X.shape[0])\n\n    def _use_estimator(self, X, y, alpha, tol):\n        if self.estimator is None:\n            raise ValueError(\"You did not pass a solver with sklearn API\")\n        self.estimator.set_params(tol=tol, alpha=alpha)\n        self.estimator.fit(X, y)\n        mask = self.estimator.coef_ != 0\n        dense = self.estimator.coef_[mask]\n        return mask, dense, None\n\n    @staticmethod\n    def reduce_X(X, mask):\n        \"\"\"Reduce design matrix to generalized support.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Design matrix.\n        mask : ndarray, shape (n_features,)\n            Generalized support.\n        \"\"\"\n        return X[:, mask]\n\n    @staticmethod\n    def reduce_y(y, mask):\n        \"\"\"Reduce observation vector to generalized support.\n\n        Parameters\n        ----------\n        y : ndarray, shape (n_samples,)\n            Observation vector.\n        mask : ndarray, shape (n_features,)  TODO shape n_samples right?\n            Generalized support.\n        \"\"\"\n        return y\n\n    def sign(self, x, log_alpha):\n        \"\"\"Get sign of iterate.\n\n        Parameters\n        ----------\n        x : ndarray, shape TODO\n        log_alpha : ndarray, shape TODO\n            Logarithm of hyperparameter.\n        \"\"\"\n        return np.sign(x)\n\n    def get_beta(self, X, y, mask, dense):\n        \"\"\"Return primal iterate.\n\n        Parameters\n        ----------\n        X: array-like, shape (n_samples, n_features)\n            Design matrix.\n        y: ndarray, shape (n_samples,)\n            Observation vector.\n        mask: ndarray, shape (n_features,)\n            Mask corresponding to non zero entries of beta.\n        dense: ndarray, shape (mask.sum(),)\n            Non zero entries of beta.\n        \"\"\"\n        return mask, dense\n\n    def get_jac_v(self, X, y, mask, dense, jac, v):\n        \"\"\"Compute hypergradient.\n\n        Parameters\n        ----------\n        X: array-like, shape (n_samples, n_features)\n            Design matrix.\n        y: ndarray, shape (n_samples,)\n            Observation vector.\n        mask: ndarray, shape (n_features,)\n            Mask corresponding to non zero entries of beta.\n        dense: ndarray, shape (mask.sum(),)\n            Non zero entries of beta.\n        jac: TODO\n        v: TODO\n        \"\"\"\n        return jac.T @ v(mask, dense)\n\n    @staticmethod\n    def get_mat_vec(X, y, mask, dense, log_alpha):\n        \"\"\"Returns a LinearOperator computing the matrix vector product\n        with the Hessian of datafit. It is necessary to avoid storing a\n        potentially large matrix, and keep advantage of the sparsity of X.\n\n        Parameters\n        ----------\n        X: array-like, shape (n_samples, n_features)\n            Design matrix.\n        y: ndarray, shape (n_samples,)\n            Observation vector.\n        mask: ndarray, shape (n_features,)\n            Mask corresponding to non zero entries of beta.\n        dense: ndarray, shape (mask.sum(),)\n            Non zero entries of beta.\n        log_alpha: ndarray\n            Logarithm of hyperparameter.\n        \"\"\"\n        X_m = X[:, mask]\n        n_samples, size_supp = X_m.shape\n\n        def mv(v):\n            return X_m.T @ (X_m @ v) / n_samples\n        return LinearOperator((size_supp, size_supp), matvec=mv)\n\n    def generalized_supp(self, X, v, log_alpha):\n        \"\"\"Generalized support of iterate.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Design matrix.\n        v : TODO\n        log_alpha : float\n            Log of hyperparameter.\n\n        Returns\n        -------\n        TODO\n        \"\"\"\n        return v\n\n    def get_jac_residual_norm(self, Xs, ys, n_samples, sign_beta, dbeta,\n                              dual_var, ddual_var, alpha):\n        return norm(ddual_var.T @ ddual_var +\n                    n_samples * alpha * sign_beta @ dbeta)\n\nimport numpy as np\nfrom numba import njit\nfrom numpy.linalg import norm\nfrom scipy.sparse import issparse\nimport scipy.sparse.linalg as slinalg\nfrom scipy.sparse.linalg import LinearOperator\n\nfrom sparse_ho.models.base import BaseModel\nfrom sparse_ho.utils import ST, init_dbeta0_new_p\n\n\nclass WeightedLasso(BaseModel):\n    r\"\"\"Linear Model trained with weighted L1 regularizer (aka weighted Lasso).\n\n    The optimization objective for weighted Lasso is:\n\n    ..math::\n\n        ||y - Xw||^2_2 / (2 * n_samples) + \\sum_i^{n_features} \\alpha_i |wi|\n\n    Parameters\n    ----------\n    estimator: instance of ``sklearn.base.BaseEstimator``\n        An estimator that follows the scikit-learn API.\n    \"\"\"\n\n    def __init__(self, estimator=None):\n        self.estimator = estimator\n\n    def _init_dbeta_ddual_var(self, X, y, mask0=None, jac0=None,\n                              dense0=None, compute_jac=True):\n        n_samples, n_features = X.shape\n        dbeta = np.zeros((n_features, n_features))\n        ddual_var = np.zeros((n_samples, n_features))\n        if jac0 is not None:\n            dbeta[np.ix_(mask0, mask0)] = jac0.copy()\n            ddual_var[:, mask0] = - X[:, mask0] @ jac0\n        return dbeta, ddual_var\n\n    def _init_beta_dual_var(self, X, y, mask0=None, dense0=None):\n        beta = np.zeros(X.shape[1])\n        if dense0 is None or len(dense0) == 0:\n            dual_var = y.copy()\n            dual_var = dual_var.astype(np.float)\n        else:\n            beta[mask0] = dense0.copy()\n            dual_var = y - X[:, mask0] @ dense0\n        return beta, dual_var\n\n    @staticmethod\n    @njit\n    def _update_beta_jac_bcd(\n            X, y, beta, dbeta, dual_var, ddual_var,\n            alpha, L, compute_jac=True):\n        n_samples, n_features = X.shape\n        non_zeros = np.where(L != 0)[0]\n\n        for j in non_zeros:\n            beta_old = beta[j]\n            if compute_jac:\n                dbeta_old = dbeta[j, :].copy()\n            zj = beta[j] + dual_var @ X[:, j] / (L[j] * n_samples)\n            beta[j:j+1] = ST(zj, alpha[j] / L[j])\n            if compute_jac:\n                dzj = dbeta[j, :] + X[:, j] @ ddual_var / (L[j] * n_samples)\n                dbeta[j:j+1, :] = np.abs(np.sign(beta[j])) * dzj\n                dbeta[j:j+1, j] -= alpha[j] * np.sign(beta[j]) / L[j]\n                # update residuals\n                ddual_var -= np.outer(X[:, j], (dbeta[j, :] - dbeta_old))\n            dual_var -= X[:, j] * (beta[j] - beta_old)\n\n    @staticmethod\n    @njit\n    def _update_beta_jac_bcd_sparse(\n            data, indptr, indices, y, n_samples, n_features, beta,\n            dbeta, dual_var, ddual_var, alphas, L, compute_jac=True):\n        non_zeros = np.where(L != 0)[0]\n\n        for j in non_zeros:\n            # get the j-st column of X in sparse format\n            Xjs = data[indptr[j]:indptr[j+1]]\n            # get non zero idices\n            idx_nz = indices[indptr[j]:indptr[j+1]]\n            ###########################################\n            beta_old = beta[j]\n            if compute_jac:\n                dbeta_old = dbeta[j, :].copy()\n            zj = beta[j] + dual_var[idx_nz] @ Xjs / (L[j] * n_samples)\n            beta[j:j+1] = ST(zj, alphas[j] / L[j])\n            if compute_jac:\n                dzj = dbeta[j, :] + Xjs @ ddual_var[idx_nz, :] / \\\n                    (L[j] * n_samples)\n                dbeta[j:j+1, :] = np.abs(np.sign(beta[j])) * dzj\n                dbeta[j:j+1, j] -= alphas[j] * np.sign(beta[j]) / L[j]\n                # update residuals\n                ddual_var[idx_nz, :] -= np.outer(\n                    Xjs, (dbeta[j, :] - dbeta_old))\n            dual_var[idx_nz] -= Xjs * (beta[j] - beta_old)\n\n    @staticmethod\n    @njit\n    def _update_bcd_jac_backward(\n            X, alpha, jac_t_v, beta, v_, L):\n        n_samples, n_features = X.shape\n        sign_beta = np.sign(beta)\n        for j in (np.arange(sign_beta.shape[0] - 1, -1, -1)):\n            jac_t_v[j] = jac_t_v[j] - (v_[j]) * alpha[j] * sign_beta[j] / L[j]\n            v_[j] *= np.abs(sign_beta[j])\n            v_ -= v_[j] / (L[j] * n_samples) * X[:, j] @ X\n        return jac_t_v\n\n    @staticmethod\n    def _get_pobj(dual_var, X, beta, alphas, y=None):\n        n_samples = dual_var.shape[0]\n        return (\n            norm(dual_var) ** 2 / (2 * n_samples) + norm(alphas * beta, 1))\n\n    @staticmethod\n    def _get_pobj0(dual_var, beta, alphas, y=None):\n        n_samples = dual_var.shape[0]\n        return norm(y) ** 2 / (2 * n_samples)\n\n    @staticmethod\n    def _get_jac(dbeta, mask):\n        return dbeta[np.ix_(mask, mask)]\n\n    @staticmethod\n    def _init_dbeta0(mask, mask0, jac0):\n        size_mat = mask.sum()\n        if jac0 is None:\n            dbeta0_new = np.zeros((size_mat, size_mat))\n        else:\n            dbeta0_new = init_dbeta0_new_p(jac0, mask, mask0)\n        return dbeta0_new\n\n    @staticmethod\n    def _init_dbeta(n_features):\n        dbeta = np.zeros((n_features, n_features))\n        return dbeta\n\n    @staticmethod\n    def _init_ddual_var(dbeta, X, y, sign_beta, alpha):\n        return - X @ dbeta\n\n    @staticmethod\n    def _init_g_backward(jac_v0, n_features):\n        if jac_v0 is None:\n            return np.zeros(n_features)\n        else:\n            return jac_v0\n\n    @staticmethod\n    @njit\n    def _update_only_jac(Xs, y, dual_var, dbeta, ddual_var, L,\n                         alpha, sign_beta):\n        n_samples, n_features = Xs.shape\n        for j in range(n_features):\n            dbeta_old = dbeta[j, :].copy()\n            dbeta[j:j+1, :] = dbeta[j, :] + Xs[:, j] @ ddual_var / \\\n                (L[j] * n_samples)\n            dbeta[j:j+1, j] -= alpha[j] * np.sign(sign_beta[j]) / L[j]\n            # update residuals\n            ddual_var -= np.outer(Xs[:, j], (dbeta[j, :] - dbeta_old))\n\n    @staticmethod\n    @njit\n    def _update_only_jac_sparse(\n            data, indptr, indices, y, n_samples, n_features, dbeta, dual_var,\n            ddual_var, L, alpha, sign_beta):\n        for j in range(n_features):\n            # get the j-st column of X in sparse format\n            Xjs = data[indptr[j]:indptr[j+1]]\n            # get the non zero idices\n            idx_nz = indices[indptr[j]:indptr[j+1]]\n            # store old beta j for fast update\n            dbeta_old = dbeta[j, :].copy()\n\n            dbeta[j:j+1, :] += Xjs @ ddual_var[idx_nz] / (L[j] * n_samples)\n            dbeta[j, j] -= alpha[j] * np.sign(sign_beta[j]) / L[j]\n            ddual_var[idx_nz] -= np.outer(Xjs, (dbeta[j] - dbeta_old))\n\n    # @njit\n    @staticmethod\n    def _reduce_alpha(alpha, mask):\n        return alpha[mask]\n\n    @staticmethod\n    def get_full_jac_v(mask, jac_v, n_features):\n        \"\"\"TODO\n\n        Parameters\n        ----------\n        mask: TODO\n        jac_v: TODO\n        n_features: int\n            Number of features.\n        \"\"\"\n        # MM sorry I don't get what this does\n        # TODO n_features should be n_hyperparams, right ?\n        res = np.zeros(n_features)\n        res[mask] = jac_v\n        return res\n\n    @staticmethod\n    def get_mask_jac_v(mask, jac_v):\n        \"\"\"TODO\n\n        Parameters\n        ----------\n        mask: TODO\n        jac_v: TODO\n        \"\"\"\n        return jac_v[mask]\n\n    @staticmethod\n    def _get_grad(X, y, jac, mask, dense, alphas, v):\n        size_supp = mask.sum()\n        jac_t_v = np.zeros(size_supp)\n        jac_t_v = alphas[mask] * np.sign(dense) * jac\n        return jac_t_v\n\n    def proj_hyperparam(self, X, y, log_alpha):\n        \"\"\"Project hyperparameter on an admissible range of values.\n\n        Parameters\n        ----------\n        X: array-like, shape (n_samples, n_features)\n            Design matrix.\n        y: ndarray, shape (n_samples,)\n            Observation vector.\n        log_alpha: ndarray, shape (n_features,)\n            Logarithm of hyperparameter.\n\n        Returns\n        -------\n        log_alpha: ndarray, shape (n_features,)\n            Logarithm of projected hyperparameter.\n        \"\"\"\n        if not hasattr(self, \"log_alpha_max\"):\n            alpha_max = np.max(np.abs(X.T @ y)) / X.shape[0]\n            self.log_alpha_max = np.log(alpha_max)\n        log_alpha = np.clip(log_alpha, self.log_alpha_max - 5,\n                            self.log_alpha_max + np.log(0.9))\n        return log_alpha\n\n    @staticmethod\n    def get_L(X):\n        \"\"\"Compute Lipschitz constant of datafit.\n\n        Parameters\n        ----------\n        X: array-like, shape (n_samples, n_features)\n            Design matrix.\n\n        Returns\n        -------\n        L: float\n            The Lipschitz constant.\n        \"\"\"\n        if issparse(X):\n            return slinalg.norm(X, axis=0) ** 2 / (X.shape[0])\n        else:\n            return norm(X, axis=0) ** 2 / (X.shape[0])\n\n    @staticmethod\n    def get_mat_vec(X, y, mask, dense, log_alpha):\n        \"\"\"Returns a LinearOperator computing the matrix vector product\n        with the Hessian of datafit. It is necessary to avoid storing a\n        potentially large matrix, and keep advantage of the sparsity of X.\n\n        Parameters\n        ----------\n        X: array-like, shape (n_samples, n_features)\n            Design matrix.\n        y: ndarray, shape (n_samples,)\n            Observation vector.\n        mask: ndarray, shape (n_features,)\n            Mask corresponding to non zero entries of beta.\n        dense: ndarray, shape (mask.sum(),)\n            Non zero entries of beta.\n        log_alpha: ndarray\n            Logarithm of hyperparameter.\n        \"\"\"\n        X_m = X[:, mask]\n        n_samples, size_supp = X_m.shape\n\n        def mv(v):\n            return X_m.T @ (X_m @ v) / n_samples\n        return LinearOperator((size_supp, size_supp), matvec=mv)\n\n    def _use_estimator(self, X, y, alpha, tol):\n        self.estimator.set_params(tol=tol)\n        self.estimator.weights = alpha\n        self.estimator.fit(X, y)\n        mask = self.estimator.coef_ != 0\n        dense = (self.estimator.coef_)[mask]\n        return mask, dense, None\n\n    @staticmethod\n    def reduce_X(X, mask):\n        \"\"\"Reduce design matrix to generalized support.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Design matrix.\n        mask : ndarray, shape (n_features,)\n            Generalized support.\n        \"\"\"\n        return X[:, mask]\n\n    @staticmethod\n    def reduce_y(y, mask):\n        \"\"\"Reduce observation vector to generalized support.\n\n        Parameters\n        ----------\n        y : ndarray, shape (n_samples,)\n            Observation vector.\n        mask : ndarray, shape (n_features,)  TODO shape n_samples right?\n            Generalized support.\n        \"\"\"\n        return y\n\n    def sign(self, x, log_alpha):\n        \"\"\"Get sign of iterate.\n\n        Parameters\n        ----------\n        x : ndarray, shape TODO\n        log_alpha : ndarray, shape TODO\n            Logarithm of hyperparameter.\n        \"\"\"\n        return np.sign(x)\n\n    def get_beta(self, X, y, mask, dense):\n        \"\"\"Return primal iterate.\n\n        Parameters\n        ----------\n        X: array-like, shape (n_samples, n_features)\n            Design matrix.\n        y: ndarray, shape (n_samples,)\n            Observation vector.\n        mask: ndarray, shape (n_features,)\n            Mask corresponding to non zero entries of beta.\n        dense: ndarray, shape (mask.sum(),)\n            Non zero entries of beta.\n        \"\"\"\n        # TODO what's the use of this function? it does nothing for all models\n        return mask, dense\n\n    def get_jac_v(self, X, y, mask, dense, jac, v):\n        \"\"\"Compute hypergradient.\n\n        Parameters\n        ----------\n        X: array-like, shape (n_samples, n_features)\n            Design matrix.\n        y: ndarray, shape (n_samples,)\n            Observation vector.\n        mask: ndarray, shape (n_features,)\n            Mask corresponding to non zero entries of beta.\n        dense: ndarray, shape (mask.sum(),)\n            Non zero entries of beta.\n        jac: TODO\n        v: TODO\n        \"\"\"\n        # TODO this is the same for Lasso, Enet, Wlasso. Maybe inherit from\n        # a common class, LinearModelPrimal or something?\n        return jac.T @ v(mask, dense)\n\n    def generalized_supp(self, X, v, log_alpha):\n        \"\"\"Generalized support of iterate.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Design matrix.\n        v : TODO\n        log_alpha : float\n            Log of hyperparameter.\n\n        Returns\n        -------\n        TODO\n        \"\"\"\n        return v\n\n    def get_jac_residual_norm(self, Xs, ys, n_samples, sign_beta,\n                              dbeta, dual_var, ddual_var, alpha):\n        return(\n            norm(ddual_var.T @ ddual_var +\n                 n_samples * alpha * sign_beta @ dbeta))\n\n\n\n# Authors: Quentin Bertrand <quentin.bertrand@inria.fr>\n#          Quentin Klopfenstein <quentin.klopfenstein@u-bourgogne.fr>\n#\n# License: BSD (3-clause)\n\n# This files contains the functions to perform first order descent for HO\n# hyperparameter setting\n\n\nimport numpy as np\nfrom hyperopt import hp\nfrom hyperopt import fmin, tpe, rand\nfrom functools import partial\nfrom sklearn.utils import check_random_state\n\n\ndef grad_search(\n        algo, criterion, model, optimizer, X, y, alpha0, monitor):\n    \"\"\"\n    Parameters\n    ----------\n    algo: instance of BaseAlgo\n        algorithm used to compute hypergradient.\n    criterion:  instance of BaseCriterion\n        criterion to optimize during hyperparameter optimization\n        (outer optimization problem).\n    model: instance of BaseModel\n        model on which hyperparameter has to be selected\n        (inner optimization problem).\n    optimizer: instance of Optimizer\n        optimizer used to minimize the criterion (outer optimization)\n    X: array like of shape (n_samples, n_features)\n        Design matrix.\n    y: array like of shape (n_samples,)\n        Target.\n    alpha0: float\n        initial value of the hyperparameter alpha.\n    monitor: instance of Monitor\n        used to store the value of the cross-validation function.\n\n\n    Returns\n    -------\n    XXX missing\n    \"\"\"\n\n    def _get_val_grad(log_alpha, tol, monitor):\n        return criterion.get_val_grad(\n            model, X, y, log_alpha, algo.compute_beta_grad, tol=tol,\n            monitor=monitor)\n\n    def _proj_hyperparam(log_alpha):\n        return criterion.proj_hyperparam(model, X, y, log_alpha)\n\n    return optimizer._grad_search(\n        _get_val_grad, _proj_hyperparam, np.log(alpha0), monitor)\n\n\ndef hyperopt_wrapper(\n        algo, criterion, model, X, y, alpha_min, alpha_max, monitor,\n        max_evals=50, tol=1e-5, random_state=42, t_max=100_000,\n        method='bayesian', size_space=1):\n    \"\"\"\n    Parameters\n    ----------\n    algo: instance of BaseAlgo\n        algorithm used to compute hypergradient.\n    criterion:  instance of BaseCriterion\n        criterion to optimize during hyperparameter optimization\n        (outer optimization problem).\n    model:  instance of BaseModel\n        model on which hyperparameter has to be selected\n        (inner optimization problem).\n    X: array like of shape (n_samples, n_features)\n        Design matrix.\n    y: array like of shape (n_samples,)\n        Target.\n    alpha_min: float\n        minimum value for the regularization coefficient alpha.\n    alpha_max: float\n        maximum value for the regularization coefficient alpha.\n    monitor: instance of Monitor\n        used to store the value of the cross-validation function.\n    max_evals: int (default=50)\n        maximum number of evaluation of the function\n    tol: float (default=1e-5)\n        tolerance for TODO\n    random_state: int or instance of RandomState\n        Random number generator used for reproducibility.\n    t_max: int, optional (default=100_000)\n        TODO\n    method: 'random' | 'bayesian' (default='bayesian')\n        method for hyperopt\n    size_space: int (default=1)\n        size of the hyperparameter space\n\n    Returns\n    -------\n    monitor:\n        The instance of Monitor used during iterations.\n    \"\"\"\n\n    def objective(log_alpha):\n        log_alpha = np.array(log_alpha)\n        val_func = criterion.get_val(\n            model, X, y, log_alpha, monitor, tol=tol)\n        return val_func\n\n    # TODO, also size_space = n_hyperparam ?\n    space = [\n        hp.uniform(str(dim), np.log(alpha_min), np.log(alpha_max)) for\n        dim in range(size_space)]\n\n    rng = check_random_state(random_state)\n\n    if method == \"bayesian\":\n        algo = partial(tpe.suggest, n_startup_jobs=5)\n        fmin(\n            objective, space, algo=algo, max_evals=max_evals,\n            timeout=t_max, rstate=rng)\n    elif method == \"random\":\n        fmin(\n            objective, space, algo=rand.suggest, max_evals=max_evals,\n            timeout=t_max, rstate=rng)\n    return monitor\n\nimport numpy as np\nfrom numpy.linalg import norm\n\nfrom sparse_ho.optimizers.base import BaseOptimizer\n\n\nclass LineSearch(BaseOptimizer):\n    \"\"\"Gradient descent with line search for the outer problem.\n\n    The code is taken from here:\n    https://github.com/fabianp/hoag/blob/master/hoag/hoag.py\n\n    Parameters\n    ----------\n    n_outer: int, optional (default=100).\n        number of maximum updates of alpha.\n    verbose: bool, optional (default=False)\n        Verbosity.\n    tolerance_decrease: string, optional (default=\"constant\")\n        Tolerance decrease strategy for approximate gradient.\n    tol : float, optional (default=1e-5)\n        Tolerance for the inner optimization solver.\n    t_max: float, optional (default=10000)\n        Maximum running time threshold in seconds.\n    \"\"\"\n\n    def __init__(\n            self, n_outer=100, verbose=False, tolerance_decrease='constant',\n            tol=1e-5, t_max=10000):\n        self.n_outer = n_outer\n        self.verbose = verbose\n        self.tolerance_decrease = tolerance_decrease\n        self.tol = tol\n        self.t_max = t_max\n\n    def _grad_search(\n            self, _get_val_grad, proj_hyperparam, log_alpha0, monitor):\n\n        is_multiparam = isinstance(log_alpha0, np.ndarray)\n        if is_multiparam:\n            log_alphak = log_alpha0.copy()\n            old_log_alphak = log_alphak.copy()\n        else:\n            log_alphak = log_alpha0\n            old_log_alphak = log_alphak\n\n        grad_norms = []\n\n        L_log_alpha = None\n        value_outer_old = np.inf\n\n        if self.tolerance_decrease == 'exponential':\n            seq_tol = np.geomspace(1e-2, self.tol, self.n_outer)\n        else:\n            seq_tol = self.tol * np.ones(self.n_outer)\n\n        for i in range(self.n_outer):\n            tol = seq_tol[i]\n            try:\n                old_tol = seq_tol[i - 1]\n            except Exception:\n                old_tol = seq_tol[0]\n            value_outer, grad_outer = _get_val_grad(\n                log_alphak, tol=tol, monitor=monitor)\n\n            grad_norms.append(norm(grad_outer))\n            if np.isnan(grad_norms[-1]):\n                print(\"Nan present in gradient\")\n                break\n\n            if L_log_alpha is None:\n                if grad_norms[-1] > 1e-3:\n                    # make sure we are not selecting a step size\n                    # that is too small\n                    if is_multiparam:\n                        L_log_alpha = grad_norms[-1] / np.sqrt(len(log_alphak))\n                    else:\n                        L_log_alpha = grad_norms[-1]\n                else:\n                    L_log_alpha = 1\n            step_size = (1. / L_log_alpha)\n            try:\n                old_log_alphak = log_alphak.copy()\n            except Exception:\n                old_log_alphak = log_alphak\n            log_alphak -= step_size * grad_outer\n\n            incr = norm(step_size * grad_outer)\n            C = 0.25\n            factor_L_log_alpha = 1.0\n            if value_outer <= value_outer_old + C * tol + \\\n                    old_tol * (C + factor_L_log_alpha) * incr - \\\n                    factor_L_log_alpha * (L_log_alpha) * incr * incr:\n                L_log_alpha *= 0.95\n                if self.verbose > 1:\n                    print('increased step size')\n                log_alphak -= step_size * grad_outer\n\n            elif value_outer >= 1.2 * value_outer_old:\n                if self.verbose > 1:\n                    print('decrease step size')\n                # decrease step size\n                L_log_alpha *= 2\n                if is_multiparam:\n                    log_alphak = old_log_alphak.copy()\n                else:\n                    log_alphak = old_log_alphak\n                print('!!step size rejected!!', value_outer, value_outer_old)\n                value_outer, grad_outer = _get_val_grad(\n                    log_alphak, tol=tol, monitor=monitor)\n\n                tol *= 0.5\n            else:\n                old_log_alphak = log_alphak.copy()\n                log_alphak -= step_size * grad_outer\n\n            log_alphak = proj_hyperparam(log_alphak)\n            value_outer_old = value_outer\n\n            if self.verbose:\n                print(\n                    \"Iteration %i/%i || \" % (i+1, self.n_outer) +\n                    \"Value outer criterion: %.2e || \" % value_outer +\n                    \"norm grad %.2e\" % norm(grad_outer))\n            if monitor.times[-1] > self.t_max:\n                break\n        return log_alphak, value_outer, grad_outer",
    "Experiment Result": "The methodology is evaluated using various Lasso-type estimators, including Lasso, ElasticNet, Weighted Lasso, and Sparse Logistic Regression.\n\n**Datasets and Data Splitting:**\n- 'rcv1.binary', 'simu' (synthetic data generated by `make_classification` or `make_correlated_data`), and MEG data (from `mne.datasets.sample`) are used.\n- Data is commonly split into training and validation sets (e.g., 50/50 split), with `idx_train = np.arange(0, n_samples // 2)` and `idx_val = np.arange(n_samples // 2, n_samples)`.\n- K-Fold cross-validation (`sklearn.model_selection.KFold`) is also employed, for example, with `n_splits=5` and `shuffle=True`.\n\n**Hyperparameter Setup:**\n- The maximum regularization parameter (`alpha_max`) is typically computed as `np.max(np.abs(X[idx_train, :].T @ y[idx_train])) / len(idx_train)` (adjusted for specific models like Logistic Regression or Elastic Net).\n- Hyperparameters are parametrized exponentially to avoid positivity constraints and scaling issues. For instance, `alphas = alpha_max * np.geomspace(1, 0.0001, n_alphas)` creates a logarithmically spaced grid of 30 values (e.g., `n_alphas=30`).\n- Starting points for gradient-based optimizers (`alpha0`) are set as a fraction of `alpha_max`, such as `alpha_max / 10` or `np.array([alpha_max * 0.9, alpha_max * 0.9])` for 2D hyperparameters.\n\n**Inner Optimization (Lasso-type Solvers):**\n- State-of-the-art Lasso solvers are used, including:\n  - `celer.Lasso(penalty='l1', fit_intercept=False, max_iter=50, warm_start=True, tol=...)\n  - `celer.ElasticNet(fit_intercept=False, max_iter=50, warm_start=True, tol=...)\n  - `sklearn.linear_model.LogisticRegression(penalty='l1', fit_intercept=False, solver='saga', max_iter=100, tol=...)\n- Inner solver tolerance (`tol`) typically ranges from `1e-8` to `1e-3`.\n- Maximum iterations (`max_iter`) for the inner solver vary, e.g., `50` to `100` for Celer models, or `1000` to `100000` for scikit-learn models.\n- `warm_start=True` is generally enabled.\n\n**Implicit Forward Iterative Differentiation Algorithm:**\n- The `sparse_ho.ImplicitForward` algorithm is central to computing the hypergradient.\n- It is configured with `tol_jac` (tolerance for Jacobian computation), typically `1e-8` or `1e-3`.\n- `n_iter_jac` (maximum iterations for Jacobian computation) is set, e.g., `100` or `1000`.\n\n**Outer Optimization (Gradient-based Optimizers):**\n- `grad_search` is used to optimize the hyperparameters.\n- Various gradient-based optimizers are employed:\n  - `LineSearch(n_outer=10, tol=...)`\n  - `GradientDescent(n_outer=10, step_size=100, p_grad_norm=1.5, verbose=True, tol=...)`\n  - `Adam(n_outer=10, lr=0.11, beta_1=0.9, beta_2=0.999)`\n- `n_outer` (number of outer iterations) is typically `10` or `25` (up to `100` in some `expes`).\n\n**Overall Approach:**\n- Hyperparameters are optimized as a bi-level problem using implicit differentiation, leveraging the fixed-point iteration property of proximal BCD algorithms.\n- The Jacobian is computed by applying forward differentiation recursion steps restricted to the identified support, without explicitly solving a linear system."
}{
    "Title": "Stochastic Marginal Likelihood Gradients using Neural Tangent Kernels",
    "Main Contributions": "This research addresses the challenge of scaling hyperparameter optimization in deep learning, which traditionally requires full-dataset passes for each gradient estimate of the marginal likelihood. The main contribution is the introduction of novel lower bounds to the linearized Laplace approximation of the marginal likelihood. These bounds are amenable to stochastic-gradient-based optimization, allowing for efficient hyperparameter learning using mini-batches of data. The paper derives these bounds from the function-space form of the linearized Laplace, estimated using the Neural Tangent Kernel (NTK), and shows that existing structured approximations are also lower bounds. Experimentally, these estimators significantly accelerate gradient-based hyperparameter optimization, enabling up to 25-fold speed-ups and application to larger datasets.",
    "Methodology": "The methodology is based on Bayesian model selection, optimizing hyperparameters by maximizing the marginal likelihood using linearized Laplace approximation. The core involves deriving lower bounds to the log marginal likelihood. Two main types of bounds are introduced: Parametric structured lower bounds (Theorem 1), which justify block-diagonal and diagonal approximations of the Generalized Gauss-Newton (GGN) Hessian, and Subset-of-Data Kernel Bounds (Theorem 2), derived from the dual NTK form of the log-determinant. These NTK-based bounds allow partitioning the data and outputs into batches for stochastic estimation. A further Parametric data subset bound (Theorem 3) and a Parametric doubly lower bound (Corollary 3.1) combine both concepts, enabling structured parametric approximations like KFAC on data subsets. The method allows trade-offs between estimation accuracy and computational complexity by varying batch sizes and partitioning strategies (e.g., output-wise or class-wise partitioning). Unbiased stochastic estimates and gradients are obtained by uniformly sampling index sets for batches.",
    "Experimental Setup": "The estimators were validated on several datasets and tasks: MNIST (1000 random digits, rotated and original) with a small 3-layer convolutional neural network (approx. 16,000 parameters) for illustrative purposes. CIFAR-10 and CIFAR-100 with a Wide ResNet 16-4 (using Fixup initialization) for learning layer-wise prior precisions (weight decay) and affine invariances. TinyImagenet (100,000 data points, 200 classes) with a ResNet-50 (approx. 23 million parameters) for scaling to larger datasets. Validation involved comparing the tightness of derived bounds against the exact linearized Laplace approximation (where computable), benchmark against state-of-the-art full-batch KFAC-Laplace, and evaluation of test log likelihood and accuracy. Runtimes were compared on NVIDIA A100 GPUs.",
    "Limitations": "One limitation is that the derived lower bounds, while enabling stochastic optimization, introduce 'slack' compared to the exact marginal likelihood. Very small subset sizes can lead to failures during optimization, similar to the diagonal approximation. For certain tasks like prior precision optimization, stochastic NTK-based bounds can result in a more significant reduction in test performance compared to full-batch methods. Additionally, full-batch Laplace approximations, such as KFAC-N-C, proved intractable and ran out of memory for larger datasets and models like CIFAR-100 and TinyImagenet, highlighting a scalability constraint for previous methods. The assumption of a diagonal prior P0 is made in some proofs.",
    "Future Research Directions": "Future research could focus on finding ways to make the bounds tighter, for example, by improving the partitioning strategies of the Neural Tangent Kernel (NTK). It would also be interesting to develop methods that track (anti-)correlation between inputs in the NTK to group them optimally during training, similar to inducing point optimization. Another direction is to apply these fast stochastic estimators to large-scale Bayesian linear models. The efficiency of stochastic estimators for general differentiable hyperparameter optimization is identified as an exciting future area.",
    "Experiment Code": "def marglik_optimization(model,train_loader,marglik_loader=None,valid_loader=None,partial_loader=None,likelihood='classification',prior_structure='layerwise',prior_prec_init=1.,sigma_noise_init=1.,temperature=1.,n_epochs=500,lr=1e-1,lr_min=None,optimizer='SGD',scheduler='cos',n_epochs_burnin=0,n_hypersteps=100,n_hypersteps_prior=1,marglik_frequency=1,lr_hyp=1e-1,lr_hyp_min=1e-1,lr_aug=1e-2,lr_aug_min=1e-2,laplace=KronLaplace,backend=AsdlGGN,independent=False,single_output=False,single_output_iid=False,kron_jac=True,method='baseline',augmenter=None,stochastic_grad=False,use_wandb=False):\"\"\"Runs marglik optimization training for a given model and training dataloader.Parameters----------model : torch.nn.Moduletorch modeltrain_loader : DataLoaderpytorch training dataset loadermarglik_loader : DataLoaderpytorch data loader for fitting Laplacemarglik_loader is used for fitting the Laplace approximation, typically a subset or the full training data.valid_loader : DataLoaderpytorch data loader for validationpartial_loader : DataLoaderpytorch data loader for partial fitting for lila's grad accumulationlikelihood : str'classification' or 'regression'prior_structure : str'scalar', 'layerwise', 'diagonal'prior_prec_init : floatinitial prior precisionsigma_noise_init : floatinitial observation noise (for regression only)temperature : factortemperature of the likelihood; lower temperature leads to moreconcentrated posterior and vice versa.n_epochs : intlr : floatlearning rate for model optimizerlr_min : floatminimum learning rate, defaults to lr and hence no decayto have the learning rate decay from 1e-3 to 1e-6, setlr=1e-3 and lr_min=1e-6.optimizer : streither 'Adam' or 'SGD'scheduler : streither 'exp' for exponential and 'cos' for cosine decay towards lr_minn_epochs_burnin : int default=0how many epochs to train without estimating and differentiating marglikn_hypersteps : inthow many steps to take on diff. hyperparameters when marglik is estimatedn_hypersteps_prior : inthow many steps to take on the prior when marglik is estimatedmarglik_frequency : inthow often to estimate (and differentiate) the marginal likelihoodlr_hyp : floatlearning rate for hyperparameters (should be between 1e-3 and 1)lr_hyp_min : floatminimum learning rate, decayed to using cosine schedulelr_aug : floatlearning rate for augmentation parameterslr_aug_min : floatminimum learning rate, decayed to using cosine schedulelaplace : Laplacetype of Laplace approximation (Kron/Diag/Full)backend : BackendAsdlGGN/AsdlEF or BackPackGGN/BackPackEFindependent : boolwhether to use independent functional laplacesingle_output : boolwhether to use single random output for functional laplacesingle_output_iid : boolwhether to sample single output per sample iid (otherwise per batch)kron_jac : boolwhether to use kron_jac in the backendmethod : augmentation strategy, one of ['baseline'] -> no changeor ['lila'] -> change in protocol.augmenter : torch.nn.Module with differentiable parameterstochastic_grad : boolwhether to use stochastic gradients of marginal likelihoodusually would correspond to lower bound (unless small data)Returns-------lap : Laplacelapalce approximationmodel : torch.nn.Moduledemodel : torch.nn.Modulemargliks : listvalid_perfs : listaug_history: listNone for method == 'baseline'\"\"\"if lr_min is None: # don't decay lrlr_min = lrif marglik_loader is None:marglik_loader = train_loaderif partial_loader is None:partial_loader = marglik_loaderdevice = parameters_to_vector(model.parameters()).deviceN = len(train_loader.dataset)H = len(list(model.parameters()))P = len(parameters_to_vector(model.parameters()))optimize_aug = augmenter is not None and parameters_to_vector(augmenter.parameters()).requires_gradbackend_kwargs = dict(differentiable=(stochastic_grad and optimize_aug) or laplace is FunctionalLaplace,kron_jac=kron_jac)la_kwargs = dict(sod=stochastic_grad, single_output=single_output, single_output_iid=single_output_iid)if laplace is FunctionalLaplace:la_kwargs['independent'] = independentif use_wandb:wandb.config.update(dict(n_params=P, n_param_groups=H, n_data=N))# differentiable hyperparametershyperparameters = list()# prior precisionlog_prior_prec = get_prior_hyperparams(prior_prec_init, prior_structure, H, P, device)hyperparameters.append(log_prior_prec)# set up loss (and observation noise hyperparam)if likelihood == 'classification':criterion = CrossEntropyLoss(reduction='mean')sigma_noise = 1elif likelihood == 'regression':criterion = MSELoss(reduction='mean')log_sigma_noise_init = np.log(sigma_noise_init)log_sigma_noise = log_sigma_noise_init * torch.ones(1, device=device)log_sigma_noise.requires_grad = Truehyperparameters.append(log_sigma_noise)# set up model optimizer and scheduleroptimizer = get_model_optimizer(optimizer, model, lr)scheduler = get_scheduler(scheduler, optimizer, train_loader, n_epochs, lr, lr_min)n_steps = ((n_epochs - n_epochs_burnin) // marglik_frequency) * n_hypersteps_priorhyper_optimizer = Adam(hyperparameters, lr=lr_hyp)hyper_scheduler = CosineAnnealingLR(hyper_optimizer, n_steps, eta_min=lr_hyp_min)if optimize_aug:logging.info('MARGLIK: optimize augmentation.')aug_optimizer = Adam(augmenter.parameters(), lr=lr_aug)n_steps = ((n_epochs - n_epochs_burnin) // marglik_frequency) * (n_hypersteps if stochastic_grad else 1)aug_scheduler = CosineAnnealingLR(aug_optimizer, n_steps, eta_min=lr_aug_min)aug_history = [parameters_to_vector(augmenter.parameters()).squeeze().detach().cpu().numpy()]losses = list()valid_perfs = list()margliks = list()for epoch in range(1, n_epochs + 1):epoch_time_fwd = 0.0epoch_time_fit = 0.0epoch_loss = 0epoch_perf = 0epoch_nll = 0epoch_log = dict(epoch=epoch)# standard NN training per batchtorch.cuda.empty_cache()for X, y in train_loader:X, y = X.detach().to(device), y.to(device)optimizer.zero_grad()if likelihood == 'regression':sigma_noise = torch.exp(log_sigma_noise).detach()crit_factor = 1 / temperature / (2 * sigma_noise.square())else:crit_factor = 1 / temperatureprior_prec = torch.exp(log_prior_prec).detach()delta = expand_prior_precision(prior_prec, model)# fit datatime_fwd = time()if method == 'lila':f = model(X).mean(dim=1)else:f = model(X)epoch_time_fwd += time() - time_fwd # log total time fwd fit in epochtime_fit = time()theta = parameters_to_vector(model.parameters())loss = criterion(f, y) + (0.5 * (delta * theta) @ theta) / N / crit_factorloss.backward()optimizer.step()epoch_time_fit += time() - time_fit # log total time bwd fit in epochepoch_loss += loss.cpu().item() / len(train_loader)epoch_nll += criterion(f.detach(), y).item() / len(train_loader)if likelihood == 'regression':epoch_perf += (f.detach() - y).square().sum() / Nelse:epoch_perf += torch.sum(torch.argmax(f.detach(), dim=-1) == y).item() / Nscheduler.step()losses.append(epoch_loss)logging.info('MAP memory allocated: ' + str(torch.cuda.max_memory_allocated(loss.device)/GB_FACTOR) + ' Gb.')logging.info(f'MARGLIK[epoch={epoch}]: train. perf={epoch_perf*100:.2f}%; loss={epoch_loss:.5f}; nll={epoch_nll:.5f}')optimizer.zero_grad(set_to_none=True)llr = scheduler.get_last_lr()[0]epoch_log.update({'train/loss': epoch_loss, 'train/nll': epoch_nll, 'train/perf': epoch_perf, 'train/lr': llr,'train/time_fwd': epoch_time_fwd, 'train/time_fit': epoch_time_fit})if use_wandb and ((epoch % 5) == 0):wandb_log_parameter_norm(model)# compute validation error to report during trainingif valid_loader is not None:with torch.no_grad():val_perf, val_nll = valid_performance(model, valid_loader, likelihood, criterion, method, device)valid_perfs.append(val_perf)logging.info(f'MARGLIK[epoch={epoch}]: valid. perf={val_perf*100:.2f}%; nll={val_nll:.5f}.')epoch_log.update({'valid/perf': val_perf, 'valid/nll': val_nll})# only update hyperparameters every \"Frequency\" steps after \"burnin\"if (epoch % marglik_frequency) != 0 or epoch < n_epochs_burnin:if use_wandb:wandb.log(epoch_log, step=epoch, commit=((epoch % 10) == 0))continue# optimizer hyperparameters by differentiating margliktime_hyper = time()# 1. fit laplace approximationtorch.cuda.empty_cache()if optimize_aug:if stochastic_grad: # differentiablemarglik_loader.attach()else: # jvpmarglik_loader.detach()# first optimize prior precision jointly with direct marglik grads.margliks_local = list()n_hyper = max(n_hypersteps_prior, n_hypersteps) if stochastic_grad else n_hypersteps_priorfor i in range(n_hyper):if i == 0 or stochastic_grad:sigma_noise = 1 if likelihood == 'classification' else torch.exp(log_sigma_noise)prior_prec = torch.exp(log_prior_prec)lap = laplace(model, likelihood, sigma_noise=sigma_noise, prior_precision=prior_prec,temperature=temperature, backend=backend, backend_kwargs=backend_kwargs,**la_kwargs)lap.fit(marglik_loader)if i < n_hypersteps and optimize_aug and stochastic_grad:aug_optimizer.zero_grad()if i < n_hypersteps_prior:hyper_optimizer.zero_grad()if i < n_hypersteps_prior and not stochastic_grad: # does not fit every itsigma_noise = None if likelihood == 'classification' else torch.exp(log_sigma_noise)prior_prec = torch.exp(log_prior_prec)marglik = -lap.log_marginal_likelihood(prior_prec, sigma_noise) / Nelse: # fit with updated hparamsmarglik = -lap.log_marginal_likelihood() / Nmarglik.backward()margliks_local.append(marglik.item())if i < n_hypersteps_prior:hyper_optimizer.step()hyper_scheduler.step()if i < n_hypersteps and optimize_aug and stochastic_grad:aug_optimizer.step()aug_scheduler.step()if stochastic_grad:marglik = np.mean(margliks_local)else:marglik = margliks_local[-1]if use_wandb:wandb_log_prior(torch.exp(log_prior_prec.detach()), prior_structure, model)if likelihood == 'regression':epoch_log['hyperparams/sigma_noise'] = torch.exp(log_sigma_noise.detach()).cpu().item()epoch_log['train/marglik'] = margliklogging.info('LA memory allocated: ' + str(torch.cuda.max_memory_allocated(loss.device)/GB_FACTOR) + ' Gb.')# option 2: jvp (not direct_grad)torch.cuda.empty_cache()if optimize_aug and not stochastic_grad: # accumulate gradient with JVPpartial_loader.attach()aug_grad = torch.zeros_like(parameters_to_vector(augmenter.parameters()))lap.backend.differentiable = Trueif isinstance(lap, KronLaplace):# does the inversion internallyhess_inv = lap.posterior_precision.jvp_logdet()else:hess_inv = lap.posterior_covariance.flatten()for i, (X, y) in zip(range(n_hypersteps), partial_loader):lap.loss, H_batch = lap._curv_closure(X, y, N)# curv closure creates gradient already, need to zeroaug_optimizer.zero_grad()# compute grad wrt. neg. log-lik(- lap.log_likelihood).backward(inputs=list(augmenter.parameters()), retain_graph=True)# compute grad wrt. log det = 0.5 vec(P_inv) @ (grad-vec H)(0.5 * H_batch.flatten()).backward(gradient=hess_inv, inputs=list(augmenter.parameters()))aug_grad = (aug_grad + gradient_to_vector(augmenter.parameters()).data.clone())lap.backend.differentiable = Falsevector_to_gradient(aug_grad, augmenter.parameters())aug_optimizer.step()aug_scheduler.step()epoch_time_hyper = time() - time_hyperepoch_log.update({'train/time_hyper': epoch_time_hyper})if optimize_aug:aug_history.append(parameters_to_vector(augmenter.parameters()).squeeze().detach().cpu().numpy())logging.info(f'Augmentation params epoch {epoch}: {aug_history[-1]}')if use_wandb:wandb_log_invariance(augmenter)logging.info('LA memory allocated: ' + str(torch.cuda.max_memory_allocated(loss.device)/GB_FACTOR) + ' Gb.')margliks.append(marglik)del lapif use_wandb:if optimize_aug:epoch_log['train/lr_aug'] = aug_scheduler.get_last_lr()[0]epoch_log['train/lr_hyp'] = hyper_scheduler.get_last_lr()[0]wandb.log(epoch_log, step=epoch, commit=((epoch % 10) == 0))# early stopping on marginal likelihoodlogging.info(f'MARGLIK[epoch={epoch}]: marglik optimization. MargLik={margliks[-1]:.5f}, prec: {prior_prec.detach().mean().item():.2f}.')sigma_noise = 1 if sigma_noise is None else sigma_noiselap = laplace(model, likelihood, sigma_noise=sigma_noise, prior_precision=prior_prec,temperature=temperature, backend=backend, backend_kwargs=backend_kwargs,**la_kwargs)lap.fit(marglik_loader.detach())if optimize_aug:return lap, model, margliks, valid_perfs, aug_historyreturn lap, model, margliks, valid_perfs, None",
    "Experiment Result": "Laplace Approximation Types: FunctionalLaplace (for NTK/Kernel bounds), FullLaplace, KronLaplace (for KFAC-like approximations), DiagLaplace, BlockDiagLaplace.Curvature Backends: AsdlGGN, AugAsdlGGN (for augmented models/invariances), AsdlEF (Empirical Fisher), AugAsdlEF (augmented Empirical Fisher).Likelihoods: 'classification', 'regression', 'heteroscedastic_regression'.Hyperparameter Optimization (maximizing marginal likelihood):Prior Precision (`prior_prec_init`): Initialized to 1.0 (or other values, e.g., 0.089), optimized across 'scalar', 'layerwise', or 'diagonal' structures.Observation Noise (`sigma_noise_init`): Initialized to 1.0 (for regression, or 0.27).Augmentation Parameters (e.g., `AffineLayer2d`'s `rot_factor` for `method='lila'`): Initialized to 0.0, differentiable, optimized (includes rotation, translation, scaling, shearing factors).Optimization Steps: `n_hypersteps` (e.g., 1 to 100), `n_hypersteps_prior` (e.g., 1 to 10).Learning Rates: `lr_hyp` (e.g., 0.1, decayed to 0.01), `lr_aug` (e.g., 0.05, decayed to 0.005).Optimization Frequency: `marglik_frequency` (e.g., 1 or 5 epochs).Data Handling and Bounds:Subset-of-Data (Sod): `sod=True` enables stochastic estimation using subsets for the Laplace fit.Batch Sizes: `marglik_batch_size` (e.g., 10, 20, 50, 100, 250, 500, 1000) for Laplace fitting.Data Loaders: `SubsetTensorDataLoader` (uniformly sampled random subsets), `GroupedSubsetTensorDataLoader` (class-wise partitioning).Partitioning Strategies: `single_output=True` (output-wise partitioning), `single_output_iid=True` (independent single output samples), `independent_outputs=True` (for independent kernel bounds).Stochastic Gradients: `stochastic_grad=True` when using lower bound estimators (e.g., for data subsets or augmented models).Model Training:Models: MiniNet, MLP, LeNet, ResNet, WideResNet.Datasets: MNIST, FashionMNIST, CIFAR10, CIFAR100, TinyImageNet, including rotated (`mnist_r180`), translated, and scaled versions.Epochs: `n_epochs` (e.g., 100 to 500), `n_epochs_burnin` (e.g., 0 to 10).Optimizer: Adam, SGD.Learning Rate: `lr` (e.g., 1e-3 to 0.1), with cosine annealing scheduler (`scheduler='cos'`).Miscellaneous: `temperature` (1.0), `kron_jac=True` (use Kronecker approximation for Jacobians where applicable).Evaluation Metrics: Log marginal likelihood, test log likelihood, accuracy/MSE, runtime."
}{
    "Title": "AUTOMATA: Gradient Based Data Subset Selection for Compute-Efficient Hyper-parameter Tuning",
    "Main Contributions": "The paper addresses the challenge of high computational cost and time consumption in hyper-parameter optimization (HPO) for deep neural networks. It proposes AUTOMATA, a novel gradient-based data subset selection framework designed for compute-efficient hyper-parameter tuning. The main contributions include demonstrating that using small, informative data subsets (between 1% and 30%) significantly accelerates HPO, achieving speedups of 3x to 30x while maintaining comparable or even superior performance compared to tuning with the entire dataset. AUTOMATA's gradient-based subset selection is shown to outperform random subset selection and other baselines like CRAIG, and it also contributes to reduced energy consumption and CO2 emissions.",
    "Methodology": "AUTOMATA integrates three core components: a hyper-parameter search algorithm, a gradient-based subset selection (GSS) method, and a hyper-parameter scheduling algorithm. For hyper-parameter search, it utilizes algorithms such as Random Search and Tree Parzen Structured Estimator (TPE). The central GSS technique selects data subsets and their associated weights such that the weighted subset loss gradient closely approximates the full training loss gradient. This optimization problem, derived from GRAD-MATCH, is solved using Orthogonal Matching Pursuit (OMP). An efficient 'per-batch' variant of GSS is employed, selecting mini-batches rather than individual data points. For hyper-parameter scheduling, AUTOMATA incorporates methods like Hyperband and Asynchronous Successive Halving Algorithm (ASHA) for early termination of unpromising configurations. Additionally, a warm-starting strategy is used, where models are initially trained on the full dataset for a few epochs (e.g., κ=0.35 for ASHA) to generate more informative loss gradients for subsequent subset selection.",
    "Experimental Setup": "The effectiveness of AUTOMATA was empirically evaluated through extensive experiments across various real-world datasets from text (SST2, SST5, glue-SST2, TREC6), image (CIFAR10, CIFAR100, SVHN), and tabular (DNA, SATIMAGE, LETTER, CONNECT-4 from LIBSVM) domains. Baselines included RANDOM (randomly sampled subsets), CRAIG (a gradient-based subset selection method), and FULL (using the entire dataset for training). Experiments were conducted with different subset size fractions: 1%, 5%, 10%, and 30%. Model architectures varied by domain: LSTM with GloVe embeddings for text, ResNet18 and ResNet50 for images, and a two-hidden-layer multi-layer perceptron for tabular data. Hyper-parameter search spaces included learning rates, batch sizes, optimizer choices (SGD, Adam, Momentum, Nesterov), and learning rate schedulers. For fair comparison, a final training run on the entire dataset with the best-found hyper-parameters was performed for all methods (excluding FULL). Experiments were repeated 3 to 5 times. The framework was implemented using PyTorch, Ray-tune for HPO, and CORDS for subset selection, with energy consumption and CO2 emissions calculated using pyJoules and a Machine Learning Impact calculator, respectively.",
    "Limitations": "One limitation identified is that in scenarios where absolutely no performance loss is acceptable, the minimum optimal subset size for achieving speedup remains unknown. This often necessitates relying on larger subset sizes, such as 10% or 30%, which might not yield the maximum possible speedup.",
    "Future Research Directions": "Future work could focus on developing mechanisms to adaptively change subset sizes based on the model's performance for each configuration, thereby removing the current dependency on fixed subset sizes. The authors also hope that the AUTOMATA framework will encourage further research into more efficient subset selection approaches to accelerate hyper-parameter tuning and contribute to the broader goal of 'Green AI' by reducing computational and environmental costs.",
    "Experiment Code": "from ray import tune\n\nconfig = dict(setting= \"hyperparamtuning\",\n\n# parameter for subset selection\n# all settings for subset selection will be fetched from here\nsubset_config = \"configs/SL/config_gradmatchpb-warm_cifar10.py\",\n\n# parameters for hyper-parameter tuning\n# search space for hyper-parameter tuning\nspace = dict(learning_rate=tune.uniform(0.001, 0.01), \n        optimizer= tune.choice(['sgd', 'adam']),\n        trn_batch_size= tune.choice([20, 32, 64]),        \n        ),\n\n# tuning algorithm \nsearch_algo = \"TPE\",\n\n# number of hyper-parameter set to try\nnum_evals = 20,\n\n# metric to be optimized, for 'mean_loss' metric mode should be 'min'\nmetric = \"mean_accuracy\",\nmode = \"max\",\n\n# scheduler to be used (i.e ASHAScheduler)\n# scheduler terminates trials that perform poorly\n# learn more here: https://docs.ray.io/en/releases-0.7.1/tune-schedulers.html\nscheduler = 'hyperband',\n\n# where to store logs\nlog_dir = \"RayLogs/\",\n\n# resume hyper-parameter tuning from previous log\n# specify 'name' (i.e main_2021-03-09_18-33-56) below\nresume = False,\n\n# only required if you want to resume from previous checkpoint\n# it can also be specified if you don't want to resume\nname = None,\n\n# specify resources to be used per trial\n# i.e {'gpu':1, 'cpu':2}\nresources = {'gpu' : 0.5},\n\n# if True, trains model on Full dataset with the best parameter selected.\nfinal_train = True\n\n)\n",
    "Experiment Result": "The experimental setup for AUTOMATA involves hyper-parameter tuning and gradient-based subset selection (GSS) with warm-starting. The hyper-parameter search utilizes the Tree-Parzen Structured Estimator (TPE) algorithm. The search space for hyper-parameters includes:\n-   `learning_rate`: sampled uniformly between 0.001 and 0.01.\n-   `optimizer`: a choice between 'sgd' and 'adam'.\n-   `trn_batch_size`: a choice among 20, 32, and 64.\nA total of 20 hyper-parameter sets are evaluated. The optimization metric is 'mean_accuracy', with a mode of 'max'. Hyper-parameter scheduling employs the 'Hyperband' scheduler for early termination of unpromising configurations. Each trial is allocated 0.5 GPU and 1 CPU. After tuning, the model with the best configuration is trained on the full dataset.\n\nThe subset selection component, specified as `GradMatchPB-Warm` (Gradient-based Matching Per Batch with Warm-starting), is configured to select a `fraction=0.1` of the dataset every `20` epochs. The `selection_type` is 'PerBatch', meaning mini-batches are selected. A warm-starting `kappa=0.5` is used, where the model is initially trained on the full dataset for a few epochs to generate more informative loss gradients before adaptive subset selection begins. The `lam=0` and `eps=1e-100` parameters are used for the Orthogonal Matching Pursuit (OMP) solver. The gradients from the model's last linear layer (`linear_layer=True`) are utilized for subset selection. The base optimizer for the training process is SGD with a `learning_rate` of 0.05, `momentum` of 0.9, `weight_decay` of 5e-4, and `nesterov` set to True. A cosine annealing learning rate scheduler with `T_max=300` is used. The training is performed on the CIFAR-10 dataset using a ResNet18 architecture for 300 epochs on a CUDA device."
}

# Instructions
Following the instructions below, please provide an evaluation of the new method.
Since I aim to pursue research of high academic significance, I request that the assessment be conducted with rigorous standards.
- output
    - novelty_reason
        - Determine whether the new method has novelty, and output the reason.
        - The reason should be as specific as possible.
        - Carefully review the content of the studies provided in "Related Works" before outputting.
    - novelty_score
        - Score the novelty of the new method on a scale of 1 to 10, where 1 means no novelty at all and 10 means extremely high novelty.
    - significance_reason
        - Determine whether the new method is significant, and output the reason.
        - Significance includes both academic and societal importance.
    - significance_score
        - Score the significance of the new method on a scale of 1 to 10, where 1 means no significance at all and 10 means extremely high significance.
Output:
{'novelty_reason': 'SA-PASHA is the first progressive multi-fidelity scheduler that replaces ad-hoc, heuristic rank-stability tests with a statistically principled, uncertainty-aware test derived from a Deep Ranking Ensemble surrogate. Previous schedulers (ASHA, PASHA, DyHPO, FastBO, BOIL, DPL, etc.) either (i) rely on fixed ε / δ thresholds, (ii) fit learning-curve models only to decide promotions between *fixed* rungs, or (iii) use surrogate uncertainty solely inside search (EI, MES) rather than for scheduler control. SA-PASHA uniquely 1) samples full ranking distributions, 2) computes a pair-wise agreement matrix, and 3) gates resource escalation on a rank-confidence score ρ, eliminating the last hand-tuned hyper-parameter of PASHA while keeping the same asynchronous infrastructure.', 'novelty_score': 8, 'significance_reason': 'Experimental evidence on NASBench-201, LCBench and PD1 shows that SA-PASHA preserves final best accuracy while reducing FLOPs / wall-clock by 10–30 % versus PASHA and >2× versus ASHA, with <3 % computational overhead. Because it needs only a lightweight surrogate update and one new confidence threshold τ (robust over 0.8–0.95), the method can be dropped into any existing PASHA/ASHA implementation and is orthogonal to the searcher (random, MOBSTER, etc.). This translates into tangible cost savings in large-scale HPO/NAS runs, directly advancing the research goal of more efficient HPO. The removal of heuristic tuning also improves reproducibility and reduces human effort.', 'significance_score': 8}
