
Input:
You are an accomplished researcher in machine learning. You are considering a new method described in "New Method" for the research theme provided in "Research Topic". "Related Works" is a list of research papers that are highly relevant to this new method.
Based on the following instructions, output the reasons for the novelty and significance of the newly proposed method, and quantitatively evaluate them.

# Research Topic
Improving efficiency of hyperparameter optimization

# New Method
{
    "Open Problems": "1. Most BO‐based HPO algorithms either (a) train a configuration to a fixed budget (Hyperband family) or (b) embed the full learning curve in an expensive joint surrogate (e.g. BOIL, DyHPO).  Both strategies waste budget on clearly converged runs.\n2. A very cheap, task–agnostic early–stopping rule that can be fed back into a BO loop without re–designing the surrogate is still missing.",
    "Methods": "SlopeStop-BO (Slope-informed Early-Stopping Bayesian Optimisation)\n1. Online slope test  ▸  During training we keep a W–step moving window of the validation score v_t.  At step t we compute the window slope s_t = (v_t – v_{t-W})/W.\n2. Stopping rule  ▸  If s_t < ϵ·exp(–γ·t) the run is considered saturated and training is terminated immediately (two scalars ϵ,γ are global hyper-parameters chosen once per task; default ϵ=10⁻³, γ=0.01).\n3. Fast curve compression  ▸  At termination we store a two–dimensional summary per configuration x:  (μ=mean(v_{t-W: t}),  τ=t_stop / T_max) instead of the whole curve.  This removes the need for a product kernel over (x,t).\n4. Lightweight surrogate  ▸  A standard GP with ARD RBF kernel is fitted on the augmented input [x ; τ] → μ. GP hyper-parameters are updated every 2·D observations by ML; no expensive joint optimisation over logistic parameters as in BOIL.\n5. Acquisition  ▸  Expected Improvement divided by estimated cost c(x)=τ·T_max.  The next configuration is chosen with EI/c as usual.\n6. Budget bookkeeping  ▸  Because many runs stop early, the total consumed epochs fall 30-50 % below a fixed-budget scheme.",
    "Experimental Setup": "Datasets:  • LCBench (35 tabular tasks, 50 epoch learning curves)  • NAS-Bench-201 (3 image tasks, 200 epoch curves).\nBaselines: Hyperband, BOHB, DyHPO, BOIL.\nProtocol: 10 independent seeds, total wall-clock budget identical to BOIL (GPU: RTX 3090).\nHyper-parameters W=3, ϵ=1e-3, γ=0.01 fixed for all tasks.",
    "Experimental Code": "# pseudo-python\nfor conf in pool:\n    t, buf = 0, []\n    while t < T_max:\n        loss = train_one_epoch(conf)\n        buf.append(loss); t += 1\n        if len(buf) > W:\n            slope = (buf[-1]-buf[-W-1])/W\n            if slope < eps*np.exp(-gamma*t):\n                break                # ← early stop\n    mu = np.mean(buf[-W:]); tau = t / T_max\n    D.append(([conf, tau], mu, cost=t))\n    if len(D)% (2*dim)==0: refit_GP(D)\n    x_next = argmax_EI_over_cost(GP)",
    "Expected Result": "Compared with BOIL:  • ~40 % less total training epochs  • identical median simple-regret after budget normalisation.\nCompared with Hyperband/BOHB:  • ~60 % lower final regret for the same wall time.",
    "Expected Conclusion": "A purely slope-based, task-agnostic early-stopping rule plus a standard GP surrogate is enough to harvest most of the budget savings that more complex learning-curve models aim for, while keeping the optimisation loop simple and cheap.  SlopeStop-BO is therefore a practical drop-in replacement for fixed-budget BO in large-scale HPO campaigns."
}

# Related Works
{
    "Title": "Bayesian Optimization for Iterative Learning",
    "Main Contributions": "The paper addresses the challenge of expensive hyperparameter tuning for deep learning (DL) and deep reinforcement learning (DRL) systems, which typically involves numerous iterative training steps. Traditional Bayesian Optimization (BO) methods ignore intermediate training information and focus only on final performance. This work proposes a BO approach that exploits the iterative structure of learning algorithms for efficient hyperparameter tuning. Key contributions include: (1) an algorithm that optimizes the learning curve of an ML algorithm using a novel training curve compression technique instead of relying on averaged final performance; (2) an approach to learn the optimal compression curve parameters from data and a data augmentation technique for increased sample-efficiency; (3) demonstrating the efficiency of this method on tuning DRL agents and convolutional neural networks, outperforming existing baselines in identifying optimal hyperparameters in minimal wall-clock time.",
    "Methodology": "The proposed Bayesian Optimization for Iterative Learning (BOIL) approach models the cost-sensitive black-box function `f(x,t)` as a Gaussian Process (GP) over a joint space of hyperparameters `x` and training iterations `t`. It uses a product kernel `k(x,x') x k(t,t')` and approximates the training time cost `c(x,t)` with a linear regressor. The next hyperparameter `x` and iteration `t` to evaluate are selected by maximizing an acquisition function (modified Expected Improvement) normalized by the predicted cost. The core innovation is a training curve compression technique: the entire learning curve `r(·|x,t)` is compressed into a single numeric score `y` using a Sigmoid (Logistic) preference function `l(u | m0, g0)`. The parameters `m0` (middle point) and `g0` (growth rate) of this Sigmoid function are learned directly from the data by maximizing the GP log marginal likelihood. To further enhance sample efficiency and prevent ill-conditioning of the GP covariance matrix, a data augmentation technique is introduced. This method selectively includes a subset of intermediate reward sequences (`M=15` maximum augmented points) by choosing points at the maximum of the GP predictive uncertainty, while ensuring the natural log of the covariance matrix's condition number remains below a threshold (δ=20).",
    "Experimental Setup": "The algorithm was evaluated by tuning hyperparameters for two types of deep learning systems: Deep Reinforcement Learning (DRL) agents and Convolutional Neural Networks (CNNs). For DRL, experiments were conducted with a Dueling DQN agent on the CartPole-v0 environment and Advantage Actor Critic (A2C) agents on the InvertedPendulum-v2 and Reacher-v2 environments (using OpenAI gym and Mujoco). For CNNs, hyperparameters were tuned for training models on the SVHN and CIFAR10 datasets. All experimental results were averaged over 20 independent runs with different random seeds, using an NVIDIA 1080 GTX GPU and the tensorflow-gpu Python package. The DRL implementations were based on Open AI Baselines. The GP surrogate models used square-exponential kernels, with parameters estimated by maximizing marginal likelihood. Baselines for comparison included Hyperband, Continuous Multi-Task/Fidelity BO (CM-T/F-BO), vanilla Bayesian Optimization (BO), and BO with compression (BO-L). Freeze-thaw BO and Fabolas were noted as not included due to unsuitability for DRL's noisy curves or focus on different parameters (dataset size).",
    "Limitations": "Existing BO approaches often consider only the final performance of hyperparameters, ignoring valuable intermediate information from earlier training steps. Averaging performance over final iterations can be misleading due to noise and fluctuations in learning curves, especially in early training stages of DRL. Traditional stopping criteria, like the exponential decay assumed in Freeze-thaw BO, are not well-suited for DRL's unpredictable and noisy reward curves, potentially leading to premature termination of promising but fluctuating training runs. A naive approach to data augmentation by adding a full curve of points can lead to redundant data and cause serious ill-conditioning issues for the GP covariance matrix. Hyperband, another iteration-efficient method, can be misled by early performance fluctuations in noisy DRL tasks and lacks the flexibility to revise its choices until a new set of hyperparameters is sampled. A broader concern is that increasing automation in ML model training, including hyperparameter optimization, could further remove humans from the modeling process, making it harder to spot critical failures and contributing to the growing opacity of ML models.",
    "Future Research Directions": "The authors note that the proposed approach is not limited to machine learning algorithms but can be generally applied to any process exhibiting an iterative structure to be exploited. An example given is the optimization of manufacturing pipelines, where factory settings are adjusted to increase productivity. In a broader context, the work contributes to the construction of automated pipelines for ML model training and deployment. It is suggested that future work should rigorously analyze final training outcomes using a rapidly growing corpus of work addressing the interpretability of trained machine learning models and their decision-making to ensure beneficial use in real-world policy making.",
    "Experiment Code": "    @staticmethod\n    def _ei(x, gp, y_max):\n        y_max=np.asscalar(y_max)\n        mean, var = gp.predict(x, eval_MSE=True)\n        var2 = np.maximum(var, 1e-10 + 0 * var)\n        z = (mean - y_max)/np.sqrt(var2)        \n        out=(mean - y_max) * norm.cdf(z) + np.sqrt(var2) * norm.pdf(z)\n        \n        out[var2<1e-10]=0\n        return out\n\ndef apply_one_transform_logistic(curve, midpoint=-2, growth=1,MaxEpisode=1000,IsReturnCurve=False):\n    # this is the Logistic transformation, used in the paper\n    if isinstance(curve, (list,)):\n        curve=curve[0]\n        \n    def logistic_func(x):\n        return 1.0/(1+np.exp(-growth*(x-midpoint)))\n\t\n    #print(MaxEpisode)\n    my_xrange_scaled=np.linspace(-6,6, int(MaxEpisode))\n\n    my_logistic_value_scaled=logistic_func(my_xrange_scaled)\n\n    my_logistic_value_scaled=my_logistic_value_scaled[:len(curve)]\n\n    # if curve is negative, add a constant to make it positive\n    if np.max(curve)<=0 and np.min(curve)<=0:\n        curve=curve+500\n    \n    threshold=(midpoint+6-2)*len(curve)/(12)\n    threshold=np.int(threshold)\n    \n    prod_func=curve*my_logistic_value_scaled\n    \n    average=[np.mean(prod_func[threshold:pos+1]) for pos in range(threshold,len(prod_func))]\n\n    if IsReturnCurve==True:\n        return average[-1],my_logistic_value_scaled\n    else:\n        return average[-1]\n\nclass ProductGaussianProcess(object):\n    # in this class of Gaussian process, we define k( {x,t}, {x',t'} )= k(x,x')*k(t,t')\n    \n    \n    #def __init__ (self,param):\n    def __init__ (self,SearchSpace,gp_hyper=None,logistic_hyper=None,verbose=0):\n        self.noise_delta=5e-4\n        self.noise_upperbound=1e-2\n        self.mycov=self.cov_RBF_time\n        self.SearchSpace=SearchSpace\n        scaler = MinMaxScaler()\n        scaler.fit(SearchSpace.T)\n        self.Xscaler=scaler\n        self.verbose=verbose\n        self.dim=SearchSpace.shape[0]\n        \n        if gp_hyper is None:\n            self.hyper={}\n            self.hyper['var']=1 # standardise the data\n            self.hyper['lengthscale_x']=0.02 #to be optimised\n            self.hyper['lengthscale_t']=0.2 #to be optimised\n        else:\n            self.hyper=gp_hyper\n\n        \n        if logistic_hyper is None:\n            self.logistic_hyper={}\n            self.logistic_hyper['midpoint']=0.0\n            self.logistic_hyper['growth']=1.0   \n        else:\n            self.logistic_hyper=logistic_hyper\n\n        self.X=[]\n        self.T=[]\n        self.Y=[]\n        self.Y_curves=None\n#        self.hyper['lengthscale_x']_old=self.hyper['lengthscale_x']\n#        self.hyper['lengthscale_x']_old_t=self.hyper['lengthscale_x']_t\n        \n        self.alpha=[] # for Cholesky update\n        self.L=[] # for Cholesky update LL'=A\n        \n        self.MaxEpisode=0\n        \n        return None\n       \n\n    def cov_RBF_time(self, x1,t1,x2,t2,lengthscale,lengthscale_t):\n        \n        Euc_dist=euclidean_distances(x1,x2)\n        exp_dist_x=np.exp(-np.square(Euc_dist)/lengthscale)\n        \n        Euc_dist=euclidean_distances(t1,t2)\n        exp_dist_t=np.exp(-np.square(Euc_dist)/lengthscale_t)\n        \n        return exp_dist_x*exp_dist_t\n                \n    def fit(self,X,T,Y,Y_curves):\n        \"\"\"\n        Fit Gaussian Process model\n\n        Input Parameters\n        ----------\n        x: the observed points \n        t: time or number of episode\n        y: the outcome y=f(x)\n        \n        \"\"\" \n        temp=np.hstack((X,T))\n        ur = unique_rows(temp)\n        \n        T=T[ur]\n        X=X[ur]\n        Y=Y[ur]\n        \n        self.X=X\n        self.Y=Y\n        self.T=T\n        self.Y_curves=[val for idx,val in enumerate(Y_curves) if ur[idx]==True]\n        \n        for curves in self.Y_curves:\n            self.MaxEpisode=max(len(curves),self.MaxEpisode)\n        #self.Y_curves=Y_curves[myidx]\n            \n        Euc_dist_x=euclidean_distances(X,X)\n        #exp_dist_x=np.exp(-np.square(Euc_dist)/self.hyper['lengthscale_x'])+np.eye(len(X))*self.noise_delta\n    \n        Euc_dist_t=euclidean_distances(T,T)\n        #exp_dist_t=np.exp(-np.square(Euc_dist)/self.hyper['lengthscale_x']_t)+np.eye(len(X))*self.noise_delta       \n    \n        self.KK_x_x=np.exp(-np.square(Euc_dist_x)/self.hyper['lengthscale_x']\\\n                           -np.square(Euc_dist_t)/self.hyper['lengthscale_t'])+np.eye(len(X))*self.noise_delta\n          \n        if np.isnan(self.KK_x_x).any(): #NaN\n            print(\"nan in KK_x_x\")\n        \n        #self.KK_x_x_inv=np.linalg.pinv(self.KK_x_x)\n        self.L=np.linalg.cholesky(self.KK_x_x)\n        temp=np.linalg.solve(self.L,self.Y)\n        self.alpha=np.linalg.solve(self.L.T,temp)\n        self.cond_num=self.compute_condition_number()\n        \n    def compute_condition_number(self):\n        cond_num=np.linalg.cond(self.KK_x_x)\n        return cond_num\n    \n\n    def log_marginal_lengthscale_logistic_hyper(self,hyper,noise_delta):\n        \"\"\"\n        Compute Log Marginal likelihood of the GP model w.r.t. the provided lengthscale, noise_delta and Logistic hyperparameter\n        \"\"\"\n\n        def compute_log_marginal_with_logistic_hyper(lengthscale, lengthscale_t,midpoint,growth,noise_delta):\n            # compute K\n            temp=np.hstack((self.X,self.T))\n            ur = unique_rows(temp)\n            myX=self.X[ur]\n            myT=self.T[ur]\n            \n            # transform Y_curve to Y_original, then to Y\n            Y_original=transform_logistic(self.Y_curves,midpoint,growth,self.MaxEpisode)\n            myY=(Y_original-np.mean(Y_original))/np.std(Y_original)\n            \n            myY=myY[ur]\n          \n            self.Euc_dist_x=euclidean_distances(myX,myX)\n            self.Euc_dist_t=euclidean_distances(myT,myT)\n        \n            KK=np.exp(-np.square(self.Euc_dist_x)/lengthscale-np.square(self.Euc_dist_t)/lengthscale_t)\\\n                +np.eye(len(myX))*noise_delta\n                    \n            \n            try:\n                temp_inv=np.linalg.solve(KK,myY)\n            except: # singular\n                return -np.inf\n            \n            try:\n                #logmarginal=-0.5*np.dot(self.Y.T,temp_inv)-0.5*np.log(np.linalg.det(KK+noise_delta))-0.5*len(X)*np.log(2*3.14)\n                first_term=-0.5*np.dot(myY.T,temp_inv)\n                \n                # if the matrix is too large, we randomly select a part of the data for fast computation\n                if KK.shape[0]>200:\n                    idx=np.random.permutation(KK.shape[0])\n                    idx=idx[:200]\n                    KK=KK[np.ix_(idx,idx)]\n                #Wi, LW, LWi, W_logdet = pdinv(KK)\n                #sign,W_logdet2=np.linalg.slogdet(KK)\n                chol  = spla.cholesky(KK, lower=True)\n                W_logdet=np.sum(np.log(np.diag(chol)))\n                # Uses the identity that log det A = log prod diag chol A = sum log diag chol A\n    \n                #second_term=-0.5*W_logdet2\n                second_term=-W_logdet\n            except: # singular\n                return -np.inf\n            \n\n            logmarginal=first_term+second_term-0.5*len(myY)*np.log(2*3.14)\n                \n            if np.isnan(np.asscalar(logmarginal))==True:\n                print(\"lengthscale_x={:f} lengthscale_t={:f} first term ={:.4f} second  term ={:.4f}\".format(\n                        lengthscale,lengthscale_t,np.asscalar(first_term),np.asscalar(second_term)))\n\n            #print(lengthscale, lengthscale_t,midpoint,growth,\"logmarginal:\",logmarginal)\n            return np.asscalar(logmarginal)\n        \n        logmarginal=0\n\n        if not isinstance(hyper,list) and len(hyper.shape)==2:\n            logmarginal=[0]*hyper.shape[0]\n            growth=hyper[:,3]\n            midpoint=hyper[:,2]\n            lengthscale_t=hyper[:,1]\n            lengthscale_x=hyper[:,0]\n            for idx in range(hyper.shape[0]):\n                logmarginal[idx]=compute_log_marginal_with_logistic_hyper(lengthscale_x[idx],\\\n                           lengthscale_t[idx],midpoint[idx],growth[idx],noise_delta)\n        else:\n            lengthscale_x,lengthscale_t,midpoint,growth=hyper\n            logmarginal=compute_log_marginal_with_logistic_hyper(lengthscale_x,lengthscale_t,\\\n                                                                 midpoint,growth,noise_delta)\n        return logmarginal\n\n    \n    def optimize_lengthscale_SE_logistic_hyper(self,previous_hyper,noise_delta):\n        \"\"\"\n        Optimize to select the optimal lengthscale parameter\n        \"\"\"\n        \n        # define a bound on the lengthscale\n        SearchSpace_l_min=0.03\n        SearchSpace_l_max=0.3\n        \n        SearchSpace_midpoint_min=-2\n        SearchSpace_midpoint_max=3\n        \n        SearchSpace_growth_min=0.5\n        SearchSpace_growth_max=2\n        #mySearchSpace=[np.asarray([SearchSpace_lengthscale_min,SearchSpace_lengthscale_max]).T]\n        \n        mySearchSpace=np.asarray([[SearchSpace_l_min,SearchSpace_l_max],[10*SearchSpace_l_min,2*SearchSpace_l_max],\n                             [SearchSpace_midpoint_min,SearchSpace_midpoint_max],[SearchSpace_growth_min,SearchSpace_growth_max]])\n        \n        lengthscale_tries = np.random.uniform(mySearchSpace[:, 0], mySearchSpace[:, 1],size=(20, 4))\n\n        # evaluate\n        self.flagOptimizeHyperFirst=0 # for efficiency\n\n        logmarginal_tries=self.log_marginal_lengthscale_logistic_hyper(lengthscale_tries,noise_delta)\n\n        #find x optimal for init\n        idx_max=np.argmax(logmarginal_tries)\n        lengthscale_init_max=lengthscale_tries[idx_max]\n        #print lengthscale_init_max\n        \n        myopts ={'maxiter':30*self.dim,'maxfun':30*self.dim}\n\n        x_max=[]\n        max_log_marginal=None\n        \n        res = minimize(lambda x: -self.log_marginal_lengthscale_logistic_hyper(x,noise_delta),lengthscale_init_max,\n                       bounds=mySearchSpace,method=\"L-BFGS-B\",options=myopts)#L-BFGS-B\n        if 'x' not in res:\n            val=self.log_marginal_lengthscale_logistic_hyper(res,noise_delta)    \n        else:\n            val=self.log_marginal_lengthscale_logistic_hyper(res.x,noise_delta)  \n        \n        # Store it if better than previous minimum(maximum).\n        if max_log_marginal is None or val >= max_log_marginal:\n            if 'x' not in res:\n                x_max = res\n            else:\n                x_max = res.x\n            max_log_marginal = val\n            #print res.x\n\n        return x_max\n\n\n\n            \n    def optimize_lengthscale_logistic_hyper(self,prev_hyper,noise_delta):\n        # optimize both GP lengthscale and logistic hyperparameter\n\n            \n        #prev_theta=[prev_theta_x,prev_theta_t,prev_midpoint,prev_growth]\n        newlengthscale,newlengthscale_t,newmidpoint,newgrowth=self.optimize_lengthscale_SE_logistic_hyper(prev_hyper,noise_delta)\n        self.hyper['lengthscale_x']=newlengthscale\n        self.hyper['lengthscale_t']=newlengthscale_t\n        \n        # refit the model\n        temp=np.hstack((self.X,self.T))\n        ur = unique_rows(temp)\n\n        # update Y here\n        Y_original=transform_logistic(self.Y_curves,newmidpoint,newgrowth,self.SearchSpace[-1,1])\n        Y=(Y_original-np.mean(Y_original))/np.std(Y_original)\n        self.Y=Y\n        #\n        self.fit(self.X[ur],self.T[ur],self.Y[ur],self.Y_curves)\n        \n        return newlengthscale,newlengthscale_t,newmidpoint,newgrowth\n\n\n    def compute_var(self,X,T,xTest,tTest):\n        \"\"\"\n        compute variance given X and xTest\n        \n        Input Parameters\n        ----------\n        X: the observed points\n        xTest: the testing points \n        \n        Returns\n        -------\n        diag(var)\n        \"\"\" \n        \n        xTest=np.asarray(xTest)\n        xTest=np.atleast_2d(xTest)\n        \n        tTest=np.asarray(tTest)\n        tTest=np.atleast_2d(tTest)\n        tTest=np.reshape(tTest,(-1,1))\n        \n        if self.kernel_name=='SE':\n            #Euc_dist=euclidean_distances(xTest,xTest)\n            #KK_xTest_xTest=np.exp(-np.square(Euc_dist)/self.hyper['lengthscale_x'])+np.eye(xTest.shape[0])*self.noise_delta\n            #ur = unique_rows(X)\n            myX=X\n            myT=T\n            \n            Euc_dist_x=euclidean_distances(myX,myX)\n            #exp_dist_x=np.exp(-np.square(self.Euc_dist_x)/lengthscale)+np.eye(len(myX))*noise_delta\n        \n            Euc_dist_t=euclidean_distances(myT,myT)\n            #exp_dist_t=np.exp(-np.square(self.Euc_dist_t)/lengthscale_t)+np.eye(len(myX))*noise_delta      \n        \n            KK=np.exp(-np.square(Euc_dist_x)/self.hyper['lengthscale_x']-np.square(Euc_dist_t)/self.hyper['lengthscale_t'])\\\n                +np.eye(len(myX))*self.noise_delta\n                    \n                 \n            Euc_dist_test_train_x=euclidean_distances(xTest,X)\n            #Exp_dist_test_train_x=np.exp(-np.square(Euc_dist_test_train_x)/self.hyper['lengthscale_x'])\n            \n            Euc_dist_test_train_t=euclidean_distances(tTest,T)\n            #Exp_dist_test_train_t=np.exp(-np.square(Euc_dist_test_train_t)/self.hyper['lengthscale_t'])\n            \n            KK_xTest_xTrain=np.exp(-np.square(Euc_dist_test_train_x)/self.hyper['lengthscale_x']-np.square(Euc_dist_test_train_t)/self.hyper['lengthscale_t'])\n                \n        try:\n            temp=np.linalg.solve(KK,KK_xTest_xTrain.T)\n        except:\n            temp=np.linalg.lstsq(KK,KK_xTest_xTrain.T, rcond=-1)\n            temp=temp[0]\n            \n        #var=KK_xTest_xTest-np.dot(temp.T,KK_xTest_xTrain.T)\n        var=np.eye(xTest.shape[0])-np.dot(temp.T,KK_xTest_xTrain.T)\n        var=np.diag(var)\n        var.flags['WRITEABLE']=True\n        var[var<1e-100]=0\n        return var \n\n    \n        \n    def predict(self,xTest, eval_MSE=True):\n        \"\"\"\n        compute predictive mean and variance\n        Input Parameters\n        ----------\n        xTest: the testing points \n        \n        Returns\n        -------\n        mean, var\n        \"\"\"    \n\n        if len(xTest.shape)==1: # 1d\n            xTest=xTest.reshape((-1,self.X.shape[1]+1))\n            \n        tTest=xTest[:,-1]\n        tTest=np.atleast_2d(tTest)\n        tTest=np.reshape(tTest,(xTest.shape[0],-1))\n        \n        xTest=xTest[:,:-1]\n        \n        # prevent singular matrix\n        temp=np.hstack((self.X,self.T))\n        ur = unique_rows(temp)\n        \n        X=self.X[ur]\n        T=self.T[ur]\n                \n        Euc_dist_x=euclidean_distances(xTest,xTest)\n        Euc_dist_t=euclidean_distances(tTest,tTest)\n\n        KK_xTest_xTest=np.exp(-np.square(Euc_dist_x)/self.hyper['lengthscale_x']-np.square(Euc_dist_t)/self.hyper['lengthscale_t'])\\\n            +np.eye(xTest.shape[0])*self.noise_delta\n        \n        Euc_dist_test_train_x=euclidean_distances(xTest,X)\n        \n        Euc_dist_test_train_t=euclidean_distances(tTest,T)\n        \n        KK_xTest_xTrain=np.exp(-np.square(Euc_dist_test_train_x)/self.hyper['lengthscale_x']-np.square(Euc_dist_test_train_t)/self.hyper['lengthscale_t'])\n            \n        #Exp_dist_test_train_x*Exp_dist_test_train_t\n  \n        # using Cholesky update\n        mean=np.dot(KK_xTest_xTrain,self.alpha)\n        v=np.linalg.solve(self.L,KK_xTest_xTrain.T)\n        var=KK_xTest_xTest-np.dot(v.T,v)\n        \n\n        return mean.ravel(),np.diag(var)  \n\n    def posterior(self,x):\n        # compute mean function and covariance function\n        return self.predict(self,x)\n        \n    \nclass BOIL(object):\n\n    #def __init__(self, gp_params, func_params, acq_params, verbose=True):\n    def __init__(self, func, SearchSpace,acq_name=\"ei_mu_max\",verbose=1):\n\n        \"\"\"      \n        Input parameters\n        ----------\n        \n        gp_params:                  GP parameters\n        gp_params.theta:            to compute the kernel\n        gp_params.delta:            to compute the kernel\n        \n        func_params:                function to optimize\n        func_params.init bound:     initial SearchSpace for parameters\n        func_params.SearchSpace:        SearchSpace on parameters        \n        func_params.func:           a function to be optimized\n        \n        \n        acq_params:            acquisition function, \n        acq_params.acq_func['name']=['ei','ucb','poi']\n        acq_params.opt_toolbox:     optimization toolbox 'nlopt','direct','scipy'\n                            \n        Returns\n        -------\n        dim:            dimension\n        SearchSpace:         SearchSpace on original scale\n        scaleSearchSpace:    SearchSpace on normalized scale of 0-1\n        time_opt:       will record the time spent on optimization\n        gp:             Gaussian Process object\n        \"\"\"\n        \n        self.method='boil'\n        self.verbose=verbose\n        if isinstance(SearchSpace,dict):\n            # Get the name of the parameters\n            self.keys = list(SearchSpace.keys())\n            \n            self.SearchSpace = []\n            for key in list(SearchSpace.keys()):\n                self.SearchSpace.append(SearchSpace[key])\n            self.SearchSpace = np.asarray(self.SearchSpace)\n        else:\n            self.SearchSpace=np.asarray(SearchSpace)\n            \n            \n        self.dim = len(SearchSpace)\n\n        scaler = MinMaxScaler()\n        scaler.fit(self.SearchSpace[:-1,:].T)\n        \n        scalerT = MinMaxScaler()\n        SearchSpace_T=np.atleast_2d(self.SearchSpace[-1,:]).T\n        scalerT.fit(SearchSpace_T)\n\n        self.Xscaler=scaler\n        self.Tscaler=scalerT\n\n        # create a scaleSearchSpace 0-1\n        self.scaleSearchSpace=np.array([np.zeros(self.dim), np.ones(self.dim)]).T\n                \n        # function to be optimised\n        self.f = func\n    \n        # store X in original scale\n        self.X_ori= None\n\n        # store X in 0-1 scale\n        self.X = None\n        \n        # store y=f(x)\n        # (y - mean)/(max-min)\n        self.Y = None\n               \n        # y original scale\n        self.Y_ori = None\n        \n        # store the number of episode\n        self.T=None\n        self.T_original=None\n        \n        # store the cost original scale\n        self.Y_cost_original=None\n        \n        self.time_opt=0\n         \n        self.max_min_gap=self.SearchSpace[:,1]-self.SearchSpace[:,0]\n\n\n        # acquisition function\n        self.acq_name = acq_name\n        self.logmarginal=0\n\n        self.gp=ProductGaussianProcess(self.scaleSearchSpace,verbose=verbose)\n\n        # store the curves of performances\n        self.Y_curves=[]\n        \n        # store the cost original scale\n        self.Y_cost_original=None\n        \n        self.time_opt=0\n        \n        # acquisition function\n        self.acq_func = None\n   \n        self.logmarginal=0\n        \n        self.markVirtualObs=[]\n        \n        self.countVirtual=[]\n\n        self.linear_regression = linear_model.LinearRegression()\n\n        self.condition_number=[]\n        \n        # maximum number of augmentations\n        self.max_n_augmentation=10\n        self.threshold_cond=15\n        \n    def init(self, n_init_points=3, seed=1):\n        \"\"\"      \n        Input parameters\n        ----------\n        n_init_points:        # init points\n        \"\"\"\n        np.random.seed(seed)\n\n        # Generate random points\n        SearchSpace=np.copy(self.SearchSpace)\n        SearchSpace[-1,0]=SearchSpace[-1,1] # last dimension, set it to MaxIter\n\n        l = [np.random.uniform(x[0], x[1]) for _ in range(n_init_points) for x in SearchSpace] \n\n        # Concatenate new random points to possible existing\n        # points from self.explore method.           temp=np.asarray(l)\n        temp=temp.T\n        init_X=list(temp.reshape((n_init_points,-1)))\n        \n        self.X_original = np.asarray(init_X)\n        self.T_original=self.X_original[:,-1]\n        self.T_original=np.reshape(self.T_original,(n_init_points,-1))\n        \n        self.X_original=self.X_original[:,:-1] # remove the last dimension of MaxEpisode\n        self.X_original=np.reshape(self.X_original,(n_init_points,-1))\n\n        # Evaluate target function at all initialization           \n        y_init_curves, y_init_cost=self.f(init_X)\n\n        y_init_cost=np.atleast_2d(np.asarray(y_init_cost))#.astype('Float64')\n\n        self.Y_curves+=y_init_curves\n\n        # we transform the y_init_curves as the average of [ curves * logistic ]\n        y_init=transform_logistic(y_init_curves,self.gp.logistic_hyper['midpoint'],\\\n                                  self.gp.logistic_hyper['growth'], self.SearchSpace[-1,1])\n        #y_init=y_init_curves\n        y_init=np.reshape(y_init,(n_init_points,1))\n        \n        # record keeping ========================================================\n        self.Y_original = np.asarray(y_init)      \n        self.Y_cost_original=np.reshape(y_init_cost,(-1,1))\n\n        # convert it to scaleX\n        self.X = self.Xscaler.transform(np.asarray(init_X)[:,:-1])#remove the last dimension of MaxEpisode\n        #self.X=self.X[:,:-1]\n        self.X=np.reshape(self.X,(n_init_points,-1))\n\n        self.T = self.Tscaler.transform(self.T_original)\n\n        self.markVirtualObs+=[0]*n_init_points\n\n        # generating virtual observations for each initial point\n        for ii in range(n_init_points):\n            self.generating_virtual_observations(self.X[ii,:],\n                         self.T[ii],[y_init_curves[ii]],y_init_cost[0][ii],IsRandom=False)\n\n        self.Y_cost=(self.Y_cost_original-np.min(self.Y_cost_original))/(np.max(self.Y_cost_original)-np.min(self.Y_cost_original))\n\n        if np.std(self.Y_original)==0:\n            self.Y=(self.Y_original-np.mean(self.Y_original))\n        else:\n            self.Y=(self.Y_original-np.mean(self.Y_original))/np.std(self.Y_original)\n\n       \n    def utility_cost_evaluation(self,x,acq_func,isDebug=False):\n        # this is a wrapper function to evaluate at multiple x(s)\n        \n        \n        def utility_cost_evaluation_single(x,acq_func,isDebug=False):\n            # given a location x, we will evaluate the utility and cost\n            \n            utility=acq_func.acq_kind(x,gp=self.gp)\n            \n            try:\n                mean_cost=self.linear_regression.predict(np.reshape(x,(1,-1)))\n                \n            except:\n                print(x)\n                print(\"bug\")\n    \n            mean_cost=max(0,mean_cost)+0.1 # to avoid <=0 cost\n            \n            #acquisition_function_value= utility_normalized/cost_normalized\n            if 'ei' in acq_func.acq_name:\n                acquisition_function_value= np.log(utility)-np.log(mean_cost)\n            else:\n                acquisition_function_value= np.log(1+np.exp(utility))/np.log(1+np.exp(mean_cost))\n    \n            if isDebug==True:\n                print(\"acq_func at the selected point \\t utility:\",np.round(utility,decimals=4),\"\\t cost:\",mean_cost)\n                if utility==0:\n                    print(\"utility =0===============================================================================\")\n       \n            return acquisition_function_value*(-1) # since we will minimize this acquisition function\n        \n        \n        if len(x)==self.dim: # one observation\n            temp=utility_cost_evaluation_single(x,acq_func,isDebug=isDebug)\n            if isDebug==True:\n                return temp\n            else:\n                utility=np.mean(temp)\n        \n        else: # multiple observations\n            utility=[0]*len(x)\n            for idx,val in enumerate(x):\n                temp=utility_cost_evaluation_single(x=val,acq_func=acq_func,isDebug=isDebug)\n                                                     \n                utility[idx]=np.mean(temp)\n                \n            utility=np.asarray(utility)    \n        return utility   \n    \n        \n    def acq_utility_cost(self):\n        \n        # generate a set of x* at T=MaxIter\n        # instead of running optimization on the whole space, we will only operate on the region of interest\n        # the region of interest in DRL is where the MaxEpisode\n    \n        # we find maximum of EI\n\n        acq={}\n        acq['name']=self.acq_name\n        acq['dim']=self.scaleSearchSpace.shape[0]\n        acq['scaleSearchSpace']=self.scaleSearchSpace   \n    \n        if self.acq_name=='ei_mu_max':# using max of mean(x) as the incumbent\n            \n            # optimie the GP predictive mean function to find the max of mu\n            x_mu_max,mu_max_val=acq_max_with_name(gp=self.gp,scaleSearchSpace=self.scaleSearchSpace,acq_name='mu',IsReturnY=True)\n            acq['mu_max']=  mu_max_val\n\n        myacq=AcquisitionFunction(acq)\n        \n        x_min = acq_min_scipy_kwargs(myfunc=self.utility_cost_evaluation,SearchSpace=self.scaleSearchSpace,\n                        acq_func=myacq, isDebug=False)\n        \n        if self.verbose==True:\n            acq_val=self.utility_cost_evaluation(x_min,myacq,isDebug=False)\n            print(\"selected point from acq func:\",np.round(x_min,decimals=4),\"acq val=log(Utility/Cost)=\",(-1)*np.round(acq_val,decimals=4)) # since we minimize the acq func\n            if np.round(acq_val,decimals=4)==0:\n                print(\"acq value =0\")\n            \n        return x_min\n    \n    \n    def select_informative_location_by_uncertainty(self,n_virtual_obs,x_max,t_max):\n        # this function will select a list of informative locations to place a virtual obs\n        # x_max is the selected hyperparameter\n        # t_max is the selected number of epochs to train\n        \n        \n        SearchSpace=np.copy(self.scaleSearchSpace)\n        for dd in range(self.dim-1):\n            SearchSpace[dd,0],SearchSpace[dd,1]=x_max[dd],x_max[dd]\n            \n        SearchSpace[-1,1]=t_max\n        \n        temp_X,temp_T=self.X.copy(),self.T.copy()\n        temp_gp=copy.deepcopy(self.gp )\n        \n        temp_Y=np.random.random(size=(len(temp_T),1))\n        \n        temp_gp.fit(temp_X,temp_T,temp_Y,self.Y_curves)\n        \n        new_batch_T=None\n\n        pred_var_value=[0]*n_virtual_obs\n        for ii in range(n_virtual_obs):\n            x_max_pred_variance, pred_var_value[ii]=acq_max_with_name(gp=temp_gp,\n                              scaleSearchSpace=SearchSpace,acq_name='pure_exploration',IsReturnY=True)\n            \n            # stop augmenting if the uncertainty is smaller than a threshold\n            # or stop augmenting if the uncertainty is smaller than a threshold\n\n            log_cond=np.log( temp_gp.compute_condition_number() )\n            if log_cond>self.threshold_cond or pred_var_value[ii]<(self.gp.noise_delta+1e-3):\n                break\n          \n            if x_max_pred_variance[-1] in temp_T[-ii:]: # if repetition, stop augmenting\n                break\n            \n            temp_X = np.vstack((temp_X, x_max.reshape((1, -1)))) # append new x\n            temp_T = np.vstack((temp_T, x_max_pred_variance[-1].reshape((1, -1)))) # append new t\n            temp_gp.X,temp_gp.T=temp_X,temp_T\n            temp_Y=np.random.random(size=(len(temp_T),1))\n            \n            temp_gp.fit(temp_X,temp_T,temp_Y,self.Y_curves)\n\n            if new_batch_T is None:\n                new_batch_T=x_max_pred_variance[-1].reshape((1, -1))\n            else:\n                new_batch_T= np.vstack((new_batch_T, x_max_pred_variance[-1].reshape((1, -1))))\n        \n#        if self.verbose:\n#            print(\"pred_var_value at the augmented points:\",np.round( pred_var_value,decimals=4))\n\n        if new_batch_T is None:\n            return [],0\n\n        else:\n            output=np.sort(new_batch_T.ravel()).tolist()\n            return output, len(output)\n\n    \n    def generating_virtual_observations(self,x_max,t_max,y_original_curves,y_cost_original,IsRandom=False):\n        \n        #temp_X_new_original=x_max*self.max_min_gap[:-1]+self.SearchSpace[:-1,0]\n        temp_X_new_original=self.Xscaler.inverse_transform(np.reshape(x_max,(-1,self.dim-1)))\n\n        # selecting MAX number of virtual observations, e.g., we dont want to augment more than 10 points\n        max_n_virtual_obs=np.int(t_max*self.max_n_augmentation)\n        if max_n_virtual_obs==0:\n            self.countVirtual.append(0)\n            return\n        \n        if IsRandom==True:# select informative locations by random uniform   \n            l = [np.random.uniform(0, t_max) for _ in range(max_n_virtual_obs)]\n        else:\n            # select informative locations by uncertainty as in the paper\n            l,n_virtual_obs=self.select_informative_location_by_uncertainty(max_n_virtual_obs,x_max,t_max)        \n            \n        self.countVirtual.append(n_virtual_obs)\n        \n        if self.verbose:\n            np.set_printoptions(suppress=True)\n            print(\"Max #augmented points\",max_n_virtual_obs, \"\\t #augmented points \",len(l),\n                  \"\\t Augmented points: \",np.round(l,decimals=3))\n            \n        l_original=[self.SearchSpace[-1,0]+val*self.max_min_gap[-1] for val in l]\n        #l_original=[self.Tscaler.inverse_transform(val) for val in l]\n                           \n        virtual_obs_t_original=np.asarray(l_original).T\n        virtual_obs_t=np.asarray(l).T\n        \n        # compute y_original for the virtual observations\n        y_virtual_original=[0]*n_virtual_obs\n        for ii in range(n_virtual_obs):\n            \n            idx=np.int(virtual_obs_t_original[ii])\n            \n            temp_curve=y_original_curves[0][:idx+1]\n            self.markVirtualObs.append(1)\n\n            y_virtual_original[ii]=transform_logistic([temp_curve],\\\n                      self.gp.logistic_hyper['midpoint'],self.gp.logistic_hyper['growth'],self.SearchSpace[-1,1])\n           \n            self.X = np.vstack((self.X, x_max.reshape((1, -1))))\n            self.X_original=np.vstack((self.X_original, temp_X_new_original))\n        \n            self.T = np.vstack((self.T, virtual_obs_t[ii].reshape((1, -1))))\n            temp=np.asarray(virtual_obs_t_original[ii])\n            self.T_original=np.vstack((self.T_original, temp.reshape((1, -1))))\n\n\n            self.Y_original = np.append(self.Y_original,[y_virtual_original[ii]])\n            self.Y_curves.append(temp_curve)\n            \n            # interpolating the cost for augmented observation\n            y_cost_estimate=y_cost_original*virtual_obs_t[ii]\n            self.Y_cost_original = np.append(self.Y_cost_original,[y_cost_estimate])\n            \n        \n#        if self.verbose:\n#            temp_y_original_whole_curve=transform_logistic(y_original_curves,\\\n#                               self.gp.logistic_hyper['midpoint'],self.gp.logistic_hyper['growth'],self.SearchSpace[-1,1])\n#            print(np.round(temp_y_original_whole_curve,decimals=4), np.round(y_virtual_original,decimals=4))\n#            \n        \n    def suggest_nextpoint(self): # logistic, time-cost, virtual\n        \"\"\"\n        Main optimization method.\n\n\n        Returns\n        -------\n        x: recommented point for evaluation\n        \"\"\"\n \n        # init a new Gaussian Process============================================\n        self.gp=ProductGaussianProcess(self.scaleSearchSpace,self.gp.hyper,self.gp.logistic_hyper)\n        self.gp.fit(self.X, self.T,self.Y,self.Y_curves)\n            \n        # we store the condition number here=====================================\n        self.condition_number.append(self.gp.cond_num)\n        if self.verbose:\n            print(\"ln of conditioning number of GP covariance matrix\", np.round(np.log(self.gp.cond_num),decimals=1))\n\n        # count number of real observations\n        count=len(self.markVirtualObs)-np.sum(self.markVirtualObs)\n        count=np.int(count)\n\n        # optimize GP hyperparameters and Logistic hyper after 3*d iterations\n        if  len(self.Y)%(2*self.dim)==0:\n\n            hyper=[self.gp.hyper['lengthscale_x'],self.gp.hyper['lengthscale_t'], \\\n                   self.gp.logistic_hyper['midpoint'],self.gp.logistic_hyper['growth']]\n            newlengthscale_x,newlengthscale_t,new_midpoint, new_growth = self.gp.optimize_lengthscale_logistic_hyper(hyper,self.gp.noise_delta)\n            \n            self.gp.hyper['lengthscale_x']=newlengthscale_x\n            self.gp.hyper['lengthscale_t']=self.gp.hyper['lengthscale_t']\n            self.gp.logistic_hyper['midpoint']=new_midpoint\n            self.gp.logistic_hyper['growth']=new_growth\n          \n            if self.verbose:\n                print(\"==estimated lengthscale_x={:.4f}   lengthscale_t={:.3f}   Logistic_m0={:.1f}   Logistic_g0={:.1f}\".format(\n                    newlengthscale_x,newlengthscale_t,new_midpoint,new_growth))\n                \n        # Set acquisition function\n        start_opt=time.time()\n\n        # linear regression is used to fit the cost\n        # fit X and T\n        combine_input=np.hstack((self.X,self.T))\n        self.linear_regression.fit(combine_input,self.Y_cost)\n        \n        # maximize the acquisition function to select the next point =================================\n        x_max_temp=self.acq_utility_cost()\n        x_max=x_max_temp[:-1]\n        t_max=x_max_temp[-1]       \n            \n        # record keeping stuffs ====================================================\n        # record the optimization time\n        finished_opt=time.time()\n        elapse_opt=finished_opt-start_opt\n        self.time_opt=np.hstack((self.time_opt,elapse_opt))\n\n        # this is for house keeping stuff        \n        self.markVirtualObs.append(0)\n\n        self.X = np.vstack((self.X, x_max.reshape((1, -1))))\n        self.T = np.vstack((self.T, t_max.reshape((1, -1))))\n\n        # compute X in original scale\n        temp_X_new_original=self.Xscaler.inverse_transform(np.reshape(x_max,(-1,self.dim-1)))\n        #temp_X_new_original=x_max*self.max_min_gap[:-1]+self.SearchSpace[:-1,0]\n        self.X_original=np.vstack((self.X_original, temp_X_new_original))\n        \n        #temp_T_new_original=t_max*self.max_min_gap[-1]+self.SearchSpace[-1,0]\n        temp_T_new_original=self.Tscaler.inverse_transform(np.reshape(t_max,(-1,1)))\n        self.T_original=np.vstack((self.T_original, temp_T_new_original))\n\n        # evaluate Y using original X\n        x_original_to_test=x_max_temp*self.max_min_gap+self.SearchSpace[:,0]\n\n        # evaluate the black-box function=================================================\n        y_original_curves, y_cost_original= self.f(x_original_to_test)\n        \n        # compute the utility score by transformation\n        y_original=transform_logistic(y_original_curves,\\\n              self.gp.logistic_hyper['midpoint'],self.gp.logistic_hyper['growth'],self.SearchSpace[-1,1])\n        \n        if len(y_original_curves)==1: # list\n            self.Y_curves.append(y_original_curves[0])\n        else:\n            self.Y_curves.append(y_original_curves)\n\n        \n        self.Y_original = np.append(self.Y_original,y_original)\n        self.Y_cost_original = np.append(self.Y_cost_original,y_cost_original)\n\n        # augmenting virtual observations =====================================================\n        self.generating_virtual_observations(x_max,t_max,y_original_curves,y_cost_original[0])\n        \n        # update Y after change Y_original        \n        if np.std(self.Y_original)==0:\n            self.Y=(self.Y_original-np.mean(self.Y_original))\n        else:\n            self.Y=(self.Y_original-np.mean(self.Y_original))/np.std(self.Y_original)\n            \n        self.Y_cost=(self.Y_cost_original-np.min(self.Y_cost_original))/(np.max(self.Y_cost_original)-np.min(self.Y_cost_original))\n                    \n        #if self.verbose:\n        np.set_printoptions(suppress=True)\n\n        print(\"[original scale] x={} t={:.0f} current y={:.2f}, ybest={:.2f}\".format( np.round(self.X_original[-1],decimals=4),\\\n              np.asscalar(self.T_original[-1]),np.asscalar(self.Y_original[-1]), np.asscalar(self.Y_original.max())))",
    "Experiment Result": "GP Model: Gaussian Process (GP) over a joint space of hyperparameters (x) and training iterations (t). Kernel: Product kernel k(x,x') * k(t,t'). Cost Model: Linear regressor (`sklearn.linear_model.LinearRegression()`). Acquisition Function: Modified Expected Improvement (`'ei_mu_max'`), normalized by predicted cost using `log(utility) - log(mean_cost)`. Training Curve Compression: Sigmoid (Logistic) preference function (`apply_one_transform_logistic(curve, midpoint, growth, MaxEpisode)`) with `midpoint` and `growth` parameters. Hyperparameter Learning: GP lengthscales (`lengthscale_x`, `lengthscale_t`) and Logistic preference function parameters (`midpoint`, `growth`) are optimized by maximizing the GP log marginal likelihood, performed after every `2*D` iterations (`len(self.Y)%(2*self.dim)==0`). Data Augmentation: Selectively includes intermediate reward sequences at points of maximum GP predictive uncertainty (`acq_name='pure_exploration'`). Maximum number of augmented points per real observation is `10` (`max_n_augmentation=10`). The natural log of the covariance matrix's condition number is constrained to be below `15` (`threshold_cond=15`). GP Noise Delta: `5e-4` (`self.noise_delta=5e-4`)."
}{
    "Title": "Hyperparameter Optimization through Neural Network Partitioning",
    "Main Contributions": "This paper introduces Partitioned Neural Networks (PNNs), a novel and efficient method for hyperparameter optimization that does not require a separate validation set and can be performed in a single training run. Inspired by the marginal likelihood, the method partitions the training data into K shards and the neural network model into K parameter partitions. It defines an \"out-of-training-sample\" loss on unseen data shards as the objective for hyperparameter optimization. The approach is computationally significantly cheaper than alternative marginal likelihood methods and is particularly beneficial for scenarios with limited data or challenging retraining costs, such as federated learning, while also reducing communication overhead in FL.",
    "Methodology": "The core methodology involves partitioning the neural network's weights (w) into C partitions (w1,...,wC) and the training data into C chunks (D1,...,DC). For each k-th subnetwork (w(k)s), only parameters (w1,...,wk) are optimized on data (D1:k), while subsequent parameters (wk+1,...,wC) are set to default values (e.g., initialization). Hyperparameters (ψ) are optimized using an objective function LML (Eq. 2), an approximation to the marginal likelihood lower-bound, which measures the sum of out-of-sample losses on data chunks Dk for a subnetwork trained on D1:k-1. This process avoids the need to compute or invert a Hessian and enables stochastic gradient descent for hyperparameter updates. A random weight partitioning scheme is primarily used, where a fixed proportion of weights in each layer is randomly assigned to a partition. In federated learning, clients are assigned to data chunks and compute gradients for their associated parameter partitions and hyperparameters, with only modified partitions communicated to the server.",
    "Experimental Setup": "Experiments are conducted across various tasks and datasets. For a toy input selection task, MLPs are trained on a synthetic dataset with informative and spurious features to demonstrate the LML's ability to identify correct models and learn input masks. For invariance learning through data augmentations, experiments are performed on MNIST, CIFAR10, TinyImagenet, and their rotated variants (rotCIFAR10, rotTinyImagenet, rotMNIST) using CNN and fixupResNet architectures (ResNet-8, ResNet-14, ResNet-50). Baselines include standard training (no augmentations), Augerino, Differentiable Laplace, and Last-layer Marginal Likelihood. Comparisons are also made against traditional training/validation splits. In Federated Learning, non-i.i.d. versions of MNIST and CIFAR10 (with label and rotation skew) are used across 100 clients, evaluating PNNs against FedAvg and FedAvg + Augerino, optimizing data augmentation parameters and dropout rates. Validation is primarily through test accuracy and log-likelihood, including performance in low-data regimes and sensitivity to partitioning strategies.",
    "Limitations": "The method inherently requires an additional forward-backward pass to update hyperparameters, leading to increased, though still comparatively lower, computational costs. Empirically, partitioned networks may require more training iterations for convergence. Partitioning the network can constrain its capacity, potentially leading to some performance loss compared to a full, non-partitioned network trained with perfectly optimized hyperparameters. The choice of partitioning strategy (number of chunks, data proportions, parameter proportions) introduces an additional hyperparameter that may require tuning for optimal performance, although a default strategy (more parameters/data to early chunks) performs reasonably well.",
    "Future Research Directions": "Future work could explore alternative partitioning schemes to potentially sidestep the need for separate forward-backward passes during hyperparameter updates. Investigating dynamic partitioning of network parameters during training is another avenue. Strategies to alleviate the performance loss caused by network partitioning, such as adjusting training rounds or increasing network capacity, are also suggested. Further exploration of design choices for hyperparameter optimization, including accumulating gradients from different chunks or interleaving updates at less frequent intervals, could be beneficial. Finally, exploring different sequential updating schemes for clients in the federated learning setting is also a potential research direction.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "Implicit differentiation of Lasso-type models for hyperparameter optimization",
    "Main Contributions": "The paper addresses the challenge of setting regularization parameters for Lasso-type estimators, which is crucial but difficult. Existing gradient-based hyperparameter optimization methods suffer from high memory consumption (forward/backward automatic differentiation) or numerical instability/prohibitive cost and smoothness assumptions (implicit differentiation). The main contribution is an efficient implicit differentiation algorithm, named 'Implicit Forward Iterative Differentiation' (Algorithm 2), specifically tailored for non-smooth Lasso-type problems. This algorithm avoids matrix inversion, scales to high-dimensional data by leveraging solution sparsity, and decouples the computation of the Jacobian from the regression coefficients. It proves that forward iterative differentiation of block coordinate descent (BCD) converges linearly towards the true gradient once the support is identified. Experiments demonstrate that the proposed method outperforms a wide range of standard hyperparameter optimization methods in optimizing held-out error or the Stein Unbiased Risk Estimator (SURE) on both simulated and real high-dimensional datasets.",
    "Methodology": "The methodology frames hyperparameter optimization as a bi-level optimization problem, aiming to minimize a criterion (e.g., held-out loss, SURE) over hyperparameters, subject to the inner problem of solving the Lasso-type model for regression coefficients. The core technique involves estimating the gradient with respect to the hyperparameters using implicit differentiation. Instead of solving a large linear system, the proposed 'Implicit Forward Iterative Differentiation' (Algorithm 2) leverages the fixed-point iteration property of proximal Block Coordinate Descent (BCD) algorithms. It uses a key observation that after a finite number of iterations, the support (non-zero coefficients) of the Lasso solution is identified, making the partial derivatives constant. This allows for decoupling the computation: first, the regression coefficients are computed using any state-of-the-art convex solver, and then the Jacobian is computed in a separate step by applying forward differentiation recursion steps restricted to the identified support. This leads to efficient computation and linear convergence rates for the Jacobian once the support is identified. The method is compared against traditional implicit differentiation (solving an s x s linear system), forward iterative differentiation, and non-gradient methods like grid-search, random-search, lattice hypercube sampling, and Bayesian optimization.",
    "Experimental Setup": "The experiments were conducted using Python with Numba for critical BCD parts, and the code is released as an open-source package 'sparse-ho'. For fairness, all methods used the same vanilla BCD algorithm (Algorithm 5) for the inner optimization problem, stopping when the relative change in the cost function fell below a tolerance of 10^-5. Gradient-based methods employed a line-search strategy following Pedregosa (2016). Initialization for Lasso was set at λ_max - log(10), while for the weighted Lasso, a regularized bi-level problem (Equation 18) provided an initial point. Competitors included: gradient-based (Implicit Forward Iterative Differentiation (proposed), Implicit Differentiation, Forward Iterative Differentiation) and non-gradient-based (Grid-search, Random-search, Lattice Hypercube, Bayesian optimization). Performance was evaluated using held-out loss (splitting data into train, validation, test sets) and the Stein Unbiased Risk Estimator (SURE), approximated using Finite Differences Monte-Carlo. Datasets included: real-world high-dimensional datasets like rcv1 (n=20,242, p=19,959), 20news (n=11,314, p=130,107), and finance (n=16,087, p=1,668,737). Simulated data (n=100, varying p from 200 to 10,000, 5 non-zero coefficients, SNR=3) were used for MSE evaluation with the SURE criterion. The method was also applied to the non-convex MCP estimator with 2 hyperparameters on rcv1 and 20news datasets.",
    "Limitations": "The theoretical guarantees (e.g., for weak differentiability and convergence of the Jacobian) assume that the inner Lasso-type optimization problem has a unique solution. While the set of hyperparameters leading to non-unique solutions is typically of measure zero, the theory does not strictly cover these pathological settings, although the algorithm can still be applied empirically. The current theoretical framework also does not cover non-convex penalty functions like MCP, even though numerical experiments show promising performance. The overall hyperparameter optimization problem itself is generally non-convex, which means that gradient descent is not guaranteed to converge to a global minimum. Additionally, the SURE criterion requires prior knowledge of the noise variance. The implicit differentiation method (Algorithm 1) can suffer from slow convergence or numerical instability when solving ill-conditioned linear systems, especially in high dimensions, making it less efficient in certain scenarios compared to the proposed method. The backward iterative differentiation method was found to be orders of magnitude slower and memory-intensive, particularly for high-dimensional output spaces.",
    "Future Research Directions": "Future research could focus on extending the theoretical guarantees to non-convex penalty functions, such as the MCP, to formally justify the application of gradient-based hyperparameter optimization in such cases. Further investigation into settings where the Lasso solution is not unique could also be a direction for future work. The two-step nature of the proposed algorithm, which decouples coefficient and Jacobian computation, opens avenues for leveraging advanced state-of-the-art Lasso solvers that use techniques like active sets or screening rules. Exploring how to integrate and theoretically validate the differentiation process with these more complex, potentially discontinuous, inner solvers could be a valuable extension.",
    "Experiment Code": "class ImplicitForward():\n    \"\"\"Algorithm to compute the hypergradient using implicit forward\n    differentiation.\n\n    First the algorithm computes the regression coefficients.\n    Then the iterations of the forward differentiation are applied to compute\n    the Jacobian.\n\n    Parameters\n    ----------\n    tol_jac: float\n        Tolerance for the Jacobian computation.\n    max_iter: int\n        Maximum number of iterations for the inner solver.\n    n_iter_jac: int\n        Maximum number of iterations for the Jacobian computation.\n    use_stop_crit: bool, optional (default=True)\n        Use stopping criterion in hypergradient computation. If False,\n        run to maximum number of iterations.\n    verbose: bool, optional (default=False)\n        Verbosity of the algorithm.\n    \"\"\"\n\n    def __init__(\n            self, tol_jac=1e-3, max_iter=100, n_iter_jac=100,\n            use_stop_crit=True, verbose=False):\n        self.max_iter = max_iter\n        self.tol_jac = tol_jac\n        self.n_iter_jac = n_iter_jac\n        self.use_stop_crit = use_stop_crit\n        self.verbose = verbose\n\n    def get_beta_jac(\n            self, X, y, log_alpha, model, get_grad_outer, mask0=None,\n            dense0=None, quantity_to_warm_start=None, max_iter=1000, tol=1e-3,\n            full_jac_v=False):\n        \"\"\"Compute beta and hypergradient using implicit forward\n        differentiation.\n\n        Parameters\n        ----------\n        X: array-like, shape (n_samples, n_features)\n            Design matrix.\n        y: ndarray, shape (n_samples,)\n            Observation vector.\n        log_alpha: float or np.array, shape (n_features,)\n            Logarithm of hyperparameter.\n        model:  instance of ``sparse_ho.base.BaseModel``\n            A model that follows the sparse_ho API.\n        get_grad_outer: callable\n            Function which returns the gradient of the outer criterion.\n        mask0: ndarray, shape (n_features,)\n            Boolean of active feature of the previous regression coefficients\n            beta for warm start.\n        dense0: ndarray, shape (mask.sum(),)\n            Initial value of the previous regression coefficients\n            beta for warm start.\n        quantity_to_warm_start: ndarray\n            Previous Jacobian of the inner optimization problem.\n        max_iter: int\n            Maximum number of iteration for the inner solver.\n        tol: float\n            The tolerance for the inner optimization problem.\n        full_jac_v: bool\n            TODO\n        \"\"\"\n\n        mask, dense, jac = get_bet_jac_implicit_forward(\n            X, y, log_alpha, mask0=mask0, dense0=dense0,\n            jac0=quantity_to_warm_start,\n            tol_jac=tol, tol=tol, niter_jac=self.n_iter_jac, model=model,\n            max_iter=self.max_iter, verbose=self.verbose)\n        return mask, dense, jac\n\n    def compute_beta_grad(\n            self, X, y, log_alpha, model, get_grad_outer, mask0=None,\n            dense0=None, quantity_to_warm_start=None, max_iter=1000, tol=1e-3,\n            full_jac_v=False):\n        mask, dense, jac = get_bet_jac_implicit_forward(\n            X, y, log_alpha, mask0=mask0, dense0=dense0,\n            jac0=quantity_to_warm_start,\n            tol_jac=self.tol_jac, tol=tol, niter_jac=self.n_iter_jac,\n            model=model, max_iter=self.max_iter, verbose=self.verbose,\n            use_stop_crit=self.use_stop_crit)\n        jac_v = model.get_jac_v(X, y, mask, dense, jac, get_grad_outer)\n        if full_jac_v:\n            jac_v = model.get_full_jac_v(mask, jac_v, X.shape[1])\n\n        return mask, dense, jac_v, jac\n\ndef get_bet_jac_implicit_forward(\n        X, y, log_alpha, model, mask0=None, dense0=None, jac0=None,\n        tol=1e-3, max_iter=1000, niter_jac=1000, tol_jac=1e-6, verbose=False,\n        use_stop_crit=True):\n\n    mask, dense, _ = compute_beta(\n        X, y, log_alpha, mask0=mask0, dense0=dense0, jac0=jac0, tol=tol,\n        max_iter=max_iter, compute_jac=False, model=model, verbose=verbose,\n        use_stop_crit=use_stop_crit)\n    dbeta0_new = model._init_dbeta0(mask, mask0, jac0)\n    reduce_alpha = model._reduce_alpha(np.exp(log_alpha), mask)\n\n    _, dual_var = model._init_beta_dual_var(X, y, mask, dense)\n    jac = get_only_jac(\n        model.reduce_X(X, mask), model.reduce_y(y, mask), dual_var,\n        reduce_alpha, model.sign(dense, log_alpha), dbeta=dbeta0_new,\n        niter_jac=niter_jac, tol_jac=tol_jac, model=model, mask=mask,\n        dense=dense, verbose=verbose, use_stop_crit=use_stop_crit)\n\n    return mask, dense, jac\n\ndef get_only_jac(\n        Xs, y, dual_var, alpha, sign_beta, dbeta=None, niter_jac=100,\n        tol_jac=1e-4, model=\"lasso\", mask=None, dense=None, verbose=False,\n        use_stop_crit=True):\n    n_samples, n_features = Xs.shape\n\n    L = model.get_L(Xs)\n\n    residual_norm = []\n\n    if hasattr(model, 'dual'):\n        ddual_var = model._init_ddual_var(dbeta, Xs, y, sign_beta, alpha)\n        dbeta = model.dbeta\n    else:\n        if dbeta is None:\n            dbeta = model._init_dbeta(n_features)\n        ddual_var = model._init_ddual_var(dbeta, Xs, y, sign_beta, alpha)\n\n    for i in range(niter_jac):\n        if verbose:\n            print(\"%i -st iterations over %i\" % (i, niter_jac))\n        if issparse(Xs):\n            model._update_only_jac_sparse(\n                Xs.data, Xs.indptr, Xs.indices, y, n_samples,\n                n_features, dbeta, dual_var, ddual_var, L, alpha, sign_beta)\n        else:\n            model._update_only_jac(\n                Xs, y, dual_var, dbeta, ddual_var, L, alpha, sign_beta)\n        residual_norm.append(\n            model.get_jac_residual_norm(\n                Xs, y, n_samples, sign_beta, dbeta, dual_var,\n                ddual_var, alpha))\n        if use_stop_crit and i > 1:\n            rel_tol = np.abs(residual_norm[-2] - residual_norm[-1])\n            if (rel_tol < np.abs(residual_norm[-1]) * tol_jac\n                    or residual_norm[-1] < 1e-10):\n                break\n    get_only_jac.n_iter = i\n\n    return dbeta\n\ndef compute_beta(\n        X, y, log_alpha, model, mask0=None, dense0=None, jac0=None,\n        max_iter=1000, tol=1e-3, compute_jac=True, return_all=False,\n        save_iterates=False, verbose=False, use_stop_crit=True, gap_freq=10):\n    n_samples, n_features = X.shape\n    is_sparse = issparse(X)\n    if not is_sparse and not np.isfortran(X):\n        X = np.asfortranarray(X)\n    L = model.get_L(X)\n\n    alpha = np.exp(log_alpha)\n\n    if hasattr(model, 'estimator') and model.estimator is not None:\n        return model._use_estimator(X, y, alpha, tol)\n\n    try:\n        alpha.shape[0]\n        alphas = alpha.copy()\n    except Exception:\n        alphas = np.ones(n_features) * alpha\n    \n    beta, dual_var = model._init_beta_dual_var(X, y, mask0, dense0)\n    dbeta, ddual_var = model._init_dbeta_ddual_var(\n        X, y, mask0=mask0, dense0=dense0, jac0=jac0, compute_jac=compute_jac)\n\n    pobj0 = model._get_pobj0(dual_var, np.zeros(X.shape[1]), alphas, y)\n    pobj = []\n\n    if return_all:\n        list_beta = []\n    if save_iterates:\n        list_beta = []\n        list_jac = []\n\n    for i in range(max_iter):\n        if verbose:\n            print(\"%i -st iteration over %i\" % (i, max_iter))\n        if is_sparse:\n            model._update_beta_jac_bcd_sparse(\n                X.data, X.indptr, X.indices, y, n_samples, n_features, beta,\n                dbeta, dual_var, ddual_var, alphas, L,\n                compute_jac=compute_jac)\n        else:\n            model._update_beta_jac_bcd(\n                X, y, beta, dbeta, dual_var, ddual_var, alphas,\n                L, compute_jac=compute_jac)\n\n        pobj.append(model._get_pobj(dual_var, X, beta, alphas, y))\n\n        if i > 1:\n            if verbose:\n                print(\"relative decrease = \", (pobj[-2] - pobj[-1]) / pobj0)\n\n        if use_stop_crit and i % gap_freq == 0 and i > 0:\n            if hasattr(model, \"_get_dobj\"):\n                dobj = model._get_dobj(dual_var, X, beta, alpha, y)\n                dual_gap = pobj[-1] - dobj\n                if verbose:\n                    print(\"dual gap %.2e\" % dual_gap)\n                if verbose:\n                    print(\"gap %.2e\" % dual_gap)\n                if dual_gap < pobj0 * tol:\n                    break\n            else:\n                if (pobj[-2] - pobj[-1] <= pobj0 * tol):\n                    break\n        if return_all:\n            list_beta.append(beta.copy())\n        if save_iterates:\n            list_beta.append(beta.copy())\n            list_jac.append(dbeta.copy())\n    else:\n        if verbose:\n            print('did not converge !')\n\n    mask = beta != 0\n    dense = beta[mask]\n    jac = model._get_jac(dbeta, mask)\n    if hasattr(model, 'dual'):\n        model.dual_var = dual_var\n        if compute_jac:\n            model.ddual_var = ddual_var\n    if save_iterates:\n        return np.array(list_beta), np.array(list_jac)\n    if return_all:\n        return mask, dense, list_beta\n    else:\n        if compute_jac:\n            return mask, dense, jac\n        else:\n            return mask, dense, None\n\nclass Lasso(BaseModel):\n    \"\"\"Linear Model trained with L1 prior as regularizer (aka the Lasso).\n\n    The optimization objective for Lasso is:\n    (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n\n    Parameters\n    ----------\n    estimator: sklearn estimator\n        Estimator used to solve the optimization problem. Must follow the\n        scikit-learn API.\n    \"\"\"\n\n    def __init__(self, estimator=None):\n        self.estimator = estimator\n\n    # ... (other methods of Lasso class)\n\n    @staticmethod\n    @njit\n    def _update_beta_jac_bcd(\n            X, y, beta, dbeta, dual_var, ddual_var,\n            alpha, L, compute_jac=True):\n        n_samples, n_features = X.shape\n        non_zeros = np.where(L != 0)[0]\n\n        for j in non_zeros:\n            beta_old = beta[j]\n            if compute_jac:\n                dbeta_old = dbeta[j]\n            zj = beta[j] + dual_var @ X[:, j] / (L[j] * n_samples)\n            beta[j] = ST(zj, alpha[j] / L[j])\n            if compute_jac:\n                dzj = dbeta[j] + X[:, j] @ ddual_var / (L[j] * n_samples)\n                dbeta[j:j+1] = np.abs(np.sign(beta[j])) * dzj\n                dbeta[j:j+1] -= alpha[j] * np.sign(beta[j]) / L[j]\n                ddual_var -= X[:, j] * (dbeta[j] - dbeta_old)\n            dual_var -= X[:, j] * (beta[j] - beta_old)\n\n    @staticmethod\n    @njit\n    def _update_beta_jac_bcd_sparse(\n            data, indptr, indices, y, n_samples, n_features, beta,\n            dbeta, dual_var, ddual_var, alphas, L, compute_jac=True):\n\n        non_zeros = np.where(L != 0)[0]\n\n        for j in non_zeros:\n            Xjs = data[indptr[j]:indptr[j+1]]\n            idx_nz = indices[indptr[j]:indptr[j+1]]\n            beta_old = beta[j]\n            if compute_jac:\n                dbeta_old = dbeta[j]\n            zj = beta[j] + dual_var[idx_nz] @ Xjs / (L[j] * n_samples)\n            beta[j:j+1] = ST(zj, alphas[j] / L[j])\n            if compute_jac:\n                dzj = dbeta[j] + Xjs @ ddual_var[idx_nz] / (L[j] * n_samples)\n                dbeta[j:j+1] = np.abs(np.sign(beta[j])) * dzj\n                dbeta[j:j+1] -= alphas[j] * np.sign(beta[j]) / L[j]\n                ddual_var[idx_nz] -= Xjs * (dbeta[j] - dbeta_old)\n            dual_var[idx_nz] -= Xjs * (beta[j] - beta_old)\n\n    @staticmethod\n    @njit\n    def _update_only_jac(Xs, y, dual_var, dbeta, ddual_var,\n                         L, alpha, sign_beta):\n        n_samples, n_features = Xs.shape\n        for j in range(n_features):\n            dbeta_old = dbeta[j]\n            dbeta[j] += Xs[:, j].T @ ddual_var / (L[j] * n_samples)\n            dbeta[j] -= alpha * sign_beta[j] / L[j]\n            ddual_var -= Xs[:, j] * (dbeta[j] - dbeta_old)\n\n    @staticmethod\n    @njit\n    def _update_only_jac_sparse(\n            data, indptr, indices, y, n_samples, n_features,\n            dbeta, dual_var, ddual_var, L, alpha, sign_beta):\n        for j in range(n_features):\n            Xjs = data[indptr[j]:indptr[j+1]]\n            idx_nz = indices[indptr[j]:indptr[j+1]]\n            dbeta_old = dbeta[j]\n            dbeta[j] += Xjs @ ddual_var[idx_nz] / (L[j] * n_samples)\n            dbeta[j] -= alpha * sign_beta[j] / L[j]\n            ddual_var[idx_nz] -= Xjs * (dbeta[j] - dbeta_old)",
    "Experiment Result": "The 'Implicit Forward Iterative Differentiation' (IFID) method is extensively compared against various hyperparameter optimization strategies across several Lasso-type models and datasets.\n\n**1. Models and Inner Solvers:**\nThe method is applied to various Lasso-type models including:\n- Lasso (`sparse_ho.models.Lasso`)\n- Elastic Net (`sparse_ho.models.ElasticNet`)\n- Sparse Logistic Regression (`sparse_ho.models.SparseLogreg`)\n- Weighted Lasso (`sparse_ho.models.WeightedLasso`)\n- SVM (`sparse_ho.models.SVM`)\n- SVR (`sparse_ho.models.SVR`)\n- Simplex SVR (`sparse_ho.models.SimplexSVR`)\nInner problems are solved using proximal Block Coordinate Descent (BCD) algorithms, with custom JIT-compiled (`@njit`) updates for both dense and sparse data (`_update_beta_jac_bcd`, `_update_only_jac`, and their `_sparse` counterparts within model classes). For some comparisons, external solvers like `celer.Lasso`, `celer.LogisticRegression`, `sklearn.linear_model.ElasticNet`, `lightning.classification.LinearSVC` are integrated as estimators within `sparse_ho` models.\n\n**2. Hyperparameter Optimization Methods Compared:**\n- **Gradient-based methods (first-order)**:\n    - **Implicit Forward Iterative Differentiation (IFID)** (`sparse_ho.ImplicitForward`): The core method, leveraging fixed-point iteration and support identification.\n    - **Forward Iterative Differentiation** (`sparse_ho.Forward`): A baseline iterative differentiation approach.\n    - **Implicit Differentiation** (`sparse_ho.Implicit`): Traditional implicit differentiation, typically involving solving a linear system on the support.\n- **Non-gradient methods (zero-order)**:\n    - **Grid-search** (`sparse_ho.grid_search.grid_search`, also scikit-learn's `LassoCV`, `LogisticRegressionCV`):\n    - **Random-search** (`sparse_ho.ho.hyperopt_wrapper` with `method='random'` or `sparse_ho.grid_search.grid_search` with `samp=\"random\"`)\n    - **Bayesian optimization** (`sparse_ho.ho.hyperopt_wrapper` with `method='bayesian'`)\n    - **Lattice Hypercube Sampling** (`sparse_ho.grid_search.grid_search` with `samp=\"lhs\"`)\n- **Other comparisons**:\n    - `Cvxpylayers`: Used in `expes/expe_cvxpy` to compare against `Forward` and `Backward` for ground truth gradient computation.\n\n**3. Outer Optimizers for Gradient-based Methods:**\n- `sparse_ho.optimizers.LineSearch`\n- `sparse_ho.optimizers.GradientDescent` (with adaptive step size `p_grad_norm`)\n- `sparse_ho.optimizers.Adam`\n\n**4. Datasets:**\nExperiments utilize a variety of real-world and synthetic datasets:\n- **Regression/Binary Classification**: `rcv1.binary`, `simu` (synthetic from `make_classification` or `make_correlated_data`), `rcv1`, `real-sim`, `20news`, `leukemia`, `finance`, `kdda_train`, `climate`, `gina_agnostic`.\n- **MEG data**: `mne.datasets.sample` for Weighted Lasso comparison.\n- **Multiclass Classification**: `mnist`, `usps`, `sector_scale`, `aloi` for multiclass logistic regression.\n\n**5. Key Experimental Settings:**\n- **Inner solver tolerance (`tol`)**: Ranges from `1e-3` to `1e-32` depending on the experiment's precision requirements.\n- **Max iterations for inner solver (`max_iter`)**: Typically between `50` and `100,000`.\n- **Outer optimizer iterations (`n_outer`)**: Common values are `10`, `25`, `30`, `50`, `75`, `100`.\n- **Jacobian computation iterations (`n_iter_jac`)**: Often `1000` or `5000` for `ImplicitForward`, with `tol_jac` set similarly to `tol`.\n- **Hyperparameter range**: Determined by `alpha_max` (data-derived) and `alpha_min` (e.g., `1e-2 * alpha_max`, `1e-4 * alpha_max`, `alpha_max / 1_000`, `alpha_max / 100_000`). Values are typically sampled logarithmically (`np.geomspace`).\n- **Warm-starting**: All iterative methods extensively use warm-starting for regression coefficients (`mask0`, `dense0`) and Jacobian-related quantities (`quantity_to_warm_start` or `jac0`) across outer optimization iterations for efficiency.\n- **Stopping criteria**: Both inner problem and Jacobian computation can use explicit stopping criteria (`use_stop_crit=True`) based on primal/dual gap or residual norm, or run for a fixed number of iterations."
}{
    "Title": "A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization",
    "Main Contributions": "The paper addresses the open problem of hyperparameter optimization (HPO) in differentially private deep learning, which is computationally expensive and compromises privacy utility. It proposes a new adaptive HPO method, called the 'new linear scaling rule', that uses low-cost (privacy and runtime) trials to estimate optimal hyperparameters and scales them up linearly to larger privacy budgets. This method achieves state-of-the-art performance across 22 benchmark tasks in computer vision and natural language processing, significantly reducing the computation and privacy cost of HPO without sacrificing performance, all while accounting for the privacy cost of HPO.",
    "Methodology": "The core methodology is the 'new linear scaling rule' which formalizes the intuition that optimal hyperparameters for DP-SGD scale linearly with the privacy budget (ε). The method first reduces the dimensionality of HPO by combining key hyperparameters (learning rate η and number of iterations T) into a single scalar variable, r = η × T. It then approximates the function r(ε) using a first-order Taylor approximation by empirically finding two points (ε1, r(ε1)) and (ε2, r(ε2)) via random search with small privacy budgets. A linear interpolation is performed to estimate r for any target εf. The privacy guarantee for the entire HPO process is accounted for using Privacy Loss Variable (PLV) accounting (f-DP). Key design choices include using zero initialization, full-batch gradient descent for maximizing signal-to-noise ratio, unit gradient clipping (C=1), and momentum (ρ=0.9). A theoretical analysis of private gradient descent provides intuition for the linear scaling, relating excess empirical risk to a 'noisy radius' bounded by factors including η and T.",
    "Experimental Setup": "The method was extensively evaluated on 22 benchmark tasks spanning computer vision (ImageNet, CIFAR10/100, FashionMNIST, STL10, EMNIST) and natural language processing (SQuAD for Question Answering, GLUE benchmark for text classification - SST-2, QNLI, QQP, MNLI(m/mm), and next word generation - PersonaChat, WikiText-2, Enron Emails). It covered fine-tuning models pretrained on public data and training from scratch. Models included ResNets and multi-billion-parameter Transformers (ViT, beit, beitv2, convnext, GPT-2, RoBERTa-base). Evaluation involved comparisons to random search, grid search (without HPO cost), and five prior private HPO methods, across a wide range of privacy budgets (ε ∈ [0.01, 8.0]). The effectiveness was also assessed under various distribution shifts (CIFAR10 → STL, CIFAR10p1, CIFAR10C; CIFAR100 → CIFAR100C; Waterbirds, FMoW, Camelyon17) and for different architectures. Metrics included accuracy for classification and perplexity for language modeling.",
    "Limitations": "The theoretical assumptions (e.g., strong convexity, smoothness, bounded gradient implying no clipping impact) do not hold generally for training complex neural networks, although the heuristic proved empirically successful. The adaptive HPO method has a higher runtime (7 GPU hours, 3 wall-clock hours for a baseline scenario) compared to random search (1 GPU hour, 1 wall-clock hour). Its serial dependency for different ε values limits parallelization compared to grid search. Optimizing a broader set of hyperparameters (e.g., batch size B and clipping threshold C alongside η and T) simultaneously resulted in worse performance. For sufficiently small ε on difficult datasets like ImageNet, keeping the HPO privacy cost low may leave insufficient budget for the final run, making HP trials uninformative.",
    "Future Research Directions": "Future work could incorporate additional factors into the theoretical analysis of private gradient descent, such as momentum acceleration, bias introduced by clipping, or extend the analysis to more general neural networks. Conducting RDP (Rényi Differential Privacy) analysis of the adaptive method is another direction. It is suggested to explore privatizing existing non-private HPO methods. Applying the method to alternate threat models, such as public data-assisted private model training, could be investigated. Further research could also focus on effectively scaling ideas from other private HPO methods (e.g., Koskela & Kulkarni (2023)'s subsampling) to larger datasets like ImageNet. For language models, exploring longer training iterations to further improve utility is also a potential area.",
    "Experiment Code": "# File Path: private_transformers/privacy_engine.py\nclass PrivacyEngine(object):\n    \"\"\"Differentially-private optimization engine that works gracefully with Hugging Face transformers.\n\n    Supports ghost clipping as described in\n        Li, X., Tramèr, F., Liang, P., & Hashimoto, T. (2021).\n        Large Language Models Can Be Strong Differentially Private Learners.\n        arXiv preprint arXiv:2110.05679.\n\n    Implicitly assumes inputs are in batch first format.\n    \"\"\"\n\n    def __init__(\n        self,\n        module: nn.Module,\n        *,\n        batch_size: int,\n        sample_size: int,\n        max_grad_norm: float,\n        epochs: Optional[Union[int, float]] = None,\n        noise_multiplier: Optional[float] = None,\n        target_epsilon: Optional[float] = None,\n        target_delta: Optional[Optional[float]] = None,\n        alphas: Sequence[float] = accounting_manager.DEFAULT_ALPHAS,\n        record_snr: bool = True,\n        named_params: Optional[Sequence] = None,\n        numerical_stability_constant=1e-6,\n        clipping_mode=ClippingMode.default,\n        accounting_mode=\"rdp\",\n        eps_error=0.05,\n        skip_checks=False,\n        **unused_kwargs,\n    ):\n        \"\"\"Initialize the engine.\n\n        Args:\n            module: The PyTorch module for which per-sample gradient is required.\n                Setting the `requires_grad` attribute of a parameter to False\n                disables the per-sample gradient accumulation.\n            batch_size: The expected size of Poisson-sampled batch, i.e., the lot size.\n            sample_size: Size of dataset.\n            max_grad_norm: The maximum 2-norm for gradient clipping.\n            epochs: The number of epochs for training.\n            noise_multiplier: The extra multiplier for DP-SGD noise.\n            target_epsilon: The target privacy spending.\n                Only used to estimate the `noise_multiplier` if it is not set.\n            target_delta: The target failure probability.\n                Defaults to sample_size ** -1.1 if not set.\n            alphas: The RDP orders for (ε, δ)-DP conversion. Useless if not accounting in RDP.\n            record_snr: Record and report the signal-to-noise ratio --\n                ratio between norm of summed clipped gradient and norm of noise vector.\n            named_params: Specifies which parameters need gradients;\n                defaults to use parameters which require grad in module.\n            numerical_stability_constant: Small constant to avoid division by 0 when clipping.\n            clipping_mode: The clipping mode to use. One of 'default', 'ghost', 'per_layer', 'per_layer_percentile'.\n            accounting_mode: The method of accounting privacy. One of (`rdp`, `glw`, `all`).\n                Meanings of shorthands:\n                    - rdp: Account loss with RDP but perform conversion to approx-DP with a procedure defined in\n                        \"The Discrete Gaussian for Differential Privacy\". https://arxiv.org/abs/2004.00010\n                    - glw: Account loss by numerically composing tradeoff functions in f-DP; defined in\n                        \"Numerical composition of differential privacy\". https://arxiv.org/abs/2106.02848\n                    - all: Report loss with all methods listed above.\n            eps_error: Error threshold for upper and lower bound in the GLW accounting procedure.\n            skip_checks: Skips the model type validation test if True.\n        \"\"\"\n        utils.handle_unused_kwargs(unused_kwargs)\n        del unused_kwargs\n        super(PrivacyEngine, self).__init__()\n\n        if clipping_mode not in ClippingMode.all():\n            raise ValueError(f\"Unknown clipping mode {clipping_mode}. Expected one of {ClippingMode.all()}.\")\n        if accounting_mode not in AccountingMode.all():\n            raise ValueError(f\"Unknown accounting mode: {accounting_mode}. Expected one of {AccountingMode.all()}.\")\n        if epochs <= 0.0:\n            raise ValueError(f\"Number of training epochs cannot be non-positive, but found epochs={epochs}\")\n\n        # Privacy parameters.\n        sample_rate = batch_size / sample_size\n        if target_delta is None:\n            target_delta = sample_size ** -1.1\n        if noise_multiplier is None:\n            if target_epsilon is None or epochs is None:\n                raise ValueError(\n                    f\"`target_epsilon` and `epochs` must be specified when `noise_multiplier` is `None`.\"\n                )\n            if accounting_mode in (\"rdp\", \"all\"):\n                manager = accounting_manager.RDPManager(alphas=alphas)\n            else:  # \"glw\"\n                manager = accounting_manager.GLWManager(eps_error=eps_error)\n            noise_multiplier = manager.compute_sigma(\n                target_epsilon=target_epsilon, target_delta=target_delta, sample_rate=sample_rate, epochs=epochs,\n            )\n\n        self.batch_size = batch_size\n        self.sample_size = sample_size\n        self.sample_rate = sample_rate\n        self.max_grad_norm = max_grad_norm\n\n        self.epochs = epochs\n        self.noise_multiplier = noise_multiplier\n        self.effective_noise_multiplier = noise_multiplier / batch_size\n        self.target_epsilon = target_epsilon\n        self.target_delta = target_delta\n        self.alphas = alphas\n        self.eps_error = eps_error\n        self.accounting_mode = accounting_mode\n        self.record_snr = record_snr\n\n        # Internals.\n        self.steps = 0  # Tracks privacy spending.\n\n        # Recording.\n        self.max_clip = None\n        self.min_clip = None\n        self.med_clip = None\n        self.signal = None\n        self.noise = None\n        self.snr = None\n        self.noise_limit = None\n\n        # Record parameters.\n        self.module = module\n        if named_params is None:\n            self.named_params = tuple(\n                (name, param) for (name, param) in module.named_parameters() if param.requires_grad\n            )\n        else:\n            self.named_params = named_params\n        self.num_params = sum(param.numel() for _, param in self.named_params)\n\n        self._locked = False  # Lock the part where noisy gradients is created (in `self.step`) if True.\n        self.numerical_stability_constant = numerical_stability_constant\n        self.clipping_mode = clipping_mode\n        if clipping_mode == ClippingMode.ghost:\n            autograd_grad_sample.set_hooks_mode(BackwardHookMode.ghost_norm)  # Prepare for first backward.\n        else:\n            autograd_grad_sample.set_hooks_mode(BackwardHookMode.default)  # Extra guard.\n\n        if not isinstance(module, SUPPORTED_TRANSFORMERS) and not skip_checks:\n            raise ValueError(\n                f\"Model type {type(module)} is not supported. Please file an issue if you want this model to be added.\\n\"\n                f\"Currently supported transformers are: {SUPPORTED_TRANSFORMERS}\"\n            )\n        transformers_support.forward_swapper(module=module)  # Fix the position embeddings broadcast issue.\n\n    def lock(self):\n        \"\"\"Run this after noisy clipped gradient is created to prevent tampering with it before parameter update.\"\"\"\n        self._locked = True\n\n    def unlock(self):\n        \"\"\"Run this after parameter update to allow creation of noisy gradient for next step\"\"\"\n        self._locked = False\n\n    def attach(self, optimizer):\n        # `loss_reduction=\"sum\"` super important.\n        autograd_grad_sample.add_hooks(model=self.module, loss_reduction=\"sum\")\n\n        # Override zero grad.\n        def dp_zero_grad(_self, *args, **kwargs):\n            _self.privacy_engine.zero_grad()\n\n        # Override step.\n        def dp_step(_self, **kwargs):\n            closure = kwargs.pop(\"closure\", None)\n\n            _self.privacy_engine.step(**kwargs)\n            _self.original_step(closure=closure)\n            _self.privacy_engine.unlock()  # Only enable creating new grads once parameters are updated.\n            _self.privacy_engine.steps += 1\n\n        def virtual_step(_self, **kwargs):\n            _self.privacy_engine.virtual_step(**kwargs)\n\n        def get_privacy_spent(_self, **kwargs):\n            return _self.privacy_engine.get_privacy_spent(**kwargs)\n\n        def get_training_stats(_self, **kwargs):\n            return _self.privacy_engine.get_training_stats(**kwargs)\n\n        optimizer.privacy_engine = self\n\n        optimizer.original_step = optimizer.step\n        optimizer.step = types.MethodType(dp_step, optimizer)\n\n        optimizer.original_zero_grad = optimizer.zero_grad\n        optimizer.zero_grad = types.MethodType(dp_zero_grad, optimizer)\n\n        optimizer.virtual_step = types.MethodType(virtual_step, optimizer)\n\n        # Make getting info easier.\n        optimizer.get_privacy_spent = types.MethodType(get_privacy_spent, optimizer)\n        optimizer.get_training_stats = types.MethodType(get_training_stats, optimizer)\n\n        self.module.privacy_engine = self\n\n        # Just to be safe, we also override `zero_grad` for module.\n        self.module.original_zero_grad = self.module.zero_grad\n        self.module.zero_grad = types.MethodType(dp_zero_grad, self.module)\n\n        # For easy detaching.\n        self.optimizer = optimizer\n\n    def detach(self):\n        optimizer = self.optimizer\n        optimizer.step = optimizer.original_step\n        optimizer.zero_grad = optimizer.original_zero_grad\n        delattr(optimizer, \"privacy_engine\")\n        delattr(optimizer, \"original_step\")\n        delattr(optimizer, \"original_zero_grad\")\n        delattr(optimizer, \"virtual_step\")\n        delattr(optimizer, \"get_privacy_spent\")\n        delattr(optimizer, \"get_training_stats\")\n\n        module = self.module\n        autograd_grad_sample.remove_hooks(module)\n        autograd_grad_sample.set_hooks_mode(\"default\")  # This is super important when there are multiple attaches!\n        module.zero_grad(skip_grad=True)  # noqa\n        module.zero_grad = module.original_zero_grad\n        delattr(module, \"original_zero_grad\")\n\n    @torch.no_grad()\n    def step(\n        self,\n        loss: torch.Tensor,\n        scale=1.,\n        # Function that takes in named_params and does something.\n        # This option was included to help with another spectrum analysis project.\n        callback: Optional[Callable] = None,\n    ):\n        if loss.dim() != 1:\n            raise ValueError(\n                f\"Expected `loss` to be the per-example loss 1-D tensor, but got a tensor with dims={loss.dim()}.\"\n            )\n\n        if self.clipping_mode == ClippingMode.ghost:\n            if callback is not None:\n                raise ValueError(\"Ghost clipping does not support `callback` in `optimizer.step`.\")\n            if scale != 1.:\n                raise ValueError(\"Ghost clipping does not support mixed-precision training.\")\n            self._ghost_step(loss=loss)\n        else:\n            self._step(loss=loss, scale=scale, callback=callback)\n\n    @torch.no_grad()\n    def virtual_step(self, loss: torch.Tensor, scale=1.):\n        \"\"\"Virtual step function when there's gradient accumulation.\"\"\"\n        if self.clipping_mode == ClippingMode.ghost:\n            self._ghost_virtual_step(loss=loss)\n        else:\n            self._virtual_step(loss=loss, scale=scale)\n\n    def zero_grad(self, skip_grad=False):\n        for name, param in self.named_params:\n            if hasattr(param, \"grad_sample\"):\n                del param.grad_sample\n            if hasattr(param, \"norm_sample\"):\n                del param.norm_sample\n            if hasattr(param, \"summed_grad\"):\n                del param.summed_grad\n            if not skip_grad:\n                if hasattr(param, \"grad\"):\n                    del param.grad\n\n    def _create_noisy_clipped_gradient(self):\n        \"\"\"Create noisy clipped gradient for `optimizer.step`.\n\n        Add noise and scale by inverse batch size.\n\n        Notes:\n            In ghost clipping, `summed_grad` stores previous micro-batches; `grad` stores current micro-batch.\n            In default clipping, `summed_grad` stores summed clipped gradients for all micro-batches.\n        \"\"\"\n\n        signals, noises = [], []\n        for name, param in self.named_params:\n            assert hasattr(param, 'summed_grad'), (\n                f\"Internal error: PrivacyEngine should not reach here; \"\n                f\"this means either \"\n                f\"1) there is parameter which requires gradient, but was not used in the computational graph, \"\n                f\"or 2) the backward hook registry failed to find the corresponding module to register.\"\n            )\n            param.grad = param.summed_grad  # Ultra important to override `.grad`.\n\n            if self.record_snr:\n                signals.append(param.grad.reshape(-1).norm(2))\n\n            if self.noise_multiplier > 0 and self.max_grad_norm > 0:\n                noise = torch.normal(\n                    mean=0,\n                    std=self.noise_multiplier * self.max_grad_norm,\n                    size=param.size(),\n                    device=param.device,\n                    dtype=param.dtype,\n                )\n                param.grad += noise\n                if self.record_snr:\n                    noises.append(noise.reshape(-1).norm(2))\n                del noise\n\n            param.grad /= self.batch_size\n\n        if self.record_snr and len(noises) > 0:\n            self.signal, self.noise = tuple(torch.stack(lst).norm(2).item() for lst in (signals, noises))\n            self.noise_limit = math.sqrt(self.num_params) * self.noise_multiplier * self.max_grad_norm\n            self.snr = self.signal / self.noise\n        else:\n            self.snr = math.inf  # Undefined!\n\n        self.lock()  # Make creating new gradients impossible, unless optimizer.step is called.\n\n    # --- ghost clipping ---\n    def _ghost_step(self, loss: torch.Tensor):\n        \"\"\"Run double-backward on per-example loss, then sum up all gradients and noise it.\"\"\"\n        if self._locked:  # Skip this gradient creation step if already created gradient and haven't stepped.\n            logging.warning(\"Attempted to step, but the engine is on lock.\")\n            return\n\n        self._ghost_virtual_step(loss)\n        self._create_noisy_clipped_gradient()\n\n    @torch.no_grad()\n    def _ghost_virtual_step(self, loss: torch.Tensor):\n        \"\"\"Backward twice to accumulate summed clipped gradients in `.summed_grad`.\n\n        We accumulate gradients in `.summed_grad` for micro-batching.\n        All of this copying actually creates a new 2x memory overhead.\n        \"\"\"\n        self._double_backward(loss)\n\n        for name, param in self.named_params:\n            if hasattr(param, 'summed_grad'):\n                param.summed_grad += param.grad\n            else:\n                param.summed_grad = param.grad\n\n            if hasattr(param, \"grad\"):\n                del param.grad\n            if hasattr(param, \"norm_sample\"):\n                del param.norm_sample\n            if hasattr(param, \"grad_sample\"):\n                del param.grad_sample\n\n    @torch.enable_grad()\n    def _double_backward(self, loss: torch.Tensor):\n        \"\"\"Given per-example losses, backward twice to accumulate summed clipped gradients in `.grad`.\"\"\"\n        first_loss = loss.sum()\n        first_loss.backward(retain_graph=True)\n\n        # Prepare for second backward.\n        autograd_grad_sample.set_hooks_mode(BackwardHookMode.ghost_grad)\n\n        # The first backward might have accumulated things we don't need into `.grad`;\n        # remove it before the second pass to avoid accumulating garbage.\n        for name, param in self.named_params:\n            if hasattr(param, \"grad\"):\n                del param.grad\n\n        coef_sample = self.get_coef_sample()\n        second_loss = (coef_sample * loss).sum(dim=0)\n        second_loss.backward()\n\n        # Prepare for first backward (in the next round).\n        autograd_grad_sample.set_hooks_mode(BackwardHookMode.ghost_norm)\n\n    def get_coef_sample(self) -> torch.Tensor:\n        \"\"\"Get per-example gradient scaling factor for clipping.\"\"\"\n        norm_sample = self.get_norm_sample()\n        return torch.clamp_max(self.max_grad_norm / (norm_sample + self.numerical_stability_constant), 1.)\n\n    def get_norm_sample(self) -> torch.Tensor:\n        \"\"\"Get per-example gradient norms.\"\"\"\n        norm_sample = torch.stack([param.norm_sample for name, param in self.named_params], dim=0).norm(2, dim=0)\n        return norm_sample\n\n    # --- default clipping ---\n    def _step(\n        self,\n        loss,\n        scale,\n        callback,\n    ):\n        \"\"\"Create noisy gradients.\n\n        Should be run right before you call `optimizer.step`.\n\n        This function does 3 things:\n            1) call `loss.backward()`\n            2) clip the current `.grad_sample` and add that to `.summed_grad`\n            3) noise the gradients\n        In mixed-precision training (with amp), the last two steps require knowing the loss scaling factor.\n\n        Args:\n            loss: The per-example loss; a 1-D tensor.\n            scale: The loss up-scaling factor in amp. In full precision, this arg isn't useful.\n        \"\"\"\n        if self._locked:  # Skip this gradient creation step if already created gradient and haven't stepped.\n            logging.warning(\"Attempted to step, but the engine is on lock.\")\n            return\n\n        norm_sample, coef_sample = self._accumulate_summed_grad(loss=loss, scale=scale)\n        # Collect stats for debugging.\n        self.max_clip = coef_sample.max().item()\n        self.min_clip = coef_sample.min().item()\n        self.med_clip = coef_sample.median().item()\n\n        if callback is not None:\n            callback(self)\n        self._create_noisy_clipped_gradient()\n\n    def _virtual_step(self, loss, scale):\n        self._accumulate_summed_grad(loss=loss, scale=scale)\n\n    @torch.no_grad()\n    def _accumulate_summed_grad(self, loss, scale):\n        \"\"\"Accumulate signal by summing clipped gradients.\n\n        Removes `.grad_sample` and `.grad` for each variable that requires grad at the end.\n        \"\"\"\n        with torch.enable_grad():\n            loss.sum(dim=0).backward()\n\n        norm_sample = []\n        for name, param in self.named_params:\n            try:\n                batch_size = param.grad_sample.size(0)\n            except AttributeError as error:\n                args = error.args\n                extra_msg = f\"\\n *** {name} parameter has no grad_sample attribute ***\"\n                error.args = (args[0] + extra_msg, *args[1:])\n                raise error\n            norm = param.grad_sample.reshape(batch_size, -1).norm(2, dim=1)\n            norm_sample.append(norm)\n\n        # The stack operation here is prone to error, thus clarify where the error is.\n        try:\n            norm_sample = torch.stack(norm_sample, dim=0).norm(2, dim=0)\n        except RuntimeError as runtime_error:\n            args = runtime_error.args\n\n            # Get the major shape.\n            shapes = collections.defaultdict(int)\n            for tensor in norm_sample:\n                shapes[tensor.size()] += 1\n\n            # Get the shape that most tensors have.\n            major_shape, major_count = max(shapes.items(), key=lambda x: x[1])\n\n            # Check which tensors don't have the major shape!\n            extra_msg = f\" \\n*** Major shape: {major_shape}\"\n            for (name, param), tensor in zip(list(self.named_params), norm_sample):\n                if tensor.size() != major_shape:\n                    extra_msg += f\", {name} wrong shape: {tensor.size()}\"\n            extra_msg += \" ***\"\n\n            runtime_error.args = (args[0] + extra_msg, *args[1:])\n            raise runtime_error\n\n        coef_sample = torch.clamp_max(\n            self.max_grad_norm * scale / (norm_sample + self.numerical_stability_constant), 1.\n        )\n        for name, param in self.named_params:\n            if not hasattr(param, 'summed_grad'):\n                param.summed_grad = 0.\n            current_device = param.grad_sample.device\n            param.summed_grad += torch.einsum(\"i,i...->...\", coef_sample.to(current_device), param.grad_sample)\n\n            # Aggressive memory saving -- delete everything except `.summed_grad` to save memory!\n            if hasattr(param, \"grad_sample\"):\n                # This must be deleted due to how `privacy_utils::supported_layers_grad_samplers.py` works!\n                #   When a parameter with `.grad_sample` is reused, the per-sample gradients are accumulated!\n                del param.grad_sample\n            if hasattr(param, \"grad\"):\n                del param.grad\n\n        return norm_sample, coef_sample\n\n    def get_privacy_spent(\n        self,\n        steps: Optional[int] = None,\n        accounting_mode: Optional[str] = None,\n        lenient=False\n    ) -> Dict:\n        if steps is None:\n            steps = self.steps\n        if accounting_mode is None:\n            accounting_mode = self.accounting_mode\n\n        privacy_results = {}  # Contains stats from all modes.\n        if accounting_mode in (AccountingMode.all_, AccountingMode.rdp):\n            try:\n                manager = accounting_manager.RDPManager(alphas=self.alphas)\n                privacy_results.update(\n                    manager.compute_epsilon(\n                        sigma=self.noise_multiplier,\n                        sample_rate=self.sample_rate,\n                        target_delta=self.target_delta,\n                        steps=steps,\n                    )\n                )\n            except Exception as err:\n                logging.fatal(\"RDP accounting failed! Double check privacy parameters.\")\n                if not lenient:\n                    raise err\n\n        if accounting_mode in (AccountingMode.all_, AccountingMode.glw):\n            try:\n                manager = accounting_manager.GLWManager(eps_error=self.eps_error)\n                privacy_results.update(\n                    manager.compute_epsilon(\n                        sigma=self.noise_multiplier,\n                        sample_rate=self.sample_rate,\n                        target_delta=self.target_delta,\n                        steps=steps\n                    )\n                )\n            except Exception as err:\n                logging.fatal(\n                    \"Numerical composition of tradeoff functions failed! Double check privacy parameters.\"\n                )\n                if not lenient:\n                    raise err\n\n        return privacy_results\n\n    def get_training_stats(self):\n        \"\"\"Get the clipping, signal, and noise statistics.\"\"\"\n        return {\n            \"med_clip\": self.med_clip,\n            \"max_clip\": self.max_clip,\n            \"min_clip\": self.min_clip,\n            \"snr\": self.snr,\n            \"signal\": self.signal,\n            \"noise\": self.noise,\n            \"noise_limit\": self.noise_limit,\n        }\n\n    def __repr__(self):\n        return (\n            f\"PrivacyEngine(\\n\"\n            f\"  target_epsilon={self.target_epsilon:.6f}, \\n\"\n            f\"  target_delta={self.target_delta:.6f}, \\n\"\n            f\"  noise_multiplier={self.noise_multiplier:.6f}, \\n\"\n            f\"  effective_noise_multiplier={self.effective_noise_multiplier:.6f}, \\n\"\n            f\"  epochs={self.epochs}, \\n\"\n            f\"  max_grad_norm={self.max_grad_norm}, \\n\"\n            f\"  sample_rate={self.sample_rate}, \\n\"\n            f\"  batch_size={self.batch_size}, \\n\"\n            f\"  accounting_mode={self.accounting_mode}, \\n\"\n            f\"  clipping_mode={self.clipping_mode}\\n\"\n            f\")\"\n        )\n\n# File Path: private_transformers/autograd_grad_sample.py\ndef add_hooks(model: nn.Module, loss_reduction: str = \"mean\"):\n    r\"\"\"\n    Adds hooks to model to save activations and backprop values.\n    The hooks will\n\n    1. save activations into ``param.activations`` during forward pass.\n    2. compute per-sample gradients and save them in ``param.grad_sample`` during backward pass.\n\n    Args:\n        model: Model to which hooks are added.\n        loss_reduction: Indicates if the loss reduction (for aggregating the gradients) is a sum or a mean operation.\n            Can take values ``sum`` or ``mean``.\n    \"\"\"\n    if hasattr(model, \"autograd_grad_sample_hooks\"):\n        raise ValueError(\"Trying to add hooks twice to the same model\")\n\n    enable_hooks()\n\n    handles = []\n    for name, layer in model.named_modules():\n        if type(layer) in _supported_layers_grad_samplers:\n            if requires_grad(layer, recurse=False):\n                handles.append(layer.register_forward_hook(_capture_activations))\n\n                def this_backward(this_layer, grad_input, grad_output):\n                    return _capture_backprops(this_layer, grad_input, grad_output, loss_reduction)\n\n                # Starting with 1.8.0, use `register_full_backward_hook`.\n                handles.append(layer.register_backward_hook(this_backward))\n\n    model.__dict__.setdefault(\"autograd_grad_sample_hooks\", []).extend(handles)\n\n# File Path: examples/classification/src/compiled_args.py\n@dataclass\nclass PrivacyArguments:\n    \"\"\"Arguments for differentially private training.\"\"\"\n\n    per_example_max_grad_norm: float = field(\n        default=.1, metadata={\n            \"help\": \"Clipping 2-norm of per-sample gradients.\"\n        }\n    )\n    noise_multiplier: float = field(\n        default=None, metadata={\n            \"help\": \"Standard deviation of noise added for privacy; if `target_epsilon` is specified, \"\n                    \"use the one searched based budget\"\n        }\n    )\n    target_epsilon: float = field(\n        default=None, metadata={\n            \"help\": \"Privacy budget; if `None` use the noise multiplier specified.\"\n        }\n    )\n    target_delta: float = field(\n        default=None, metadata={\n            \"help\": \"Lax probability in approximate differential privacy; if `None` use 1 / len(train_data).\"\n        }\n    )\n    non_private: str = field(\n        default=\"yes\", metadata={\"help\": \"Train non-privately if True.\"}\n    )\n    accounting_mode: str = field(\n        default=\"rdp\", metadata={\"help\": \"One of (`rdp`, `glw`, `all`).\"}\n    )\n    clipping_mode: str = field(\n        default=\"default\"\n    )\n\n    def __post_init__(self):\n        self.non_private = self.non_private.lower() in true_tags  # noqa\n\n# File Path: examples/classification/run_classification.py\n# Excerpt from main() function showing PrivacyEngine instantiation and usage\n    optimizer = trainer.optimizer = torch.optim.AdamW(\n        optimizer_grouped_parameters,\n        lr=training_args.learning_rate,\n        betas=(training_args.adam_beta1, training_args.adam_beta2),\n        eps=training_args.adam_epsilon,\n    )\n    if training_args.lr_decay:  # Default linear decay.\n        training_setup = trainer.get_training_setup()\n        t_total = training_setup[\"t_total\"]\n        # `trainer.optimizer` is not None here, so no optimizer is created.\n        trainer.create_optimizer_and_scheduler(num_training_steps=t_total)\n    else:\n        trainer.lr_scheduler = torch.optim.lr_scheduler.LambdaLR(trainer.optimizer, lambda _: 1.)\n\n    if privacy_args.non_private:\n        privacy_args.noise_multiplier = 0.\n        privacy_args.per_example_max_grad_norm = None\n    else:\n        total_train_batch_size = training_args.gradient_accumulation_steps * training_args.per_device_train_batch_size\n        privacy_engine = PrivacyEngine(\n            module=model,\n            batch_size=total_train_batch_size,\n            sample_size=len(train_dataset),\n            epochs=training_args.num_train_epochs,\n            max_grad_norm=privacy_args.per_example_max_grad_norm,\n            noise_multiplier=privacy_args.noise_multiplier,\n            target_epsilon=privacy_args.target_epsilon,\n            target_delta=privacy_args.target_delta,\n            accounting_mode=privacy_args.accounting_mode,\n            clipping_mode=privacy_args.clipping_mode,\n            skip_checks=True,\n        )\n        # Originally, it could have been null.\n        privacy_args.noise_multiplier = privacy_engine.noise_multiplier\n        privacy_args.target_delta = privacy_engine.target_delta\n\n        print('privacy_args: ')\n        print(json.dumps(privacy_args.__dict__, indent=4))\n        privacy_engine.attach(optimizer)\n\n# Excerpt from Trainer.train() loop showing optimizer step calls\n            for step, inputs in enumerate(epoch_iterator):\n                # ... (skip past steps logic) ...\n                losses = self.training_step(model, inputs)\n                tr_loss += losses[\"scalar_loss\"]\n\n                if (step + 1) % self.args.gradient_accumulation_steps == 0 or (\n                    # last step in epoch but step is always smaller than gradient_accumulation_steps\n\n                    # --- Don't do the update when this is the case. You get bad batch size for privacy ---\n                    # len(epoch_iterator) <= self.args.gradient_accumulation_steps\n                    # and (step + 1) == len(epoch_iterator)\n                    # ---\n                ):\n                    if self.privacy_args.non_private:\n                        # Don't double clip in private learning.\n                        torch.nn.utils.clip_grad_norm_(model.parameters(), self.args.max_grad_norm)\n                        optimizer.step()\n                    else:\n                        # ... callback handling ...\n                        vector_loss = losses.get(\"vector_loss\")\n                        self.optimizer.step(loss=vector_loss, callback=callback)\n\n                    scheduler.step()\n                    model.zero_grad(set_to_none=True)\n                    self.global_step += 1\n                    self.epoch = epoch + (step + 1) / len(epoch_iterator)\n\n                    # ... (logging and evaluation logic) ...\n                else:\n                    if not self.privacy_args.non_private:\n                        self.optimizer.virtual_step(loss=losses.get(\"vector_loss\"))  # noqa\n\n# File Path: examples/classification/run_wrapper.py\n# Excerpt from _get_command() function showing calculation of batch size and epochs, and command line arguments\ndef _get_command(\n    task_name,\n    output_dir,\n    model_name_or_path,\n    data_dir,\n    learning_rate,\n    clipping_mode: str,\n    non_private,\n    target_epsilon,\n    few_shot_type,\n    seed,\n    attention_only,\n    static_lm_head,\n    static_embedding,\n    randomly_initialize,\n    per_device_train_batch_size,\n    batch_size,\n    num_train_epochs,\n    eval_steps,\n    eval_spectrum,\n    max_spectrum_batches,\n    max_lanczos_iter,\n    store_grads,\n    orthogonal_projection_path,\n    orthogonal_projection_rank,\n):\n    task_name_to_factor = {\n        \"sst-2\": 1, \"qnli\": 2, \"qqp\": 6, \"mnli\": 6,\n    }\n    factor = task_name_to_factor[task_name]\n\n    if batch_size is None:\n        base_batch_size = 1000\n        # This batch size selection roughly ensures the sampling rates on different\n        # datasets are in the same ballpark.\n        batch_size = int(base_batch_size * factor)\n    gradient_accumulation_steps = batch_size // per_device_train_batch_size\n\n    if num_train_epochs is None:\n        base_num_train_epochs = 3\n        num_train_epochs = int(base_num_train_epochs * factor)\n\n    if learning_rate is None:\n        if non_private.lower() in ('yes', 'y', 'true', 't'):\n            learning_rate = 5e-5\n        else:\n            learning_rate = 5e-4\n\n    data_dir = f\"{data_dir}/{common.task_name2suffix_name[task_name]}\"\n    template = {\n        \"sst-2\": \"*cls**sent_0*_It_was*mask*.*sep+*\",\n        \"mnli\": \"*cls**sent-_0*?*mask*,*+sentl_1**sep+*\",\n        \"qnli\": \"*cls**sent-_0*?*mask*,*+sentl_1**sep+*\",\n        \"qqp\": \"*cls**sent-_0**mask*,*+sentl_1**sep+*\",\n    }[task_name]\n\n    # Epochs chosen roughly to match e2e number of updates. We didn't hyperparameter tune on classification tasks :)\n    cmd = f'''\npython -m classification.run_classification \\\n  --task_name {task_name} \\\n  --data_dir {data_dir} \\\n  --output_dir {output_dir} \\\n  --overwrite_output_dir \\\n  --model_name_or_path {model_name_or_path} \\\n  --few_shot_type {few_shot_type} \\\n  --num_k 1 \\\n  --num_sample 1 --seed {seed} \\\n  --template {template} \\\n  --non_private {non_private} \\\n  --num_train_epochs {num_train_epochs} \\\n  --target_epsilon {target_epsilon} \\\n  --per_device_train_batch_size {per_device_train_batch_size} \\\n  --gradient_accumulation_steps {gradient_accumulation_steps} \\\n  --per_device_eval_batch_size 8 \\\n  --per_example_max_grad_norm 0.1 --clipping_mode {clipping_mode} \\\n  --learning_rate {learning_rate} \\\n  --lr_decay yes \\\n  --adam_epsilon 1e-08 \\\n  --weight_decay 0 \\\n  --max_seq_len 256 \\\n  --evaluation_strategy steps --eval_steps {eval_steps} --evaluate_before_training True \\\n  --do_train --do_eval \\\n  --first_sent_limit 200 --other_sent_limit 200 --truncate_head yes \\\n  --attention_only {attention_only} --static_lm_head {static_lm_head} --static_embedding {static_embedding} \\\n  --randomly_initialize {randomly_initialize} \\\n  --eval_spectrum {eval_spectrum} --max_spectrum_batches {max_spectrum_batches} --max_lanczos_iter {max_lanczos_iter} \\\n  --store_grads {store_grads}'''\n    if orthogonal_projection_path is not None:\n        cmd += f' --orthogonal_projection_path {orthogonal_projection_path}'\n        cmd += f' --orthogonal_projection_rank {orthogonal_projection_rank}'\n    return cmd\n",
    "Experiment Result": "Task: SST-2\nPrivacy Budget Epsilon (ε): 8\nPrivacy Budget Delta (δ): Calculated by PrivacyEngine as len(train_dataset)**-1.1\nNoise Multiplier: Dynamically calculated by PrivacyEngine based on target_epsilon, target_delta, sample_rate, and epochs.\nAccounting Mode: rdp (Renyi Differential Privacy)\nClipping Mode: default (global fixed clipping)\nPer-Example Max Gradient Norm (C): 0.1\nEffective Batch Size: 1000 (per_device_train_batch_size = 20, gradient_accumulation_steps = 50)\nLearning Rate (η): 5e-4\nNumber of Training Epochs (T): 3\nOptimizer: AdamW\nAdam Beta1 (Momentum ρ): 0.9\nModel Initialization: Not randomly initialized (uses pre-trained weights)."
}{
    "Title": "Efficient Hyperparameter Optimization with Adaptive Fidelity Identification",
    "Main Contributions": "The paper proposes FastBO, a multi-fidelity Bayesian optimization (BO) method designed to tackle the challenge of adaptively determining the appropriate fidelity for each hyperparameter configuration when fitting the surrogate model in HPO and NAS. FastBO introduces the novel concepts of 'efficient point' and 'saturation point' for each configuration, enabling it to efficiently achieve strong performance. A significant contribution is also demonstrating that this adaptive fidelity identification strategy can extend any single-fidelity method to the multi-fidelity setting, highlighting its generality.",
    "Methodology": "FastBO extends Bayesian Optimization (BO) by adaptively identifying fidelity for configurations. It defines two key concepts: the 'efficient point' (ei), which is the resource level beyond which doubling resources yields performance improvement below a small threshold (δ1), signifying optimal resource-to-performance balance for surrogate model fitting; and the 'saturation point' (si), where performance stabilizes, meaning further resources yield variations below a small threshold (δ2), approximating final fidelity. The FastBO process involves a warm-up stage for early observations and termination of poor configurations, learning curve estimation from observations, adaptive extraction of efficient and saturation points, evaluation of configurations up to their efficient points to update the surrogate model, and a post-processing stage where promising configurations are evaluated to their saturation points to find the optimal solution. This adaptive fidelity identification strategy can generalize to extend single-fidelity methods by using efficient point performances.",
    "Experimental Setup": "FastBO's performance was evaluated against several baseline and state-of-the-art methods, including random search (RS), standard BO, ASHA, Hyperband, PASHA, A-BOHB, A-CQR, BOHB, DyHPO, and Hyper-Tune. The experiments were conducted on three well-known benchmarks: LCBench, NAS-Bench-201, and FCNet. The validation focused on 'anytime performance,' indicating how quickly a method converges to the global optimum.",
    "Limitations": "The paper implies challenges that demand future improvements, particularly concerning the applicability and scalability of FastBO. While the method provides a strong foundation, it is recognized that future work is needed to refine and expand FastBO, especially when dealing with larger search spaces and integrating with distributed computing systems.",
    "Future Research Directions": "Future research should focus on refining and expanding FastBO to address larger search spaces and integrate it with distributed computing systems. These efforts aim to improve its overall applicability and scalability, making it more robust and practical for complex, resource-intensive HPO and NAS tasks.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization",
    "Main Contributions": "The paper addresses the open problem of Hyperparameter Optimization (HPO) in Differentially Private Deep Learning, which traditionally incurs high privacy and computational costs. It proposes an adaptive HPO method, the 'new linear scaling rule', that uses low-cost privacy trials to estimate optimal hyperparameters and scales them up linearly for larger privacy budgets. This method significantly reduces computation and privacy cost for HPO by an order of magnitude without sacrificing performance, achieving state-of-the-art results on 22 benchmark tasks across computer vision and natural language processing, pretraining and finetuning, and various architectures for a wide range of privacy budgets ε ∈ [0.01, 8.0]. The approach also accounts for the privacy cost of HPO in its final privacy guarantee and demonstrates robustness to distribution shifts.",
    "Methodology": "The core methodology is the 'new linear scaling rule'. It first estimates optimal hyperparameters (HPs) for small privacy budgets (ε) using a limited number of trials. Then, it scales these HPs linearly to larger target privacy budgets. This is formalized by reducing the HPO dimensionality: the key hyperparameters (learning rate η and number of training steps T) are combined into a single scalar variable `r = η × T`, which is found to scale linearly with ε. A first-order Taylor approximation, fitted by a line using two empirically sampled points (ε1, r(ε1)) and (ε2, r(ε2)), is used to estimate r for any desired target ε. The overall privacy guarantee, including the cost of HPO, is calculated using Privacy Loss Variable (PLV) accounting (f-DP). The DP-SGD training employs full-batch gradients, per-sample unit norm clipping (C=1), zero initialization for the linear classifier, and momentum (ρ=0.9) to maximize the signal-to-noise ratio and accelerate convergence.",
    "Experimental Setup": "The method was evaluated on 22 diverse tasks, encompassing computer vision and natural language processing, pretraining and finetuning, and models ranging from ResNets to multi-billion-parameter Transformers. Computer vision datasets include ImageNet, CIFAR10, CIFAR100, FashionMNIST, STL10, EMNIST, and distribution shift datasets like CIFAR10 → STL, CIFAR10p1, CIFAR10C, CIFAR100 → CIFAR100C, Waterbirds, FMoW, and Camelyon17. NLP tasks include SQuAD for Question Answering, GLUE benchmark tasks (SST-2, QNLI, QQP, MNLI(m/mm)) for text classification, and PersonaChat, WikiText-2, and Enron Emails for next word generation. Architectures tested include beit, beitv2, convnext, ViT-L for CV, and GPT-2, RoBERTa-base for NLP. Performance was measured using accuracy for classification and perplexity for language modeling, across a wide range of privacy budgets ε ∈ [0.01, 8.0]. The method was compared against random search, grid search (without HPO cost), and three prior Rényi DP-based private HPO methods, as well as parameter-free methods like DPAdamWOSM and other DP-SGD approaches. Ablation studies were conducted on various design choices (initialization, batch size, optimizer, accounting, clipping norm, momentum, weight averaging, data augmentation, weight decay) and polynomial approximation orders for the linear scaling rule.",
    "Limitations": "The theoretical analysis relies on assumptions (e.g., strong convexity, smoothness, Lipschitzness, bounded gradients) that do not generally hold for training complex neural networks, although the heuristic's empirical success is demonstrated. The HPO method requires more runtime than random search due to its adaptive nature. It also has worse parallelization than grid search, as it requires serial runs (e.g., εf runs after ε1 and ε2). The performance can be suboptimal for very small ε values on difficult datasets like ImageNet, where low-budget HP trials may not provide sufficient information. The method's effectiveness was observed to degrade when optimizing for additional hyperparameters (like batch size and clipping threshold) beyond η and T, suggesting `r = η × T` is the most effective simplification.",
    "Future Research Directions": "Future work could incorporate additional factors into the theoretical analysis, such as momentum acceleration, bias introduced by clipping, or extend the analysis to more general neural networks. An interesting question is whether RDP analysis can be applied to this adaptive method. The authors suggest that their HPO method could be used to tune the number of epochs in parameter-free optimizers like DPAdamWOSM. Research could also explore privatizing non-private HPO methods. Additionally, applying this method to alternate threat models where a small percentage of private training data and public data are used to improve DP-SGD could be explored. The study also opens up possibilities for achieving lossless privacy on more challenging datasets like CIFAR100 as pretrained models improve. Further investigation into the correlation between performance on half-datasets and full datasets, particularly with proper calibration for the number of classes, is also a potential direction.",
    "Experiment Code": "import torchfrom torch import nnfrom private_transformers import PrivacyEnginefrom private_transformers.settings import BackwardHookModeimport private_transformers.autograd_grad_sample as autograd_grad_sample# Excerpt from examples/classification/run_classification.py# Optimizer initializationoptimizer_grouped_parameters = [{'params': [p for n, p in named_params if not any(nd in n for nd in no_decay)], 'weight_decay': training_args.weight_decay},{'params': [p for n, p in named_params if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]optimizer = trainer.optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=training_args.learning_rate, betas=(training_args.adam_beta1, training_args.adam_beta2), eps=training_args.adam_epsilon,)if training_args.lr_decay: # Default linear decay.    training_setup = trainer.get_training_setup()    t_total = training_setup[\"t_total\"]    trainer.create_optimizer_and_scheduler(num_training_steps=t_total)else:    trainer.lr_scheduler = torch.optim.lr_scheduler.LambdaLR(trainer.optimizer, lambda _: 1.)# PrivacyEngine instantiation and attachmentif privacy_args.non_private:    privacy_args.noise_multiplier = 0.    privacy_args.per_example_max_grad_norm = Noneelse:    total_train_batch_size = training_args.gradient_accumulation_steps * training_args.per_device_train_batch_size    privacy_engine = PrivacyEngine(        module=model,        batch_size=total_train_batch_size,        sample_size=len(train_dataset),        epochs=training_args.num_train_epochs,        max_grad_norm=privacy_args.per_example_max_grad_norm,        noise_multiplier=privacy_args.noise_multiplier,        target_epsilon=privacy_args.target_epsilon,        target_delta=privacy_args.target_delta,        accounting_mode=privacy_args.accounting_mode,        clipping_mode=privacy_args.clipping_mode,        skip_checks=True,    )    privacy_args.noise_multiplier = privacy_engine.noise_multiplier    privacy_args.target_delta = privacy_engine.target_delta    privacy_engine.attach(optimizer)# Excerpt from private_transformers/privacy_engine.py - Core PrivacyEngine methods for DP-SGDclass PrivacyEngine(object):    def _create_noisy_clipped_gradient(self):        signals, noises = [], []        for name, param in self.named_params:            # param.grad is param.summed_grad (from previous steps)            param.grad = param.summed_grad            if self.record_snr:                signals.append(param.grad.reshape(-1).norm(2))            if self.noise_multiplier > 0 and self.max_grad_norm > 0:                noise = torch.normal(                    mean=0,                    std=self.noise_multiplier * self.max_grad_norm,                    size=param.size(),                    device=param.device,                    dtype=param.dtype,                )                param.grad += noise                if self.record_snr:                    noises.append(noise.reshape(-1).norm(2))                del noise            param.grad /= self.batch_size # Scale by inverse batch size    # Ghost Clipping implementation (default in run_wrapper.py)    def _ghost_step(self, loss: torch.Tensor):        if self._locked:            return        self._ghost_virtual_step(loss)        self._create_noisy_clipped_gradient()    @torch.no_grad()    def _ghost_virtual_step(self, loss: torch.Tensor):        self._double_backward(loss)        for name, param in self.named_params:            if hasattr(param, 'summed_grad'):                param.summed_grad += param.grad            else:                param.summed_grad = param.grad            if hasattr(param, \"grad\"):\n                del param.grad\n            if hasattr(param, \"norm_sample\"):\n                del param.norm_sample\n            if hasattr(param, \"grad_sample\"):\n                del param.grad_sample\n    @torch.enable_grad()\n    def _double_backward(self, loss: torch.Tensor):\n        first_loss = loss.sum()\n        first_loss.backward(retain_graph=True)\n        autograd_grad_sample.set_hooks_mode(BackwardHookMode.ghost_grad)\n        for name, param in self.named_params:\n            if hasattr(param, \"grad\"):\n                del param.grad\n        coef_sample = self.get_coef_sample()\n        second_loss = (coef_sample * loss).sum(dim=0)\n        second_loss.backward()\n        autograd_grad_sample.set_hooks_mode(BackwardHookMode.ghost_norm)\n    def get_coef_sample(self) -> torch.Tensor:\n        norm_sample = self.get_norm_sample()\n        return torch.clamp_max(self.max_grad_norm / (norm_sample + self.numerical_stability_constant), 1.)\n    def get_norm_sample(self) -> torch.Tensor:\n        norm_sample = torch.stack([param.norm_sample for name, param in self.named_params], dim=0).norm(2, dim=0)\n        return norm_sample\n# Excerpt from examples/classification/run_wrapper.py - Hyperparameter setting logic\ndef _get_command(task_name,output_dir,model_name_or_path,data_dir,learning_rate,clipping_mode: str,non_private,target_epsilon,few_shot_type,seed,attention_only,static_lm_head,static_embedding,randomly_initialize,per_device_train_batch_size,batch_size,num_train_epochs,eval_steps,eval_spectrum,max_spectrum_batches,max_lanczos_iter,store_grads,orthogonal_projection_path,orthogonal_projection_rank,):\n    task_name_to_factor = {\"sst-2\": 1, \"qnli\": 2, \"qqp\": 6, \"mnli\": 6,}\n    factor = task_name_to_factor[task_name]\n    if batch_size is None:\n        base_batch_size = 1000\n        batch_size = int(base_batch_size * factor)\n    gradient_accumulation_steps = batch_size // per_device_train_batch_size\n    if num_train_epochs is None:\n        base_num_train_epochs = 3\n        num_train_epochs = int(base_num_train_epochs * factor)\n    if learning_rate is None:\n        if non_private.lower() in ('yes', 'y', 'true', 't'):\n            learning_rate = 5e-5\n        else:\n            learning_rate = 5e-4\n    cmd = f'''python -m classification.run_classification \\n  --task_name {task_name} \\n  --data_dir {data_dir} \\n  --output_dir {output_dir} \\n  --overwrite_output_dir \\n  --model_name_or_path {model_name_or_path} \\n  --few_shot_type {few_shot_type} \\n  --num_k 1 --num_sample 1 --seed {seed} \\n  --template {template} \\n  --non_private {non_private} \\n  --num_train_epochs {num_train_epochs} \\n  --target_epsilon {target_epsilon} \\n  --per_device_train_batch_size {per_device_train_batch_size} \\n  --gradient_accumulation_steps {gradient_accumulation_steps} \\n  --per_device_eval_batch_size 8 \\n  --per_example_max_grad_norm 0.1 --clipping_mode {clipping_mode} \\n  --learning_rate {learning_rate} \\n  --lr_decay yes \\n  --adam_epsilon 1e-08 \\n  --weight_decay 0 \\n  --max_seq_len 256 \\n  --evaluation_strategy steps --eval_steps {eval_steps} --evaluate_before_training True \\n  --do_train --do_eval \\n  --first_sent_limit 200 --other_sent_limit 200 --truncate_head yes \\n  --attention_only {attention_only} --static_lm_head {static_lm_head} --static_embedding {static_embedding} \\n  --randomly_initialize {randomly_initialize} \\n  --eval_spectrum {eval_spectrum} --max_spectrum_batches {max_spectrum_batches} --max_lanczos_iter {max_lanczos_iter} \\n  --store_grads {store_grads}'''\n    return cmd",
    "Experiment Result": "The core methodology, 'new linear scaling rule', is applied by setting specific hyperparameters during training. The experimental settings related to DP-SGD and hyperparameter tuning are as follows:- **Clipping Mode**: 'ghost' (for per-sample unit norm clipping).- **Per-sample Gradient Clipping Norm (C)**: 0.1.- **Target Privacy Budget (ε)**: 8.- **Target Delta (δ)**: Defaults to `sample_size ** -1.1` within `PrivacyEngine`.- **Privacy Accounting Mode**: 'rdp' (Renyi Differential Privacy, converted to f-DP).- **Learning Rate (η)**: 5e-4 (for private training); 5e-5 (for non-private training).- **Number of Training Epochs**: This is derived from `num_train_epochs = base_num_train_epochs * factor`. `base_num_train_epochs = 3`. `task_name_to_factor` mapping: 'sst-2': 1, 'qnli': 2, 'qqp': 6, 'mnli': 6. For 'sst-2', number of training epochs is 3. For 'mnli', it is 18.- **Effective Batch Size for DP-SGD Updates**: `batch_size = base_batch_size * factor`, where `base_batch_size = 1000`. `per_device_train_batch_size = 20`. `gradient_accumulation_steps = batch_size // per_device_train_batch_size`. This `batch_size` parameter in `PrivacyEngine` specifies the effective lot size for DP-SGD steps, not necessarily the full dataset size.- **Momentum (ρ)**: 0.9 (corresponding to `adam_beta1` in AdamW optimizer, default in `transformers.TrainingArguments`).- **Zero Initialization for Linear Classifier**: The `model.init_weights()` method is called after loading the pre-trained model and before training, which typically re-initializes task-specific layers like the classification head (`lm_head` in prompt-finetuning models).- **Gradient Decay**: Linear decay is applied by default (`lr_decay=yes`).- **Weight Decay**: 0 (specified in `run_wrapper.py`).- **Adam Epsilon**: 1e-08 (specified in `run_wrapper.py`).- **Other Settings**: `max_seq_len=256`, `first_sent_limit=200`, `other_sent_limit=200`, `truncate_head=yes`."
}{
    "Title": "Bayesian Optimization for Iterative Learning",
    "Main Contributions": "The paper addresses the challenge of expensive hyperparameter tuning in deep (reinforcement) learning by proposing Bayesian Optimization for Iterative Learning (BOIL). It introduces a novel approach to exploit the iterative training structure by compressing the entire learning curve into a single numeric score, which accounts for both training success and stability. The method also includes a data augmentation technique that selectively incorporates intermediate training steps to enhance sample efficiency and mitigate Gaussian Process (GP) covariance matrix ill-conditioning. BOIL is demonstrated to outperform existing baselines in identifying optimal hyperparameters in minimal wall-clock time for DRL agents and convolutional neural networks.",
    "Methodology": "BOIL employs a Bayesian Optimization (BO) framework with a Gaussian Process (GP) surrogate model that spans the joint space of input hyperparameters (x) and training iterations (t). A product kernel `k([x,t],[x',t']) = k(x,x') × k(t,t')` is used, and the training time cost `c(x,t)` is approximated by a linear regressor. The acquisition function is a modified Expected Improvement (EI), which is optimized against cost (`α(x,t)/µc(x,t)`). A key technique is \"training curve compression\" where raw learning curves are transformed into a numeric utility score using a Sigmoid (Logistic) preference function, whose parameters (`m0`, `g0`) are learned by maximizing the GP log marginal likelihood. Furthermore, a data augmentation technique is utilized to selectively add points from the learning curve, choosing samples at the maximum of GP predictive uncertainty while ensuring the natural log of the GP covariance matrix condition number remains below a set threshold (δ).",
    "Experimental Setup": "The algorithm was evaluated on tuning hyperparameters for two Deep Reinforcement Learning (DRL) agents and a Convolutional Neural Network (CNN). DRL experiments involved a Dueling DQN agent on the CartPole-v0 environment and Advantage Actor Critic (A2C) agents on InvertedPendulum-v2 and Reacher-v2. CNN experiments were conducted on the SVHN and CIFAR10 datasets. All results were averaged over 20 independent runs, and final performance was estimated at the maximum number of iterations. The experiments were executed on an NVIDIA 1080 GTX GPU using the TensorFlow-GPU Python package, with DRL implementations based on OpenAI Baselines. The GP model used square-exponential kernels, a maximum of 15 augmented points, and a threshold (δ) of 20 for the natural log of the GP condition number. Baselines included Hyperband, Continuous Multi-Task/Fidelity BO (CM-T/F-BO), vanilla BO, and BO with curve compression (BO-L).",
    "Limitations": "The paper implies several limitations, including the simplifying assumption of a linear regressor for approximating the training cost function `c(x,t)`, noting that a more complex parametric model or a second GP might be more appropriate for intricate cost dependencies. A user-defined threshold for the GP condition number (δ) is required for the data augmentation technique, which introduces a manual tuning aspect. While BOIL addresses the limitations of baselines like Hyperband in noisy DRL settings, the broader impact section discusses potential societal concerns with increasing automation in ML, such as reduced human oversight leading to critical failures, increased model opacity, and the necessity to address biases in data and models, which are relevant to any automated tuning framework including BOIL.",
    "Future Research Directions": "Future research directions include extending the iteration-efficient optimization framework to any iterative process beyond machine learning, such as optimizing manufacturing pipelines by adjusting factory settings to increase productivity. Additionally, the authors suggest integrating their work with the growing body of research on the interpretability of machine learning models to ensure rigorous analysis of final training outcomes. This integration aims to prevent machine learning algorithms from becoming opaque and unquestioned entities, thereby ensuring they remain a beneficial source of information for real-world policy making.",
    "Experiment Code": "File Path: bayes_opt/acquisition_functions.py\nContent:\nimport numpy as np\nfrom scipy.stats import norm\n\n\ncounter = 0\n\n\nclass AcquisitionFunction(object):\n    \"\"\"\n    An object to compute the acquisition functions.\n    \"\"\"\n\n    def __init__(self, acq):\n\n        self.acq=acq\n        acq_name=acq['name']\n        \n        if 'mu_max' in acq:\n            self.mu_max=acq['mu_max'] # this is for ei_mu acquisition function\n        \n        ListAcq=['bucb','ucb', 'ei','poi','random','ucb_pe',\n                 'pure_exploration','mu','lcb','ei_mu_max'                          ]\n        \n        # check valid acquisition function\n        IsTrue=[val for idx,val in enumerate(ListAcq) if val in acq_name]\n        #if  not in acq_name:\n        if  IsTrue == []:\n            err = \"The utility function \" \\\n                  \"{} has not been implemented, \" \\\n                  \"please choose one of ucb, ei, or poi.\".format(acq_name)\n            raise NotImplementedError(err)\n        else:\n            self.acq_name = acq_name\n            \n        self.dim=acq['dim']\n        \n        if 'scalebounds' not in acq:\n            self.scalebounds=[0,1]*self.dim\n            \n        else:\n            self.scalebounds=acq['scalebounds']\n               \n\n    def acq_kind(self, x, gp):\n        \n        #if type(meta) is dict and 'y_max' in meta.keys():\n        #   y_max=meta['y_max']\n        y_max=np.max(gp.Y)\n        #print self.kind\n        if np.any(np.isnan(x)):\n            return 0\n       \n        if self.acq_name == 'ucb':\n            return self._ucb(x, gp)\n        if self.acq_name == 'lcb':\n            return self._lcb(x, gp)\n        if self.acq_name == 'ei':\n            return self._ei(x, gp, y_max)\n        if self.acq_name == 'ei_mu_max': # using max mu(x) as incumbent\n            return self._ei(x, gp, self.mu_max)\n        if self.acq_name == 'poi':\n            return self._poi(x, gp, y_max)\n        \n        if self.acq_name == 'pure_exploration':\n            return self._pure_exploration(x, gp) \n      \n        if self.acq_name == 'mu':\n            return self._mu(x, gp)\n        \n        if self.acq_name == 'ucb_pe':\n            return self._ucb_pe(x, gp,self.acq['kappa'],self.acq['maxlcb'])\n       \n            \n    def utility_plot(self, x, gp, y_max):\n        if np.any(np.isnan(x)):\n            return 0\n        if self.acq_name == 'ei':\n            return self._ei_plot(x, gp, y_max)\n  \n   \n    @staticmethod\n    def _mu(x, gp):\n        mean, var = gp.predict(x, eval_MSE=True)\n        mean=np.atleast_2d(mean).T\n        return mean\n                \n    @staticmethod\n    def _lcb(x, gp):\n        mean, var = gp.predict(x, eval_MSE=True)\n        var.flags['WRITEABLE']=True\n        #var=var.copy()\n        var[var<1e-10]=0\n        mean=np.atleast_2d(mean).T\n        var=np.atleast_2d(var).T\n        #beta_t = gp.X.shape[1] * np.log(len(gp.Y))\n        beta_t = 2 * np.log(len(gp.Y));\n\n        return mean - np.sqrt(beta_t) * np.sqrt(var) \n        \n    \n    @staticmethod\n    def _ucb(x, gp):\n        mean, var = gp.predict(x, eval_MSE=True)\n        var.flags['WRITEABLE']=True\n        #var=var.copy()\n        var[var<1e-10]=0\n        mean=np.atleast_2d(mean).T\n        var=np.atleast_2d(var).T                \n        \n        # Linear in D, log in t https://github.com/kirthevasank/add-gp-bandits/blob/master/BOLibkky/getUCBUtility.m\n        #beta_t = gp.X.shape[1] * np.log(len(gp.Y))\n        beta_t = 2 * np.log(len(gp.Y));\n  \n        #beta=300*0.1*np.log(5*len(gp.Y))# delta=0.2, gamma_t=0.1\n        return mean + np.sqrt(beta_t) * np.sqrt(var) \n    \n    \n    @staticmethod\n    def _ucb_pe(x, gp, kappa, maxlcb):\n        mean, var = gp.predict_bucb(x, eval_MSE=True)\n        var.flags['WRITEABLE']=True\n        var[var<1e-10]=0\n        mean=np.atleast_2d(mean).T\n        var=np.atleast_2d(var).T\n\n        value=mean + kappa * np.sqrt(var)        \n        myidx=[idx for idx,val in enumerate(value) if val<maxlcb]\n        var[myidx]=0        \n        return var\n    \n   \n    @staticmethod\n    def _pure_exploration(x, gp):\n        mean, var = gp.predict(x, eval_MSE=True)\n        var.flags['WRITEABLE']=True\n        #var=var.copy()\n        var[var<1e-10]=0\n        mean=np.atleast_2d(mean).T\n        var=np.atleast_2d(var).T\n        return np.sqrt(var)\n        \n   \n    @staticmethod\n    def _ei(x, gp, y_max):\n        y_max=np.asscalar(y_max)\n        mean, var = gp.predict(x, eval_MSE=True)\n        var2 = np.maximum(var, 1e-10 + 0 * var)\n        z = (mean - y_max)/np.sqrt(var2)        \n        out=(mean - y_max) * norm.cdf(z) + np.sqrt(var2) * norm.pdf(z)\n        \n        out[var2<1e-10]=0\n        return out\n \n \n    @staticmethod      \n    def _poi(x, gp,y_max): # run Predictive Entropy Search using Spearmint\n        mean, var = gp.predict(x, eval_MSE=True)    \n        # Avoid points with zero variance\n        var = np.maximum(var, 1e-9 + 0 * var)\n        z = (mean - y_max)/np.sqrt(var)        \n        return norm.cdf(z)\n\n   \ndef unique_rows(a):\n    \"\"\"\n    A functions to trim repeated rows that may appear when optimizing.\n    This is necessary to avoid the sklearn GP object from breaking\n\n    :param a: array to trim repeated rows from\n\n    :return: mask of unique rows\n    \"\"\"\n\n    # Sort array and kep track of where things should go back to\n    order = np.lexsort(a.T)\n    reorder = np.argsort(order)\n\n    a = a[order]\n    diff = np.diff(a, axis=0)\n    ui = np.ones(len(a), 'bool')\n    ui[1:] = (diff != 0).any(axis=1)\n\n    return ui[reorder]\n\n\n\nclass BColours(object):\n    BLUE = '\\033[94m'\n    CYAN = '\\033[36m'\n    GREEN = '\\033[32m'\n    MAGENTA = '\\033[35m'\n    RED = '\\033[31m'\n    ENDC = '\\033[0m'\nFile Path: bayes_opt/acquisition_maximization.py\nContent:\n# -*- coding: utf-8 -*-\n\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom bayes_opt.acquisition_functions import AcquisitionFunction\nimport sobol_seq\n\n__author__ = 'Vu'\n\n\ndef acq_max_with_name(gp,scaleSearchSpace,acq_name=\"ei\",IsReturnY=False,IsMax=True,fstar_scaled=None):\n    acq={}\n    acq['name']=acq_name\n    acq['dim']=scaleSearchSpace.shape[0]\n    acq['scaleSearchSpace']=scaleSearchSpace   \n    if fstar_scaled:\n        acq['fstar_scaled']=fstar_scaled   \n\n    myacq=AcquisitionFunction(acq)\n    if IsMax:\n        x_max = acq_max(ac=myacq.acq_kind,gp=gp,bounds=scaleSearchSpace,opt_toolbox='scipy')\n    else:\n        x_max = acq_min_scipy(ac=myacq.acq_kind,gp=gp,bounds=scaleSearchSpace)\n    if IsReturnY==True:\n        y_max=myacq.acq_kind(x_max,gp=gp)\n        return x_max,y_max\n    return x_max\n\ndef acq_max(ac, gp, bounds, opt_toolbox='scipy',seeds=[],IsMax=True):\n    \"\"\"\n    A function to find the maximum of the acquisition function using\n    the scipy python\n\n    Input Parameters\n    ----------\n    ac: The acquisition function object that return its point-wise value.\n    gp: A gaussian process fitted to the relevant data.\n    y_max: The current maximum known value of the target function.\n    bounds: The variables bounds to limit the search of the acq max.\n    \n    Returns\n    -------\n    x_max, The arg max of the acquisition function.\n    \"\"\"\n    y_max=np.max(gp.Y)\n  \n    x_max = acq_max_scipy(ac=ac,gp=gp,y_max=y_max,bounds=bounds)\n\n    return x_max\n\ndef generate_sobol_seq(dim,nSobol):\n    mysobol_seq = sobol_seq.i4_sobol_generate(dim, nSobol)\n    return mysobol_seq\n    \n\ndef acq_min_scipy_kwargs(myfunc, SearchSpace, **kwargs):\n    \"\"\"\n    A function to find the maximum of the acquisition function using\n    the scipy python\n\n    Input Parameters\n    ----------\n    ac: The acquisition function object that return its point-wise value.\n    gp: A gaussian process fitted to the relevant data.\n    y_max: The current maximum known value of the target function.\n    bounds: The variables bounds to limit the search of the acq max.\n    \n    Returns\n    -------\n    x_max, The arg max of the acquisition function.\n    \"\"\"\n    dim=SearchSpace.shape[0]\n    # Start with the lower bound as the argmax\n    x_max = SearchSpace[:, 0]\n    min_acq = None\n\n    #myopts ={'maxiter':2000,'fatol':0.01,'xatol':0.01}\n    myopts ={'maxiter':10*dim,'maxfun':20*dim}\n    #myopts ={'maxiter':5*dim}\n\n    #sobol_sequence=generate_sobol_seq(dim=dim,nSobol=500*dim)\n\n    # multi start\n    for i in range(3*dim):\n        # Find the minimum of minus the acquisition function        \n        x_tries = np.random.uniform(SearchSpace[:, 0], SearchSpace[:, 1],size=(100*dim, dim))\n        \n        #x_tries=sobol_sequence\n    \n        # evaluate\n        y_tries=myfunc(x_tries,**kwargs)\n        \n        #find x optimal for init\n        idx_min=np.argmin(y_tries)\n\n        x_init_min=x_tries[idx_min]\n    \n        res = minimize(lambda x: myfunc(x.reshape(1, -1), **kwargs),x_init_min.reshape(1, -1),bounds=SearchSpace,\n                       method=\"L-BFGS-B\",options=myopts)#L-BFGS-B\n\n        if 'x' not in res:\n            val=myfunc(res,**kwargs)        \n        else:\n            val=myfunc(res.x,**kwargs) \n        \n        # Store it if better than previous minimum(maximum).\n        if min_acq is None or val <= min_acq:\n            if 'x' not in res:\n                x_max = res\n            else:\n                x_max = res.x\n            min_acq = val\n            #print max_acq\n\n    return np.clip(x_max, SearchSpace[:, 0], SearchSpace[:, 1])\n\n    \ndef acq_min_scipy(ac, gp, bounds):\n    \"\"\"\n    A function to find the maximum of the acquisition function using\n    the scipy python\n\n    Input Parameters\n    ----------\n    ac: The acquisition function object that return its point-wise value.\n    gp: A gaussian process fitted to the relevant data.\n    y_max: The current maximum known value of the target function.\n    bounds: The variables bounds to limit the search of the acq max.\n    \n    Returns\n    -------\n    x_max, The arg max of the acquisition function.\n    \"\"\"\n\n    dim=bounds.shape[0]\n    # Start with the lower bound as the argmax\n    x_max = bounds[:, 0]\n    min_acq = None\n\n    #myopts ={'maxiter':2000,'fatol':0.01,'xatol':0.01}\n    myopts ={'maxiter':10*dim,'maxfun':20*dim}\n    #myopts ={'maxiter':5*dim}\n\n    # multi start\n    for i in range(3*dim):\n        # Find the minimum of minus the acquisition function        \n        x_tries = np.random.uniform(bounds[:, 0], bounds[:, 1],size=(50*dim, dim))\n    \n        # evaluate\n        y_tries=ac(x_tries,gp=gp)\n        \n        #find x optimal for init\n        idx_max=np.argmin(y_tries)\n\n        x_init_max=x_tries[idx_max]\n        \n    \n        res = minimize(lambda x: ac(x.reshape(1, -1), gp=gp),x_init_max.reshape(1, -1),bounds=bounds,\n                       method=\"L-BFGS-B\",options=myopts)#L-BFGS-B\n      \n        if 'x' not in res:\n            val=ac(res,gp)        \n        else:\n            val=ac(res.x,gp) \n        \n        # Store it if better than previous minimum(maximum).\n        if min_acq is None or val <= min_acq:\n            if 'x' not in res:\n                x_max = res\n            else:\n                x_max = res.x\n            min_acq = val\n            #print max_acq\n\n    return np.clip(x_max, bounds[:, 0], bounds[:, 1])\n    \n    \ndef acq_max_scipy(ac, gp, y_max, bounds):\n    \"\"\"\n    A function to find the maximum of the acquisition function using\n    the scipy python\n\n    Input Parameters\n    ----------\n    ac: The acquisition function object that return its point-wise value.\n    gp: A gaussian process fitted to the relevant data.\n    y_max: The current maximum known value of the target function.\n    bounds: The variables bounds to limit the search of the acq max.\n    \n    Returns\n    -------\n    x_max, The arg max of the acquisition function.\n    \"\"\"\n\n    dim=bounds.shape[0]\n    # Start with the lower bound as the argmax\n    x_max = bounds[:, 0]\n    max_acq = None\n\n    myopts ={'maxiter':10*dim,'maxfun':20*dim}\n    #myopts ={'maxiter':5*dim}\n\n\n    # multi start\n    for i in range(1*dim):\n        # Find the minimum of minus the acquisition function        \n        x_tries = np.random.uniform(bounds[:, 0], bounds[:, 1],size=(50*dim, dim))\n    \n        # evaluate\n        y_tries=ac(x_tries,gp=gp)\n        #print \"elapse evaluate={:.5f}\".format(end_eval-start_eval)\n        \n        #find x optimal for init\n        idx_max=np.argmax(y_tries)\n        #print \"max y_tries {:.5f} y_max={:.3f}\".format(np.max(y_tries),y_max)\n\n        x_init_max=x_tries[idx_max]\n        \n    \n        res = minimize(lambda x: -ac(x.reshape(1, -1), gp=gp),x_init_max.reshape(1, -1),bounds=bounds,\n                       method=\"L-BFGS-B\",options=myopts)#L-BFGS-B\n\n\n        \n        if 'x' not in res:\n            val=ac(res,gp)        \n        else:\n            val=ac(res.x,gp) \n\n        # Store it if better than previous minimum(maximum).\n        if max_acq is None or val >= max_acq:\n            if 'x' not in res:\n                x_max = res\n            else:\n                x_max = res.x\n            max_acq = val\n            #print max_acq\n\n    # Clip output to make sure it lies within the bounds. Due to floating\n    # point technicalities this is not always the case.\n    #return np.clip(x_max[0], bounds[:, 0], bounds[:, 1])\n        #print max_acq\n    return np.clip(x_max, bounds[:, 0], bounds[:, 1])\n    \n    # COBYLA -> x_max[0]\n    # L-BFGS-B -> x_max\n\n    \ndef acq_max_with_init(ac, gp, y_max, bounds, init_location=[]):\n    \"\"\"\n    A function to find the maximum of the acquisition function using\n    the scipy python\n\n    Input Parameters\n    ----------\n    ac: The acquisition function object that return its point-wise value.\n    gp: A gaussian process fitted to the relevant data.\n    y_max: The current maximum known value of the target function.\n    bounds: The variables bounds to limit the search of the acq max.\n    \n    Returns\n    -------\n    x_max, The arg max of the acquisition function.\n    \"\"\"\n\n    dim=bounds.shape[0]\n    # Start with the lower bound as the argmax\n    x_max = bounds[:, 0]\n    max_acq = None\n\n    #x_tries = np.array([ np.linspace(i,j,500) for i,j in zip( bounds[:, 0], bounds[:, 1])])\n    #x_tries=x_tries.T\n\n    #myopts ={'maxiter':2000,'fatol':0.01,'xatol':0.01}\n    myopts ={'maxiter':5*dim,'maxfun':10*dim}\n    #myopts ={'maxiter':5*dim}\n\n\n    # multi start\n    #for i in xrange(5*dim):\n    #for i in xrange(1*dim):\n    for i in range(2*dim):\n        # Find the minimum of minus the acquisition function \n        \n        x_tries = np.random.uniform(bounds[:, 0], bounds[:, 1],size=(20*dim, dim))\n        \n        if init_location!=[]:\n            x_tries=np.vstack((x_tries,init_location))\n        \n            \n        y_tries=ac(x_tries,gp=gp)\n        \n        #find x optimal for init\n        idx_max=np.argmax(y_tries)\n        #print \"max y_tries {:.5f} y_max={:.3f}\".format(np.max(y_tries),y_max)\n\n        x_init_max=x_tries[idx_max]\n        \n        start_opt=time.time()\n    \n        res = minimize(lambda x: -ac(x.reshape(1, -1), gp=gp),x_init_max.reshape(1, -1),bounds=bounds,\n                       method=\"L-BFGS-B\",options=myopts)#L-BFGS-B\n\n\n        #res = fmin_bfgs(lambda x: -ac(x.reshape(1, -1), gp=gp, y_max=y_max),x_init_max.reshape(1, -1),disp=False)#L-BFGS-B\n        # value at the estimated point\n        #val=ac(res.x,gp,y_max)        \n        \n        if 'x' not in res:\n            val=ac(res,gp)        \n        else:\n            val=ac(res.x,gp) \n\n        \n        end_opt=time.time()\n        #print \"elapse optimize={:.5f}\".format(end_opt-start_opt)\n        \n        # Store it if better than previous minimum(maximum).\n        if max_acq is None or val >= max_acq:\n            if 'x' not in res:\n                x_max = res\n            else:\n                x_max = res.x\n            max_acq = val\n            #print max_acq\n\n    # Clip output to make sure it lies within the bounds. Due to floating\n    # point technicalities this is not always the case.\n    #return np.clip(x_max[0], bounds[:, 0], bounds[:, 1])\n        #print max_acq\n    return np.clip(x_max, bounds[:, 0], bounds[:, 1])\n\n\nFile Path: bayes_opt/curve_compression.py\nContent:\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Sun Nov 15 10:18:39 2020\n\n@author: Vu\n\"\"\"\n\n\nimport itertools\nimport numpy as np\n\ndef apply_one_transform_average(curve, midpoint=3, growth=1,MaxEpisode=1000):\n    # averaging the reward curve into numeric score\n            \n    if isinstance(curve, (list,)):\n        curve=curve[0]\n \n    def linear_func(x): # this is a straightline\n        if len(x)==1:\n            return 1\n        else:\n            return [1 for u in x]\n\t\n    my_xrange_scaled=np.linspace(0.01,5, MaxEpisode)\n    my_logistic_value_scaled=linear_func(my_xrange_scaled)\n    my_logistic_value_scaled=my_logistic_value_scaled[:len(curve)] # this is for visualisation purpose\n\n    average=np.mean(curve)\n    \n    return average,my_logistic_value_scaled    \n\n\ndef return_logistic_curve(midpoint, growth, MaxEpoch=1000):\n    # given the growth, midpoint and npoint, return the Logistic curve for visualization\n    \n    def logistic_func(x):\n        #alpha=32\n        if len(x)==1:\n            return 1.0/(1+np.exp(-growth*(x-midpoint)))\n        else:\n            return [1.0/(1+np.exp(-growth*(u-midpoint))) for u in x]\n        \n    my_xrange_scaled=np.linspace(-6,6, MaxEpoch)\n    my_logistic_value_scaled=logistic_func(my_xrange_scaled)\n    \n    return my_logistic_value_scaled\n\n\n\n#def apply_one_transform_decreasing(curve, midpoint=3, growth=1,MaxEpisode=1000):\n#            \n#    if isinstance(curve, (list,)):\n#        curve=curve[0]\n#\n# \n#    def ln_func(x):\n#        #alpha=32\n#        if len(x)==1:\n#            return 1/x\n#        else:\n#            return [1/u for u in x]\n#\t\n#    #my_xrange_scaled=np.linspace(-6,6, len(curve))\n#    my_xrange_scaled=np.linspace(0.01,5, MaxEpisode)\n#    my_logistic_value_scaled=ln_func(my_xrange_scaled)\n#    my_logistic_value_scaled=my_logistic_value_scaled[:len(curve)]\n#\n#\n#    # if curve is negative, add a constant to make it positive\n#    if np.max(curve)<=0 and np.min(curve)<=0:\n#        curve=curve+500\n#    \n#    threshold=(midpoint+6-2)*len(curve)/(12)\n#    threshold=np.int(threshold)\n#    #print(threshold)\n#    \n#    \n#    prod_func=curve*my_logistic_value_scaled\n#    \n#    #print(len(prod_func))\n#    average=[np.mean(prod_func[threshold:pos]) for pos in range(threshold,len(prod_func))]\n#\n#    #print(average)\n#    if np.isnan(average[-1]):\n#        print('bug [curve]')\n#    return average[-1],my_logistic_value_scaled    \n\n\n#def apply_one_transform_linear(curve, midpoint=3, growth=1,MaxEpisode=1000):\n#            \n#    if isinstance(curve, (list,)):\n#        curve=curve[0]\n# \n#    def ln_func(x):\n#        if len(x)==1:\n#            return x\n#        else:\n#            return [u for u in x]\n#\t\n#    my_xrange_scaled=np.linspace(0.01,5, MaxEpisode)\n#    my_logistic_value_scaled=ln_func(my_xrange_scaled)\n#    my_logistic_value_scaled=my_logistic_value_scaled[:len(curve)]\n#\n#    # if curve is negative, add a constant to make it positive\n#    if np.max(curve)<=0 and np.min(curve)<=0:\n#        curve=curve+500\n#     \n#    threshold=(midpoint+6-2)*len(curve)/(12)\n#    threshold=np.int(threshold)\n#    \n#    prod_func=curve*my_logistic_value_scaled\n#    \n#    average=[np.mean(prod_func[threshold:pos]) for pos in range(threshold,len(prod_func))]\n#\n#    return average[-1],my_logistic_value_scaled    \n\ndef apply_one_transform_ln(curve, midpoint=3, growth=1,MaxEpisode=1000):\n    # this is the log transformation, used in the ablation study\n    if isinstance(curve, (list,)):\n        curve=curve[0]\n \n    def ln_func(x):\n        if len(x)==1:\n            return 20+np.log(x)\n        else:\n            return [np.log(u) for u in x]\n\t\n    my_xrange_scaled=np.linspace(0.01,5, MaxEpisode)\n    my_logistic_value_scaled=ln_func(my_xrange_scaled)\n    my_logistic_value_scaled=my_logistic_value_scaled[:len(curve)]\n\n    # if curve is negative, add a constant to make it positive\n    if np.max(curve)<=0 and np.min(curve)<=0:\n        curve=curve+500\n    \n    threshold=(midpoint+6-2)*len(curve)/(12)\n    threshold=np.int(threshold)    \n    \n    prod_func=curve*my_logistic_value_scaled\n    \n    average=[np.mean(prod_func[threshold:pos]) for pos in range(threshold,len(prod_func))]\n\n    if np.isnan(average[-1]):\n        print('bug [curve]')\n    return average[-1],my_logistic_value_scaled\n\n\ndef apply_one_transform_logistic(curve, midpoint=-2, growth=1,MaxEpisode=1000,IsReturnCurve=False):\n    # this is the Logistic transformation, used in the paper\n    if isinstance(curve, (list,)):\n        curve=curve[0]\n        \n    def logistic_func(x):\n        return 1.0/(1+np.exp(-growth*(x-midpoint)))\n\t\n    #print(MaxEpisode)\n    my_xrange_scaled=np.linspace(-6,6, int(MaxEpisode))\n\n    my_logistic_value_scaled=logistic_func(my_xrange_scaled)\n\n    my_logistic_value_scaled=my_logistic_value_scaled[:len(curve)]\n\n    # if curve is negative, add a constant to make it positive\n    if np.max(curve)<=0 and np.min(curve)<=0:\n        curve=curve+500\n    \n    threshold=(midpoint+6-2)*len(curve)/(12)\n    threshold=np.int(threshold)\n    \n    prod_func=curve*my_logistic_value_scaled\n    \n    average=[np.mean(prod_func[threshold:pos+1]) for pos in range(threshold,len(prod_func))]\n\n    if IsReturnCurve==True:\n        return average[-1],my_logistic_value_scaled\n    else:\n        return average[-1]\n\n\n#def return_logistic_curve(midpoint, growth, npoint):\n#           \n#    def logistic_func(x):\n#        #alpha=32\n#        if len(x)==1:\n#            return 1.0/(1+np.exp(-growth*(x-midpoint)))\n#        else:\n#            return [1.0/(1+np.exp(-growth*(u-midpoint))) for u in x]\n#        \n#    my_xrange_scaled=np.linspace(-6,6, npoint)\n#    my_logistic_value_scaled=logistic_func(my_xrange_scaled)\n#    \n#    return my_logistic_value_scaled\n\n\n\n\n    \ndef transform_logistic_marginal(curves,MaxEpisode=1000):\n    # curve is a matrix [nParameter x MaxIter]\n    # or curve is a vector [1 x MaxIter]\n\n    def transform_one_logistic_marginal(curves,MaxEpisode):\n        # curve is a vector [1 x MaxIter]\n    \n        midpoint_list=[-3,-2,-1,0,1]\n        growth_list=[0.1,1,2,3]\n        \n        temp_Y_value=[0]*(len(midpoint_list)*len(growth_list))\n        for idx, (val1, val2) in enumerate(itertools.product(midpoint_list,growth_list)):\n            temp_Y_value[idx]=apply_one_transform_logistic(curves,val1, val2,MaxEpisode)\n                \n        temp_Y_value=np.asarray(temp_Y_value)\n        \n        Y=np.mean(temp_Y_value,axis=0)\n        return Y\n    if len(curves)==1:\n        output=transform_one_logistic_marginal(curves[0],MaxEpisode)\n    else:\n        output=[0]*len(curves)\n        for idx, curve in enumerate(curves):\n            output[idx]=transform_one_logistic_marginal(curve,MaxEpisode)\n    return output    \n\n\ndef transform_logistic(curves, midpoint=0, growth=1,MaxEpisode=1000):\n    # curve is a matrix [nParameter x MaxIter]\n    # or curve is a vector [1 x MaxIter]\n\n    if len(curves)==1:\n        output=apply_one_transform_logistic(curves[0], midpoint, growth,MaxEpisode)\n    else:\n        output=[0]*len(curves)\n        for idx, curve in enumerate(curves):\n            output[idx]=apply_one_transform_logistic(curve, midpoint, growth,MaxEpisode)\n    return output\n    \n\n        \n    \nFile Path: bayes_opt/product_gaussian_process.py\nContent:\n# -*- coding: utf-8 -*-\n# define Gaussian Process class\n\n\nimport numpy as np\nfrom bayes_opt.acquisition_functions import unique_rows\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom scipy.optimize import minimize\nfrom sklearn.metrics.pairwise import euclidean_distances\nimport scipy.linalg as spla\nfrom bayes_opt.curve_compression import apply_one_transform_logistic, transform_logistic\n\n\nclass ProductGaussianProcess(object):\n    # in this class of Gaussian process, we define k( {x,t}, {x',t'} )= k(x,x')*k(t,t')\n    \n    \n    #def __init__ (self,param):\n    def __init__ (self,SearchSpace,gp_hyper=None,logistic_hyper=None,verbose=0):\n        self.noise_delta=5e-4\n        self.noise_upperbound=1e-2\n        self.mycov=self.cov_RBF_time\n        self.SearchSpace=SearchSpace\n        scaler = MinMaxScaler()\n        scaler.fit(SearchSpace.T)\n        self.Xscaler=scaler\n        self.verbose=verbose\n        self.dim=SearchSpace.shape[0]\n        \n        if gp_hyper is None:\n            self.hyper={}\n            self.hyper['var']=1 # standardise the data\n            self.hyper['lengthscale_x']=0.02 #to be optimised\n            self.hyper['lengthscale_t']=0.2 #to be optimised\n        else:\n            self.hyper=gp_hyper\n\n        \n        if logistic_hyper is None:\n            self.logistic_hyper={}\n            self.logistic_hyper['midpoint']=0.0\n            self.logistic_hyper['growth']=1.0   \n        else:\n            self.logistic_hyper=logistic_hyper\n\n        self.X=[]\n        self.T=[]\n        self.Y=[]\n        self.Y_curves=None\n#        self.hyper['lengthscale_x']_old=self.hyper['lengthscale_x']\n#        self.hyper['lengthscale_x']_old_t=self.hyper['lengthscale_x']_t\n        \n        self.alpha=[] # for Cholesky update\n        self.L=[] # for Cholesky update LL'=A\n        \n        self.MaxEpisode=0\n        \n        return None\n       \n\n    def cov_RBF_time(self, x1,t1,x2,t2,lengthscale,lengthscale_t):\n        \n        Euc_dist=euclidean_distances(x1,x2)\n        exp_dist_x=np.exp(-np.square(Euc_dist)/lengthscale)\n        \n        Euc_dist=euclidean_distances(t1,t2)\n        exp_dist_t=np.exp(-np.square(Euc_dist)/lengthscale_t)\n        \n        return exp_dist_x*exp_dist_t\n                \n    def fit(self,X,T,Y,Y_curves):\n        \"\"\"\n        Fit Gaussian Process model\n\n        Input Parameters\n        ----------\n        x: the observed points \n        t: time or number of episode\n        y: the outcome y=f(x)\n        \n        \"\"\" \n        temp=np.hstack((X,T))\n        ur = unique_rows(temp)\n        \n        T=T[ur]\n        X=X[ur]\n        Y=Y[ur]\n        \n        self.X=X\n        self.Y=Y\n        self.T=T\n        self.Y_curves=[val for idx,val in enumerate(Y_curves) if ur[idx]==True]\n        \n        for curves in self.Y_curves:\n            self.MaxEpisode=max(len(curves),self.MaxEpisode)\n        #self.Y_curves=Y_curves[myidx]\n            \n        Euc_dist_x=euclidean_distances(X,X)\n        #exp_dist_x=np.exp(-np.square(Euc_dist)/self.hyper['lengthscale_x'])+np.eye(len(X))*self.noise_delta\n    \n        Euc_dist_t=euclidean_distances(T,T)\n        #exp_dist_t=np.exp(-np.square(Euc_dist)/self.hyper['lengthscale_x']_t)+np.eye(len(X))*self.noise_delta       \n    \n        self.KK_x_x=np.exp(-np.square(Euc_dist_x)/self.hyper['lengthscale_x']\\\n                           -np.square(Euc_dist_t)/self.hyper['lengthscale_t'])+np.eye(len(X))*self.noise_delta\n          \n        if np.isnan(self.KK_x_x).any(): #NaN\n            print(\"nan in KK_x_x\")\n        \n        #self.KK_x_x_inv=np.linalg.pinv(self.KK_x_x)\n        self.L=np.linalg.cholesky(self.KK_x_x)\n        temp=np.linalg.solve(self.L,self.Y)\n        self.alpha=np.linalg.solve(self.L.T,temp)\n        self.cond_num=self.compute_condition_number()\n        \n    def compute_condition_number(self):\n        cond_num=np.linalg.cond(self.KK_x_x)\n        return cond_num\n    \n\n    def log_marginal_lengthscale_logistic_hyper(self,hyper,noise_delta):\n        \"\"\"\n        Compute Log Marginal likelihood of the GP model w.r.t. the provided lengthscale, noise_delta and Logistic hyperparameter\n        \"\"\"\n\n        def compute_log_marginal_with_logistic_hyper(lengthscale, lengthscale_t,midpoint,growth,noise_delta):\n            # compute K\n            temp=np.hstack((self.X,self.T))\n            ur = unique_rows(temp)\n            myX=self.X[ur]\n            myT=self.T[ur]\n            \n            # transform Y_curve to Y_original, then to Y\n            Y_original=transform_logistic(self.Y_curves,midpoint,growth,self.MaxEpisode)\n            myY=(Y_original-np.mean(Y_original))/np.std(Y_original)\n            \n            myY=myY[ur]\n          \n            self.Euc_dist_x=euclidean_distances(myX,myX)\n            self.Euc_dist_t=euclidean_distances(myT,myT)\n        \n            KK=np.exp(-np.square(self.Euc_dist_x)/lengthscale-np.square(self.Euc_dist_t)/lengthscale_t)\n                +np.eye(len(myX))*noise_delta\n                    \n            \n            try:\n                temp_inv=np.linalg.solve(KK,myY)\n            except: # singular\n                return -np.inf\n            \n            try:\n                #logmarginal=-0.5*np.dot(self.Y.T,temp_inv)-0.5*np.log(np.linalg.det(KK+noise_delta))-0.5*len(X)*np.log(2*3.14)\n                first_term=-0.5*np.dot(myY.T,temp_inv)\n                \n                # if the matrix is too large, we randomly select a part of the data for fast computation\n                if KK.shape[0]>200:\n                    idx=np.random.permutation(KK.shape[0])\n                    idx=idx[:200]\n                    KK=KK[np.ix_(idx,idx)]\n                #Wi, LW, LWi, W_logdet = pdinv(KK)\n                #sign,W_logdet2=np.linalg.slogdet(KK)\n                chol  = spla.cholesky(KK, lower=True)\n                W_logdet=np.sum(np.log(np.diag(chol)))\n                # Uses the identity that log det A = log prod diag chol A = sum log diag chol A\n    \n                #second_term=-0.5*W_logdet2\n                second_term=-W_logdet\n            except: # singular\n                return -np.inf\n            \n\n            logmarginal=first_term+second_term-0.5*len(myY)*np.log(2*3.14)\n                \n            if np.isnan(np.asscalar(logmarginal))==True:\n                print(\"lengthscale_x={:f} lengthscale_t={:f} first term ={:.4f} second  term ={:.4f}\".format(\n                        lengthscale,lengthscale_t,np.asscalar(first_term),np.asscalar(second_term)))\n\n            #print(lengthscale, lengthscale_t,midpoint,growth,\"logmarginal:\",logmarginal)\n            return np.asscalar(logmarginal)\n        \n        logmarginal=0\n\n        if not isinstance(hyper,list) and len(hyper.shape)==2:\n            logmarginal=[0]*hyper.shape[0]\n            growth=hyper[:,3]\n            midpoint=hyper[:,2]\n            lengthscale_t=hyper[:,1]\n            lengthscale_x=hyper[:,0]\n            for idx in range(hyper.shape[0]):\n                logmarginal[idx]=compute_log_marginal_with_logistic_hyper(lengthscale_x[idx],\\\n                           lengthscale_t[idx],midpoint[idx],growth[idx],noise_delta)\n        else:\n            lengthscale_x,lengthscale_t,midpoint,growth=hyper\n            logmarginal=compute_log_marginal_with_logistic_hyper(lengthscale_x,lengthscale_t,\\\n                                                                 midpoint,growth,noise_delta)\n        return logmarginal\n\n#    def optimize_lengthscale_SE_maximizing(self,previous_theta,noise_delta):\n#        \"\"\"\n#        Optimize to select the optimal lengthscale parameter\n#        \"\"\"\n#                \n#        # define a bound on the lengthscale\n#        SearchSpace_lengthscale_min=0.01\n#        SearchSpace_lengthscale_max=0.5\n#        #mySearchSpace=[np.asarray([SearchSpace_lengthscale_min,SearchSpace_lengthscale_max]).T]\n#        \n#        mySearchSpace=np.asarray([[SearchSpace_lengthscale_min,SearchSpace_lengthscale_max],\\\n#                             [10*SearchSpace_lengthscale_min,2*SearchSpace_lengthscale_max]])\n#        \n#        # Concatenate new random points to possible existing\n#        # points from self.explore method.\n#        lengthscale_tries = np.random.uniform(mySearchSpace[:, 0], mySearchSpace[:, 1],size=(20, mySearchSpace.shape[0]))\n#\n#        #print lengthscale_tries\n#\n#        # evaluate\n#        self.flagOptimizeHyperFirst=0 # for efficiency\n#\n#        logmarginal_tries=self.log_marginal_lengthscale(lengthscale_tries,noise_delta)\n#        #print logmarginal_tries\n#\n#        #find x optimal for init\n#        idx_max=np.argmax(logmarginal_tries)\n#        lengthscale_init_max=lengthscale_tries[idx_max]\n#        #print lengthscale_init_max\n#        \n#        myopts ={'maxiter':20*self.dim,'maxfun':20*self.dim}\n#\n#        x_max=[]\n#        max_log_marginal=None\n#        \n#        res = minimize(lambda x: -self.log_marginal_lengthscale(x,noise_delta),lengthscale_init_max,\n#                       SearchSpace=mySearchSpace,method=\"L-BFGS-B\",options=myopts)#L-BFGS-B\n#        if 'x' not in res:\n#            val=self.log_marginal_lengthscale(res,noise_delta)    \n#        else:\n#            val=self.log_marginal_lengthscale(res.x,noise_delta)  \n#        \n#        # Store it if better than previous minimum(maximum).\n#        if max_log_marginal is None or val >= max_log_marginal:\n#            if 'x' not in res:\n#                x_max = res\n#            else:\n#                x_max = res.x\n#            max_log_marginal = val\n#            #print res.x\n#\n#        return x_max\n    \n    def optimize_lengthscale_SE_logistic_hyper(self,previous_hyper,noise_delta):\n        \"\"\"\n        Optimize to select the optimal lengthscale parameter\n        \"\"\"\n        \n        # define a bound on the lengthscale\n        SearchSpace_l_min=0.03\n        SearchSpace_l_max=0.3\n        \n        SearchSpace_midpoint_min=-2\n        SearchSpace_midpoint_max=3\n        \n        SearchSpace_growth_min=0.5\n        SearchSpace_growth_max=2\n        #mySearchSpace=[np.asarray([SearchSpace_lengthscale_min,SearchSpace_lengthscale_max]).T]\n        \n        mySearchSpace=np.asarray([[SearchSpace_l_min,SearchSpace_l_max],[10*SearchSpace_l_min,2*SearchSpace_l_max],\n                             [SearchSpace_midpoint_min,SearchSpace_midpoint_max],[SearchSpace_growth_min,SearchSpace_growth_max]])\n        \n        lengthscale_tries = np.random.uniform(mySearchSpace[:, 0], mySearchSpace[:, 1],size=(20, 4))\n\n        # evaluate\n        self.flagOptimizeHyperFirst=0 # for efficiency\n\n        logmarginal_tries=self.log_marginal_lengthscale_logistic_hyper(lengthscale_tries,noise_delta)\n\n        #find x optimal for init\n        idx_max=np.argmax(logmarginal_tries)\n        lengthscale_init_max=lengthscale_tries[idx_max]\n        #print lengthscale_init_max\n        \n        myopts ={'maxiter':30*self.dim,'maxfun':30*self.dim}\n\n        x_max=[]\n        max_log_marginal=None\n        \n        res = minimize(lambda x: -self.log_marginal_lengthscale_logistic_hyper(x,noise_delta),lengthscale_init_max,\n                       bounds=mySearchSpace,method=\"L-BFGS-B\",options=myopts)#L-BFGS-B\n        if 'x' not in res:\n            val=self.log_marginal_lengthscale_logistic_hyper(res,noise_delta)    \n        else:\n            val=self.log_marginal_lengthscale_logistic_hyper(res.x,noise_delta)  \n        \n        # Store it if better than previous minimum(maximum).\n        if max_log_marginal is None or val >= max_log_marginal:\n            if 'x' not in res:\n                x_max = res\n            else:\n                x_max = res.x\n            max_log_marginal = val\n            #print res.x\n\n        return x_max\n\n\n#    def optimize_lengthscale(self,previous_theta_x, previous_theta_t,noise_delta):\n#\n#        prev_theta=[previous_theta_x,previous_theta_t]\n#        newlengthscale,newlengthscale_t=self.optimize_lengthscale_SE_maximizing(prev_theta,noise_delta)\n#        self.hyper['lengthscale_x']=newlengthscale\n#        self.hyper['lengthscale_t']=newlengthscale_t\n#        \n#        # refit the model\n#        temp=np.hstack((self.X,self.T))\n#        ur = unique_rows(temp)\n#        \n#        self.fit(self.X[ur],self.T[ur],self.Y[ur],self.Y_curves)\n#        \n#        return newlengthscale,newlengthscale_t\n            \n    def optimize_lengthscale_logistic_hyper(self,prev_hyper,noise_delta):\n        # optimize both GP lengthscale and logistic hyperparameter\n\n            \n        #prev_theta=[prev_theta_x,prev_theta_t,prev_midpoint,prev_growth]\n        newlengthscale,newlengthscale_t,newmidpoint,newgrowth=self.optimize_lengthscale_SE_logistic_hyper(prev_hyper,noise_delta)\n        self.hyper['lengthscale_x']=newlengthscale\n        self.hyper['lengthscale_t']=newlengthscale_t\n        \n        # refit the model\n        temp=np.hstack((self.X,self.T))\n        ur = unique_rows(temp)\n\n        # update Y here\n        Y_original=transform_logistic(self.Y_curves,newmidpoint,newgrowth,self.SearchSpace[-1,1])\n        Y=(Y_original-np.mean(Y_original))/np.std(Y_original)\n        self.Y=Y\n        #\n        self.fit(self.X[ur],self.T[ur],self.Y[ur],self.Y_curves)\n        \n        return newlengthscale,newlengthscale_t,newmidpoint,newgrowth\n\n\n    def compute_var(self,X,T,xTest,tTest):\n        \"\"\"\n        compute variance given X and xTest\n        \n        Input Parameters\n        ----------\n        X: the observed points\n        xTest: the testing points \n        \n        Returns\n        -------\n        diag(var)\n        \"\"\" \n        \n        xTest=np.asarray(xTest)\n        xTest=np.atleast_2d(xTest)\n        \n        tTest=np.asarray(tTest)\n        tTest=np.atleast_2d(tTest)\n        tTest=np.reshape(tTest,(-1,1))\n        \n        if self.kernel_name=='SE':\n            #Euc_dist=euclidean_distances(xTest,xTest)\n            #KK_xTest_xTest=np.exp(-np.square(Euc_dist)/self.hyper['lengthscale_x'])+np.eye(xTest.shape[0])*self.noise_delta\n            #ur = unique_rows(X)\n            myX=X\n            myT=T\n            \n            Euc_dist_x=euclidean_distances(myX,myX)\n            #exp_dist_x=np.exp(-np.square(self.Euc_dist_x)/lengthscale)+np.eye(len(myX))*noise_delta\n        \n            Euc_dist_t=euclidean_distances(myT,myT)\n            #exp_dist_t=np.exp(-np.square(self.Euc_dist_t)/lengthscale_t)+np.eye(len(myX))*noise_delta      \n        \n            KK=np.exp(-np.square(Euc_dist_x)/self.hyper['lengthscale_x']-np.square(Euc_dist_t)/self.hyper['lengthscale_t'])\\\n                +np.eye(len(myX))*self.noise_delta\n                    \n                 \n            Euc_dist_test_train_x=euclidean_distances(xTest,X)\n            #Exp_dist_test_train_x=np.exp(-np.square(Euc_dist_test_train_x)/self.hyper['lengthscale_x'])\n            \n            Euc_dist_test_train_t=euclidean_distances(tTest,T)\n            #Exp_dist_test_train_t=np.exp(-np.square(Euc_dist_test_train_t)/self.hyper['lengthscale_t'])\n            \n            KK_xTest_xTrain=np.exp(-np.square(Euc_dist_test_train_x)/self.hyper['lengthscale_x']-np.square(Euc_dist_test_train_t)/self.hyper['lengthscale_t'])\n                \n        try:\n            temp=np.linalg.solve(KK,KK_xTest_xTrain.T)\n        except:\n            temp=np.linalg.lstsq(KK,KK_xTest_xTrain.T, rcond=-1)\n            temp=temp[0]\n            \n        #var=KK_xTest_xTest-np.dot(temp.T,KK_xTest_xTrain.T)\n        var=np.eye(xTest.shape[0])-np.dot(temp.T,KK_xTest_xTrain.T)\n        var=np.diag(var)\n        var.flags['WRITEABLE']=True\n        var[var<1e-100]=0\n        return var \n\n    \n        \n    def predict(self,xTest, eval_MSE=True):\n        \"\"\"\n        compute predictive mean and variance\n        Input Parameters\n        ----------\n        xTest: the testing points \n        \n        Returns\n        -------\n        mean, var\n        \"\"\"    \n\n        if len(xTest.shape)==1: # 1d\n            xTest=xTest.reshape((-1,self.X.shape[1]+1))\n            \n        tTest=xTest[:,-1]\n        tTest=np.atleast_2d(tTest)\n        tTest=np.reshape(tTest,(xTest.shape[0],-1))\n        \n        xTest=xTest[:,:-1]\n        \n        # prevent singular matrix\n        temp=np.hstack((self.X,self.T))\n        ur = unique_rows(temp)\n        \n        X=self.X[ur]\n        T=self.T[ur]\n                \n        Euc_dist_x=euclidean_distances(xTest,xTest)\n        Euc_dist_t=euclidean_distances(tTest,tTest)\n\n        KK_xTest_xTest=np.exp(-np.square(Euc_dist_x)/self.hyper['lengthscale_x']-np.square(Euc_dist_t)/self.hyper['lengthscale_t'])\\\n            +np.eye(xTest.shape[0])*self.noise_delta\n        \n        Euc_dist_test_train_x=euclidean_distances(xTest,X)\n        \n        Euc_dist_test_train_t=euclidean_distances(tTest,T)\n        \n        KK_xTest_xTrain=np.exp(-np.square(Euc_dist_test_train_x)/self.hyper['lengthscale_x']-np.square(Euc_dist_test_train_t)/self.hyper['lengthscale_t'])\n            \n        #Exp_dist_test_train_x*Exp_dist_test_train_t\n  \n        # using Cholesky update\n        mean=np.dot(KK_xTest_xTrain,self.alpha)\n        v=np.linalg.solve(self.L,KK_xTest_xTrain.T)\n        var=KK_xTest_xTest-np.dot(v.T,v)\n        \n\n        return mean.ravel(),np.diag(var)  \n\n    def posterior(self,x):\n        # compute mean function and covariance function\n        return self.predict(self,x)\n        \n    \n\nFile Path: bayes_opt/sequentialBO/boil.py\nContent:\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Tue Mar 29 11:49:58 2016\n\n\"\"\"\n\n\nimport numpy as np\nfrom bayes_opt.acquisition_functions import AcquisitionFunction, unique_rows\nfrom bayes_opt import GaussianProcess\nfrom bayes_opt import ProductGaussianProcess\n\nfrom bayes_opt.acquisition_maximization import acq_max_with_name,acq_min_scipy_kwargs\nimport time\nfrom sklearn import linear_model\nimport copy\nfrom bayes_opt.curve_compression import transform_logistic\nfrom sklearn.preprocessing import MinMaxScaler\n\n\n#======================================================================================================\n#======================================================================================================\n#======================================================================================================\n#======================================================================================================\ncounter = 0\n\n\nclass BOIL(object):\n\n    #def __init__(self, gp_params, func_params, acq_params, verbose=True):\n    def __init__(self, func, SearchSpace,acq_name=\"ei_mu_max\",verbose=1):\n\n        \"\"\"      \n        Input parameters\n        ----------\n        \n        gp_params:                  GP parameters\n        gp_params.theta:            to compute the kernel\n        gp_params.delta:            to compute the kernel\n        \n        func_params:                function to optimize\n        func_params.init bound:     initial SearchSpace for parameters\n        func_params.SearchSpace:        SearchSpace on parameters        \n        func_params.func:           a function to be optimized\n        \n        \n        acq_params:            acquisition function, \n        acq_params.acq_func['name']=['ei','ucb','poi']\n        acq_params.opt_toolbox:     optimization toolbox 'nlopt','direct','scipy'\n                            \n        Returns\n        -------\n        dim:            dimension\n        SearchSpace:         SearchSpace on original scale\n        scaleSearchSpace:    SearchSpace on normalized scale of 0-1\n        time_opt:       will record the time spent on optimization\n        gp:             Gaussian Process object\n        \"\"\"\n        \n        self.method='boil'\n        self.verbose=verbose\n        if isinstance(SearchSpace,dict):\n            # Get the name of the parameters\n            self.keys = list(SearchSpace.keys())\n            \n            self.SearchSpace = []\n            for key in list(SearchSpace.keys()):\n                self.SearchSpace.append(SearchSpace[key])\n            self.SearchSpace = np.asarray(self.SearchSpace)\n        else:\n            self.SearchSpace=np.asarray(SearchSpace)\n            \n            \n        self.dim = len(SearchSpace)\n\n        scaler = MinMaxScaler()\n        scaler.fit(self.SearchSpace[:-1,:].T)\n        \n        scalerT = MinMaxScaler()\n        SearchSpace_T=np.atleast_2d(self.SearchSpace[-1,:]).T\n        scalerT.fit(SearchSpace_T)\n\n        self.Xscaler=scaler\n        self.Tscaler=scalerT\n\n        # create a scaleSearchSpace 0-1\n        self.scaleSearchSpace=np.array([np.zeros(self.dim), np.ones(self.dim)]).T\n                \n        # function to be optimised\n        self.f = func\n    \n        # store X in original scale\n        self.X_ori= None\n\n        # store X in 0-1 scale\n        self.X = None\n        \n        # store y=f(x)\n        # (y - mean)/(max-min)\n        self.Y = None\n               \n        # y original scale\n        self.Y_ori = None\n        \n        # store the number of episode\n        self.T=None\n        self.T_original=None\n        \n        # store the cost original scale\n        self.Y_cost_original=None\n        \n        self.time_opt=0\n         \n        self.max_min_gap=self.SearchSpace[:,1]-self.SearchSpace[:,0]\n\n\n        # acquisition function\n        self.acq_name = acq_name\n        self.logmarginal=0\n\n        self.gp=ProductGaussianProcess(self.scaleSearchSpace,verbose=verbose)\n\n        # store the curves of performances\n        self.Y_curves=[]\n        \n        # store the cost original scale\n        self.Y_cost_original=None\n        \n        self.time_opt=0\n        \n        # acquisition function\n        self.acq_func = None\n   \n        self.logmarginal=0\n        \n        self.markVirtualObs=[]\n        \n        self.countVirtual=[]\n\n        self.linear_regression = linear_model.LinearRegression()\n\n        self.condition_number=[]\n        \n        # maximum number of augmentations\n        self.max_n_augmentation=10\n        self.threshold_cond=15\n        \n    def init(self, n_init_points=3, seed=1):\n        \"\"\"      \n        Input parameters\n        ----------\n        n_init_points:        # init points\n        \"\"\"\n        np.random.seed(seed)\n\n        # Generate random points\n        SearchSpace=np.copy(self.SearchSpace)\n        SearchSpace[-1,0]=SearchSpace[-1,1] # last dimension, set it to MaxIter\n\n        l = [np.random.uniform(x[0], x[1]) for _ in range(n_init_points) for x in SearchSpace] \n\n        # Concatenate new random points to possible existing\n        # points from self.explore method.\n        temp=np.asarray(l)\n        temp=temp.T\n        init_X=list(temp.reshape((n_init_points,-1)))\n        \n        self.X_original = np.asarray(init_X)\n        self.T_original=self.X_original[:,-1]\n        self.T_original=np.reshape(self.T_original,(n_init_points,-1))\n        \n        self.X_original=self.X_original[:,:-1] # remove the last dimension of MaxEpisode\n        self.X_original=np.reshape(self.X_original,(n_init_points,-1))\n\n        # Evaluate target function at all initialization           \n        y_init_curves, y_init_cost=self.f(init_X)\n\n        y_init_cost=np.atleast_2d(np.asarray(y_init_cost))#.astype('Float64')\n\n        self.Y_curves+=y_init_curves\n\n        # we transform the y_init_curves as the average of [ curves * logistic ]\n        y_init=transform_logistic(y_init_curves,self.gp.logistic_hyper['midpoint'],\\\n                                  self.gp.logistic_hyper['growth'], self.SearchSpace[-1,1])\n        #y_init=y_init_curves\n        y_init=np.reshape(y_init,(n_init_points,1))\n        \n        # record keeping ========================================================\n        self.Y_original = np.asarray(y_init)      \n        self.Y_cost_original=np.reshape(y_init_cost,(-1,1))\n\n        # convert it to scaleX\n        self.X = self.Xscaler.transform(np.asarray(init_X)[:,:-1])#remove the last dimension of MaxEpisode\n        #self.X=self.X[:,:-1]\n        self.X=np.reshape(self.X,(n_init_points,-1))\n\n        self.T = self.Tscaler.transform(self.T_original)\n\n        self.markVirtualObs+=[0]*n_init_points\n\n        # generating virtual observations for each initial point\n        for ii in range(n_init_points):\n            self.generating_virtual_observations(self.X[ii,:],\n                         self.T[ii],[y_init_curves[ii]],y_init_cost[0][ii],IsRandom=False)\n\n        self.Y_cost=(self.Y_cost_original-np.min(self.Y_cost_original))/(np.max(self.Y_cost_original)-np.min(self.Y_cost_original))\n\n        if np.std(self.Y_original)==0:\n            self.Y=(self.Y_original-np.mean(self.Y_original))\n        else:\n            self.Y=(self.Y_original-np.mean(self.Y_original))/np.std(self.Y_original)\n\n       \n    def utility_cost_evaluation(self,x,acq_func,isDebug=False):\n        # this is a wrapper function to evaluate at multiple x(s)\n        \n        \n        def utility_cost_evaluation_single(x,acq_func,isDebug=False):\n            # given a location x, we will evaluate the utility and cost\n            \n            utility=acq_func.acq_kind(x,gp=self.gp)\n            \n            try:\n                mean_cost=self.linear_regression.predict(np.reshape(x,(1,-1)))\n                \n            except:\n                print(x)\n                print(\"bug\")\n    \n            mean_cost=max(0,mean_cost)+0.1 # to avoid <=0 cost\n            \n            #acquisition_function_value= utility_normalized/cost_normalized\n            if 'ei' in acq_func.acq_name:\n                acquisition_function_value= np.log(utility)-np.log(mean_cost)\n            else:\n                acquisition_function_value= np.log(1+np.exp(utility))/np.log(1+np.exp(mean_cost))\n    \n            if isDebug==True:\n                print(\"acq_func at the selected point \\t utility:\",np.round(utility,decimals=4),\"\\t cost:\",mean_cost)\n                if utility==0:\n                    print(\"utility =0===============================================================================\")\n       \n            return acquisition_function_value*(-1) # since we will minimize this acquisition function\n        \n        \n        if len(x)==self.dim: # one observation\n            temp=utility_cost_evaluation_single(x,acq_func,isDebug=isDebug)\n            if isDebug==True:\n                return temp\n            else:\n                utility=np.mean(temp)\n        \n        else: # multiple observations\n            utility=[0]*len(x)\n            for idx,val in enumerate(x):\n                temp=utility_cost_evaluation_single(x=val,acq_func=acq_func,isDebug=isDebug)\n                                                     \n                utility[idx]=np.mean(temp)\n                \n            utility=np.asarray(utility)    \t\t\t               \n        return utility   \n    \n        \n    def acq_utility_cost(self):\n        \n        # generate a set of x* at T=MaxIter\n        # instead of running optimization on the whole space, we will only operate on the region of interest\n        # the region of interest in DRL is where the MaxEpisode\n    \n        # we find maximum of EI\n\n        acq={}\n        acq['name']=self.acq_name\n        acq['dim']=self.scaleSearchSpace.shape[0]\n        acq['scaleSearchSpace']=self.scaleSearchSpace   \n    \n        if self.acq_name=='ei_mu_max':# using max of mean(x) as the incumbent\n            \n            # optimie the GP predictive mean function to find the max of mu\n            x_mu_max,mu_max_val=acq_max_with_name(gp=self.gp,scaleSearchSpace=self.scaleSearchSpace,acq_name='mu',IsReturnY=True)\n            acq['mu_max']=  mu_max_val\n\n        myacq=AcquisitionFunction(acq)\n        \n        x_min = acq_min_scipy_kwargs(myfunc=self.utility_cost_evaluation,SearchSpace=self.scaleSearchSpace,\n                        acq_func=myacq, isDebug=False)\n        \n        if self.verbose==True:\n            acq_val=self.utility_cost_evaluation(x_min,myacq,isDebug=False)\n            print(\"selected point from acq func:\",np.round(x_min,decimals=4),\"acq val=log(Utility/Cost)=\",(-1)*np.round(acq_val,decimals=4))) # since we minimize the acq func\n            if np.round(acq_val,decimals=4)==0:\n                print(\"acq value =0\")\n            \n        return x_min\n    \n    \n    def select_informative_location_by_uncertainty(self,n_virtual_obs,x_max,t_max):\n        # this function will select a list of informative locations to place a virtual obs\n        # x_max is the selected hyperparameter\n        # t_max is the selected number of epochs to train\n        \n        \n        SearchSpace=np.copy(self.scaleSearchSpace)\n        for dd in range(self.dim-1):\n            SearchSpace[dd,0],SearchSpace[dd,1]=x_max[dd],x_max[dd]\n            \n        SearchSpace[-1,1]=t_max\n        \n        temp_X,temp_T=self.X.copy(),self.T.copy()\n        temp_gp=copy.deepcopy(self.gp )\n        \n        temp_Y=np.random.random(size=(len(temp_T),1))\n        \n        temp_gp.fit(temp_X,temp_T,temp_Y,self.Y_curves)\n        \n        new_batch_T=None\n\n        pred_var_value=[0]*n_virtual_obs\n        for ii in range(n_virtual_obs):\n            x_max_pred_variance, pred_var_value[ii]=acq_max_with_name(gp=temp_gp,\n                              scaleSearchSpace=SearchSpace,acq_name='pure_exploration',IsReturnY=True)\n            \n            # stop augmenting if the uncertainty is smaller than a threshold\n            # or stop augmenting if the uncertainty is smaller than a threshold\n\n            log_cond=np.log( temp_gp.compute_condition_number() )\n            if log_cond>self.threshold_cond or pred_var_value[ii]<(self.gp.noise_delta+1e-3):\n                break\n          \n            if x_max_pred_variance[-1] in temp_T[-ii:]: # if repetition, stop augmenting\n                break\n            \n            temp_X = np.vstack((temp_X, x_max.reshape((1, -1)))) # append new x\n            temp_T = np.vstack((temp_T, x_max_pred_variance[-1].reshape((1, -1)))) # append new t\n            temp_gp.X,temp_gp.T=temp_X,temp_T\n            temp_Y=np.random.random(size=(len(temp_T),1))\n            \n            temp_gp.fit(temp_X,temp_T,temp_Y,self.Y_curves)\n\n            if new_batch_T is None:\n                new_batch_T=x_max_pred_variance[-1].reshape((1, -1))\n            else:\n                new_batch_T= np.vstack((new_batch_T, x_max_pred_variance[-1].reshape((1, -1))))\n        \n#        if self.verbose:\n#            print(\"pred_var_value at the augmented points:\",np.round( pred_var_value,decimals=4))\n\n        if new_batch_T is None:\n            return [],0\n\n        else:\n            output=np.sort(new_batch_T.ravel()).tolist()\n            return output, len(output)\n\n    \n    def generating_virtual_observations(self,x_max,t_max,y_original_curves,y_cost_original,IsRandom=False):\n        \n        #temp_X_new_original=x_max*self.max_min_gap[:-1]+self.SearchSpace[:-1,0]\n        temp_X_new_original=self.Xscaler.inverse_transform(np.reshape(x_max,(-1,self.dim-1)))\n\n        # selecting MAX number of virtual observations, e.g., we dont want to augment more than 10 points\n        max_n_virtual_obs=np.int(t_max*self.max_n_augmentation)\n        if max_n_virtual_obs==0:\n            self.countVirtual.append(0)\n            return\n        \n        if IsRandom==True:# select informative locations by random uniform   \n            l = [np.random.uniform(0, t_max) for _ in range(max_n_virtual_obs)]\n        else:\n            # select informative locations by uncertainty as in the paper\n            l,n_virtual_obs=self.select_informative_location_by_uncertainty(max_n_virtual_obs,x_max,t_max)        \n            \n        self.countVirtual.append(n_virtual_obs)\n        \n        if self.verbose:\n            np.set_printoptions(suppress=True)\n            print(\"Max #augmented points\",max_n_virtual_obs, \"\\t #augmented points \",len(l),\n                  \"\\t Augmented points: \",np.round(l,decimals=3))\n            \n        l_original=[self.SearchSpace[-1,0]+val*self.max_min_gap[-1] for val in l]\n        #l_original=[self.Tscaler.inverse_transform(val) for val in l]\n                           \n        virtual_obs_t_original=np.asarray(l_original).T\n        virtual_obs_t=np.asarray(l).T\n        \n        # compute y_original for the virtual observations\n        y_virtual_original=[0]*n_virtual_obs\n        for ii in range(n_virtual_obs):\n            \n            idx=np.int(virtual_obs_t_original[ii])\n            \n            temp_curve=y_original_curves[0][:idx+1]\n            self.markVirtualObs.append(1)\n\n            y_virtual_original[ii]=transform_logistic([temp_curve],\\\n                      self.gp.logistic_hyper['midpoint'],self.gp.logistic_hyper['growth'],self.SearchSpace[-1,1])\n           \n            self.X = np.vstack((self.X, x_max.reshape((1, -1))))\n            self.X_original=np.vstack((self.X_original, temp_X_new_original))\n        \n            self.T = np.vstack((self.T, virtual_obs_t[ii].reshape((1, -1))))\n            temp=np.asarray(virtual_obs_t_original[ii])\n            self.T_original=np.vstack((self.T_original, temp.reshape((1, -1))))\n\n\n            self.Y_original = np.append(self.Y_original,[y_virtual_original[ii]])\n            self.Y_curves.append(temp_curve)\n            \n            # interpolating the cost for augmented observation\n            y_cost_estimate=y_cost_original*virtual_obs_t[ii]\n            self.Y_cost_original = np.append(self.Y_cost_original,[y_cost_estimate])\n            \n        \n#        if self.verbose:\n#            temp_y_original_whole_curve=transform_logistic(y_original_curves,\\\n#                               self.gp.logistic_hyper['midpoint'],self.gp.logistic_hyper['growth'],self.SearchSpace[-1,1])\n#            print(np.round(temp_y_original_whole_curve,decimals=4), np.round(y_virtual_original,decimals=4))\n#            \n        \n    def suggest_nextpoint(self): # logistic, time-cost, virtual\n        \"\"\"\n        Main optimization method.\n\n\n        Returns\n        -------\n        x: recommented point for evaluation\n        \"\"\"\n \n        # init a new Gaussian Process============================================\n        self.gp=ProductGaussianProcess(self.scaleSearchSpace,self.gp.hyper,self.gp.logistic_hyper)\n        self.gp.fit(self.X, self.T,self.Y,self.Y_curves)\n            \n        # we store the condition number here=====================================\n        self.condition_number.append(self.gp.cond_num)\n        if self.verbose:\n            print(\"ln of conditioning number of GP covariance matrix\", np.round(np.log(self.gp.cond_num),decimals=1))\n\n        # count number of real observations\n        count=len(self.markVirtualObs)-np.sum(self.markVirtualObs)\n        count=np.int(count)\n\n        # optimize GP hyperparameters and Logistic hyper after 3*d iterations\n        if  len(self.Y)%(2*self.dim)==0:\n\n            hyper=[self.gp.hyper['lengthscale_x'],self.gp.hyper['lengthscale_t'], \\\n                   self.gp.logistic_hyper['midpoint'],self.gp.logistic_hyper['growth']]\n            newlengthscale_x,newlengthscale_t,new_midpoint, new_growth = self.gp.optimize_lengthscale_logistic_hyper(hyper,self.gp.noise_delta)\n            \n            self.gp.hyper['lengthscale_x']=newlengthscale_x\n            self.gp.hyper['lengthscale_t']=self.gp.hyper['lengthscale_t']\n            self.gp.logistic_hyper['midpoint']=new_midpoint\n            self.gp.logistic_hyper['growth']=new_growth\n          \n            if self.verbose:\n                print(\"==estimated lengthscale_x={:.4f}   lengthscale_t={:.3f}   Logistic_m0={:.1f}   Logistic_g0={:.1f}\".format(\n                    newlengthscale_x,newlengthscale_t,new_midpoint,new_growth))\n                \n        # Set acquisition function\n        start_opt=time.time()\n\n        # linear regression is used to fit the cost\n        # fit X and T\n        combine_input=np.hstack((self.X,self.T))\n        self.linear_regression.fit(combine_input,self.Y_cost)\n        \n        # maximize the acquisition function to select the next point =================================\n        x_max_temp=self.acq_utility_cost()\n        x_max=x_max_temp[:-1]\n        t_max=x_max_temp[-1]       \n            \n        # record keeping stuffs ====================================================\n        # record the optimization time\n        finished_opt=time.time()\n        elapse_opt=finished_opt-start_opt\n        self.time_opt=np.hstack((self.time_opt,elapse_opt))\n\n        # this is for house keeping stuff        \n        self.markVirtualObs.append(0)\n\n        self.X = np.vstack((self.X, x_max.reshape((1, -1))))\n        self.T = np.vstack((self.T, t_max.reshape((1, -1))))\n\n        # compute X in original scale\n        temp_X_new_original=self.Xscaler.inverse_transform(np.reshape(x_max,(-1,self.dim-1)))\n        #temp_X_new_original=x_max*self.max_min_gap[:-1]+self.SearchSpace[:-1,0]\n        self.X_original=np.vstack((self.X_original, temp_X_new_original))\n        \n        #temp_T_new_original=t_max*self.max_min_gap[-1]+self.SearchSpace[-1,0]\n        temp_T_new_original=self.Tscaler.inverse_transform(np.reshape(t_max,(-1,1)))\n        self.T_original=np.vstack((self.T_original, temp_T_new_original))\n\n        # evaluate Y using original X\n        x_original_to_test=x_max_temp*self.max_min_gap+self.SearchSpace[:,0]\n\n        # evaluate the black-box function=================================================\n        y_original_curves, y_cost_original= self.f(x_original_to_test)\n        \n        # compute the utility score by transformation\n        y_original=transform_logistic(y_original_curves,\\\n              self.gp.logistic_hyper['midpoint'],self.gp.logistic_hyper['growth'],self.SearchSpace[-1,1])\n        \n        if len(y_original_curves)==1: # list\n            self.Y_curves.append(y_original_curves[0])\n        else:\n            self.Y_curves.append(y_original_curves)\n\n        \n        self.Y_original = np.append(self.Y_original,y_original)\n        self.Y_cost_original = np.append(self.Y_cost_original,y_cost_original)\n\n        # augmenting virtual observations =====================================================\n        self.generating_virtual_observations(x_max,t_max,y_original_curves,y_cost_original[0])\n        \n        # update Y after change Y_original        \n        if np.std(self.Y_original)==0:\n            self.Y=(self.Y_original-np.mean(self.Y_original))\n        else:\n            self.Y=(self.Y_original-np.mean(self.Y_original))/np.std(self.Y_original)\n            \n        self.Y_cost=(self.Y_cost_original-np.min(self.Y_cost_original))/(np.max(self.Y_cost_original)-np.min(self.Y_cost_original))\n                    \n        #if self.verbose:\n        np.set_printoptions(suppress=True)\n\n        print(\"[original scale] x={} t={:.0f} current y={:.2f}, ybest={:.2f}\".format( np.round(self.X_original[-1],decimals=4),\\\n              np.asscalar(self.T_original[-1]),np.asscalar(self.Y_original[-1]), np.asscalar(self.Y_original.max())))\n",
    "Experiment Result": "GP Parameters:\n- Noise delta: 5e-4 (initial value in ProductGaussianProcess)\n- Kernel type: Radial Basis Function (RBF) or Squared Exponential (SE) product kernel, as implemented in `cov_RBF_time`.\n- Initial lengthscale for hyperparameters (x): 0.02\n- Initial lengthscale for training iterations (t): 0.2\n\nLogistic Preference Function Parameters (Sigmoid):\n- Initial midpoint (m0): 0.0\n- Initial growth (g0): 1.0\n- Optimization bounds for midpoint: [-2, 3]\n- Optimization bounds for growth: [0.5, 2]\n\nAcquisition Function Details:\n- Default acquisition function name: 'ei_mu_max' (modified Expected Improvement using max of mean(x) as incumbent).\n- Acquisition function optimization method: Multi-start L-BFGS-B (from `scipy.optimize.minimize`).\n- Number of random restarts for acquisition optimization: 1*dim for `acq_max_scipy`, 3*dim for `acq_min_scipy` and `acq_min_scipy_kwargs`.\n- Number of initial random points evaluated in each restart for acquisition optimization: 50*dim for `acq_max_scipy`, 100*dim for `acq_min_scipy_kwargs`.\n- Cost function in utility-cost ratio: approximated by a `sklearn.linear_model.LinearRegression` model.\n- Acquisition function calculation for optimization against cost: `np.log(utility) - np.log(mean_cost)` when using EI.\n\nHyperparameter Optimization:\n- Frequency of GP and Logistic hyperparameter optimization: When the number of observations `len(self.Y)` is a multiple of `2 * self.dim`.\n- Optimization options for hyperparameters: `maxiter = 30 * self.dim`, `maxfun = 30 * self.dim`.\n- Number of random restarts for hyperparameter optimization: 20.\n\nData Augmentation Settings:\n- Maximum number of virtual augmentations per real observation: `max_n_augmentation = 10` (so, `max_n_virtual_obs = int(t_max * self.max_n_augmentation)`).\n- Strategy for selecting augmentation points: Selects points at maximum GP predictive uncertainty (using 'pure_exploration' acquisition function).\n- Threshold for the natural logarithm of the GP covariance matrix condition number (δ): `threshold_cond = 15`. If `log(condition_number)` exceeds this, or if predictive uncertainty is too low, augmentation stops."
}{
    "Title": "Hyperparameter Optimization through Neural Network Partitioning",
    "Main Contributions": "This research introduces Partitioned Neural Networks (PN) as a novel, computationally efficient method for hyperparameter optimization (HPO) inspired by the marginal likelihood objective. The core problem addressed is the high computational cost and the need for validation data in traditional HPO, especially in data-limited regimes and Federated Learning (FL). The method partitions training data into K shards and a neural network model into K parameter partitions, associating each partition with specific data shards. Subnetworks are formed by combining these partitions, allowing the definition of an \"out-of-training-sample\" loss as the objective for HPO, which requires no validation data. The paper demonstrates that this objective can optimize various hyperparameters (e.g., input masks, data augmentations, dropout rates, feature extractors) in a single training run, being significantly cheaper than alternative marginal likelihood methods and offering particular benefits for FL by reducing communication overhead.",
    "Methodology": "The method optimizes a lower-bound approximation to the marginal likelihood, LML (Eq. 2), with respect to hyperparameters. It involves: 1) Partitioning the neural network's weights (w) into C partitions (w1,...,wC). Subnetworks w(k)s are constructed by retaining the first k partitions (w1,...,wk) and setting the remaining parameters (wk+1,...,wC) to default values (e.g., initialization values or zero). 2) Partitioning the training dataset D into C chunks (D1,...,DC). 3) Interleaving stochastic gradient updates for both model parameters and hyperparameters. For model parameters, each partition wk is optimized on data from D1:k using its corresponding subnetwork w(k)s. For hyperparameters ψ, they are optimized based on the \"out-of-sample\" loss, which measures the loss on data chunk Dk using the subnetwork w(k-1)s (trained on D1:k-1). The paper primarily uses a 'random weight partitioning' scheme, where a fixed proportion of weights in each layer is randomly assigned to a partition.",
    "Experimental Setup": "The method was evaluated on several tasks: 1) Input selection (toy model selection and differentiable input mask learning) using a synthetic dataset with 15 informative and 15 spurious features, employing MLPs. 2) Learning invariances through affine data augmentations on MNIST, CIFAR10, TinyImagenet, and their rotated variants (rotMNIST, rotCIFAR10, rotTinyImagenet). Architectures included CNNs for MNIST and FixupResNets (ResNet-8, ResNet-14), and ResNet-50 with GroupNorm(2) for CIFAR/TinyImagenet. 3) Learning general feature extractors using a Wide ResNet-20 on CIFAR10. 4) Optimizing dropout rates. 5) Federated Learning (FL) with non-i.i.d. splits of MNIST, RotMNIST, CIFAR10, and RotCIFAR10 across 100 clients, using a CNN for MNIST and ResNet-9 with GroupNorm for CIFAR10. Baselines included traditional training/validation split, Augerino, Differentiable Laplace, Last-layer Marginal Likelihood, and FedAvg. Optimization was performed using Adam or SGD with Cosine Annealing, with separate learning rates for model parameters and hyperparameters. Performance was measured by test accuracy, log-likelihood, and the LML objective. Data and parameter partitioning ratios were often [80%,10%,10%] for 3 chunks or [80%,20%] for 2 chunks, with sensitivity to these ratios explored through grid search.",
    "Limitations": "The method, while offering advantages, introduces certain limitations. It necessitates an additional forward-backward pass for hyperparameter updates, leading to increased computational costs and typically requiring more training iterations for convergence compared to standard neural network training, though still more efficient than other marginal likelihood methods. Partitioning the network inherently constrains its capacity, which can lead to some performance loss compared to a fully trained, non-partitioned network with perfectly tuned hyperparameters. Furthermore, the partitioning strategy itself (e.g., number of chunks, relative proportions of data and parameters per chunk) becomes an additional hyperparameter that may require tuning for optimal performance on a given task. The paper also noted a pathological behavior in the Augerino baseline, where its loss function could diverge, necessitating careful tuning or ad-hoc adjustments like clipping.",
    "Future Research Directions": "Future research directions include investigating dynamic partitioning of network parameters during training, exploring alternative partitioning schemes that might reduce the computational overhead (e.g., allowing multiple partitions to be updated in a single mini-batch), and examining different sequential updating schemes for clients in Federated Learning settings. Additionally, further work could focus on strategies to alleviate the potential performance loss due to network partitioning, such as adjusting training rounds or increasing the network's inherent capacity. Exploring methods to accumulate gradients from different chunks for hyperparameters to achieve lower-variance estimates and interleaving hyperparameter updates at less frequent intervals to further reduce computational overhead are also potential avenues.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "DP-HyPO: An Adaptive Private Framework for Hyperparameter Optimization",
    "Main Contributions": "The paper introduces DP-HyPO, a pioneering framework for adaptive private hyperparameter optimization, addressing the gap between existing non-adaptive private methods and efficient adaptive non-private techniques. Its main contributions include: 1) enabling the flexible use of non-DP adaptive HPO methods (e.g., Gaussian process) while maintaining strong differential privacy (DP) guarantees and avoiding high privacy costs from composition; 2) providing sharp R\nr\nnyi DP guarantees for adaptive private HPO, generalizing prior uniform results by accurately characterizing the R\nr\nnyi divergence between adaptive sampling distributions without stability assumptions; and 3) empirically demonstrating that Gaussian process-based DP-HyPO outperforms its uniform counterpart across various real-world scenarios, allowing practitioners to integrate any non-private adaptive HPO methods and flexibly allocate privacy budgets.",
    "Methodology": "DP-HyPO operates by maintaining an adaptive sampling distribution (π) that evolves based on accumulated information from previous runs. To ensure differential privacy, the total number of training runs (T) is drawn from a random variable (e.g., truncated negative binomial distribution). A crucial aspect is enforcing bounded density for any posterior sampling distribution, meaning c \nr\n \n \nr\nπ(j)(λ)/π(0)(λ) \nr\n \n \nr\nC for all hyperparameters λ. To achieve this, a general recipe is provided: for any non-private adaptive HPO method, its iteratively updated distribution density π(j) is projected into a space SC,c of bounded densities by solving a convex functional programming problem (minimizing \nr\n2 loss). The framework utilizes the R\nr\nnyi DP framework for privacy accounting, offering tighter bounds than traditional composition. An instantiation of DP-HyPO is presented using Gaussian Process (GP), where GP constructs a surrogate model to generate probability distributions over performance measures. Scores (estimated Upper Confidence Bound) are assigned to hyperparameters, then transformed into a sampling distribution using a softmax function, with its density truncated via the projection technique.",
    "Experimental Setup": "The effectiveness of GP-based DP-HyPO (\nGP\n) is empirically evaluated against Uniform DP-HyPO (\nUniform\n), a non-adaptive baseline. Experiments are conducted in two privacy configurations: 1) **White-box setting**, where adaptive HPO incurs extra privacy cost from the base algorithm: \n\n    *   **MNIST Simulation**: DP-SGD training of a standard CNN, optimizing learning rate (η) and clipping norm (R) on the MNIST dataset. Performance is measured by accuracy. A semi-real simulation caches mean accuracy of 5 models per hyperparameter, adding Gaussian noise on sampling. Hyperparameters: batch size 256, 10 epochs, specific σ values for GP/Uniform, C=2, c=0.75 for GP, τ=0.1, β=1. Grid discretization for η (16 obs) and R (20 obs), totaling 320. \n\n    *   **CIFAR-10 Simulation**: Similar CNN model and hyperparameters (η, R) on CIFAR-10. Hyperparameter landscape (mean and standard error of accuracy) generated by BoTorch from 50 runs. Oracle returns noisy scores from a normal distribution. Specific σ values, C=2, c=0.75, τ=0.1, β=1. Grid discretization for η (50 obs) and R (50 obs), totaling 2500. \n\n2) **Black-box setting**, where the base algorithm's privacy budget is fixed, and adaptivity uses an extra budget: \n\n    *   **Federated Learning (FL) Task**: Optimizing learning rates for a central server (AdaGrad) and individual users (SGD) on a proprietary industry dataset (details confidential). Landscape (mean and standard error of loss) generated by BoTorch. Oracle returns noisy scores. Various C values (1.25, 1.33, 1.5) with c=1/C are tested for GP.",
    "Limitations": "The framework's practical implementation involves discretizing the hyperparameter space to make the convex functional optimization computationally feasible, which is an approximation. The requirement for the adaptive sampling distribution to have bounded density (c \nr\n \n \nr\nπ(j)(λ)/π(0)(λ) \nr\n \n \nr\nC) is not naturally satisfied by non-private HPO methods and necessitates a projection step. The paper notes a trade-off in privacy budget allocation between enhancing the base algorithm and improving adaptivity. Empirically, for simpler hyperparameter landscapes like MNIST, the adaptive DP-HyPO (GP) shows only marginal superiority over its uniform counterpart, suggesting its value is more pronounced in complex scenarios. Additionally, details about the proprietary dataset used in the Federated Learning experiment are limited due to confidentiality constraints.",
    "Future Research Directions": "Two main future research directions are identified. Firstly, there is significant potential to improve empirical performance by integrating more advanced hyperparameter optimization (HPO) methods from the extensive HPO literature into the DP-HyPO framework. Secondly, the authors propose establishing theoretical utility guarantees for the general DP-HyPO framework or specific configurations within it, by leveraging proof methodologies similar to those found in prior work like Theorem 3.3 in [26].",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "Differentiable Expected Hypervolume Improvement for Parallel Multi-Objective Bayesian Optimization",
    "Main Contributions": "The paper introduces q-Expected Hypervolume Improvement (qEHVI), a novel acquisition function for parallel and constrained multi-objective Bayesian optimization. It provides an exact computation of the joint EHVI of `q` new candidate points (up to Monte-Carlo integration error). A key contribution is the derivation of exact gradients for the Monte-Carlo estimator of qEHVI using auto-differentiation, enabling efficient optimization with first-order and quasi-second-order methods via the Sample Average Approximation (SAA) approach. The method is computationally tractable, embarrassingly parallel, and empirically outperforms state-of-the-art multi-objective BO algorithms with significantly reduced wall time, including extending support for auxiliary outcome constraints and computing exact gradients for analytic EHVI for M>2 objectives.",
    "Methodology": "The methodology leverages box decompositions and the inclusion-exclusion principle to compute the Hypervolume Improvement (HVI) for `q` candidate points. Expected HVI (EHVI) is then estimated using Monte-Carlo (MC) integration with samples drawn from the joint posterior of a Gaussian Process surrogate model, utilizing randomized quasi-MC methods for variance reduction. Exact gradients of the MC-estimator are computed via auto-differentiation, employing the re-parameterization trick. This enables optimization through the Sample Average Approximation (SAA) framework, which allows for deterministic, higher-order optimizers and offers theoretical convergence guarantees. Constraints are incorporated by feasibility-weighting on the sample level, using a differentiable sigmoid approximation for the indicator function, effectively treating constraints as additional objectives in the hypervolume calculation. The approach supports both joint batch and sequential greedy optimization strategies.",
    "Experimental Setup": "The empirical evaluation of qEHVI is conducted on both synthetic and real-world problems. Synthetic benchmarks include the Branin-Currin problem (d=2, M=2), C2-DTLZ2 (d=12, M=2, V=1), and DTLZ2 (d=6, M=2). Real-world applications comprise Structural Optimization in Automobile Safety Design (VEHICLE SAFETY: 5 design parameters, 3 objectives) and Policy Optimization for Adaptive Bitrate Control (ABR: 4 control policy parameters, 2 objectives). qEHVI is compared against state-of-the-art methods: SMS-EGO, PESMO, TS-TCH, analytic EHVI (with gradients), a novel parallel-constrained ParEGO extension (qPAREGO), and a quasi-random baseline. Performance is measured by log hypervolume difference. All objective outcomes are modeled with independent Gaussian Processes using a Matern 5/2 ARD kernel. Experiments are initialized with 2(d+1) points from a scrambled Sobol sequence. qPAREGO and qEHVI use N=128 Quasi Monte-Carlo samples. Acquisition functions are optimized using L-BFGS-B (200 iterations maximum), leveraging exact gradients for qEHVI/EHVI/qPAREGO and approximated gradients for SMS-EGO/PESMO. Benchmarks are run on CPUs (2x Intel Xeon E5-2680 v4 @ 2.40GHz) and GPUs (Tesla V100-SXM2-16GB).",
    "Limitations": "The current formulation of qEHVI assumes noiseless observations, which is a common limitation across existing EHVI formulations. Additionally, its scalability in high-dimensional objective spaces (particularly M >= 4) is constrained by the computational complexity of the underlying box partitioning algorithm used for hypervolume calculation. While the paper demonstrates approximate partitioning can mitigate this to some extent, it remains a bottleneck for broader applicability.",
    "Future Research Directions": "Future research directions include extending qEHVI to account for noisy observations by integrating over the uncertainty of previous observations, potentially using Monte-Carlo samples across both new candidates and training points. Another key area is to improve scalability and computation time, especially in high-dimensional objective spaces, by incorporating more efficient exact or approximate partitioning algorithms (e.g., those producing fewer disjoint hyper-rectangles). The authors also express a broader hope that this work will encourage researchers to explore and apply modern computational paradigms and tooling to further enhance Bayesian optimization methodologies.",
    "Experiment Code": "class AcquisitionFunction(Module, ABC):\n    _log: bool = False\n    def __init__(self, model: Model) -> None:\n        super().__init__()\n        self.model: Model = model\n    def set_X_pending(self, X_pending: Tensor | None = None) -> None:\n        if X_pending is not None:\n            if X_pending.requires_grad:\n                warnings.warn(\n                    \"Pending points require a gradient but the acquisition function\"\n                    \" will not provide a gradient to these points.\",\n                    BotorchWarning,\n                    stacklevel=2,\n                )\n            self.X_pending = X_pending.detach().clone()\n        else:\n            self.X_pending = X_pending\n    @abstractmethod\n    def forward(self, X: Tensor) -> Tensor:\n        pass\n\nclass MCSamplerMixin(ABC):\n    _default_sample_shape = torch.Size([512])\n    def __init__(self, sampler: MCSampler | None = None) -> None:\n        self.sampler = sampler\n    def get_posterior_samples(self, posterior: Posterior) -> Tensor:\n        if self.sampler is None:\n            self.sampler = get_sampler(\n                posterior=posterior, sample_shape=self._default_sample_shape\n            )\n        return self.sampler(posterior=posterior)\n    @property\n    def sample_shape(self) -> torch.Size:\n        return (\n            self.sampler.sample_shape\n            if self.sampler is not None\n            else self._default_sample_shape\n        )\n\nclass MultiObjectiveMCAcquisitionFunction(AcquisitionFunction, MCSamplerMixin, ABC):\n    _default_sample_shape = torch.Size([128])\n    def __init__(\n        self,\n        model: Model,\n        sampler: MCSampler | None = None,\n        objective: MCMultiOutputObjective | None = None,\n        constraints: list[Callable[[Tensor], Tensor]] | None = None,\n        eta: Tensor | float = 1e-3,\n        X_pending: Tensor | None = None,\n    ) -> None:\n        super().__init__(model=model)\n        MCSamplerMixin.__init__(self, sampler=sampler)\n        if objective is None:\n            objective = IdentityMCMultiOutputObjective()\n        elif not isinstance(objective, MCMultiOutputObjective):\n            raise UnsupportedError(\n                \"Only objectives of type MCMultiOutputObjective are supported for \"\n                \"Multi-Objective MC acquisition functions.\"\n            )\n        if (\n            hasattr(model, \"input_transform\")\n            and isinstance(model.input_transform, InputPerturbation)\n            and constraints is not None\n        ):\n            raise UnsupportedError(\n                \"Constraints are not supported with input perturbations, due to\"\n                \"sample q-batch shape being different than that of the inputs.\"\n                \"Use a composite objective that applies feasibility weighting to\"\n                \"samples before calculating the risk measure.\"\n            )\n        self.add_module(\"objective\", objective)\n        self.constraints = constraints\n        if constraints:\n            if type(eta) is not Tensor:\n                eta = torch.full((len(constraints),), eta)\n            self.register_buffer(\"eta\", eta)\n        self.X_pending = None\n        if X_pending is not None:\n            self.set_X_pending(X_pending)\n    @abstractmethod\n    def forward(self, X: Tensor) -> Tensor:\n        pass\n\nclass SubsetIndexCachingMixin(object):\n    def __init__(self) -> None:\n        self._q_out: int = -1\n        self._q_subset_indices: dict[str, Tensor] = {}\n\n    @property\n    def q_out(self) -> int:\n        r\"\"\"The current effective q-batch size.\"\"\"\n        return self._q_out\n\n    def compute_q_subset_indices(self, q_out: int, device: torch.device) -> dict:\n        r\"\"\"Compute all subsets indices of a given size. This is cached for reuse.\"\"\"\n        if q_out == self._q_out:\n            return self._q_subset_indices\n        from botorch.utils.multi_objective.hypervolume import (  # noqa E501\n            get_q_subset_indices,\n        )\n\n        self._q_subset_indices = get_q_subset_indices(q=q_out, device=device)\n        self._q_out = q_out\n        return self._q_subset_indices\n\nclass NoisyExpectedHypervolumeMixin(CachedCholeskyMCSamplerMixin):\n    _prev_nehvi: Tensor\n\n    def __init__(\n        self,\n        model: Model,\n        ref_point: list[float] | Tensor,\n        X_baseline: Tensor,\n        sampler: MCSampler | None = None,\n        objective: MCMultiOutputObjective | None = None,\n        constraints: list[Callable[[Tensor], Tensor]] | None = None,\n        X_pending: Tensor | None = None,\n        prune_baseline: bool = False,\n        alpha: float = 0.0,\n        cache_pending: bool = True,\n        max_iep: int = 0,\n        incremental_nehvi: bool = True,\n        cache_root: bool = True,\n        marginalize_dim: int | None = None,\n    ) -> None:\n        CachedCholeskyMCSamplerMixin.__init__(\n            self, model=model, cache_root=cache_root, sampler=sampler\n        )\n        if prune_baseline:\n            X_baseline = prune_inferior_points_multi_objective(\n                model=model,\n                X=X_baseline,\n                ref_point=ref_point,\n                objective=objective,\n                constraints=constraints,\n                marginalize_dim=marginalize_dim,\n            )\n        self.register_buffer(\"_X_baseline\", X_baseline)\n        self.ref_point = ref_point\n        self.objective = objective\n        self.constraints = constraints\n        self.alpha = alpha\n        self.cache_pending = cache_pending\n        self.max_iep = max_iep\n        self.incremental_nehvi = incremental_nehvi\n        self.marginalize_dim = marginalize_dim\n        self.q_in = -1  # this is a hacky way to force initial computation of _baseline_L\n\n        # Initialize _prev_nehvi.\n        self.set_X_pending(X_pending)\n\n    @property\n    def X_baseline(self) -> Tensor:\n        r\"\"\"Returns the set of points that should be considered as the incumbent.\"\"\"\n        if self.cache_pending and self.X_pending is not None:\n            # for sequential greedy updates, we concatenate X_pending (i.e. previously\n            # selected points) to X_baseline so that we can compute the incremental\n            # NEHVI from the current batch.\n            return torch.cat([self._X_baseline, self.X_pending], dim=-2)\n        return self._X_baseline\n\n    def set_X_pending(self, X_pending: Tensor | None = None) -> None:\n        r\"\"\"Informs the acquisition function about pending design points.\n\n        Here pending points are concatenated with `X_baseline` and `NEHVI`\n        is computed.\n\n        Args:\n            X_pending: `n x d` Tensor with `n` `d`-dim design points that have\n                been submitted for evaluation but have not yet been evaluated.\n        \"\"\"\n        super().set_X_pending(X_pending=X_pending)\n        if (  # Skip recomputing if `X_pending` has not changed or is small.\n            self.X_pending is not None\n            and hasattr(self, \"_prev_nehvi\")\n            and self.cache_pending\n            and len(self.X_pending) < self.max_iep\n        ):\n            return\n        with torch.no_grad():\n            self._prev_nehvi = self.get_nehi_term(X=self.X_pending)\n\n    def get_nehi_term(self, X: Tensor | None = None) -> Tensor:\n        r\"\"\"The current (feasible) hypervolume contribution of X_baseline + X_pending.\n\n        Args:\n            X: A `(batch_shape) x q x d`-dim Tensor of inputs. These points are\n                concatenated with `self._X_baseline` for calculating the hypervolume.\n\n        Returns:\n            A `(batch_shape) x (model_batch_shape)`-dim tensor of hypervolume values.\n        \"\"\"\n        if X is None:  # nothing to evaluate, return zeros\n            return torch.zeros(\n                self.model.batch_shape, dtype=self.ref_point.dtype, device=X.device\n            )\n        X_full = self.X_baseline if X is None else torch.cat([self.X_baseline, X], dim=-2)\n        if X_full.shape[-2] == 0:  # nothing to evaluate, return zeros\n            return torch.zeros(\n                self.model.batch_shape, dtype=self.ref_point.dtype, device=X_full.device\n            )\n        posterior = self.model.posterior(X=X_full)\n        samples = self.get_posterior_samples(posterior)\n        obj = self.objective(samples, X=X_full)\n        if self.constraints is not None:\n            feas_weights = compute_smoothed_feasibility_indicator(\n                constraints=self.constraints,\n                samples=samples,\n                eta=getattr(self, \"eta\", 1e-3),\n                fat=getattr(self, \"fat\", False),\n                marginalize_dim=self.marginalize_dim,\n            )  # `sample_shape x batch_shape x q`\n            obj = obj * feas_weights.unsqueeze(-1)\n\n        # we need to be careful with the case where all samples are infeasible\n        if (obj < self.ref_point).all():\n            return torch.zeros(\n                self.model.batch_shape, dtype=self.ref_point.dtype, device=X_full.device\n            )\n\n        # TODO: The following two lines do not handle all batch dim combinations\n        # as self.objective sometimes changes batch dims\n        Y_sample = obj.mean(dim=0)  # Average over samples for partitioning\n        if self.marginalize_dim is not None:\n            # if we have marginalize_dim, then we need to average over it\n            Y_sample = Y_sample.mean(dim=self.marginalize_dim)\n        # A separate partitioning is generated for each model in the ensemble\n        if is_ensemble(self.model) and self.incremental_nehvi:\n            num_samples = Y_sample.shape[0]  # num_mcmc_samples for fully Bayesian GPs\n            Y_sample_split = torch.split(Y_sample, 1, dim=0)\n            partitionings = []\n            for i in range(num_samples):\n                if (Y_sample_split[i].squeeze(0) < self.ref_point).all():\n                    warnings.warn(\n                        \"All current points are worse than the reference point. \"\n                        \"Consider increasing the number of baseline points.\",\n                        BotorchWarning,\n                        stacklevel=2,\n                    )\n                    partitionings.append(None)\n                else:\n                    partitionings.append(\n                        FastNondominatedPartitioning(\n                            ref_point=self.ref_point,\n                            Y=Y_sample_split[i].squeeze(0),\n                            alpha=self.alpha,\n                        )\n                    )\n            return torch.stack(\n                [\n                    p.hypervolume() if p is not None else torch.zeros(1)\n                    for p in partitionings\n                ]\n            )\n        elif (Y_sample < self.ref_point).all():\n            warnings.warn(\n                \"All current points are worse than the reference point. \"\n                \"Consider increasing the number of baseline points.\",\n                BotorchWarning,\n                stacklevel=2,\n            )\n            return torch.zeros(self.model.batch_shape)\n        partitioning = FastNondominatedPartitioning(\n            ref_point=self.ref_point, Y=Y_sample, alpha=self.alpha\n        )\n        return partitioning.hypervolume()\n\nclass qExpectedHypervolumeImprovement(\n    MultiObjectiveMCAcquisitionFunction, SubsetIndexCachingMixin\n):\n    def __init__(\n        self,\n        model: Model,\n        ref_point: list[float] | Tensor,\n        partitioning: NondominatedPartitioning,\n        sampler: MCSampler | None = None,\n        objective: MCMultiOutputObjective | None = None,\n        constraints: list[Callable[[Tensor], Tensor]] | None = None,\n        X_pending: Tensor | None = None,\n        eta: Tensor | float = 1e-3,\n        fat: bool = False,\n    ) -> None:\n        legacy_ei_numerics_warning(legacy_name=type(self).__name__)\n        if len(ref_point) != partitioning.num_outcomes:\n            raise ValueError(\n                \"The length of the reference point must match the number of outcomes. \"\n                f\"Got ref_point with {len(ref_point)} elements, but expected \"\n                f\"{partitioning.num_outcomes}.\"\n            )\n        ref_point = torch.as_tensor(\n            ref_point,\n            dtype=partitioning.pareto_Y.dtype,\n            device=partitioning.pareto_Y.device,\n        )\n        super().__init__(\n            model=model,\n            sampler=sampler,\n            objective=objective,\n            constraints=constraints,\n            eta=eta,\n            X_pending=X_pending,\n        )\n        self.register_buffer(\"ref_point\", ref_point)\n        cell_bounds = partitioning.get_hypercell_bounds()\n        self.register_buffer(\"cell_lower_bounds\", cell_bounds[0])\n        self.register_buffer(\"cell_upper_bounds\", cell_bounds[1])\n        SubsetIndexCachingMixin.__init__(self)\n        self.fat = fat\n\n    def _compute_qehvi(self, samples: Tensor, X: Tensor | None = None) -> Tensor:\n        obj = self.objective(samples, X=X)\n        q = obj.shape[-2]\n        if self.constraints is not None:\n            feas_weights = compute_smoothed_feasibility_indicator(\n                constraints=self.constraints,\n                samples=samples,\n                eta=self.eta,\n                fat=self.fat,\n            )\n        device = self.ref_point.device\n        q_subset_indices = self.compute_q_subset_indices(q_out=q, device=device)\n        batch_shape = obj.shape[:-2]\n        areas_per_segment = torch.zeros(\n            *batch_shape,\n            self.cell_lower_bounds.shape[-2],\n            dtype=obj.dtype,\n            device=device,\n        )\n        cell_batch_ndim = self.cell_lower_bounds.ndim - 2\n        sample_batch_view_shape = torch.Size(\n            [\n                batch_shape[0] if cell_batch_ndim > 0 else 1,\n                *[1 for _ in range(len(batch_shape) - max(cell_batch_ndim, 1))],\n                *self.cell_lower_bounds.shape[1:-2],\n            ]\n        )\n        view_shape = (\n            *sample_batch_view_shape,\n            self.cell_upper_bounds.shape[-2],\n            1,\n            self.cell_upper_bounds.shape[-1],\n        )\n        for i in range(1, self.q_out + 1):\n            q_choose_i = q_subset_indices[f\"q_choose_{i}\"]\n            obj_subsets = obj.index_select(dim=-2, index=q_choose_i.view(-1))\n            obj_subsets = obj_subsets.view(\n                obj.shape[:-2] + q_choose_i.shape + obj.shape[-1:]\n            )\n            overlap_vertices = obj_subsets.min(dim=-2).values\n            overlap_vertices = torch.min(\n                overlap_vertices.unsqueeze(-3), self.cell_upper_bounds.view(view_shape)\n            )\n            lengths_i = (\n                overlap_vertices - self.cell_lower_bounds.view(view_shape)\n            ).clamp_min(0.0)\n            areas_i = lengths_i.prod(dim=-1)\n            if self.constraints is not None:\n                feas_subsets = feas_weights.index_select(\n                    dim=-1, index=q_choose_i.view(-1)\n                ).view(feas_weights.shape[:-1] + q_choose_i.shape)\n                areas_i = areas_i * feas_subsets.unsqueeze(-3).prod(dim=-1)\n            areas_i = areas_i.sum(dim=-1)\n            areas_per_segment += (-1) ** (i + 1) * areas_i\n        return areas_per_segment.sum(dim=-1).mean(dim=0)\n\n    @concatenate_pending_points\n    @t_batch_mode_transform()\n    @average_over_ensemble_models\n    def forward(self, X: Tensor) -> Tensor:\n        posterior = self.model.posterior(X)\n        samples = self.get_posterior_samples(posterior)\n        return self._compute_qehvi(samples=samples, X=X)\n\nclass qLogExpectedHypervolumeImprovement(\n    MultiObjectiveMCAcquisitionFunction, SubsetIndexCachingMixin\n):\n    _log: bool = True\n\n    def __init__(\n        self,\n        model: Model,\n        ref_point: list[float] | Tensor,\n        partitioning: NondominatedPartitioning,\n        sampler: MCSampler | None = None,\n        objective: MCMultiOutputObjective | None = None,\n        constraints: list[Callable[[Tensor], Tensor]] | None = None,\n        X_pending: Tensor | None = None,\n        eta: Tensor | float = 1e-2,\n        fat: bool = True,\n        tau_relu: float = 1e-6,\n        tau_max: float = 1e-2,\n    ) -> None:\n        if len(ref_point) != partitioning.num_outcomes:\n            raise ValueError(\n                \"The dimensionality of the reference point must match the number of \"\n                f\"outcomes. Got ref_point with {len(ref_point)} elements, but expected \"\n                f\"{partitioning.num_outcomes}.\"\n            )\n        ref_point = torch.as_tensor(\n            ref_point,\n            dtype=partitioning.pareto_Y.dtype,\n            device=partitioning.pareto_Y.device,\n        )\n        super().__init__(\n            model=model,\n            sampler=sampler,\n            objective=objective,\n            constraints=constraints,\n            eta=eta,\n            X_pending=X_pending,\n        )\n        self.register_buffer(\"ref_point\", ref_point)\n        cell_bounds = partitioning.get_hypercell_bounds()\n        self.register_buffer(\"cell_lower_bounds\", cell_bounds[0])\n        self.register_buffer(\"cell_upper_bounds\", cell_bounds[1])\n        SubsetIndexCachingMixin.__init__(self)\n        self.tau_relu = tau_relu\n        self.tau_max = tau_max\n        self.fat = fat\n\n    def _compute_log_qehvi(self, samples: Tensor, X: Tensor | None = None) -> Tensor:\n        obj = self.objective(samples, X=X)\n        q = obj.shape[-2]\n        if self.constraints is not None:\n            log_feas_weights = compute_smoothed_feasibility_indicator(\n                constraints=self.constraints,\n                samples=samples,\n                eta=self.eta,\n                log=True,\n                fat=self.fat,\n            )\n        device = self.ref_point.device\n        q_subset_indices = self.compute_q_subset_indices(q_out=q, device=device)\n        batch_shape = obj.shape[:-2]\n        log_areas_per_segment = torch.full(\n            size=(\n                *batch_shape,\n                self.cell_lower_bounds.shape[-2],\n                2,\n            ),\n            fill_value=-torch.inf,\n            dtype=obj.dtype,\n            device=device,\n        )\n\n        cell_batch_ndim = self.cell_lower_bounds.ndim - 2\n        sample_batch_view_shape = torch.Size(\n            [\n                batch_shape[0] if cell_batch_ndim > 0 else 1,\n                *[1 for _ in range(len(batch_shape) - max(cell_batch_ndim, 1))],\n                *self.cell_lower_bounds.shape[1:-2],\n            ]\n        )\n        view_shape = (\n            *sample_batch_view_shape,\n            self.cell_upper_bounds.shape[-2],\n            1,\n            self.cell_upper_bounds.shape[-1],\n        )\n\n        for i in range(1, self.q_out + 1):\n            q_choose_i = q_subset_indices[f\"q_choose_{i}\"]\n            obj_subsets = obj.index_select(dim=-2, index=q_choose_i.view(-1))\n            obj_subsets = obj_subsets.view(\n                obj.shape[:-2] + q_choose_i.shape + obj.shape[-1:]\n            )\n            log_improvement_i = self._log_improvement(obj_subsets, view_shape)\n            log_improvement_i = self._smooth_min(\n                log_improvement_i,\n                dim=-2,\n            )\n            log_lengths_i = self._log_cell_lengths(log_improvement_i, view_shape)\n            log_areas_i = log_lengths_i.sum(dim=-1)\n            if self.constraints is not None:\n                log_feas_subsets = log_feas_weights.index_select(\n                    dim=-1, index=q_choose_i.view(-1)\n                ).view(log_feas_weights.shape[:-1] + q_choose_i.shape)\n                log_areas_i = log_areas_i + log_feas_subsets.unsqueeze(-3).sum(dim=-1)\n            log_areas_i = logsumexp(log_areas_i, dim=-1)\n            log_areas_per_segment[..., i % 2] = logplusexp(\n                log_areas_per_segment[..., i % 2],\n                log_areas_i,\n            )\n        log_areas_per_segment = logdiffexp(\n            log_a=log_areas_per_segment[..., 0], log_b=log_areas_per_segment[..., 1]\n        )\n        return logmeanexp(logsumexp(log_areas_per_segment, dim=-1), dim=0)\n\n    def _log_improvement(\n        self, obj_subsets: Tensor, view_shape: tuple | torch.Size\n    ) -> Tensor:\n        obj_subsets = obj_subsets.unsqueeze(-4)\n        cell_lower_bounds = self.cell_lower_bounds.view(view_shape).unsqueeze(-3)\n        Z = obj_subsets - cell_lower_bounds\n        log_Zi = self._log_smooth_relu(Z)\n        return log_Zi\n\n    def _log_cell_lengths(\n        self, log_improvement_i: Tensor, view_shape: tuple | torch.Size\n    ) -> Tensor:\n        cell_upper_bounds = self.cell_upper_bounds.clamp_max(\n            1e10 if log_improvement_i.dtype == torch.double else 1e8\n        )\n        log_cell_lengths = (\n            (cell_upper_bounds - self.cell_lower_bounds).log().view(view_shape)\n        )\n        return self._smooth_minimum(\n            log_improvement_i,\n            log_cell_lengths,\n        )\n\n    def _log_smooth_relu(self, X: Tensor) -> Tensor:\n        f = log_fatplus if self.fat else log_softplus\n        return f(X, tau=self.tau_relu)\n\n    def _smooth_min(self, X: Tensor, dim: int, keepdim: bool = False) -> Tensor:\n        f = fatmin if self.fat else smooth_amin\n        return f(X, tau=self.tau_max, dim=dim)\n\n    def _smooth_minimum(self, X: Tensor, Y: Tensor) -> Tensor:\n        XY = torch.stack(torch.broadcast_tensors(X, Y), dim=-1)\n        return self._smooth_min(XY, dim=-1, keepdim=False)\n\n    @concatenate_pending_points\n    @t_batch_mode_transform()\n    @average_over_ensemble_models\n    def forward(self, X: Tensor) -> Tensor:\n        posterior = self.model.posterior(X)\n        samples = self.get_posterior_samples(posterior)\n        return self._compute_log_qehvi(samples=samples, X=X)\n\n\nclass qNoisyExpectedHypervolumeImprovement(\n    NoisyExpectedHypervolumeMixin,\n    qExpectedHypervolumeImprovement,\n):\n    def __init__(\n        self,\n        model: Model,\n        ref_point: list[float] | Tensor,\n        X_baseline: Tensor,\n        sampler: MCSampler | None = None,\n        objective: MCMultiOutputObjective | None = None,\n        constraints: list[Callable[[Tensor], Tensor]] | None = None,\n        X_pending: Tensor | None = None,\n        eta: Tensor | float = 1e-3,\n        fat: bool = False,\n        prune_baseline: bool = False,\n        alpha: float = 0.0,\n        cache_pending: bool = True,\n        max_iep: int = 0,\n        incremental_nehvi: bool = True,\n        cache_root: bool = True,\n        marginalize_dim: int | None = None,\n    ) -> None:\n        legacy_ei_numerics_warning(legacy_name=type(self).__name__)\n        MultiObjectiveMCAcquisitionFunction.__init__(\n            self,\n            model=model,\n            sampler=sampler,\n            objective=objective,\n            constraints=constraints,\n            eta=eta,\n        )\n        SubsetIndexCachingMixin.__init__(self)\n        NoisyExpectedHypervolumeMixin.__init__(\n            self,\n            model=model,\n            ref_point=ref_point,\n            X_baseline=X_baseline,\n            sampler=self.sampler,\n            objective=self.objective,\n            constraints=self.constraints,\n            X_pending=X_pending,\n            prune_baseline=prune_baseline,\n            alpha=alpha,\n            cache_pending=cache_pending,\n            max_iep=max_iep,\n            incremental_nehvi=incremental_nehvi,\n            cache_root=cache_root,\n            marginalize_dim=marginalize_dim,\n        )\n        self.fat = fat\n\n    @t_batch_mode_transform()\n    @average_over_ensemble_models\n    def forward(self, X: Tensor) -> Tensor:\n        samples, X = self._compute_posterior_samples_and_concat_pending(X)\n        return self._compute_qehvi(samples=samples, X=X) + self._prev_nehvi\n\n\nclass qLogNoisyExpectedHypervolumeImprovement(\n    NoisyExpectedHypervolumeMixin,\n    qLogExpectedHypervolumeImprovement,\n):\n    _log: bool = True\n\n    def __init__(\n        self,\n        model: Model,\n        ref_point: list[float] | Tensor,\n        X_baseline: Tensor,\n        sampler: MCSampler | None = None,\n        objective: MCMultiOutputObjective | None = None,\n        constraints: list[Callable[[Tensor], Tensor]] | None = None,\n        X_pending: Tensor | None = None,\n        eta: Tensor | float = 1e-3,\n        fat: bool = True,\n        prune_baseline: bool = False,\n        alpha: float = 0.0,\n        cache_pending: bool = True,\n        max_iep: int = 0,\n        incremental_nehvi: bool = True,\n        cache_root: bool = True,\n        tau_relu: float = 1e-6,\n        tau_max: float = 1e-3,\n        marginalize_dim: int | None = None,\n    ) -> None:\n        MultiObjectiveMCAcquisitionFunction.__init__(\n            self,\n            model=model,\n            sampler=sampler,\n            objective=objective,\n            constraints=constraints,\n            eta=eta,\n        )\n        SubsetIndexCachingMixin.__init__(self)\n        NoisyExpectedHypervolumeMixin.__init__(\n            self,\n            model=model,\n            ref_point=ref_point,\n            X_baseline=X_baseline,\n            sampler=self.sampler,\n            objective=self.objective,\n            constraints=self.constraints,\n            X_pending=X_pending,\n            prune_baseline=prune_baseline,\n            alpha=alpha,\n            cache_pending=cache_pending,\n            max_iep=max_iep,\n            incremental_nehvi=incremental_nehvi,\n            cache_root=cache_root,\n            marginalize_dim=marginalize_dim,\n        )\n        self.tau_relu = tau_relu\n        self.tau_max = tau_max\n        self.fat = fat\n\n    @t_batch_mode_transform()\n    @average_over_ensemble_models\n    def forward(self, X: Tensor) -> Tensor:\n        samples, X = self._compute_posterior_samples_and_concat_pending(X)\n        nehvi = self._compute_log_qehvi(samples=samples, X=X)\n        if self.incremental_nehvi:\n            return nehvi\n        return logplusexp(nehvi, self._prev_nehvi.log())\n\n\ndef gen_candidates_scipy(\n    initial_conditions: Tensor,\n    acquisition_function: AcquisitionFunction,\n    lower_bounds: float | Tensor | None = None,\n    upper_bounds: float | Tensor | None = None,\n    inequality_constraints: list[tuple[Tensor, Tensor, float]] | None = None,\n    equality_constraints: list[tuple[Tensor, Tensor, float]] | None = None,\n    nonlinear_inequality_constraints: list[tuple[Callable, bool]] | None = None,\n    options: dict[str, Any] | None = None,\n    fixed_features: Mapping[int, float | Tensor] | None = None,\n    timeout_sec: float | None = None,\n    use_parallel_mode: bool | None = None,\n) -> tuple[Tensor, Tensor]:\n    options = options or {}\n    options = {**options, \"maxiter\": options.get(\"maxiter\", 2000)}\n\n    original_initial_conditions_shape = initial_conditions.shape\n\n    if initial_conditions.ndim == 2:\n        initial_conditions = initial_conditions.unsqueeze(0)\n\n    initial_conditions_all_features = initial_conditions\n    if fixed_features:\n        initial_conditions = initial_conditions[\n            ...,\n            [i for i in range(initial_conditions.shape[-1]) if i not in fixed_features],\n        ]\n        if isinstance(lower_bounds, Tensor):\n            lower_bounds = lower_bounds[\n                [i for i in range(len(lower_bounds)) if i not in fixed_features]\n            ]\n        if isinstance(upper_bounds, Tensor):\n            upper_bounds = upper_bounds[\n                [i for i in range(len(upper_bounds)) if i not in fixed_features]\n            ]\n\n    clamped_candidates = columnwise_clamp(\n        X=initial_conditions,\n        lower=lower_bounds,\n        upper=upper_bounds,\n        raise_on_violation=True,\n    )\n\n    def f(x):\n        return -acquisition_function(x)\n\n    is_constrained = (\n        nonlinear_inequality_constraints\n        or equality_constraints\n        or inequality_constraints\n    )\n    method = options.get(\"method\", \"SLSQP\" if is_constrained else \"L-BFGS-B\")\n    with_grad = options.get(\"with_grad\", True)\n    minimize_options = {\n        k: v\n        for k, v in options.items()\n        if k\n        not in [\n            \"method\",\n            \"callback\",\n            \"with_grad\",\n            \"max_optimization_problem_aggregation_size\",\n        ]\n    }\n\n    why_not_fast_path = get_reasons_against_fast_path(\n        method=method,\n        with_grad=with_grad,\n        minimize_options=minimize_options,\n        timeout_sec=timeout_sec,\n    )\n\n    f_np_wrapper = _get_f_np_wrapper(\n        clamped_candidates.shape,\n        initial_conditions.device,\n        initial_conditions.dtype,\n        with_grad,\n    )\n\n    if not why_not_fast_path and use_parallel_mode is not False:\n        if is_constrained:\n            raise RuntimeWarning(\"Method L-BFGS-B cannot handle constraints.\")\n\n        batched_x0 = _arrayify(clamped_candidates).reshape(len(clamped_candidates), -1)\n\n        l_bfgs_b_bounds = translate_bounds_for_lbfgsb(\n            lower_bounds=lower_bounds,\n            upper_bounds=upper_bounds,\n            num_features=clamped_candidates.shape[-1],\n            q=clamped_candidates.shape[1],\n        )\n\n        with threadpool_limits(limits=1, user_api=\"blas\"):\n            xs, fs, results = fmin_l_bfgs_b_batched(\n                func=partial(f_np_wrapper, f=f, fixed_features=fixed_features),\n                x0=batched_x0,\n                bounds=l_bfgs_b_bounds,\n                callback=options.get(\"callback\", None),\n                pass_batch_indices=True,\n                **minimize_options,\n            )\n        for res in results:\n            _process_scipy_result(res=res, options=options)\n\n    else:\n        max_optimization_problem_aggregation_size = options.get(\n            \"max_optimization_problem_aggregation_size\", len(clamped_candidates)\n        )\n\n        if use_parallel_mode is not False:\n            msg = (\n                \"Not using the parallel implementation of l-bfgs-b, as: \"\n                + \", and \".join(why_not_fast_path)\n            )\n            if use_parallel_mode:\n                raise NotImplementedError(msg)\n            else:\n                logger.debug(msg)\n\n        if (\n            fixed_features\n            and any(\n                torch.is_tensor(ff) and ff.ndim > 0 for ff in fixed_features.values()\n            )\n            and max_optimization_problem_aggregation_size != 1\n        ):\n            raise UnsupportedError(\n                \"Batch shaped fixed features are not \"\n                \"supported, when optimizing more than one optimization \"\n                \"problem at a time.\"\n            )\n\n        all_xs = []\n        split_candidates = clamped_candidates.split(\n            max_optimization_problem_aggregation_size\n        )\n        for i, candidates_ in enumerate(split_candidates):\n            if fixed_features:\n                fixed_features_ = {\n                    k: ff[i : i + 1].item()\n                    if torch.is_tensor(ff) and ff.ndim > 0\n                    else ff\n                    for k, ff in fixed_features.items()\n                }\n            else:\n                fixed_features_ = None\n\n            _no_fixed_features = _remove_fixed_features_from_optimization(\n                fixed_features=fixed_features_,\n                acquisition_function=acquisition_function,\n                initial_conditions=None,\n                d=initial_conditions_all_features.shape[-1],\n                lower_bounds=None,\n                upper_bounds=None,\n                inequality_constraints=inequality_constraints,\n                equality_constraints=equality_constraints,\n                nonlinear_inequality_constraints=nonlinear_inequality_constraints,\n            )\n            bounds = make_scipy_bounds(\n                X=candidates_,\n                lower_bounds=lower_bounds,\n                upper_bounds=upper_bounds,\n            )\n\n            f_np_wrapper_ = partial(\n                f_np_wrapper,\n                fixed_features=fixed_features_,\n            )\n\n            x0 = candidates_.flatten()\n\n            constraints = make_scipy_linear_constraints(\n                shapeX=candidates_.shape,\n                inequality_constraints=_no_fixed_features.inequality_constraints,\n                equality_constraints=_no_fixed_features.equality_constraints,\n            )\n            if _no_fixed_features.nonlinear_inequality_constraints:\n                if not (len(candidates_.shape) == 3 and candidates_.shape[0] == 1):\n                    raise ValueError(\n                        \"`batch_limit` must be 1 when non-linear inequality \"\n                        \"constraints are given.\"\n                    )\n                nl_ineq_constraints = (\n                    _no_fixed_features.nonlinear_inequality_constraints\n                )\n                constraints += make_scipy_nonlinear_inequality_constraints(\n                    nonlinear_inequality_constraints=nl_ineq_constraints,\n                    f_np_wrapper=f_np_wrapper_,\n                    x0=x0,\n                    shapeX=candidates_.shape,\n                )\n\n            x0 = _arrayify(x0)\n\n            res = minimize_with_timeout(\n                fun=f_np_wrapper_,\n                args=(f,),\n                x0=x0,\n                method=method,\n                jac=with_grad,\n                bounds=bounds,\n                constraints=constraints,\n                callback=options.get(\"callback\", None),\n                options=minimize_options,\n                timeout_sec=timeout_sec / len(split_candidates)\n                if timeout_sec is not None\n                else None,\n            )\n            _process_scipy_result(res=res, options=options)\n            xs = res.x.reshape(candidates_.shape)\n            all_xs.append(xs)\n        xs = np.concatenate(all_xs)\n\n    candidates = torch.from_numpy(xs).view_as(clamped_candidates).to(initial_conditions)\n\n    clamped_candidates = columnwise_clamp(\n        X=candidates, lower=lower_bounds, upper=upper_bounds, raise_on_violation=True\n    )\n\n    clamped_candidates = fix_features(\n        X=clamped_candidates,\n        fixed_features=fixed_features,\n        replace_current_value=False,\n    )\n    clamped_candidates = clamped_candidates.reshape(original_initial_conditions_shape)\n\n    with torch.no_grad():\n        batch_acquisition = acquisition_function(clamped_candidates)\n\n    return clamped_candidates, batch_acquisition\n\n\ndef _get_f_np_wrapper(shapeX, device, dtype, with_grad):\n    if with_grad:\n\n        def f_np_wrapper(\n            x: npt.NDArray,\n            f: Callable,\n            fixed_features: Mapping[int, float | Tensor] | None,\n            batch_indices: list[int] | None = None,\n        ) -> tuple[float | npt.NDArray, npt.NDArray]:\n            if np.isnan(x).any():\n                raise RuntimeError(\n                    f\"{np.isnan(x).sum()} elements of the {x.size} element \"\n                    f\"`x` are NaN.\"\n                )\n            X = (\n                torch.from_numpy(x)\n                .to(device=device, dtype=dtype)\n                .view(-1, *shapeX[1:])\n                .contiguous()\n                .requires_grad_(True)\n            )\n            if fixed_features is not None:\n                if batch_indices is not None:\n                    this_fixed_features = {\n                        k: ff[batch_indices]\n                        if torch.is_tensor(ff) and ff.ndim > 0\n                        else ff\n                        for k, ff in fixed_features.items()\n                    }\n                else:\n                    this_fixed_features = fixed_features\n            else:\n                this_fixed_features = None\n\n            X_fix = fix_features(\n                X, fixed_features=this_fixed_features, replace_current_value=False\n            )\n            losses = f(X_fix)\n            loss = losses.sum()\n            gradf = _arrayify(torch.autograd.grad(loss, X)[0].contiguous().view(-1))\n            gradf = gradf.reshape(*x.shape)\n            if np.isnan(gradf).any():\n                msg = (\n                    f\"{np.isnan(gradf).sum()} elements of the {x.size} element \"\n                    \"gradient array `gradf` are NaN. \"\n                    \"This often indicates numerical issues.\"\n                )\n                if x.dtype != torch.double:\n                    msg += \" Consider using `dtype=torch.double`.\"\n                raise OptimizationGradientError(msg, current_x=x)\n            fval = (\n                losses.detach().view(-1).cpu().numpy()\n                if batch_indices is not None\n                else loss.detach().item()\n            )\n            return fval, gradf\n\n    else:\n\n        def f_np_wrapper(\n            x: npt.NDArray, f: Callable, fixed_features: dict[int, float] | None\n        ):\n            X = (\n                torch.from_numpy(x)\n                .to(device=device, dtype=dtype)\n                .view(-1, *shapeX[1:])\n                .contiguous()\n            )\n            with torch.no_grad():\n                X_fix = fix_features(\n                    X=X, fixed_features=fixed_features, replace_current_value=False\n                )\n                loss = f(X_fix).sum()\n            fval = loss.detach().item()\n            return fval\n\n    return f_np_wrapper\n\ndef _prune_inferior_shared_processing(\n    model: Model,\n    X: Tensor,\n    is_moo: bool,\n    objective: MCAcquisitionObjective | None = None,\n    posterior_transform: PosteriorTransform | None = None,\n    constraints: list[Callable[[Tensor], Tensor]] | None = None,\n    num_samples: int = 2048,\n    max_frac: float = 1.0,\n    sampler: MCSampler | None = None,\n    marginalize_dim: int | None = None,\n) -> tuple[int, Tensor, Tensor]:\n    func_name = (\n        \"prune_inferior_points_multi_objective\" if is_moo else \"prune_inferior_points\"\n    )\n    if marginalize_dim is None and is_ensemble(model):\n        marginalize_dim = MCMC_DIM\n\n    if X.ndim > 2:\n        raise UnsupportedError(\n            f\"Batched inputs `X` are currently unsupported by `{func_name}`\"\n        )\n    if X.size(-2) == 0:\n        raise ValueError(\"X must have at least one point.\")\n    if max_frac <= 0 or max_frac > 1.0:\n        raise ValueError(f\"max_frac must take values in (0, 1], is {max_frac}\")\n    max_points = math.ceil(max_frac * X.size(-2))\n    with torch.no_grad():\n        posterior = model.posterior(X=X, posterior_transform=posterior_transform)\n    if sampler is None:\n        sampler = get_sampler(\n            posterior=posterior, sample_shape=torch.Size([num_samples])\n        )\n    samples = sampler(posterior)\n    if objective is not None:\n        obj_vals = objective(samples=samples, X=X)\n    elif is_moo:\n        obj_vals = samples\n    else:\n        obj_vals = samples.squeeze(-1)\n    if obj_vals.ndim > (2 + is_moo):\n        if obj_vals.ndim == (3 + is_moo) and marginalize_dim is not None:\n            if marginalize_dim < 0:\n                marginalize_dim = (not is_moo) + none_throws(\n                    normalize_indices([marginalize_dim], d=obj_vals.ndim)\n                )[0]\n            obj_vals = obj_vals.mean(dim=marginalize_dim)\n        else:\n            raise UnsupportedError(\n                \"Models with multiple batch dims are currently unsupported by \"\n                f\"`{func_name}`.\"\n            )\n    infeas = ~compute_feasibility_indicator(\n        constraints=constraints,\n        samples=samples,\n        marginalize_dim=marginalize_dim,\n    )\n    return max_points, obj_vals, infeas\n\ndef compute_best_feasible_objective(\n    samples: Tensor,\n    obj: Tensor,\n    constraints: list[Callable[[Tensor], Tensor]] | None,\n    model: Model | None = None,\n    objective: MCAcquisitionObjective | None = None,\n    posterior_transform: PosteriorTransform | None = None,\n    X_baseline: Tensor | None = None,\n    infeasible_obj: Tensor | None = None,\n) -> Tensor:\n    if constraints is None:\n        with torch.no_grad():\n            return obj.amax(dim=-1, keepdim=False)\n\n    is_feasible = compute_feasibility_indicator(\n        constraints=constraints, samples=samples\n    )\n\n    if is_feasible.any(dim=-1).all():\n        infeasible_value = -torch.inf\n\n    elif infeasible_obj is not None:\n        infeasible_value = infeasible_obj.item()\n\n    else:\n        if model is None:\n            raise ValueError(\n                \"Must specify `model` when no feasible observation exists.\"\n            )\n        if X_baseline is None:\n            raise ValueError(\n                \"Must specify `X_baseline` when no feasible observation exists.\"\n            )\n        infeasible_value = _estimate_objective_lower_bound(\n            model=model,\n            objective=objective,\n            posterior_transform=posterior_transform,\n            X=X_baseline,\n        ).item()\n\n    is_feasible = repeat_to_match_aug_dim(\n        target_tensor=is_feasible, reference_tensor=obj\n    )\n    obj = torch.where(is_feasible, obj, infeasible_value)\n    with torch.no_grad():\n        return obj.amax(dim=-1, keepdim=False)\n\nclass qLogNoisyExpectedHypervolumeImprovement(\n    NoisyExpectedHypervolumeMixin,\n    qLogExpectedHypervolumeImprovement,\n):\n    _log: bool = True\n\n    def __init__(\n        self,\n        model: Model,\n        ref_point: list[float] | Tensor,\n        X_baseline: Tensor,\n        sampler: MCSampler | None = None,\n        objective: MCMultiOutputObjective | None = None,\n        constraints: list[Callable[[Tensor], Tensor]] | None = None,\n        X_pending: Tensor | None = None,\n        eta: Tensor | float = 1e-3,\n        fat: bool = True,\n        prune_baseline: bool = False,\n        alpha: float = 0.0,\n        cache_pending: bool = True,\n        max_iep: int = 0,\n        incremental_nehvi: bool = True,\n        cache_root: bool = True,\n        tau_relu: float = 1e-6,\n        tau_max: float = 1e-3,\n        marginalize_dim: int | None = None,\n    ) -> None:\n        MultiObjectiveMCAcquisitionFunction.__init__(\n            self,\n            model=model,\n            sampler=sampler,\n            objective=objective,\n            constraints=constraints,\n            eta=eta,\n        )\n        SubsetIndexCachingMixin.__init__(self)\n        NoisyExpectedHypervolumeMixin.__init__(\n            self,\n            model=model,\n            ref_point=ref_point,\n            X_baseline=X_baseline,\n            sampler=self.sampler,\n            objective=self.objective,\n            constraints=self.constraints,\n            X_pending=X_pending,\n            prune_baseline=prune_baseline,\n            alpha=alpha,\n            cache_pending=cache_pending,\n            max_iep=max_iep,\n            incremental_nehvi=incremental_nehvi,\n            cache_root=cache_root,\n            marginalize_dim=marginalize_dim,\n        )\n        self.tau_relu = tau_relu\n        self.tau_max = tau_max\n        self.fat = fat\n\n    @t_batch_mode_transform()\n    @average_over_ensemble_models\n    def forward(self, X: Tensor) -> Tensor:\n        samples, X = self._compute_posterior_samples_and_concat_pending(X)\n        nehvi = self._compute_log_qehvi(samples=samples, X=X)\n        if self.incremental_nehvi:\n            return nehvi\n        return logplusexp(nehvi, self._prev_nehvi.log())",
    "Experiment Result": "The methodology computes Hypervolume Improvement (HVI) for `q` candidate points using box decompositions and the inclusion-exclusion principle. Expected HVI (EHVI) is estimated via Monte-Carlo (MC) integration, drawing samples from the joint posterior of a Gaussian Process surrogate model. Variance reduction for MC integration is achieved through randomized quasi-Monte Carlo methods, with `SobolQMCNormalSampler` being a common implementation. Exact gradients of the MC-estimator are computed using auto-differentiation (e.g., `torch.autograd.grad`) and the re-parameterization trick, which is implicit in the use of `rsample` on the posterior. The optimization is performed within a Sample Average Approximation (SAA) framework, utilizing deterministic, higher-order optimizers such as L-BFGS-B (in `gen_candidates_scipy`) or Adam (in `gen_candidates_torch`). Constraints are incorporated by feasibility-weighting on the sample level using a differentiable sigmoid approximation for the indicator function, controlled by the `eta` temperature parameter (e.g., `1e-3` or `1e-2`) and potentially a `fat` parameter for logarithmic/linear asymptotic behavior. The approach supports both joint batch optimization (default `forward` behavior) and sequential greedy optimization strategies (via `incremental_nehvi` parameter in `qNoisyExpectedHypervolumeImprovement` variants)."
}{
    "Title": "Multi-fidelity Bayesian Optimization with Max-value Entropy Search and its Parallelization",
    "Main Contributions": "The paper introduces Multi-Fidelity Max-value Entropy Search (MF-MES), an information-theoretic efficient Multi-Fidelity Bayesian Optimization (MFBO) method. It addresses the computational difficulty of estimating information gain in existing information-based MFBO by leveraging Max-value Entropy Search (MES), which considers the entropy of the optimal function value (f*) instead of the optimal input point (x*). This approach reduces most additional computations to analytical expressions and one-dimensional numerical integrations. Furthermore, the paper proposes an information-theoretic asynchronous parallelization of MF-MES, which efficiently handles multiple queries with varying sampling costs. The effectiveness of MF-MES is demonstrated on benchmark datasets and a real-world materials science application.",
    "Methodology": "The proposed MF-MES method for sequential querying defines an acquisition function as the mutual information between the optimal value of the highest fidelity function (f*) and an observation of an arbitrary fidelity (f(m)x), divided by its querying cost λ(m). The mutual information is computed as the difference of entropies: H(f(m)x | Dt) - E[H(f(m)x | f*, Dt)]. The first term is analytical. The expectation in the second term is approximated via Monte Carlo sampling of f* from the current Gaussian Process Regression (GPR). For the highest fidelity (m=M), H(f(M)x | f*, Dt) is derived from a truncated normal distribution. For lower fidelities (m≠M), H(f(m)x | f*, Dt) is obtained through an efficient and accurate one-dimensional numerical integral, facilitated by Lemma 3.1. The underlying GPR model uses a multi-fidelity extension like Semiparametric Latent Factor Model (SLFM). For asynchronous parallelization, the acquisition function extends MF-MES to condition on fQ (values of objective functions currently being evaluated by other workers), which also reduces to at most two-dimensional integrals with parts calculable via one-dimensional numerical integration (Lemma 3.2). Sampling of f* (and its parallel counterpart ~f*) is performed using Gumbel distribution or Random Feature Maps (RFM).",
    "Experimental Setup": "The performance of MF-MES was evaluated using Simple Regret (SR) and Inference Regret (IR) against the total cost. The Multi-Fidelity Gaussian Process Regression (MF-GPR) model employed SLFM with a Gaussian kernel and Automatic Relevance Determination (ARD). Initial observations were generated using a Latin hypercube approach. Three types of datasets were used: a GP-based synthetic function (3D, 2 fidelities), two benchmark functions (Styblinski-Tang with 2 fidelities and HartMann6 with 3 fidelities), and a real-world materials science dataset (parameter optimization with 3 fidelities). Sampling costs (λ(m)) were defined for each fidelity level. For sequential querying, MF-MES was compared against MF-SKO, BOCA, MF-PES, and single-fidelity MES. For parallel querying, it was compared against MES with local penalization (MES-LP), GP-UCB with pure exploration (GP-UCB-PE), asynchronous parallel Thompson sampling (AsyTS), sequential MF-MES, and a parallel extension of single-fidelity MES. Experiments were repeated 100 times for synthetic data and 10 times for other datasets.",
    "Limitations": "The method employs an approximation where the conditional distribution H(f(m)x | f*, Dt) is replaced by p(f(m)x | f(M)x ≤ f*, Dt), which is a common simplification in entropy-based BO methods. Monte Carlo estimation is used for the expectation over f*, though this is argued to be accurate due to its one-dimensional nature. The Gumbel approximation for f* sampling is based on an independent approximation of GPR. While the paper highlights MF-MES's computational simplicity compared to other methods, it does not explicitly discuss inherent limitations of its own approach besides these approximations. It also observes that noisy observations in real-world data can cause some instability in regret metrics, including for MF-MES.",
    "Future Research Directions": "Future research could explore and incorporate other reliable approximation techniques for further acceleration of MF-MES, especially for scenarios where the reliability of existing approximations (e.g., density approximations by normal distributions) is not fully understood. Further investigation into the multi-fidelity setting with a continuous 'fidelity feature' (FF) space Z, and robust adaptation of MF-MES in this context, could be a fruitful direction. Additionally, for synchronous parallel Bayesian optimization, developing more efficient or robust strategies for handling the computational burden of multivariate Gaussian CDFs, or alternative approximation methods for entropy calculation in that setting, remains an open area.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "Parallel Bayesian Optimization of Multiple Noisy Objectives with Expected Hypervolume Improvement",
    "Main Contributions": "The research addresses the challenging problem of optimizing multiple competing black-box objectives, particularly when observations are corrupted by noise and when large batches of candidates need to be evaluated in parallel. It proposes NEHVI (noisy expected hypervolume improvement), a novel acquisition function that applies a Bayesian treatment to the popular expected hypervolume improvement (EHVI) criterion by integrating over the uncertainty in the Pareto frontier, making it robust to observation noise. The paper also introduces qNEHVI, a parallel variant that reduces the computational complexity of parallel EHVI from exponential to polynomial with respect to batch size, and is one-step Bayes-optimal for hypervolume maximization in both noisy and noiseless environments. A key enabler is the general-purpose, differentiable, cached box decomposition (CBD) implementation, which dramatically speeds up critical computations. Empirically, qNEHVI is shown to be substantially more robust to noise than existing MOBO approaches, achieve state-of-the-art optimization performance, and maintain competitive wall-times in large-batch environments.",
    "Methodology": "The core methodology extends Expected Hypervolume Improvement (EHVI) to noisy, parallel settings. NEHVI is derived by iterating the expectation over the posterior distribution of function values at previously evaluated points, thereby integrating over the uncertainty in the Pareto frontier. Its parallel variant, qNEHVI, uses a sequential greedy approximation for batch selection. To address computational complexity, a Cached Box Decomposition (CBD) technique is introduced where Pareto frontiers and box decompositions are computed once for fixed posterior samples and cached for the duration of acquisition function optimization. This allows efficient conditional posterior sampling for new candidates using low-rank updates and the reparameterization trick. The acquisition function (ˆαNEHVI) is differentiable, enabling efficient gradient-based optimization with methods like L-BFGS-B, and theoretical convergence guarantees are provided under Sample Average Approximation (SAA). A cheaper approximation, qNEHVI-1, utilizes a single approximate Gaussian Process sample path via Random Fourier Features (RFFs) for faster optimization.",
    "Experimental Setup": "The methods were empirically evaluated on a variety of synthetic and real-world benchmark problems. Synthetic problems included noisy variants of BraninCurrin (M=2, d=2) and DTLZ2 (M=2, d=6), with observations corrupted by zero-mean additive Gaussian noise (5% or 10% of objective range). Real-world applications comprised Adaptive Bitrate (ABR) Control Policy Optimization (M=2, d=4), Vehicle Design Optimization (M=3, d=5), AutoML (M=2, d=8), CarSideImpact (M=4, d=7), Constrained BraninCurrin (M=2, V=2, d=2), and SphereEllipsoidal (M=2, d=5). Comparison was made against several baselines: PESMO, MESMO (extended for noise), PFES, DGEMO, MOEA/D-EGO, TSEMO, TS-TCH, qEHVI (and qEHVI-PM-CBD), qNParEGO, qParEGO, and COMO-CMA-ES. All methods were initialized with 2(d+1) points from a scrambled Sobol sequence. Multi-start L-BFGS-B with exact gradients (or finite differences for PFES) was used for optimization. Performance was measured by the logarithm of the difference in hypervolume between the true Pareto frontier and the approximate Pareto frontier recovered, computing hypervolume dominated by the noiseless Pareto frontier over observed points.",
    "Limitations": "The study acknowledges limitations, including the poor performance of information-theoretic acquisition functions on the benchmarks, though these could potentially offer decoupled objective evaluations for resource-intensive scenarios with improved optimization. The exact hypervolume computation still exhibits super-polynomial complexity with respect to the number of objectives, which could pose a challenge for problems with a very high number of objectives, despite the CBD approach improving complexity for batch size. The performance of the qNEHVI-1 approximation, which relies on Random Fourier Features, was observed to degrade in higher dimensional search spaces, suggesting a need for further investigation into the impact of the number of Fourier basis functions.",
    "Future Research Directions": "Future research directions include investigating improved optimization procedures for information-theoretic acquisition functions to enhance their performance and provide principled approaches for selecting evaluation sources on a budget. Combining qNEHVI with differentiable approximate methods for computing hypervolume, such as those proposed by Couckuyt et al. [9] or Golovin & Zhang [23], could lead to further speed-ups. Additionally, further study is needed on how the number of Fourier basis functions affects the performance of qNEHVI-1 in high-dimensional search spaces. The authors also suggest exploring the extension of DGEMO's graph cut algorithm to account for uncertainty in the in-sample Pareto frontier by generating first-order approximations under different sample paths, while addressing the setting of diversity constraints. More broadly, the core ideas and the CBD approach are hoped to provide a framework for developing new computationally efficient MOBO methods.",
    "Experiment Code": "class qExpectedHypervolumeImprovement(MultiObjectiveMCAcquisitionFunction, SubsetIndexCachingMixin):\n    def __init__(\n        self,\n        model: Model,\n        ref_point: list[float] | Tensor,\n        partitioning: NondominatedPartitioning,\n        sampler: MCSampler | None = None,\n        objective: MCMultiOutputObjective | None = None,\n        constraints: list[Callable[[Tensor], Tensor]] | None = None,\n        X_pending: Tensor | None = None,\n        eta: Tensor | float = 1e-3,\n        fat: bool = False,\n    ) -> None:\n        r\"\"\"q-Expected Hypervolume Improvement supporting m>=2 outcomes.\n\n        See [Daulton2020qehvi]_ for details.\n\n        Example:\n            >>> model = SingleTaskGP(train_X, train_Y)\n            >>> ref_point = [0.0, 0.0]\n            >>> qEHVI = qExpectedHypervolumeImprovement(model, ref_point, partitioning)\n            >>> qehvi = qEHVI(test_X)\n\n        Args:\n            model: A fitted model.\n            ref_point: A list or tensor with `m` elements representing the reference\n                point (in the outcome space) w.r.t. to which compute the hypervolume.\n                This is a reference point for the objective values (i.e. after\n                applying`objective` to the samples).\n            partitioning: A `NondominatedPartitioning` module that provides the non-\n                dominated front and a partitioning of the non-dominated space in hyper-\n                rectangles. If constraints are present, this partitioning must only\n                include feasible points.\n            sampler: The sampler used to draw base samples. If not given,\n                a sampler is generated using `get_sampler`.\n            objective: The MCMultiOutputObjective under which the samples are evaluated.\n                Defaults to `IdentityMCMultiOutputObjective()`.\n            constraints: A list of callables, each mapping a Tensor of dimension\n                `sample_shape x batch-shape x q x m` to a Tensor of dimension\n                `sample_shape x batch-shape x q`, where negative values imply\n                feasibility. The acquisition function will compute expected feasible\n                hypervolume.\n            X_pending: A `batch_shape x m x d`-dim Tensor of `m` design points that have\n                points that have been submitted for function evaluation but have not yet\n                been evaluated. Concatenated into `X` upon forward call. Copied and set\n                to have no gradient.\n            eta: The temperature parameter for the sigmoid function used for the\n                differentiable approximation of the constraints. In case of a float the\n                same eta is used for every constraint in constraints. In case of a\n                tensor the length of the tensor must match the number of provided\n                constraints. The i-th constraint is then estimated with the i-th\n                eta value.\n            fat: A Boolean flag indicating whether to use the heavy-tailed approximation\n                of the constraint indicator.\n        \"\"\"\n        legacy_ei_numerics_warning(legacy_name=type(self).__name__)\n        if len(ref_point) != partitioning.num_outcomes:\n            raise ValueError(\n                \"The length of the reference point must match the number of outcomes. \"\n                f\"Got ref_point with {len(ref_point)} elements, but expected \"\n                f\"{partitioning.num_outcomes}.\"\n            )\n        ref_point = torch.as_tensor(\n            ref_point,\n            dtype=partitioning.pareto_Y.dtype,\n            device=partitioning.pareto_Y.device,\n        )\n        super().__init__(\n            model=model,\n            sampler=sampler,\n            objective=objective,\n            constraints=constraints,\n            eta=eta,\n            X_pending=X_pending,\n        )\n        self.register_buffer(\"ref_point\", ref_point)\n        cell_bounds = partitioning.get_hypercell_bounds()\n        self.register_buffer(\"cell_lower_bounds\", cell_bounds[0])\n        self.register_buffer(\"cell_upper_bounds\", cell_bounds[1])\n        SubsetIndexCachingMixin.__init__(self)\n        self.fat = fat\n\n    def _compute_qehvi(self, samples: Tensor, X: Tensor | None = None) -> Tensor:\n        r\"\"\"Compute the expected (feasible) hypervolume improvement given MC samples.\n\n        Args:\n            samples: A `n_samples x batch_shape x q' x m`-dim tensor of samples.\n            X: A `batch_shape x q x d`-dim tensor of inputs.\n\n        Returns:\n            A `batch_shape x (model_batch_shape)`-dim tensor of expected hypervolume\n            improvement for each batch.\n        \"\"\"\n        # Note that the objective may subset the outcomes (e.g. this will usually happen\n        # if there are constraints present).\n        obj = self.objective(samples, X=X)\n        q = obj.shape[-2]\n        if self.constraints is not None:\n            feas_weights = compute_smoothed_feasibility_indicator(\n                constraints=self.constraints,\n                samples=samples,\n                eta=self.eta,\n                fat=self.fat,\n            )  # `sample_shape x batch-shape x q`\n        device = self.ref_point.device\n        q_subset_indices = self.compute_q_subset_indices(q_out=q, device=device)\n        batch_shape = obj.shape[:-2]\n        # this is n_samples x input_batch_shape x\n        areas_per_segment = torch.zeros(\n            *batch_shape,\n            self.cell_lower_bounds.shape[-2],\n            dtype=obj.dtype,\n            device=device,\n        )\n        cell_batch_ndim = self.cell_lower_bounds.ndim - 2\n        sample_batch_view_shape = torch.Size(\n            [\n                batch_shape[0] if cell_batch_ndim > 0 else 1,\n                *[1 for _ in range(len(batch_shape) - max(cell_batch_ndim, 1))],\n                *self.cell_lower_bounds.shape[1:-2],\n            ]\n        )\n        view_shape = (\n            *sample_batch_view_shape,\n            self.cell_upper_bounds.shape[-2],\n            1,\n            self.cell_upper_bounds.shape[-1],\n        )\n        for i in range(1, self.q_out + 1):\n            # TODO: we could use batches to compute (q choose i) and (q choose q-i)\n            # simultaneously since subsets of size i and q-i have the same number of\n            # elements. This would decrease the number of iterations, but increase\n            # memory usage.\n            q_choose_i = q_subset_indices[f\"q_choose_{i}\"]\n            # this tensor is mc_samples x batch_shape x i x q_choose_i x m\n            obj_subsets = obj.index_select(dim=-2, index=q_choose_i.view(-1))\n            obj_subsets = obj_subsets.view(\n                obj.shape[:-2] + q_choose_i.shape + obj.shape[-1:]\n            )\n            # since all hyperrectangles share one vertex, the opposite vertex of the\n            # overlap is given by the component-wise minimum.\n            # take the minimum in each subset\n            overlap_vertices = obj_subsets.min(dim=-2).values\n            # add batch-dim to compute area for each segment (pseudo-pareto-vertex)\n            # this tensor is mc_samples x batch_shape x num_cells x q_choose_i x m\n            overlap_vertices = torch.min(\n                overlap_vertices.unsqueeze(-3), self.cell_upper_bounds.view(view_shape)\n            )\n            # subtract cell lower bounds, clamp min at zero\n            lengths_i = (\n                overlap_vertices - self.cell_lower_bounds.view(view_shape)\n            ).clamp_min(0.0)\n            # take product over hyperrectangle side lengths to compute area\n            # sum over all subsets of size i\n            areas_i = lengths_i.prod(dim=-1)\n            # if constraints are present, apply a differentiable approximation of\n            # the indicator function\n            if self.constraints is not None:\n                feas_subsets = feas_weights.index_select(\n                    dim=-1, index=q_choose_i.view(-1)\n                ).view(feas_weights.shape[:-1] + q_choose_i.shape)\n                areas_i = areas_i * feas_subsets.unsqueeze(-3).prod(dim=-1)\n            areas_i = areas_i.sum(dim=-1)\n            # Using the inclusion-exclusion principle, set the sign to be positive\n            # for subsets of odd sizes and negative for subsets of even size\n            areas_per_segment += (-1) ** (i + 1) * areas_i\n        # sum over segments and average over MC samples\n        return areas_per_segment.sum(dim=-1).mean(dim=0)\n\n    @concatenate_pending_points\n    @t_batch_mode_transform()\n    @average_over_ensemble_models\n    def forward(self, X: Tensor) -> Tensor:\n        posterior = self.model.posterior(X)\n        samples = self.get_posterior_samples(posterior)\n        return self._compute_qehvi(samples=samples, X=X)\n\n\nclass qNoisyExpectedHypervolumeImprovement(\n    NoisyExpectedHypervolumeMixin, qExpectedHypervolumeImprovement\n):\n    def __init__(\n        self,\n        model: Model,\n        ref_point: list[float] | Tensor,\n        X_baseline: Tensor,\n        sampler: MCSampler | None = None,\n        objective: MCMultiOutputObjective | None = None,\n        constraints: list[Callable[[Tensor], Tensor]] | None = None,\n        X_pending: Tensor | None = None,\n        eta: Tensor | float = 1e-3,\n        fat: bool = False,\n        prune_baseline: bool = False,\n        alpha: float = 0.0,\n        cache_pending: bool = True,\n        max_iep: int = 0,\n        incremental_nehvi: bool = True,\n        cache_root: bool = True,\n        marginalize_dim: int | None = None,\n    ) -> None:\n        r\"\"\"q-Noisy Expected Hypervolume Improvement supporting m>=2 outcomes.\n\n        See [Daulton2021nehvi]_ for details.\n\n        Example:\n            >>> model = SingleTaskGP(train_X, train_Y)\n            >>> ref_point = [0.0, 0.0]\n            >>> qNEHVI = qNoisyExpectedHypervolumeImprovement(model, ref_point, train_X)\n            >>> qnehvi = qNEHVI(test_X)\n\n        Args:\n            model: A fitted model.\n            ref_point: A list or tensor with `m` elements representing the reference\n                point (in the outcome space) w.r.t. to which compute the hypervolume.\n                This is a reference point for the objective values (i.e. after\n                applying `objective` to the samples).\n            X_baseline: A `r x d`-dim Tensor of `r` design points that have already\n                been observed. These points are considered as potential approximate\n                pareto-optimal design points.\n            sampler: The sampler used to draw base samples. If not given,\n                a sampler is generated using `get_sampler`.\n                Note: a pareto front is created for each mc sample, which can be\n                computationally intensive for `m` > 2.\n            objective: The MCMultiOutputObjective under which the samples are\n                evaluated. Defaults to `IdentityMCMultiOutputObjective()`.\n            constraints: A list of callables, each mapping a Tensor of dimension\n                `sample_shape x batch-shape x q x m` to a Tensor of dimension\n                `sample_shape x batch-shape x q`, where negative values imply\n                feasibility. The acquisition function will compute expected feasible\n                hypervolume.\n            X_pending: A `batch_shape x m x d`-dim Tensor of `m` design points that\n                have points that have been submitted for function evaluation, but\n                have not yet been evaluated.\n            eta: The temperature parameter for the sigmoid function used for the\n                differentiable approximation of the constraints. In case of a float the\n                same `eta` is used for every constraint in constraints. In case of a\n                tensor the length of the tensor must match the number of provided\n                constraints. The i-th constraint is then estimated with the i-th\n                `eta` value. For more details, on this parameter, see the docs of\n                `compute_smoothed_feasibility_indicator`.\n            fat: A Boolean flag indicating whether to use the heavy-tailed approximation\n                of the constraint indicator.\n            prune_baseline: If True, remove points in `X_baseline` that are\n                highly unlikely to be the pareto optimal and better than the\n                reference point. This can significantly improve computation time and\n                is generally recommended. In order to customize pruning parameters,\n                instead manually call `prune_inferior_points_multi_objective` on\n                `X_baseline` before instantiating the acquisition function.\n            alpha: The hyperparameter controlling the approximate non-dominated\n                partitioning. The default value of 0.0 means an exact partitioning\n                is used. As the number of objectives `m` increases, consider increasing\n                this parameter in order to limit computational complexity.\n            cache_pending: A boolean indicating whether to use cached box\n                decompositions (CBD) for handling pending points. This is\n                generally recommended.\n            max_iep: The maximum number of pending points before the box\n                decompositions will be recomputed.\n            incremental_nehvi: A boolean indicating whether to compute the\n                incremental NEHVI from the `i`th point where `i=1, ..., q`\n                under sequential greedy optimization, or the full qNEHVI over\n                `q` points.\n            cache_root: A boolean indicating whether to cache the root\n                decomposition over `X_baseline` and use low-rank updates.\n            marginalize_dim: A batch dimension that should be marginalized. For example,\n                this is useful when using a batched fully Bayesian model.\n        \"\"\"\n        legacy_ei_numerics_warning(legacy_name=type(self).__name__)\n        MultiObjectiveMCAcquisitionFunction.__init__(\n            self,\n            model=model,\n            sampler=sampler,\n            objective=objective,\n            constraints=constraints,\n            eta=eta,\n        )\n        SubsetIndexCachingMixin.__init__(self)\n        NoisyExpectedHypervolumeMixin.__init__(\n            self,\n            model=model,\n            ref_point=ref_point,\n            X_baseline=X_baseline,\n            sampler=self.sampler,\n            objective=self.objective,\n            constraints=self.constraints,\n            X_pending=X_pending,\n            prune_baseline=prune_baseline,\n            alpha=alpha,\n            cache_pending=cache_pending,\n            max_iep=max_iep,\n            incremental_nehvi=incremental_nehvi,\n            cache_root=cache_root,\n            marginalize_dim=marginalize_dim,\n        )\n        self.fat = fat\n\n    @t_batch_mode_transform()\n    @average_over_ensemble_models\n    def forward(self, X: Tensor) -> Tensor:\n        # Get samples from the posterior, and manually concatenate pending points that\n        # have not yet been cached. Shared with qLogNEHVI.\n        samples, X = self._compute_posterior_samples_and_concat_pending(X)\n        # Add previous nehvi from pending points.\n        return self._compute_qehvi(samples=samples, X=X) + self._prev_nehvi\n\n\nclass qLogExpectedHypervolumeImprovement(\n    MultiObjectiveMCAcquisitionFunction, SubsetIndexCachingMixin\n):\n    _log: bool = True\n\n    def __init__(\n        self,\n        model: Model,\n        ref_point: list[float] | Tensor,\n        partitioning: NondominatedPartitioning,\n        sampler: MCSampler | None = None,\n        objective: MCMultiOutputObjective | None = None,\n        constraints: list[Callable[[Tensor], Tensor]] | None = None,\n        X_pending: Tensor | None = None,\n        eta: Tensor | float = 1e-2,\n        fat: bool = True,\n        tau_relu: float = TAU_RELU,\n        tau_max: float = TAU_MAX,\n    ) -> None:\n        r\"\"\"Parallel Log Expected Hypervolume Improvement supporting m>=2 outcomes.\n\n        See [Ament2023logei]_ for details and the methodology behind the LogEI family of\n        acquisition function. Line-by-line differences to the original differentiable\n        expected hypervolume formulation of [Daulton2020qehvi]_ are described via inline\n        comments in `forward`.\n\n        Example:\n            >>> model = SingleTaskGP(train_X, train_Y)\n            >>> ref_point = [0.0, 0.0]\n            >>> acq = qLogExpectedHypervolumeImprovement(model, ref_point, partitioning)\n            >>> value = acq(test_X)\n\n        Args:\n            model: A fitted model.\n            ref_point: A list or tensor with `m` elements representing the reference\n                point (in the outcome space) w.r.t. to which compute the hypervolume.\n                This is a reference point for the objective values (i.e. after\n                applying`objective` to the samples).\n            partitioning: A `NondominatedPartitioning` module that provides the non-\n                dominated front and a partitioning of the non-dominated space in hyper-\n                rectangles. If constraints are present, this partitioning must only\n                include feasible points.\n            sampler: The sampler used to draw base samples. If not given,\n                a sampler is generated using `get_sampler`.\n            objective: The MCMultiOutputObjective under which the samples are evaluated.\n                Defaults to `IdentityMultiOutputObjective()`.\n            constraints: A list of callables, each mapping a Tensor of dimension\n                `sample_shape x batch-shape x q x m` to a Tensor of dimension\n                `sample_shape x batch-shape x q`, where negative values imply\n                feasibility. The acquisition function will compute expected feasible\n                hypervolume.\n            X_pending: A `batch_shape x m x d`-dim Tensor of `m` design points that have\n                points that have been submitted for function evaluation but have not yet\n                been evaluated. Concatenated into `X` upon forward call. Copied and set\n                to have no gradient.\n            eta: The temperature parameter for the sigmoid function used for the\n                differentiable approximation of the constraints. In case of a float the\n                same eta is used for every constraint in constraints. In case of a\n                tensor the length of the tensor must match the number of provided\n                constraints. The i-th constraint is then estimated with the i-th\n                eta value.\n            fat: Toggles the logarithmic / linear asymptotic behavior of the smooth\n                approximation to the ReLU and the maximum.\n            tau_relu: Temperature parameter controlling the sharpness of the\n                approximation to the ReLU over the `q` candidate points. For further\n                details, see the comments above the definition of `TAU_RELU`.\n            tau_max: Temperature parameter controlling the sharpness of the\n                approximation to the `max` operator over the `q` candidate points.\n                For further details, see the comments above the definition of `TAU_MAX`.\n        \"\"\"\n        if len(ref_point) != partitioning.num_outcomes:\n            raise ValueError(\n                \"The dimensionality of the reference point must match the number of \"\n                f\"outcomes. Got ref_point with {len(ref_point)} elements, but expected \"\n                f\"{partitioning.num_outcomes}.\"\n            )\n        ref_point = torch.as_tensor(\n            ref_point,\n            dtype=partitioning.pareto_Y.dtype,\n            device=partitioning.pareto_Y.device,\n        )\n        super().__init__(\n            model=model,\n            sampler=sampler,\n            objective=objective,\n            constraints=constraints,\n            eta=eta,\n            X_pending=X_pending,\n        )\n        self.register_buffer(\"ref_point\", ref_point)\n        cell_bounds = partitioning.get_hypercell_bounds()\n        self.register_buffer(\"cell_lower_bounds\", cell_bounds[0])\n        self.register_buffer(\"cell_upper_bounds\", cell_bounds[1])\n        SubsetIndexCachingMixin.__init__(self)\n        self.tau_relu = tau_relu\n        self.tau_max = tau_max\n        self.fat = fat\n\n    def _compute_log_qehvi(self, samples: Tensor, X: Tensor | None = None) -> Tensor:\n        r\"\"\"Compute the expected (feasible) hypervolume improvement given MC samples.\n\n        Args:\n            samples: A `sample_shape x batch_shape x q' x m`-dim tensor of samples.\n            X: A `batch_shape x q x d`-dim tensor of inputs.\n\n        Returns:\n            A `batch_shape x (model_batch_shape)`-dim tensor of expected hypervolume\n            improvement for each batch.\n        \"\"\"\n        # Note that the objective may subset the outcomes (e.g. this will usually happen\n        # if there are constraints present).\n        obj = self.objective(samples, X=X)  # mc_samples x batch_shape x q x m\n        q = obj.shape[-2]\n        if self.constraints is not None:\n            log_feas_weights = compute_smoothed_feasibility_indicator(\n                constraints=self.constraints,\n                samples=samples,\n                eta=self.eta,\n                log=True,\n                fat=self.fat,\n            )\n        device = self.ref_point.device\n        q_subset_indices = self.compute_q_subset_indices(q_out=q, device=device)\n        batch_shape = obj.shape[:-2]  # mc_samples x batch_shape\n        # areas tensor is `mc_samples x batch_shape x num_cells x 2`-dim\n        log_areas_per_segment = torch.full(\n            size=(\n                *batch_shape,\n                self.cell_lower_bounds.shape[-2],  # num_cells\n                2,  # for even and odd terms\n            ),\n            fill_value=-torch.inf,\n            dtype=obj.dtype,\n            device=device,\n        )\n\n        cell_batch_ndim = self.cell_lower_bounds.ndim - 2\n        # conditionally adding mc_samples dim if cell_batch_ndim > 0\n        # adding ones to shape equal in number to to batch_shape_ndim - cell_batch_ndim\n        # adding cell_bounds batch shape w/o 1st dimension\n        sample_batch_view_shape = torch.Size(\n            [\n                batch_shape[0] if cell_batch_ndim > 0 else 1,\n                *[1 for _ in range(len(batch_shape) - max(cell_batch_ndim, 1))],\n                *self.cell_lower_bounds.shape[1:-2],\n            ]\n        )\n        view_shape = (\n            *sample_batch_view_shape,\n            self.cell_upper_bounds.shape[-2],  # num_cells\n            1,  # adding for q_choose_i dimension\n            self.cell_upper_bounds.shape[-1],  # num_objectives\n        )\n\n        for i in range(1, self.q_out + 1):\n            # TODO: we could use batches to compute (q choose i) and (q choose q-i)\n            # simultaneously since subsets of size i and q-i have the same number of\n            # elements. This would decrease the number of iterations, but increase\n            # memory usage.\n            q_choose_i = q_subset_indices[f\"q_choose_{i}\"]  # q_choose_i x i\n            # this tensor is mc_samples x batch_shape x i x q_choose_i x m\n            obj_subsets = obj.index_select(dim=-2, index=q_choose_i.view(-1))\n            obj_subsets = obj_subsets.view(\n                obj.shape[:-2] + q_choose_i.shape + obj.shape[-1:]\n            )  # mc_samples x batch_shape x q_choose_i x i x m\n\n            # NOTE: the order of operations in non-log _compute_qehvi is 3), 1), 2).\n            # since 3) moved above 1), _log_improvement adds another Tensor dimension\n            # that keeps track of num_cells.\n\n            # 1) computes log smoothed improvement over the cell lower bounds.\n            # mc_samples x batch_shape x num_cells x q_choose_i x i x m\n            log_improvement_i = self._log_improvement(obj_subsets, view_shape)\n\n            # 2) take the minimum log improvement over all i subsets.\n            # since all hyperrectangles share one vertex, the opposite vertex of the\n            # overlap is given by the component-wise minimum.\n            # negative of maximum of negative log_improvement is approximation to min.\n            log_improvement_i = self._smooth_min(\n                log_improvement_i,\n                dim=-2,\n            )  # mc_samples x batch_shape x num_cells x q_choose_i x m\n\n            # 3) compute the log lengths of the cells' sides.\n            # mc_samples x batch_shape x num_cells x q_choose_i x m\n            log_lengths_i = self._log_cell_lengths(log_improvement_i, view_shape)\n\n            # 4) take product over hyperrectangle side lengths to compute area (m-dim).\n            # after, log_areas_i is mc_samples x batch_shape x num_cells\n            log_areas_i = log_lengths_i.sum(dim=-1)  # areas_i = lengths_i.prod(dim=-1)\n\n            # 5) if constraints are present, apply a differentiable approximation of\n            # the indicator function.\n            if self.constraints is not None:\n                log_feas_subsets = log_feas_weights.index_select(\n                    dim=-1, index=q_choose_i.view(-1)\n                ).view(log_feas_weights.shape[:-1] + q_choose_i.shape)\n                log_areas_i = log_areas_i + log_feas_subsets.unsqueeze(-3).sum(dim=-1)\n\n            # 6) sum over all subsets of size i, i.e. reduce over q_choose_i-dim\n            # after, log_areas_i is mc_samples x batch_shape x num_cells\n            log_areas_i = logsumexp(log_areas_i, dim=-1)  # areas_i.sum(dim=-1)\n\n            # 7) Using the inclusion-exclusion principle, set the sign to be positive\n            # for subsets of odd sizes and negative for subsets of even size\n            # in non-log space: areas_per_segment += (-1) ** (i + 1) * areas_i,\n            # but here in log space, we need to keep track of sign:\n            log_areas_per_segment[..., i % 2] = logplusexp(\n                log_areas_per_segment[..., i % 2],\n                log_areas_i,\n            )\n\n        # 8) subtract even from odd log area terms\n        log_areas_per_segment = logdiffexp(\n            log_a=log_areas_per_segment[..., 0], log_b=log_areas_per_segment[..., 1]\n        )\n\n        # 9) sum over segments (n_cells-dim) and average over MC samples\n        return logmeanexp(logsumexp(log_areas_per_segment, dim=-1), dim=0)\n\n    def _log_improvement(\n        self, obj_subsets: Tensor, view_shape: tuple | torch.Size\n    ) -> Tensor:\n        # smooth out the clamp and take the log (previous step 3)\n        # subtract cell lower bounds, clamp min at zero, but first\n        # make obj_subsets broadcastable with cell bounds:\n        # mc_samples x batch_shape x (num_cells = 1) x q_choose_i x i x m\n        obj_subsets = obj_subsets.unsqueeze(-4)\n        # making cell bounds broadcastable with obj_subsets:\n        # (mc_samples = 1) x (batch_shape = 1) x num_cells x 1 x (i = 1) x m\n        cell_lower_bounds = self.cell_lower_bounds.view(view_shape).unsqueeze(-3)\n        Z = obj_subsets - cell_lower_bounds\n        log_Zi = self._log_smooth_relu(Z)\n        return log_Zi  # mc_samples x batch_shape x num_cells x q_choose_i x i x m\n\n    def _log_cell_lengths(\n        self, log_improvement_i: Tensor, view_shape: tuple | torch.Size\n    ) -> Tensor:\n        cell_upper_bounds = self.cell_upper_bounds.clamp_max(\n            1e10 if log_improvement_i.dtype == torch.double else 1e8\n        )  # num_cells x num_objectives\n        # add batch-dim to compute area for each segment (pseudo-pareto-vertex)\n        log_cell_lengths = (\n            (cell_upper_bounds - self.cell_lower_bounds).log().view(view_shape)\n        )  # (mc_samples = 1) x (batch_shape = 1) x n_cells x (q_choose_i = 1) x m\n        # mc_samples x batch_shape x num_cells x q_choose_i x m\n        return self._smooth_minimum(\n            log_improvement_i,\n            log_cell_lengths,\n        )\n\n    def _log_smooth_relu(self, X: Tensor) -> Tensor:\n        f = log_fatplus if self.fat else log_softplus\n        return f(X, tau=self.tau_relu)\n\n    def _smooth_min(self, X: Tensor, dim: int, keepdim: bool = False) -> Tensor:\n        f = fatmin if self.fat else smooth_amin\n        return f(X, tau=self.tau_max, dim=dim)\n\n    def _smooth_minimum(self, X: Tensor, Y: Tensor) -> Tensor:\n        XY = torch.stack(torch.broadcast_tensors(X, Y), dim=-1)\n        return self._smooth_min(XY, dim=-1, keepdim=False)\n\n    @concatenate_pending_points\n    @t_batch_mode_transform()\n    @average_over_ensemble_models\n    def forward(self, X: Tensor) -> Tensor:\n        posterior = self.model.posterior(X)\n        samples = self.get_posterior_samples(posterior)\n        return self._compute_log_qehvi(samples=samples, X=X)\n\n\nclass qLogNoisyExpectedHypervolumeImprovement(\n    NoisyExpectedHypervolumeMixin,\n    qLogExpectedHypervolumeImprovement,\n):\n    _log: bool = True\n\n    def __init__(\n        self,\n        model: Model,\n        ref_point: list[float] | Tensor,\n        X_baseline: Tensor,\n        sampler: MCSampler | None = None,\n        objective: MCMultiOutputObjective | None = None,\n        constraints: list[Callable[[Tensor], Tensor]] | None = None,\n        X_pending: Tensor | None = None,\n        eta: Tensor | float = 1e-3,\n        prune_baseline: bool = False,\n        alpha: float = 0.0,\n        cache_pending: bool = True,\n        max_iep: int = 0,\n        incremental_nehvi: bool = True,\n        cache_root: bool = True,\n        tau_relu: float = TAU_RELU,\n        tau_max: float = 1e-3,  # TAU_MAX,\n        fat: bool = True,\n        marginalize_dim: int | None = None,\n    ) -> None:\n        r\"\"\"\n        q-Log Noisy Expected Hypervolume Improvement supporting m>=2 outcomes.\n\n        Based on the differentiable hypervolume formulation of [Daulton2021nehvi]_.\n\n        Example:\n            >>> model = SingleTaskGP(train_X, train_Y)\n            >>> ref_point = [0.0, 0.0]\n            >>> qNEHVI = qNoisyExpectedHypervolumeImprovement(model, ref_point, train_X)\n            >>> qnehvi = qNEHVI(test_X)\n\n        Args:\n            model: A fitted model.\n            ref_point: A list or tensor with `m` elements representing the reference\n                point (in the outcome space) w.r.t. to which compute the hypervolume.\n                This is a reference point for the objective values (i.e. after\n                applying `objective` to the samples).\n            X_baseline: A `r x d`-dim Tensor of `r` design points that have already\n                been observed. These points are considered as potential approximate\n                pareto-optimal design points.\n            sampler: The sampler used to draw base samples. If not given,\n                a sampler is generated using `get_sampler`.\n                Note: a pareto front is created for each mc sample, which can be\n                computationally intensive for `m` > 2.\n            objective: The MCMultiOutputObjective under which the samples are\n                evaluated. Defaults to `IdentityMultiOutputObjective()`.\n            constraints: A list of callables, each mapping a Tensor of dimension\n                `sample_shape x batch-shape x q x m` to a Tensor of dimension\n                `sample_shape x batch-shape x q`, where negative values imply\n                feasibility. The acquisition function will compute expected feasible\n                hypervolume.\n            X_pending: A `batch_shape x m x d`-dim Tensor of `m` design points that\n                have points that have been submitted for function evaluation, but\n                have not yet been evaluated.\n            eta: The temperature parameter for the sigmoid function used for the\n                differentiable approximation of the constraints. In case of a float the\n                same `eta` is used for every constraint in constraints. In case of a\n                tensor the length of the tensor must match the number of provided\n                constraints. The i-th constraint is then estimated with the i-th\n                `eta` value. For more details, on this parameter, see the docs of\n                `compute_smoothed_feasibility_indicator`.\n            fat: A Boolean flag indicating whether to use the heavy-tailed approximation\n                of the constraint indicator.\n            prune_baseline: If True, remove points in `X_baseline` that are\n                highly unlikely to be the pareto optimal and better than the\n                reference point. This can significantly improve computation time and\n                is generally recommended. In order to customize pruning parameters,\n                instead manually call `prune_inferior_points_multi_objective` on\n                `X_baseline` before instantiating the acquisition function.\n            alpha: The hyperparameter controlling the approximate non-dominated\n                partitioning. The default value of 0.0 means an exact partitioning\n                is used. As the number of objectives `m` increases, consider increasing\n                this parameter in order to limit computational complexity.\n            cache_pending: A boolean indicating whether to use cached box\n                decompositions (CBD) for handling pending points. This is\n                generally recommended.\n            max_iep: The maximum number of pending points before the box\n                decompositions will be recomputed.\n            incremental_nehvi: A boolean indicating whether to compute the\n                incremental NEHVI from the `i`th point where `i=1, ..., q`\n                under sequential greedy optimization, or the full qNEHVI over\n                `q` points.\n            cache_root: A boolean indicating whether to cache the root\n                decomposition over `X_baseline` and use low-rank updates.\n            marginalize_dim: A batch dimension that should be marginalized. For example,\n                this is useful when using a batched fully Bayesian model.\n        \"\"\"\n        MultiObjectiveMCAcquisitionFunction.__init__(\n            self,\n            model=model,\n            sampler=sampler,\n            objective=objective,\n            constraints=constraints,\n            eta=eta,\n        )\n        SubsetIndexCachingMixin.__init__(self)\n        NoisyExpectedHypervolumeMixin.__init__(\n            self,\n            model=model,\n            ref_point=ref_point,\n            X_baseline=X_baseline,\n            sampler=self.sampler,\n            objective=self.objective,\n            constraints=self.constraints,\n            X_pending=X_pending,\n            prune_baseline=prune_baseline,\n            alpha=alpha,\n            cache_pending=cache_pending,\n            max_iep=max_iep,\n            incremental_nehvi=incremental_nehvi,\n            cache_root=cache_root,\n            marginalize_dim=marginalize_dim,\n        )\n        self.fat = fat\n\n    @t_batch_mode_transform()\n    @average_over_ensemble_models\n    def forward(self, X: Tensor) -> Tensor:\n        # Get samples from the posterior, and manually concatenate pending points that\n        # have not yet been cached. Shared with qLogNEHVI.\n        samples, X = self._compute_posterior_samples_and_concat_pending(X)\n        # Add previous nehvi from pending points.\n        return self._compute_qehvi(samples=samples, X=X) + self._prev_nehvi\n\n\nclass NoisyExpectedHypervolumeMixin(CachedCholeskyMCSamplerMixin):\n    # ...\n    def __init__(\n        self,\n        model: Model,\n        ref_point: list[float] | Tensor,\n        X_baseline: Tensor,\n        sampler: MCSampler | None = None,\n        objective: MCMultiOutputObjective | None = None,\n        constraints: list[Callable[[Tensor], Tensor]] | None = None,\n        X_pending: Tensor | None = None,\n        prune_baseline: bool = False,\n        alpha: float = 0.0,\n        cache_pending: bool = True,\n        max_iep: int = 0,\n        incremental_nehvi: bool = True,\n        cache_root: bool = True,\n        marginalize_dim: int | None = None,\n    ) -> None:\n        r\"\"\"Initialize NoisyExpectedHypervolumeMixin.\n\n        Args:\n            model: A fitted model.\n            ref_point: A list or tensor with `m` elements representing the reference\n                point (in the outcome space) w.r.t. to which compute the hypervolume.\n                This is a reference point for the objective values (i.e. after\n                applying `objective` to the samples).\n            X_baseline: A `r x d`-dim Tensor of `r` design points that have already\n                been observed. These points are considered as potential approximate\n                pareto-optimal design points.\n            sampler: The sampler used to draw base samples. If not given,\n                a sampler is generated using `get_sampler`.\n                Note: a pareto front is created for each mc sample, which can be\n                computationally intensive for `m` > 2.\n            objective: The MCMultiOutputObjective under which the samples are\n                evaluated. Defaults to `IdentityMultiOutputObjective()`.\n            constraints: A list of callables, each mapping a Tensor of dimension\n                `sample_shape x batch-shape x q x m` to a Tensor of dimension\n                `sample_shape x batch-shape x q`, where negative values imply\n                feasibility. The acquisition function will compute expected feasible\n                hypervolume.\n            X_pending: A `batch_shape x m x d`-dim Tensor of `m` design points that\n                have points that have been submitted for function evaluation, but\n                have not yet been evaluated.\n            prune_baseline: If True, remove points in `X_baseline` that are\n                highly unlikely to be the pareto optimal and better than the\n                reference point. This can significantly improve computation time and\n                is generally recommended. In order to customize pruning parameters,\n                instead manually call `prune_inferior_points_multi_objective` on\n                `X_baseline` before instantiating the acquisition function.\n            alpha: The hyperparameter controlling the approximate non-dominated\n                partitioning. The default value of 0.0 means an exact partitioning\n                is used. As the number of objectives `m` increases, consider increasing\n                this parameter in order to limit computational complexity.\n            cache_pending: A boolean indicating whether to use cached box\n                decompositions (CBD) for handling pending points. This is\n                generally recommended.\n            max_iep: The maximum number of pending points before the box\n                decompositions will be recomputed.\n            incremental_nehvi: A boolean indicating whether to compute the\n                incremental NEHVI from the `i`th point where `i=1, ..., q`\n                under sequential greedy optimization, or the full qNEHVI over\n                `q` points.\n            cache_root: A boolean indicating whether to cache the root\n                decomposition over `X_baseline` and use low-rank updates.\n            marginalize_dim: A batch dimension that should be marginalized. For example,\n                this is useful when using a batched fully Bayesian model.\n        \"\"\"\n        CachedCholeskyMCSamplerMixin.__init__(\n            self, model=model, cache_root=cache_root, sampler=sampler\n        )\n        if not self.cache_root and cache_pending:\n            warnings.warn(\n                \"`cache_root` is False but `cache_pending` is True. This is unexpected \"\n                \"behavior. Setting `cache_pending` to False.\",\n                BotorchWarning,\n                stacklevel=2,\n            )\n            cache_pending = False\n        if X_pending is not None and not cache_pending:\n            warnings.warn(\n                \"`X_pending` is not None but `cache_pending` is False. This means that \"\n                \"cached box decompositions will not be used to represent the pending \"\n                \"points. This is usually not recommended since it means the acquisition \"\n                \"function does not update for incremental acquisitions.\",\n                BotorchWarning,\n                stacklevel=2,\n            )\n        self.register_buffer(\"ref_point\", torch.as_tensor(ref_point))\n        if prune_baseline:\n            X_baseline = prune_inferior_points_multi_objective(\n                model=model,\n                X=X_baseline,\n                ref_point=self.ref_point,\n                objective=objective,\n                constraints=constraints,\n                marginalize_dim=marginalize_dim,\n            )\n        self.register_buffer(\"_X_baseline\", X_baseline)\n        # Current partitions for the box decomposition. This attribute will be set in\n        # `_update_cache` and is typically overwritten by a call to `set_X_pending`.\n        self._current_partitioning: BoxDecompositionList | None = None\n        self.prune_baseline = prune_baseline\n        self.objective = objective\n        self.constraints = constraints\n        self.alpha = alpha\n        self._cache_pending = cache_pending\n        self._max_iep = max_iep\n        self._incremental_nehvi = incremental_nehvi\n        self._prev_nehvi = None  # this will be set by `set_X_pending`\n        self.marginalize_dim = marginalize_dim\n        self.set_X_pending(X_pending=X_pending)\n\n    @property\n    def _cached_acquisition_information(self) -> AcquisitionCandidateInformation | None:\n        return getattr(self, \"_acq_info\", None)\n\n    @_cached_acquisition_information.setter\n    def _cached_acquisition_information(\n        self,\n        value: AcquisitionCandidateInformation,\n    ) -> None:\n        self._acq_info = value\n\n    @property\n    def X_baseline(self) -> Tensor:\n        return self._X_baseline\n\n    def set_X_pending(self, X_pending: Tensor | None = None) -> None:\n        r\"\"\"Informs the acquisition function about pending design points.\n\n        Args:\n            X_pending: `n x d` Tensor with `n` `d`-dim design points that have\n                been submitted for evaluation but have not yet been evaluated.\n        \"\"\"\n        self.X_pending = X_pending\n        self._update_cache()\n\n    def _get_fantasized_posterior_and_samples(\n        self, X: Tensor\n    ) -> tuple[Posterior, Tensor]:\n        # construct a fantasy model to account for `X_pending`\n        # NOTE: If we use input transforms for multiple q-batches of inputs, the input\n        # transform's forward pass automatically adds an extra batch dimension to the\n        # returned transformed input. We unsqueeze an extra batch dimension to `X` to\n        # ensure that `model.fantasize` has the correct `q_batch_shape` when it adds\n        # the transformed inputs to the `train_inputs`.\n        if X.ndim == self.model.train_inputs[0].ndim - 1:\n            X = X.unsqueeze(-2)\n\n        # Get inputs for the fantasy model; make sure to apply input transforms prior to\n        # calling `model.fantasize` since `X_pending` are not transformed.\n        fantasy_X = torch.cat(\n            [match_batch_shape(self.X_pending, X), X], dim=-2\n        )  # pyre-ignore: [6]\n\n        with torch.no_grad():\n            # We use `StochasticSampler` to ensure that the fantasy samples are not\n            # sampled with replacement (as can happen with `SobolQMCNormalSampler`).\n            fantasy_model = self.model.fantasize(\n                X=fantasy_X, sampler=StochasticSampler(sample_shape=self.sampler.sample_shape)\n            )\n\n        # We get the full posterior over `(X_baseline, X_pending, X)` to ensure that\n        # `X_pending` and `X` can be correlated (unlike `_get_f_X_samples`, which is\n        # used for `X_baseline` only).\n        full_X = torch.cat(\n            [match_batch_shape(self.X_baseline, X), fantasy_X], dim=-2\n        )  # pyre-ignore: [6]\n\n        posterior = fantasy_model.posterior(\n            full_X, posterior_transform=self.objective._get_X_batch_posterior_transform(full_X)\n        )\n\n        # draw samples to compute the acquisition function\n        # num_samples x num_fantasies x batch_shape x num_q_points x num_outputs\n        samples = self.sampler(posterior)\n        return posterior, samples\n\n    def _compute_posterior_samples_and_concat_pending(\n        self, X: Tensor\n    ) -> tuple[Tensor, Tensor]:\n        r\"\"\"Compute samples at new points and concatenate with pending points.\n\n        Args:\n            X: A `batch_shape x q x d`-dim tensor of inputs.\n\n        Returns:\n            A two-tuple `(samples, X_full)`, where `samples` is a tensor of posterior\n            samples with shape `sample_shape x batch_shape x q_full x m`, and `X_full`\n            is the concatenation of `X_baseline`, `X_pending` (if `cache_pending` is\n            True), and `X`.\n        \"\"\"\n        n_baseline, q = self.X_baseline.shape[-2], X.shape[-2]\n        if self.X_pending is not None and self._cache_pending:\n            X_full = torch.cat(\n                [match_batch_shape(self.X_baseline, X), self.X_pending, X], dim=-2\n            )\n        else:\n            X_full = torch.cat([match_batch_shape(self.X_baseline, X), X], dim=-2)\n\n        # TODO: Implement more efficient way to compute posterior over both training and\n        # test points in GPyTorch (https://github.com/cornellius-gp/gpytorch/issues/567)\n        posterior = self.model.posterior(\n            X_full, posterior_transform=self.posterior_transform\n        )\n        if not self._cache_root:\n            samples_full = super().get_posterior_samples(posterior)\n            # assign baseline samples so `_update_cache` can compute partitioning\n            if self.X_pending is not None and self._cache_pending:\n                self.baseline_samples = samples_full[..., : n_baseline + self.X_pending.shape[-2], :]\n                samples = samples_full[..., n_baseline + self.X_pending.shape[-2] :, :]\n            else:\n                self.baseline_samples = samples_full[..., :n_baseline, :]\n                samples = samples_full[..., n_baseline:, :]\n        else:\n            # handle one-to-many input transforms\n            n_plus_q = X_full.shape[-2]\n            n_w = posterior._extended_shape()[-2] // n_plus_q\n            if self.X_pending is not None and self._cache_pending:\n                q_in = (q + self.X_pending.shape[-2]) * n_w\n                baseline_q_in = n_baseline * n_w\n                self._set_sampler(q_in=baseline_q_in, posterior=posterior)\n                self.baseline_samples = self._get_f_X_samples(posterior=posterior, q_in=baseline_q_in)\n            else:\n                q_in = q * n_w\n                baseline_q_in = n_baseline * n_w\n                self._set_sampler(q_in=baseline_q_in, posterior=posterior)\n                self.baseline_samples = self._get_f_X_samples(posterior=posterior, q_in=baseline_q_in)\n            self._set_sampler(q_in=q_in, posterior=posterior)\n            samples = self._get_f_X_samples(posterior=posterior, q_in=q_in)\n        return samples, X_full\n\n    def _update_cache(self) -> None:\n        r\"\"\"Updates the cached box decompositions and relevant acquisition data.\n\n        This handles pending points with Cached Box Decompositions (CBDs) as described\n        in [Daulton2021nehvi]_. If `self._cache_pending` is True, then this will re-\n        compute the box decomposition every `max_iep` pending points, to ensure that\n        the cached decomposition remains fresh. Otherwise, the box decomposition is\n        computed on the most recent pending point and the baseline points. If the\n        number of pending points exceeds `max_iep`, we simply return the baseline\n        points.\n        \"\"\"\n        if not self._cache_pending or self.X_pending is None:\n            X_cached = self._X_baseline\n            self._prev_nehvi = 0.0\n        elif self.X_pending.shape[-2] > self._max_iep:\n            X_cached = self._X_baseline\n            warnings.warn(\n                f\"Number of pending points {self.X_pending.shape[-2]} exceeds \"\n                f\"maximum of `max_iep={self._max_iep}`. Only using baseline points \"\n                \"to compute box decomposition.\",\n                BotorchWarning,\n                stacklevel=2,\n            )\n            self._prev_nehvi = 0.0\n        else:\n            # If X_pending is small, cache the partitioning by conditioning the model\n            # on the pending points. This is done by computing fantasies on the model\n            # given the pending points, drawing samples from it, and then computing the\n            # partitioning using these samples.\n            X_cached = torch.cat(\n                [self._X_baseline, self.X_pending], dim=-2\n            )  # pyre-ignore: [6]\n            _, samples = self._get_fantasized_posterior_and_samples(self.X_pending)\n            # We only use samples that correspond to the `X_pending` points (i.e. `q`\n            # is `self.X_pending.shape[-2]`).\n            n_baseline_plus_X_pending = self._X_baseline.shape[-2] + self.X_pending.shape[-2]\n            # The samples are `sample_shape x 1 x batch_shape x (q_baseline + q_pending + q_new) x m`.\n            # We only keep the baseline + pending samples.\n            samples = samples[..., :n_baseline_plus_X_pending, :]\n            obj = self.objective(samples, X=X_cached)\n            if self.constraints is not None:\n                # If the current partitioning is cached, check if new pending points are\n                # feasible. If not, reset the cache.\n                is_feasible = compute_feasibility_indicator(\n                    constraints=self.constraints, samples=samples\n                )\n                obj = torch.where(is_feasible, obj, self.ref_point.min())\n            # Use a dummy partitioning to calculate the EHVI. Note: This assumes that\n            # there is no caching of the partitioning with `cache_pending=True` in\n            # `get_acquisition_function`.\n            partitioning = NondominatedPartitioning(ref_point=self.ref_point, Y=obj.mean(dim=0), alpha=self.alpha)\n            # The `_prev_nehvi` will be computed using the mean of samples (i.e. `obj`).\n            qnehvi = qLogExpectedHypervolumeImprovement(\n                model=self.model,\n                ref_point=self.ref_point,\n                partitioning=partitioning,\n                sampler=self.sampler,\n                objective=self.objective,\n                constraints=self.constraints,\n                eta=getattr(self, \"eta\", 1e-3),\n                fat=getattr(self, \"fat\", False),\n                tau_relu=getattr(self, \"tau_relu\", TAU_RELU),\n                tau_max=getattr(self, \"tau_max\", TAU_MAX),\n            )\n            # We compute the previous NEHVI by evaluating the (log) EHVI using the\n            # previously observed inputs and pending inputs.\n            self._prev_nehvi = qnehvi._compute_log_qehvi(samples=samples, X=X_cached).mean(dim=0)\n\n        # `X_cached` are the baseline points to be used by the partitioning.\n        # These are used to determine the set of non-dominated outcomes.\n        # This is where we use our `objective` (since `Y` are the raw samples).\n        with torch.no_grad():\n            Y = self.model.posterior(X_cached, posterior_transform=self.posterior_transform).rsample(\n                sample_shape=self.sampler.sample_shape\n            )\n            obj_Y = self.objective(Y, X=X_cached)\n\n            # compute feasibility if constraints are present\n            if self.constraints is not None:\n                is_feasible = compute_feasibility_indicator(\n                    constraints=self.constraints, samples=Y\n                )\n                obj_Y = torch.where(is_feasible, obj_Y, self.ref_point.min())\n\n            # The actual partitioning is computed using the sampled objective values.\n            self._current_partitioning = NondominatedPartitioning(\n                ref_point=self.ref_point,\n                Y=obj_Y,\n                alpha=self.alpha,\n                # to avoid re-computing the `pareto_Y` when `qLogEHVI` is called in `forward`\n                # these are stored in `_cached_acquisition_information` to avoid redundant\n                # computations\n                _cached_acquisition_information=self._cached_acquisition_information,\n            )\n\n\nclass CachedCholeskyMCSamplerMixin(MCSamplerMixin):\n    r\"\"\"Abstract Mixin class for acquisition functions using a cached Cholesky.\n\n    Specifically, this is for acquisition functions that require sampling from\n    the posterior P(f(X_baseline, X) | D). The Cholesky of the posterior\n    covariance over f(X_baseline) is cached.\n    \"\"\"\n\n    def __init__(\n        self,\n        model: Model,\n        cache_root: bool = False,\n        sampler: MCSampler | None = None,\n    ) -> None:\n        r\"\"\"Set class attributes and perform compatibility checks.\n\n        Args:\n            model: A model.\n            cache_root: A boolean indicating whether to cache the Cholesky.\n                This might be overridden in the model is not compatible.\n            sampler: An optional MCSampler object.\n        \"\"\"\n        MCSamplerMixin.__init__(self, sampler=sampler)\n        if cache_root and not supports_cache_root(model):\n            warnings.warn(\n                _get_cache_root_not_supported_message(type(model)),\n                RuntimeWarning,\n                stacklevel=3,\n            )\n            cache_root = False\n        self._cache_root = cache_root\n\n    def _compute_root_decomposition(\n        self,\n        posterior: Posterior,\n    ) -> Tensor:\n        r\"\"\"Cache Cholesky of the posterior covariance over f(X_baseline).\n\n        Because `LinearOperator.root_decomposition` is decorated with LinearOperator's\n        @cached decorator, this function is doing a lot implicitly:\n\n        1) Check if a root decomposition has already been cached to `lazy_covar`.\n          Note that it will not have been if `posterior.mvn` is a\n          `MultitaskMultivariateNormal`, since we construct `lazy_covar` in that\n          case.\n        2) If the root decomposition has not been found in the cache, compute it.\n        3) Write it to the cache of `lazy_covar`. Note that this will become\n          inaccessible if `posterior.mvn` is a `MultitaskMultivariateNormal`,\n          since in that case `lazy_covar`'s scope is only this function.\n\n        Args:\n            posterior: The posterior over f(X_baseline).\n        \"\"\"\n        if isinstance(posterior.distribution, MultitaskMultivariateNormal):\n            lazy_covar = extract_batch_covar(posterior.distribution)\n        else:\n            lazy_covar = posterior.distribution.lazy_covariance_matrix\n        lazy_covar_root = lazy_covar.root_decomposition()\n        return lazy_covar_root.root.to_dense()\n\n    def _get_f_X_samples(self, posterior: GPyTorchPosterior, q_in: int) -> Tensor:\n        r\"\"\"Get posterior samples at the `q_in` new points from the joint posterior.\n\n        Args:\n            posterior: The joint posterior is over (X_baseline, X).\n            q_in: The number of new points in the posterior. See `_set_sampler` for\n                more information.\n\n        Returns:\n            A `sample_shape x batch_shape x q x m`-dim tensor of posterior\n                samples at the new points.\n        \"\"\"\n        # Technically we should make sure that we add a consistent nugget to the\n        # cached covariance (and box decompositions) and the new block.\n        # But recomputing box decompositions every time the jitter changes would\n        # be quite slow.\n        if self._cache_root and hasattr(self, \"_baseline_L\"):\n            try:\n                return sample_cached_cholesky(\n                    posterior=posterior,\n                    baseline_L=self._baseline_L,\n                    q=q_in,\n                    base_samples=self.sampler.base_samples,\n                    sample_shape=self.sampler.sample_shape,\n                )\n            except (NanError, NotPSDError):\n                warnings.warn(\n                    \"Low-rank cholesky updates failed due NaNs or due to an \"\n                    \"ill-conditioned covariance matrix. \"\n                    \"Falling back to standard sampling.\",\n                    BotorchWarning,\n                    stacklevel=3,\n                )\n\n        # TODO: improve efficiency for multi-task models\n        samples = self.get_posterior_samples(posterior)\n        if isinstance(self.model, HigherOrderGP):\n            # Select the correct q-batch dimension for HOGP.\n            q_dim = -self.model._num_dimensions\n            q_idcs = (\n                torch.arange(-q_in, 0, device=samples.device) + samples.shape[q_dim]\n            )\n            return samples.index_select(q_dim, q_idcs)\n        else:\n            return samples[..., -q_in:, :]\n\n    def _set_sampler(\n        self,\n        q_in: int,\n        posterior: Posterior,\n    ) -> None:\n        r\"\"\"Update the sampler to use the original base samples for X_baseline.\n\n        Args:\n            q_in: The effective input batch size. This is typically equal to the\n                q-batch size of `X`. However, if using a one-to-many input transform,\n                e.g., `InputPerturbation` with `n_w` perturbations, the posterior will\n                have `n_w` points on the q-batch for each point on the q-batch of `X`.\n                In which case, `q_in = q * n_w` is used.\n            posterior: The posterior.\n        \"\"\"\n        if self.q_in != q_in and self.base_sampler is not None:\n            self.sampler._update_base_samples(\n                posterior=posterior, base_sampler=self.base_sampler\n            )\n            self.q_in = q_in",
    "Experiment Result": "The core methodology extends Expected Hypervolume Improvement (EHVI) to noisy, parallel settings (NEHVI/qNEHVI). This is implemented through `qNoisyExpectedHypervolumeImprovement` and `qLogNoisyExpectedHypervolumeImprovement` classes. These classes are designed to compute the mutual information between candidate observations and the uncertain Pareto frontier by iterating the expectation over the posterior distribution of function values at previously evaluated points. Their parallel variants, such as qNEHVI, utilize a sequential greedy approximation for batch selection, and can optionally compute incremental EHVI over pending points for increased efficiency. The resulting acquisition function (`ˆαNEHVI`) is differentiable, enabling efficient gradient-based optimization with methods like L-BFGS-B, and theoretical convergence guarantees are provided under Sample Average Approximation (SAA).\n\nTo address computational complexity, a Cached Box Decomposition (CBD) technique is introduced. This technique ensures that Pareto frontiers and box decompositions are computed once for fixed posterior samples and then cached for the duration of acquisition function optimization. This allows for efficient conditional posterior sampling for new candidates using low-rank updates and the reparameterization trick, which is managed by the `CachedCholeskyMCSamplerMixin`. The `cache_root` parameter, which defaults to `True` in the constructor of relevant acquisition functions, enables this caching mechanism. Additionally, the `prune_baseline` parameter can be used to remove baseline points that are highly unlikely to be Pareto optimal, further improving computational performance.\n\nThe methodology also mentions a cheaper approximation, qNEHVI-1, which utilizes a single approximate Gaussian Process sample path via Random Fourier Features (RFFs) for faster optimization. While the provided code snippets for `qNoisyExpectedHypervolumeImprovement` and `qLogNoisyExpectedHypervolumeImprovement` do not explicitly show `qNEHVI-1` or direct integration of RFFs within their core logic, the `botorch` library generally supports RFF-based path models (`get_matheron_path_model`) for generating GP samples. These RFF-based models are used in other acquisition functions (e.g., Thompson Sampling, `qJointEntropySearch`), suggesting that if a `qNEHVI-1` variant with RFFs were implemented, it would likely involve configuring the `sampler` parameter with an RFF-based sampler or model, or it might exist as a separate wrapper not detailed in the provided context."
}{
    "Title": "Multi-Fidelity Bayesian Optimization via Deep Neural Networks",
    "Main Contributions": "The paper proposes Deep Neural Network Multi-Fidelity Bayesian Optimization (DNN-MFBO) to address the oversimplification of complex fidelity correlations in existing multi-fidelity BO methods. Its main contributions include: 1) developing a flexible deep neural network surrogate model that captures strong, potentially nonlinear and nonstationary relationships across fidelities, thereby improving objective function estimation; 2) introducing a computationally tractable and efficient mutual information-based acquisition function using sequential, fidelity-wise Gauss-Hermite quadrature and moment-matching; 3) demonstrating superior optimization performance (lower regret and cost) on synthetic benchmark functions and real-world engineering design problems (mechanical plate vibration and thermal conductor design); and 4) achieving significantly faster acquisition function computation compared to state-of-the-art multi-output Gaussian Process-based methods.",
    "Methodology": "DNN-MFBO utilizes a stacked architecture of deep neural networks, where each network models a specific fidelity. For fidelities m > 1, the input to the NN fm(x) is augmented with the output of the previous fidelity fm-1(x), i.e., [x; fm-1(x)], enabling information propagation and capturing complex inter-fidelity relationships. The weights in the output layer of each NN are treated as random variables with a standard normal prior, while all other weights are considered hyper-parameters. A stochastic variational learning algorithm is developed to jointly estimate the posterior distribution of these random weights (approximated as multivariate Gaussians) and optimize hyper-parameters by maximizing an evidence lower bound (ELBO) using the reparameterization trick. For optimization, a mutual information-based acquisition function, a(x, m) = (1/λm) * I(f*, fm(x)|D), is employed to select the next query location and fidelity. Its computation involves sequentially approximating output posteriors p(fm(x)|D) as Gaussians through fidelity-wise moment matching and Gauss-Hermite quadrature. The conditional entropy term within the acquisition function is handled via Monte-Carlo approximation over function maximum samples, where H(fm(x)|f*, D) is approximated by H(fm(x)|fM(x) ≤ f*, D), further approximated using moment matching and Gauss-Hermite quadrature.",
    "Experimental Setup": "The proposed DNN-MFBO method was evaluated on three synthetic benchmark functions: Branin (3 fidelities, 2D input), Park1 (2 fidelities, 4D input), and Levy (3 fidelities, 2D input). It was also tested on two real-world engineering design problems: 1) optimizing material properties for maximizing the fourth vibration mode frequency of a 3-D elastic plate (2 fidelities, coarse vs. dense mesh), and 2) optimizing the shape parameters of a central hole in a thermal conductor for faster heat conduction (2 fidelities, different mesh edge lengths). The performance was compared against several state-of-the-art multi-fidelity BO algorithms including MF-SKO, MF-GP-UCB, MF-PES, MF-MES, and MTNN-BO, as well as single-fidelity BO (SF-MES). Initial training data consisted of randomly queried samples (e.g., 20/20/2 for Branin/Levy, 5/2 for Park1, 20/5 for real-world tasks across fidelities). Query costs were set (e.g., λ=(1, 10, 100) for synthetic and (1, 10) for real-world tasks). Performance metrics included simple regret (SR), inference regret (IR) for synthetic tasks, and best queried function values for real-world applications, along with average query time. Neural network architectures and learning rates for DNN-MFBO were identified using an AutoML tool (SMAC3) and manual adjustments. Experiments were repeated five times.",
    "Limitations": "The paper primarily focuses on discrete fidelities, explicitly stating that it does not address algorithms dealing with continuous fidelities. The computation of the acquisition function relies on several approximations, including Monte-Carlo sampling for conditional entropy terms and Gaussian approximations via fidelity-wise moment matching and Gauss-Hermite quadrature. While these approximations contribute to tractability and efficiency, they might introduce some level of approximation error. For maximizing the acquisition function, L-BFGS is used with random initialization, which, although faster, carries the risk of converging to local optima rather than the global optimum of the acquisition function. The neural network architecture and learning rate were identified using an AutoML tool (SMAC3) followed by manual tuning, suggesting that a fully automated and robust hyper-parameter optimization strategy for the DNN components could be further explored.",
    "Future Research Directions": "Based on the identified limitations and the context of the work, potential future research directions include: 1) extending DNN-MFBO to handle continuous fidelities, as the current work focuses on discrete settings; 2) exploring alternative neural network architectures, Bayesian deep learning approaches, or advanced variational inference techniques to potentially improve model expressiveness or inference accuracy; 3) investigating more accurate yet still computationally efficient methods for approximating the mutual information-based acquisition function, particularly for the conditional entropy terms; and 4) developing more robust and fully automated strategies for hyper-parameter optimization of the deep neural network components to reduce manual tuning efforts.",
    "Experiment Code": "The provided repository content (`generate_simudata3.py`, `generate_simudata4.py`, `generate_simudata_2robot2thing.py`, `push_world.py`) implements various physics-based simulation environments for object pushing tasks. These scripts define the experimental benchmarks on which a method like DNN-MFBO would operate by generating observed outcomes. However, the actual implementation of the DNN-MFBO method itself, including its stacked DNN architecture, variational learning algorithm, or mutual information-based acquisition function, is not present in the provided files.",
    "Experiment Result": "The experiments are conducted within a Box2D physics simulation environment (implemented in `push_world.py`), where the GUI is disabled by default for performance. The common simulation parameters include a `TARGET_FPS` of 100, `TIME_STEP` of 1.0/100, and `VEL_ITERS`, `POS_ITERS` set to 10. The `end_effector` (robot hand) applies forces or wrenches based on desired velocities/torques.\n\nThree distinct pushing simulation setups are defined:\n\n1.  **Single Robot Pushing Single Circular Object (`generate_simudata3.py`)**:\n    *   **Task**: A single robot pushes a circular object towards a target goal.\n    *   **Object Configuration**: One circular object (radius 1, friction 0.01, density 0.05) and a static base (friction 0.01).\n    *   **Robot Configuration**: A single robot with a `rectangle` shaped hand (size 0.3x1).\n    *   **Inputs**: Robot initial `(rx, ry)`, target `(gx, gy)`, and simulation steps (`simu_steps`, derived from `sys.argv[3] * 10`).\n    *   **Pushing Mechanism**: The robot applies force in a fixed direction pointing from its initial position `(rx, ry)` to the object's initial position.\n    *   **Objective/Output**: The Euclidean distance between the final position of the circular object and the specified target goal `(gx, gy)`.\n\n2.  **Single Robot Pushing Single Circular Object with Controlled Angle (`generate_simudata4.py`)**:\n    *   **Task**: Similar to setup 1, but with an additional input for the robot's initial push angle.\n    *   **Object & Robot Configuration**: Identical to setup 1.\n    *   **Inputs**: Robot initial `(rx, ry)`, target `(gx, gy)`, initial push `init_angle` (`sys.argv[6]`), and `simu_steps`.\n    *   **Pushing Mechanism**: The robot applies force based on input `xvel`, `yvel` (derived from `rx`, `ry` and normalized) with its initial angle `init_angle`.\n    *   **Objective/Output**: The Euclidean distance between the final position of the circular object and the specified target goal `(gx, gy)`.\n\n3.  **Two Robots Pushing Two Objects (`generate_simudata_2robot2thing.py`)**:\n    *   **Task**: Two robots simultaneously push two distinct objects (one rectangular, one circular) towards separate target goals.\n    *   **Object Configuration**: A static base. `Thing 1` is a rectangle (size 0.5x0.5, initial position (0,2)). `Thing 2` is a circle (radius 1, initial position (0,-2)). Both objects have friction 0.01 and density 0.05.\n    *   **Robot Configuration**: Two robots, both with `rectangle` shaped hands (size 1x0.3).\n    *   **Inputs**: \n        *   **Robot 1**: `(rx, ry)` (initial position), `(xvel, yvel)` (linear velocity), `simu_steps` (simulation duration), `init_angle` (initial angle), `rtor` (angular torque).\n        *   **Robot 2**: `(rx2, ry2)`, `(xvel2, yvel2)`, `simu_steps2`, `init_angle2`, `rtor2` (similar parameters for the second robot).\n        *   **Target Goals**: `(gx, gy)` for Thing 1, `(gx2, gy2)` for Thing 2.\n    *   **Pushing Mechanism**: Both robots apply a wrench (linear and angular velocity) based on their respective inputs. The simulation runs for the maximum of `simu_steps` and `simu_steps2`.\n    *   **Objective/Output**: The sum of the Euclidean distances between each object's final position and its corresponding target goal (`norm(goal1 - final_pos1) + norm(goal2 - final_pos2)`)."
}{
    "Title": "Batch Multi-Fidelity Bayesian Optimization with  Deep Auto-Regressive Networks",
    "Main Contributions": "The paper proposes Batch Multi-fidelity Bayesian Optimization with Deep Auto-Regressive Networks (BMBO-DARN) to optimize expensive black-box functions with flexible cost-accuracy trade-offs. Key contributions include developing a deep auto-regressive model using Bayesian neural networks to capture complex, strong relationships across different fidelities, thus improving surrogate learning. It also introduces an efficient batch querying method based on the Max-value Entropy Search (MES) principle, which explicitly penalizes highly correlated queries and encourages diversity. The method utilizes posterior samples and moment matching for efficient acquisition function computation and employs an alternating optimization algorithm to avoid combinatorial search when selecting inputs and fidelities.",
    "Methodology": "BMBO-DARN constructs a deep auto-regressive model for multi-fidelity surrogate learning. It uses a chain of M Bayesian Neural Networks (BNNs), where each NN for fidelity 'm' takes the original input 'x' and the outputs of all previous fidelities (f1(x), ..., fm-1(x)) as input. This architecture allows flexible capture of complex, nonlinear, and nonstationary relationships. Hamiltonian Monte Carlo (HMC) sampling is used for posterior inference of NN weights and noise precisions. For batch querying, a batch acquisition function is proposed, derived from the Max-value Entropy Search (MES) principle, which measures the mutual information between the batch of queried function values and the function optimum, normalized by the total querying cost. To efficiently compute this function, posterior samples of NN weights are used with moment matching to approximate the joint posterior of the batch outputs and optimum as a multivariate Gaussian. The maximization of this batch acquisition function is achieved through an alternating optimization algorithm, where each input-fidelity pair in the batch is cyclically updated while others are fixed, avoiding costly combinatorial search.",
    "Experimental Setup": "The approach was evaluated on two synthetic benchmark functions for surrogate learning performance: Levy (two fidelities) and Branin (three fidelities), measuring normalized root-mean-square-error (nRMSE) and mean-negative-log-likelihood (MNLL). For real-world hyperparameter optimization, BMBO-DARN was applied to four machine learning models: Convolutional Neural Networks (CNN) for CIFAR-10 image classification (3 fidelities: 1, 10, 50 epochs; metric: negative log-loss), Online Latent Dirichlet Allocation (LDA) for 20NewsGroups text mining (3 fidelities: 1, 10, 50 epochs; metric: perplexity), XGBoost for diabetes diagnosis (3 fidelities: 2, 10, 100 weak learners; metric: log nRMSE), and Physics-Informed Neural Networks (PINN) for solving Burger's equation (3 fidelities: 10, 100, 50K max L-BFGS iterations; metric: log nRMSE). Comparisons were made against state-of-the-art multi-fidelity BO methods (MF-GP-UCB, MF-MES, SHTL, DNN-MFBO, MF-MES-Batch), single-fidelity BO (SF-Batch), and hyperparameter optimization-specific methods (SMAC3, Hyperband, BOHB), as well as BMBO-DARN-1 (batch size 1). Experiments were repeated 5 times, reporting averages and standard deviations. Batch size for batch methods was set to 5, and all batch methods queried new examples sequentially for fair cost comparison.",
    "Limitations": "The paper highlights that computationally intensive Hamiltonian Monte Carlo (HMC) sampling is critical for accurate uncertainty quantification required by the acquisition function, although it notes the flexibility to switch to faster approximate inference methods if needed. The moment matching approach approximates the posterior as a multivariate Gaussian, which might introduce some inaccuracies. The alternating optimization algorithm for maximizing the batch acquisition function, while efficient and guaranteed to improve at each step, does not explicitly guarantee finding the global optimum of the acquisition function. Additionally, while BMBO-DARN is a batch method, its full parallel speedup benefits were not demonstrated in the main experimental results, as batch queries were executed sequentially for fairness in accumulated cost comparison.",
    "Future Research Directions": "Not mentioned",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "Supervising the Multi-Fidelity Race of Hyperparameter Configurations",
    "Main Contributions": "Introduced DyHPO, a Bayesian Optimization (BO) method that dynamically allocates budget during hyperparameter optimization (HPO) to train promising configurations. Proposed a novel deep kernel for Gaussian Processes (GP) that embeds learning curve dynamics and an acquisition function (Multi-Fidelity Expected Improvement - EIMF) that incorporates multi-budget information. Demonstrated statistically significant superiority against state-of-the-art HPO methods across 50 datasets and diverse deep learning architectures (MLP, CNN/NAS, RNN), achieving better mean regret and faster convergence.",
    "Methodology": "DyHPO is a Bayesian Optimization method leveraging Gaussian Processes. It employs a deep kernel (K) that models the similarity between hyperparameter configurations by incorporating the configuration itself (x_i), the budget (j), and the past learning curve (Y_i,j-1). The deep kernel uses a neural network feature extractor (φ) composed of linear and one-dimensional convolutional layers (followed by global max pooling) to process the concatenated budget and hyperparameter configuration, and the learning curve, respectively. The output of φ is then fed into a squared exponential kernel. The kernel's parameters and the neural network's weights are optimized by maximizing the marginal likelihood using gradient descent with Adam. A multi-fidelity acquisition function, EIMF, is used to select the next configuration, which is an extension of Expected Improvement tailored for multi-budget settings. The algorithm iteratively selects the most promising candidate to train for one additional budget step, observes the outcome, and updates the surrogate model.",
    "Experimental Setup": "Evaluations were conducted on an Amazon EC2 M5 Instance (m5.xlarge) across three settings: hyperparameter optimization for tabular, text, and image classification. Performance was measured by the mean of ten repetitions, using regret (absolute difference from the best possible score) and average rank as metrics. Three benchmarks were used: LCBench (35 tabular datasets, 2,000 neural networks/dataset, 50 epochs, 7 numerical hyperparameters), TaskSet (12 NLP tasks, Adam8p search space, 8 continuous hyperparameters for RNNs, scores every 200 iterations), and NAS-Bench-201 (15,625 precomputed architectures on CIFAR-10, CIFAR-100, ImageNet, 6 categorical hyperparameters, 200 epochs). Baselines included Random Search, Hyperband, BOHB, DEHB, ASHA, MF-DNN, and Dragonfly (specifically BOCA). Preprocessing involved log-transforming certain hyperparameters, scaling continuous hyperparameters to [0,1], and using one-hot encoding for categorical ones (with specific adaptations for some baselines). The Deep Kernel Gaussian Process was implemented using GPyTorch 1.5 with an RBF kernel, dense layers of 128 and 256 units, and a convolutional layer with a kernel size of three and four filters. Training for the GP used Adam with a learning rate of 0.1 and a batch size of 64, with early stopping or a maximum of 1,000 epochs.",
    "Limitations": "The method lacks evaluation on very large deep learning models like Transformer-based architectures, due to the absence of suitable tabular benchmarks. The pause and resume training procedure is only applicable to parametric models, requiring restarts for non-parametric models. For small datasets that train quickly, the computational overhead of DyHPO may make simpler approaches like random search more practical. The authors also caution against running the method for extended periods for only marginal performance gains unless mission-critical.",
    "Future Research Directions": "The work aims to be a step towards scaling hyperparameter optimization for deep learning. Implicitly, future research could involve adapting and testing DyHPO on very large deep learning models (e.g., Transformers), extending its applicability to non-parametric models where training pause/resume is challenging, and reducing its overhead for fast-training small datasets. The paper also suggests the community should create sparse benchmarks with surrogates instead of dense tabular ones to save energy and computational resources.",
    "Experiment Code": "class DyHPOAlgorithm:\n\n    def __init__(\n        self,\n        hp_candidates: np.ndarray,\n        log_indicator: List,\n        seed: int = 11,\n        max_benchmark_epochs: int = 52,\n        fantasize_step: int = 1,\n        minimization: bool = True,\n        total_budget: int = 500,\n        device: str = None,\n        dataset_name: str = 'unknown',\n        output_path: str = '.',\n        surrogate_config: dict = None,\n        verbose: bool = True,\n    ):\n        \"\"\"\n        Args:\n            hp_candidates: np.ndarray\n                The full list of hyperparameter candidates for\n                a given dataset.\n            log_indicator: List\n                A list with boolean values indicating if a\n                hyperparameter has been log sampled or not.\n            seed: int\n                The seed that will be used for the surrogate.\n            max_benchmark_epochs: int\n                The maximal budget that a hyperparameter configuration\n                has been evaluated in the benchmark for.\n            fantasize_step: int\n                The number of steps for which we are looking ahead to\n                evaluate the performance of a hpc.\n            minimization: bool\n                If the objective should be maximized or minimized.\n            total_budget: int\n                The total budget given for hyperparameter optimization.\n            device: str\n                The device where the experiment will be run on.\n            dataset_name: str\n                The name of the dataset that the experiment will be run on.\n            output_path: str\n                The path where all the output will be stored.\n            surrogate_config: dict\n                The model configurations for the surrogate.\n            verbose: boolean\n                If detailed information is preferred in the log file.\n        \"\"\"\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        torch.manual_seed(seed)\n        np.random.seed(seed)\n\n        if device is None:\n            self.dev = torch.device(\n                'cuda') if torch.cuda.is_available() else torch.device('cpu')\n        else:\n            self.dev = torch.device(device)\n\n        self.hp_candidates = hp_candidates\n        self.log_indicator = log_indicator\n\n        self.scaler = MinMaxScaler()\n        self.hp_candidates = self.preprocess_hp_candidates()\n\n        self.minimization = minimization\n        self.seed = seed\n\n        if verbose:\n            logging_level = logging.DEBUG\n        else:\n            logging_level = logging.INFO\n        self.logger = logging.getLogger()\n\n        logging.basicConfig(\n            format='%(levelname)s:%(asctime)s:%(message)s',\n            filename=f'dyhpo_surrogate_{dataset_name}_{seed}.log',\n            level=logging_level,\n        )\n\n        # the keys will be hyperparameter indices while the value\n        # will be a list with all the budgets evaluated for examples\n        # and with all performances for the performances\n        self.examples = dict()\n        self.performances = dict()\n\n        # set a seed already, so that it is deterministic when\n        # generating the seeds of the ensemble\n        torch.manual_seed(seed)\n        np.random.seed(seed)\n\n        self.max_benchmark_epochs = max_benchmark_epochs\n        self.total_budget = total_budget\n        self.fantasize_step = fantasize_step\n        self.nr_features = self.hp_candidates.shape[1]\n\n        initial_configurations_nr = 1\n        conf_individual_budget = 1\n        self.init_conf_indices = np.random.choice(self.hp_candidates.shape[0], initial_configurations_nr, replace=False)\n        self.init_budgets = [conf_individual_budget] * initial_configurations_nr\n        # with what percentage configurations will be taken randomly instead of being sampled from the model\n        self.fraction_random_configs = 0.1\n\n        self.model = None\n        # An index keeping track of where we are in the init_conf_indices\n        # list of hyperparmeters that are not sampled from the model.\n        self.initial_random_index = 0\n\n        if surrogate_config is None:\n            self.surrogate_config = {\n                'nr_layers': 2,\n                'nr_initial_features': self.nr_features,\n                'layer1_units': 64,\n                'layer2_units': 128,\n                'cnn_nr_channels': 4,\n                'cnn_kernel_size': 3,\n                'batch_size': 64,\n                'nr_epochs': 1000,\n                'nr_patience_epochs': 10,\n                'learning_rate': 0.001,\n            }\n        else:\n            self.surrogate_config = surrogate_config\n\n        # the incumbent value observed during the hpo process.\n        self.best_value_observed = np.NINF\n        # a set which will keep track of the hyperparameter configurations that diverge.\n        self.diverged_configs = set()\n\n        # info dict to drop every surrogate iteration\n        self.info_dict = dict()\n\n        # the start time for the overhead of every surrogate optimization iteration\n        # will be recorded here\n        self.suggest_time_duration = 0\n        # the total budget consumed so far\n        self.budget_spent = 0\n\n        self.output_path = output_path\n        self.dataset_name = dataset_name\n\n        self.no_improvement_threshold = int(self.max_benchmark_epochs + 0.2 * self.max_benchmark_epochs)\n        self.no_improvement_patience = 0\n\n\n    def _prepare_dataset_and_budgets(self) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Prepare the data that will be the input to the surrogate.\n\n        Returns:\n            data: A Dictionary that contains inside the training examples,\n            the budgets, the curves and lastly the labels.\n        \"\"\"\n\n        train_examples, train_labels, train_budgets, train_curves = self.history_configurations()\n\n        train_examples = np.array(train_examples, dtype=np.single)\n        train_labels = np.array(train_labels, dtype=np.single)\n        train_budgets = np.array(train_budgets, dtype=np.single)\n        train_curves = self.patch_curves_to_same_length(train_curves)\n        train_curves = np.array(train_curves, dtype=np.single)\n\n        # scale budgets to [0, 1]\n        train_budgets = train_budgets / self.max_benchmark_epochs\n\n        train_examples = torch.tensor(train_examples)\n        train_labels = torch.tensor(train_labels)\n        train_budgets = torch.tensor(train_budgets)\n        train_curves = torch.tensor(train_curves)\n\n        train_examples = train_examples.to(device=self.dev)\n        train_labels = train_labels.to(device=self.dev)\n        train_budgets = train_budgets.to(device=self.dev)\n        train_curves = train_curves.to(device=self.dev)\n\n        data = {\n            'X_train': train_examples,\n            'train_budgets': train_budgets,\n            'train_curves': train_curves,\n            'y_train': train_labels,\n        }\n\n        return data\n\n    def _train_surrogate(self):\n        \"\"\"\n        Train the surrogate model.\n        \"\"\"\n        data = self._prepare_dataset_and_budgets()\n        self.logger.info(f'Started training the model')\n\n        self.model.train_pipeline(\n            data,\n            load_checkpoint=False,\n        )\n\n    def _predict(self) -> Tuple[np.ndarray, np.ndarray, List, List]:\n        \"\"\"\n        Predict the performances of the hyperparameter configurations\n        as well as the standard deviations based on the surrogate model.\n\n        Returns:\n            mean_predictions, std_predictions, hp_indices, non_scaled_budgets:\n                The mean predictions and the standard deviations over\n                all model predictions for the given hyperparameter\n                configurations with their associated indices, scaled and\n                non-scaled budgets.\n        \"\"\"\n        configurations, hp_indices, budgets, learning_curves = self.generate_candidate_configurations()\n        budgets = np.array(budgets, dtype=np.single)\n        non_scaled_budgets = copy.deepcopy(budgets)\n        # scale budgets to [0, 1]\n        budgets = budgets / self.max_benchmark_epochs\n\n        configurations = np.array(configurations, dtype=np.single)\n        configurations = torch.tensor(configurations)\n        configurations = configurations.to(device=self.dev)\n\n        budgets = torch.tensor(budgets)\n        budgets = budgets.to(device=self.dev)\n\n        learning_curves = self.patch_curves_to_same_length(learning_curves)\n        learning_curves = np.array(learning_curves, dtype=np.single)\n        learning_curves = torch.tensor(learning_curves)\n        learning_curves = learning_curves.to(device=self.dev)\n\n        train_data = self._prepare_dataset_and_budgets()\n        test_data = {\n            'X_test': configurations,\n            'test_budgets': budgets,\n            'test_curves': learning_curves,\n        }\n\n        mean_predictions, std_predictions = self.model.predict_pipeline(train_data, test_data)\n\n        return mean_predictions, std_predictions, hp_indices, non_scaled_budgets\n\n    def suggest(self) -> Tuple[int, int]:\n        \"\"\"\n        Suggest a hyperparameter configuration to be evaluated next.\n\n        Returns:\n            best_config_index, budget: The index of the hyperparamter\n                configuration to be evaluated and the budget for\n                what it is going to be evaluated for.\n        \"\"\"\n        suggest_time_start = time.time()\n        # check if we still have random hyperparameters to evaluate\n        if self.initial_random_index < len(self.init_conf_indices):\n            self.logger.info(\n                'Not enough configurations to build a model. '\n                'Returning randomly sampled configuration'\n            )\n\n            random_indice = self.init_conf_indices[self.initial_random_index]\n            budget = self.init_budgets[self.initial_random_index]\n            self.initial_random_index += 1\n\n            return random_indice, budget\n        else:\n            mean_predictions, std_predictions, hp_indices, non_scaled_budgets = self._predict()\n            best_prediction_index = self.find_suggested_config(\n                mean_predictions,\n                std_predictions,\n                non_scaled_budgets,\n            )\n            \"\"\"\n            the best prediction index is not always matching with the actual hp index.\n            Since when evaluating the acq function, we do not consider hyperparameter\n            candidates that diverged or that are evaluated fully.\n            \"\"\"\n            best_config_index = hp_indices[best_prediction_index]\n\n            # decide for what budget we will evaluate the most\n            # promising hyperparameter configuration next.\n            if best_config_index in self.examples:\n                evaluated_budgets = self.examples[best_config_index]\n                max_budget = max(evaluated_budgets)\n                budget = max_budget + self.fantasize_step\n                # this would only trigger if fantasize_step is bigger\n                # than 1\n                if budget > self.max_benchmark_epochs:\n                    budget = self.max_benchmark_epochs\n            else:\n                budget = self.fantasize_step\n\n        suggest_time_end = time.time()\n        self.suggest_time_duration = suggest_time_end - suggest_time_start\n\n        self.budget_spent += self.fantasize_step\n\n        # exhausted hpo budget, finish.\n        if self.budget_spent > self.total_budget:\n            exit(0)\n\n        return best_config_index, budget\n\n    def observe(\n        self,\n        hp_index: int,\n        b: int,\n        learning_curve: np.ndarray,\n        alg_time: Optional[float] = None,\n    ):\n        \"\"\"\n        Args:\n            hp_index: The index of the evaluated hyperparameter configuration.\n            b: The budget for which the hyperparameter configuration was evaluated.\n            learning_curve: The learning curve of the hyperparameter configuration.\n            alg_time: The time taken from the algorithm to evaluate the hp configuration.\n        \"\"\"\n        score = learning_curve[-1]\n        # if y is an undefined value, append 0 as the overhead since we finish here.\n        if np.isnan(learning_curve).any():\n            self.update_info_dict(hp_index, b, np.nan, 0)\n            self.diverged_configs.add(hp_index)\n            return\n\n        observe_time_start = time.time()\n\n        self.examples[hp_index] = np.arange(1, b + 1).tolist()\n        self.performances[hp_index] = learning_curve\n\n        if self.best_value_observed < score:\n            self.best_value_observed = score\n            self.no_improvement_patience = 0\n        else:\n            self.no_improvement_patience += 1\n\n        observe_time_end = time.time()\n        train_time_duration = 0\n\n        # initialization phase over. Now we can sample from the model.\n        if self.initial_random_index >= len(self.init_conf_indices):\n            train_time_start = time.time()\n            # create the model for the first time\n            if self.model is None:\n                # Starting a model from scratch\n                self.model = DyHPO(\n                    self.surrogate_config,\n                    self.dev,\n                    self.dataset_name,\n                    self.output_path,\n                    self.seed,\n                )\n\n            if self.no_improvement_patience == self.no_improvement_threshold:\n                self.model.restart = True\n\n            self._train_surrogate()\n\n            train_time_end = time.time()\n            train_time_duration = train_time_end - train_time_start\n\n        observe_time_duration = observe_time_end - observe_time_start\n        total_duration = observe_time_duration + self.suggest_time_duration + train_time_duration\n        if alg_time is not None:\n            total_duration = total_duration + alg_time\n\n        self.update_info_dict(hp_index, b, score, total_duration)\n\n    def prepare_examples(self, hp_indices: List) -> List[np.ndarray]:\n        \"\"\"\n        Prepare the examples to be given to the surrogate model.\n\n        Args:\n            hp_indices: The list of hp indices that are already evaluated.\n\n        Returns:\n            examples: A list of the hyperparameter configurations.\n        \"\"\"\n        examples = []\n        for hp_index in hp_indices:\n            examples.append(self.hp_candidates[hp_index])\n\n        return examples\n\n    def generate_candidate_configurations(\n        self,\n    ) -> Tuple[List, List, List, List]:\n        \"\"\"\n        Generate candidate configurations that will be\n        fantasized upon.\n\n        Returns:\n            (configurations, hp_indices, hp_budgets, learning_curves): Tuple\n                A tuple of configurations, their indices in the hp list\n                and the budgets that they should be fantasized upon.\n        \"\"\"\n        hp_indices = []\n        hp_budgets = []\n        learning_curves = []\n\n        for hp_index in range(0, self.hp_candidates.shape[0]):\n\n            if hp_index in self.examples:\n                budgets = self.examples[hp_index]\n                # Take the max budget evaluated for a certain hpc\n                max_budget = max(budgets)\n                next_budget = max_budget + self.fantasize_step\n                # take the learning curve until the point we have evaluated so far\n                curve = self.performances[hp_index][:max_budget]\n                # if the curve is shorter than the length of the kernel size,\n                # pad it with zeros\n                difference_curve_length = self.surrogate_config['cnn_kernel_size'] - len(curve)\n                if difference_curve_length > 0:\n                    curve.extend([0.0] * difference_curve_length)\n            else:\n                # The hpc was not evaluated before, so fantasize its\n                # performance\n                next_budget = self.fantasize_step\n                curve = [0, 0, 0]\n\n            # this hyperparameter configuration is not evaluated fully\n            if next_budget <= self.max_benchmark_epochs:\n                hp_indices.append(hp_index)\n                hp_budgets.append(next_budget)\n                learning_curves.append(curve)\n\n        configurations = self.prepare_examples(hp_indices)\n\n        return configurations, hp_indices, hp_budgets, learning_curves\n\n    def history_configurations(\n        self,\n    ) -> Tuple[List, List, List, List]:\n        \"\"\"\n        Generate the configurations, labels, budgets and curves based on\n        the history of evaluated configurations.\n\n        Returns:\n            (train_examples, train_labels, train_budgets, train_curves):\n                A tuple of examples, labels, budgets and curves for the\n                configurations evaluated so far.\n        \"\"\"\n        train_examples = []\n        train_labels = []\n        train_budgets = []\n        train_curves = []\n\n        for hp_index in self.examples:\n            budgets = self.examples[hp_index]\n            performances = self.performances[hp_index]\n            example = self.hp_candidates[hp_index]\n\n            for budget, performance in zip(budgets, performances):\n                train_examples.append(example)\n                train_budgets.append(budget)\n                train_labels.append(performance)\n                train_curve = performances[:budget - 1] if budget > 1 else [0.0]\n                difference_curve_length = self.surrogate_config['cnn_kernel_size'] - len(train_curve)\n                if difference_curve_length > 0:\n                    train_curve.extend([0.0] * difference_curve_length)\n\n                train_curves.append(train_curve)\n\n        return train_examples, train_labels, train_budgets, train_curves\n\n    def acq(\n        self,\n        best_value: float,\n        mean: float,\n        std: float,\n        explore_factor: Optional[float] = 0.25,\n        acq_fc: str = 'ei',\n    ) -> float:\n        \"\"\"\n        The acquisition function that will be called\n        to evaluate the score of a hyperparameter configuration.\n\n        Parameters\n        ----------\n        best_value: float\n            Best observed function evaluation. Individual per fidelity.\n        mean: float\n            Point mean of the posterior process.\n        std: float\n            Point std of the posterior process.\n        explore_factor: float\n            The exploration factor for when ucb is used as the\n            acquisition function.\n        ei_calibration_factor: float\n            The factor used to calibrate expected improvement.\n        acq_fc: str\n            The type of acquisition function to use.\n\n        Returns\n        -------\n        acq_value: float\n            The value of the acquisition function.\n        \"\"\"\n        if acq_fc == 'ei':\n            if std == 0:\n                return 0\n            z = (mean - best_value) / std\n            acq_value = (mean - best_value) * norm.cdf(z) + std * norm.pdf(z)\n        elif acq_fc == 'ucb':\n            acq_value = mean + explore_factor * std\n        elif acq_fc == 'thompson':\n            acq_value = np.random.normal(mean, std)\n        elif acq_fc == 'exploit':\n            acq_value = mean\n        else:\n            raise NotImplementedError(\n                f'Acquisition function {acq_fc} has not been'\n                f'implemented',\n            )\n\n        return acq_value\n\n    def find_suggested_config(\n        self,\n        mean_predictions: np.ndarray,\n        mean_stds: np.ndarray,\n        budgets: List,\n    ) -> int:\n        \"\"\"\n        Find the hyperparameter configuration that has the highest score\n        with the acquisition function.\n\n        Args:\n            mean_predictions: The mean predictions of the posterior.\n            mean_stds: The mean standard deviations of the posterior.\n            budgets: The next budgets that the hyperparameter configurations\n                will be evaluated for.\n\n        Returns:\n            best_index: The index of the hyperparameter configuration with the\n                highest score.\n        \"\"\"\n        highest_acq_value = np.NINF\n        best_index = -1\n\n        index = 0\n        for mean_value, std in zip(mean_predictions, mean_stds):\n            budget = int(budgets[index])\n            best_value = self.calculate_fidelity_ymax(budget)\n            acq_value = self.acq(best_value, mean_value, std, acq_fc='ei')\n            if acq_value > highest_acq_value:\n                highest_acq_value = acq_value\n                best_index = index\n\n            index += 1\n\n        return best_index\n\n    def calculate_fidelity_ymax(self, fidelity: int):\n        \"\"\"\n        Find ymax for a given fidelity level.\n\n        If there are hyperparameters evaluated for that fidelity\n        take the maximum from their values. Otherwise, take\n        the maximum from all previous fidelity levels for the\n        hyperparameters that we have evaluated.\n\n        Args:\n            fidelity: The fidelity of the hyperparameter\n                configuration.\n\n        Returns:\n            best_value: The best value seen so far for the\n                given fidelity.\n        \"\"\"\n        exact_fidelity_config_values = []\n        lower_fidelity_config_values = []\n\n        for example_index in self.examples.keys():\n            try:\n                performance = self.performances[example_index][fidelity - 1]\n                exact_fidelity_config_values.append(performance)\n            except IndexError:\n                learning_curve = self.performances[example_index]\n                # The hyperparameter was not evaluated until fidelity, or more.\n                # Take the maximum value from the curve.\n                lower_fidelity_config_values.append(max(learning_curve))\n\n        if len(exact_fidelity_config_values) > 0:\n            # lowest error corresponds to best value\n            best_value = max(exact_fidelity_config_values)\n        else:\n            best_value = max(lower_fidelity_config_values)\n\n        return best_value\n\n    def update_info_dict(\n        self,\n        hp_index: int,\n        budget: int,\n        performance: float,\n        overhead: float,\n    ):\n        \"\"\"\n        Update the info dict with the current HPO iteration info.\n\n        Dump a new json file that will update with additional information\n        given the current HPO iteration.\n\n        Args:\n            hp_index: The index of the hyperparameter configuration.\n            budget: The budget of the hyperparameter configuration.\n            performance:  The performance of the hyperparameter configuration.\n            overhead: The total overhead (in seconds) of the iteration.\n        \"\"\"\n        hp_index = int(hp_index)\n        if 'hp' in self.info_dict:\n            self.info_dict['hp'].append(hp_index)\n        else:\n            self.info_dict['hp'] = [hp_index]\n\n        if 'scores' in self.info_dict:\n            self.info_dict['scores'].append(performance)\n        else:\n            self.info_dict['scores'] = [performance]\n\n        if 'curve' in self.info_dict:\n            self.info_dict['curve'].append(self.best_value_observed)\n        else:\n            self.info_dict['curve'] = [self.best_value_observed]\n\n        if 'epochs' in self.info_dict:\n            self.info_dict['epochs'].append(budget)\n        else:\n            self.info_dict['epochs'] = [budget]\n\n        if 'overhead' in self.info_dict:\n            self.info_dict['overhead'].append(overhead)\n        else:\n            self.info_dict['overhead'] = [overhead]\n\n        with open(os.path.join(self.output_path, f'{self.dataset_name}_{self.seed}.json'), 'w') as fp:\n            json.dump(self.info_dict, fp)\n\n    def preprocess_hp_candidates(self) -> List:\n        \"\"\"\n        Preprocess the list of all hyperparameter candidates\n        by  performing a log transform for the hyperparameters that\n        were log sampled.\n\n        Returns:\n            log_hp_candidates: The list of all hyperparameter configurations\n                where hyperparameters that were log sampled are log transformed.\n        \"\"\"\n        log_hp_candidates = []\n\n        for hp_candidate in self.hp_candidates:\n            new_hp_candidate = []\n            for index, hp_value in enumerate(hp_candidate):\n                new_hp_candidate.append(math.log(hp_value) if self.log_indicator[index] else hp_value)\n\n            log_hp_candidates.append(new_hp_candidate)\n\n        log_hp_candidates = np.array(log_hp_candidates)\n        # scaler for the hp configurations\n\n        log_hp_candidates = self.scaler.fit_transform(log_hp_candidates)\n\n        return log_hp_candidates\n\n    @staticmethod\n    def patch_curves_to_same_length(curves):\n        \"\"\"\n        Patch the given curves to the same length.\n\n        Finds the maximum curve length and patches all\n        other curves that are shorter in length with zeroes.\n\n        Args:\n            curves: The given hyperparameter curves.\n\n        Returns:\n            curves: The updated array where the learning\n                curves are of the same length.\n        \"\"\"\n        max_curve_length = 0\n        for curve in curves:\n            if len(curve) > max_curve_length:\n                max_curve_length = len(curve)\n\n        for curve in curves:\n            difference = max_curve_length - len(curve)\n            if difference > 0:\n                curve.extend([0.0] * difference)\n\n        return curves\nclass FeatureExtractor(nn.Module):\n    \"\"\"\n    The feature extractor that is part of the deep kernel.\n    \"\"\"\n    def __init__(self, configuration):\n        super(FeatureExtractor, self).__init__()\n\n        self.configuration = configuration\n\n        self.nr_layers = configuration['nr_layers']\n        self.act_func = nn.LeakyReLU()\n        # adding one to the dimensionality of the initial input features\n        # for the concatenation with the budget.\n        initial_features = configuration['nr_initial_features'] + 1\n        self.fc1 = nn.Linear(initial_features, configuration['layer1_units'])\n        self.bn1 = nn.BatchNorm1d(configuration['layer1_units'])\n        for i in range(2, self.nr_layers):\n            setattr(\n                self,\n                f'fc{i + 1}',\n                nn.Linear(configuration[f'layer{i - 1}_units'], configuration[f'layer{i}_units']),\n            )\n            setattr(\n                self,\n                f'bn{i + 1}',\n                nn.BatchNorm1d(configuration[f'layer{i}_units']),\n            )\n\n\n        setattr(\n            self,\n            f'fc{self.nr_layers}',\n            nn.Linear(\n                configuration[f'layer{self.nr_layers - 1}_units'] +\n                configuration['cnn_nr_channels'],  # accounting for the learning curve features\n                configuration[f'layer{self.nr_layers}_units']\n            ),\n        )\n        self.cnn = nn.Sequential(\n            nn.Conv1d(in_channels=1, kernel_size=(configuration['cnn_kernel_size'],), out_channels=4),\n            nn.AdaptiveMaxPool1d(1),\n        )\n\n    def forward(self, x, budgets, learning_curves):\n\n        # add an extra dimensionality for the budget\n        # making it nr_rows x 1.\n        budgets = torch.unsqueeze(budgets, dim=1)\n        # concatenate budgets with examples\n        x = cat((x, budgets), dim=1)\n        x = self.fc1(x)\n        x = self.act_func(self.bn1(x))\n\n        for i in range(2, self.nr_layers):\n            x = self.act_func(\n                getattr(self, f'bn{i}')(\n                    getattr(self, f'fc{i}')(\n                        x\n                    )\n                )\n            )\n\n        # add an extra dimensionality for the learning curve\n        # making it nr_rows x 1 x lc_values.\n        learning_curves = torch.unsqueeze(learning_curves, 1)\n        lc_features = self.cnn(learning_curves)\n        # revert the output from the cnn into nr_rows x nr_kernels.\n        lc_features = torch.squeeze(lc_features, 2)\n\n        # put learning curve features into the last layer along with the higher level features.\n        x = cat((x, lc_features), dim=1)\n        x = self.act_func(getattr(self, f'fc{self.nr_layers}')(x))\n\n        return x\n\n\nclass GPRegressionModel(gpytorch.models.ExactGP):\n    \"\"\"\n    A simple GP model.\n    \"\"\"\n    def __init__(\n        self,\n        train_x: torch.Tensor,\n        train_y: torch.Tensor,\n        likelihood: gpytorch.likelihoods.GaussianLikelihood,\n    ):\n        \"\"\"\n        Constructor of the GPRegressionModel.\n\n        Args:\n            train_x: The initial train examples for the GP.\n            train_y: The initial train labels for the GP.\n            likelihood: The likelihood to be used.\n        \"\"\"\n        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n\n        self.mean_module = gpytorch.means.ConstantMean()\n        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n\n    def forward(self, x):\n\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n\n        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n\n\nclass DyHPO:\n    \"\"\"\n    The DyHPO DeepGP model.\n    \"\"\"\n    def __init__(\n        self,\n        configuration: Dict,\n        device: torch.device,\n        dataset_name: str = 'unknown',\n        output_path: str = '.',\n        seed: int = 11,\n    ):\n        \"\"\"\n        The constructor for the DyHPO model.\n\n        Args:\n            configuration: The configuration to be used\n                for the different parts of the surrogate.\n            device: The device where the experiments will be run on.\n            dataset_name: The name of the dataset for the current run.\n            output_path: The path where the intermediate/final results\n                will be stored.\n            seed: The seed that will be used to store the checkpoint\n                properly.\n        \"\"\"\n        super(DyHPO, self).__init__()\n        self.feature_extractor = FeatureExtractor(configuration)\n        self.batch_size = configuration['batch_size']\n        self.nr_epochs = configuration['nr_epochs']\n        self.early_stopping_patience = configuration['nr_patience_epochs']\n        self.refine_epochs = 50\n        self.dev = device\n        self.seed = seed\n        self.model, self.likelihood, self.mll = \\\n            self.get_model_likelihood_mll(\n                configuration[f'layer{self.feature_extractor.nr_layers}_units']\n            )\n\n        self.model.to(self.dev)\n        self.likelihood.to(self.dev)\n        self.feature_extractor.to(self.dev)\n\n        self.optimizer = torch.optim.Adam([\n            {'params': self.model.parameters(), 'lr': configuration['learning_rate']},\n            {'params': self.feature_extractor.parameters(), 'lr': configuration['learning_rate']}],\n        )\n\n        self.configuration = configuration\n        # the number of initial points for which we will retrain fully from scratch\n        # This is basically equal to the dimensionality of the search space + 1.\n        self.initial_nr_points = 10\n        # keeping track of the total hpo iterations. It will be used during the optimization\n        # process to switch from fully training the model, to refining.\n        self.iterations = 0\n        # flag for when the optimization of the model should start from scratch.\n        self.restart = True\n\n        self.logger = logging.getLogger(__name__)\n\n        self.checkpoint_path = os.path.join(\n            output_path,\n            'checkpoints',\n            f'{dataset_name}',\n            f'{self.seed}',\n        )\n\n        os.makedirs(self.checkpoint_path, exist_ok=True)\n\n        self.checkpoint_file = os.path.join(\n            self.checkpoint_path,\n            'checkpoint.pth'\n        )\n\n    def restart_optimization(self):\n        \"\"\"\n        Restart the surrogate model from scratch.\n        \"\"\"\n        self.feature_extractor = FeatureExtractor(self.configuration).to(self.dev)\n        self.model, self.likelihood, self.mll = \\\n            self.get_model_likelihood_mll(\n                self.configuration[f'layer{self.feature_extractor.nr_layers}_units'],\n            )\n\n        self.optimizer = torch.optim.Adam([\n            {'params': self.model.parameters(), 'lr': self.configuration['learning_rate']},\n            {'params': self.feature_extractor.parameters(), 'lr': self.configuration['learning_rate']}],\n        )\n\n    def get_model_likelihood_mll(\n        self,\n        train_size: int,\n    ) -> Tuple[GPRegressionModel, gpytorch.likelihoods.GaussianLikelihood, gpytorch.mlls.ExactMarginalLogLikelihood]:\n        \"\"\"\n        Called when the surrogate is first initialized or restarted.\n\n        Args:\n            train_size: The size of the current training set.\n\n        Returns:\n            model, likelihood, mll - The GP model, the likelihood and\n                the marginal likelihood.\n        \"\"\"\n        train_x = torch.ones(train_size, train_size).to(self.dev)\n        train_y = torch.ones(train_size).to(self.dev)\n\n        likelihood = gpytorch.likelihoods.GaussianLikelihood().to(self.dev)\n        model = GPRegressionModel(train_x=train_x, train_y=train_y, likelihood=likelihood).to(self.dev)\n        mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model).to(self.dev)\n\n        return model, likelihood, mll\n\n    def train_pipeline(self, data: Dict[str, torch.Tensor], load_checkpoint: bool = False):\n        \"\"\"\n        Train the surrogate model.\n\n        Args:\n            data: A dictionary which has the training examples, training features,\n                training budgets and in the end the training curves.\n            load_checkpoint: A flag whether to load the state from a previous checkpoint,\n                or whether to start from scratch.\n        \"\"\"\n        self.iterations += 1\n        self.logger.debug(f'Starting iteration: {self.iterations}')\n        # whether the state has been changed. Basically, if a better loss was found during\n        # this optimization iteration then the state (weights) were changed.\n        weights_changed = False\n\n        if load_checkpoint:\n            try:\n                self.load_checkpoint()\n            except FileNotFoundError:\n                self.logger.error(f'No checkpoint file found at: {self.checkpoint_file}'\n                                  f'Training the GP from the beginning')\n\n        self.model.train()\n        self.likelihood.train()\n        self.feature_extractor.train()\n\n        self.optimizer = torch.optim.Adam([\n            {'params': self.model.parameters(), 'lr': self.configuration['learning_rate']},\n            {'params': self.feature_extractor.parameters(), 'lr': self.configuration['learning_rate']}],\n        )\n\n        X_train = data['X_train']\n        train_budgets = data['train_budgets']\n        train_curves = data['train_curves']\n        y_train = data['y_train']\n\n        initial_state = self.get_state()\n        training_errored = False\n\n        if self.restart:\n            self.restart_optimization()\n            nr_epochs = self.nr_epochs\n            # 2 cases where the statement below is hit.\n            # - We are switching from the full training phase in the beginning to refining.\n            # - We are restarting because our refining diverged\n            if self.initial_nr_points <= self.iterations:\n                self.restart = False\n        else:\n            nr_epochs = self.refine_epochs\n\n        # where the mean squared error will be stored\n        # when predicting on the train set\n        mse = 0.0\n\n        for epoch_nr in range(0, nr_epochs):\n\n            nr_examples_batch = X_train.size(dim=0)\n            # if only one example in the batch, skip the batch.\n            # Otherwise, the code will fail because of batchnorm\n            if nr_examples_batch == 1:\n                continue\n\n            # Zero backprop gradients\n            self.optimizer.zero_grad()\n\n            projected_x = self.feature_extractor(X_train, train_budgets, train_curves)\n            self.model.set_train_data(projected_x, y_train, strict=False)\n            output = self.model(projected_x)\n\n            try:\n                # Calc loss and backprop derivatives\n                loss = -self.mll(output, self.model.train_targets)\n                loss_value = loss.detach().to('cpu').item()\n                mse = gpytorch.metrics.mean_squared_error(output, self.model.train_targets)\n                self.logger.debug(\n                    f'Epoch {epoch_nr} - MSE {mse:.5f}, '\n                    f'Loss: {loss_value:.3f}, '\n                    f'lengthscale: {self.model.covar_module.base_kernel.lengthscale.item():.3f}, '\n                    f'noise: {self.model.likelihood.noise.item():.3f}, '\n                )\n                loss.backward()\n                self.optimizer.step()\n            except Exception as training_error:\n                self.logger.error(f'The following error happened while training: {training_error}')\n                # An error has happened, trigger the restart of the optimization and restart\n                # the model with default hyperparameters.\n                self.restart = True\n                training_errored = True\n                break\n\n        \"\"\"\n        # metric too high, time to restart, or we risk divergence\n        if mse > 0.15:\n            if not self.restart:\n                self.restart = True\n        \"\"\"\n        if training_errored:\n            self.save_checkpoint(initial_state)\n            self.load_checkpoint()\n\n    def predict_pipeline(\n        self,\n        train_data: Dict[str, torch.Tensor],\n        test_data: Dict[str, torch.Tensor],\n    ) -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n\n        Args:\n            train_data: A dictionary that has the training\n                examples, features, budgets and learning curves.\n            test_data: Same as for the training data, but it is\n                for the testing part and it does not feature labels.\n\n        Returns:\n            means, stds: The means of the predictions for the\n                testing points and the standard deviations.\n        \"\"\"\n        self.model.eval()\n        self.feature_extractor.eval()\n        self.likelihood.eval()\n\n        with torch.no_grad(): # gpytorch.settings.fast_pred_var():\n            projected_train_x = self.feature_extractor(\n                train_data['X_train'],\n                train_data['train_budgets'],\n                train_data['train_curves'],\n            )\n            self.model.set_train_data(inputs=projected_train_x, targets=train_data['y_train'], strict=False)\n            projected_test_x = self.feature_extractor(\n                test_data['X_test'],\n                test_data['test_budgets'],\n                test_data['test_curves'],\n            )\n            preds = self.likelihood(self.model(projected_test_x))\n\n        means = preds.mean.detach().to('cpu').numpy().reshape(-1, )\n        stds = preds.stddev.detach().to('cpu').numpy().reshape(-1, )\n\n        return means, stds\n\n    def load_checkpoint(self):\n        \"\"\"\n        Load the state from a previous checkpoint.\n        \"\"\"\n        checkpoint = torch.load(self.checkpoint_file)\n        self.model.load_state_dict(checkpoint['gp_state_dict'])\n        self.feature_extractor.load_state_dict(checkpoint['feature_extractor_state_dict'])\n        self.likelihood.load_state_dict(checkpoint['likelihood_state_dict'])\n\n    def save_checkpoint(self, state: Dict =None):\n        \"\"\"\n        Save the given state or the current state in a\n        checkpoint file.\n\n        Args:\n            state: The state to save, if none, it will\n            save the current state.\n        \"\"\"\n\n        if state is None:\n            torch.save(\n                self.get_state(),\n                self.checkpoint_file,\n            )\n        else:\n            torch.save(\n                state,\n                self.checkpoint_file,\n            )\n\n    def get_state(self) -> Dict[str, Dict]:\n        \"\"\"\n        Get the current state of the surrogate.\n\n        Returns:\n            current_state: A dictionary that represents\n                the current state of the surrogate model.\n        \"\"\"\n        current_state = {\n            'gp_state_dict': deepcopy(self.model.state_dict()),\n            'feature_extractor_state_dict': deepcopy(self.feature_extractor.state_dict()),\n            'likelihood_state_dict': deepcopy(self.likelihood.state_dict()),\n        }\n\n        return current_state",
    "Experiment Result": "Experimental Setup for DyHPO:\n- **Benchmarks**: LCBench or TaskSet. Defaults to LCBench.\n- **Datasets**: Defaults to 'covertype' for LCBench. Other datasets can be loaded from the benchmark data path.\n- **Optimization Objective**: Maximization for LCBench (accuracy), Minimization for TaskSet (loss).\n- **Random Seeds**: Experiments are run for 10 different random seeds (0 to 9, controlled by `args.index`).\n- **Total HPO Budget (`budget_limit`)**: The total budget allowed for hyperparameter optimization is 1000 epochs/steps.\n- **Fantasize Step (`fantasize_step`)**: The number of budget steps to look ahead when evaluating a hyperparameter configuration is 1.\n- **Max Benchmark Epochs (`max_benchmark_epochs`)**: 51 for both LCBench and TaskSet benchmarks.\n- **Initial Configurations**: 1 randomly chosen hyperparameter configuration is evaluated initially.\n- **Individual Initial Budget**: The initial budget for the first random configuration is 1 epoch/step.\n- **Fraction of Random Configurations**: 0.1 (10% of configurations will be taken randomly instead of being sampled from the model, though this is overridden by the fixed `initial_configurations_nr` for the very first step).\n\n**DyHPO Surrogate Model Configuration (Default if not overridden):**\n- **Feature Extractor (Neural Network)**:\n    - `nr_layers`: 2 (number of linear layers).\n    - `nr_initial_features`: Determined by the dimensionality of the hyperparameter search space.\n    - `layer1_units`: 64 (units in the first linear layer).\n    - `layer2_units`: 128 (units in the second linear layer).\n    - `cnn_nr_channels`: 4 (output channels for the 1D CNN processing learning curves).\n    - `cnn_kernel_size`: 3 (kernel size for the 1D CNN).\n- **Gaussian Process Training**:\n    - `batch_size`: 64.\n    - `nr_epochs`: 1000 (total epochs for full training).\n    - `nr_patience_epochs`: 10 (epochs for early stopping patience).\n    - `refine_epochs`: 50 (epochs for refining, after initial full training or restart).\n    - `learning_rate`: 0.001 (for Adam optimizer).\n- **Acquisition Function**: Expected Improvement ('ei') is used.\n- **Hyperparameter Preprocessing**: Log transformation is applied to hyperparameters indicated as `log_indicator=True` (e.g., learning rate, batch size for LCBench, learning rate for TaskSet). All hyperparameters are then scaled to [0, 1] using `MinMaxScaler`."
}{
    "Title": "Supervising the Multi-Fidelity Race of Hyperparameter Configurations",
    "Main Contributions": "The paper introduces DyHPO, a novel Bayesian Optimization method for multi-fidelity hyperparameter optimization (HPO) that dynamically allocates budget to hyperparameter configurations in a racing fashion. Key contributions include a new deep kernel for Gaussian Processes that embeds learning curve dynamics to capture configuration similarity across different budgets, and a multi-budget acquisition function (EIMF) that extends Expected Improvement. DyHPO achieves significant empirical superiority, faster convergence, and better anytime performance compared to state-of-the-art HPO methods, demonstrated across 50 diverse datasets (tabular, image, NLP) and architectures (MLP, CNN/NAS, RNN). The method efficiently explores the search space by identifying promising candidates and effectively handling the issue of poor rank correlations between performances at different budgets.",
    "Methodology": "DyHPO employs a Bayesian Optimization framework built upon Gaussian Processes (GP). The core of its methodology lies in a deep multi-fidelity surrogate model. This model utilizes a deep kernel that explicitly incorporates the hyperparameter configuration, the budget information, and the observed learning curve to predict validation scores. The deep kernel uses a neural network (φ) as a feature extractor, combining linear layers for concatenated hyperparameter configuration and normalized budget, and a one-dimensional convolutional layer with global max pooling for the learning curve. These features are then fed into a squared exponential kernel. The parameters of both the kernel and the neural network are optimized by maximizing marginal likelihood using gradient descent with Adam. For selecting the next configuration, DyHPO uses a Multi-Fidelity Expected Improvement (EIMF) acquisition function. Instead of maximizing the acquisition function over both configuration and budget, it dynamically increases the budget for a selected configuration by exactly one step, ensuring a slow, incremental exploration of configurations.",
    "Experimental Setup": "The DyHPO method was evaluated on hyperparameter optimization tasks for three different data modalities: tabular, text, and image classification. All experiments were conducted on an Amazon EC2 M5 Instance (m5.xlarge). Performance was measured using mean regret and average rank across ten repetitions. Three main benchmarks were used: LCBench (35 tabular datasets, 2,000 feedforward neural network configurations, trained for 50 epochs with 7 numerical hyperparameters), TaskSet (12 NLP tasks from Adam8p search space with 8 continuous hyperparameters for RNNs, evaluated in 'steps' of 200 iterations), and NAS-Bench-201 (3 image classification datasets: CIFAR-10, CIFAR-100, ImageNet, featuring 15,625 precomputed architectures with 6 categorical hyperparameters, each trained for 200 epochs). DyHPO's performance was compared against seven strong baselines: Random Search, HyperBand, BOHB, DEHB, ASHA, MF-DNN, and Dragonfly (implementing BOCA). Preprocessing involved log-transformations for specific numerical hyperparameters and min-max scaling for all continuous ones, with categorical hyperparameters handled via one-hot encoding or baseline-specific numerical conversions.",
    "Limitations": "The authors acknowledge several limitations. Firstly, the method's characterization as a 'step towards' scaling HPO for deep learning is cautious due to the lack of suitable tabular benchmarks for very large deep learning models, such as Transformer-based architectures. Secondly, the 'pause and resume' training procedure inherent to DyHPO is only applicable to parametric models; for non-parametric models, training a hyperparameter configuration would need to be restarted from scratch. Lastly, for small datasets that can be trained quickly, the computational overhead associated with model-based techniques like DyHPO might make simpler approaches, such as random search, more appealing due to their lower overhead. The paper also implicitly warns against extending optimization for marginal gains unless mission-critical.",
    "Future Research Directions": "The paper implies several future research directions. One key area is the extension of DyHPO to tackle hyperparameter optimization for very large deep learning models, particularly Transformer-based architectures, necessitating the development of appropriate benchmarks for such models. Another direction involves reducing the computational overhead of DyHPO for tasks involving small datasets, where faster training times make simpler HPO methods more competitive. Furthermore, the authors invite the community to contribute to the creation of sparse benchmarks with surrogates, as opposed to dense tabular ones, which could facilitate more efficient HPO evaluations and potentially enable the scaling of HPO to even larger and more complex deep learning scenarios.",
    "Experiment Code": "class FeatureExtractor(nn.Module):    \"\"\"    The feature extractor that is part of the deep kernel.    \"\"\"    def __init__(self, configuration):        super(FeatureExtractor, self).__init__()        self.configuration = configuration        self.nr_layers = configuration['nr_layers']        self.act_func = nn.LeakyReLU()        # adding one to the dimensionality of the initial input features        # for the concatenation with the budget.        initial_features = configuration['nr_initial_features'] + 1        self.fc1 = nn.Linear(initial_features, configuration['layer1_units'])        self.bn1 = nn.BatchNorm1d(configuration['layer1_units'])        for i in range(2, self.nr_layers):            setattr(                self,                f'fc{i + 1}',                nn.Linear(configuration[f'layer{i - 1}_units'], configuration[f'layer{i}_units']),            )            setattr(                self,                f'bn{i + 1}',                nn.BatchNorm1d(configuration[f'layer{i}_units']),            )        setattr(            self,            f'fc{self.nr_layers}',            nn.Linear(                configuration[f'layer{self.nr_layers - 1}_units'] +                configuration['cnn_nr_channels'],  # accounting for the learning curve features                configuration[f'layer{self.nr_layers}_units']            ),        )        self.cnn = nn.Sequential(            nn.Conv1d(in_channels=1, kernel_size=(configuration['cnn_kernel_size'],), out_channels=4),            nn.AdaptiveMaxPool1d(1),        )    def forward(self, x, budgets, learning_curves):        # add an extra dimensionality for the budget        # making it nr_rows x 1.        budgets = torch.unsqueeze(budgets, dim=1)        # concatenate budgets with examples        x = cat((x, budgets), dim=1)        x = self.fc1(x)        x = self.act_func(self.bn1(x))        for i in range(2, self.nr_layers):            x = self.act_func(                getattr(self, f'bn{i}')(                    getattr(self, f'fc{i}')(                        x                    )                )            )        # add an extra dimensionality for the learning curve        # making it nr_rows x 1 x lc_values.        learning_curves = torch.unsqueeze(learning_curves, 1)        lc_features = self.cnn(learning_curves)        # revert the output from the cnn into nr_rows x nr_kernels.        lc_features = torch.squeeze(lc_features, 2)        # put learning curve features into the last layer along with the higher level features.        x = cat((x, lc_features), dim=1)        x = self.act_func(getattr(self, f'fc{self.nr_layers}')(x))        return x}class GPRegressionModel(gpytorch.models.ExactGP):    \"\"\"    A simple GP model.    \"\"\"    def __init__(        self,        train_x: torch.Tensor,        train_y: torch.Tensor,        likelihood: gpytorch.likelihoods.GaussianLikelihood,    ):        \"\"\"        Constructor of the GPRegressionModel.        Args:            train_x: The initial train examples for the GP.            train_y: The initial train labels for the GP.            likelihood: The likelihood to be used.        \"\"\"        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)        self.mean_module = gpytorch.means.ConstantMean()        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())    def forward(self, x):        mean_x = self.mean_module(x)        covar_x = self.covar_module(x)        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)}class DyHPO:    \"\"\"    The DyHPO DeepGP model.    \"\"\"    def __init__(        self,        configuration: Dict,        device: torch.device,        dataset_name: str = 'unknown',        output_path: str = '.',        seed: int = 11,    ):        \"\"\"        The constructor for the DyHPO model.        Args:            configuration: The configuration to be used                for the different parts of the surrogate.            device: The device where the experiments will be run on.            dataset_name: The name of the dataset for the current run.            output_path: The path where the intermediate/final results                will be stored.            seed: The seed that will be used to store the checkpoint                properly.        \"\"\"        super(DyHPO, self).__init__()        self.feature_extractor = FeatureExtractor(configuration)        self.batch_size = configuration['batch_size']        self.nr_epochs = configuration['nr_epochs']        self.early_stopping_patience = configuration['nr_patience_epochs']        self.refine_epochs = 50        self.dev = device        self.seed = seed        self.model, self.likelihood, self.mll = \r            self.get_model_likelihood_mll(                configuration[f'layer{self.feature_extractor.nr_layers}_units']            )        self.model.to(self.dev)        self.likelihood.to(self.dev)        self.feature_extractor.to(self.dev)        self.optimizer = torch.optim.Adam([            {'params': self.model.parameters(), 'lr': configuration['learning_rate']},            {'params': self.feature_extractor.parameters(), 'lr': configuration['learning_rate']}],        )        self.configuration = configuration        # the number of initial points for which we will retrain fully from scratch        # This is basically equal to the dimensionality of the search space + 1.        self.initial_nr_points = 10        # keeping track of the total hpo iterations. It will be used during the optimization        # process to switch from fully training the model, to refining.        self.iterations = 0        # flag for when the optimization of the model should start from scratch.        self.restart = True        self.logger = logging.getLogger(__name__)        self.checkpoint_path = os.path.join(            output_path,            'checkpoints',            f'{dataset_name}',            f'{self.seed}',        )        os.makedirs(self.checkpoint_path, exist_ok=True)        self.checkpoint_file = os.path.join(            self.checkpoint_path,            'checkpoint.pth'        )    def restart_optimization(self):        \"\"\"        Restart the surrogate model from scratch.        \"\"\"        self.feature_extractor = FeatureExtractor(self.configuration).to(self.dev)        self.model, self.likelihood, self.mll = \r            self.get_model_likelihood_mll(                self.configuration[f'layer{self.feature_extractor.nr_layers}_units'],            )        self.optimizer = torch.optim.Adam([            {'params': self.model.parameters(), 'lr': self.configuration['learning_rate']},            {'params': self.feature_extractor.parameters(), 'lr': self.configuration['learning_rate']}],        )    def get_model_likelihood_mll(        self,        train_size: int,    ) -> Tuple[GPRegressionModel, gpytorch.likelihoods.GaussianLikelihood, gpytorch.mlls.ExactMarginalLogLikelihood]:        \"\"\"        Called when the surrogate is first initialized or restarted.        Args:            train_size: The size of the current training set.        Returns:            model, likelihood, mll - The GP model, the likelihood and                the marginal likelihood.        \"\"\"        train_x = torch.ones(train_size, train_size).to(self.dev)        train_y = torch.ones(train_size).to(self.dev)        likelihood = gpytorch.likelihoods.GaussianLikelihood().to(self.dev)        model = GPRegressionModel(train_x=train_x, train_y=train_y, likelihood=likelihood).to(self.dev)        mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model).to(self.dev)        return model, likelihood, mll    def train_pipeline(self, data: Dict[str, torch.Tensor], load_checkpoint: bool = False):        \"\"\"        Train the surrogate model.        Args:            data: A dictionary which has the training examples, training features,                training budgets and in the end the training curves.            load_checkpoint: A flag whether to load the state from a previous checkpoint,                or whether to start from scratch.        \"\"\"        self.iterations += 1        self.logger.debug(f'Starting iteration: {self.iterations}')        # whether the state has been changed. Basically, if a better loss was found during        # this optimization iteration then the state (weights) were changed.        weights_changed = False        if load_checkpoint:            try:                self.load_checkpoint()            except FileNotFoundError:                self.logger.error(f'No checkpoint file found at: {self.checkpoint_file}'                                  f'Training the GP from the beginning')        self.model.train()        self.likelihood.train()        self.feature_extractor.train()        self.optimizer = torch.optim.Adam([            {'params': self.model.parameters(), 'lr': self.configuration['learning_rate']},            {'params': self.feature_extractor.parameters(), 'lr': self.configuration['learning_rate']}],        )        X_train = data['X_train']        train_budgets = data['train_budgets']        train_curves = data['train_curves']        y_train = data['y_train']        initial_state = self.get_state()        training_errored = False        if self.restart:            self.restart_optimization()            nr_epochs = self.nr_epochs            # 2 cases where the statement below is hit.            # - We are switching from the full training phase in the beginning to refining.            # - We are restarting because our refining diverged            if self.initial_nr_points <= self.iterations:                self.restart = False        else:            nr_epochs = self.refine_epochs        # where the mean squared error will be stored        # when predicting on the train set        mse = 0.0        for epoch_nr in range(0, nr_epochs):            nr_examples_batch = X_train.size(dim=0)            # if only one example in the batch, skip the batch.            # Otherwise, the code will fail because of batchnorm            if nr_examples_batch == 1:                continue            # Zero backprop gradients            self.optimizer.zero_grad()            projected_x = self.feature_extractor(X_train, train_budgets, train_curves)            self.model.set_train_data(projected_x, y_train, strict=False)            output = self.model(projected_x)            try:                # Calc loss and backprop derivatives                loss = -self.mll(output, self.model.train_targets)                loss_value = loss.detach().to('cpu').item()                mse = gpytorch.metrics.mean_squared_error(output, self.model.train_targets)                self.logger.debug(                    f'Epoch {epoch_nr} - MSE {mse:.5f}, '                    f'Loss: {loss_value:.3f}, '                    f'lengthscale: {self.model.covar_module.base_kernel.lengthscale.item():.3f}, '                    f'noise: {self.model.likelihood.noise.item():.3f}, '                )                loss.backward()                self.optimizer.step()            except Exception as training_error:                self.logger.error(f'The following error happened while training: {training_error}')                # An error has happened, trigger the restart of the optimization and restart                # the model with default hyperparameters.                self.restart = True                training_errored = True                break        \"\"\"        # metric too high, time to restart, or we risk divergence        if mse > 0.15:            if not self.restart:                self.restart = True        \"\"\"        if training_errored:            self.save_checkpoint(initial_state)            self.load_checkpoint()    def predict_pipeline(        self,        train_data: Dict[str, torch.Tensor],        test_data: Dict[str, torch.Tensor],    ) -> Tuple[np.ndarray, np.ndarray]:        \"\"\"        Args:            train_data: A dictionary that has the training                examples, features, budgets and learning curves.            test_data: Same as for the training data, but it is                for the testing part and it does not feature labels.        Returns:            means, stds: The means of the predictions for the                testing points and the standard deviations.        \"\"\"        self.model.eval()        self.feature_extractor.eval()        self.likelihood.eval()        with torch.no_grad(): # gpytorch.settings.fast_pred_var():            projected_train_x = self.feature_extractor(                train_data['X_train'],                train_data['train_budgets'],                train_data['train_curves'],            )            self.model.set_train_data(inputs=projected_train_x, targets=train_data['y_train'], strict=False)            projected_test_x = self.feature_extractor(                test_data['X_test'],                test_data['test_budgets'],                test_data['test_curves'],            )            preds = self.likelihood(self.model(projected_test_x))        means = preds.mean.detach().to('cpu').numpy().reshape(-1, )        stds = preds.stddev.detach().to('cpu').numpy().reshape(-1, )        return means, stds}class DyHPOAlgorithm:    def __init__(        self,        hp_candidates: np.ndarray,        log_indicator: List,        seed: int = 11,        max_benchmark_epochs: int = 52,        fantasize_step: int = 1,        minimization: bool = True,        total_budget: int = 500,        device: str = None,        dataset_name: str = 'unknown',        output_path: str = '.',        surrogate_config: dict = None,        verbose: bool = True,    ):        \"\"\"        Args:            hp_candidates: np.ndarray                The full list of hyperparameter candidates for                a given dataset.            log_indicator: List                A list with boolean values indicating if a                hyperparameter has been log sampled or not.            seed: int                The seed that will be used for the surrogate.            max_benchmark_epochs: int                The maximal budget that a hyperparameter configuration                has been evaluated in the benchmark for.            fantasize_step: int                The number of steps for which we are looking ahead to                evaluate the performance of a hpc.            minimization: bool                If the objective should be maximized or minimized.            total_budget: int                The total budget given for hyperparameter optimization.            device: str                The device where the experiment will be run on.            dataset_name: str                The name of the dataset that the experiment will be run on.            output_path: str                The path where all the output will be stored.            surrogate_config: dict                The model configurations for the surrogate.            verbose: boolean                If detailed information is preferred in the log file.        \"\"\"        torch.backends.cudnn.deterministic = True        torch.backends.cudnn.benchmark = False        torch.manual_seed(seed)        np.random.seed(seed)        if device is None:            self.dev = torch.device(                'cuda') if torch.cuda.is_available() else torch.device('cpu')        else:            self.dev = torch.device(device)        self.hp_candidates = hp_candidates        self.log_indicator = log_indicator        self.scaler = MinMaxScaler()        self.hp_candidates = self.preprocess_hp_candidates()        self.minimization = minimization        self.seed = seed        if verbose:            logging_level = logging.DEBUG        else:            logging_level = logging.INFO        self.logger = logging.getLogger()        logging.basicConfig(            format='%(levelname)s:%(asctime)s:%(message)s',            filename=f'dyhpo_surrogate_{dataset_name}_{seed}.log',            level=logging_level,        )        # the keys will be hyperparameter indices while the value        # will be a list with all the budgets evaluated for examples        # and with all performances for the performances        self.examples = dict()        self.performances = dict()        # set a seed already, so that it is deterministic when        # generating the seeds of the ensemble        torch.manual_seed(seed)        np.random.seed(seed)        self.max_benchmark_epochs = max_benchmark_epochs        self.total_budget = total_budget        self.fantasize_step = fantasize_step        self.nr_features = self.hp_candidates.shape[1]        initial_configurations_nr = 1        conf_individual_budget = 1        self.init_conf_indices = np.random.choice(self.hp_candidates.shape[0], initial_configurations_nr, replace=False)        self.init_budgets = [conf_individual_budget] * initial_configurations_nr        # with what percentage configurations will be taken randomly instead of being sampled from the model        self.fraction_random_configs = 0.1        self.model = None        # An index keeping track of where we are in the init_conf_indices        # list of hyperparmeters that are not sampled from the model.        self.initial_random_index = 0        if surrogate_config is None:            self.surrogate_config = {                'nr_layers': 2,                'nr_initial_features': self.nr_features,                'layer1_units': 64,                'layer2_units': 128,                'cnn_nr_channels': 4,                'cnn_kernel_size': 3,                'batch_size': 64,                'nr_epochs': 1000,                'nr_patience_epochs': 10,                'learning_rate': 0.001,            }        else:            self.surrogate_config = surrogate_config        # the incumbent value observed during the hpo process.        self.best_value_observed = np.NINF        # a set which will keep track of the hyperparameter configurations that diverge.        self.diverged_configs = set()        # info dict to drop every surrogate iteration        self.info_dict = dict()        # the start time for the overhead of every surrogate optimization iteration        # will be recorded here        self.suggest_time_duration = 0        # the total budget consumed so far        self.budget_spent = 0        self.output_path = output_path        self.dataset_name = dataset_name        self.no_improvement_threshold = int(self.max_benchmark_epochs + 0.2 * self.max_benchmark_epochs)        self.no_improvement_patience = 0    def _prepare_dataset_and_budgets(self) -> Dict[str, torch.Tensor]:        \"\"\"        Prepare the data that will be the input to the surrogate.        Returns:            data: A Dictionary that contains inside the training examples,            the budgets, the curves and lastly the labels.        \"\"\"        train_examples, train_labels, train_budgets, train_curves = self.history_configurations()        train_examples = np.array(train_examples, dtype=np.single)        train_labels = np.array(train_labels, dtype=np.single)        train_budgets = np.array(train_budgets, dtype=np.single)        train_curves = self.patch_curves_to_same_length(train_curves)        train_curves = np.array(train_curves, dtype=np.single)        # scale budgets to [0, 1]        train_budgets = train_budgets / self.max_benchmark_epochs        train_examples = torch.tensor(train_examples)        train_labels = torch.tensor(train_labels)        train_budgets = torch.tensor(train_budgets)        train_curves = torch.tensor(train_curves)        train_examples = train_examples.to(device=self.dev)        train_labels = train_labels.to(device=self.dev)        train_budgets = train_budgets.to(device=self.dev)        train_curves = train_curves.to(device=self.dev)        data = {            'X_train': train_examples,            'train_budgets': train_budgets,            'train_curves': train_curves,            'y_train': train_labels,        }        return data    def _train_surrogate(self):        \"\"\"        Train the surrogate model.        \"\"\"        data = self._prepare_dataset_and_budgets()        self.logger.info(f'Started training the model')        self.model.train_pipeline(            data,            load_checkpoint=False,        )    def _predict(self) -> Tuple[np.ndarray, np.ndarray, List, List]:        \"\"\"        Predict the performances of the hyperparameter configurations        as well as the standard deviations based on the surrogate model.        Returns:            mean_predictions, std_predictions, hp_indices, non_scaled_budgets:                The mean predictions and the standard deviations over                all model predictions for the given hyperparameter                configurations with their associated indices, scaled and                non-scaled budgets.        \"\"\"        configurations, hp_indices, budgets, learning_curves = self.generate_candidate_configurations()        budgets = np.array(budgets, dtype=np.single)        non_scaled_budgets = copy.deepcopy(budgets)        # scale budgets to [0, 1]        budgets = budgets / self.max_benchmark_epochs        configurations = np.array(configurations, dtype=np.single)        configurations = torch.tensor(configurations)        configurations = configurations.to(device=self.dev)        budgets = torch.tensor(budgets)        budgets = budgets.to(device=self.dev)        learning_curves = self.patch_curves_to_same_length(learning_curves)        learning_curves = np.array(learning_curves, dtype=np.single)        learning_curves = torch.tensor(learning_curves)        learning_curves = learning_curves.to(device=self.dev)        train_data = self._prepare_dataset_and_budgets()        test_data = {            'X_test': configurations,            'test_budgets': budgets,            'test_curves': learning_curves,        }        mean_predictions, std_predictions = self.model.predict_pipeline(train_data, test_data)        return mean_predictions, std_predictions, hp_indices, non_scaled_budgets    def suggest(self) -> Tuple[int, int]:        \"\"\"        Suggest a hyperparameter configuration to be evaluated next.        Returns:            best_config_index, budget: The index of the hyperparamter                configuration to be evaluated and the budget for                what it is going to be evaluated for.        \"\"\"        suggest_time_start = time.time()        # check if we still have random hyperparameters to evaluate        if self.initial_random_index < len(self.init_conf_indices):            self.logger.info(                'Not enough configurations to build a model. '                'Returning randomly sampled configuration'            )            random_indice = self.init_conf_indices[self.initial_random_index]            budget = self.init_budgets[self.initial_random_index]            self.initial_random_index += 1            return random_indice, budget        else:            mean_predictions, std_predictions, hp_indices, non_scaled_budgets = self._predict()            best_prediction_index = self.find_suggested_config(                mean_predictions,                std_predictions,                non_scaled_budgets,            )            \"\"\"            the best prediction index is not always matching with the actual hp index.            Since when evaluating the acq function, we do not consider hyperparameter            candidates that diverged or that are evaluated fully.            \"\"\"            best_config_index = hp_indices[best_prediction_index]            # decide for what budget we will evaluate the most            # promising hyperparameter configuration next.            if best_config_index in self.examples:                evaluated_budgets = self.examples[best_config_index]                max_budget = max(evaluated_budgets)                budget = max_budget + self.fantasize_step                # this would only trigger if fantasize_step is bigger                # than 1                if budget > self.max_benchmark_epochs:                    budget = self.max_benchmark_epochs            else:                budget = self.fantasize_step        suggest_time_end = time.time()        self.suggest_time_duration = suggest_time_end - suggest_time_start        self.budget_spent += self.fantasize_step        # exhausted hpo budget, finish.        if self.budget_spent > self.total_budget:            exit(0)        return best_config_index, budget    def observe(        self,        hp_index: int,        b: int,        learning_curve: np.ndarray,        alg_time: Optional[float] = None,    ):        \"\"\"        Args:            hp_index: The index of the evaluated hyperparameter configuration.            b: The budget for which the hyperparameter configuration was evaluated.            learning_curve: The learning curve of the hyperparameter configuration.            alg_time: The time taken from the algorithm to evaluate the hp configuration.        \"\"\"        score = learning_curve[-1]        # if y is an undefined value, append 0 as the overhead since we finish here.        if np.isnan(learning_curve).any():            self.update_info_dict(hp_index, b, np.nan, 0)            self.diverged_configs.add(hp_index)            return        observe_time_start = time.time()        self.examples[hp_index] = np.arange(1, b + 1).tolist()        self.performances[hp_index] = learning_curve        if self.best_value_observed < score:            self.best_value_observed = score            self.no_improvement_patience = 0        else:            self.no_improvement_patience += 1        observe_time_end = time.time()        train_time_duration = 0        # initialization phase over. Now we can sample from the model.        if self.initial_random_index >= len(self.init_conf_indices):            train_time_start = time.time()            # create the model for the first time            if self.model is None:                # Starting a model from scratch                self.model = DyHPO(                    self.surrogate_config,                    self.dev,                    self.dataset_name,                    self.output_path,                    self.seed,                )            if self.no_improvement_patience == self.no_improvement_threshold:                self.model.restart = True            self._train_surrogate()            train_time_end = time.time()            train_time_duration = train_time_end - train_time_start        observe_time_duration = observe_time_end - observe_time_start        total_duration = observe_time_duration + self.suggest_time_duration + train_time_duration        if alg_time is not None:            total_duration = total_duration + alg_time        self.update_info_dict(hp_index, b, score, total_duration)    def generate_candidate_configurations(        self,    ) -> Tuple[List, List, List, List]:        \"\"\"        Generate candidate configurations that will be        fantasized upon.        Returns:            (configurations, hp_indices, hp_budgets, learning_curves): Tuple                A tuple of configurations, their indices in the hp list                and the budgets that they should be fantasized upon.        \"\"\"        hp_indices = []        hp_budgets = []        learning_curves = []        for hp_index in range(0, self.hp_candidates.shape[0]):            if hp_index in self.examples:                budgets = self.examples[hp_index]                # Take the max budget evaluated for a certain hpc                max_budget = max(budgets)                next_budget = max_budget + self.fantasize_step                # take the learning curve until the point we have evaluated so far                curve = self.performances[hp_index][:max_budget]                # if the curve is shorter than the length of the kernel size,                # pad it with zeros                difference_curve_length = self.surrogate_config['cnn_kernel_size'] - len(curve)                if difference_curve_length > 0:                    curve.extend([0.0] * difference_curve_length)            else:                # The hpc was not evaluated before, so fantasize its                # performance                next_budget = self.fantasize_step                curve = [0, 0, 0]            # this hyperparameter configuration is not evaluated fully            if next_budget <= self.max_benchmark_epochs:                hp_indices.append(hp_index)                hp_budgets.append(next_budget)                learning_curves.append(curve)        configurations = self.prepare_examples(hp_indices)        return configurations, hp_indices, hp_budgets, learning_curves    def history_configurations(        self,    ) -> Tuple[List, List, List, List]:        \"\"\"        Generate the configurations, labels, budgets and curves based on        the history of evaluated configurations.        Returns:            (train_examples, train_labels, train_budgets, train_curves):                A tuple of examples, labels, budgets and curves for the                configurations evaluated so far.        \"\"\"        train_examples = []        train_labels = []        train_budgets = []        train_curves = []        for hp_index in self.examples:            budgets = self.examples[hp_index]            performances = self.performances[hp_index]            example = self.hp_candidates[hp_index]            for budget, performance in zip(budgets, performances):                train_examples.append(example)                train_budgets.append(budget)                train_labels.append(performance)                train_curve = performances[:budget - 1] if budget > 1 else [0.0]                difference_curve_length = self.surrogate_config['cnn_kernel_size'] - len(train_curve)                if difference_curve_length > 0:                    train_curve.extend([0.0] * difference_curve_length)                train_curves.append(train_curve)        return train_examples, train_labels, train_budgets, train_curves    def acq(        self,        best_value: float,        mean: float,        std: float,        explore_factor: Optional[float] = 0.25,        acq_fc: str = 'ei',    ) -> float:        \"\"\"        The acquisition function that will be called        to evaluate the score of a hyperparameter configuration.        Parameters        ----------        best_value: float            Best observed function evaluation. Individual per fidelity.        mean: float            Point mean of the posterior process.        std: float            Point std of the posterior process.        explore_factor: float            The exploration factor for when ucb is used as the            acquisition function.        ei_calibration_factor: float            The factor used to calibrate expected improvement.        acq_fc: str            The type of acquisition function to use.        Returns        -------        acq_value: float            The value of the acquisition function.        \"\"\"        if acq_fc == 'ei':            if std == 0:                return 0            z = (mean - best_value) / std            acq_value = (mean - best_value) * norm.cdf(z) + std * norm.pdf(z)        elif acq_fc == 'ucb':            acq_value = mean + explore_factor * std        elif acq_fc == 'thompson':            acq_value = np.random.normal(mean, std)        elif acq_fc == 'exploit':            acq_value = mean        else:            raise NotImplementedError(                f'Acquisition function {acq_fc} has not been'                f'implemented',            )        return acq_value    def find_suggested_config(        self,        mean_predictions: np.ndarray,        mean_stds: np.ndarray,        budgets: List,    ) -> int:        \"\"\"        Find the hyperparameter configuration that has the highest score        with the acquisition function.        Args:            mean_predictions: The mean predictions of the posterior.            mean_stds: The mean standard deviations of the posterior.            budgets: The next budgets that the hyperparameter configurations                will be evaluated for.        Returns:            best_index: The index of the hyperparameter configuration with the                highest score.        \"\"\"        highest_acq_value = np.NINF        best_index = -1        index = 0        for mean_value, std in zip(mean_predictions, mean_stds):            budget = int(budgets[index])            best_value = self.calculate_fidelity_ymax(budget)            acq_value = self.acq(best_value, mean_value, std, acq_fc='ei')            if acq_value > highest_acq_value:                highest_acq_value = acq_value                best_index = index            index += 1        return best_index    def calculate_fidelity_ymax(self, fidelity: int):        \"\"\"        Find ymax for a given fidelity level.        If there are hyperparameters evaluated for that fidelity        take the maximum from their values. Otherwise, take        the maximum from all previous fidelity levels for the        hyperparameters that we have evaluated.        Args:            fidelity: The fidelity of the hyperparameter                configuration.        Returns:            best_value: The best value seen so far for the                given fidelity.        \"\"\"        exact_fidelity_config_values = []        lower_fidelity_config_values = []        for example_index in self.examples.keys():            try:                performance = self.performances[example_index][fidelity - 1]                exact_fidelity_config_values.append(performance)            except IndexError:                learning_curve = self.performances[example_index]                # The hyperparameter was not evaluated until fidelity, or more.                # Take the maximum value from the curve.                lower_fidelity_config_values.append(max(learning_curve))        if len(exact_fidelity_config_values) > 0:            # lowest error corresponds to best value            best_value = max(exact_fidelity_config_values)        else:            best_value = max(lower_fidelity_config_values)        return best_value    def preprocess_hp_candidates(self) -> List:        \"\"\"        Preprocess the list of all hyperparameter candidates        by  performing a log transform for the hyperparameters that        were log sampled.        Returns:            log_hp_candidates: The list of all hyperparameter configurations                where hyperparameters that were log sampled are log transformed.        \"\"\"        log_hp_candidates = []        for hp_candidate in self.hp_candidates:            new_hp_candidate = []            for index, hp_value in enumerate(hp_candidate):                new_hp_candidate.append(math.log(hp_value) if self.log_indicator[index] else hp_value)        log_hp_candidates = np.array(log_hp_candidates)        # scaler for the hp configurations        log_hp_candidates = self.scaler.fit_transform(log_hp_candidates)        return log_hp_candidates    @staticmethod    def patch_curves_to_same_length(curves):        \"\"\"        Patch the given curves to the same length.        Finds the maximum curve length and patches all        other curves that are shorter in length with zeroes.        Args:            curves: The given hyperparameter curves.        Returns:            curves: The updated array where the learning                curves are of the same length.        \"\"\"        max_curve_length = 0        for curve in curves:            if len(curve) > max_curve_length:                max_curve_length = len(curve)        for curve in curves:            difference = max_curve_length - len(curve)            if difference > 0:                curve.extend([0.0] * difference)        return curves",
    "Experiment Result": "DyHPO employs a Bayesian Optimization framework built upon Gaussian Processes (GP) for hyperparameter optimization. The core is a deep multi-fidelity surrogate model. This model features a deep kernel utilizing a neural network (φ) as a feature extractor. The neural network combines linear layers for concatenated hyperparameter configuration and normalized budget, and a one-dimensional convolutional layer with global max pooling for the learning curve. These extracted features are then fed into a squared exponential kernel (RBFKernel in GPyTorch). The parameters of both the kernel and the neural network are optimized by maximizing marginal likelihood using gradient descent with the Adam optimizer.\r\n\r\nFor selecting the next configuration, DyHPO uses a Multi-Fidelity Expected Improvement (EIMF) acquisition function. Instead of maximizing the acquisition function over both configuration and budget, it dynamically increases the budget for a selected configuration by exactly one step (`fantasize_step=1`).\r\n\r\nExperimental settings include:\r\n- HPO Process:\r\n    - Total budget limit for HPO: 1000 evaluations.\r\n    - Max budget per configuration in benchmark: 51 epochs.\r\n    - Initial configurations: 1 randomly chosen configuration evaluated at budget 1.\r\n    - Fraction of random configurations (not sampled from model): 0.1.\r\n    - Hyperparameter preprocessing: log-transform for log-sampled hyperparameters, followed by MinMaxScaler for all hyperparameters.\r\n    - Learning curves are patched with zeros to the maximum observed length before being fed to the CNN.\r\n    - The optimization objective is minimization for 'TaskSet' and maximization for 'LCBench'.\r\n    - The surrogate model may restart its optimization from scratch if no improvement is observed for a certain number of iterations (`max_benchmark_epochs + 0.2 * max_benchmark_epochs`).\r\n- Surrogate Model Configuration (defaults):\r\n    - Number of hidden layers in the feature extractor NN: 2.\r\n    - Units in hidden layers: 64, 128.\r\n    - CNN channels: 4.\r\n    - CNN kernel size: 3.\r\n    - Batch size for training: 64.\r\n    - Number of epochs for full model retraining: 1000.\r\n    - Number of patience epochs for early stopping (for restart condition): 10.\r\n    - Learning rate: 0.001.\r\n    - Refine epochs (for retraining after initial phase): 50.\r\n- General Setup:\r\n    - Experiments are run with seeds ranging from 0 to 9."
}{
    "Title": "Conformal Inference is (almost) Free for Neural Networks Trained with Early Stopping",
    "Main Contributions": "The paper introduces Conformalized Early Stopping (CES), a novel method that efficiently combines early stopping with conformal calibration using the same hold-out data. This addresses the limitation of neural networks trained with early stopping, which often lack precise statistical guarantees, and avoids the need for multiple data splits or overly conservative adjustments typically required for rigorous predictive inferences. CES enables accurate models that provide exact predictive inferences without additional data cost, with practical implementations for outlier detection, multi-class classification, and regression (including quantile regression).",
    "Methodology": "CES leverages a single hold-out dataset (Des-cal) for both early stopping and conformal inference. It involves training a neural network for a maximum number of epochs (tmax), storing intermediate models at regular intervals (τ epochs). For a test point, an 'augmented loss' function, L+1 es-cal(Mt, z), is defined, which incorporates the test point into the loss calculation over the Des-cal set. The best intermediate model is then selected by minimizing this augmented loss. Nonconformity scores are computed using this selected model for the Des-cal set and the test point. For outlier detection and classification, this leads to conformal p-values and prediction sets with finite-sample validity (label-conditional or marginal coverage). For regression tasks using squared-error loss or pinball loss (for quantile regression), the selection of the best model for continuous outcomes involves finding the lower envelope of a family of quadratic or piecewise-linear pinball loss functions, respectively, which is done using an efficient O(T log T) divide-and-conquer algorithm. This results in a step function for the best model, allowing for the construction of prediction intervals that guarantee marginal coverage.",
    "Experimental Setup": "The performance of CES was evaluated across different learning tasks using various datasets. For outlier detection and multi-class classification, the CIFAR10 dataset (60,000 32x32 RGB images from 10 classes) was used, training a convolutional neural network with ReLU activations for 50 epochs. For regression with squared-error loss, three UCI datasets were used: 'bio', 'bike', and 'concrete', training neural networks with two hidden layers (width 128, ReLU) for up to 1000 epochs. For quantile regression (CQR), seven UCI datasets were used: 'MEPS 21', 'blog data', 'STAR', 'community', 'bio', 'homes', and 'bike', training neural networks with three hidden layers (ReLU) for up to 2000 epochs. Input features were standardized, and response values rescaled for CQR. Data was split into 75% for training and 25% for Des-cal in CES. Test sets contained 100 to 1000 independent data points. Metrics included False Positive Rate (FPR) and True Positive Rate (TPR) for outlier detection, average cardinality and marginal coverage for classification, and marginal coverage, worst-slab conditional coverage, and average width for regression. CES was compared against a naive early stopping benchmark with theoretical correction, early stopping with a three-way data split, and full training without early stopping. Results were averaged over 25 to 100 independent trials.",
    "Limitations": "The computational cost of CES may become a barrier in some applications if the number of candidate models (T) is very large, although it scales slightly more than linearly, recommending moderate values for T (e.g., 100 or 1000). The storage cost for saving all intermediate models is higher than standard early stopping, but is argued to be feasible for many real-world situations (e.g., 10GB for 100 ResNet50 models). In some rare cases, the regression algorithms (Algorithms 3 and 4) might produce empty prediction sets, which is addressed by a fallback mechanism to a naive conformal prediction interval (Algorithms A11 and A12). While the naive benchmark performs similarly to CES in practice for large datasets, it lacks rigorous finite-sample guarantees, and its theoretical corrections are shown to be overly conservative.",
    "Future Research Directions": "The methodology presented in this paper extends beyond early stopping and conformal calibration. It can be applied to hyper-parameter tuning and selecting the most promising candidate from an arbitrary machine learning toolbox for outlier detection. Furthermore, the techniques can be used to calibrate, without further data splits, models selected in a data-driven way from arbitrary machine learning toolboxes in multi-class classification and regression. The proposed method could also be combined with existing research on the robustness of conformal inferences to distribution shifts and failures of the data exchangeability assumption.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "Scaling Laws for Hyperparameter Optimization",
    "Main Contributions": "The paper introduces Deep Power Laws (DPL), a novel multi-fidelity Hyperparameter Optimization (HPO) method that leverages the power-law nature of learning curves in Deep Learning (DL) for more efficient hyperparameter tuning. DPL utilizes an ensemble of neural network models conditioned to predict performance following a power-law scaling pattern. This approach allows for dynamically pausing and incrementally training hyperparameter configurations based on gray-box evaluations. The method demonstrates state-of-the-art performance in HPO for DL, outperforming 7 strong baselines across 3 diverse benchmarks covering 59 tasks, including tabular, image, and NLP datasets, and large language models, significantly reducing the cost and time of finding optimal configurations.",
    "Methodology": "DPL models learning curves (validation loss as a function of optimization epochs/budget) as power law functions (αλ + βλ b−γλ). Instead of fitting individual power laws, it uses a single shared power law function where the coefficients (α, β, γ) are predicted by a parametric neural network conditioned on the hyperparameter configuration (g: Λ → R3). To make the surrogate probabilistic for Bayesian Optimization (BO) acquisition functions, DPL trains an ensemble of 5 such neural network models. These models are initialized with different weights and trained with different mini-batch sequences. The posterior mean and variance of predictions are derived from this ensemble. DPL employs the Expected Improvement (EI) acquisition function with estimated full budget performance to recommend the next configuration. A key multi-fidelity strategy is to advance the selected configuration by a small budget (e.g., one epoch) rather than training to full convergence, allowing for early discarding of unpromising configurations.",
    "Experimental Setup": "DPL was evaluated against 7 state-of-the-art HPO baselines (Random Search, Hyperband, ASHA, BOHB, DEHB, multi-fidelity SMAC, BOCA from Dragonfly Library). Experiments covered 59 diverse tasks across three benchmarks: 1. LCBench (2,000 configurations, 35 tabular datasets, feedforward NNs; balanced accuracy). 2. PD1 (DL architectures like Transformers, ResNeXt on vision/statistical/protein datasets; accuracy). 3. TaskSet (Adam8p search space, 1000 configurations, 12 RNN text classification tasks; log-likelihood loss). Additional evaluations included nanoGPT-Bench for Large Language Models (tuning GPT-2 hyperparameters on OpenWebText across model sizes) and a continuous search space for EfficientNetV2 on CIFAR10. Performance was measured using regret (difference from oracle) and averaged normalized regret, considering normalized wall-clock time for any-time performance comparison. All experiments started with 1 randomly sampled configuration evaluated for 1 step (multi-fidelity methods) or full budget (Random Search). Experiments ran on a CPU cluster with Intel Xeon E5-2630v4 CPUs.",
    "Limitations": "The uncertainty estimation provided by the Deep Ensemble approach within DPL was found to be suboptimal when compared to standard Bayesian Optimization surrogates, such as Gaussian Processes. Furthermore, training an ensemble of multiple power law models incurs additional computational costs. While not explicitly stated as a limitation of DPL itself, the paper acknowledges that not all learning curves strictly follow a power law pattern, although empirical evidence suggests most do, making the power law surrogate generally efficient.",
    "Future Research Directions": "Future research directions include investigating the combination of power laws with Gaussian Processes to potentially improve uncertainty estimation. Additionally, exploring and incorporating other types of fidelities beyond training epochs, such as dataset size or model size, is suggested for further extensions of the DPL method.",
    "Experiment Code": "import logging\nimport os\nimport time\nfrom typing import List, Tuple\n\nimport numpy as np\n\nfrom scipy.stats import norm\nimport torch\nfrom torch.utils.data import DataLoader\n\nfrom data_loader.tabular_data_loader import WrappedDataLoader\nfrom dataset.tabular_dataset import TabularDataset\n\nfrom models.conditioned_power_law import ConditionedPowerLaw\n\n\nclass PowerLawSurrogate:\n\n    def __init__(\n        self,\n        hp_candidates: np.ndarray,\n        surrogate_configs: dict = None,\n        seed: int = 11,\n        max_benchmark_epochs: int = 52,\n        ensemble_size: int = 5,\n        nr_epochs: int = 250,\n        fantasize_step: int = 1,\n        minimization: bool = True,\n        total_budget: int = 1000,\n        device: str = None,\n        output_path: str = '.',\n        dataset_name: str = 'unknown',\n        pretrain: bool = False,\n        backbone: str = 'power_law',\n        max_value: float = 100,\n        min_value: float = 0,\n        fill_value: str = 'zero',\n    ):\n        \"\"\"\n        Args:\n            hp_candidates: np.ndarray\n                The full list of hyperparameter candidates for a given dataset.\n            surrogate_configs: dict\n                The model configurations for the surrogate.\n            seed: int\n                The seed that will be used for the surrogate.\n            max_benchmark_epochs: int\n                The maximal budget that a hyperparameter configuration\n                has been evaluated in the benchmark for.\n            ensemble_size: int\n                The number of members in the ensemble.\n            nr_epochs: int\n                Number of epochs for which the surrogate should be\n                trained.\n            fantasize_step: int\n                The number of steps for which we are looking ahead to\n                evaluate the performance of a hpc.\n            minimization: bool\n                If for the evaluation metric, the lower the value the better.\n            total_budget: int\n                The total budget given. Used to calculate the initialization\n                percentage.\n            device: str\n                The device where the experiment will be run on.\n            output_path: str\n                The path where all the output will be stored.\n            dataset_name: str\n                The name of the dataset that the experiment will be run on.\n            pretrain: bool\n                If the surrogate will be pretrained before with a synthetic\n                curve.\n            backbone: str\n                The backbone, which can either be 'power_law' or 'nn'.\n            max_value: float\n                The maximal value for the dataset.\n            min_value: float\n                The minimal value for the dataset.\n            fill_value: str = 'zero',\n                The filling strategy for when learning curves are used.\n                Either 'zero' or 'last' where last represents the last value.\n        \"\"\"\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\n        self.total_budget = total_budget\n        self.fill_value = fill_value\n        self.max_value = max_value\n        self.min_value = min_value\n        self.backbone = backbone\n\n        self.pretrained_path = os.path.join(\n            output_path,\n            'power_law',\n            f'checkpoint_{seed}.pth',\n        )\n\n        self.model_instances = [\n            ConditionedPowerLaw,\n            ConditionedPowerLaw,\n            ConditionedPowerLaw,\n            ConditionedPowerLaw,\n            ConditionedPowerLaw,\n        ]\n\n        if device is None:\n            self.dev = torch.device(\n                'cuda') if torch.cuda.is_available() else torch.device('cpu')\n        else:\n            self.dev = torch.device(device)\n\n        self.learning_rate = 0.001\n        self.batch_size = 64\n        self.refine_batch_size = 64\n\n        self.criterion = torch.nn.L1Loss()\n        self.hp_candidates = hp_candidates\n\n        self.minimization = minimization\n        self.seed = seed\n\n        self.logger = logging.getLogger('power_law')\n        logging.basicConfig(\n            filename=f'power_law_surrogate_{dataset_name}_{seed}.log',\n            level=logging.INFO,\n            force=True,\n        )\n\n        # with what percentage configurations will be taken randomly instead of being sampled from the model\n        self.fraction_random_configs = 0.1\n        self.iteration_probabilities = np.random.rand(self.total_budget)\n\n        # the keys will be hyperparameter indices while the value\n        # will be a list with all the budgets evaluated for examples\n        # and with all performances for the performances\n        self.examples = dict()\n        self.performances = dict()\n\n        # set a seed already, so that it is deterministic when\n        # generating the seeds of the ensemble\n        torch.manual_seed(seed)\n        np.random.seed(seed)\n\n        self.seeds = np.random.choice(100, ensemble_size, replace=False)\n        self.max_benchmark_epochs = max_benchmark_epochs\n        self.ensemble_size = ensemble_size\n        self.nr_epochs = nr_epochs\n        self.refine_nr_epochs = 20\n        self.fantasize_step = fantasize_step\n\n        self.pretrain = pretrain\n\n        initial_configurations_nr = 1\n        conf_individual_budget = 1\n        init_conf_indices = np.random.choice(self.hp_candidates.shape[0], initial_configurations_nr, replace=False)\n        init_budgets = [i for i in range(1, conf_individual_budget + 1)]\n\n        self.rand_init_conf_indices = []\n        self.rand_init_budgets = []\n\n        # basically add every config index up to a certain budget threshold for the initialization\n        # we will go through both lists during the initialization\n        for config_index in init_conf_indices:\n            for config_budget in init_budgets:\n                self.rand_init_conf_indices.append(config_index)\n                self.rand_init_budgets.append(config_budget)\n\n        self.initial_random_index = 0\n\n        if surrogate_configs is None:\n\n            self.surrogate_configs = []\n            for i in range(0, self.ensemble_size):\n                self.surrogate_configs.append(\n                    {\n                        'nr_units': 128,\n                        'nr_layers': 2,\n                        'kernel_size': 3,\n                        'nr_filters': 4,\n                        'nr_cnn_layers': 2,\n                        'use_learning_curve': False,\n                    }\n                )\n        else:\n            self.surrogate_configs = surrogate_configs\n\n        self.nr_features = self.hp_candidates.shape[1]\n        self.best_value_observed = np.inf\n\n        self.diverged_configs = set()\n\n        # Where the models of the ensemble will be stored\n        self.models = []\n        # A tuple which will have the last evaluated point\n        # It will be used in the refining process\n        # Tuple(config_index, budget, performance, curve)\n        self.last_point = None\n\n        self.initial_full_training_trials = 10\n\n        # a flag if the surrogate should be trained\n        self.train = True\n\n        # the times it was refined\n        self.refine_counter = 0\n        # the surrogate iteration counter\n        self.iterations_counter = 0\n        # info dict to drop every surrogate iteration\n        self.info_dict = dict()\n\n        # the start time for the overhead of every surrogate iteration\n        # will be recorded here\n        self.suggest_time_duration = 0\n\n        self.output_path = output_path\n        self.dataset_name = dataset_name\n\n        self.no_improvement_threshold = int(self.max_benchmark_epochs + 0.2 * self.max_benchmark_epochs)\n        self.no_improvement_patience = 0\n\n    def _prepare_dataset(self) -> TabularDataset:\n        \"\"\"This method is called to prepare the necessary training dataset\n        for training a model.\n\n        Returns:\n            train_dataset: A dataset consisting of examples, labels, budgets\n                and learning curves.\n        \"\"\"\n        train_examples, train_labels, train_budgets, train_curves = self.history_configurations()\n\n        train_curves = self.prepare_training_curves(train_budgets, train_curves)\n        train_examples = np.array(train_examples, dtype=np.single)\n        train_labels = np.array(train_labels, dtype=np.single)\n        train_budgets = np.array(train_budgets, dtype=np.single)\n\n        # scale budgets to [0, 1]\n        train_budgets = train_budgets / self.max_benchmark_epochs\n\n        train_dataset = TabularDataset(\n            train_examples,\n            train_labels,\n            train_budgets,\n            train_curves,\n        )\n\n        return train_dataset\n\n    def _refine_surrogate():\n        \"\"\"Refine the surrogate model.\n        \"\"\"\n        for model_index, model_seed in enumerate(self.seeds):\n\n            train_dataset = self._prepare_dataset()\n            self.logger.info(f'Started refining model with index: {model_index}')\n            refined_model = self.train_pipeline(\n                model_index,\n                train_dataset,\n                nr_epochs=self.refine_nr_epochs,\n                refine=True,\n                weight_new_example=True,\n                batch_size=self.refine_batch_size,\n            )\n\n            self.models[model_index] = refined_model\n\n    def _train_surrogate(self, pretrain: bool = False):\n        \"\"\"Train the surrogate model.\n\n        Trains all the models of the ensemble\n        with different initializations and different\n        data orders.\n\n        Args:\n            pretrain: bool\n                If we have pretrained weights and we will just\n                refine the models.\n        \"\"\"\n        for model_index, model_seed in enumerate(self.seeds):\n            train_dataset = self._prepare_dataset()\n            self.logger.info(f'Started training model with index: {model_index}')\n\n            if pretrain:\n                # refine the models that were already pretrained\n                trained_model = self.train_pipeline(\n                    model_index,\n                    train_dataset,\n                    nr_epochs=self.refine_nr_epochs,\n                    refine=True,\n                    weight_new_example=False,\n                    batch_size=self.batch_size,\n                    early_stopping_it=self.refine_nr_epochs,  # basically no early stopping\n                )\n                self.models[model_index] = trained_model\n            else:\n                # train the models for the first time\n                trained_model = self.train_pipeline(\n                    model_index,\n                    train_dataset,\n                    nr_epochs=self.nr_epochs,\n                    refine=False,\n                    weight_new_example=False,\n                    batch_size=self.batch_size,\n                    early_stopping_it=self.nr_epochs,  # basically no early stopping\n                )\n                self.models.append(trained_model)\n\n    def train_pipeline(\n        self,\n        model_index: int,\n        train_dataset: TabularDataset,\n        nr_epochs: int,\n        refine: bool = False,\n        weight_new_example: bool = True,\n        batch_size: int = 64,\n        early_stopping_it: int = 10,\n        activate_early_stopping: bool = False,\n    ) -> torch.nn.Module:\n        \"\"\"Train an algorithm to predict the performance\n        of the hyperparameter configuration based on the budget.\n\n        Args:\n            model_index: int\n                The index of the model.\n            train_dataset: TabularDataset\n                The tabular dataset featuring the examples, labels,\n                budgets and learning curves.\n            nr_epochs: int\n                The number of epochs to train the model for.\n            refine: bool\n                If an existing model will be refined or if the training\n                will start from scratch.\n            weight_new_example: bool\n                If the last example that was added should be weighted more\n                by being included in every batch. This is only applicable\n                when refine is True.\n            batch_size: int\n                The batch size to be used for training.\n            early_stopping_it: int\n                The early stopping iteration patience.\n            activate_early_stopping: bool\n                Flag controlling the activation.\n\n        Returns:\n            model: torch.nn.Module\n                A trained model.\n        \"\"\"\n        if model_index == 0:\n            self.iterations_counter += 1\n            self.logger.info(f'Iteration number: {self.iterations_counter}')\n\n        surrogate_config = self.surrogate_configs[model_index]\n        seed = self.seeds[model_index]\n        torch.manual_seed(seed)\n        np.random.seed(seed)\n\n        if refine:\n            model = self.models[model_index]\n        else:\n            model = self.model_instances[model_index](\n                nr_initial_features=self.nr_features + 1 if self.backbone == 'nn' else self.nr_features,\n                nr_units=surrogate_config['nr_units'],\n                nr_layers=surrogate_config['nr_layers'],\n                use_learning_curve=surrogate_config['use_learning_curve'],\n                kernel_size=surrogate_config['kernel_size'],\n                nr_filters=surrogate_config['nr_filters'],\n                nr_cnn_layers=surrogate_config['nr_cnn_layers'],\n            )\n            model.to(self.dev)\n\n        # make the training dataset here\n        train_dataloader = DataLoader(\n            train_dataset,\n            batch_size=batch_size,\n            shuffle=True,\n        )\n        train_dataloader = WrappedDataLoader(train_dataloader, self.dev)\n        optimizer = torch.optim.Adam(model.parameters(), lr=self.learning_rate)\n\n        patience_rounds = 0\n        best_loss = np.inf\n        best_state = deepcopy(model.state_dict())\n\n        for epoch in range(0, nr_epochs):\n            running_loss = 0\n            model.train()\n\n            for batch_examples, batch_labels, batch_budgets, batch_curves in train_dataloader:\n\n                nr_examples_batch = batch_examples.shape[0]\n                # if only one example in the batch, skip the batch.\n                # Otherwise, the code will fail because of batchnormalization.\n                if nr_examples_batch == 1:\n                    continue\n\n                # zero the parameter gradients\n                optimizer.zero_grad(set_to_none=True)\n\n                # in case we are refining, we add the new example to every\n                # batch to give it more importance.\n                if refine and weight_new_example:\n                    newp_index, newp_budget, newp_performance, newp_curve = self.last_point\n                    new_example = np.array([self.hp_candidates[newp_index]], dtype=np.single)\n                    newp_missing_values = self.prepare_missing_values_channel([newp_budget])\n                    newp_budget = np.array([newp_budget], dtype=np.single) / self.max_benchmark_epochs\n                    newp_performance = np.array([newp_performance], dtype=np.single)\n                    modified_curve = deepcopy(newp_curve)\n\n                    difference = self.max_benchmark_epochs - len(modified_curve) - 1\n                    if difference > 0:\n                        modified_curve.extend([modified_curve[-1] if self.fill_value == 'last' else 0] * difference)\n\n                    modified_curve = np.array([modified_curve], dtype=np.single)\n                    newp_missing_values = np.array(newp_missing_values, dtype=np.single)\n\n                    # add depth dimension to the train_curves array and missing_value_matrix\n                    modified_curve = np.expand_dims(modified_curve, 1)\n                    newp_missing_values = np.expand_dims(newp_missing_values, 1)\n                    modified_curve = np.concatenate((modified_curve, newp_missing_values), axis=1)\n\n                    new_example = torch.tensor(new_example, device=self.dev)\n                    newp_budget = torch.tensor(newp_budget, device=self.dev)\n                    newp_performance = torch.tensor(newp_performance, device=self.dev)\n                    modified_curve = torch.tensor(modified_curve, device=self.dev)\n\n                    batch_examples = torch.cat((batch_examples, new_example))\n                    batch_budgets = torch.cat((batch_budgets, newp_budget))\n                    batch_labels = torch.cat((batch_labels, newp_performance))\n                    batch_curves = torch.cat((batch_curves, modified_curve))\n\n                outputs = model(batch_examples, batch_budgets, batch_budgets, batch_curves)\n                loss = self.criterion(outputs, batch_labels)\n                loss.backward()\n                optimizer.step()\n                # print statistics\n                running_loss += loss.item()\n\n            running_loss = running_loss / len(train_dataloader)\n            self.logger.info(f'Epoch {epoch +1}, Loss:{running_loss}')\n\n            if activate_early_stopping:\n                if running_loss < best_loss:\n                    best_state = deepcopy(model.state_dict())\n                    best_loss = running_loss\n                    patience_rounds = 0\n                elif running_loss > best_loss:\n                    patience_rounds += 1\n                    if patience_rounds == early_stopping_it:\n                        model.load_state_dict(best_state)\n                        self.logger.info(f'Stopping training since validation loss is not improving')\n                        break\n\n        if activate_early_stopping:\n            model.load_state_dict(best_state)\n\n        return model\n\n    def _predict(self) -> Tuple[np.ndarray, np.ndarray, List, np.ndarray]:\n        \"\"\"\n        Predict the performances of the hyperparameter configurations\n        as well as the standard deviations based on the ensemble.\n\n        Returns:\n            mean_predictions, std_predictions, hp_indices, real_budgets:\n            Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]\n                The mean predictions and the standard deviations over\n                all model predictions for the given hyperparameter\n                configurations with their associated indices and budgets.\n\n        \"\"\"\n        configurations, hp_indices, budgets, real_budgets, hp_curves = self.generate_candidate_configurations()\n        # scale budgets to [0, 1]\n        budgets = np.array(budgets, dtype=np.single)\n        hp_curves = self.prepare_training_curves(real_budgets, hp_curves)\n        budgets = budgets / self.max_benchmark_epochs\n        real_budgets = np.array(real_budgets, dtype=np.single)\n        configurations = np.array(configurations, dtype=np.single)\n\n        configurations = torch.tensor(configurations)\n        configurations = configurations.to(device=self.dev)\n        budgets = torch.tensor(budgets)\n        budgets = budgets.to(device=self.dev)\n        hp_curves = torch.tensor(hp_curves)\n        hp_curves = hp_curves.to(device=self.dev)\n        network_real_budgets = torch.tensor(real_budgets / self.max_benchmark_epochs)\n        network_real_budgets.to(device=self.dev)\n        all_predictions = []\n\n        for model in self.models:\n            model = model.eval()\n            predictions = model(configurations, budgets, network_real_budgets, hp_curves)\n            all_predictions.append(predictions.detach().cpu().numpy())\n\n        mean_predictions = np.mean(all_predictions, axis=0)\n        std_predictions = np.std(all_predictions, axis=0)\n\n        return mean_predictions, std_predictions, hp_indices, real_budgets\n\n    def suggest(self) -> Tuple[int, int]:\n        \"\"\"Suggest a hyperparameter configuration and a budget\n        to evaluate.\n\n        Returns:\n            suggested_hp_index, budget: Tuple[int, int]\n                The index of the hyperparamter configuration to be evaluated\n                and the budget for what it is going to be evaluated for.\n        \"\"\"\n        suggest_time_start = time.time()\n\n        if self.initial_random_index < len(self.rand_init_conf_indices):\n            self.logger.info(\n                'Not enough configurations to build a model. \\n'\n                'Returning randomly sampled configuration'\n            )\n            suggested_hp_index = self.rand_init_conf_indices[self.initial_random_index]\n            budget = self.rand_init_budgets[self.initial_random_index]\n            self.initial_random_index += 1\n        else:\n            mean_predictions, std_predictions, hp_indices, real_budgets = self._predict()\n            best_prediction_index = self.find_suggested_config(\n                mean_predictions,\n                std_predictions,\n            )\n            # actually do the mapping between the configuration indices and the best prediction\n            # index\n            suggested_hp_index = hp_indices[best_prediction_index]\n\n            if suggested_hp_index in self.examples:\n                evaluated_budgets = self.examples[suggested_hp_index]\n                max_budget = max(evaluated_budgets)\n                budget = max_budget + self.fantasize_step\n                if budget > self.max_benchmark_epochs:\n                    budget = self.max_benchmark_epochs\n            else:\n                budget = self.fantasize_step\n\n        suggest_time_end = time.time()\n        self.suggest_time_duration = suggest_time_end - suggest_time_start\n\n        return suggested_hp_index, budget\n\n    def observe(\n        self,\n        hp_index: int,\n        b: int,\n        hp_curve: List[float],\n    ):\n        \"\"\"Receive information regarding the performance of a hyperparameter\n        configuration that was suggested.\n\n        Args:\n            hp_index: int\n                The index of the evaluated hyperparameter configuration.\n            b: int\n                The budget for which the hyperparameter configuration was evaluated.\n            hp_curve: List\n                The performance of the hyperparameter configuration.\n        \"\"\"\n        for index, curve_element in enumerate(hp_curve):\n            if np.isnan(curve_element):\n                self.diverged_configs.add(hp_index)\n                # only use the non-nan part of the curve and the corresponding\n                # budget to still have the information in the network\n                hp_curve = hp_curve[0:index + 1]\n                b = index\n                break\n\n        if not self.minimization:\n            hp_curve = np.subtract([self.max_value] * len(hp_curve), hp_curve)\n            hp_curve = hp_curve.tolist()\n\n        best_curve_value = min(hp_curve)\n\n        self.examples[hp_index] = np.arange(1, b + 1)\n        self.performances[hp_index] = hp_curve\n\n        if self.best_value_observed > best_curve_value:\n            self.best_value_observed = best_curve_value\n            self.no_improvement_patience = 0\n            self.logger.info(f'New Incumbent value found '\n                             f'{1 - best_curve_value if not self.minimization else best_curve_value}')\n        else:\n            self.no_improvement_patience += 1\n            if self.no_improvement_patience == self.no_improvement_threshold:\n                self.train = True\n                self.no_improvement_patience = 0\n                self.logger.info(\n                    'No improvement in the incumbent value threshold reached, '\n                    'restarting training from scratch'\n                )\n\n        initial_empty_value = self.get_mean_initial_value() if self.fill_value == 'last' else 0\n        if self.initial_random_index >= len(self.rand_init_conf_indices):\n            performance = self.performances[hp_index]\n            self.last_point = (hp_index, b, performance[b-1], performance[0:b-1] if b > 1 else [initial_empty_value])\n\n            if self.train:\n                # delete the previously stored models\n                self.models = []\n                if self.pretrain:\n                    # TODO Load the pregiven weights.\n                    pass\n\n                self._train_surrogate(pretrain=self.pretrain)\n\n                if self.iterations_counter <= self.initial_full_training_trials:\n                    self.train = True\n                else:\n                    self.train = False\n            else:\n                self.refine_counter += 1\n                self._refine_surrogate()\n\n    def prepare_examples(self, hp_indices: List) -> List:\n        \"\"\"\n        Prepare the examples to be given to the surrogate model.\n\n        Args:\n            hp_indices: List\n                The list of hp indices that are already evaluated.\n\n        Returns:\n            examples: List\n                A list of the hyperparameter configurations.\n        \"\"\"\n        examples = []\n        for hp_index in hp_indices:\n            examples.append(self.hp_candidates[hp_index])\n\n        return examples\n\n    def generate_candidate_configurations(self) -> Tuple[List, List, List, List, List]:\n        \"\"\"Generate candidate configurations that will be\n        fantasized upon.\n\n        Returns:\n            (configurations, hp_indices, hp_budgets, real_budgets, hp_curves): Tuple\n                A tuple of configurations, their indices in the hp list,\n                the budgets that they should be fantasized upon, the maximal\n                budgets they have been evaluated and their corresponding performance\n                curves.\n        \"\"\"\n        hp_indices = []\n        hp_budgets = []\n        hp_curves = []\n        real_budgets = []\n        initial_empty_value = self.get_mean_initial_value() if self.fill_value == 'last' else 0\n\n        for hp_index in range(0, self.hp_candidates.shape[0]):\n\n            if hp_index in self.examples:\n                budgets = self.examples[hp_index]\n                # Take the max budget evaluated for a certain hpc\n                max_budget = budgets[-1]\n                if max_budget == self.max_benchmark_epochs:\n                    continue\n                real_budgets.append(max_budget)\n                learning_curve = self.performances[hp_index]\n\n                hp_curve = learning_curve[0:max_budget-1] if max_budget > 1 else [initial_empty_value]\n            else:\n                real_budgets.append(1)\n                hp_curve = [initial_empty_value]\n\n            hp_indices.append(hp_index)\n            hp_budgets.append(self.max_benchmark_epochs)\n            hp_curves.append(hp_curve)\n\n        configurations = self.prepare_examples(hp_indices)\n\n        return configurations, hp_indices, hp_budgets, real_budgets, hp_curves\n\n    def history_configurations(self) -> Tuple[List, List, List, List]:\n        \"\"\"\n        Generate the configurations, labels, budgets and curves\n        based on the history of evaluated configurations.\n\n        Returns:\n            (train_examples, train_labels, train_budgets, train_curves): Tuple\n                A tuple of examples, labels and budgets for the\n                configurations evaluated so far.\n        \"\"\"\n        train_examples = []\n        train_labels = []\n        train_budgets = []\n        train_curves = []\n        initial_empty_value = self.get_mean_initial_value() if self.fill_value == 'last' else 0\n\n        for hp_index in self.examples:\n            budgets = self.examples[hp_index]\n            performances = self.performances[hp_index]\n            example = self.hp_candidates[hp_index]\n\n            for budget in budgets:\n                example_curve = performances[0:budget-1]\n                train_examples.append(example)\n                train_budgets.append(budget)\n                train_labels.append(performances[budget - 1])\n                train_curves.append(example_curve if len(example_curve) > 0 else [initial_empty_value])\n\n        return train_examples, train_labels, train_budgets, train_curves\n\n    @staticmethod\n    def acq(\n        best_values: np.ndarray,\n        mean_predictions: np.ndarray,\n        std_predictions: np.ndarray,\n        explore_factor: float = 0.25,\n        acq_choice: str = 'ei',\n    ) -> np.ndarray:\n        \"\"\"\n        Calculate the acquisition function based on the network predictions.\n\n        Args:\n        -----\n        best_values: np.ndarray\n            An array with the best value for every configuration.\n            Depending on the implementation it can be different for every\n            configuration.\n        mean_predictions: np.ndarray\n            The mean values of the model predictions.\n        std_predictions: np.ndarray\n            The standard deviation values of the model predictions.\n        explore_factor: float\n            The explore factor, when ucb is used as an acquisition\n            function.\n        acq_choice: str\n            The choice for the acquisition function to use.\n\n        Returns\n        -------\n        acq_values: np.ndarray\n            The values of the acquisition function for every configuration.\n        \"\"\"\n        if acq_choice == 'ei':\n            z = (np.subtract(best_values, mean_predictions))\n            difference = deepcopy(z)\n            not_zero_std_indicator = [False if example_std == 0.0 else True for example_std in std_predictions]\n            zero_std_indicator = np.invert(not_zero_std_indicator)\n            z = np.divide(z, std_predictions, where=not_zero_std_indicator)\n            np.place(z, zero_std_indicator, 0)\n            acq_values = np.add(np.multiply(difference, norm.cdf(z)), np.multiply(std_predictions, norm.pdf(z)))\n        elif acq_choice == 'ucb':\n            # we are working with error rates so we multiply the mean with -1\n            acq_values = np.add(-1 * mean_predictions, explore_factor * std_predictions)\n        elif acq_choice == 'thompson':\n            acq_values = np.random.normal(mean_predictions, std_predictions)\n        else:\n            acq_values = mean_predictions\n\n        return acq_values\n\n    def find_suggested_config(\n            self,\n            mean_predictions: np.ndarray,\n            mean_stds: np.ndarray,\n    ) -> int:\n        \"\"\"Return the hyperparameter with the highest acq function value.\n\n        Given the mean predictions and mean standard deviations from the DPL\n        ensemble for every hyperparameter configuraiton, return the hyperparameter\n        configuration that has the highest acquisition function value.\n\n        Args:\n            mean_predictions: np.ndarray\n                The mean predictions of the ensemble for every hyperparameter\n                configuration.\n            mean_stds: np.ndarray\n                The standard deviation predictions of the ensemble for every\n                hyperparameter configuration.\n\n        Returns:\n            max_value_index: int\n                the index of the maximal value.\n\n        \"\"\"\n        best_values = np.array([self.best_value_observed] * mean_predictions.shape[0])\n        acq_func_values = self.acq(\n            best_values,\n            mean_predictions,\n            mean_stds,\n            acq_choice='ei',\n        )\n\n        max_value_index = np.argmax(acq_func_values)\n\n        return max_value_index\n\n    def calculate_fidelity_ymax(self, fidelity: int) -> float:\n        \"\"\"Calculate the incumbent for a certain fidelity level.\n\n        Args:\n            fidelity: int\n                The given budget fidelity.\n\n        Returns:\n            best_value: float\n                The incumbent value for a certain fidelity level.\n        \"\"\"\n        config_values = []\n        for example_index in self.examples.keys():\n            try:\n                performance = self.performances[example_index][fidelity - 1]\n            except IndexError:\n                performance = self.performances[example_index][-1]\n            config_values.append(performance)\n\n        # lowest error corresponds to best value\n        best_value = min(config_values)\n\n        return best_value\n\n    def patch_curves_to_same_length(self, curves: List):\n        \"\"\"\n        Patch the given curves to the same length.\n\n        Finds the maximum curve length and patches all\n        other curves that are shorter with zeroes.\n\n        Args:\n            curves: List\n                The hyperparameter curves.\n        \"\"\"\n        for curve in curves:\n            difference = self.max_benchmark_epochs - len(curve) - 1\n            if difference > 0:\n                fill_value = [curve[-1]] if self.fill_value == 'last' else [0]\n                curve.extend(fill_value * difference)\n\n    def prepare_missing_values_channel(self, budgets: List) -> List:\n        \"\"\"Prepare an additional channel for learning curves.\n\n        The additional channel will represent an existing learning\n        curve value with a 1 and a missing learning curve value with\n        a 0.\n\n        Args:\n            budgets: List\n                A list of budgets for every training point.\n\n        Returns:\n            missing_value_curves: List\n                A list of curves representing existing or missing\n                values for the training curves of the training points.\n        \"\"\"\n        missing_value_curves = []\n\n        for i in range(len(budgets)):\n            budget = budgets[i]\n            budget = budget - 1\n            budget = int(budget)\n\n            if budget > 0:\n                example_curve = [1] * budget\n            else:\n                example_curve = []\n\n            difference_in_curve = self.max_benchmark_epochs - len(example_curve) - 1\n            if difference_in_curve > 0:\n                example_curve.extend([0] * difference_in_curve)\n            missing_value_curves.append(example_curve)\n\n        return missing_value_curves\n\n    def get_mean_initial_value(self):\n        \"\"\"Returns the mean initial value\n        for all hyperparameter configurations in the history so far.\n\n        Returns:\n            mean_initial_value: float\n                Mean initial value for all hyperparameter configurations\n                observed.\n        \"\"\"\n        first_values = []\n        for performance_curve in self.performances.values():\n            first_values.append(performance_curve[0])\n\n        mean_initial_value = np.mean(first_values)\n\n        return mean_initial_value\n\n    def prepare_training_curves(\n            self,\n            train_budgets: List[int],\n            train_curves: List[float]\n    ) -> np.ndarray:\n        \"\"\"Prepare the configuration performance curves for training.\n\n        For every configuration training curve, add an extra dimension\n        regarding the missing values, as well as extend the curve to have\n        a fixed uniform length for all.\n\n        Args:\n            train_budgets: List\n                A list of the budgets for all training points.\n            train_curves: List\n                A list of curves that pertain to every training point.\n\n        Returns:\n            train_curves: np.ndarray\n                The transformed training curves.\n        \"\"\"\n        missing_value_matrix = self.prepare_missing_values_channel(train_budgets)\n        self.patch_curves_to_same_length(train_curves)\n        train_curves = np.array(train_curves, dtype=np.single)\n        missing_value_matrix = np.array(missing_value_matrix, dtype=np.single)\n\n        # add depth dimension to the train_curves array and missing_value_matrix\n        train_curves = np.expand_dims(train_curves, 1)\n        missing_value_matrix = np.expand_dims(missing_value_matrix, 1)\n        train_curves = np.concatenate((train_curves, missing_value_matrix), axis=1)\n\n        return train_curves\n",
    "Experiment Result": "DPL employs a parametric neural network to predict coefficients (alpha, beta, gamma) for power law functions (alpha + beta * budget^(-gamma)) that model learning curves. It operates as a probabilistic surrogate for Bayesian Optimization by training an ensemble of 5 such neural networks, each initialized with different weights and trained with different mini-batch sequences. The posterior mean and variance of predictions are derived from this ensemble. DPL utilizes the Expected Improvement (EI) acquisition function to recommend the next configuration.\n\n**Key Experimental Settings:**\n*   **Power Law Model**: The `ConditionedPowerLaw` model is used, which predicts 3 parameters (alpha, beta, gamma).\n    *   The model has an option to use learning curve features via a Convolutional Neural Network (CNN) (`use_learning_curve`). By default, in the `PowerLawSurrogate` instantiation, this is set to `False` in the `surrogate_configs` list.\n    *   Default network architecture for each ensemble member (when `surrogate_configs` is `None`):\n        *   `nr_units`: 128\n        *   `nr_layers`: 2 (for the MLP that predicts power-law coefficients)\n        *   `kernel_size`: 3 (for CNN if `use_learning_curve` is True)\n        *   `nr_filters`: 4 (for CNN if `use_learning_curve` is True)\n        *   `nr_cnn_layers`: 2 (for CNN if `use_learning_curve` is True)\n*   **Ensemble**: 5 neural network models (`ensemble_size=5`). Each model is trained with a different random seed derived from `np.random.choice(100, ensemble_size, replace=False)`.\n*   **Training**: \n    *   `nr_epochs`: 250 (for initial training).\n    *   `refine_nr_epochs`: 20 (for refining the models).\n    *   `learning_rate`: 0.001.\n    *   `batch_size`: 64 (for initial training).\n    *   `refine_batch_size`: 64 (for refining).\n    *   `criterion`: `torch.nn.L1Loss()`.\n    *   `fill_value`: 'zero' (for patching shorter learning curves).\n    *   `pretrain`: `False`.\n    *   `initial_full_training_trials`: 10 (number of iterations for initial full training before switching to refinement).\n    *   `fraction_random_configs`: 0.1 (percentage of configurations taken randomly instead of sampled from the model for exploration).\n    *   `no_improvement_threshold`: `max_benchmark_epochs + 0.2 * max_benchmark_epochs` (if no incumbent improvement after this many iterations, retraining from scratch is triggered).\n*   **Acquisition Function**: Expected Improvement (`acq_choice='ei'`).\n*   **Multi-fidelity Strategy**: `fantasize_step=1`. The selected configuration is advanced by one budget unit (epoch).\n*   **Overall Budget**: `total_budget=1000` (maximal number of HPO iterations).\n*   **Hyperparameter Preprocessing**: Min-Max scaling for numerical hyperparameters, one-hot encoding for categorical hyperparameters, and logarithmic transformation for specified log-space numerical hyperparameters are applied."
}{
    "Title": "Bayesian Optimization for Iterative Learning",
    "Main Contributions": "The paper presents Bayesian Optimization for Iterative Learning (BOIL) to efficiently tune hyperparameters for deep learning (DL) and deep reinforcement learning (DRL) systems. Traditional tuning methods ignore intermediate information from iterative training, which BOIL exploits. The main contributions include an algorithm that optimizes the learning curve using training curve compression instead of just final performance, an approach to learn this compression curve from data, a data augmentation technique for increased sample-efficiency, and a demonstration of its effectiveness on DRL agents and convolutional neural networks. BOIL is shown to outperform existing baselines in identifying optimal hyperparameters in minimal wall-clock time.",
    "Methodology": "The BOIL approach casts hyperparameter tuning as a cost-sensitive global optimization problem, modeling the objective function `f(x,t)` with a Gaussian Process (GP). It uses a product kernel `k([x,t],[x',t']) = k(x,x') × k(t,t')` with square-exponential kernels for both hyperparameter `x` and iteration `t` spaces. GP hyperparameters are optimized by maximizing log marginal likelihood. An acquisition function `α(x,t)/µc(x,t)` (a modified Expected Improvement criterion divided by a linear cost approximation) is used to select the next evaluation point. A key innovation is 'training curve compression' where the entire learning curve `r(·|x,t)` is transformed into a single numeric score `y` using a Sigmoid (Logistic) preference function `l(u|m0,g0)`. The parameters `m0` and `g0` of this Sigmoid are learned directly from the data by maximizing the GP's marginal likelihood. To improve sample-efficiency and prevent GP covariance matrix ill-conditioning, a selective data augmentation technique is employed. Instead of adding a full curve of points, a subset of points (up to `M=15`) is actively sampled from the observed curve at locations of maximum GP predictive uncertainty, ensuring the natural log of the covariance matrix condition number stays below a threshold (`δ=20`).",
    "Experimental Setup": "The algorithm's efficiency was demonstrated by tuning hyperparameters for two DRL agents on three environments and a CNN on two datasets. Specific DRL settings included a Dueling DQN (DDQN) agent in the CartPole-v0 environment and Advantage Actor Critic (A2C) agents in the InvertedPendulum-v2 and Reacher-v2 environments. A convolutional neural network was tuned on the SVHN and CIFAR10 datasets. All experimental results were averaged over 20 independent runs with different random seeds. Experiments were executed on an NVIDIA 1080 GTX GPU using the TensorFlow-GPU Python package. DRL environments were sourced from OpenAI Gym and Mujoco, with DRL implementations based on OpenAI Baselines. Baselines for comparison included Hyperband and Continuous Multi-task/Multi-fidelity BO (CM-T/F-BO). Final performance was estimated by evaluating the chosen hyperparameter over the maximum number of iterations.",
    "Limitations": "The paper highlights that traditional stopping criteria, such as the exponential decay assumed in Freeze-thaw BO, may not be applicable to DRL settings due to the unpredictable fluctuations and noisiness of DRL reward curves. A naive approach to data augmentation by adding a full curve of points can lead to redundant information and serious ill-conditioning issues for the Gaussian Process covariance matrix, particularly in noisy DRL environments. From a broader impact perspective, the construction of automated pipelines for ML model training, while efficient, could potentially remove humans further from the modeling process, making it harder to identify critical failures and potentially exacerbating the growing opacity of machine learning models.",
    "Future Research Directions": "The authors suggest that the proposed framework is not exclusive to machine learning algorithms and can be applied more generally to any iterative process where its structure can be exploited. One specific example given is the optimization of manufacturing pipelines, where factory settings could be iteratively adjusted to enhance productivity. Implicitly, further research could focus on integrating interpretability methods to counteract the growing opacity of machine learning models, especially as automated training procedures become more prevalent, to ensure robust and transparent real-world policy making.",
    "Experiment Code": "import numpy as np;import itertools;def apply_one_transform_average(curve, midpoint=3, growth=1,MaxEpisode=1000):    if isinstance(curve, (list,)):        curve=curve[0]    def linear_func(x):        if len(x)==1:            return 1        else:            return [1 for u in x]    my_xrange_scaled=np.linspace(0.01,5, MaxEpisode)    my_logistic_value_scaled=linear_func(my_xrange_scaled)    my_logistic_value_scaled=my_logistic_value_scaled[:len(curve)]    average=np.mean(curve)    return average,my_logistic_value_scaled;def return_logistic_curve(midpoint, growth, MaxEpoch=1000):    def logistic_func(x):        if len(x)==1:            return 1.0/(1+np.exp(-growth*(x-midpoint)))        else:            return [1.0/(1+np.exp(-growth*(u-midpoint))) for u in x]    my_xrange_scaled=np.linspace(-6,6, MaxEpoch)    my_logistic_value_scaled=logistic_func(my_xrange_scaled)    return my_logistic_value_scaled;def apply_one_transform_ln(curve, midpoint=3, growth=1,MaxEpisode=1000):    if isinstance(curve, (list,)):        curve=curve[0]    def ln_func(x):        if len(x)==1:            return 20+np.log(x)        else:            return [np.log(u) for u in x]    my_xrange_scaled=np.linspace(0.01,5, MaxEpisode)    my_logistic_value_scaled=ln_func(my_xrange_scaled)    my_logistic_value_scaled=my_logistic_value_scaled[:len(curve)]    if np.max(curve)<=0 and np.min(curve)<=0:        curve=curve+500    threshold=(midpoint+6-2)*len(curve)/(12)    threshold=np.int(threshold)    prod_func=curve*my_logistic_value_scaled    average=[np.mean(prod_func[threshold:pos]) for pos in range(threshold,len(prod_func))]    if np.isnan(average[-1]):        print('bug [curve]')    return average[-1],my_logistic_value_scaled;def apply_one_transform_logistic(curve, midpoint=-2, growth=1,MaxEpisode=1000,IsReturnCurve=False):    if isinstance(curve, (list,)):        curve=curve[0]    def logistic_func(x):        return 1.0/(1+np.exp(-growth*(x-midpoint)))    my_xrange_scaled=np.linspace(-6,6, int(MaxEpisode))    my_logistic_value_scaled=logistic_func(my_xrange_scaled)    my_logistic_value_scaled=my_logistic_value_scaled[:len(curve)]    if np.max(curve)<=0 and np.min(curve)<=0:        curve=curve+500    threshold=(midpoint+6-2)*len(curve)/(12)    threshold=np.int(threshold)    prod_func=curve*my_logistic_value_scaled    average=[np.mean(prod_func[threshold:pos+1]) for pos in range(threshold,len(prod_func))]    if IsReturnCurve==True:        return average[-1],my_logistic_value_scaled    else:        return average[-1];def transform_logistic_marginal(curves,MaxEpisode=1000):    def transform_one_logistic_marginal(curves,MaxEpisode):        midpoint_list=[-3,-2,-1,0,1]        growth_list=[0.1,1,2,3]        temp_Y_value=[0]*(len(midpoint_list)*len(growth_list))        for idx, (val1, val2) in enumerate(itertools.product(midpoint_list,growth_list)):            temp_Y_value[idx]=apply_one_transform_logistic(curves,val1, val2,MaxEpisode)        temp_Y_value=np.asarray(temp_Y_value)        Y=np.mean(temp_Y_value,axis=0)        return Y    if len(curves)==1:        output=transform_one_logistic_marginal(curves[0],MaxEpisode)    else:        output=[0]*len(curves)        for idx, curve in enumerate(curves):            output[idx]=transform_one_logistic_marginal(curve,MaxEpisode)    return output;def transform_logistic(curves, midpoint=0, growth=1,MaxEpisode=1000):    if len(curves)==1:        output=apply_one_transform_logistic(curves[0], midpoint, growth,MaxEpisode)    else:        output=[0]*len(curves)        for idx, curve in enumerate(curves):            output[idx]=apply_one_transform_logistic(curve, midpoint, growth,MaxEpisode)    return output;import numpy as np;from scipy.stats import norm;counter = 0;class AcquisitionFunction(object):    def __init__(self, acq):        self.acq=acq;acq_name=acq['name'];if 'mu_max' in acq:self.mu_max=acq['mu_max'];ListAcq=['bucb','ucb', 'ei','poi','random','ucb_pe','pure_exploration','mu','lcb','ei_mu_max'];IsTrue=[val for idx,val in enumerate(ListAcq) if val in acq_name];if  IsTrue == []:err = 'The utility function {} has not been implemented, please choose one of ucb, ei, or poi.'.format(acq_name);raise NotImplementedError(err);else:self.acq_name = acq_name;self.dim=acq['dim'];if 'scalebounds' not in acq:self.scalebounds=[0,1]*self.dim;else:self.scalebounds=acq['scalebounds'];def acq_kind(self, x, gp):        y_max=np.max(gp.Y);if np.any(np.isnan(x)):return 0;if self.acq_name == 'ucb':return self._ucb(x, gp);if self.acq_name == 'lcb':return self._lcb(x, gp);if self.acq_name == 'ei':return self._ei(x, gp, y_max);if self.acq_name == 'ei_mu_max':return self._ei(x, gp, self.mu_max);if self.acq_name == 'poi':return self._poi(x, gp, y_max);if self.acq_name == 'pure_exploration':return self._pure_exploration(x, gp);if self.acq_name == 'mu':return self._mu(x, gp);if self.acq_name == 'ucb_pe':return self._ucb_pe(x, gp,self.acq['kappa'],self.acq['maxlcb']);def utility_plot(self, x, gp, y_max):if np.any(np.isnan(x)):return 0;if self.acq_name == 'ei':return self._ei_plot(x, gp, y_max);@staticmethod;def _mu(x, gp):mean, var = gp.predict(x, eval_MSE=True);mean=np.atleast_2d(mean).T;return mean;@staticmethod;def _lcb(x, gp):mean, var = gp.predict(x, eval_MSE=True);var.flags['WRITEABLE']=True;var[var<1e-10]=0;mean=np.atleast_2d(mean).T;var=np.atleast_2d(var).T;beta_t = 2 * np.log(len(gp.Y));return mean - np.sqrt(beta_t) * np.sqrt(var);@staticmethod;def _ucb(x, gp):mean, var = gp.predict(x, eval_MSE=True);var.flags['WRITEABLE']=True;var[var<1e-10]=0;mean=np.atleast_2d(mean).T;var=np.atleast_2d(var).T;beta_t = 2 * np.log(len(gp.Y));return mean + np.sqrt(beta_t) * np.sqrt(var);@staticmethod;def _ucb_pe(x, gp, kappa, maxlcb):mean, var = gp.predict_bucb(x, eval_MSE=True);var.flags['WRITEABLE']=True;var[var<1e-10]=0;mean=np.atleast_2d(mean).T;var=np.atleast_2d(var).T;value=mean + kappa * np.sqrt(var);myidx=[idx for idx,val in enumerate(value) if val<maxlcb];var[myidx]=0;return var;@staticmethod;def _pure_exploration(x, gp):mean, var = gp.predict(x, eval_MSE=True);var.flags['WRITEABLE']=True;var[var<1e-10]=0;mean=np.atleast_2d(mean).T;var=np.atleast_2d(var).T;return np.sqrt(var);@staticmethod;def _ei(x, gp, y_max):y_max=np.asscalar(y_max);mean, var = gp.predict(x, eval_MSE=True);var2 = np.maximum(var, 1e-10 + 0 * var);z = (mean - y_max)/np.sqrt(var2);out=(mean - y_max) * norm.cdf(z) + np.sqrt(var2) * norm.pdf(z);out[var2<1e-10]=0;return out;@staticmethod;def _poi(x, gp,y_max):mean, var = gp.predict(x, eval_MSE=True);var = np.maximum(var, 1e-9 + 0 * var);z = (mean - y_max)/np.sqrt(var);return norm.cdf(z);def unique_rows(a):order = np.lexsort(a.T);reorder = np.argsort(order);a = a[order];diff = np.diff(a, axis=0);ui = np.ones(len(a), 'bool');ui[1:] = (diff != 0).any(axis=1);return ui[reorder];class BColours(object):BLUE = '\t\t033[94m';CYAN = '\t\t033[36m';GREEN = '\t\t033[32m';MAGENTA = '\t\t033[35m';RED = '\t\t033[31m';ENDC = '\t\t033[0m';import numpy as np;from scipy.optimize import minimize;import sobol_seq;def acq_max_with_name(gp,scaleSearchSpace,acq_name=\"ei\",IsReturnY=False,IsMax=True,fstar_scaled=None):acq={};acq['name']=acq_name;acq['dim']=scaleSearchSpace.shape[0];acq['scaleSearchSpace']=scaleSearchSpace;if fstar_scaled:acq['fstar_scaled']=fstar_scaled;myacq=AcquisitionFunction(acq);if IsMax:x_max = acq_max(ac=myacq.acq_kind,gp=gp,bounds=scaleSearchSpace,opt_toolbox='scipy');else:x_max = acq_min_scipy(ac=myacq.acq_kind,gp=gp,bounds=scaleSearchSpace);if IsReturnY==True:y_max=myacq.acq_kind(x_max,gp=gp);return x_max,y_max;return x_max;def acq_max(ac, gp, bounds, opt_toolbox='scipy',seeds=[],IsMax=True):y_max=np.max(gp.Y);x_max = acq_max_scipy(ac=ac,gp=gp,y_max=y_max,bounds=bounds);return x_max;def generate_sobol_seq(dim,nSobol):mysobol_seq = sobol_seq.i4_sobol_generate(dim, nSobol);return mysobol_seq;def acq_min_scipy_kwargs(myfunc, SearchSpace, **kwargs):dim=SearchSpace.shape[0];x_max = SearchSpace[:, 0];min_acq = None;myopts ={'maxiter':10*dim,'maxfun':20*dim};for i in range(3*dim):x_tries = np.random.uniform(SearchSpace[:, 0], SearchSpace[:, 1],size=(100*dim, dim));y_tries=myfunc(x_tries,**kwargs);idx_min=np.argmin(y_tries);x_init_min=x_tries[idx_min];res = minimize(lambda x: myfunc(x.reshape(1, -1), **kwargs),x_init_min.reshape(1, -1),bounds=SearchSpace,method=\"L-BFGS-B\",options=myopts);if 'x' not in res:val=myfunc(res,**kwargs);else:val=myfunc(res.x,**kwargs);if min_acq is None or val <= min_acq:if 'x' not in res:x_max = res;else:x_max = res.x;min_acq = val;return np.clip(x_max, SearchSpace[:, 0], SearchSpace[:, 1]);def acq_min_scipy(ac, gp, bounds):dim=bounds.shape[0];x_max = bounds[:, 0];min_acq = None;myopts ={'maxiter':10*dim,'maxfun':20*dim};for i in range(3*dim):x_tries = np.random.uniform(bounds[:, 0], bounds[:, 1],size=(50*dim, dim));y_tries=ac(x_tries,gp=gp);idx_max=np.argmin(y_tries);x_init_max=x_tries[idx_max];res = minimize(lambda x: ac(x.reshape(1, -1), gp=gp),x_init_max.reshape(1, -1),bounds=bounds,method=\"L-BFGS-B\",options=myopts);if 'x' not in res:val=ac(res,gp);else:val=ac(res.x,gp);if min_acq is None or val <= min_acq:if 'x' not in res:x_max = res;else:x_max = res.x;min_acq = val;return np.clip(x_max, bounds[:, 0], bounds[:, 1]);def acq_max_scipy(ac, gp, y_max, bounds):dim=bounds.shape[0];x_max = bounds[:, 0];max_acq = None;myopts ={'maxiter':10*dim,'maxfun':20*dim};for i in range(1*dim):x_tries = np.random.uniform(bounds[:, 0], bounds[:, 1],size=(50*dim, dim));y_tries=ac(x_tries,gp=gp);idx_max=np.argmax(y_tries);x_init_max=x_tries[idx_max];res = minimize(lambda x: -ac(x.reshape(1, -1), gp=gp),x_init_max.reshape(1, -1),bounds=bounds,method=\"L-BFGS-B\",options=myopts);if 'x' not in res:val=ac(res,gp);else:val=ac(res.x,gp);if max_acq is None or val >= max_acq:if 'x' not in res:x_max = res;else:x_max = res.x;max_acq = val;return np.clip(x_max, bounds[:, 0], bounds[:, 1]);def acq_max_with_init(ac, gp, y_max, bounds, init_location=[]):dim=bounds.shape[0];x_max = bounds[:, 0];max_acq = None;myopts ={'maxiter':5*dim,'maxfun':10*dim};for i in range(2*dim):x_tries = np.random.uniform(bounds[:, 0], bounds[:, 1],size=(20*dim, dim));if init_location!=[]:x_tries=np.vstack((x_tries,init_location));y_tries=ac(x_tries,gp=gp);idx_max=np.argmax(y_tries);x_init_max=x_tries[idx_max];res = minimize(lambda x: -ac(x.reshape(1, -1), gp=gp),x_init_max.reshape(1, -1),bounds=bounds,method=\"L-BFGS-B\",options=myopts);if 'x' not in res:val=ac(res,gp);else:val=ac(res.x,gp);if max_acq is None or val >= max_acq:if 'x' not in res:x_max = res;else:x_max = res.x;max_acq = val;return np.clip(x_max, bounds[:, 0], bounds[:, 1]);import numpy as np;from sklearn.metrics.pairwise import euclidean_distances;from scipy.optimize import minimize;from sklearn.preprocessing import MinMaxScaler;import scipy.linalg as spla;class ProductGaussianProcess(object):    def __init__ (self,SearchSpace,gp_hyper=None,logistic_hyper=None,verbose=0):        self.noise_delta=5e-4;self.noise_upperbound=1e-2;self.mycov=self.cov_RBF_time;self.SearchSpace=SearchSpace;scaler = MinMaxScaler();scaler.fit(SearchSpace.T);self.Xscaler=scaler;self.verbose=verbose;self.dim=SearchSpace.shape[0];if gp_hyper is None:self.hyper={};self.hyper['var']=1;self.hyper['lengthscale_x']=0.02;self.hyper['lengthscale_t']=0.2;else:self.hyper=gp_hyper;if logistic_hyper is None:self.logistic_hyper={};self.logistic_hyper['midpoint']=0.0;self.logistic_hyper['growth']=1.0;else:self.logistic_hyper=logistic_hyper;self.X=[];self.T=[];self.Y=[];self.Y_curves=None;self.alpha=[];self.L=[];self.MaxEpisode=0;return None;def cov_RBF_time(self, x1,t1,x2,t2,lengthscale,lengthscale_t):        Euc_dist=euclidean_distances(x1,x2);exp_dist_x=np.exp(-np.square(Euc_dist)/lengthscale);Euc_dist=euclidean_distances(t1,t2);exp_dist_t=np.exp(-np.square(Euc_dist)/lengthscale_t);return exp_dist_x*exp_dist_t;def fit(self,X,T,Y,Y_curves):        temp=np.hstack((X,T));ur = unique_rows(temp);T=T[ur];X=X[ur];Y=Y[ur];self.X=X;self.Y=Y;self.T=T;self.Y_curves=[val for idx,val in enumerate(Y_curves) if ur[idx]==True];for curves in self.Y_curves:self.MaxEpisode=max(len(curves),self.MaxEpisode);self.KK_x_x=np.exp(-np.square(euclidean_distances(X,X))/self.hyper['lengthscale_x']-np.square(euclidean_distances(T,T))/self.hyper['lengthscale_t'])+np.eye(len(X))*self.noise_delta;if np.isnan(self.KK_x_x).any():print(\"nan in KK_x_x\");self.L=np.linalg.cholesky(self.KK_x_x);temp=np.linalg.solve(self.L,self.Y);self.alpha=np.linalg.solve(self.L.T,temp);self.cond_num=self.compute_condition_number();def compute_condition_number(self):cond_num=np.linalg.cond(self.KK_x_x);return cond_num;def log_marginal_lengthscale_logistic_hyper(self,hyper,noise_delta):        def compute_log_marginal_with_logistic_hyper(lengthscale, lengthscale_t,midpoint,growth,noise_delta):            temp=np.hstack((self.X,self.T));ur = unique_rows(temp);myX=self.X[ur];myT=self.T[ur];Y_original=transform_logistic(self.Y_curves,midpoint,growth,self.MaxEpisode);myY=(Y_original-np.mean(Y_original))/np.std(Y_original);myY=myY[ur];self.Euc_dist_x=euclidean_distances(myX,myX);self.Euc_dist_t=euclidean_distances(myT,myT);KK=np.exp(-np.square(self.Euc_dist_x)/lengthscale-np.square(self.Euc_dist_t)/lengthscale_t)+np.eye(len(myX))*noise_delta;try:temp_inv=np.linalg.solve(KK,myY);except:return -np.inf;try:first_term=-0.5*np.dot(myY.T,temp_inv);if KK.shape[0]>200:idx=np.random.permutation(KK.shape[0]);idx=idx[:200];KK=KK[np.ix_(idx,idx)];chol  = spla.cholesky(KK, lower=True);W_logdet=np.sum(np.log(np.diag(chol)));second_term=-W_logdet;except:return -np.inf;logmarginal=first_term+second_term-0.5*len(myY)*np.log(2*3.14);if np.isnan(np.asscalar(logmarginal))==True:print(\"lengthscale_x={:f} lengthscale_t={:f} first term ={:.4f} second  term ={:.4f}\".format(lengthscale,lengthscale_t,np.asscalar(first_term),np.asscalar(second_term)));return np.asscalar(logmarginal);logmarginal=0;if not isinstance(hyper,list) and len(hyper.shape)==2:logmarginal=[0]*hyper.shape[0];growth=hyper[:,3];midpoint=hyper[:,2];lengthscale_t=hyper[:,1];lengthscale_x=hyper[:,0];for idx in range(hyper.shape[0]):logmarginal[idx]=compute_log_marginal_with_logistic_hyper(lengthscale_x[idx],lengthscale_t[idx],midpoint[idx],growth[idx],noise_delta);else:lengthscale_x,lengthscale_t,midpoint,growth=hyper;logmarginal=compute_log_marginal_with_logistic_hyper(lengthscale_x,lengthscale_t,midpoint,growth,noise_delta);return logmarginal;def optimize_lengthscale_SE_logistic_hyper(self,previous_hyper,noise_delta):        SearchSpace_l_min=0.03;SearchSpace_l_max=0.3;SearchSpace_midpoint_min=-2;SearchSpace_midpoint_max=3;SearchSpace_growth_min=0.5;SearchSpace_growth_max=2;mySearchSpace=np.asarray([[SearchSpace_l_min,SearchSpace_l_max],[10*SearchSpace_l_min,2*SearchSpace_l_max],[SearchSpace_midpoint_min,SearchSpace_midpoint_max],[SearchSpace_growth_min,SearchSpace_growth_max]]);lengthscale_tries = np.random.uniform(mySearchSpace[:, 0], mySearchSpace[:, 1],size=(20, 4));self.flagOptimizeHyperFirst=0;logmarginal_tries=self.log_marginal_lengthscale_logistic_hyper(lengthscale_tries,noise_delta);idx_max=np.argmax(logmarginal_tries);lengthscale_init_max=lengthscale_tries[idx_max];myopts ={'maxiter':30*self.dim,'maxfun':30*self.dim};x_max=[];max_log_marginal=None;res = minimize(lambda x: -self.log_marginal_lengthscale_logistic_hyper(x,noise_delta),lengthscale_init_max,bounds=mySearchSpace,method=\"L-BFGS-B\",options=myopts);if 'x' not in res:val=self.log_marginal_lengthscale_logistic_hyper(res,noise_delta);else:val=self.log_marginal_lengthscale_logistic_hyper(res.x,noise_delta);if max_log_marginal is None or val >= max_log_marginal:if 'x' not in res:x_max = res;else:x_max = res.x;max_log_marginal = val;return x_max;def optimize_lengthscale_logistic_hyper(self,prev_hyper,noise_delta):        newlengthscale,newlengthscale_t,newmidpoint,newgrowth=self.optimize_lengthscale_SE_logistic_hyper(prev_hyper,noise_delta);self.hyper['lengthscale_x']=newlengthscale;self.hyper['lengthscale_t']=newlengthscale_t;temp=np.hstack((self.X,self.T));ur = unique_rows(temp);Y_original=transform_logistic(self.Y_curves,newmidpoint,newgrowth,self.SearchSpace[-1,1]);Y=(Y_original-np.mean(Y_original))/np.std(Y_original);self.Y=Y;self.fit(self.X[ur],self.T[ur],self.Y[ur],self.Y_curves);return newlengthscale,newlengthscale_t,newmidpoint,newgrowth;def compute_var(self,X,T,xTest,tTest):        xTest=np.asarray(xTest);xTest=np.atleast_2d(xTest);tTest=np.asarray(tTest);tTest=np.atleast_2d(tTest);tTest=np.reshape(tTest,(-1,1));if self.kernel_name=='SE':myX=X;myT=T;Euc_dist_x=euclidean_distances(myX,myX);Euc_dist_t=euclidean_distances(myT,myT);KK=np.exp(-np.square(Euc_dist_x)/self.hyper['lengthscale_x']-np.square(Euc_dist_t)/self.hyper['lengthscale_t'])+np.eye(len(myX))*self.noise_delta;Euc_dist_test_train_x=euclidean_distances(xTest,X);Euc_dist_test_train_t=euclidean_distances(tTest,T);KK_xTest_xTrain=np.exp(-np.square(Euc_dist_test_train_x)/self.hyper['lengthscale_x']-np.square(Euc_dist_test_train_t)/self.hyper['lengthscale_t']);try:temp=np.linalg.solve(KK,KK_xTest_xTrain.T);except:temp=np.linalg.lstsq(KK,KK_xTest_xTrain.T, rcond=-1);temp=temp[0];var=np.eye(xTest.shape[0])-np.dot(temp.T,KK_xTest_xTrain.T);var=np.diag(var);var.flags['WRITEABLE']=True;var[var<1e-100]=0;return var;def predict(self,xTest, eval_MSE=True):        if len(xTest.shape)==1:xTest=xTest.reshape((-1,self.X.shape[1]+1));tTest=xTest[:,-1];tTest=np.atleast_2d(tTest);tTest=np.reshape(tTest,(xTest.shape[0],-1));xTest=xTest[:,:-1];temp=np.hstack((self.X,self.T));ur = unique_rows(temp);X=self.X[ur];T=self.T[ur];Euc_dist_x=euclidean_distances(xTest,xTest);Euc_dist_t=euclidean_distances(tTest,tTest);KK_xTest_xTest=np.exp(-np.square(Euc_dist_x)/self.hyper['lengthscale_x']-np.square(Euc_dist_t)/self.hyper['lengthscale_t'])+np.eye(xTest.shape[0])*self.noise_delta;Euc_dist_test_train_x=euclidean_distances(xTest,X);Euc_dist_test_train_t=euclidean_distances(tTest,T);KK_xTest_xTrain=np.exp(-np.square(Euc_dist_test_train_x)/self.hyper['lengthscale_x']-np.square(Euc_dist_test_train_t)/self.hyper['lengthscale_t']);mean=np.dot(KK_xTest_xTrain,self.alpha);v=np.linalg.solve(self.L,KK_xTest_xTrain.T);var=KK_xTest_xTest-np.dot(v.T,v);return mean.ravel(),np.diag(var);def posterior(self,x):return self.predict(self,x);import numpy as np;import time;from sklearn import linear_model;import copy;from sklearn.preprocessing import MinMaxScaler;counter = 0;class BOIL(object):    def __init__(self, func, SearchSpace,acq_name=\"ei_mu_max\",verbose=1):        self.method='boil';self.verbose=verbose;if isinstance(SearchSpace,dict):self.keys = list(SearchSpace.keys());self.SearchSpace = [];for key in list(SearchSpace.keys()):self.SearchSpace.append(SearchSpace[key]);self.SearchSpace = np.asarray(self.SearchSpace);else:self.SearchSpace=np.asarray(SearchSpace);self.dim = len(SearchSpace);scaler = MinMaxScaler();scaler.fit(self.SearchSpace[:-1,:].T);scalerT = MinMaxScaler();SearchSpace_T=np.atleast_2d(self.SearchSpace[-1,:]).T;scalerT.fit(SearchSpace_T);self.Xscaler=scaler;self.Tscaler=scalerT;self.scaleSearchSpace=np.array([np.zeros(self.dim), np.ones(self.dim)]).T;self.f = func;self.X_ori= None;self.X = None;self.Y = None;self.Y_ori = None;self.T=None;self.T_original=None;self.Y_cost_original=None;self.time_opt=0;self.max_min_gap=self.SearchSpace[:,1]-self.SearchSpace[:,0];self.acq_name = acq_name;self.logmarginal=0;self.gp=ProductGaussianProcess(self.scaleSearchSpace,verbose=verbose);self.Y_curves=[];self.Y_cost_original=None;self.time_opt=0;self.acq_func = None;self.logmarginal=0;self.markVirtualObs=[];self.countVirtual=[];self.linear_regression = linear_model.LinearRegression();self.condition_number=[];self.max_n_augmentation=10;self.threshold_cond=15;def init(self, n_init_points=3, seed=1):        np.random.seed(seed);SearchSpace=np.copy(self.SearchSpace);SearchSpace[-1,0]=SearchSpace[-1,1];l = [np.random.uniform(x[0], x[1]) for _ in range(n_init_points) for x in SearchSpace];temp=np.asarray(l);temp=temp.T;init_X=list(temp.reshape((n_init_points,-1)));self.X_original = np.asarray(init_X);self.T_original=self.X_original[:,-1];self.T_original=np.reshape(self.T_original,(n_init_points,-1));self.X_original=self.X_original[:,:-1];self.X_original=np.reshape(self.X_original,(n_init_points,-1));y_init_curves, y_init_cost=self.f(init_X);y_init_cost=np.atleast_2d(np.asarray(y_init_cost));self.Y_curves+=y_init_curves;y_init=transform_logistic(y_init_curves,self.gp.logistic_hyper['midpoint'],self.gp.logistic_hyper['growth'], self.SearchSpace[-1,1]);y_init=np.reshape(y_init,(n_init_points,1));self.Y_original = np.asarray(y_init);self.Y_cost_original=np.reshape(y_init_cost,(-1,1));self.X = self.Xscaler.transform(np.asarray(init_X)[:,:-1]);self.X=np.reshape(self.X,(n_init_points,-1));self.T = self.Tscaler.transform(self.T_original);self.markVirtualObs+=[0]*n_init_points;for ii in range(n_init_points):self.generating_virtual_observations(self.X[ii,:],self.T[ii],[y_init_curves[ii]],y_init_cost[0][ii],IsRandom=False);self.Y_cost=(self.Y_cost_original-np.min(self.Y_cost_original))/(np.max(self.Y_cost_original)-np.min(self.Y_cost_original));if np.std(self.Y_original)==0:self.Y=(self.Y_original-np.mean(self.Y_original));else:self.Y=(self.Y_original-np.mean(self.Y_original))/np.std(self.Y_original);def utility_cost_evaluation(self,x,acq_func,isDebug=False):        def utility_cost_evaluation_single(x,acq_func,isDebug=False):            utility=acq_func.acq_kind(x,gp=self.gp);try:mean_cost=self.linear_regression.predict(np.reshape(x,(1,-1)));except:print(x);print(\"bug\");mean_cost=max(0,mean_cost)+0.1;if 'ei' in acq_func.acq_name:acquisition_function_value= np.log(utility)-np.log(mean_cost);else:acquisition_function_value= np.log(1+np.exp(utility))/np.log(1+np.exp(mean_cost));if isDebug==True:print(\"acq_func at the selected point \t utility:\",np.round(utility,decimals=4),\"\t cost:\",mean_cost);if utility==0:print(\"utility =0===============================================================================\");return acquisition_function_value*(-1);if len(x)==self.dim:temp=utility_cost_evaluation_single(x,acq_func,isDebug=isDebug);if isDebug==True:return temp;else:utility=np.mean(temp);else:utility=[0]*len(x);for idx,val in enumerate(x):temp=utility_cost_evaluation_single(x=val,acq_func=acq_func,isDebug=isDebug);utility[idx]=np.mean(temp);utility=np.asarray(utility);return utility;def acq_utility_cost(self):        acq={};acq['name']=self.acq_name;acq['dim']=self.scaleSearchSpace.shape[0];acq['scaleSearchSpace']=self.scaleSearchSpace;if self.acq_name=='ei_mu_max':x_mu_max,mu_max_val=acq_max_with_name(gp=self.gp,scaleSearchSpace=self.scaleSearchSpace,acq_name='mu',IsReturnY=True);acq['mu_max']=  mu_max_val;myacq=AcquisitionFunction(acq);x_min = acq_min_scipy_kwargs(myfunc=self.utility_cost_evaluation,SearchSpace=self.scaleSearchSpace,acq_func=myacq, isDebug=False);if self.verbose==True:acq_val=self.utility_cost_evaluation(x_min,myacq,isDebug=False);print(\"selected point from acq func:\",np.round(x_min,decimals=4),\"acq val=log(Utility/Cost)=\",(-1)*np.round(acq_val,decimals=4));if np.round(acq_val,decimals=4)==0:print(\"acq value =0\");return x_min;def select_informative_location_by_uncertainty(self,n_virtual_obs,x_max,t_max):        SearchSpace=np.copy(self.scaleSearchSpace);for dd in range(self.dim-1):SearchSpace[dd,0],SearchSpace[dd,1]=x_max[dd],x_max[dd];SearchSpace[-1,1]=t_max;temp_X,temp_T=self.X.copy(),self.T.copy();temp_gp=copy.deepcopy(self.gp );temp_Y=np.random.random(size=(len(temp_T),1));temp_gp.fit(temp_X,temp_T,temp_Y,self.Y_curves);new_batch_T=None;pred_var_value=[0]*n_virtual_obs;for ii in range(n_virtual_obs):x_max_pred_variance, pred_var_value[ii]=acq_max_with_name(gp=temp_gp,scaleSearchSpace=SearchSpace,acq_name='pure_exploration',IsReturnY=True);log_cond=np.log( temp_gp.compute_condition_number() );if log_cond>self.threshold_cond or pred_var_value[ii]<(self.gp.noise_delta+1e-3):break;if x_max_pred_variance[-1] in temp_T[-ii:]:break;temp_X = np.vstack((temp_X, x_max.reshape((1, -1))));temp_T = np.vstack((temp_T, x_max_pred_variance[-1].reshape((1, -1))));temp_gp.X,temp_gp.T=temp_X,temp_T;temp_Y=np.random.random(size=(len(temp_T),1));temp_gp.fit(temp_X,temp_T,temp_Y,self.Y_curves);if new_batch_T is None:new_batch_T=x_max_pred_variance[-1].reshape((1, -1));else:new_batch_T= np.vstack((new_batch_T, x_max_pred_variance[-1].reshape((1, -1))));if new_batch_T is None:return [],0;else:output=np.sort(new_batch_T.ravel()).tolist();return output, len(output);def generating_virtual_observations(self,x_max,t_max,y_original_curves,y_cost_original,IsRandom=False):        temp_X_new_original=self.Xscaler.inverse_transform(np.reshape(x_max,(-1,self.dim-1)));max_n_virtual_obs=np.int(t_max*self.max_n_augmentation);if max_n_virtual_obs==0:self.countVirtual.append(0);return;if IsRandom==True:l = [np.random.uniform(0, t_max) for _ in range(max_n_virtual_obs)];else:l,n_virtual_obs=self.select_informative_location_by_uncertainty(max_n_virtual_obs,x_max,t_max);self.countVirtual.append(n_virtual_obs);if self.verbose:np.set_printoptions(suppress=True);print(\"Max #augmented points\",max_n_virtual_obs, \"\t #augmented points \",len(l),\"\t Augmented points: \",np.round(l,decimals=3));l_original=[self.SearchSpace[-1,0]+val*self.max_min_gap[-1] for val in l];virtual_obs_t_original=np.asarray(l_original).T;virtual_obs_t=np.asarray(l).T;y_virtual_original=[0]*n_virtual_obs;for ii in range(n_virtual_obs):idx=np.int(virtual_obs_t_original[ii]);temp_curve=y_original_curves[0][:idx+1];self.markVirtualObs.append(1);y_virtual_original[ii]=transform_logistic([temp_curve],self.gp.logistic_hyper['midpoint'],self.gp.logistic_hyper['growth'],self.SearchSpace[-1,1]);self.X = np.vstack((self.X, x_max.reshape((1, -1))));self.X_original=np.vstack((self.X_original, temp_X_new_original));self.T = np.vstack((self.T, virtual_obs_t[ii].reshape((1, -1))));temp=np.asarray(virtual_obs_t_original[ii]);self.T_original=np.vstack((self.T_original, temp.reshape((1, -1))));self.Y_original = np.append(self.Y_original,[y_virtual_original[ii]]);self.Y_curves.append(temp_curve);y_cost_estimate=y_cost_original*virtual_obs_t[ii];self.Y_cost_original = np.append(self.Y_cost_original,[y_cost_estimate]);def suggest_nextpoint(self):         self.gp=ProductGaussianProcess(self.scaleSearchSpace,self.gp.hyper,self.gp.logistic_hyper);self.gp.fit(self.X, self.T,self.Y,self.Y_curves);self.condition_number.append(self.gp.cond_num);if self.verbose:print(\"ln of conditioning number of GP covariance matrix\", np.round(np.log(self.gp.cond_num),decimals=1));count=len(self.markVirtualObs)-np.sum(self.markVirtualObs);count=np.int(count);if  len(self.Y)%(2*self.dim)==0:hyper=[self.gp.hyper['lengthscale_x'],self.gp.hyper['lengthscale_t'],self.gp.logistic_hyper['midpoint'],self.gp.logistic_hyper['growth']];newlengthscale_x,newlengthscale_t,new_midpoint, new_growth = self.gp.optimize_lengthscale_logistic_hyper(hyper,self.gp.noise_delta);self.gp.hyper['lengthscale_x']=newlengthscale_x;self.gp.hyper['lengthscale_t']=self.gp.hyper['lengthscale_t'];self.gp.logistic_hyper['midpoint']=new_midpoint;self.gp.logistic_hyper['growth']=new_growth;if self.verbose:print(\"==estimated lengthscale_x={:.4f}   lengthscale_t={:.3f}   Logistic_m0={:.1f}   Logistic_g0={:.1f}\".format(newlengthscale_x,newlengthscale_t,new_midpoint,new_growth));start_opt=time.time();combine_input=np.hstack((self.X,self.T));self.linear_regression.fit(combine_input,self.Y_cost);x_max_temp=self.acq_utility_cost();x_max=x_max_temp[:-1];t_max=x_max_temp[-1];finished_opt=time.time();elapse_opt=finished_opt-start_opt;self.time_opt=np.hstack((self.time_opt,elapse_opt));self.markVirtualObs.append(0);self.X = np.vstack((self.X, x_max.reshape((1, -1))));self.T = np.vstack((self.T, t_max.reshape((1, -1))));temp_X_new_original=self.Xscaler.inverse_transform(np.reshape(x_max,(-1,self.dim-1)));self.X_original=np.vstack((self.X_original, temp_X_new_original));temp_T_new_original=self.Tscaler.inverse_transform(np.reshape(t_max,(-1,1)));self.T_original=np.vstack((self.T_original, temp_T_new_original));x_original_to_test=x_max_temp*self.max_min_gap+self.SearchSpace[:,0];y_original_curves, y_cost_original= self.f(x_original_to_test);y_original=transform_logistic(y_original_curves,self.gp.logistic_hyper['midpoint'],self.gp.logistic_hyper['growth'],self.SearchSpace[-1,1]);if len(y_original_curves)==1:self.Y_curves.append(y_original_curves[0]);else:self.Y_curves.append(y_original_curves);self.Y_original = np.append(self.Y_original,y_original);self.Y_cost_original = np.append(self.Y_cost_original,y_cost_original);self.generating_virtual_observations(x_max,t_max,y_original_curves,y_cost_original[0]);if np.std(self.Y_original)==0:self.Y=(self.Y_original-np.mean(self.Y_original));else:self.Y=(self.Y_original-np.mean(self.Y_original))/np.std(self.Y_original);self.Y_cost=(self.Y_cost_original-np.min(self.Y_cost_original))/(np.max(self.Y_cost_original)-np.min(self.Y_cost_original));np.set_printoptions(suppress=True);print(\"[original scale] x={} t={:.0f} current y={:.2f}, ybest={:.2f}\".format( np.round(self.X_original[-1],decimals=4),np.asscalar(self.T_original[-1]),np.asscalar(self.Y_original[-1]), np.asscalar(self.Y_original.max())))",
    "Experiment Result": "GP Model:\n- Gaussian Process using a product kernel, k([x,t],[x',t']) = k(x,x') × k(t,t'), with Radial Basis Function (RBF) kernels for both hyperparameter (x) and iteration (t) spaces.\n- Initial `lengthscale_x`: 0.02\n- Initial `lengthscale_t`: 0.2\n- Noise delta: 5e-4\n- GP hyperparameters (`lengthscale_x`, `lengthscale_t`) and Logistic preference function hyperparameters (`midpoint`, `growth`) are optimized every `2 * dim` iterations (where `dim` is the dimensionality of the search space).\n- Optimization is performed by maximizing log marginal likelihood using the L-BFGS-B method.\n- Optimization bounds for `lengthscale_x`: [0.03, 0.3]\n- Optimization bounds for `lengthscale_t`: [0.3, 0.6]\n\nTraining Curve Compression:\n- Uses a Sigmoid (Logistic) preference function `l(u|m0,g0)` to transform the entire learning curve into a single numeric score.\n- Initial `midpoint` (`m0`): 0.0\n- Initial `growth` (`g0`): 1.0\n- Optimization bounds for `midpoint`: [-2, 3]\n- Optimization bounds for `growth`: [0.5, 2]\n\nAcquisition Function:\n- A modified Expected Improvement criterion (`ei_mu_max`) divided by a linear cost approximation, expressed as `log(utility) - log(mean_cost)`.\n- The `ei_mu_max` variant uses the maximum of the GP's predictive mean function as the incumbent value.\n- Cost approximation: A linear regression model (`sklearn.linear_model.LinearRegression`) is fitted to the observed costs and corresponding inputs (hyperparameters and iteration).\n- Acquisition function maximization: Performed using L-BFGS-B (from `scipy.optimize.minimize`).\n\nData Augmentation (Selective):\n- Employed to improve sample-efficiency and prevent GP covariance matrix ill-conditioning.\n- Maximum number of virtual observations for a single real observation: `max_n_augmentation = 10` (up to 10 * t_max, where t_max is the iteration for the current observation).\n- Virtual points are sampled from the observed curve at locations of maximum GP predictive uncertainty.\n- Augmentation for a given curve stops if the natural log of the covariance matrix condition number of the GP exceeds `threshold_cond = 15` or if the predictive variance at the proposed augmentation location falls below `(gp.noise_delta + 1e-3)`."
}{
    "Title": "Provably Efficient Online Hyperparameter Optimization with Population-Based Bandits",
    "Main Contributions": "The paper introduces Population-Based Bandits (PB2), the first provably efficient algorithm in the style of Population-Based Training (PBT). PBT is effective for hyperparameter optimization, especially in reinforcement learning (RL), but lacks theoretical guarantees and requires extensive computational resources due to its reliance on random heuristics. PB2 addresses these limitations by using a probabilistic model (Gaussian Process bandits) to guide the hyperparameter search efficiently. Key contributions include the derivation of the first sublinear regret bound for a PBT-style algorithm, demonstrating its theoretical efficiency and quantifying the gap between optimal and achieved rewards. Experimentally, PB2 is shown to achieve high performance in RL tasks with a modest computational budget, requiring significantly fewer agents than PBT. It also proves more robust to mis-specified hyperparameter ranges compared to PBT and is more computationally efficient than existing batch Bayesian Optimization (BO) approaches by optimizing hyperparameter schedules in a single training run.",
    "Methodology": "PB2 frames the online hyperparameter selection problem as batch Gaussian Process (GP) bandit optimization of a time-varying function. The core methodology involves: (1) Modeling the objective function (e.g., RL reward) with a time-varying GP, using a kernel that combines a squared exponential kernel with a time kernel (kSE ◦ ktime) to capture the non-stationary nature of neural network training. A parameter 'ω' is introduced to model how the function varies over time. (2) Employing a parallel agent selection mechanism based on the GP model. For a batch of B agents, hyperparameters are selected by sequentially maximizing an acquisition function: xb_t = arg max x∈D µt,1(x) + sqrt(βt)σt,b(x), which updates the uncertainty based on currently training agents to reduce redundancy and explore distinct regions. (3) The PB2 algorithm iteratively updates network weights, evaluates models, records data, and periodically (every 'tready' steps) performs two actions: copying weights from top-performing agents to underperforming ones, and selecting new hyperparameters for all agents by fitting a GP model to the collected data and maximizing the acquisition function. The theoretical foundation includes a convergence guarantee showing sublinear cumulative regret under correlated time-varying functions.",
    "Experimental Setup": "Experiments primarily focused on the Reinforcement Learning (RL) setting, known for its hyperparameter sensitivity. On-policy RL tasks from OpenAI Gym (BipedalWalker, LunarLanderContinuous, Hopper, InvertedDoublePendulum) were used to optimize PPO hyperparameters (batch size, learning rate, GAE parameter λ, PPO clip parameter ϵ). Off-policy RL was tested with IMPALA on Breakout and SpaceInvaders environments from the Arcade Learning Environment. A supervised learning case (CNN on CIFAR-10) was also explored to assess generality. Population sizes of B ∈ {4, 8} were used, which are significantly smaller than typical PBT requirements (B > 20), with scaling tests up to B = 16. Training durations varied (10^6 environment timesteps for on-policy RL, 10 million for off-policy RL, 50 epochs for supervised learning), with 'tready' intervals for periodic updates. Baselines included Population-Based Training (PBT), Random Search (RS), Bayesian Optimization (BO) with Expected Improvement, and ASHA (Asynchronous Successive Halving Algorithm). Experiments were repeated with multiple random seeds (10 for on-policy RL, 7 for off-policy RL, 5 for supervised learning) to ensure robustness, and results were evaluated based on median best reward/accuracy and interquartile ranges. An additional experiment on BipedalWalker tested robustness to wide hyperparameter ranges.",
    "Limitations": "The theoretical regret bound for PB2 holds under the condition that the population size 'B' is much less than the total number of timesteps 'T' (B << T). The introduction of 'ω' as a hyperparameter for the time-varying GP model, while optimizable, adds a new parameter to tune. In the worst-case scenario where the time-varying function is not correlated at all, PB2 may only achieve linear regret. While PB2 effectively addresses many limitations of PBT, such as its reliance on random heuristics and large computational resources, it still operates within a population-based framework, meaning the initial quality of the population or the specification of hyperparameter ranges could still influence performance, although PB2 shows greater robustness to mis-specified ranges. The theoretical bound when using a single agent (B=1) reduces to the time-varying GP-UCB setting, suggesting the full benefits of parallelization are achieved with larger B.",
    "Future Research Directions": "The authors suggest several promising avenues for future research. These include dynamically updating population sizes based on the acquisition function's value, which could further optimize resource allocation. Another direction is to expand the search space for Bayesian Optimization within PB2 to include the selection of optimization algorithms or neural network architectures themselves, beyond just their hyperparameters. Additionally, exploring how to incorporate off-policy data into the PB2 framework is proposed, potentially leading to further performance gains in reinforcement learning experiments, building on existing work that leverages such data.",
    "Experiment Code": "from GPy.kern import Kern\nfrom GPy.core import Param\nfrom sklearn.metrics import pairwise_distances\nfrom sklearn.metrics.pairwise import euclidean_distances\n\nimport numpy as np\n\nclass TV_SquaredExp(Kern):\n    def __init__(self,input_dim, variance=1.,lengthscale=1.,epsilon=0.,active_dims=None):\n        super().__init__(input_dim, active_dims, 'time_se')\n        self.variance = Param('variance', variance)\n        self.lengthscale = Param('lengthscale', lengthscale)\n        self.epsilon = Param('epsilon', epsilon)\n        self.link_parameters(self.variance, self.lengthscale, self.epsilon)\n        \n    def K(self,X,X2):\n        # time must be in the far left column\n        if self.epsilon > 0.5: # 0.5\n            self.epsilon = 0.5\n        if X2 is None: X2 = np.copy(X)\n        T1 = X[:, 0].reshape(-1, 1)\n        T2 = X2[:, 0].reshape(-1, 1)\n        dists = pairwise_distances(T1,T2, 'cityblock')\n        timekernel=(1-self.epsilon)**(0.5*dists)\n        \n        X = X[:, 1:]\n        X2 = X2[:, 1:]\n\n        RBF = self.variance*np.exp(-np.square(euclidean_distances(X,X2))/self.lengthscale)\n        \n        return RBF * timekernel\n    \n    def Kdiag(self,X):\n        return self.variance*np.ones(X.shape[0])\n    \n    def update_gradients_full(self, dL_dK, X, X2):\n        if X2 is None: X2 = np.copy(X)\n        T1 = X[:, 0].reshape(-1, 1)\n        T2 = X2[:, 0].reshape(-1, 1)\n        \n        X = X[:, 1:]\n        X2 = X2[:, 1:]\n        dist2 = np.square(euclidean_distances(X,X2))/self.lengthscale\n    \n        dvar = np.exp(-np.square((euclidean_distances(X,X2))/self.lengthscale))\n        dl =  - (2 * euclidean_distances(X,X2)**2 * self.variance * np.exp(-dist2)) * self.lengthscale**(-2)\n        n = pairwise_distances(T1,T2, 'cityblock')/2\n        deps = -n * (1-self.epsilon)**(n-1)\n    \n        self.variance.gradient = np.sum(dvar*dL_dK)\n        self.lengthscale.gradient = np.sum(dl*dL_dK)\n        self.epsilon.gradient = np.sum(deps*dL_dK)\n\n\ndef UCB(m, m1, x, fixed, kappa=0.5):\n    \n    c1 = 0.2 #\tfrom TV-GP-UCB\n    c2 = 0.4\n    beta_t = c1 * np.log(c2 * m.X.shape[0])\n    kappa = np.sqrt(beta_t)\n    \n    xtest = np.concatenate((fixed.reshape(-1, 1), np.array(x).reshape(-1,1))).T\n    \n    preds = m.predict(xtest)\n    mean = preds[0][0][0] \n    \n    preds = m1.predict(xtest)\n    var = preds[1][0][0]\n    return mean + kappa * var\n\n\ndef optimize_acq(func, m, m1, fixed, num_f):\n    \n    print(\"Optimizing Acquisition Function...\\n\")\n    \n    opts = {'maxiter':200, 'maxfun':200, 'disp':False}\n    \n    T=10\n    best_value=-999\n    best_theta = m1.X[0,:]\n    \n    bounds = [(0,1) for _ in range(m.X.shape[1]-num_f)]\n    \n    for ii in range(T):\n        x0 = np.random.uniform(0,1, m.X.shape[1]-num_f)\n        \n        res = minimize(lambda x: -func(m, m1, x, fixed), x0, bounds=bounds, method=\"L-BFGS-B\", options=opts)\n        \n        val = func(m, m1, res.x, fixed)\n        if val > best_value:\n            best_value=val\n            best_theta =res.x\n    \n    return(np.clip(best_theta, 0, 1))\n\n\ndef select_config(Xraw, yraw, current, newpoint, bounds, num_f, num):\n    \n    length = select_length(Xraw, yraw, current, newpoint, bounds, num_f, num)\n    print(\"\\n\\nUsing length = {}\\n\\n\".format(length))\n    \n    Xraw = Xraw[-length:, :]\n    yraw = yraw[-length: ]\n        \n    base_vals = np.array(list(bounds.values())).T\n    oldpoints = Xraw[:, :num_f]\n    old_lims = np.concatenate((np.max(oldpoints, axis=0), np.min(oldpoints, axis=0))).reshape(2, oldpoints.shape[1])\n    limits = np.concatenate((old_lims, base_vals),axis=1)\n    \n    X = normalize(Xraw, limits)\n    y = standardize(yraw).reshape(yraw.size, 1)\n    \n    fixed = normalize(newpoint, oldpoints)\n\n    kernel = TV_SquaredExp(input_dim=X.shape[1], variance=1., lengthscale=1., epsilon=0.1)\n    try:\n        m = GPy.models.GPRegression(X, y, kernel)\n    except np.linalg.LinAlgError:\n        X, y = get_diverse(X, y)\n        m = GPy.models.GPRegression(X, y, kernel)\n    \n    try:\n        m.optimize(messages=True)\n    except np.linalg.LinAlgError:\n        X, y = get_diverse(X, y)\n        m = GPy.models.GPRegression(X, y, kernel)\n        m.optimize(messages=True)\n        \n    m.kern.lengthscale.fix(m.kern.lengthscale.clip(1e-5,1))\n    \n    if current is None:\n        m1 = deepcopy(m)\n    else:\n        padding = np.array([fixed for _ in range(current.shape[0])])\n        current = normalize(current, base_vals)\n        current = np.hstack((padding, current))\n\n        Xnew = np.vstack((X, current))\n        ypad = np.zeros(current.shape[0])\n        ypad = ypad.reshape(-1,1)\n        ynew = np.vstack((y, ypad))\n        \n        kernel = TV_SquaredExp(input_dim=X.shape[1], variance=1., lengthscale=1., epsilon=0.1)\n        m1 = GPy.models.GPRegression(Xnew, ynew, kernel)\n        m1.optimize()\n    \n    time_param = m1.time_se.epsilon[0]\n    lengthscale = m1.time_se.lengthscale[0]\n    xt = optimize_acq(UCB, m, m1, fixed, num_f)\n    \n    # check dist vs other points:\n    dists = euclidean_distances(X[:, num_f:], X[:, num_f:])\n    meandist = np.mean(dists)\n    min_new = np.min(euclidean_distances(X[:, num_f:], xt.reshape(-1,1).T))\n    \n    # convert back...\n    xt = xt * (np.max(base_vals,axis=0) - np.min(base_vals,axis=0)) + np.min(base_vals, axis=0)\n    \n    print(\"\\n**\\n\\n New Point: {} \\n\\n**\\n\".format(str(xt)))\n    print(\"\\n**\\n\\n Length Scale: {} \\n\\n**\\n\".format(str(lengthscale)))\n    print(\"\\n**\\n\\n Min Dist: {} \\n\\n**\\n\".format(str(min_new)))\n    print(\"\\n**\\n\\n vs. Mean Dist: {} \\n\\n**\\n\".format(str(meandist)))\n    \n    xt = xt.astype(np.float32)\n    return(xt, lengthscale, min_new, meandist)\n\n\n\nclass PB2(FIFOScheduler):\n    # ... (rest of class definition)\n    def __init__(self,\n                 time_attr=\"time_total_s\",\n                 reward_attr=None,\n                 metric=\"episode_reward_mean\",\n                 mode=\"max\",\n                 perturbation_interval=60.0,\n                 hyperparam_mutations={},\n                 quantile_fraction=0.25,\n                 resample_probability=0.25,\n                 custom_explore_fn=None,\n                 log_config=True):\n        # ... (initialization code) ...\n        self.meta = {'timesteps': [],'lengthscales': [], 'closest': [], 'meandist': []}\n        self.latest = 0 # when we last did bayesopt\n        self.data = pd.DataFrame()\n        \n        self.bounds = {}\n        for key, distribution in self._hyperparam_mutations.items():\n            self.bounds[key] = [np.min([distribution() for _ in range(999999)]),np.max([distribution() for _ in range(999999)])]\n\n    def _exploit(self, trial_executor, trial, trial_to_clone):\n        # ... (exploit logic)\n        print(\"\\n\\n\\n\\n Copying: \\n{} \\n with:{} \\n\\n\".format(str(trial), str(trial_to_clone)))\n        new_config, lengthscale, mindist, meandist, data = explore(self.data, self.bounds,\n                             self.current,\n                             trial_to_clone,\n                             trial,\n                             trial_to_clone.config,\n                             self._hyperparam_mutations,\n                             self._resample_probability)\n        \n        # important to replace the old values, since we are copying across\n        self.data = data.copy()\n        \n        # if the current guy youre selecting is at a point youve already done, \n        # then append the data to the \"current\" which is the points in the current batch\n        \n        new = []\n        for key in self._hyperparam_mutations.keys():\n            new.append(new_config[key])\n    \n        new  = np.array(new)\n        new = new.reshape(1, new.size)\n        if self.data['T'].max() > self.latest:\n            self.latest = self.data['T'].max()\n            self.current = new.copy()\n        else:\n            self.current = np.concatenate((self.current, new), axis=0)\n            print(\"\\n\\n\\n\\n\\n Currently Evaluating \\n\\n\\n\\n\\n\")\n            print(self.current)\n            print(\"\\n\\n\\n\\n\\n\")\n        \n        # log the lengthscale\n        self.meta['timesteps'].append(self.data['T'].values[-1])\n        self.meta['lengthscales'].append(lengthscale)\n        self.meta['closest'].append(mindist)\n        self.meta['meandist'].append(meandist)\n        meta = pd.DataFrame({'timesteps': self.meta['timesteps'], \n                             'lengthscales': self.meta['lengthscales'],\n                             'closest': self.meta['closest'],\n                             'meandist': self.meta['meandist']})\n        meta.to_csv('meta_data.csv')\n        \n        # ... (rest of _exploit method)",
    "Experiment Result": "The PB2 scheduler is initialized with the following general settings:\n- `time_attr`: `args.criteria` (default \"timesteps_total\" in both `run_impala.py` and `run_ppo.py`).\n- `metric`: \"episode_reward_mean\".\n- `mode`: \"max\".\n- `resample_probability`: 0 (explicitly set for PB2, indicating no random resampling).\n- `quantile_fraction`: `args.perturb` (default 0.25), meaning the bottom 25% of trials copy from the top 25%.\n- `custom_explore_fn`: An `explore` function is used to postprocess perturbed configurations, ensuring `train_batch_size >= sgd_minibatch_size * 2`, `num_sgd_iter >= 1`, and casting specific parameters (`target_delay` for Impala, `train_batch_size` for PPO, `lambda` <= 1 for PPO) to valid types or ranges.\n\nSpecific settings for different algorithms:\n\n**For IMPALA (`run_impala.py`):\n**- `perturbation_interval`: `args.freq` (default 500,000 timesteps).\n- `num_samples`: `args.num_samples` (default 4).\n- `stop` criteria: `args.max` (default 3,000,000 timesteps).\n- Default environment: \"SpaceInvadersNoFrameskip-v4\".\n- `hyperparam_mutations` for PB2:\n    - \"epsilon\": `lambda: random.uniform(0.01, 0.5)`\n    - \"entropy_coeff\": `lambda: random.uniform(0.001, 0.1)`\n    - \"lr\": `lambda: random.uniform(1e-5, 1e-2)`\n\n**For PPO (`run_ppo.py`):\n**- `perturbation_interval`: `args.freq` (default 50,000 timesteps).\n- `num_samples`: `args.num_samples` (default 4).\n- `stop` criteria: `args.max` (default 1,000,000 timesteps).\n- Default environment: \"BipedalWalker-v2\".\n- `hyperparam_mutations` for PB2:\n    - \"lambda\": `lambda: random.uniform(0.9, 1.0)`\n    - \"clip_param\": `lambda: random.uniform(0.1, 0.5)`\n    - \"lr\": `lambda: random.uniform(1e-3, 1e-5)`\n    - \"train_batch_size\": `lambda: random.randint(int(args.batchsize.split(\"_\")[0]), int(args.batchsize.split(\"_\")[1]))` (where `args.batchsize` defaults to \"1000_60000\")"
}

# Instructions
Following the instructions below, please provide an evaluation of the new method.
Since I aim to pursue research of high academic significance, I request that the assessment be conducted with rigorous standards.
- output
    - novelty_reason
        - Determine whether the new method has novelty, and output the reason.
        - The reason should be as specific as possible.
        - Carefully review the content of the studies provided in "Related Works" before outputting.
    - novelty_score
        - Score the novelty of the new method on a scale of 1 to 10, where 1 means no novelty at all and 10 means extremely high novelty.
    - significance_reason
        - Determine whether the new method is significant, and output the reason.
        - Significance includes both academic and societal importance.
    - significance_score
        - Score the significance of the new method on a scale of 1 to 10, where 1 means no significance at all and 10 means extremely high significance.
Output:
{'novelty_reason': 'SlopeStop-BO departs from existing learning-curve-aware HPO methods (BOIL, DyHPO, MF-MES, etc.) by replacing expensive curve-based modelling with a two-line, task-agnostic slope test that (1) decides on-the-fly when to stop a run and (2) lets a standard GP operate on a compressed 2-D summary (last-window mean, relative budget).  None of the cited works use a pure first-order statistic of the curve as both the stopping criterion and the only additional surrogate input; they either fix budgets (Hyperband family) or fit joint kernels / deep networks over the full curve.  This eliminates joint (x,t) kernels, CNN feature extractors, virtual observations, and costly hyper-parameter learning, offering a new point on the complexity / performance pareto-front.', 'novelty_score': 4, 'significance_reason': 'Experiments show that on LCBench and NAS-Bench-201 SlopeStop-BO cuts total training epochs by ~40 % vs. BOIL and ~50 % vs. fixed-budget BO, while matching BOIL’s median regret and outperforming Hyperband/BOHB by ~60 % at equal wall-time.  Because the method needs only two global scalars (ϵ,γ) and a plain GP, it can be plugged into existing BO codebases with minimal engineering and scales to large campaign budgets where surrogate-side cost dominates.  The demonstrated resource savings at identical optimisation quality make the contribution practically important for real-world large-scale HPO where GPU time is the bottleneck.', 'significance_score': 4}
